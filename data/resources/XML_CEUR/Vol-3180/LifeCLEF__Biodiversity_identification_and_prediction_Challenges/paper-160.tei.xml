<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,363.16,15.42;1,89.29,106.66,383.90,15.42">A Global-Scale Plant Identification using Deep Learning: NEUON Submission to PlantCLEF 2022</title>
				<funder>
					<orgName type="full">Malaysia</orgName>
				</funder>
				<funder>
					<orgName type="full">NEUON AI SDN. BHD.</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,66.76,11.96"><forename type="first">Sophia</forename><surname>Chulif</surname></persName>
							<email>schulif@swinburne.edu.</email>
							<affiliation key="aff0">
								<orgName type="institution">Swinburne University of Technology Sarawak Campus</orgName>
								<address>
									<postCode>93350</postCode>
									<region>Sarawak</region>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Artificial Intelligence</orgName>
								<orgName type="institution">NEUON AI</orgName>
								<address>
									<postCode>94300</postCode>
									<settlement>Sarwak</settlement>
									<region>Malayisa</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,173.98,134.97,61.22,11.96"><forename type="first">Sue</forename><forename type="middle">Han</forename><surname>Lee</surname></persName>
							<email>shlee@swinburne.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Swinburne University of Technology Sarawak Campus</orgName>
								<address>
									<postCode>93350</postCode>
									<region>Sarawak</region>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.20,134.97,93.03,11.96"><forename type="first">Yang</forename><forename type="middle">Loong</forename><surname>Chang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Artificial Intelligence</orgName>
								<orgName type="institution">NEUON AI</orgName>
								<address>
									<postCode>94300</postCode>
									<settlement>Sarwak</settlement>
									<region>Malayisa</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,363.16,15.42;1,89.29,106.66,383.90,15.42">A Global-Scale Plant Identification using Deep Learning: NEUON Submission to PlantCLEF 2022</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">EBDE987004499DC9E11E36518BDDE7D0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>plant classification</term>
					<term>convolutional neural network</term>
					<term>deep learning</term>
					<term>machine learning</term>
					<term>computer vision</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the increasing knowledge of plants globally, it is becoming difficult for human experts to identify plants manually and systematically. Vascular plants alone are estimated to be more than 300,000 species. However, deep learning methods have recently made progress in automating plant identification. The PlantCLEF 2022 challenge this year aims to tackle the problems faced in global plant identification. With the aggregation of various data from different sources, it is a real problem to deal with big data consisting of many classes, unbalanced classes, inaccuracies, duplications, and a diversity of visual contents and quality. Given a training dataset of 4 million images and 80,000 species, the task of the challenge was to identify the correct plant species from 26,868 multi-image plant observations. This paper describes the submissions made by our team to PlantCLEF 2022. We trained several deep learning models based on the Inception-v4 and Inception-ResNet-v2 architectures. The types of networks constructed were a single convolutional neural network (CNN) and a triplet network. They were either initialised on weights pre-trained from the ImageNet dataset or the weights pre-trained from PlantCLEF 2022 dataset. Although we intended to compare the performance between our single CNN and triplet models, unfortunately, we did not manage to obtain the complete results due to resource and time constraints. Nevertheless, we submitted nine runs and our best submission achieved a Macro Averaged Mean Reciprocal Rank score of 0.6078, placing 4th among the 45 submitted runs. In addition, we have shown that web or noisy data does improve generalisation in the identification. Moreover, the ensemble of models from different network architectures, i.e., Inception-v4 and Inception-ResNet-v2, give higher accuracy than a single model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The LifeCLEF Plant Identification Challenge (PlantCLEF) <ref type="bibr" coords="1,354.23,502.12,13.00,10.91" target="#b0">[1]</ref> is part of the Conference and Labs of the Evaluation Forum (CLEF), which tackles various multilingual and multimodal information access evaluations <ref type="bibr" coords="1,225.86,529.22,11.27,10.91" target="#b1">[2]</ref>. This year, the focus of PlantCLEF 2022 was to classify 80,000 plant species. Compared to its past editions, PlantCLEF 2022 offered the largest ever number of classes for training, making it resource-intensive, which is often the case in real-world applications. In the context of global plant identification, the aggregation of various data from different sources poses many challenges. It is a real problem to deal with big data consisting of many classes, unbalanced classes, inaccuracies, duplications, and a diversity of visual contents and quality. The total training datasets provided in this challenge consisted of 4 million images and were grouped as "trusted" or "web". In addition, the metadata included more levels of plant taxonomy, i.e., class, order, family, genus, and species. This paper presents our approach, the network architectures used, the training setup implemented, and the results obtained from our submissions to PlantCLEF 2022.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data</head><p>Two training datasets were downloaded: "trusted" and "web". After removing some duplicates with the same name, the trusted dataset totalled 2,885,052 images. It is derived from academic sources and collaborative platforms, signifying a higher certainty of quality. Meanwhile, the web dataset totalled 1,071,627 images. It is based on search engine queries and suffered from notable errors, which were then semi-automatically cleaned by the organisers. In addition, from the trusted dataset, we segregated 63,119 images to serve as our validation dataset. This validation dataset consists of unique species belonging to one plant observation id from our trusted train dataset. Lastly, the test set consisted of 55,306 images from 26,868 plant observations. The details of the datasets used are represented in Table <ref type="table" coords="2,321.24,438.69,3.74,10.91" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Network Architecture and Models</head><p>The networks implemented in our approach were based on the Inception-v4 and Inception-ResNet-v2 architectures <ref type="bibr" coords="2,199.31,501.97,11.51,10.91" target="#b2">[3]</ref>. Two types of networks were constructed: a single convolutional neural network (CNN) and a triplet network. They were either initialised on weights pre-trained from the ImageNet dataset <ref type="bibr" coords="2,210.50,529.07,12.84,10.91" target="#b3">[4]</ref> or the weights pre-trained from PlantCLEF 2022 dataset.</p><p>Motivation for Triplet Network Convolutional Neural Networks (CNNs) have effectively solved classification tasks in various domains and even perform comparably equal or better than humans. However, a pre-defined number of classes is required before training. If the classification task is assigned with new labels, the network has to be retrained. In addition, CNNs work best when there is sufficient training data.</p><p>In a global-scale plant identification task, it is impractical to retrain the network every time a new species is discovered. Furthermore, many plant species, especially those in remote areas, are rarely photographed. More often than not, the available data for training follows a long-tail Considering these concerns, we opted to implement the metric-learning-based triplet network. Its goal is to learn the similarity and dissimilarities between classes instead of directly classifying them. The network accomplishes this by minimising the embedding distance of the same species while maximising the embedding distance of different species. A small embedding distance indicates the same species. Meanwhile, a large embedding distance indicates different species. Besides, it does not require a pre-defined number of classes before the training. Moreover, as shown in the previous PlantCLEF editions <ref type="bibr" coords="3,275.38,327.40,11.23,10.91" target="#b4">[5,</ref><ref type="bibr" coords="3,289.21,327.40,7.49,10.91" target="#b5">6]</ref>, our triplet networks <ref type="bibr" coords="3,393.56,327.40,11.23,10.91" target="#b6">[7,</ref><ref type="bibr" coords="3,407.39,327.40,8.88,10.91" target="#b7">8]</ref> can generalise plant species equally well with or without less training data than a conventional CNN.</p><p>Single CNN This network resembles a conventional Inception-v4 and Inception-ResNet-v2 neural network. Similarly, it consists of convolutional layers, pooling layers, dropout layers and fully-connected layers, which return the softmax probabilities of its prediction. The multi-task classification is adopted in this network by utilising the five taxonomy labels: Class, Order, Family, Genus, and Species. Table <ref type="table" coords="3,239.83,423.50,5.00,10.91" target="#tab_1">2</ref> shows the multi-task classification labels and their number of classes. The network architecture is visualised in Figure <ref type="figure" coords="3,352.45,437.05,19.87,10.91" target="#fig_1">1 (A)</ref>.</p><p>Triplet Network This network resembles the single CNN mentioned above. However, it consists of two streams and instead of using its fully-connected layer for its predictions, it is used to compute the plants' image embedding representation. In addition, a batch normalisation layer is added, followed by L2-normalisation, and finally, a triplet loss layer<ref type="foot" coords="3,404.90,504.30,3.71,7.97" target="#foot_0">1</ref> to train the optimum embedding representation of the plants. Furthermore, instead of its original 1536 features in the fully-connected layer, we reduced its final feature vector to 500. Due to resource limitation, we did not adopt the multi-classification approach in this training. Only the Species taxonomy label is utilised. The network architecture is visualised in Figure <ref type="figure" coords="3,377.20,560.25,5.07,10.91" target="#fig_0">2</ref> (A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Training Setup</head><p>The networks were set up using Tensorflow 1.12 <ref type="bibr" coords="3,309.82,609.58,12.93,10.91" target="#b8">[9]</ref> and TF-Slim <ref type="bibr" coords="3,383.22,609.58,18.00,10.91" target="#b9">[10]</ref> library with the hyperparameters described in Table <ref type="table" coords="3,226.45,623.13,3.77,10.91" target="#tab_2">3</ref>. Random cropping, colour distortion, and horizontal flipping were also applied to the images during training of the networks. The scripts and lists used are available at https://github.com/NeuonAI/plantclef2022_challenge. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Inference Procedure</head><p>The two main evaluation methods of the models used the Argmax function and embedding dictionary similarity comparison. Argmax is used in the single CNN, while the embedding dictionary similarity comparison is used in the triplet network. The inference procedure for the single CNN and triplet network are visualised in Figures <ref type="figure" coords="4,361.10,319.33,16.49,10.91" target="#fig_1">1 (B</ref>) and 2 (B), respectively. The following steps describe the overall inference procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single CNN</head><p>1. Group the images based on the same observation id (if the test set is used). image, there will be 10 predictions for each image for each label (Class, Order, Family, Genus, Species). 5. Average the 10 prediction probabilities of each image classification. 6. Obtain the Top-1 and Top-5 accuracy (if the validation set is used). 7. Obtain the Top-30 accuracy (if the test set is used).</p><p>Triplet Network Before inference, a sample from the training dataset (trusted) is randomly chosen to create a dictionary list. The dictionary list totalled 592,258 images which consist of 80,000 species. A maximum of ten images represent each species in the dictionary. 1. Group the dictionary images based on the same species. 2. Augment the dictionary images to 10 variations through cropping and flipping. Note that the 10 different variations include the cropping of the top-right, top-left, bottom-right, bottom-left, and centre of the image. Then, these 5 images are horizontally-flipped to obtain a total of 10 image variations. 3. Feed the augmented dictionary images into the network. 4. Obtain the image embeddings of each image. Note that since there are 10 image variations, there will be 10 embeddings for each image. 5. Average the 10 embeddings and save them as a dictionary reference. 6. Repeat steps 2 to 5 until all the 80,000 species embeddings are collected. 7. Group the test image(s) based on the same observation id (if the test set is used). 8. Augment the validation / test image(s) to 10 variations through cropping and flipping as previously. 9. Feed the augmented validation / test images into the network. 10. Obtain the image embeddings of each image. Note that since there are 10 image variations, there will be 10 embeddings for each image. 11. Average the 10 embeddings to obtain the single embedding of each validation / test image. 12. Compute the cosine similarity between the single image embedding and the saved dictionary. 13. Obtain the cosine distance by subtracting the computed cosine similarity from the value of 1. 14. Employ inverse distance weighting on the cosine distance. 15. Acquire the probabilities of the single image embedding mapped to the dictionary. 16. The species mapped to the highest probability denotes the class of the species. 17. Obtain the Top-1 and Top-5 accuracy (if the validation set is used). 18. Obtain the Top-30 accuracy (if the test set is used). 19. Repeat steps 8 to 18 until all the images are evaluated.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>The networks we experimented with were variations of single CNN and triplet networks. They differ in their network architecture, training data, and weights initialised. Table <ref type="table" coords="6,452.33,421.72,5.17,10.91">4</ref> shows the details of our experimented networks. As seen in Table <ref type="table" coords="6,330.41,435.27,3.66,10.91">4</ref>, models 1 to 5 are single CNNs, models 6 to 8 are triplet networks, and models 9 to 12 are single CNN whose weights were initialised from the triplet network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Results</head><p>From the validation dataset, we computed the Top-1 and Top-5 accuracy of the models and tabulated them in Tables <ref type="table" coords="6,207.21,525.64,5.17,10.91">5</ref> and<ref type="table" coords="6,236.36,525.64,3.81,10.91">6</ref>, respectively. Due to the large training dataset and time constraints, most of the models trained were not saturated before we evaluated them. In addition, not all the models experimented with were used in the submissions.</p><p>Based on our experiments, most of the models trained with a higher number of iterations performed better in the validation dataset. Nevertheless, the higher number of iterations does not necessarily depict higher performance. Comparing Model 2 (421,517 steps) and Model 4 (522,583 steps), Model 2 performed better with a Top-1 accuracy of 0.462 compared to Model 4 of 0.4545. Since the validation set was built from the trusted dataset, which was what Model 2 was trained on, it may have resulted in this bias. Furthermore, we find that the single CNN initialised from the triplet network (Model 12b) performed slightly better than the single CNN initialised from ImageNet (Model 5b). Since our models did not saturate, we cannot give a definite answer  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We trained several Inception-v4 and Inception-ResNet-v2 single and triplet deep learning models for the plant identification task in PlantCLEF 2022. Due to its large number of species and training data, it was indeed resource-intensive to experiment. Due to resource and time constraints, our models were not rightfully saturated, and we did not experiment as intended. Therefore, we would like to look into the performance between our single CNN and our triplet models for future work. It is worth looking into their performance when they are both saturated and compared with the same evaluation methods and on different validation sets focusing on unbalanced classes. Nevertheless, we submitted nine runs and our best submission achieved a Macro Averaged Mean Reciprocal Rank score of 0.6078, placing 4th among the 45 submitted runs. In addition, we have shown that web or noisy data does improve generalisation in the identification. Furthermore, the ensemble of models from different network architectures, i.e., Inception-v4 and Inception-ResNet-v2, give higher accuracy than a single model. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,103.64,395.45,404.03,10.91;4,116.56,409.00,391.27,10.91;4,116.56,422.55,389.42,10.91;4,116.56,436.09,174.25,10.91;4,103.64,450.71,289.05,10.91;4,103.64,465.33,404.03,10.91"><head>2 .</head><label>2</label><figDesc>Augment the validation / test image(s) to 10 variations through cropping and flipping. Note that the 10 different variations include the cropping of the top-right, top-left, bottomright, bottom-left, and centre of the image. Then, these 5 images are horizontally-flipped to obtain a total of 10 image variations. 3. Feed the augmented validation / test images into the network. 4. Obtain the prediction results of the images. Note that since there are 10 variations of each.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,89.29,294.82,416.70,8.93;5,89.29,306.83,342.97,8.87;5,89.29,84.19,416.70,204.05"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The training and inference procedure of the Single CNN.Figure 1 (A) illustrates the training process of the network. Figure 1 (B) illustrates the inference process of the network.</figDesc><graphic coords="5,89.29,84.19,416.70,204.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,89.29,334.30,416.69,8.93;6,89.29,346.30,378.07,8.87"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The training and inference procedure of the Triplet Network.Figure 2 (A) illustrates the training process of the network. Figure 2 (B) illustrates the inference process of the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,89.29,334.30,416.69,8.93;6,89.29,346.30,378.07,8.87"><head></head><label></label><figDesc>Figure 2: The training and inference procedure of the Triplet Network.Figure 2 (A) illustrates the training process of the network. Figure 2 (B) illustrates the inference process of the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,89.29,334.30,416.69,8.93;6,89.29,346.30,378.07,8.87;6,89.29,84.19,416.70,243.52"><head></head><label></label><figDesc>Figure 2: The training and inference procedure of the Triplet Network.Figure 2 (A) illustrates the training process of the network. Figure 2 (B) illustrates the inference process of the network.</figDesc><graphic coords="6,89.29,84.19,416.70,243.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,95.27,119.88,16.70,8.87;9,128.06,119.88,26.18,8.87;9,494.21,119.88,20.78,8.87;9,95.27,137.31,4.63,8.87;9,128.06,137.31,76.95,8.87;9,494.20,137.31,25.35,8.87;9,95.27,154.75,4.63,8.87;9,128.06,154.75,158.16,8.87;9,494.20,154.75,25.35,8.87;9,95.27,172.18,4.63,8.87;9,128.06,172.18,215.04,8.87;9,494.20,172.18,25.35,8.87;9,95.27,195.59,4.63,8.87;9,128.06,189.61,275.55,8.87;9,494.21,189.61,25.35,8.87;9,128.06,201.57,105.39,8.87;9,95.27,219.00,4.63,8.87;9,128.06,219.00,332.60,8.87;9,494.20,219.00,25.35,8.87;9,95.27,236.43,4.63,8.87;9,128.06,236.43,192.58,8.87;9,494.20,236.43,25.35,8.87;9,95.27,253.81,5.12,8.93;9,128.06,253.81,331.44,8.93;9,494.20,253.81,28.05,8.93;9,95.27,277.27,4.63,8.87;9,128.06,271.29,309.97,8.87;9,494.21,271.29,25.35,8.87;9,128.06,283.25,207.82,8.87;9,95.27,306.66,4.63,8.87;9,128.06,300.68,281.52,8.87;9,494.21,300.68,20.72,8.87;9,128.06,312.64,105.39,8.87"><head></head><label></label><figDesc>Trusted) + Single-IR (Trusted) + Single-I (Trusted + Web) + 0.6038 Single-IR (Trusted + Web) 5 Single-I (Trusted + Web) + Single-IR (Trusted + Web) + Triplet-IR (Trusted + Web) (Trusted) + Single-I (Trusted + Web) + Single-IR (Trusted + Web) + 0.6011 Single-T-IR (Trusted + Web) + Single-T-IR (Trusted) 9 Single-IR (Trusted) + Single-IR (Trusted) + Single-I (Trusted + Web) + 0.603 Single-IR (Trusted + Web)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="10,89.29,371.19,202.64,8.93;10,89.29,84.19,416.69,274.44"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The Official Results of PlantCLEF 2022.</figDesc><graphic coords="10,89.29,84.19,416.69,274.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,88.99,90.49,322.14,92.86"><head>Table 1</head><label>1</label><figDesc>Details of the Train, Validation, and Test datasets.</figDesc><table coords="2,184.14,121.19,227.00,62.16"><row><cell>Dataset</cell><cell cols="2">No. of images No. of species</cell></row><row><cell>Train (Trusted)</cell><cell>2,821,933</cell><cell>80,000</cell></row><row><cell cols="2">Train (Trusted + Web) 3,893,560</cell><cell>80,000</cell></row><row><cell>Validation</cell><cell>63,119</cell><cell>63,119</cell></row><row><cell>Test</cell><cell>55,306</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,88.99,90.49,416.99,152.98"><head>Table 2</head><label>2</label><figDesc>Details of the multi-task classification labels in the Train dataset. , resulting in the CNN performing well in classes with many training data and poorer in classes with few or no training data.</figDesc><table coords="3,248.72,119.88,97.84,74.12"><row><cell>Label</cell><cell>No. of classes</cell></row><row><cell>Class</cell><cell>8</cell></row><row><cell>Order</cell><cell>84</cell></row><row><cell cols="2">Family 483</cell></row><row><cell>Genus</cell><cell>9,603</cell></row><row><cell cols="2">Species 80,000</cell></row></table><note coords="3,89.29,219.00,51.39,10.91"><p>distribution</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,88.99,90.49,363.16,141.60"><head>Table 3</head><label>3</label><figDesc>Details of the Network Training Hyperparameters.</figDesc><table coords="4,143.12,122.10,309.03,109.98"><row><cell>Hyperparameter</cell><cell>Single CNN</cell><cell>Triplet Network</cell></row><row><cell>Batch Size</cell><cell>128</cell><cell>128</cell></row><row><cell>Input Image Size</cell><cell>299 × 299 × 3</cell><cell>299 × 299 × 3</cell></row><row><cell>Optimizer</cell><cell>Adam Optimizer [11]</cell><cell>Adam Optimizer [11]</cell></row><row><cell>Initial Learning Rate</cell><cell>0.0001</cell><cell>0.0001</cell></row><row><cell cols="2">End-layers Learning Rate 0.0001</cell><cell>0.00001</cell></row><row><cell>Weight Decay</cell><cell>0.00004</cell><cell>0.00004</cell></row><row><cell>Learning Dropout rate</cell><cell>0.2</cell><cell>0.2</cell></row><row><cell>Loss Function</cell><cell cols="2">Softmax Cross Entropy Triplet Loss</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,88.99,90.49,135.01,20.87"><head>Table 7</head><label>7</label><figDesc>Performance of our Submissions.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,108.93,671.04,355.09,8.97"><p>The triplet loss is computed using triplet_semihard_loss function provided in Tensorflow 1.12<ref type="bibr" coords="3,453.46,671.04,10.55,8.97" target="#b8">[9]</ref> </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The resources of this project is supported by <rs type="funder">NEUON AI SDN. BHD.</rs>, <rs type="funder">Malaysia</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>if it improves the model. However, if we were to compare both models in the same training iteration, the model initialised from the triplet network weights (Model 12b) did perform better than the model initialised on ImageNet weights (Model 5b). Among the different networks trained, the triplet network performed the worst, but this is because their evaluation methods differ from one another (Argmax vs dictionary similarity comparison). Moreover, they were trained significantly less than their single CNN counterpart. In addition, the triplet network relies on the image dictionary for inference. The classes with fewer image samples, especially those with a single sample, may not have provided enough features in the dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Submissions</head><p>We submitted nine runs to PlantCLEF 2022, and its details are tabulated in Table <ref type="table" coords="7,445.85,470.85,3.70,10.91">7</ref>. Apart from Run 1, our submissions were constructed from the ensemble of various models. Run 7, which constitutes three single CNNs based on the Inception-v4 and Incpetion-ResNet-v2 architectures, and trained on the trusted and web data, performed the best among our submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results</head><p>The test set was evaluated based on the average Mean Reciprocal Rank (MRR) per species, also known as the Macro-Averaged Mean Reciprocal Rank (MA-MRR) score. Our best Run <ref type="bibr" coords="7,495.01,574.74,11.63,10.91" target="#b6">(7)</ref> achieved an MA-MRR score of 0.6078 and was the fourth-highest among the 45 submissions. On the other hand, the top submission scored an MA-MRR of 0.6269. The results of the overall submissions are summarised in Figure <ref type="figure" coords="7,262.11,615.39,3.74,10.91">3</ref>.</p><p>Comparing our Run 2 (trusted only) and Run 3 (trusted and web), the models trained on both trusted and web datasets performed better. Once again, showing that web or noisy data does improve the generalisation of deep learning as in PlantCLEF 2017 <ref type="bibr" coords="7,376.29,656.03,16.09,10.91" target="#b11">[12]</ref>. Furthermore, combining models of different architectures, i.e., Inception-v4 and Inception-ResNet-v2 did slightly boost the performance from 0.5461 (Run 1: Inception-v4 only) to 0.5536 (Run2: Inception-v4 and Inception-ResNet-v2). Since our triplet models and single CNN initialised on triplet weights did not saturate, it did not help the ensemble models. Consequently, Run 3 and 7 dropped in performance when added with the triplet or single CNN triplet initialised models, as observed in Run 5 and 8.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,638.82,393.33,10.91;9,112.66,652.37,393.32,10.91;9,112.66,665.92,57.08,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,237.09,638.82,268.90,10.91;9,112.66,652.37,60.82,10.91">Overview of PlantCLEF 2022: Image-based plant identification at global scale</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,195.99,652.37,309.99,10.91;9,112.66,665.92,26.38,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,408.79,394.53,10.91;10,112.66,422.34,394.53,10.91;10,112.66,435.89,393.33,10.91;10,112.66,449.44,393.33,10.91;10,112.66,462.99,353.54,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,198.52,435.89,307.47,10.91;10,112.66,449.44,247.50,10.91">Overview of lifeclef 2022: an evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,383.00,449.44,122.99,10.91;10,112.66,462.99,280.38,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,476.54,393.33,10.91;10,112.66,490.09,393.33,10.91;10,112.66,503.63,79.69,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,331.70,476.54,174.29,10.91;10,112.66,490.09,184.05,10.91">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,319.03,490.09,186.96,10.91;10,112.66,503.63,50.10,10.91">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,517.18,394.53,10.91;10,112.28,530.73,395.55,10.91;10,112.66,544.28,236.19,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,249.07,530.73,214.65,10.91">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,472.07,530.73,35.76,10.91;10,112.66,544.28,147.19,10.91">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,557.83,393.59,10.91;10,112.66,571.38,253.38,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,242.83,557.83,195.53,10.91">Overview of lifeclef plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,482.19,557.83,24.05,10.91;10,112.66,571.38,222.69,10.91">CLEF 2020-Conference and labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,584.93,394.53,10.91;10,112.66,598.48,393.33,10.91;10,112.66,612.03,116.14,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,239.02,584.93,264.14,10.91">Overview of plantclef 2021: cross-domain plant identification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,127.39,598.48,337.22,10.91">Working Notes of CLEF 2021-Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2021">2936. 2021</date>
			<biblScope unit="page" from="1422" to="1436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,625.58,395.01,10.91;10,112.66,639.13,325.58,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,207.79,625.58,295.85,10.91">Herbarium-field triplet network for cross-domain plant identification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chulif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,309.62,639.13,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>neuon submission to lifeclef 2020 plant</note>
</biblStruct>

<biblStruct coords="10,112.66,652.68,393.32,10.91;10,112.66,666.22,394.52,10.91;11,112.66,86.97,65.30,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,214.34,652.68,291.64,10.91;10,112.66,666.22,238.56,10.91">Improved herbarium-field triplet network for cross-domain plant identification: Neuon submission to lifeclef 2021 plant</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chulif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,377.83,666.22,99.29,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1526" to="1539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,100.52,394.53,10.91;11,112.48,114.06,395.34,10.91;11,112.66,127.61,394.53,10.91;11,112.66,141.16,395.17,10.91;11,112.66,154.71,394.53,10.91;11,112.30,168.26,395.36,10.91;11,112.66,181.81,330.68,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="11,160.36,168.26,316.02,10.91">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/,softwareavailablefromtensorflow.org" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,195.36,393.53,10.91;11,112.66,208.91,394.04,10.91;11,112.66,222.46,394.10,10.91;11,112.66,236.01,105.10,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<ptr target="https://github.com/google-research/tf-slim" />
		<title level="m" coord="11,301.63,195.36,204.55,10.91;11,112.66,208.91,297.22,10.91">TensorFlow-Slim: A lightweight library for defining, training and evaluating complex models in tensorflow</title>
		<imprint>
			<date type="published" when="2016-06-29">2016. 29-June-2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,249.56,393.33,10.91;11,112.66,263.11,102.10,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m" coord="11,251.96,249.56,172.98,10.91">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,276.66,393.32,10.91;11,112.66,290.20,393.33,10.91;11,112.66,303.75,133.18,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,245.16,276.66,260.82,10.91;11,112.66,290.20,202.18,10.91">Plant identification based on noisy web data: the amazing performance of deep learning (lifeclef 2017)</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,345.55,290.20,160.43,10.91;11,112.66,303.75,77.06,10.91">CLEF: Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">1866</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
