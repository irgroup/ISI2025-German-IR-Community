<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,402.55,15.42">Efficient Model Integration for Snake Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.10,113.06,32.42,11.96"><forename type="first">Jun</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,134.17,113.06,55.28,11.96"><forename type="first">Hao</forename><surname>Chang</surname></persName>
							<email>changhaoustc@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,202.09,113.06,75.64,11.96"><forename type="first">Zhongpeng</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,290.38,113.06,63.48,11.96"><forename type="first">Guochen</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,366.50,113.06,64.77,11.96"><forename type="first">Liwen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,443.92,113.06,39.97,11.96"><forename type="first">Keda</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Ping An Technology Co., Ltd</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,127.00,64.69,11.96"><forename type="first">Shenshen</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,166.63,127.00,63.79,11.96"><forename type="first">Zhihong</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Ping An Technology Co., Ltd</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,248.35,127.00,55.54,11.96"><forename type="first">Zepeng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Ping An Technology Co., Ltd</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<region>Guangdong</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,321.82,127.00,46.39,11.96"><forename type="first">Fang</forename><surname>Gao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Guangxi University</orgName>
								<address>
									<settlement>Nanning</settlement>
									<region>Guangxi</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,399.20,127.00,63.09,11.96"><forename type="first">Feng</forename><surname>Shuang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Guangxi University</orgName>
								<address>
									<settlement>Nanning</settlement>
									<region>Guangxi</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,402.55,15.42">Efficient Model Integration for Snake Classification</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">6C0BFD684E4EEB8331F5A45FA467FE84</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Snake identification</term>
					<term>EfficientNets</term>
					<term>Swin Transformer</term>
					<term>BEiT</term>
					<term>model integration</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An accurate AI-driven system for automating snake species is of great value, allowing doctors to quickly diagnose the condition of injured people and thus effectively reducing deaths due to snake bites. SnakeCLEF 2022 challenge provides a dataset of 1,572 snake species, with information on their habitats. Because the dataset has a long-tailed distribution, it is difficult to perform accurate identification. We train the models by using the methods of AutoAugment, RandAugment and Focal Loss. Finally, we use the model integration method to effectively improve the recognition accuracy and finally achieve macro ùëì1 score of 71.82% on the private leaderboard. Our code can be found at https://github.com/CZP-1/Anefficient-model-integration-based-snake-classification-algorithm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Establishing a snake identification system is important for biodiversity, conservation and global health. But snakes identify visually high intra-class differences and low inter-class differences due to geographic location, color morphology, gender, or age. In contrast to general image classification, the goal of fine-grained image classification is to correctly classify subclasses that belong to the same superclass. Therefore, snake identification is a challenging fine-grained recognition task. Publicly available datasets and benchmarks accelerate machine learning research and allow quantitative comparisons of new approaches. In the fields of deep learning and computer vision, rapid progress over the past decade has been largely driven by the publication of large-scale image datasets. The same is true for the problem of FGVC, where datasets, such as iNaturalist <ref type="bibr" coords="1,208.63,519.46,12.30,10.91" target="#b0">[1]</ref>, have helped develop and evaluate new methods for fine-grained domain adaptation. But there has been a lack of research on snakes. This paper describes a method for image-based snake species identification submitted by the USTC-IAT-United team to the SnakeCLEF 2022 challenge <ref type="bibr" coords="1,236.84,560.10,14.13,10.91" target="#b1">[2]</ref>-a part of LifeCLEF 2022 workshop <ref type="bibr" coords="1,406.11,560.10,15.23,10.91" target="#b2">[3,</ref><ref type="bibr" coords="1,424.07,560.10,7.57,10.91" target="#b3">4]</ref>.</p><p>Generally, the main methods of FGVC mainly focus on how to make the network focus on the most discriminative regions, such as part-based models and attention-based models. Inspired by human observational behavior, these methods introduce localization-induced biases to neural networks with complex structures. In addition, some data augmentation methods and loss functions can also make the model pay more attention to fine-grained feature regions. When some species are visually indistinguishable, some extra-visual information can assist fine-grained recognition, such as spatio-temporal priors and textual descriptions. For example, it is very common that most or all images of a particular snake species may come from a few countries, or even a single country, which inspires us to combine geographic information with fine-grained classification. However, there are currently few studies on snake-related datasets. This motivates us to explore the combination of each module of the deep neural network and model fusion effect for snake fine-grained recognition method.</p><p>To solve the problem of snake fine-grained recognition, we use an exploratory data analysis of the SnakeCLEF 2022 dataset and process the characteristics of the dataset such as imbalance and fine-grained accordingly. We apply state-of-the-art (SOTA) data augment methods to expand the dataset, and use Convolutional Neural Network (CNN) and Transformer <ref type="bibr" coords="2,425.84,290.20,14.76,10.91" target="#b4">[5]</ref> models with a large number of parameters as the backbone network to extract image features. In addition, we use various loss functions and attention mechanisms to deal with indistinguishable features and data imbalance, respectively. We found a weak improvement in the macro ùêπ 1 score of snake recognition results using Focal Loss and Seesaw Loss, although this takes more time.</p><p>The contribution of this paper are summarized as follows:</p><p>‚Ä¢ We test the performance of different backbone networks based on CNN (EfficientNets <ref type="bibr" coords="2,480.27,380.47,12.43,10.91" target="#b5">[6,</ref><ref type="bibr" coords="2,494.57,380.47,8.04,10.91" target="#b6">7]</ref>) and transformer (Swin-L <ref type="bibr" coords="2,223.56,394.02,13.70,10.91" target="#b7">[8]</ref>, BEiT-L <ref type="bibr" coords="2,272.67,394.02,14.06,10.91" target="#b8">[9]</ref>) on snake classification tasks. ‚Ä¢ We use different data augment methods and loss functions on the snake classification task and find a effective combination. ‚Ä¢ We give appropriate weights to different models for model integration and achieve macro ùêπ 1 -score of 78.27% on the public leaderboard and 71.82% on the private leaderboard of the SnakeCLEF 2022.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Fine-grained classification</head><p>Existing fine-grained classification methods can be divided into visual-only classification methods and multimodal classification methods. The former relies entirely on visual information to solve the problem of fine-grained classification, while the latter tries to use multimodal data to build a joint representation that merges multimodal information to facilitate fine-grained recognition. Fine-grained classification methods that rely solely on vision can be broadly classified into two categories: localization methods <ref type="bibr" coords="2,275.39,611.81,17.91,10.91" target="#b9">[10]</ref> and feature encoding methods <ref type="bibr" coords="2,433.72,611.81,16.25,10.91" target="#b10">[11]</ref>.</p><p>Early work <ref type="bibr" coords="2,152.94,625.36,17.99,10.91" target="#b11">[12]</ref> used partial annotations as supervision to make the network notice subtle differences between certain species and suffer from their expensive annotations. RA-CNN <ref type="bibr" coords="2,488.14,638.91,17.85,10.91" target="#b12">[13]</ref> was proposed to amplify subtle regions to recursively learn to distinguish region attention and region-based feature representations at multiple scales in a mutually reinforcing manner. NTSNet <ref type="bibr" coords="3,127.62,100.52,17.88,10.91" target="#b13">[14]</ref> proposed a self-supervised mechanism to efficiently localize information regions.</p><p>Feature encoding methods are dedicated to enriching feature representation capabilities to improve the performance of fine-grained classification. CAP <ref type="bibr" coords="3,377.19,127.61,18.07,10.91" target="#b14">[15]</ref> designed context-aware attention pools to capture subtle changes in images. TransFG <ref type="bibr" coords="3,370.77,141.16,18.07,10.91" target="#b15">[16]</ref> proposed a part selection module that applies a visual transformer to select discriminative image patches.MetaFormer <ref type="bibr" coords="3,486.53,154.71,19.45,10.91" target="#b16">[17]</ref> suggests pooling layer is an alternate to self-attention. Compared with localization methods, feature encoding methods are difficult to clearly distinguish the distinguishing regions between different species.</p><p>To distinguish these challenging visual categories, additional information, i.e., geographic location, attributes, and textual descriptions, can be helpfully utilized. Geo-Aware <ref type="bibr" coords="3,438.37,222.46,17.75,10.91" target="#b17">[18]</ref> introduced geographic information prior to fine-grained classification and systematically examined various previous approaches using geographic information, including post-processing, whitelisting, and feature modulation. Presence-only <ref type="bibr" coords="3,243.83,263.11,17.82,10.91" target="#b18">[19]</ref> also introduced a spatio-temporal prior to the network, which was shown to be effective in improving the final classification performance. CVL <ref type="bibr" coords="3,487.94,276.66,18.05,10.91" target="#b19">[20]</ref> proposed a two-branch network in which one branch learns visual features and the other branch learns textual features, and finally combines these two parts to obtain the final latent semantic representation. All the above methods were designed for specific prior information and cannot be flexibly adapted to different auxiliary information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Snake identification</head><p>The SnakeCLEF dataset is a well-known snake identification dataset, and many teams continue to explore further based on this dataset. BME-TMIT <ref type="bibr" coords="3,316.30,394.13,22.98,10.91" target="#b20">[21]</ref> use a two-stage approach for subject detection and image classification, and augmented the classification's macro ùêπ 1 score with location information. CMP <ref type="bibr" coords="3,204.24,421.23,22.91,10.91" target="#b21">[22]</ref> used residual convolutional neural networks of different sizes, and combined the original and improved cross-entropy to improve the classification performance. In addition, the use of voting for fusion further improved the accuracy on the Snake dataset. FHDO-BCSG <ref type="bibr" coords="3,142.17,461.87,23.50,10.91" target="#b22">[23]</ref> combined the current SOTA CNNs and Transformers, and performed model fusion to obtain excellent score on the SnakeCLEF 2021 challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>As shown in Figure <ref type="figure" coords="3,179.27,534.05,3.78,10.91" target="#fig_0">1</ref>, we use various data augment methods on the data, and then train four models : Swin-L <ref type="bibr" coords="3,157.86,547.60,14.14,10.91" target="#b7">[8]</ref>, EfficientNet-B7 <ref type="bibr" coords="3,245.42,547.60,13.20,10.91" target="#b5">[6]</ref>, EfficientNet-L2 <ref type="bibr" coords="3,331.20,547.60,13.09,10.91" target="#b6">[7]</ref>, and BEiT-L <ref type="bibr" coords="3,398.24,547.60,13.84,10.91" target="#b8">[9]</ref>. Finally, we fuse the features extracted from each model to obtain the final classification results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>For SnakeCLEF 2022 Chenllage, the dataset is based on 187,129 snake observations with 318,532 photographs belonging to 1,572 snake species and observed in 208 countries. The photographs originated from the online biodiversity platform -iNaturalist. In fact, for the trainset, there are a total of 270,251 images from 158,698 snake observations belonging to 1,572 snake species and observed in 207 countries. In addition to the image data information, the challenge also provides metadata, including information on whether snake species are endemic and where snakes are observed. However, despite some exploration and attempts, we still have not found an effective method to improve the accuracy of snake identification using metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data Augment</head><p>We do not experiment with subtle data augment combinations on the SnakeCLEF 2022 dataset, but use NAS-based (Neural Architecture Search) or improved data augment methods, namely AutoAugment <ref type="bibr" coords="4,146.07,388.18,20.78,10.91" target="#b23">[24]</ref>, RandAugment <ref type="bibr" coords="4,232.83,388.18,21.11,10.91" target="#b24">[25]</ref>, and TrivialAugment <ref type="bibr" coords="4,348.28,388.18,19.20,10.91" target="#b25">[26]</ref>.</p><p>AutoAugment. AutoAugment is a simple search-based data augment method. The main idea of which is to create a search space of data augment strategies and evaluate the quality of a particular strategy directly on some datasets. In the experiments of AutoAugment, the authors design a search space where each strategy consists of many substrategies, and in each batch a substrategy is chosen randomly for each image. The substrategies contain two operations, each of which is an image processing method, such as translation, rotation or clipping, and for each operation there is a set of probabilities and magnitudes to characterize the nature of the use of this operation. Using a search algorithm, search for the best policy that enables the neural network to achieve the highest validation set accuracy on the target dataset. AutoAugment achieves the same performance as the semi-supervised approach without using any unlabeled data. Finally, the strategies learned from one dataset can be directly transferred to other similar datasets and perform equally well. For example, the strategy learned in ImageNet can achieve state-of-the-art accuracy on the fine-grained visual classification dataset Stanford Cars without fine-tuning the pre-trained model with new data.</p><p>RandAugment. RandAugment investigates a data augment strategy based on NAS method search, which provides a significant improvement in search cost compared to earlier NAS search data augment strategies. NAS-method-based data augment strategies (e.g., AA and other methods) suffer from two major drawbacks. First, they use separate search phases, thus increasing the complexity of training and greatly increasing the consumption of computational resources. In addition, since the search phases are separate, these methods cannot adjust the regularization strength according to the model or dataset size. That is, we usually train small models by training them on small datasets and then apply them to train large models. The proposal of RandAugment (later referred to as RA) solves these two problems by significantly reducing the search space and allowing training directly on the target task without a proxy task, and by tailoring the regularization strength of data augment to different datasets and models.</p><p>TrivialAugment. The TrivialAugment data augment strategy originates from the NAS approach and outperforms NAS, implementing SOTA's data augment strategy in a simpler way. Although the approach of data augment using NAS method for automatic search is effective, the limitation lies in the need to trade-off the search efficiency and the performance of data augmentation. To solve this problem, TrivialAugment data augment strategy (later referred to as TA) is proposed. Compared with previous data augment strategies, TA is parameter-free and uses only one data augment method per image, so its search cost is almost free compared to AA and even RA, and it achieves SOTA results. TA uses the same data augment method as RandAugment. Specifically, data augment is defined as consisting of a data augment function a and the corresponding intensity value m (some data augment functions do not use intensity values). For each image, TA samples a data augmentation function and an intensity value uniformly, and then returns the augmented image. In addition, while previous methods tend to overlay multiple data augment methods, TA only uses a single data augment method for each image. Using such an approach, the TA-augmented dataset can be viewed as a single image augmented separately using all data augment methods, and then uniformly sampled from it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Image feature extraction backbones</head><p>Backbone is crucial for feature extraction of SnakeCLEF 2022 dataset. Different backbones have different learning ability for features and different focus on the dataset, and the choice of different models can bring us richer data features. In this challenge, we use the following models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Swin Transformer</head><p>There are two main challenges in the application of Transformer to the image field. Visual entities are highly variable, and the visual Transformer performance may not be very good in different scenes. With many pixel points, Transformer's global self-attention-based computation leads to a large computational effort. To address these two problems, Swin Transformer proposes a Transformer with a hierarchical design that includes a sliding window operation, which consists of a non-overlapping local window and an overlapping cross-window. Restricting the attention computation to a single window can introduce the localization of CNNs convolution operations on the one hand and save computation on the other. The overall architecture of Swin Transformer is hierarchical, with four stages, each of which reduces the resolution of the input feature map and expands the perceptual field layer by layer like CNNs. There are several places where it is handled differently than ViT <ref type="bibr" coords="5,289.90,619.60,18.21,10.91" target="#b26">[27]</ref>. ViT will position-encode the embedding on the input, while Swin-T is here as an option. Swin-T does a relative position encoding when calculating Attention. ViT will add a learnable parameter separately as a token for classification, while Swin-T directly averages and outputs classification, which is somewhat similar to the final global average of CNN. pooling layer. On the ImageNet22K <ref type="bibr" coords="6,370.39,86.97,20.67,10.91" target="#b27">[28]</ref> dataset, the accuracy rate of Swin Transformer can reach an astonishing 86.4%, which is one of the current SOTA models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">EfficientNet</head><p>The traditional practice of model scaling is to arbitrarily increase the depth or width of the CNNs, or to use a larger input image resolution for training and evaluation. While these approaches do improve accuracy, they typically require long periods of manual tuning and still often yield sub-optimal performance. A new approach to model scaling is to use a simple and efficient composite coefficient to scale CNNs in a more structured way. Unlike traditional methods that arbitrarily scale network dimensions such as width, depth, and resolution, EfficientNets uniformly scales network dimensions with a fixed set of scale scaling factors. By using this novel scaling method and AutoML (Auto Machine Learning) techniques, the authors call this model EfficientNets, which is up to 10 times more efficient (smaller and faster). To understand the effect of network scaling, the authors systematically studied the effect of scaling different dimensions on the model. While scaling individual dimensions can improve model performance, the authors observe that balancing all dimensions of the network based on available resources can maximize overall performance. In addition, the effectiveness of model scaling relies heavily on the baseline network. To further improve performance, the authors also develop a new baseline network that optimizes accuracy and efficiency by performing neural structure search using the AutoML MNAS framework. The final architecture uses moving inverse bottleneck convolution (MBConv).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Bidirectional Encoder representation from Image Transformer</head><p>Following the development of BERT <ref type="bibr" coords="6,244.08,415.96,21.35,10.91" target="#b28">[29]</ref> in the field of natural language processing. proposed a masked image modeling task to pretrain visual Transformers. Specifically, in our pre-training, each image has two views, namely image patches (e.g. 16√ó16 pixels) and visual tokens (i.e. discrete tokens). They first "tokenize" the original image into visual tokens. Then randomly mask some image patches and feed them into the backbone Transformer. The goal of pretraining is to recover original visual tokens from corrupted image patches. After pretraining BEiT, they directly fine-tune model parameters on downstream tasks by appending task layers on the pretrained encoder. Experimental results on image classification and semantic segmentation show that the model achieves better results than previous pre-training methods. For example, base-size BEiT achieves 83.2% top-1 accuracy on ImageNet-1K <ref type="bibr" coords="6,357.48,537.90,19.53,10.91" target="#b27">[28]</ref>, significantly outperforming DeiT <ref type="bibr" coords="6,109.53,551.45,20.24,10.91" target="#b29">[30]</ref> (81.8%) trained from scratch under the same settings. Furthermore, large-size BEiT achieves 86.3% accuracy using only ImageNet-1K, even better than ViT-L <ref type="bibr" coords="6,426.63,565.00,19.54,10.91" target="#b26">[27]</ref> (85.2%) with supervised pretraining on ImageNet-22K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss function</head><p>For the task of snake classification, the Loss function we use is a cross-entropy loss function like the work for training. In addition, we use Focal Loss <ref type="bibr" coords="6,322.96,641.83,18.96,10.91" target="#b30">[31]</ref> and Seesaw Loss <ref type="bibr" coords="6,418.43,641.83,18.96,10.91" target="#b31">[32]</ref> to mitigate the problem of long-tailed distribution of the dataset. Where Focal Loss uses a modulating factor to the cross-entropy loss to reduce the loss contribution from easy examples and elevate the importance of hard examples, Seesaw Loss achieves a relative balance of positive and negative sample gradients by dynamically reducing the weight of the excessive negative sample gradients imposed by the head category on the tail category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Fine-grained classification strategy to improve ùêπ 1 score</head><p>We use the fine-grained classification method of PIM <ref type="bibr" coords="7,330.13,163.79,20.99,10.91" target="#b32">[33]</ref> to help us with the task of snake classification. PIM is an excellent method for fine-grained classification that automatically finds the most discriminative regions and uses local features to provide features that are more helpful for classification. PIM is a plug-in module, so it can be integrated into very many common CNN-based or Transformer-based network backbone, such as Swin Transformer, EfficientNet, etc. The plugin module can output pixel-level feature maps and fuse filtered features to enhance fine-grained visual classification. It can be briefly explained by selecting appropriate output feature maps from the backbone's blocks to input to a weakly supervised selector to filter out regions with strong discriminative power or regions with little relevance to classification, and finally fusing the features from the selector's output with a combiner to obtain prediction results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Firstly, we train various well-known architectures such as EfficientNet-B7, EfficientNet-L2, Swin-L, and BEiT-L on the CNN and Transformer families respectively. Furthermore, we use data augment methods to improve the performance of the model. Finally, the models of various different architectures are integrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>In this section, we describe the complete training and evaluation procedure, including the training strategy, image augment, and testing procedures. At first, the dataset is split 9:1 between training and validation to select backbones. All architectures are initialized with publicly available pretrained checkpoints and the same policies are trained using the PyTorch framework in a Tesla A100 Tensor Core GPU. All neural networks are optimized using stochastic gradient descent with momentum set to 0.9. The starting learning rate (LR) is set to 0.01 and is further reduced by a specific adaptive learning rate planning strategy and the multi-step adjustment strategy is used when training on the full dataset. In order to speed up the efficiency of the model, the batch size is determined according to the memory consumption, which is between 4 and 128. For training, in addition to the conventional data augment methods, we also adopt a more advanced automatic augment technology. More specifically, we use random horizontal flip with 50% probability, random vertical flip with 50% probability, random adjustment crop with 0.8 -1.0 scale, random brightness/contrast adjustment with 40% probability. All images are resized to the desired network input size: in the CNN performance experiments, the input size is 600 √ó 600. For Swin Transformer, the input size is 384 √ó 384, while on BEiT, it is 512 √ó 512.</p><p>In the process of training the model, we replace various loss functions for evaluation. In addition, data augmentation is introduced to fine-tune the training of the data based on the full amount of very unbalanced dataset used for training in the first phase, hoping to improve the accuracy and robustness of the model. Finally, model ensemble is applied, which is fused separately from the result layer and the feature layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Metrics</head><p>According to the requirements of the competition, the evaluation metrics we use is the macro ùêπ 1 score, which is not affected by the class frequency and is more suitable for the long-tailed class distribution observed in nature. Interestingly, despite the high performance requirements for overall classification in nature-related applications, most existing benchmarks only use accuracy as a scoring criterion. Given that the dataset is highly imbalanced and has long-tailed distributions, the learning process may ignore the least present species. Furthermore, using ùêπ ùëö 1 score allows to easily assign a cost value to each label's two error types and measure more task-related performance. For example, mistaking venomous snake species for non-venomous snake species is a more serious problem when it comes to snake identification. Define ùêπ ùëö 1 score as the mean of all ùêπ 1 scores:</p><formula xml:id="formula_0" coords="8,191.33,335.56,314.66,24.43">ùëÉ ùëüùëíùëêùëñùë†ùëñùëúùëõ = ùëá ùëÉ ùëá ùëÉ + ùêπ ùëÉ , ùëÖùëíùëêùëéùëôùëô = ùëá ùëÉ ùëá ùëÉ + ùêπ ùëÅ<label>(1)</label></formula><formula xml:id="formula_1" coords="8,226.36,366.38,279.63,24.43">ùêπ ùëÜ 1 = 2 √ó ùëÉ ùëüùëíùëêùëñùë†ùëñùëúùëõ √ó ùëÖùëíùëêùëéùëôùëô ùëÉ ùëüùëíùëêùëñùë†ùëñùëúùëõ + ùëÖùëíùëêùëéùëôùëô<label>(2)</label></formula><formula xml:id="formula_2" coords="8,258.59,398.17,247.39,33.89">ùêπ ùëö 1 = 1 ùëÅ ùëÅ ‚àëÔ∏Å ùëÜ=1 ùêπ ùëÜ 1<label>(3)</label></formula><p>where ùëÅ represents the number of classes and ùëÜ is the species index.Final Macro ùêπ 1 score is calculated by computing the ùêπ 1 score for each species as the harmonic mean of the species Precision and the Recall .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>In this section, we compare the performance of well-known CNN-based models and Transformerbased models on the macro ùêπ 1 score. Additionally, we calculate the highest macro ùêπ 1 score of the single model on the dataset. Finally, the best results are obtained using model integration. The impact of different data augments. In order to reduce the impact of long-tail distribution and fine-grained on identification, we use different data augmentation methods on the Swin-L, such as AutoAugment, RandAugmet, TrivialAugment. The results can be seen in Table <ref type="table" coords="8,115.79,596.67,3.74,10.91" target="#tab_0">1</ref>. The results show that the combination of RandAugment and Swin-L works best.</p><p>Fine-grained classification strategy. The attention mechanism PIM is introduced to solve part of the problem of Fine-grained classification. As shown in Table <ref type="table" coords="8,392.49,623.77,3.67,10.91">2</ref>, we can see a significant improvement over the original baseline. It is worth noting that we do not use the full data set, but divide the training set and the validation set according to 9:1 for training, and conduct online evaluation on the test set of the challenge. However, it takes more than twice as long  to train a model with PIM module than to train only backbone. Due to our limited computing resources, there is no way to train all the backbones with PIM module. We believe that if we train all the models with PIM and then do model integration, we can improve our performance greatly.</p><p>The impact of different loss function. In addition, the influence of different loss functions on the model accuracy is also tested. As shown in Table <ref type="table" coords="9,336.29,474.94,3.67,10.91" target="#tab_1">3</ref>, while training on 90% trainset, Focal Loss performs better in fine-grained classification.</p><p>The effect of different backbones. As shown in Table <ref type="table" coords="9,346.09,502.03,3.66,10.91" target="#tab_2">4</ref>, we use different backbone training on the full dataset of this challenge, adopt a high-performing data augment approach, replace the loss function with Focal Loss to obtain the results of the online testset evaluation. It is a pity that we do not use PIM because it requires a lot of computing resources and time. It can be seen that EfficientNet-L2 has the best performance, the performance of BEiT-L is only a little worse than that of EfficientNet-L2. From the comparison of the number of parameters, it seems that the model with more parameters works better for CNN-based architecture and transformer-based architecture. Of course, it needs more experiments to prove this.</p><p>Model Integration and Challenge Score. We fuse the softmax layers of each model in turn in different proportions according to the macro ùêπ 1 score order of each model, and use the parameters of the last layer to infer the category of that image. The fusion weights of each of these models are carefully tuned by us, and the final macro ùêπ 1 score is shown in Table <ref type="table" coords="9,480.61,651.08,3.79,10.91">5</ref>. We achieve a result of 71.82% on the public leaderboard (20% of the test data) and a score of 78.22% on the private leaderboard (approximately 80% of the test data). Surprisingly, from the private leaderboard, the result without Swin-L is slightly higher, reaching 71.93%. In the end, as shown in Table <ref type="table" coords="10,230.36,441.80,3.66,10.91">6</ref>, we rank 6th on the public leaderboard and 7th on the private leaderboard.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,211.42,416.69,8.93;4,89.29,223.42,416.70,8.87;4,89.29,235.38,313.45,8.87;4,89.29,84.19,416.70,119.80"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our method for snake classification: firstly, we use data augment methods to pre-process the data, then the processed data is used to train different backbone networks, and finally the features extracted from each model are fused to obtain the final classification results.</figDesc><graphic coords="4,89.29,84.19,416.70,119.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,88.99,90.49,326.37,185.91"><head>Table 1</head><label>1</label><figDesc>The effect of different data augments (on public leaderboard)</figDesc><table coords="9,88.99,120.26,326.37,156.14"><row><cell>Augment method</cell><cell></cell><cell cols="2">ùêπ ùëö 1 score Score fluctuation</cell></row><row><cell>Swin-L</cell><cell></cell><cell>68.14%</cell><cell>-</cell></row><row><cell cols="2">Swin-L + AutoAugment</cell><cell>68.29%</cell><cell>+0.15%</cell></row><row><cell cols="2">Swin-L + RandAugment</cell><cell>68.44%</cell><cell>+0.3%</cell></row><row><cell cols="3">Swin-L + TrivialAugment 68.33%</cell><cell>+0.19%</cell></row><row><cell>Table 2</cell><cell></cell><cell></cell></row><row><cell cols="4">The effect of PIM module (training on 90% trainset, on public leaderboard)</cell></row><row><cell>method</cell><cell cols="3">ùêπ ùëö 1 score Score fluctuation</cell></row><row><cell>Swin-L</cell><cell cols="2">62.38%</cell><cell>-</cell></row><row><cell cols="2">Swin-L + PIM 63.8%</cell><cell></cell><cell>+1.42%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,88.99,298.65,343.66,81.83"><head>Table 3</head><label>3</label><figDesc>The effect of different loss function (training on 90% trainset, on public leaderboard)</figDesc><table coords="9,169.08,328.43,257.12,52.05"><row><cell>loss function</cell><cell cols="2">ùêπ ùëö 1 score Score fluctuation</cell></row><row><cell>EfficientNet-B7 + CE Loss</cell><cell>68.14%</cell><cell>-</cell></row><row><cell cols="2">EfficientNet-B7 + Seesaw Loss 68.64%</cell><cell>+0.5%</cell></row><row><cell>EfficientNet-B7 + Focal Loss</cell><cell>68.89%</cell><cell>+0.75%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,88.99,90.49,430.28,297.51"><head>Table 4</head><label>4</label><figDesc>The effect of different backbones</figDesc><table coords="10,88.99,118.04,430.28,269.96"><row><cell>Backbone</cell><cell cols="4">Parameter ùêπ ùëö 1 score (Public) ùêπ ùëö 1 score (Private)</cell></row><row><cell>Swin-L</cell><cell>197M</cell><cell>68.14%</cell><cell></cell><cell>61.7%</cell></row><row><cell cols="2">Efficientnet-B7 66M</cell><cell>71.79%</cell><cell></cell><cell>64.99%</cell></row><row><cell>BEiT-L</cell><cell>306M</cell><cell>74.77%</cell><cell></cell><cell>67.61%</cell></row><row><cell cols="2">EfficientNet-L2 480M</cell><cell>75.77%</cell><cell></cell><cell>69.6%</cell></row><row><cell>Table 5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Effect of Model Integration</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Backbone</cell><cell></cell><cell cols="2">weight</cell><cell>ùêπ ùëö 1 score(Public) ùêπ ùëö 1 score(Private)</cell></row><row><cell>EfficientNet-L2</cell><cell></cell><cell>1</cell><cell></cell><cell>75.77%</cell><cell>69.6%</cell></row><row><cell>EfficientNet-L2 + BEiT-L</cell><cell></cell><cell>6:4</cell><cell></cell><cell>77.58%</cell><cell>71.76%</cell></row><row><cell cols="2">EfficientNet-L2 + BEiT-L + EfficientNet-B7</cell><cell>6:4:6</cell><cell></cell><cell>78.22%</cell><cell>71.93%</cell></row><row><cell cols="5">EfficientNet-L2 + BEiT-L + EfficientNet-B7 + Swin-L 6:4:6:0.1 78.27%</cell><cell>71.82%</cell></row><row><cell>Table 6</cell><cell></cell><cell></cell><cell></cell></row><row><cell>The final score</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">leaderboard ùêπ ùëö 1 score rank</cell></row><row><cell></cell><cell>Public</cell><cell>78.27%</cell><cell>6</cell></row><row><cell></cell><cell>Private</cell><cell>71.82%</cell><cell>7</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5.">Conclusions</head><p>This paper proposes a model integration approach for fine-grained classification of SnakeCLEF 2022 dataset. Since this dataset has the characteristics of fine-grained and long-tailed distribution. We finally train four models, EfficientNet-L2, BEiT-L, EfficientNet-B7 and Swin-L. We find that for CNN-based or transformer-based models, models with a larger number of parameters work better. Then we fuse the features extracted from each model by the model integration method and obtain a significant improvement in performance. Finally, after adjusting the weights, we achieve macro ùêπ 1 score of 71.82% on private leaderboard and macro ùêπ 1 score of 78.27% on the public leaderboard. A noteworthy conclusion is that model integration is only better when single models perform similarly. If a model's macro ùêπ 1 score much lower than other models (like Swin-L), the improvement it can bring is limited and may even reduce our original macro ùêπ 1 score. A foreseeable result is that the final macro ùêπ 1 score can be improved if we continue to add suitable backbones for model integration.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="11,112.66,111.28,394.53,10.91;11,112.66,124.83,393.33,10.91;11,112.66,138.38,383.04,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,167.27,124.83,251.61,10.91">The inaturalist species classification and detection dataset</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,441.39,124.83,64.60,10.91;11,112.66,138.38,285.12,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8769" to="8778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,151.93,393.33,10.91;11,112.66,165.48,393.33,10.91;11,112.66,179.03,159.39,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,292.93,151.93,107.55,10.91;11,429.01,151.93,76.98,10.91;11,112.66,165.48,165.68,10.91">Automated snake species identification on a global scale</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hr√∫z</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,300.64,165.48,205.35,10.91;11,112.66,179.03,128.70,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Overview of SnakeCLEF</note>
</biblStruct>

<biblStruct coords="11,112.66,192.57,394.53,10.91;11,112.66,206.12,393.33,10.91;11,112.66,219.67,393.33,10.91;11,112.66,233.22,168.28,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,185.62,206.12,320.37,10.91;11,112.66,219.67,208.65,10.91">Lifeclef 2022 teaser: An evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,343.19,219.67,162.80,10.91;11,112.66,233.22,38.01,10.91">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="390" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,246.77,394.53,10.91;11,112.66,260.32,394.53,10.91;11,112.66,273.87,393.33,10.91;11,112.66,287.42,393.33,10.91;11,112.66,300.97,353.54,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,198.52,273.87,307.47,10.91;11,112.66,287.42,247.50,10.91">Overview of lifeclef 2022: an evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqu√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>≈†ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,383.00,287.42,122.99,10.91;11,112.66,300.97,280.38,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,314.52,395.17,10.91;11,112.66,328.07,393.33,10.91;11,112.33,341.62,29.19,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,484.04,314.52,23.79,10.91;11,112.66,328.07,143.41,10.91">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">≈Å</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,264.71,328.07,228.49,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Polosukhin</note>
</biblStruct>

<biblStruct coords="11,112.66,355.17,394.53,10.91;11,112.66,368.71,346.82,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,178.42,355.17,323.86,10.91">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,127.29,368.71,202.02,10.91">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,382.26,393.33,10.91;11,112.66,395.81,393.33,10.91;11,112.66,409.36,157.22,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,281.28,382.26,224.71,10.91;11,112.66,395.81,54.86,10.91">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,189.29,395.81,316.70,10.91;11,112.66,409.36,49.16,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,422.91,393.33,10.91;11,112.39,436.46,393.60,10.91;11,112.66,450.01,250.30,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,370.89,422.91,135.10,10.91;11,112.39,436.46,181.28,10.91">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,317.36,436.46,188.63,10.91;11,112.66,450.01,142.26,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,463.56,393.33,10.91;11,112.66,477.11,107.17,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m" coord="11,226.97,463.56,203.67,10.91">Beit: Bert pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,490.66,393.33,10.91;11,112.66,504.21,393.33,10.91;11,112.66,517.76,283.16,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,209.69,490.66,296.30,10.91;11,112.66,504.21,173.82,10.91">Weakly supervised complementary parts models for fine-grained image classification from the bottom up</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,309.54,504.21,196.44,10.91;11,112.66,517.76,185.04,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3034" to="3043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,531.30,393.33,10.91;11,112.39,544.85,394.79,10.91;11,112.66,558.40,80.57,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,306.74,531.30,199.25,10.91;11,112.39,544.85,76.08,10.91">Hierarchical bilinear pooling for fine-grained visual recognition</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,209.91,544.85,292.21,10.91">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="574" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,571.95,393.33,10.91;11,112.66,585.50,393.33,10.91;11,112.66,599.05,240.15,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,429.86,571.95,76.12,10.91;11,112.66,585.50,168.53,10.91">Cross-x learning for fine-grained visual categorization</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-N</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,308.79,585.50,197.20,10.91;11,112.66,599.05,142.26,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8242" to="8251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,612.60,393.33,10.91;11,112.66,626.15,393.32,10.91;11,112.66,639.70,276.41,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,215.77,612.60,290.22,10.91;11,112.66,626.15,194.88,10.91">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,330.89,626.15,175.09,10.91;11,112.66,639.70,178.50,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4438" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,653.25,393.33,10.91;11,112.66,666.80,394.53,10.91;12,112.66,86.97,80.57,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,337.67,653.25,168.31,10.91;11,112.66,666.80,56.26,10.91">Learning to navigate for fine-grained classification</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,191.55,666.80,310.44,10.91">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="420" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,100.52,393.54,10.91;12,112.66,114.06,323.87,10.91" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Behera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wharton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hewage</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06635</idno>
		<title level="m" coord="12,311.07,100.52,195.13,10.91;12,112.66,114.06,142.15,10.91">Context-aware attentional pooling (cap) for fine-grained visual classification</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,127.61,393.71,10.91;12,112.66,141.16,393.33,10.91;12,112.33,154.71,29.19,10.91" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07976</idno>
		<title level="m" coord="12,457.57,127.61,48.79,10.91;12,112.66,141.16,239.30,10.91">Transfg: A transformer architecture for fine-grained recognition</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,168.26,393.61,10.91;12,112.26,181.81,393.94,10.91;12,112.30,195.36,245.57,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,403.38,168.26,102.88,10.91;12,112.26,181.81,113.95,10.91">Metaformer is actually what you need for vision</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,252.69,181.81,253.50,10.91;12,112.30,195.36,137.31,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10819" to="10829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,208.91,393.33,10.91;12,112.66,222.46,393.33,10.91;12,112.66,236.01,262.63,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,458.38,208.91,47.61,10.91;12,112.66,222.46,171.00,10.91">Geo-aware networks for fine-grained recognition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Potetz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,309.82,222.46,196.16,10.91;12,112.66,236.01,194.36,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,249.56,393.33,10.91;12,112.66,263.11,393.53,10.91;12,112.30,276.66,124.54,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,259.01,249.56,246.98,10.91;12,112.66,263.11,57.10,10.91">Presence-only geographical priors for fine-grained image classification</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,193.33,263.11,312.85,10.91;12,112.30,276.66,26.65,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9596" to="9606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,290.20,394.62,10.91;12,112.66,303.75,394.53,10.91;12,112.66,317.30,65.30,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,182.67,290.20,304.38,10.91">Fine-grained image classification via combining vision and language</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,112.66,303.75,364.29,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5994" to="6002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,330.85,393.33,10.91;12,112.66,344.40,312.01,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,202.64,330.85,303.34,10.91;12,112.66,344.40,89.01,10.91">Incorporation of object detection models and location data into snake species classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Borsodi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Papp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,228.02,344.40,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1499" to="1511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,357.95,393.33,10.91;12,112.66,371.50,103.57,10.91" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="12,302.26,357.95,203.73,10.91;12,112.66,371.50,71.65,10.91">A deep learning method for visual recognition of snake species</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Chamidullin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>≈†ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,385.05,395.17,10.91;12,112.66,398.60,395.01,10.91;12,112.41,412.15,48.96,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="12,231.91,385.05,275.92,10.91;12,112.66,398.60,213.18,10.91">Efficientnets and vision transformers for snake species identification using image and location information</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,357.29,398.60,101.81,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1477" to="1498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,425.70,395.17,10.91;12,112.66,439.25,393.53,10.91;12,112.30,452.79,225.28,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="12,358.94,425.70,148.89,10.91;12,112.66,439.25,118.26,10.91">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,255.11,439.25,251.08,10.91;12,112.30,452.79,137.31,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,466.34,395.17,10.91;12,112.66,479.89,393.33,10.91;12,112.66,493.44,317.74,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="12,302.02,466.34,205.81,10.91;12,112.66,479.89,172.74,10.91">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,308.62,479.89,197.37,10.91;12,112.66,493.44,229.32,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,506.99,394.53,10.91;12,112.66,520.54,395.00,10.91;12,112.41,534.09,38.81,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="12,211.54,506.99,290.80,10.91">Trivialaugment: Tuning-free yet state-of-the-art data augmentation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">G</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,127.24,520.54,334.38,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="774" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,547.64,395.16,10.91;12,112.66,561.19,395.17,10.91;12,112.66,574.74,349.55,10.91" xml:id="b26">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m" coord="12,330.08,561.19,177.76,10.91;12,112.66,574.74,167.84,10.91">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,588.29,393.33,10.91;12,112.66,601.84,394.53,10.91;12,112.66,615.39,103.61,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="12,346.64,588.29,159.35,10.91;12,112.66,601.84,67.28,10.91">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,228.08,601.84,274.55,10.91">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,628.93,393.33,10.91;12,112.66,642.48,363.59,10.91" xml:id="b28">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="12,353.43,628.93,152.55,10.91;12,112.66,642.48,181.08,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,656.03,393.33,10.91;12,112.66,669.58,393.33,10.91;13,112.66,86.97,219.40,10.91" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="12,408.51,656.03,97.48,10.91;12,112.66,669.58,235.84,10.91">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>J√©gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,376.08,669.58,129.91,10.91;13,112.66,86.97,78.76,10.91">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,100.52,394.61,10.91;13,112.66,114.06,395.01,10.91" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="13,327.53,100.52,159.84,10.91">Focal loss for dense object detection</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,112.66,114.06,299.48,10.91">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,127.61,394.53,10.91;13,112.66,141.16,393.58,10.91;13,112.66,154.71,341.92,10.91" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="13,112.66,141.16,228.32,10.91">Seesaw loss for long-tailed instance segmentation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,370.86,141.16,135.39,10.91;13,112.66,154.71,244.01,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9695" to="9704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,168.26,395.17,10.91;13,112.66,181.81,197.93,10.91" xml:id="b32">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P.-Y</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-C</forename><surname>Kao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03822</idno>
		<title level="m" coord="13,258.65,168.26,249.17,10.91;13,112.66,181.81,16.17,10.91">A novel plug-in module for fine-grained visual classification</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
