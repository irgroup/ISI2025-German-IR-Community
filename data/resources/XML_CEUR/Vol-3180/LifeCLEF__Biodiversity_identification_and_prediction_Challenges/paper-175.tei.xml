<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.40,84.74,382.01,15.42;1,89.29,106.66,295.54,15.42">When Large Kernel Meets Vision Transformer: A Solution for SnakeCLEF &amp; FungiCLEF</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.95,134.97,51.12,11.96"><forename type="first">Yang</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,152.71,134.97,53.78,11.96"><forename type="first">Xuhao</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,237.48,134.97,51.88,11.96"><forename type="first">Zijian</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.40,84.74,382.01,15.42;1,89.29,106.66,295.54,15.42">When Large Kernel Meets Vision Transformer: A Solution for SnakeCLEF &amp; FungiCLEF</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">7C6C36BD2AF15D5F49694C2E30144FED</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>FungiCLEF</term>
					<term>SnakeCLEF</term>
					<term>Openset recognition</term>
					<term>Fine-grained image recognition</term>
					<term>Long-tailed recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>LifeCLEF 2022 is an evaluation campaign that is being organized as part of the CLEF initiative labs. This paper record solutions of two competitions in LifeCLEF 2022, i.e., SnakeCLEF 2022 and FungiCLEF 2022. The SnakeCLEF aims at building an automatic and robust image-based system for snake species identification while FungiCLEF aims at automatic recognize fungi species with both images and rich metadata. These two competitions contain a number of challenges, such as fine-grained image recognition, long-tailed recognition and openset recognition. In this paper, we utilize existing efficient techniques and tricks to deal with the long-tailed challenge in both SnakeCLEF and FungiCLEF. We also propose a new backbone by combining the large kernel convolution and vision transformer as both of them have shown superior performance in recognition tasks. For the SnakeCLEF competition, our team achieves a 85.4% Macro F1-Score on the private leaderboard while for the FungiCLEF competition, we achieve a 78.9% Macro F1-Score. Codes are available at: https://github.com/sinbais/CLEF2022.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Building accurate knowledge of the identity and the evolution of species is essential for the sustainable development of humanity, as well as for biodiversity conservation. However, the difficulty of identifying animals and fungi is hindering the aggregation of new data and knowledge. Identifying and naming living organisms is almost impossible for the general public and is often difficult even for professionals and naturalists <ref type="bibr" coords="1,359.83,466.66,11.58,10.91" target="#b0">[1]</ref>. The LifeCLEF Lab has been promoting and evaluating advances in this domain for over 10 years and has has achieved a lot of meaningful results <ref type="bibr" coords="1,186.60,493.75,11.36,10.91" target="#b1">[2,</ref><ref type="bibr" coords="1,200.68,493.75,7.47,10.91" target="#b2">3,</ref><ref type="bibr" coords="1,210.88,493.75,7.57,10.91" target="#b3">4]</ref>.</p><p>In this paper, we combine existing effective techniques for the state-of-the-art pre-trained models and utilize advanced methods in the long-tailed recognition task to give solutions for both competitions (cf. <ref type="bibr" coords="1,192.76,534.40,43.67,10.91">Section 4)</ref>. We also design a new backbone by combining two recent hot topics, i.e., large kernel convolution and vision transformers (cf. Section 3.1). Our overall strategy was to test as many models as possible and spend less time on fine-tuning. The goal was to have many diverse models for ensembling rather than some highly tuned ones. In the ensemble stage, our strategy was to choose the best combination that we thought can avoid overfitting and spend the rest of the time fitting the public leaderboard. In the following of this section, we introduce these two competitions once more.</p><p>CLEF 2022: Conference and Labs of the Evaluation Forum, September 5-8, 2022, Bologna, Italy shenyang 98@njust.edu.cn (Y. Shen); sunxh@njust.edu.cn (X. Sun); zhuzijian@njust.edu.cn (Z. Zhu)</p><p>The SnakeCLEF competition aims at building an automatic and robust image-based system for snake species identification, which is an important goal for biodiversity, conservation, and global health. With recent estimates of 81,410-137,880 deaths and up to three times as many victims of amputations, permanent disability and disfigurement (globally each year) caused by venomous snakebite, such a system has the potential to improve eco-epidemiological data and treatment outcomes (e.g. based on the specific use of antivenoms). This applies especially in remote geographic areas and developing countries, where automatic snake species identification has the greatest potential to save lives. <ref type="bibr" coords="2,263.84,181.81,11.36,10.91" target="#b4">[5,</ref><ref type="bibr" coords="2,277.92,181.81,7.47,10.91" target="#b0">1,</ref><ref type="bibr" coords="2,288.12,181.81,7.57,10.91" target="#b5">6]</ref>.</p><p>The FungiCLEF competition focuses on recognize fungi in the open world. Automatic recognition of fungi species assists mycologists, citizen scientists and nature enthusiasts in species identification in the wild. Its availability supports the collection of valuable biodiversity data. In practice, species identification typically does not depend solely on the visual observation of the specimen but also on other information available to the observer -such as habitat, substrate, location and time. Thanks to rich metadata, precise annotations, and baselines available to all competitors, the challenge provides a benchmark for image recognition with the use of additional information. Moreover, the toxicity of a mushroom can be crucial for the decision of a mushroom picker <ref type="bibr" coords="2,229.52,303.75,11.36,10.91" target="#b6">[7,</ref><ref type="bibr" coords="2,243.60,303.75,7.47,10.91" target="#b0">1,</ref><ref type="bibr" coords="2,253.80,303.75,7.57,10.91" target="#b5">6]</ref>.  For this year challenge, organizers prepared a dataset based on 187,129 snake observations with 318,532 photographs belonging to 1,572 snake species and observed in 208 countries. The data were gathered from the online biodiversity platform -iNaturalist. <ref type="foot" coords="3,404.56,98.76,3.71,7.97" target="#foot_0">1</ref>The provided dataset has a heavy long-tailed class distribution, where the most frequent species is represented by 6,472 images and the least frequent species by just 5 samples <ref type="bibr" coords="3,474.91,127.61,11.43,10.91" target="#b4">[5]</ref>.  The FungiCLEF challenge dataset is based on the data from the Danish Fungi 2020 dataset <ref type="bibr" coords="3,492.15,453.62,11.28,10.91" target="#b7">[8]</ref>, which contains 295,938 training images belonging to 1,604 species observed mostly in Denmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Datasets and Evaluation Protocol</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Dataset for FungiCLEF</head><p>All training samples passed an expert validation process, guaranteeing high-quality labels. Rich observation metadata about habitat, substrate, time, location, EXIF are provided.</p><p>The test set contains 59,420 observations with 118,676 images and 3,134 species, covering the whole year and includes observations collected across all substrate and habitat types <ref type="bibr" coords="3,468.09,521.36,11.43,10.91" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Evaluation Protocol</head><p>The evaluation metric for this competition is Mean (Macro) F1-Score:</p><formula xml:id="formula_0" coords="3,243.43,593.12,258.70,33.79">Macro F 1 = 1 N N ‚àëÔ∏Å i=i F 1 i , (<label>1</label></formula><formula xml:id="formula_1" coords="3,502.13,604.01,3.86,10.91">)</formula><p>where ùëñ is the species index and ùëÅ is the number of classes/species. The F1 score, commonly used in information retrieval, measures accuracy using the statistics precision (P) and recall (R).</p><p>The macro F1 score is not biased by class frequencies and is more suitable for the long-tailed class distributions observed in nature. Precision is the ratio of true positives (TP) to all predicted positives (TP + FP). The Recall is the ratio of true positives (TP) to all actual positives (TP + FN). The F1 metric weights recall and precision equally, and a good retrieval algorithm will maximize both precision and recall simultaneously. Thus, moderately good performance on both will be favoured over extremely good performance on one and poor performance on the other. the huge potential of attention-based models. However, self-attention is originally designed for NLP. It has three shortcomings when dealing with computer vision tasks. (1) It treats images as 1D sequences which neglects the 2D structure of images. (2) The quadratic complexity is too expensive for high-resolution images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>(3) It only achieves spatial adaptability but ignores the adaptability in channels dimension. For vision tasks, different channels often represent different objects <ref type="bibr" coords="4,93.76,651.73,23.16,14.89" target="#b10">[11,</ref><ref type="bibr" coords="4,116.92,651.73,17.37,14.89" target="#b24">25]</ref>. Channel adaptability is also proven important for visual tasks <ref type="bibr" coords="4,41.76,669.60,24.33,14.89" target="#b33">[34,</ref><ref type="bibr" coords="4,66.09,669.60,18.25,14.89">86,</ref><ref type="bibr" coords="4,84.34,669.60,18.25,14.89">60,</ref><ref type="bibr" coords="4,102.59,669.60,18.25,14.89">81,</ref><ref type="bibr" coords="4,120.84,669.60,18.25,14.89" target="#b10">11]</ref>. To solve these problems, we propose a novel visual attention method, namely, LKA. It involves the pros of self-attention such as adaptability and long range dependence. Besides, it benefits from the advantages of convolution such as making use of local contextual information. We first introduce the CoLKANet. Actually, it is a combination of large kernel attention (cf. Fig. <ref type="figure" coords="4,107.96,492.09,4.12,10.91" target="#fig_5">3</ref>) and vision transformer. Since the breakthrough of AlexNet <ref type="bibr" coords="4,381.16,492.09,17.84,10.91" target="#b9">[10]</ref> and ResNet <ref type="bibr" coords="4,454.62,492.09,17.83,10.91" target="#b10">[11]</ref> Convolutional Neural Networks (CNNs) have been the dominating model architecture for computer vision. Meanwhile, with the success of self-attention models in natural language processing, many previous works have attempted to bring in the power of attention into computer vision. When pre-trained on large-scale weakly labeled JFT-300M dataset, ViT can achieve comparable results to state-of-the-art CNNs. In this year, researchers revisited large kernel design in CNNs and found that using a few large convolutional kernels instead of a stack of small kernels could be a more powerful paradigm <ref type="bibr" coords="4,221.42,586.94,16.10,10.91" target="#b11">[12]</ref>. From previous research, we can see that earlier convolution helps transformer see better. Therefore, following this idea, we combine large kernel attention with vision transformer. We use the structure in <ref type="bibr" coords="4,309.59,614.03,37.19,10.91">VAN [9]</ref> and by following CoAtnet <ref type="bibr" coords="4,469.60,614.03,16.39,10.91" target="#b12">[13]</ref>, we replace those earlier CNN structures. Overall structure of the CoLKANet can refer to Fig. <ref type="figure" coords="4,488.48,627.58,3.74,10.91" target="#fig_6">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Vision MLPs</head><formula xml:id="formula_2" coords="5,116.03,95.35,349.16,141.49">Input Output ‚Ä¶ q 1 k 1 v 1 q 2 k 2 v 2 Œ± 1,1 Œ± 1,2 Œ± 1,n q n k n v n (c) Self-attention (b) Large Kernel Attention ‚Ä¶ (a) Overall Structure (b) (b) (b) (c) (c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Other Classification Models</head><p>For other classification models, we have tried EfficientNet <ref type="bibr" coords="5,359.66,309.45,16.41,10.91" target="#b13">[14]</ref>, RepVGG <ref type="bibr" coords="5,425.99,309.45,16.41,10.91" target="#b14">[15]</ref>, EfficientNet-V2 <ref type="bibr" coords="5,104.20,323.00,16.41,10.91" target="#b15">[16]</ref>, Swin Transformer <ref type="bibr" coords="5,214.88,323.00,16.41,10.91" target="#b16">[17]</ref>, VOLO <ref type="bibr" coords="5,270.69,323.00,16.41,10.91" target="#b17">[18]</ref>, ViTAE <ref type="bibr" coords="5,328.42,323.00,16.41,10.91" target="#b18">[19]</ref>, ViT <ref type="bibr" coords="5,372.76,323.00,18.07,10.91" target="#b19">[20]</ref> and ConvNeXt <ref type="bibr" coords="5,465.67,323.00,16.41,10.91" target="#b20">[21]</ref>. We choose the combination of Swin Transformer, VOLO, ConvNeXt, ViT and our CoLKANet as the final solution. We found that CNNs are more likely to overfit in this competition (except ConvNeXt) while Transformer seems to be more stable. However, when raising the input resolution of ConvNeXt and EfficientNet, it will gain a substantial improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Details and Tricks</head><p>In this section, we report the details for different backbones and describe all the tricks that we have used to generate the final submission. We also report the scores we have recorded on both public leaderboard and final leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Details for Vision Transformers</head><p>For different competitions, we use different settings for vision transformers.</p><p>SnakeCLEF. We use ViT, Swin Transformer, VOLO and the proposed CoLKANet as backbones. We calculate the 5-fold cross validation accuracy while training with the image resolution of 384 √ó 384 for Swin Transformer and 448 √ó 448 for VOLO. We trained the ViT and CoLKANet by using the whole dataset with the image resolution of 384 √ó 384. We choose 1.2 √ó 10 4 as the initial learning rate while 10 -5 /7 as the minimum learning rate and set weight decay as 2√ó10 -5 . A good technique to reduce overfitting is to stop the model from becoming overconfident. This can be achieved by softening the ground truth using Label Smoothing <ref type="bibr" coords="5,407.84,622.70,16.39,10.91" target="#b21">[22]</ref>. We set the Label Smoothing value as 0.1 according to the original paper <ref type="bibr" coords="5,332.83,636.25,16.17,10.91" target="#b21">[22]</ref>. All these backbone are trained for 15 epochs without warmup. AdamW <ref type="bibr" coords="5,255.93,649.80,17.91,10.91" target="#b22">[23]</ref> optimizer is utilized for training.</p><p>FungiCLEF. We use Swin Transformer, VOLO and the proposed CoLKANet as backbones. We calculate the 5-fold cross validation accuracy while training with the image resolution of 384 √ó 384 for Swin Transformer while 448 √ó 448 for VOLO. We trained the CoLKANet by using the whole dataset with the image resolution of 384 √ó 384. We choose 1.2 √ó 10 4 as the initial learning rate while 10 -5 /7 as the minimum learning rate and set weight decay as 2 √ó 10 -5 . Label Smoothing is set as 0.1. All these backbone are trained for 15 epochs without warmup. AdamW <ref type="bibr" coords="6,128.59,168.26,17.91,10.91" target="#b22">[23]</ref> optimizer is utilized for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Details for CNNs</head><p>We only use ConvNeXt as the CNN backbone in the final submission for both SnakeCLEF and FungiCLEF. We calculate the 5-fold cross validation points while training with the resolution of 384 √ó 384. We also train a single model which use the whole dataset with the image resolution of 448 √ó 448. We choose 1.2 √ó 10 4 as the maximum learning rate while 10 -5 /7 as the minimum learning rate and set weight decay as 2 √ó 10 -5 . Label Smoothing is set as 0.1. We apply warmup and gradually increase the learning rate for 3 epochs. Then, another optimization is to apply Cosine Schedule to adjust our LR during the following 15 epochs. AdamW <ref type="bibr" coords="6,431.27,299.28,18.06,10.91" target="#b22">[23]</ref> optimizer is utilized for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Loss Functions</head><p>As the evaluation metric is Macro F1-Score, we have to deal with the long-tailed problem (If the evaluation metric is Micro F1-Score, adpot the original Cross Entropy Loss is enough.). Because the head classes contain much more training examples, the network makes the weight norm of the head classes larger to approach the optimal solution. It results in predicted probabilities mainly near 1.0. Another fact is that distributions of predicted probability are related to instance numbers. Unlike balanced recognition, applying different strategies for these classes is necessary for solving the long-tailed problem. Therefore, we adopt label-aware smoothing <ref type="bibr" coords="6,450.47,443.85,17.98,10.91" target="#b23">[24]</ref> to solve the over-confidence in cross-entropy and varying distributions of predicted probability issues for both SnakeCLEF and FungiCLEF. It is expressed as:</p><formula xml:id="formula_3" coords="6,147.56,494.75,358.43,33.72">ùëô(ùëû, ùëù) = - ùêæ ‚àëÔ∏Å ùëñ=1 ùëû ùëñ ùëôùëúùëîùëù ùëñ , ùëû ùëñ = {Ô∏É 1 -ùúñ ùë¶ = 1 -ùëì (ùëÅ ùë¶ ), ùëñ = ùë¶ , ùúñùë¶ ùêæ-1 = ùëì (ùëÅùë¶) ùêæ-1 , otherwise ,<label>(2)</label></formula><p>where ùúñ ùë¶ is a small label smoothing factor for Class-ùë¶, relating to its class number ùëÅ ùë¶ . Details about label-aware smoothing can refer the original paper <ref type="bibr" coords="6,346.61,552.99,16.25,10.91" target="#b23">[24]</ref>. It also worth noting that if we increase the weight of some tail classes and head classes, we will get a higher score on the public leaderboard (but useless in private leaderboard). We explain this phenomenon in Section 6.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Important Tricks</head><p>Bellow are 8 important tricks we used during both SnakeCLEF and FungiCLEF.</p><p>‚Ä¢ Augmentation: Augmentation is very import during training stage. We tried combination of RandomResizedCrop, Transpose, HorizontalFlip, VerticalFlip, ShiftScaleRotate, IAAPiece-wiseAffine, HueSaturationValue, RandomBrightnessContrast, OpticalDistortion, GridDistortion, ElasticTransform, Cutout and CoarseDropout designed in Albumentations <ref type="bibr" coords="7,419.05,127.61,17.82,10.91" target="#b24">[25]</ref> at the begining of the competitions. During the middle stage, we replaced them with TrivialAugment <ref type="bibr" coords="7,487.91,141.16,18.07,10.91" target="#b25">[26]</ref> and gain around 0.5% improvement for each single model. We also used Random Erasing <ref type="bibr" coords="7,486.88,154.71,16.24,10.91" target="#b26">[27]</ref>, CutMix <ref type="bibr" coords="7,125.64,168.26,17.88,10.91" target="#b27">[28]</ref> and Mixup <ref type="bibr" coords="7,196.88,168.26,17.88,10.91" target="#b28">[29]</ref> throughout the competitions. They provide strong regularization effects by softening not only the labels but also the images.</p><p>‚Ä¢ Confusion Matrix: A typical way to analyze model performance is using the confusion matrix. As we split the training images into 5 equal parts (i.e., 5-fold), we get the confusion matrix from the valid part when training those 5-fold models (We did not use confusion matrix for models trained with the whole dataset). This trick gain around 0.2% improvement when ensemble different models.</p><p>‚Ä¢ Normalization of Output: Besides confusion matrix, we hope to vote for each model (e.g., Models A, B and C predict that the image belongs to category 1, 2, 1 respectively. Finally we set the image as category 1.) but it performs poor. So we temporarily set another trick, i.e., scale the model output and normalize the maximum value to ùõº, ùõº can not be one for it may be too close to the voting strategy, formula is as follows:</p><p>ùëÅ ùëúùëüùëö(ùëì (ùë•)) = (1/ùëöùëéùë•(ùëì (ùë•))) ùõº ùëì (ùë•) .</p><p>(</p><formula xml:id="formula_4" coords="7,498.27,341.81,7.72,10.91">)<label>3</label></formula><p>It is found that models performs well when ùõº is 0.15 or 0.20. This trick gain around 0.1% improvement when ensemble different models.</p><p>‚Ä¢ Test Time Augementation: It is an very important trick during all the competitions. However, it may lead to overfitting. Performing this trick is very easy: just crop the test images for around 8 to 13 times and calculate the average score. It gains around 0.6% improvement for each single model but unfortunately, it did not work on the private leaderboard in SnakeCLEF.</p><p>‚Ä¢ Pseudo Labelling: We only perform pseudo labelling for SnakeCLEF. This trick did not work in FungiCLEF (We only tried it once and it did not work on public leaderboard. This may be caused by the openset problem.). We generate pseudo labels on test dataset only for tail categories (train data less than 100) by clustering methods, and finetune the model on both training and these test data. It gains around 0.9% improvement on the public leaderboard but useless on the private leaderboard.</p><p>‚Ä¢ Weight Decay Tuning: Our standard recipe uses ‚Ñì2 regularization to reduce overfitting. The Weight Decay parameter controls the degree of the regularization (the larger the stronger) and is applied universally to all learned parameters of the model by default <ref type="bibr" coords="7,430.18,556.01,16.37,10.91" target="#b29">[30]</ref>. More about separating the Normalization parameters from the rest can refer ClassyVision <ref type="bibr" coords="7,437.63,569.56,16.25,10.91" target="#b30">[31]</ref>.</p><p>‚Ä¢ Exponential Moving Average (EMA): EMA <ref type="bibr" coords="7,308.54,583.11,18.07,10.91" target="#b31">[32]</ref> is a technique that allows one to push the accuracy of a model without increasing its complexity or inference time. It performs an exponential moving average on the model weights and this leads to increased accuracy and more stable models. The averaging happens every few iterations and its decay parameter was tuned via grid search.</p><p>‚Ä¢ FixRes mitigations: It is a very important trick in CNNs (Transformers fix image size so it can not be used) and we only use this trick in ConvNeXt. The model performed significantly better if the resolution used during validation was increased from the training size. This effect is studied in detail on the FixRes paper <ref type="bibr" coords="8,218.02,100.52,17.82,10.91" target="#b32">[33]</ref> and two mitigations are proposed: a) one could try to reduce the training resolution so that the accuracy on the validation resolution is maximized or b) one could fine-tune the model on a two-phase training so that it adjusts on the target resolution. Another very important phenomenon is that if we improve the resolution of training images, we will easily gain a better score. The image scaling ratio is set as 0.758 and 0.875 according to the experiments in the original paper <ref type="bibr" coords="8,272.69,168.26,18.06,10.91" target="#b32">[33]</ref> and <ref type="bibr" coords="8,312.95,168.26,18.06,10.91" target="#b29">[30]</ref> . It gains around 0.8% improvement in SnakeCLEF but only 0.2% improvement in FungiCLEF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Generating the Final Submission</head><p>It is very easy for us to generate the final csv file. Specifically, we generate only one prediction for each observation. If one observation has several different test images, we use the model to calculate the predicted value of each image and then calculate the average value for that observation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">SnakeCLEF</head><p>In this section, we report the scores we had recorded in SnakeCLEF. Details can be found in Table <ref type="table" coords="9,116.47,121.08,3.81,10.91" target="#tab_0">1</ref>. Except ViT and VOLO, all the pretrained models are from ImageNet-22k <ref type="bibr" coords="9,459.68,121.08,16.41,10.91" target="#b34">[35]</ref>. Each backbone can be found in timm <ref type="bibr" coords="9,231.92,134.63,16.21,10.91" target="#b35">[36]</ref>. Swin refers to swin_large_patch4_window12_384_in22k, ConvNeXt refers to convnext_large_in22k, VOLO refers to volo_d4_448.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">FungiCLEF</head><p>In this section, we report the scores we had recorded in FungiCLEF. Details can be found in Table <ref type="table" coords="9,115.42,211.46,3.68,10.91" target="#tab_1">2</ref>. Except VOLO, all the pretrained models are from ImageNet-22k <ref type="bibr" coords="9,407.41,211.46,16.13,10.91" target="#b34">[35]</ref>. It worth mention that in this competition, combine models train with LabelAwaerSmoothing (i.e., deal with the long-tailed problem) and CrossEntropy (i.e., do not deal with the long-tailed problem) will get around 1.6% improvement on each single model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Gap Between the Public Leaderboard and Private Leaderboard.</head><p>First of all, at the begining of these two competitions, the most important thing is to guess the data distribution of the test dataset. Based on our experience, we guessed that the data distribution for the test dataset should be roughly the same as the provided train/val dataset, and the scores for the 20% data which showed on the public leaderboard should also fit the distribution of the test dataset (cf. Fig. <ref type="figure" coords="10,256.23,325.61,17.77,10.91" target="#fig_7">5 (b)</ref>). However, during these two competitions, we found if we artificially increase the weight of some categories (especially on SnakeCLEF which did not suffer from OSR), we would get a better score on the public leaderboard. These categories were chosen at random and the best random state gain around 1% improvement on the public leaderborad. At the outset, we thought it a mismatch in the number of some tail categories between the training dataset and the test dataset.</p><p>While towards the end of these two competitions, we made a rather important discovery. In the FungiCLEF competition, if we submit a csv with all labels set as -1, we can get a score of about 0.0005 on the public leaderboard. As the FungiCLEF competition contains 1,604 categories. We derived this score backwards by using the formula for Macro-F1 and got a result that the whole dataset should contain about 60% to 70% openset images (i.e., label= -1).</p><p>Give an example, assuming a total of 59,420 results are to be submitted, then 34,000 of these results have a ground truth of -1 and the other 25,000 samples between 0 and 1,603, a document with all -1's has a macro F1 value of approximately 0.00045. We added this threshold to our earlier submissions and, not surprisingly, the score dropped dramatically. Obviously this is an unreasonable result. Therefore, we had a wild guess: 20% of the data in the public leaderboard, which means, randomly select 20% of these 1,604 categories in FungiCLEF to calculate the score (cf. Fig. <ref type="figure" coords="10,150.82,555.94,14.45,10.91" target="#fig_7">5 (c</ref>)). The same goes for the SnakeCLEF.</p><p>In this setting, we calculate the openset images in the whole dataset should be around 10%. However, the overall strategy: fitting the public leaderboard, seems overfitted. The fungi competition, on the other hand, did not have a significant reduction on the private leaderboard after adjusting the appropriate threshold, as the -1 category had a far greater effect than the other categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Only A Threshold Strategy is Sufficient for the Openset Problem.</head><p>The ability to identify whether or not a test sample belongs to one of the semantic classes in a classifier's training set is critical to practical deployment of the model. Sagar Vaze et al. <ref type="bibr" coords="11,103.04,134.63,18.07,10.91" target="#b36">[37]</ref> demonstrated that the ability of a classifier to make the 'none-of-above' decision is highly correlated with its accuracy on the closed-set classes. They also use this correlation to boost the performance of the maximum softmax probability OSR 'baseline' by improving its closed-set accuracy and with this strong baseline achieve state-of-the-art on a number of OSR benchmarks. Therefore, we only use a simple threshold for the FungiCLEF competition. We also tried post-processing with meta data (by using MetaFormer <ref type="bibr" coords="11,400.67,202.38,17.10,10.91" target="#b37">[38]</ref>) but useless in this competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Fine-grained image recognition is an important problem in computer vision. Combined with the long-tailed problem and the openset problem, the SnakeCLEF and the FungiCLEF become more challenging. In this paper, we report the advanced techniques we had used to deal with these challenges. By combining the recent hot topics in computer vision tasks, i.e., large kernel and vision transformer, we also construct a new model named CoLKANet. For the SnakeCLEF competition, our team achieves a 85.4% Macro F1-Score on the private leaderboard. For the FungiCLEF competition, our team achieves a 78.9% Macro F1-Score on the private leaderboard.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,362.25,149.33,10.71;2,223.62,480.02,51.04,8.51;2,333.43,545.28,72.04,8.51;2,181.00,554.14,8.51,18.56;2,181.00,534.85,8.51,17.10;2,181.00,512.64,8.51,20.02;2,181.00,479.18,8.51,31.25;2,284.18,587.16,84.59,8.51"><head>2. 1 .</head><label>1</label><figDesc>Dataset for SnakeCLEF[Natrix natrix] [Pseudonaja guttata] Head class (most samples) Tail class (most classes)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,172.06,639.32,252.70,8.93;2,129.38,376.27,322.58,249.20"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Class distribution of the training set of SnakeCLEF.</figDesc><graphic coords="2,129.38,376.27,322.58,249.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,192.34,303.36,7.97,17.39;3,192.34,285.28,7.97,16.02;3,192.34,264.47,7.97,18.75;3,192.34,233.12,7.97,29.28;3,307.35,366.89,79.25,7.97;3,202.29,267.65,107.21,7.97;3,333.19,323.51,99.31,7.97;3,333.19,333.39,72.38,7.97"><head></head><label></label><figDesc>Head class (most samples) Tail class (most classes) [Trametes versicolor (L.) Lloyd] [Podosphaera aphanis (Wallr.) U.Braun &amp; S.Takam.]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="3,172.77,424.71,251.27,8.93;3,141.38,176.11,309.41,233.47"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Class distribution of the training set of FungiCLEF.</figDesc><graphic coords="3,141.38,176.11,309.41,233.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="4,41.76,382.84,516.89,13.40;4,41.76,399.22,516.88,13.40;4,41.76,415.60,516.89,13.40;4,41.76,431.98,516.88,13.40;4,41.76,448.36,516.88,13.40;4,41.76,464.74,516.88,13.40;4,41.76,481.12,369.02,13.40;4,119.29,267.57,361.82,94.05"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Decomposition diagram of large-kernel convolution. A standard convolution can be decomposed into three parts: a depth-wise convolution (DW-Conv), a depth-wise dilation convolution (DW-D-Conv) and a 1√ó1 convolution (1√ó1 Conv). The colored grids represent the location of convolution kernel and the yellow grid means the center point. The diagram shows that a 13√ó13 convolution is decomposed into a 5√ó5 depthwise convolution, a 5√ó5 depth-wise dilation convolution with dilation rate 3 and 1√ó1 convolution. Note: zero paddings are omitted in above figure.</figDesc><graphic coords="4,119.29,267.57,361.82,94.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="4,89.29,377.17,417.79,9.15;4,88.93,389.40,417.05,8.87;4,89.29,400.42,417.69,9.96;4,89.29,413.31,416.70,8.87;4,89.29,424.33,416.69,9.96;4,89.29,436.28,417.79,9.96;4,89.29,449.17,172.41,8.87"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Decomposition diagram of large-kernel convolution [9]. ùêª, ùëä, ùê∂ represents for the height, weight and channel of a tensor. A standard convolution can be decomposed into three parts: a depth-wise convolution (DW-Conv), a depth-wise dilation convolution (DW-D-Conv) and a 1 √ó 1 convolution (1 √ó 1 Conv). The colored grids represent the location of convolution kernel and the yellow grid means the center point. The diagram shows that a 13 √ó 13 convolution is decomposed into a 5 √ó 5 depth-wise convolution, a 5 √ó 5 depth-wise dilation convolution with dilation rate 3 and 1 √ó 1 convolution. Note: zero paddings are omitted in above figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="5,191.82,251.28,213.17,8.93"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The proposed structure of the CoLKANet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="10,89.29,252.14,417.30,8.93;10,89.29,264.14,416.70,8.87;10,89.29,276.10,416.69,8.87;10,89.29,288.06,342.50,8.87;10,99.71,84.19,395.84,160.52"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Data distribution of SnakeCLEF. (a) A smaple distribution of the original training data. (b) Our guess about the data distribution of the test dataset on the public leaderborad. (c) The actual distribution of the test dataset on the public leaderboard. The blue dots represent the number of samples contained in a category while the red dots indicate that the number of samples is 0.</figDesc><graphic coords="10,99.71,84.19,395.84,160.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,88.99,328.61,413.11,305.66"><head>Table 1</head><label>1</label><figDesc>Results on SnakeCLEF.</figDesc><table coords="8,95.27,358.01,406.83,276.26"><row><cell>Backbone</cell><cell>Train Resolution</cell><cell>Score</cell><cell>Comments</cell></row><row><cell>Swin baseline</cell><cell>384√ó 384</cell><cell>75.6%</cell><cell>Without any tricks.</cell></row><row><cell>ViT</cell><cell>384√ó 384</cell><cell>79.8%</cell><cell>Pretained model from MAE[34].</cell></row><row><cell>Swin single model</cell><cell>384√ó 384</cell><cell>77.4¬±0.5%</cell><cell>5-fold single model.</cell></row><row><cell>Swin 5-fold ensemble</cell><cell>384√ó 384</cell><cell>80.7%</cell><cell>All the folds trained with LabelAwaerSmoothing and CrossEntropy.</cell></row><row><cell>VOLO single model</cell><cell>448√ó 448</cell><cell>78.7¬±0.7%</cell><cell>5-fold single model.</cell></row><row><cell>VOLO 5-fold + Swin 5-fold</cell><cell>-</cell><cell>82.6%</cell><cell>All the VOLO models are trained with LabelAwaerSmoothing.</cell></row><row><cell>CoLKANet</cell><cell>384√ó 384</cell><cell>80.1%</cell><cell>Train with LabelAwaerSmoothing.</cell></row><row><cell>ConvNeXt single model</cell><cell>384√ó 384</cell><cell>77.9¬±0.8%</cell><cell>5-fold single model.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0,2,4 fold trained with</cell></row><row><cell>ConvNeXt 5-fold ensemble</cell><cell>384√ó 384</cell><cell>81.4%</cell><cell>LabelAwaerSmoothing</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1,3 fold trained with CrossEntropy.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ConvNeXt 448 without FixRes +</cell></row><row><cell>ConvNeXt+CoLKANet</cell><cell>-</cell><cell>83.9%</cell><cell>5-fold CoLKANet 384 + CoLKANet</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Weight ratio is 2:2:1.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ConvNeXt 448 +</cell></row><row><cell></cell><cell></cell><cell></cell><cell>5-fold ConvNeXt 384 +</cell></row><row><cell>Ensemble</cell><cell>-</cell><cell>85.4%</cell><cell>5-fold VOLO + CoLKANet +</cell></row><row><cell></cell><cell></cell><cell></cell><cell>5-fold Swin + ViT</cell></row><row><cell></cell><cell></cell><cell></cell><cell>with all the tricks in Section 4.4.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,88.99,281.11,413.11,259.66"><head>Table 2</head><label>2</label><figDesc>Results on FungiCLEF.</figDesc><table coords="9,95.27,312.73,406.83,228.04"><row><cell>Backbone</cell><cell cols="2">Train Resolution Score</cell><cell>Comments</cell></row><row><cell>Swin baseline</cell><cell>384√ó 384</cell><cell>71.7%</cell><cell>Without any tricks.</cell></row><row><cell>Swin single model</cell><cell>384√ó 384</cell><cell>73.4¬±0.5%</cell><cell>5-fold single model.</cell></row><row><cell>Swin 5-fold ensemble</cell><cell>384√ó 384</cell><cell>76.5%</cell><cell>All the folds trained with LabelAwaerSmoothing and CrossEntropy.</cell></row><row><cell>VOLO single model</cell><cell>448√ó 448</cell><cell>73.8¬±0.8%</cell><cell>5-fold single model.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0,2,4 fold trained with</cell></row><row><cell>VOLO 5-fold ensemble</cell><cell>448√ó 448</cell><cell>76.2%</cell><cell>LabelAwaerSmoothing</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1,3 fold trained with CrossEntropy</cell></row><row><cell>CoLKANet</cell><cell>384√ó 384</cell><cell>75.1%</cell><cell>Train with LabelAwaerSmoothing.</cell></row><row><cell>ConvNeXt single model</cell><cell>384√ó 384</cell><cell>73.6¬±0.5%</cell><cell>5-fold single model.</cell></row><row><cell>ConvNeXt 5-fold ensemble</cell><cell>384√ó 384</cell><cell>76.1%</cell><cell>All the folds trained with LabelAwaerSmoothing and CrossEntropy.</cell></row><row><cell>ConvNeXt</cell><cell>448√ó 448</cell><cell>75.7%</cell><cell>Train with LabelAwaerSmoothing.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ConvNeXt 448 +</cell></row><row><cell></cell><cell></cell><cell></cell><cell>5-fold ConvNeXt 384 +</cell></row><row><cell>Ensemble</cell><cell>-</cell><cell>78.9%</cell><cell>5-fold VOLO + CoLKANet +</cell></row><row><cell></cell><cell></cell><cell></cell><cell>5-fold Swin</cell></row><row><cell></cell><cell></cell><cell></cell><cell>with all the tricks in Section 4.4.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,108.93,671.04,73.66,8.97"><p>www.inaturalist.org</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,112.66,414.48,394.53,10.91;11,112.66,428.03,393.33,10.91;11,112.66,441.57,393.33,10.91;11,112.66,455.12,168.28,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,181.87,428.03,324.12,10.91;11,112.66,441.57,208.65,10.91">LifeCLEF 2022 teaser: An evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,343.19,441.57,162.80,10.91;11,112.66,455.12,38.01,10.91">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="390" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,468.67,394.53,10.91;11,112.66,482.22,395.17,10.91;11,112.66,495.77,393.54,10.91;11,112.66,509.32,218.63,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,258.91,482.22,248.92,10.91;11,112.66,495.77,65.63,10.91">Lifeclef 2017 lab overview: multimedia species identification challenges</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-C</forename><surname>Lombardo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqu√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,201.41,495.77,304.79,10.91;11,112.66,509.32,87.87,10.91">International conference of the cross-language evaluation forum for European languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="255" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,522.87,394.62,10.91;11,112.66,536.42,364.37,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,386.66,522.87,120.62,10.91;11,112.66,536.42,103.34,10.91">Ifsc/usp at imageclef 2012: Plant identification task</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">B</forename><surname>Florindo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">N</forename><surname>Gon√ßalves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">M</forename><surname>Bruno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,242.25,536.42,199.40,10.91">CLEF (Online Working Notes/Labs/Workshop</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,549.97,394.61,10.91;11,112.66,563.52,315.59,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,239.45,549.97,219.60,10.91">Plant identification in an open-world (lifeclef 2016)</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,481.06,549.97,26.21,10.91;11,112.66,563.52,201.59,10.91">CLEF: Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="1609">1609. 2016</date>
			<biblScope unit="page" from="428" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,577.07,393.33,10.91;11,112.66,590.62,393.33,10.91;11,112.66,604.17,159.39,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,292.93,577.07,107.55,10.91;11,429.01,577.07,76.98,10.91;11,112.66,590.62,165.68,10.91">Automated snake species identification on a global scale</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hr√∫z</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,300.64,590.62,205.35,10.91;11,112.66,604.17,128.70,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Overview of SnakeCLEF</note>
</biblStruct>

<biblStruct coords="11,112.66,617.71,394.53,10.91;11,112.66,631.26,394.53,10.91;11,112.66,644.81,393.33,10.91;12,112.66,86.97,393.33,10.91;12,112.66,100.52,353.54,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,194.37,644.81,311.62,10.91;12,112.66,86.97,247.50,10.91">Overview of LifeCLEF 2022: an evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Go√´au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqu√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>≈†ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,383.00,86.97,122.99,10.91;12,112.66,100.52,280.38,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,114.06,393.33,10.91;12,112.66,127.61,395.17,10.91;12,112.66,141.16,232.29,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,339.05,114.06,110.41,10.91;12,480.07,114.06,25.92,10.91;12,112.66,127.61,223.69,10.91">Fungi recognition as an open set classification problem</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>≈†ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,365.49,127.61,142.34,10.91;12,112.66,141.16,201.59,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Overview of FungiCLEF</note>
</biblStruct>

<biblStruct coords="12,112.66,154.71,394.53,10.91;12,112.66,168.26,393.33,10.91;12,112.66,181.81,392.48,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,112.66,168.26,281.26,10.91">Danish fungi 2020-not just another image recognition dataset</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>≈†ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Jeppesen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heilmann-Clausen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Laess√∏e</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Fr√∏slev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,420.58,168.26,85.40,10.91;12,112.66,181.81,294.59,10.91">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1525" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,195.36,393.60,10.91;12,112.66,208.91,146.44,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M.-H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z.-N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09741</idno>
		<title level="m" coord="12,362.79,195.36,109.69,10.91">Visual attention network</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,222.46,393.32,10.91;12,112.66,236.01,354.28,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,295.19,222.46,210.79,10.91;12,112.66,236.01,70.43,10.91">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,191.91,236.01,230.24,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,249.56,395.17,10.91;12,112.66,263.11,395.01,10.91;12,112.41,276.66,38.81,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,259.74,249.56,203.38,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,488.38,249.56,19.45,10.91;12,112.66,263.11,348.39,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,290.20,394.62,10.91;12,112.66,303.75,353.16,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.06717</idno>
		<title level="m" coord="12,354.39,290.20,152.89,10.91;12,112.66,303.75,169.65,10.91">Scaling up your kernels to 31x31: Revisiting large kernel design in CNNs</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,317.30,393.33,10.91;12,112.66,330.85,354.88,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,254.27,317.30,251.72,10.91;12,112.66,330.85,19.47,10.91">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,139.95,330.85,233.51,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3965" to="3977" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,344.40,394.52,10.91;12,112.66,357.95,352.61,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,178.09,344.40,324.20,10.91">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,127.29,357.95,207.49,10.91">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,371.50,393.33,10.91;12,112.66,385.05,393.33,10.91;12,112.66,398.60,159.65,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,332.61,371.50,173.38,10.91;12,112.66,385.05,46.69,10.91">RepvVGG: Making VGG-style convnets great again</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,181.79,385.05,324.20,10.91;12,112.66,398.60,51.39,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13733" to="13742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,412.15,393.33,10.91;12,112.66,425.70,107.17,10.91" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00298</idno>
		<title level="m" coord="12,198.09,412.15,230.16,10.91">EfficientNetv2: Smaller models and faster training</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,439.25,393.33,10.91;12,112.39,452.79,366.29,10.91" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m" coord="12,370.89,439.25,135.10,10.91;12,112.39,452.79,183.23,10.91">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,466.34,394.52,10.91;12,112.66,479.89,173.79,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13112</idno>
		<title level="m" coord="12,297.23,466.34,205.46,10.91">VOLO: Vision outlooker for visual recognition</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,493.44,393.33,10.91;12,112.66,506.99,386.25,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,275.48,493.44,230.50,10.91;12,112.66,506.99,100.02,10.91">ViTAE: Vision transformer advanced by exploring intrinsic inductive bias</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,220.60,506.99,233.51,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,520.54,395.16,10.91;12,112.66,534.09,393.33,10.91;12,112.41,547.64,393.58,10.91;12,112.33,561.19,29.19,10.91" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m" coord="12,421.13,534.09,84.86,10.91;12,112.41,547.64,247.84,10.91">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,574.74,394.53,10.91;12,112.66,588.29,173.79,10.91" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.03545</idno>
		<title level="m" coord="12,395.24,574.74,107.37,10.91">A convnet for the 2020s</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,601.84,393.33,10.91;12,112.66,615.39,393.33,10.91;12,112.66,628.93,184.87,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="12,344.36,601.84,161.63,10.91;12,112.66,615.39,88.38,10.91">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,225.73,615.39,280.25,10.91;12,112.66,628.93,86.75,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,642.48,332.14,10.91" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<title level="m" coord="12,219.53,642.48,193.34,10.91">Fixing weight decay regularization in adam</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,656.03,394.62,10.91;12,112.66,669.58,394.53,10.91;13,112.66,86.97,100.87,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="12,259.21,656.03,223.84,10.91">Improving calibration for long-tailed recognition</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,112.66,669.58,389.80,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="16489" to="16498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,100.52,394.53,10.91;13,112.28,114.06,395.00,10.91;13,112.66,127.61,334.84,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="13,112.28,114.06,254.97,10.91">Albumentations: Fast and flexible image augmentations</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Buslaev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">I</forename><surname>Iglovikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Khvedchenya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Parinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Druzhinin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalinin</surname></persName>
		</author>
		<idno type="DOI">10.3390/info11020125</idno>
		<ptr target="https://www.mdpi.com/2078-2489/11/2/125.doi:10.3390/info11020125" />
	</analytic>
	<monogr>
		<title level="j" coord="13,379.45,114.06,54.58,10.91">Information</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,141.16,394.53,10.91;13,112.66,154.71,395.00,10.91;13,112.41,168.26,38.81,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="13,211.54,141.16,290.80,10.91">Trivialaugment: Tuning-free yet state-of-the-art data augmentation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">G</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,127.24,154.71,334.38,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="774" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,181.81,394.62,10.91;13,112.66,195.36,394.53,10.91;13,112.41,208.91,27.76,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="13,322.30,181.81,161.24,10.91">Random erasing data augmentation</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,112.66,195.36,267.21,10.91">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,222.46,393.33,10.91;13,112.66,236.01,393.32,10.91;13,112.66,249.56,240.15,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="13,328.85,222.46,177.14,10.91;13,112.66,236.01,181.87,10.91">CutMix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,317.15,236.01,188.84,10.91;13,112.66,249.56,142.26,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,263.11,395.17,10.91;13,112.66,276.66,197.93,10.91" xml:id="b28">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m" coord="13,331.02,263.11,176.81,10.91;13,112.66,276.66,16.17,10.91">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,112.66,290.20,29.33,10.91;13,163.15,290.20,44.83,10.91;13,233.76,290.20,21.98,10.91;13,276.90,290.20,9.12,10.91;13,307.19,290.20,21.79,10.91;13,350.14,290.20,68.99,10.91;13,440.29,290.20,32.36,10.91;13,493.82,290.20,14.01,10.91;13,112.66,303.75,14.61,10.91;13,148.70,303.75,58.79,10.91;13,228.91,303.75,24.37,10.91;13,274.71,303.75,48.47,10.91;13,349.28,303.75,23.14,10.91;13,393.85,303.75,112.85,10.91;13,112.66,317.30,333.84,10.91" xml:id="b29">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Vasilis</forename><surname>Vryniotis</surname></persName>
		</author>
		<ptr target="https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/" />
		<title level="m" coord="13,233.76,290.20,21.98,10.91;13,276.90,290.20,9.12,10.91;13,307.19,290.20,21.79,10.91;13,350.14,290.20,68.99,10.91;13,440.29,290.20,32.36,10.91;13,493.82,290.20,14.01,10.91;13,112.66,303.75,14.61,10.91;13,148.70,303.75,58.79,10.91;13,228.91,303.75,24.37,10.91;13,274.71,303.75,44.06,10.91">How to train state-of-the-art models using torchvision&apos;s latest primitives</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,330.85,394.53,10.91;13,112.66,344.40,394.04,10.91;13,112.66,357.95,166.34,10.91" xml:id="b30">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Adcock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guerin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Changhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/ClassyVision" />
		<title level="m" coord="13,359.25,344.40,56.17,10.91">Classy vision</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,371.50,395.17,10.91;13,112.66,385.05,173.54,10.91" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="13,161.27,371.50,282.38,10.91">Exponential moving average versus moving exponential average</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Klinker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,452.17,371.50,55.66,10.91;13,112.66,385.05,94.68,10.91">Mathematische Semesterberichte</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="97" to="107" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,398.60,394.53,10.91;13,112.28,412.15,275.03,10.91" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="13,312.58,398.60,190.03,10.91">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>J√©gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,112.28,412.15,230.24,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,425.70,393.33,10.91;13,112.66,439.25,393.32,10.91;13,112.66,452.79,159.65,10.91" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="13,330.08,425.70,175.91,10.91;13,112.66,439.25,34.52,10.91">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,170.26,439.25,335.73,10.91;13,112.66,452.79,51.39,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,466.34,393.33,10.91;13,112.66,479.89,394.53,10.91;13,112.66,493.44,103.61,10.91" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="13,345.70,466.34,160.29,10.91;13,112.66,479.89,65.95,10.91">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,224.82,479.89,277.69,10.91">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,506.99,8.98,10.91;13,139.08,506.99,51.02,10.91;13,211.20,506.99,35.67,10.91;13,264.31,506.99,27.43,10.91;13,309.18,506.99,34.80,10.91;13,365.09,506.99,141.60,10.91;13,112.66,520.54,281.70,10.91" xml:id="b35">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4414861</idno>
		<idno>doi:</idno>
		<ptr target="10.5281/zenodo.4414861" />
		<title level="m" coord="13,211.20,506.99,35.67,10.91;13,264.31,506.99,27.43,10.91;13,309.18,506.99,29.82,10.91">Pytorch image models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,534.09,393.53,10.91;13,112.66,547.64,244.42,10.91" xml:id="b36">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vaze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06207</idno>
		<title level="m" coord="13,292.43,534.09,213.76,10.91;13,112.66,547.64,61.95,10.91">Open-set recognition: A good closed-set classifier is all you need</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,112.66,561.19,393.53,10.91;13,112.66,574.74,288.50,10.91" xml:id="b37">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02751</idno>
		<title level="m" coord="13,307.65,561.19,198.53,10.91;13,112.66,574.74,106.31,10.91">Metaformer: A unified meta framework for fine-grained recognition</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
