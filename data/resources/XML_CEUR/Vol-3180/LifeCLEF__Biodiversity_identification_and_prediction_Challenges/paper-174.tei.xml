<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.78,84.74,408.61,15.42;1,89.29,106.66,375.11,15.42;1,89.29,128.58,108.84,15.43">TUC Media Computing at BirdCLEF 2022: Strategies in identifying bird sounds in a complex acoustic environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,156.89,145.14,11.96"><forename type="first">Arunodhayan</forename><surname>Sampathkumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universität Chemnitz</orgName>
								<address>
									<addrLine>Str. der Nationen 62</addrLine>
									<postCode>09111</postCode>
									<settlement>Chemnitz</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,246.65,156.89,80.04,11.96"><forename type="first">Danny</forename><surname>Kowerko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universität Chemnitz</orgName>
								<address>
									<addrLine>Str. der Nationen 62</addrLine>
									<postCode>09111</postCode>
									<settlement>Chemnitz</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.78,84.74,408.61,15.42;1,89.29,106.66,375.11,15.42;1,89.29,128.58,108.84,15.43">TUC Media Computing at BirdCLEF 2022: Strategies in identifying bird sounds in a complex acoustic environments</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">EBDB7DF4AAD618241842FA23D79F7715</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Acoustic environment</term>
					<term>Convolutional Neural Network</term>
					<term>Sound Event Detection</term>
					<term>Data Augmentation</term>
					<term>Ensemble</term>
					<term>Birdcall Identification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Birds play an essential role in monitoring the quality of the environment, pollution, and climate changes. Advancements in convolutional neural networks allow us to recognize birds, hence assisting researchers in monitoring the bird population and biodiversity in an ecosystem. This research paper proposed a pipeline that deals with data augmentation strategies and Sound Event Detection (SED) methods to recognize birds in complex environments. Our proposed solution achieved 69th rank among 807 teams at the BirdCLEF 2022 challenge hosted in Kaggle.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The BirdCLEF 2022 challenge proposes to identify rare/endangered bird species from soundscape recordings recorded in Hawaii's location. The challenge was hosted from February 15, 2022 to May 24, 2022 <ref type="bibr" coords="1,199.78,416.72,11.43,10.91" target="#b0">[1]</ref>. Dataset-The training dataset was populated with 14,853 short audio recordings of 152 bird species uploaded by users of Xeno-canto <ref type="bibr" coords="1,278.34,443.82,11.58,10.91" target="#b1">[2]</ref>. Additionally, the training data was supported with metadata containing the information of recording location, type of birds chirp (bird call or song), etc. The test set was populated with approximately 5,500 recordings each 1 minute long to be used for scoring, 21 endangered bird species were used for scoring on the test set (note that participant cannot access these audios). These audio files are sampled to 32 kHz in Ogg format. The preprocessing and the process of generating Mel-spectrograms are discussed in section 3.1. Problem-The training set consists of long Ogg recordings with multiple bird species present, which denotes multilabel classifications. The competition evaluation (F1 score) relies on identifying 21 rare bird species (not 152 bird species), some bird species contain only one recording sample which is 1 to 3 seconds in length as presented in the Figure <ref type="figure" coords="2,450.10,86.97,3.81,10.91" target="#fig_1">1</ref>. The most challenging and motivation of the competition are the weakly labeled train data, and there are multiple distribution domain shifts present, namely shifts in input space, shifts in the prior probability of labels, and shifts in the function which connects train and test recordings. Domain shifts in this competition are large differences in data characteristics between train (clean recordings) and test (noisy recordings) generalizing models on unseen data difficult, as visualized in the Figure <ref type="figure" coords="2,195.68,168.26,3.74,10.91" target="#fig_3">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Total no of Samples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class Labels</head><p>Samples per species for scored birds   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Previous BirdCLEF (2020, 2021) challenges proposed problems related with recognition of bird events in a complex acoustic environments <ref type="bibr" coords="3,285.50,395.33,11.51,10.91" target="#b2">[3]</ref>, <ref type="bibr" coords="3,300.85,395.33,11.51,10.91" target="#b1">[2]</ref>. Researchers from the previous challenges proposed deep learning techniques based on Convolutional Neural Network (CNN) to recognize birds on a large scale <ref type="bibr" coords="3,181.34,422.43,11.28,10.91" target="#b3">[4]</ref>. State-of-the-art CNNs which performed better to recognize birds when combined with different augmentation strategies, metadata information (latitude, longitude, type of call, etc.) improves the performance <ref type="bibr" coords="3,284.83,449.52,11.39,10.91" target="#b3">[4]</ref>, <ref type="bibr" coords="3,300.02,449.52,11.39,10.91" target="#b4">[5]</ref>, <ref type="bibr" coords="3,317.94,449.52,11.38,10.91" target="#b5">[6]</ref>. Furthermore, Pretrained Audio Neural Network (PANN) from the DCASE 2021 audio challenge provided a better generalization capability in predicting the audio events when compared with previous audio-related CNN models <ref type="bibr" coords="3,123.57,490.17,11.39,10.91" target="#b6">[7]</ref>. Sound Event Detection (SED) approaches usually employs two-dimensional CNNs to extract useful time and frequency information from the given audio sample, then combine the information with an attention head to predict the bird events, here the attention head can be either a CNN or Recurrent Neural Network (RNN) <ref type="bibr" coords="3,328.73,530.82,11.43,10.91" target="#b7">[8]</ref>, <ref type="bibr" coords="3,346.70,530.82,11.43,10.91" target="#b8">[9]</ref>, <ref type="bibr" coords="3,364.67,530.82,16.25,10.91" target="#b9">[10]</ref>, <ref type="bibr" coords="3,387.71,530.82,16.25,10.91" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed methods</head><p>The section explains the main components of our solution to the BirdCLEF 2022 Birdcall Identification Challenge. The section comprises dataset preprocessing, data augmentation, Training setup, and Model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data preprocessing</head><p>The raw audio files from the training set of random 5 seconds length were converted to Melspectrograms using torchaudio library <ref type="bibr" coords="4,257.13,121.08,16.09,10.91" target="#b11">[12]</ref>. The following parameters were selected to generate Mel-spectrograms: sample rate 32 kHz, Mel bins 224, minimum frequency 0 Hz, maximum frequency 16000 Hz, fast Fourier transform window (n-fft) 2048, and hop-length to 512. The visual inspection of Mel-spectrograms was performed on a random 5 seconds clip to investigate the non-bird events and hence determine the threshold to have a noise (non-bird events) or silence. The Mel-spectrograms containing noise or silence were removed and treated as noise for the augmentation process. The training data set was split into 5 different stratified folds using Scikit-learn library <ref type="bibr" coords="4,203.25,215.93,16.25,10.91" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data augmentation</head><p>We implemented 8 different augmentation techniques to improve the generalization and robustness of the models. The following augmentation techniques were applied to the raw audio recordings.</p><p>1. Gaussian noise-Adding Gaussian noises with randomly chosen weights to our audio signal and re-normalize the results. 2. Pink noise-The background noise was generated as pink noise. The pink noise was generated using the colored noise library <ref type="bibr" coords="4,310.02,356.16,16.41,10.91" target="#b13">[14]</ref>. Adding background noise is a mixup augmentation where labels of background noise are neglected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Tanh distortion-This technique is adding distortion to recordings. The tanh() function</head><p>can give a rounded "soft clipping" kind of distortion, and the distortion amount is proportional to the loudness of the input and the pre-gain. Tanh is symmetric, so the positive and negative parts of the signal are squashed in the same way. 4. Denoise transform-The denoising steps are the following. Apply the fft to the signal followed by computing the frequencies associated with each coefficient hence keeping only the coefficients that have a low enough frequency (in absolute). Compute the inverse fft <ref type="bibr" coords="4,129.09,480.04,16.25,10.91" target="#b14">[15]</ref>. 5. Gain-Multiply the audio by a random amplitude factor to reduce or increase the volume.</p><p>This technique can help a model become somewhat invariant to the overall gain of the input audio. 6. Loudness normalization-Apply a constant amount of gain to match a specific loudness. 7. Mixup -The training examples for mixup can be constructed using the following formula:</p><formula xml:id="formula_0" coords="4,261.32,577.80,244.66,11.36">x = 𝑥 𝑖 + (1 -𝜏 ) × 𝑥 𝑗<label>(1)</label></formula><formula xml:id="formula_1" coords="4,262.06,597.14,243.93,11.36">y = 𝑦 𝑖 + (1 -𝜏 ) × 𝑦 𝑗<label>(2)</label></formula><p>where   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Models Architecture</head><p>In Table <ref type="table" coords="5,127.97,543.49,5.07,10.91">4</ref> we discuss the model architectures used in our experiments. Single models had reasonable better results when validated using our stratified 5 fold cross-validation. Our validation data were augmented with pink noise to replicate the test set, having strong cross-validation will yield a better performance of the model. From the previous BirdCLEF challenges, Sound Event Detection (SED) models played a crucial role in achieving the top performances. We used the SED based model adopted from DCASE 2021 <ref type="bibr" coords="5,335.49,611.24,16.41,10.91" target="#b15">[16]</ref>. The encoder part of SED models is DenseNet-121, ResNet-50, Noisy student EfficientNet-b0(EfficientNet-b0-ns) whereas the decoder part is the attention head. The output embedding of the encoder is average-pooled along the frequency dimension before being fed into the SED decoder. For the SED decoder, we use 2 layers, 1D CNN and temporal network bidirectional GRU with a hidden size of 256 as presented in Figure <ref type="figure" coords="6,174.64,86.97,3.66,10.91" target="#fig_6">4</ref>. The SED problem was formulated as a multi-label multi-class classification task for bird events. We employ 2 fully connected (FC) layers to produce the SED output. The first FC layer comprises 512 hidden units, which is followed by a sigmoid activation function to produce the posterior probabilities of the bird events. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Details</head><p>A Titan-RTX GPU with 24 GB VRAM has been used to train our models, the reports of training time for each epoch are discussed in the Table <ref type="table" coords="6,288.41,306.11,3.66,10.91" target="#tab_1">1</ref>. Models were trained for 30 epochs using a batch size of 64 and the Adam Optimizer. The loss used was based on Binary Cross-Entropy (BCR) based focal loss. The pytorch-based sampler was used <ref type="bibr" coords="6,325.88,333.20,17.76,10.91" target="#b16">[17]</ref> to balance the train set. Additionally, we use a cosine annealing-based Learning Rate Scheduler with a base Learning Rate (LR) of 0.001. We track the loss function, F1 score, recall during our training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>Table <ref type="table" coords="6,116.87,418.88,5.17,10.91">3</ref> presents the best augmentation combinations, the focus was to develop a model to recognize bird sounds effectively without adding any soundscape recordings to the training samples. The best single augmentation from set 1 had an absolute increase of 8% in the F1 score, when compared with baseline. The best combination augmentations are tanh distortion, mixup different bird species, Gaussian noise, denoise transformation, and loudness normalization achieved a 12% increase in the score when compared with the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Set</head><p>Description Set 1 Single augmentation Set 2 Ranked augmentations 1 and 2 are combined, followed by ranks 3, 4, 5 and 6, 7, 8 are combined. Set 3 The top augmentation combination from set 2 are combined with other augmentation combinations from set 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Description of each set from Table <ref type="table" coords="6,232.05,601.44,3.41,8.87">3</ref>.</p><p>Tables <ref type="table" coords="6,132.01,628.93,5.17,10.91" target="#tab_4">5</ref> and<ref type="table" coords="6,159.71,628.93,5.17,10.91">4</ref> summarize the results. Data augmentation improves the model robustness against anthropogenic noise from the test set as discussed in the Table <ref type="table" coords="6,390.86,642.48,3.66,10.91">4</ref>. The ensemble technique used in this competition was bagging. The bagging of 5 models with different augmentation strategies achieves better performance. The single model results are summarized in Table <ref type="table" coords="6,489.71,669.58,3.74,10.91">4</ref>.  <ref type="table" coords="7,116.06,285.16,5.12,8.93">3</ref> Different augmentation combinations and their respective F1 scores. The model architecture used here was SED-DenseNet-121. For the set descriptions, be referred to Table <ref type="table" coords="7,378.59,309.12,3.48,8.87">2</ref>. The numbers in the heading refer to the IDs in Table <ref type="table" coords="7,188.76,321.07,4.63,8.87" target="#tab_1">1</ref> .</p><p>Table <ref type="table" coords="7,125.98,356.53,4.97,10.91">4</ref> presents single model results of SED-DenseNet-121, SED-ResNet-50, SED-EfficientNet-b0-ns. The models were trained along with the Set 3 data augmentation combination. A detailed description is documented in Table <ref type="table" coords="7,245.25,383.63,3.67,10.91">3</ref>. The best results were obtained when these augmentation combinations were trained along with SED-EfficientNet-b0-ns and CV-BG which achieved an F1 score of 0.74 in PU-LB and 0.69 in PR-LB. Table <ref type="table" coords="7,119.49,437.83,5.17,10.91" target="#tab_4">5</ref> presents ensemble results. The 5 model ensemble of SED-EfficientNet-b0-ns achieved a better F1 score of 0.75 in PU-LB and 0.71 in PR-LB. The difference between the CV-BG and PU-LB for SED-DenseNet-121, ResNet-50 was -8 to 12% whereat for SED-EfficientNet-b0-ns was -3 to 5%. The ensemble result achieved the 69th place out of 807 teams, as shown in Figure <ref type="figure" coords="7,207.57,492.02,3.74,10.91" target="#fig_7">5</ref>.   Bagging ensemble results using combinations of models shown in Table <ref type="table" coords="8,385.13,473.81,3.41,8.87">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Discussion</head><p>The presented research helps to automate the monitoring of bird species to improve the quality of life and keep track of endangered bird species and climate changes. Replicating test set background noise (set soundscape noise) was difficult, in the proposed system the noise-based augmentation strategies without soundscape noise achieve a better F1 score on the test set. The BirdCLEF 2022 focuses on recognizing endangered species, particularly in the Hawaii location where some classes in the train set had a very limited number of samples which made the competition more challenging. In all BirdCLEF challenges, the major problem was the domain shift between train and test set were the recordings of train set were distinct concerning test set since both the dataset was recorded using different microphones. This year's competition focused on recognizing 21 major endangered species in Hawaii, whereat the distribution of samples was majorly unbalanced and some classes even had only one sample.</p><p>The larger increase in performance was obtained by data augmentation methods. Augmentations like tanh, denoise, Gaussian, and pink noise boosted the performance along with mixing up different bird species. The results of single augmentation scored in the range of 0.58 -0.63 (F1 score) which were better than baseline performance. The best-combined augmentation was set 3.3 which performed 12% better than the baseline. Some augmentation strategies such as time stretch and pitch shift turned out to be ineffective in our training pipeline as they consume significantly longer time for training the model.</p><p>The test set focused on predicting 21 classes. Unfortunately training only those 21 classes failed, since the dataset was unbalanced as discussed in Figure <ref type="figure" coords="9,401.23,222.46,3.73,10.91" target="#fig_1">1</ref>. Increasing the length of the audio samples to 10 or 30 seconds chunks had a bad influence on the model performance and even consumed more training time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,89.29,531.33,386.86,8.93"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Distribution of sample frequencies of 21 endangered bird species in Hawaii location.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,274.43,185.44,48.19,7.17;3,275.97,294.69,45.11,7.17"><head></head><label></label><figDesc>(a) Train Sample (b) Test Sample</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="3,89.29,310.13,416.69,8.93;3,89.29,322.13,44.58,8.87;3,197.83,201.42,201.60,89.33"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the domain shift between a) train set with low noise and b) test set with high noise level.</figDesc><graphic coords="3,197.83,201.42,201.60,89.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,134.04,299.44,77.55,8.97;5,258.09,299.44,69.35,8.97;5,363.74,299.44,97.97,8.97;5,118.68,372.20,108.26,8.97;5,130.92,384.01,27.20,7.98;5,264.75,372.20,56.05,8.97;5,398.25,372.20,28.95,8.97;5,149.50,455.93,46.63,8.97;5,257.33,455.93,70.89,8.97;5,373.43,455.93,78.59,8.97"><head></head><label></label><figDesc>(a) No Augmentation (b) Gaussian Noise (c) Background Pink Noise (d) Mixup Random Bird Species (e) Vertical Roll (f) Gain (g) Loudness (h) Tanh Distortion (i) Denoise transform</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="5,89.29,473.36,416.69,8.93;5,89.29,485.37,53.44,8.87"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of different augmentation techniques on time domain, frequency domain and spectrogram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="6,89.29,226.90,244.68,8.93;6,99.64,150.21,395.99,64.12"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example of a multilabel SED classification model.</figDesc><graphic coords="6,99.64,150.21,395.99,64.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="7,89.29,657.80,204.39,8.93"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Leadership board rank based on PR-LB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,103.64,616.48,402.35,52.53"><head>per epoch (min) DenseNet-121 EfficientNet-b0-ns ResNet-50</head><label></label><figDesc>(𝑥 𝑖 ,𝑦 𝑖 ) and (𝑥 𝑗 ,𝑦 𝑗 ) are the two ramdomly selected examples from the training data where 𝜏 is the mixed ratio. 𝜏 is set as 0.1.8. Vertical roll-This method is applied on spectrograms. Roll the spectrogram with respect to height * 𝛼, where 𝛼 is the roll factor (eg.0.05).</figDesc><table coords="5,112.23,85.93,356.34,105.32"><row><cell cols="2">ID Data Augmentation Time Taken 1 Gaussian Noise Time Domain Spectro-gram 6</cell><cell>4</cell><cell>6</cell></row><row><cell>2 Pink Noise</cell><cell>6</cell><cell>4</cell><cell>6</cell></row><row><cell>3 Tanh distortion</cell><cell>6</cell><cell>4</cell><cell>6</cell></row><row><cell>4 Denoise Transform</cell><cell>6</cell><cell>5</cell><cell>7</cell></row><row><cell>5 Gain</cell><cell>4</cell><cell>4</cell><cell>4</cell></row><row><cell>6 Loudness Normalization</cell><cell>4</cell><cell>4</cell><cell>4</cell></row><row><cell>7 Mixup Random Bird Species</cell><cell>9</cell><cell>5</cell><cell>9</cell></row><row><cell>8 Vertical Roll</cell><cell>8</cell><cell>5</cell><cell>8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.99,199.88,402.34,20.87"><head>Table 1</head><label>1</label><figDesc>List of augmentation strategies, their domain of application (time vs. spectral), and respective IDs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,88.99,461.81,32.19,8.93"><head>Table 5</head><label>5</label><figDesc></figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,308.18,395.17,10.91;9,112.66,321.73,328.16,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,245.84,308.18,34.50,10.91;9,309.00,308.18,198.83,10.91;9,112.66,321.73,16.78,10.91">Bird call identification in soundscape recordings</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,152.19,321.73,257.93,10.91">CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2021">2021. 2022</date>
		</imprint>
	</monogr>
	<note>Birdclef</note>
</biblStruct>

<biblStruct coords="9,112.66,335.28,394.53,10.91;9,112.66,348.83,394.53,10.91;9,112.66,362.38,22.69,10.91" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="9,112.66,348.83,389.50,10.91">Overview of birdclef 2020: Bird sound recognition in complex acoustic environments</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Clapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hopping</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,375.93,394.53,10.91;9,112.66,389.48,393.33,10.91;9,112.66,403.03,328.66,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,112.66,389.48,331.54,10.91">Overview of birdclef 2021: Bird call identification in soundscape recordings</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,467.18,389.48,38.81,10.91;9,112.66,403.03,297.96,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,416.58,395.17,10.91;9,112.26,430.13,100.14,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="9,168.33,416.58,339.50,10.91;9,112.26,430.13,24.88,10.91">Audio-based bird species identification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,443.67,313.10,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,167.73,443.67,182.90,10.91">Bird species identification in soundscapes</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,374.03,443.67,21.05,10.91">CLEF</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,457.22,393.53,10.91;9,112.66,470.77,394.04,10.91;9,112.26,484.32,394.92,10.91;9,112.36,500.31,168.57,7.90" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,324.46,457.22,181.73,10.91;9,112.66,470.77,123.08,10.91">Birdnet: A deep learning solution for avian diversity monitoring</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ecoinf.2021.101236</idno>
		<ptr target="https://doi.org/10.1016/j.ecoinf.2021.101236" />
	</analytic>
	<monogr>
		<title level="j" coord="9,252.72,470.77,103.34,10.91">Ecological Informatics</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">101236</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,511.42,393.33,10.91;9,112.66,524.97,394.62,10.91;9,112.66,538.52,239.89,10.91" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="9,376.93,511.42,129.06,10.91;9,112.66,524.97,233.39,10.91">Panns: Large-scale pretrained audio neural networks for audio pattern recognition</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1912.10211" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,552.07,393.33,10.91;9,112.66,565.62,365.41,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="9,319.21,552.07,154.19,10.91">Focal loss for dense object detection</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1708.02002" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,579.17,393.33,10.91;9,112.66,592.72,365.41,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,277.19,579.17,193.94,10.91">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1608.06993" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,606.27,393.33,10.91;9,112.66,619.81,394.04,10.91;9,112.66,633.36,284.56,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,378.48,606.27,127.51,10.91;9,112.66,619.81,135.75,10.91">Robust sound event detection in bioacoustic sensor networks</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lostanlen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farnsworth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kelling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0214168</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0214168.doi:10.1371/journal.pone.0214168" />
	</analytic>
	<monogr>
		<title level="j" coord="9,257.30,619.81,48.89,10.91">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,646.91,393.33,10.91;10,112.66,86.97,394.62,10.91;10,112.31,100.52,218.89,10.91" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="9,407.03,646.91,98.95,10.91;10,112.66,86.97,201.62,10.91">Learning sound event classifiers from web audio with noisy labels</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Favory</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1901.01189" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,114.06,394.53,10.91;10,112.66,127.61,394.52,10.91;10,112.66,141.16,394.52,10.91;10,112.34,154.71,394.93,10.91;10,112.31,168.26,288.85,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,144.04,154.71,277.62,10.91">Torchaudio: Building blocks for audio and speech processing</title>
		<author>
			<persName coords=""><forename type="first">Y.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chourdia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Astafurov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-F</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pollack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Genzel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mahadeokar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Goldsborough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narenthiran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Quenneville-Bélair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2110.15018</idno>
		<ptr target="https://arxiv.org/abs/2110.15018.doi:10.48550/ARXIV.2110.15018" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,181.81,394.53,10.91;10,112.66,195.36,394.53,10.91;10,112.66,208.91,393.32,10.91;10,112.66,222.46,176.63,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,228.10,208.91,182.14,10.91">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,419.22,208.91,86.76,10.91;10,112.66,222.46,82.55,10.91">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,236.01,395.00,10.91;10,112.66,249.56,194.84,10.91" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="10,197.63,236.01,279.88,10.91">Generate gaussian distributed noise with a power law spectrum</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Timmer</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Koenig</surname></persName>
		</author>
		<ptr target="https://pypi.org/project/colorednoise/" />
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,263.11,394.53,10.91;10,112.66,276.66,364.20,10.91" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Defossez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2006.12847</idno>
		<ptr target="https://arxiv.org/abs/2006.12847.doi:10.48550/ARXIV.2006.12847" />
		<title level="m" coord="10,259.51,263.11,242.65,10.91">Real time speech enhancement in the waveform domain</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,290.20,393.32,10.91;10,112.66,303.75,393.32,10.91;10,112.66,317.30,394.04,10.91;10,112.66,330.85,300.57,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,331.02,290.20,174.97,10.91;10,112.66,303.75,314.78,10.91">Sound event localization and detection of overlapping sources using convolutional recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Adavanne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Politis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nikunen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTSP.2018.2885636</idno>
		<ptr target="https://ieeexplore.ieee.org/abstract/document/8567942.doi:10.1109/JSTSP.2018.2885636" />
	</analytic>
	<monogr>
		<title level="j" coord="10,436.39,303.75,69.59,10.91;10,112.66,317.30,166.26,10.91">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="34" to="48" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,344.40,394.04,10.91;10,112.66,357.95,128.48,10.91" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">D S</forename><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://github.com/ufoym/imbalanced-dataset-sampler" />
		<title level="m" coord="10,193.88,344.40,127.09,10.91">Imbalanced dataset sampler</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
