<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.78,84.74,361.92,15.42;1,88.78,106.66,377.40,15.42">Transfer Learning with Self-Supervised Vision Transformer for Large-Scale Plant Identification</title>
				<funder ref="#_rDqTBju">
					<orgName type="full">Korea government (MSIT)</orgName>
				</funder>
				<funder ref="#_faanzVq">
					<orgName type="full">National Research Foundation of Korea</orgName>
					<orgName type="abbreviated">NRF</orgName>
				</funder>
				<funder ref="#_n3bvAah">
					<orgName type="full">Korea Smart Farm R&amp;D Foundation (KosFarm)</orgName>
				</funder>
				<funder>
					<orgName type="full">Ministry of Agriculture, Food and Rural Affairs</orgName>
					<orgName type="abbreviated">MAFRA</orgName>
				</funder>
				<funder ref="#_gg4GwCE">
					<orgName type="full">Rural Development Administration</orgName>
					<orgName type="abbreviated">RDA</orgName>
				</funder>
				<funder>
					<orgName type="full">Ministry of Science and ICT (MSIT)</orgName>
				</funder>
				<funder ref="#_Nq2Uw5v">
					<orgName type="full">Ministry of Education</orgName>
				</funder>
				<funder>
					<orgName type="full">Korea Institute of Planning and Evaluation for Technology in Food, Agriculture, and Forestry</orgName>
					<orgName type="abbreviated">IPET</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,51.45,11.96"><forename type="first">Mingle</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics Engineering</orgName>
								<orgName type="institution">Jeonbuk National University</orgName>
								<address>
									<postCode>54896</postCode>
									<settlement>Jeonbuk</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Core Research Institute of Intelligent Robots</orgName>
								<orgName type="institution">Jeonbuk National University</orgName>
								<address>
									<postCode>54896</postCode>
									<settlement>Jeonbuk</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,158.67,134.97,51.68,11.96"><forename type="first">Sook</forename><surname>Yoon</surname></persName>
							<email>syoon@mokpo.ac.kr</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Mokpo National University</orgName>
								<address>
									<postCode>58554</postCode>
									<settlement>Jeonnam</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,223.00,134.97,77.61,11.96"><forename type="first">Yongchae</forename><surname>Jeong</surname></persName>
							<email>ycjeong@jbnu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics Engineering</orgName>
								<orgName type="institution">Jeonbuk National University</orgName>
								<address>
									<postCode>54896</postCode>
									<settlement>Jeonbuk</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,313.26,134.97,45.74,11.96"><forename type="first">Jaesu</forename><surname>Lee</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Rural Development Administration</orgName>
								<address>
									<postCode>54875</postCode>
									<settlement>Jeonbuk</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,390.00,134.97,73.97,11.96"><forename type="first">Dong</forename><forename type="middle">Sun</forename><surname>Park</surname></persName>
							<email>dspark@jbnu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics Engineering</orgName>
								<orgName type="institution">Jeonbuk National University</orgName>
								<address>
									<postCode>54896</postCode>
									<settlement>Jeonbuk</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Core Research Institute of Intelligent Robots</orgName>
								<orgName type="institution">Jeonbuk National University</orgName>
								<address>
									<postCode>54896</postCode>
									<settlement>Jeonbuk</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.78,84.74,361.92,15.42;1,88.78,106.66,377.40,15.42">Transfer Learning with Self-Supervised Vision Transformer for Large-Scale Plant Identification</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">0F1F69BE410ED93E09C37C562D5E7716</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>plant identification</term>
					<term>image classification</term>
					<term>transfer learning</term>
					<term>computer vision</term>
					<term>self-supervised</term>
					<term>vision transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper is a working note for the PlantCLEF2022 challenge aiming to identify plants with a large-scale dataset, several millions of images and 80,000 classes. Although there are many images, each class only includes 36 images around on average and thus it can be regarded as a few-shot image classification. To address this issue, transfer learning is validated to be useful in many scenarios and a popular strategy is employing a convolution neural network (CNN) pretrained in a supervised manner. But inspired by the literature on computer vision, we instead leverage a self-supervised vision transformer (ViT) and secure the first place with MA-MRR 0.62692, 0.019 higher than the second place, and 0.116 than the third. Furthermore, we achieve 0.64079 if training the model twenty epochs longer. Compared to the popular strategy with CNN, self-supervised ViT has two advantages. First, ViT does not embrace any inductive bias, such as translating invariance embraced in CNN, and thus owns a more powerful model capacity. Second, self-supervised pretraining obtains a task-agnostic feature extractor that may be better for the downstream task. To be more specific, a recently proposed self-supervised ViT model pretrained in ImageNet, masked autoencoder (MAE), is finetuned in PlantCLEF2022 dataset and then tested to report the evaluation. Except for the challenge, we discuss its possible impacts, such as taking the dataset to pretrain a model for plant-related tasks. Especially, our preliminary results suggest that the pretrained model in PlantCLEF2022 essentially contributes to image-based plant disease recognition on several public datasets. Via our analysis and experimental results, we believe that our work encourages the community to utilize the self-supervised ViT model, the PlantCLEF2022 dataset, and our pretrained model in the dataset. Our codes and trained model are public at https://github.com/xml94/PlantCLEF2022.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recognizing different species is one of requirements to maintain biodiversity, however, it requires human experts to spend much time with a high cost <ref type="bibr" coords="1,316.46,561.48,11.27,10.91" target="#b0">[1]</ref>. Simultaneously, deep learning has been showing its potential to automatically classify images. The PlantCLEF2022 challenge <ref type="bibr" coords="1,472.84,575.03,11.42,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,486.98,575.03,9.00,10.91" target="#b1">2]</ref> is held towards this issue to identify plant species given their images and owns a large-scale and complex dataset taken from real field with 80,000 classes and millions of images. Although there are many images in total, each class averagely owns about 36 images and thus the challenge can be regarded as a few-shot image classification <ref type="bibr" coords="2,292.18,406.22,11.48,10.91" target="#b2">[3]</ref>. To address the image classification, transfer learning is verified to be one effective and efficient method, by which useful knowledge can be adapted from a much bigger source dataset to a small target dataset <ref type="bibr" coords="2,389.49,433.32,11.43,10.91" target="#b3">[4]</ref>.</p><p>A popular strategy to perform transfer learning is pretraining a convolution neural network (CNN) in a supervised manner in a source dataset, ImageNet <ref type="bibr" coords="2,376.80,460.41,12.99,10.91" target="#b4">[5]</ref> and then the network is finetuned in a target dataset <ref type="bibr" coords="2,219.91,473.96,11.48,10.91" target="#b5">[6,</ref><ref type="bibr" coords="2,234.23,473.96,7.52,10.91" target="#b6">7,</ref><ref type="bibr" coords="2,244.59,473.96,7.52,10.91" target="#b7">8,</ref><ref type="bibr" coords="2,254.95,473.96,7.65,10.91" target="#b8">9]</ref>. However, there are two weak points in this strategy. First, as pretrained in a supervised way, the learned features of the model is task related <ref type="bibr" coords="2,89.29,501.06,16.41,10.91" target="#b9">[10]</ref>, which could be not good when the target dataset is not similar to the source dataset, especially for a fine-grained target dataset <ref type="bibr" coords="2,273.40,514.61,16.09,10.91" target="#b10">[11]</ref>. In fact, the ImageNet is far from the fine-grained PlantCLEF2022 dataset as discussed in the next section. Second, CNNs embrace an inductive bias, such as translation invariance <ref type="bibr" coords="2,247.35,541.71,17.89,10.91" target="#b11">[12]</ref> that the corresponding class or identity is same if one image is translated a little. Due to the inductive bias, the capacity of CNN-based models is limited and the capacity could be improved if the inductive bias is removed.</p><p>Fortunately, the two weak points are eased very recently by self-supervised learning (SSL) <ref type="bibr" coords="2,89.29,595.91,17.94,10.91" target="#b12">[13]</ref> and vision transformer (ViT) <ref type="bibr" coords="2,240.84,595.91,16.27,10.91" target="#b11">[12]</ref>, respectively. On one hand, SSL employs an predefined task, such as reconstruction <ref type="bibr" coords="2,214.12,609.46,16.14,10.91" target="#b13">[14]</ref>, context prediction <ref type="bibr" coords="2,320.04,609.46,16.13,10.91" target="#b14">[15]</ref>, and predicting rotations <ref type="bibr" coords="2,451.58,609.46,16.14,10.91" target="#b15">[16]</ref>, instead of supervised signals. On the other hand, ViT discards the local connection and shared filters of CNNs and holds global attentions. Considering the character of the PlantCLEF2022 dataset and the current success on computer vision, we employ a self-supervised pretrained vision transformer to address the PlantCLEF2022 challenge and the strategy has been proved to significantly contribute to plant disease recognition <ref type="bibr" coords="3,333.36,86.97,16.41,10.91" target="#b16">[17]</ref>. The comparison of the popular strategy and our strategy is illustrated in Figure <ref type="figure" coords="3,309.96,100.52,3.81,10.91" target="#fig_0">1</ref>. To be more specific, a ViT-based masked autoencoder (MAE) pretrained in a self-supervised manner in ImageNet is finetuned and tested in the PlantCLEF2022 dataset. In spite of the simpleness, we secure the first place of the official challenge with MA-MRR 0.62692, 0.019 higher than the second place and 0.116 than the third. Moreover we found that we can further get a better performance, 0.64079, if we fine-tune the model longer.</p><p>Except for the challenge, we also discuss its possible impacts. We first recognize two distinct characters of the PlantCLEF2022 dataset. On one hand, it is related with observation-level image classification, instead of usual image-level image classification. One observation refers to multiple images taken for a specific target, such as one field plant in the PlantCLEF2022 dataset, which makes one class with heterogeneous and robust features. On the other hand, PlantCLEF2022 introduced a large-scale plant-related image dataset with an immense image variance <ref type="bibr" coords="3,130.56,263.11,16.41,10.91" target="#b17">[18]</ref>. Because of the two characters, we argue that a pretrained model in the Plant-CLEF2022 dataset can be a powerful start point towards plant image-related applications. Not trivially, our preliminary experimental results suggest that it benefits plant disease recognition on several public dataset, such as quicker convergence speed and better performance even with few number of images. To facilitate the related applications, we public our codes and pretrained model at https://github.com/xml94/PlantCLEF2022 and we hope that our work encourages the community to utilize the self-supervised ViT model, the PlantCLEF2022 dataset, and our pretrained model in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Material and evaluation metric</head><p>Training dataset. PlantCLEF2022 dataset consists of training and testing dataset. The official training dataset has two categories, web and trusted. The web one covers about 1.1 million images and 57,000 classes, in which the images and their annotations are not directly from human experts. In contrast, the trusted one in a global scale is collected and annotated by experts, with 2,885,052 images and 80,000 classes. Each class includes 36.1 images averagely and to ease the class-imbalance issue, has no more than 100 images. Except for the images and their labels, some other information are also given, such as species and genus in plant taxonomy. As the datasets are huge and the limitation of computing devices, we just utilize the trusted one as training dataset considering the quality of the annotations.</p><p>The PlantCLEF2022 challenge differs from other plant-related tasks. First, the task objective is recognizing one plant identity given their images whereas other related tasks focus on disease recognition for specific plant, Plant Village <ref type="bibr" coords="3,279.47,565.62,16.10,10.91" target="#b18">[19]</ref>, tomato leaf disease <ref type="bibr" coords="3,388.32,565.62,16.09,10.91" target="#b19">[20]</ref>, and apple leaf disease <ref type="bibr" coords="3,89.29,579.17,16.31,10.91" target="#b20">[21]</ref>. Second, the main visual content of every image is diverse, such as leaf, fruit, flower, and habitat, not just leaf as in Plant Village and tomato dataset. Figure <ref type="figure" coords="3,383.43,592.72,5.02,10.91" target="#fig_1">2</ref> and Figure <ref type="figure" coords="3,440.94,592.72,5.02,10.91" target="#fig_2">3</ref> display some images of two classes from PlantCLEF2022 training dataset and we can see that the backgrounds, viewpoints, illuminations, sizes, and colors are different. Therefore, image variations <ref type="bibr" coords="3,471.30,619.81,17.99,10.91" target="#b17">[18]</ref> are essentially big, which requires classification model to learn robust yet heterogeneous features for each class. Besides, the images in PlantCLEF2022 are collected from real field with multiple resolutions, instead of lab as Plant Village with the same resolution.  <ref type="table" coords="4,388.04,570.77,3.74,10.91" target="#tab_0">1</ref>.</p><p>Evaluation metric. Macro averaged mean reciprocal rank (MA-MRR) is utilized to evaluated to different submissions for PlantCLEF2022 challenge. The challenge requests a submission with a rank based on score with a given length for each testing observation, and the rank is thirty for the challenge. Assume there are ğ‘ classes in the testing dataset and class ğ‘› has ğ‘‚ ğ‘› observations. Mathematically, MA-MRR can be formalized as<ref type="foot" coords="4,360.97,636.76,3.39,7.97" target="#foot_0">1</ref>,<ref type="foot" coords="4,367.74,636.76,3.39,7.97" target="#foot_1">2</ref> : </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ğ‘€ ğ´</head><formula xml:id="formula_0" coords="5,231.99,516.51,273.99,33.74">-ğ‘€ ğ‘…ğ‘… = 1 ğ‘ ğ‘ âˆ‘ï¸ ğ‘›=1 1 ğ‘‚ ğ‘› ğ‘‚ğ‘› âˆ‘ï¸ ğ‘–=1 1 ğ‘Ÿğ‘ğ‘›ğ‘˜ ğ‘– ,<label>(1)</label></formula><p>where ğ‘Ÿğ‘ğ‘›ğ‘˜ ğ‘– refers to the rank position of the first relevant ground-truth label for the ğ‘–-th observation from one class. Conceptually, if all of observation are classified correctly in the first, then MA-MRR is one where the accuracy is hundred percent. In contrast, if MA-MRR is smaller then the model is worse. Intuitively, the observation-level enable us to observe different parts of plants, while MA-MRR allows more than one chances to recognize plant species for a specific image, 30 chances in the challenge. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model and finetuning details</head><p>Masked autoencoder (MAE) <ref type="bibr" coords="6,225.85,389.16,17.76,10.91" target="#b21">[22]</ref> is chose to achieve the challenge as a self-supervised vision transformer because of its high performance and stable training process. Figure <ref type="figure" coords="6,441.78,402.71,5.00,10.91">5</ref> illustrates its high-level architecture. In MAE, an image is firstly split into patches that are then randomly blocked. To save computation, only the unblocked patches are fed to an encoder to extract features, followed by a decoder to reconstruct the whole image. To pretrain the model, MAE employs a reconstruction loss and the original input image as the ground truth. After pretraining, the decoder is discarded and only the encoder is utilized taking unblocked images with a auxliary classifier head.</p><p>We borrow the ViT-large MAE model pretrained in ImageNet1k dataset. Random cropping and random horizontal flipping are leveraged as data augmentation strategy and the random masking, 75% as ratio, is another type of data augmentation. The model is trained with bacth size 4,096 and 800 epochs, 40 epochs as warming up. Besides, the learning rate is 0.00015 with 0.05 weight decay. The normalization is performed in block-level, instead of image-level as usual. Although only ViT-large model is leveraged for PlantCLEF2022 challenge, ViT-huge model is also encouraged but taken time and devices into consideration, we did not do experiments with this model.</p><p>Finetuning process. As finetuning the ViT-large MAE model with only four RTX 3090 GPUs, we set the actual batch size 512 and train the model 100 epochs. AdamW is employed as optimizer with learning rate 0.0005, 0.65 layer decay, and 0.05 weight decay. MixUp <ref type="bibr" coords="6,468.53,633.04,18.04,10.91" target="#b22">[23]</ref> and CutMix <ref type="bibr" coords="6,125.42,646.59,17.84,10.91" target="#b23">[24]</ref> are utilized as data augmentation. Besides, the added classifier is a linear function with 80,000 as the number of output, the number of class in PlantCLEF2022 trusted training  dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Integration towards observation</head><p>As discussed in the second section, the PlantCLEF2022 challenge is a classification in observationlevel, instead of image-level, and each observation is asked to be with a class-score rank. Therefore, we should integrate the decisions when one observation has multiple images as our model obtains individual decision for each image. In this subsection, we analyze several possible strategies to get the final decision for each observation. Let ğ‘Ÿ ğ‘— ğ‘  denote the testing probability score rank of the ğ‘—-th image of one observation. Accordingly, ğ‘Ÿ ğ‘— ğ‘ is the testing class rank with the same length. Besides, ğ‘Ÿ ğ‘ ğ‘– and ğ‘Ÿ ğ‘  ğ‘– are the ğ‘–-th class-score pair and means the class and the corresponding probability score, respectively. The rank requires ğ‘Ÿ ğ‘ ğ‘ &gt; ğ‘Ÿ ğ‘ ğ‘ if ğ‘ &lt; ğ‘. Formally, the final desired output {ğ‘Ÿ ğ‘  , ğ‘Ÿ ğ‘ }, pair of class and corresponding score, is formulated as</p><formula xml:id="formula_1" coords="7,231.18,572.85,274.81,14.19">{ğ‘Ÿ ğ‘ , ğ‘Ÿ ğ‘  } = â„(âˆª ğ‘› ğ‘—=1 ({ğ‘Ÿ ğ‘— ğ‘ , ğ‘Ÿ ğ‘— ğ‘  })),<label>(2)</label></formula><p>where â„ means an integration operation and the observation has ğ‘› individual images. There are three possible integration operations:</p><p>â€¢ Single-random: {ğ‘Ÿ ğ‘ , ğ‘Ÿ ğ‘  } = {ğ‘Ÿ ğ‘— ğ‘ , ğ‘Ÿ ğ‘— ğ‘  } where ğ‘— = ğ‘Ÿğ‘ğ‘›ğ‘‘(ğ‘›). Randomly sample an image from one observation and use the class-score rank pair of the random sample as the counterpart of the observation. In this way, observation-level classification deteriorates Figure <ref type="figure" coords="8,122.28,255.44,3.90,8.93">5</ref>: The high-level architecture of MAE <ref type="bibr" coords="8,286.28,255.49,15.07,8.87" target="#b21">[22]</ref>. With MAE, an image is split into patches that are then randomly blocked. The unblocked patches are fed to an encoder, followed by a decoder to reconstruct the whole input image. After the unsupervised pratraining, the decoder is discarded and only the encoder are utilized in down-stream task. The input is not blocked and a specific classifier is added after the encoder when fine-tuning the model in a target task. As the model is based on ViT and pretrained in an unsupervised manner, we termed our strategy ViT-based unsupervised transfer learning.</p><p>into image-level and hence this performance can be regarded as a baseline to see the impact of observation.</p><p>â€¢ Single-highest:</p><formula xml:id="formula_2" coords="8,188.73,390.66,251.60,15.71">{ğ‘Ÿ ğ‘ , ğ‘Ÿ ğ‘  } = {ğ‘Ÿ ğ‘— ğ‘ , ğ‘Ÿ ğ‘— ğ‘  } where ğ‘Ÿ ğ‘— ğ‘  1 = ğ‘šğ‘ğ‘¥{ğ‘Ÿ 1 ğ‘  1 , ğ‘Ÿ 2 ğ‘  1 , ..., ğ‘Ÿ ğ‘› ğ‘  1 }.</formula><p>Single means the final rank pair is from only one image and highest denotes the highest top-1 score. Therefore, the rank pair is taken as the final prediction for the observation if the image has the highest top-1 score.</p><p>â€¢ Multi-sorted. Multi means that the final rank pair is from multiple images, instead of a single image. In this way, the scores of all images from an same observation are sorted: ğ‘ ğ‘œğ‘Ÿğ‘¡(âˆª ğ‘› ğ‘—=1 âˆª 30 ğ‘–=1 ğ‘Ÿ ğ‘— ğ‘  ğ‘– ); and then, after removing duplicates of same classes, the first required length <ref type="bibr" coords="8,147.73,489.43,16.65,10.91" target="#b29">(30)</ref> of class-score pair are taken as the final ranking pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Submissions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Official submissions</head><p>We report seven official submissions by fine-tuning the MAE model with different epochs. Because the first run was utilized to check the submission format, we do not report it. As the training dataset is huge, we did not finish the training process before the deadline of the challenge. With the setting as given in implementation detail subsection, fine-tuning costs about 5 hours for every epoch. We just finished 80 epochs before the deadline. Besides, we just applied the single-highest strategy towards observation because of limited time. Our results of official submissions are shown in Table <ref type="table" coords="8,265.60,650.32,5.05,10.91" target="#tab_3">2</ref> and Figure <ref type="figure" coords="8,323.43,650.32,3.72,10.91">6</ref>. We find that the performance becomes better when training longer and it seems that the performance can be improved further. The performance of our official submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 6:</head><p>The tendency of our official submissions. It is seems that our method can be improved further via fine-tuning longer.</p><p>Team MA-MRR Ours 0.62692 Second place 0.60781 Third place 0.51043 Fourth place 0.46010</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>The comparison of the official final performances for the PlantCLEF2022 challenge. Our method outperforms others by a clear margin.</p><p>3 shows several performances from different teams. From the table, our method outperforms other rivals by a clear margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Late submissions</head><p>After official submissions, we fine-tuned our model longer until 100 epochs. Figure <ref type="figure" coords="9,461.89,570.09,5.09,10.91" target="#fig_4">7</ref> displays the complete performance of our submissions via fine-tuning a MAE model. We can see that training longer contributes more. Further, it seems that the performance can be improved more by extending the training process, which is leaved to our future work. Besides, we validate the performance of the single-highest and multi-sorted and the performances are displayes in Table <ref type="table" coords="9,89.04,637.84,3.81,10.91" target="#tab_4">4</ref>. The multi-sorted one slightly surpasses the single-highest one, which demonstrates that observation-level identification has potential than image-level, as the single-highest can be cast as to find the optimal way to identify the species of plant. Because of time limitation, we did Performances of different integration strategies, single-highest and multi-sorted. not perform the single-random strategy but we guess that it was inferior to the single-highest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussions</head><p>Future work. Although we secured the first place in the PlantCLEF2022 challenge, there are several points left. First of all, to achieve better performance, training longer is one of possible way. Besides, using more datasets to pretrain a model in a self-supervised manner, including PlantCLEF2022, is also encouraged. Secondly, the reasons why our strategy is effective and efficient for the challenge are not clear in current experiments. Towards this issue, ablation studies are desired, such replacing CNN with ViT and replacing supervised with self-supervised. Third, we only leveraged the large model of MAE, other types of model with different training strategies and loss function are also possible, such as contrast learning-based <ref type="bibr" coords="10,446.58,541.65,18.07,10.91" target="#b24">[25]</ref> [26] and text-image pair-based <ref type="bibr" coords="10,186.46,555.20,16.09,10.91" target="#b26">[27]</ref>. Fourth, we only used images and their annotations for the challenge and the meta-data introduced in material section were not utilized. Therefore, we emphasize that our work is just a start point about the challenge and more understandings can be made in the future. Observations. Our work validates the effectiveness of observation-level classification. We argue that this idea can be extended to other related task where taking multiple images to improve the performance of a task. For example, most of plant diseases are recognized based on front leaf <ref type="bibr" coords="10,147.91,650.05,18.02,10.91" target="#b18">[19]</ref> [20] [28] <ref type="bibr" coords="10,210.14,650.05,16.37,10.91" target="#b28">[29]</ref>. On the other hand, some patterns in the back side of leaf are useful to recognize the type of disease <ref type="bibr" coords="10,263.29,663.60,16.33,10.91" target="#b29">[30]</ref>. Thus, observation-level classification would ease Dataset Details PlantVillage Includes leaves of 16 plants with 38 classes and 54,305 images. Images are taken in lab with similar illuminations and simple background. Some diseases are split into two parts according to their severities. Apple2020</p><p>Includes only apple leaves with 3,642 images and 4 classes. Images are taken in the real field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apple2021</head><p>This dataset is an extension of Apple2020 but with 18,632 images and 6 classes. TaiwanTomato Owns 622 images of tomato leaves in 5 classes. Images are taken in the real field with more complex images. As having less images, it can be taken as a harder dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5</head><p>Details of four plant disease datasets. some applications and is encouraged.</p><p>Dataset. As training a model in a similar source domain with a big dataset is beneficial to down-stream task with a little dataset <ref type="bibr" coords="11,275.87,288.09,16.56,10.91" target="#b26">[27,</ref><ref type="bibr" coords="11,295.27,288.09,12.42,10.91" target="#b10">11]</ref>, PlantCLEF2022 dataset can be regarded as one of source dataset for plant-related down-stream tasks. For example, recognizing plant disease, classifying weed from crops, and detecting fruits. We perform some preliminary experiments on plant disease recognition on four public datasets, PlantVillage <ref type="bibr" coords="11,436.40,328.73,16.19,10.91" target="#b18">[19]</ref>, Apple2020 <ref type="bibr" coords="11,89.29,342.28,16.23,10.91" target="#b20">[21]</ref>, Apple2021 <ref type="bibr" coords="11,161.92,342.28,16.23,10.91" target="#b30">[31]</ref>, and TaiwanTomato <ref type="bibr" coords="11,273.51,342.28,16.23,10.91" target="#b31">[32]</ref>. Details of the plant disease dataset are given in Table <ref type="table" coords="11,115.27,355.83,3.66,10.91">5</ref>. The annotated images of the four datasets are split into three parts, training, validation and testing. The validation and testing dataset own 20% of the total annotated dataset while the training dataset owns 20%, 40%, and 60%. We compare two pretrained models to verify the PlantCLEF2022 dataset in plant disease recognition. One of the pretrained model is the model from MAE <ref type="bibr" coords="11,137.62,410.03,16.09,10.91" target="#b21">[22]</ref>, self-supervised trained in ImageNet1k dataset. Another one is our model in this paper, finetuning the MAE model in PlantCLEF2022 dataset. The accuracy of the experiments is shown in Table <ref type="table" coords="11,171.07,437.13,5.15,10.91">6</ref> and two validation accuracy during the training process are displayed in Figure <ref type="figure" coords="11,121.53,450.68,5.17,10.91">8</ref> and Figure <ref type="figure" coords="11,182.35,450.68,3.81,10.91">9</ref>. From the table and figures, we can see that the pretrained model in PlantCLEF2022 can improve the performance and speed the convergence, even with few labeled images. Therefore, we public not only our codes but also the fine-tuned models to encourage related applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we leveraged a very simple transfer learning strategy to achieve the PlantCLEF2022 challenge and secured the first place with a clear superiority to other rivals. Our strategy differs from current popular strategy in two ways, pretrained in a self-supervised way instead of supervised, based on vision transformer instead of convolution neural network, which gives a better feature space and a more powerful model, respectively. Simultaneously, we analyze the PlantCLEF2022 training and testing datasets that include millions of images with plenty of meta-data. Through our analysis, we believe that the PlantCLEF2022 dataset are going to contribute plant related tasks, such as plant disease recognition as validated in our preliminary experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,317.61,416.87,8.93;2,89.29,329.61,416.70,8.87;2,89.29,341.57,412.83,8.87"><head>FinetuneFigure 1 :</head><label>1</label><figDesc>Figure 1: Comparison between the popular strategy of transfer learning, supervised CNN, and our strategy, self-supervised ViT. Our method differs from the popular one in two cases, ViT as feature extractor instead of CNN, and self-supervised pretraining in the source domain without annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,399.34,416.69,8.93;4,89.29,411.34,315.12,8.87;4,397.18,319.06,54.97,73.30"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Images of Cycas armstrongii Miq species from PlantCLEF2022 training dataset. The images from the same species are heterogeneous in background, viewpoint, and size.</figDesc><graphic coords="4,397.18,319.06,54.97,73.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,89.29,456.68,416.69,8.93;5,89.29,468.69,331.03,8.87;5,372.80,361.56,132.71,88.47"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Images of Aralia nudicaulis L. species from PlantCLEF2022 training dataset. The images from the same plant species are heterogeneous in background, illumination, and color.</figDesc><graphic coords="5,372.80,361.56,132.71,88.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,89.29,327.88,416.69,8.93;7,89.29,339.89,416.69,8.87;7,89.29,351.84,349.44,8.87"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Six observations of testing dataset in PlantCLEF2022. One observation refers to an actual plant and we can take multiple images for single observation. The PlantCLEF2022 challenge requires classification in observation level, instead of image level as usual image classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,89.29,338.63,416.69,8.93;10,89.29,350.63,366.13,8.87;10,172.63,165.36,250.02,166.68"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The complete performance of our submissions via fine-tuning a MAE model 100 epochs with single-highest integration strategy. The official submissions are highlighted by red colors.</figDesc><graphic coords="10,172.63,165.36,250.02,166.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,88.99,86.42,412.15,230.76"><head>Table 1</head><label>1</label><figDesc>Dataset DetailsTraining Has two sub-datasets, web and trusted. The web one has 1.1 million images and 57,000 classes of plant where the annotation is searched from internet and thus may be with noises. The trusted one, annotated by human experts, includes 2,885,052 images and covers 80,000 classes. The Trusted one gives meta-data about the organs or habitat of plant in each image. The images in the trusted training dataset embrace a huge variation, such as illuminations, background, colors. Figure 2 and Figure 3 display some images from two individual types of plant. Testing Includes 26,868 observations with 55,306 images. The classification should be done in observation-level, not image-level as usual image classification requests. One observation refers to one actual plant with several pictures taken from different viewpoints, which requires a classification model to combine the predictions of multiple images taken from a same observation. Figure 4 shows six observations with diverse organs or habitats. Besides, the testing dataset only share part of the plant identity in the trusted training dataset, instead of the same as usual image classification. Simultaneously, the images in the training and testing dataset are collected from real field and in a similar resolution, about 450 Ã— 600. Details of the training and testing dataset in the PlantCLEF2022 challenge.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,481.98,663.87,24.00,10.91"><head>Table 3</head><label>3</label><figDesc></figDesc><table coords="9,130.03,86.42,348.83,33.57"><row><cell></cell><cell></cell><cell></cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell>Epoch</cell><cell>12</cell><cell>15</cell><cell>24</cell><cell>49</cell><cell>67</cell><cell>77</cell><cell>80</cell></row><row><cell>MA-MRR</cell><cell cols="7">0.55865 0.56772 0.58110 0.60219 0.61632 0.62497 0.62692</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,88.99,128.94,32.19,8.93"><head>Table 2</head><label>2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,88.99,86.42,288.89,51.05"><head>Table 4</head><label>4</label><figDesc></figDesc><table coords="10,217.39,86.42,160.50,33.18"><row><cell cols="3">Epoch Single-highest Multi-sorted</cell></row><row><cell>80</cell><cell>0.62692</cell><cell>No</cell></row><row><cell>100</cell><cell>0.63668</cell><cell>0.64079</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,108.93,660.08,192.13,8.97"><p>https://en.wikipedia.org/wiki/Mean_reciprocal_rank</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,108.93,671.04,311.39,8.97"><p>https://androidkt.com/micro-macro-averages-for-imbalance-multiclass-classification/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported by <rs type="programName">Basic Science Research Program</rs> through the <rs type="funder">National Research Foundation of Korea (NRF)</rs> funded by the <rs type="funder">Ministry of Education</rs> (No. <rs type="grantNumber">2019R1A6A1A09031717</rs>). This work was supported by <rs type="funder">Korea Institute of Planning and Evaluation for Technology in Food, Agriculture, and Forestry (IPET)</rs> and <rs type="funder">Korea Smart Farm R&amp;D Foundation (KosFarm)</rs> through <rs type="programName">Smart Farm Innovation Technology Development Program</rs>, funded by <rs type="funder">Ministry of Agriculture, Food and Rural Affairs (MAFRA)</rs> and <rs type="funder">Ministry of Science and ICT (MSIT)</rs>, <rs type="funder">Rural Development Administration (RDA)</rs> (<rs type="grantNumber">421027-04</rs>). This work was supported by the <rs type="funder">National Research Foundation of Korea (NRF)</rs> grant funded by the <rs type="funder">Korea government (MSIT)</rs> (<rs type="grantNumber">2020R1A2C2013060</rs>).</p><p>Figure 9: The validation accuracy curve of Apple2020 dataset. Pretrained with PlantCLEF2022 obtains better performance.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_faanzVq">
					<orgName type="program" subtype="full">Basic Science Research Program</orgName>
				</org>
				<org type="funding" xml:id="_Nq2Uw5v">
					<idno type="grant-number">2019R1A6A1A09031717</idno>
				</org>
				<org type="funding" xml:id="_n3bvAah">
					<orgName type="program" subtype="full">Smart Farm Innovation Technology Development Program</orgName>
				</org>
				<org type="funding" xml:id="_gg4GwCE">
					<idno type="grant-number">421027-04</idno>
				</org>
				<org type="funding" xml:id="_rDqTBju">
					<idno type="grant-number">2020R1A2C2013060</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 6</head><p>The accuracy of plant recognition on multiple public datasets with different pretrained model. 20%, 40%, and 60% are the ratio of training dataset from the original target dataset with annotation. Validation and testing dataset are awalys in 20%. The validation is utilized to choose the best training model that is further tested in the testing dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Online Resources</head><p>The codes and pretrained model are available via â€¢ GitHub, â€¢ Pretrained model in Google Drive.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="13,112.66,352.16,393.33,10.91;13,112.66,365.71,393.32,10.91;13,112.66,379.26,57.08,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,237.09,352.16,268.90,10.91;13,112.66,365.71,60.82,10.91">Overview of PlantCLEF 2022: Image-based plant identification at global scale</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,195.99,365.71,309.99,10.91;13,112.66,379.26,26.38,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,392.81,394.53,10.91;13,112.66,406.36,394.53,10.91;13,112.66,419.91,393.33,10.91;13,112.66,433.46,393.33,10.91;13,112.66,447.01,353.54,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,198.52,419.91,307.47,10.91;13,112.66,433.46,247.50,10.91">Overview of lifeclef 2022: an evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>PlanquÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Å ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,383.00,433.46,122.99,10.91;13,112.66,447.01,280.38,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,460.56,393.33,10.91;13,112.66,474.11,292.68,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,287.49,460.56,218.50,10.91;13,112.66,474.11,76.87,10.91">Generalizing from a few examples: A survey on few-shot learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,197.84,474.11,138.79,10.91">ACM computing surveys (csur)</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,487.66,393.33,10.91;13,112.66,501.21,169.07,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,195.85,487.66,132.37,10.91">A survey on transfer learning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,336.65,487.66,169.33,10.91;13,112.66,501.21,74.99,10.91">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,514.76,393.33,10.91;13,112.66,528.30,394.53,10.91;13,112.66,541.85,103.61,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,346.64,514.76,159.35,10.91;13,112.66,528.30,67.28,10.91">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,228.08,528.30,274.55,10.91">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,555.40,395.17,10.91;13,112.66,568.95,213.69,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,273.56,555.40,234.27,10.91;13,112.66,568.95,38.69,10.91">Plant species identification using transfer learningplantclef</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">H</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rakesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,197.73,568.95,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,582.50,395.01,10.91;13,112.66,596.05,325.58,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,207.79,582.50,295.85,10.91">Herbarium-field triplet network for cross-domain plant identification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chulif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,309.62,596.05,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>neuon submission to lifeclef 2020 plant</note>
</biblStruct>

<biblStruct coords="13,112.66,609.60,394.53,10.91;13,112.14,623.15,137.82,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,228.51,609.60,274.49,10.91">Weighted pseudo labeling refinement for plant identification</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">D</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,112.14,623.15,105.91,10.91">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,636.70,393.32,10.91;13,112.66,650.25,384.48,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,214.34,636.70,291.64,10.91;13,112.66,650.25,238.51,10.91">Improved herbarium-field triplet network for cross-domain plant identification: Neuon submission to lifeclef 2021 plant</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chulif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,359.31,650.25,105.91,10.91">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,663.80,393.33,10.91;14,112.66,86.97,393.32,10.91;14,112.66,100.52,182.19,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,275.12,663.80,230.87,10.91;14,112.66,86.97,102.76,10.91">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,238.50,86.97,267.49,10.91;14,112.66,100.52,84.28,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,114.06,393.33,10.91;14,112.66,127.61,394.52,10.91;14,112.66,141.16,22.69,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="14,248.22,114.06,183.47,10.91">Do better imagenet models transfer better?</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,452.94,114.06,53.05,10.91;14,112.66,127.61,319.37,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2661" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,154.71,395.16,10.91;14,112.66,168.26,395.17,10.91;14,112.66,181.81,395.17,10.91;14,112.66,195.36,73.36,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,330.08,168.26,177.76,10.91;14,112.66,181.81,169.53,10.91">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,304.90,181.81,202.94,10.91;14,112.66,195.36,43.59,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,208.91,393.71,10.91;14,112.66,222.46,395.01,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="14,189.09,208.91,317.27,10.91;14,112.66,222.46,26.68,10.91">Self-supervised visual feature learning with deep neural networks: A survey</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,146.62,222.46,270.68,10.91">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="4037" to="4058" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,236.01,393.33,10.91;14,112.66,249.56,169.24,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="14,279.59,236.01,226.40,10.91;14,112.66,249.56,39.20,10.91">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,160.68,249.56,32.21,10.91">science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,263.11,393.33,10.91;14,112.66,276.66,394.53,10.91;14,112.66,290.20,65.30,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,264.03,263.11,241.96,10.91;14,112.66,276.66,42.72,10.91">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,177.90,276.66,300.23,10.91">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,303.75,393.33,10.91;14,112.66,317.30,363.13,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="14,237.15,303.75,268.83,10.91;14,112.66,317.30,38.13,10.91">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,173.59,317.30,272.31,10.91">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,330.85,395.17,10.91;14,112.66,344.40,189.99,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="14,250.10,330.85,257.73,10.91;14,112.66,344.40,16.17,10.91">Unsupervised transfer learning for plant anomaly recognition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,136.80,344.40,92.06,10.91">Smart Media Journal</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,357.95,393.32,10.91;14,112.66,371.50,307.51,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01491</idno>
		<title level="m" coord="14,287.06,357.95,218.92,10.91;14,112.66,371.50,125.40,10.91">A comprehensive survey of image augmentation techniques for deep learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,385.05,393.33,10.91;14,112.66,398.60,393.33,10.91;14,112.33,412.15,29.19,10.91" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>SalathÃ©</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08060</idno>
		<title level="m" coord="14,251.44,385.05,254.55,10.91;14,112.66,398.60,240.20,10.91">An open access repository of images on plant health to enable the development of mobile disease diagnostics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,425.70,393.33,10.91;14,112.66,439.25,393.33,10.91;14,112.66,452.79,147.62,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="14,317.37,425.70,188.62,10.91;14,112.66,439.25,298.60,10.91">Style-consistent image translation: A novel data augmentation paradigm to improve plant disease recognition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,425.77,439.25,80.22,10.91;14,112.66,452.79,33.25,10.91">Frontiers in Plant Science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="773142" to="773142" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,466.34,393.33,10.91;14,112.66,479.89,394.93,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="14,349.50,466.34,156.49,10.91;14,112.66,479.89,181.61,10.91">The plant pathology challenge 2020 data set to classify foliar disease of apples</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Thapa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,302.53,479.89,133.02,10.91">Applications in Plant Sciences</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11390</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,493.44,393.33,10.91;14,112.66,506.99,216.02,10.91" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<title level="m" coord="14,330.08,493.44,175.91,10.91;14,112.66,506.99,34.05,10.91">Masked autoencoders are scalable vision learners</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,520.54,395.17,10.91;14,112.66,534.09,309.29,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="14,331.02,520.54,176.81,10.91;14,112.66,534.09,16.17,10.91">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,151.43,534.09,240.50,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,547.64,393.32,10.91;14,112.66,561.19,393.32,10.91;14,112.66,574.74,233.71,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="14,329.08,547.64,176.90,10.91;14,112.66,561.19,182.01,10.91">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,317.28,561.19,188.70,10.91;14,112.66,574.74,136.06,10.91">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,588.29,394.53,10.91;14,112.66,601.84,395.00,10.91;14,112.66,615.39,48.96,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="14,214.10,588.29,288.56,10.91">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,127.24,601.84,334.38,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9640" to="9649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,628.93,395.17,10.91;14,112.66,642.48,393.58,10.91;14,112.66,656.03,300.99,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="14,474.41,628.93,33.43,10.91;14,112.66,642.48,236.54,10.91">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>JÃ©gou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,373.55,642.48,132.69,10.91;14,112.66,656.03,203.10,10.91">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9650" to="9660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,669.58,394.53,10.91;15,112.66,86.97,393.33,10.91;15,112.66,100.52,394.52,10.91;15,112.66,114.06,22.69,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="15,237.25,86.97,268.73,10.91;15,112.66,100.52,50.66,10.91">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,186.65,100.52,211.00,10.91">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,127.61,393.33,10.91;15,112.66,141.16,393.33,10.91;15,112.33,154.71,62.36,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="15,275.71,127.61,230.27,10.91;15,112.66,141.16,184.11,10.91">Identification method of vegetable diseases based on transfer learning and attention mechanism</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,305.88,141.16,182.49,10.91">Computers and Electronics in Agriculture</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page">106703</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,168.26,394.53,10.91;15,112.66,181.81,133.40,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="15,178.22,168.26,324.59,10.91">Semi-supervised few-shot learning approach for plant diseases recognition</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,112.66,181.81,64.68,10.91">Plant Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,195.36,393.33,10.91;15,112.66,208.91,393.33,10.91;15,112.66,222.46,120.57,10.91" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="15,295.44,195.36,210.55,10.91;15,112.66,208.91,333.66,10.91">High-performance deep neural network-based tomato plant diseases and pests diagnosis system with refinement filter bank</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,455.01,208.91,50.98,10.91;15,112.66,222.46,57.83,10.91">Frontiers in plant science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">1162</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,236.01,393.33,10.91;15,112.66,249.56,214.43,10.91" xml:id="b30">
	<monogr>
		<title level="m" type="main" coord="15,348.51,236.01,157.48,10.91;15,112.66,249.56,182.51,10.91">The plant pathology 2021 challenge dataset to classify foliar disease of apples</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Thapa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,263.11,344.81,10.91" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="15,234.59,263.11,107.58,10.91">Dataset of tomato leaves</title>
		<author>
			<persName coords=""><forename type="first">M.-L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-H</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,350.32,263.11,67.43,10.91">Mendeley Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
