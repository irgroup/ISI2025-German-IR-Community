<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,360.54,15.42;1,89.29,106.66,179.61,15.42">Extreme Automatic Plant Identification Under Constrained Resources</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.10,134.97,97.89,11.96"><forename type="first">Jose</forename><surname>Carranza-Rojas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Costa Rica Institute of Technology</orgName>
								<address>
									<settlement>Cartago</settlement>
									<country key="CR">Costa Rica</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
								<address>
									<settlement>San Jose</settlement>
									<country key="CR">Costa Rica</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,204.92,134.97,134.75,11.96"><forename type="first">Ruben</forename><surname>Gonzalez-Villanueva</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Costa Rica Institute of Technology</orgName>
								<address>
									<settlement>Cartago</settlement>
									<country key="CR">Costa Rica</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,352.31,134.97,116.47,11.96"><forename type="first">Kelvin</forename><surname>Jimenez-Morales</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Costa Rica Institute of Technology</orgName>
								<address>
									<settlement>Cartago</settlement>
									<country key="CR">Costa Rica</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,148.92,116.61,11.96"><forename type="first">Kevin</forename><surname>Quesada-Montero</surname></persName>
							<email>kevin707qm@estudiantec.cr</email>
							<affiliation key="aff0">
								<orgName type="institution">Costa Rica Institute of Technology</orgName>
								<address>
									<settlement>Cartago</settlement>
									<country key="CR">Costa Rica</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,218.27,148.92,137.05,11.96"><forename type="first">Esteban</forename><forename type="middle">A</forename><surname>Esquivel-Barboza</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Costa Rica Institute of Technology</orgName>
								<address>
									<settlement>Cartago</settlement>
									<country key="CR">Costa Rica</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,385.53,148.92,116.41,11.96"><forename type="first">Nicole</forename><surname>Carvajal-Barboza</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Costa Rica Institute of Technology</orgName>
								<address>
									<settlement>Cartago</settlement>
									<country key="CR">Costa Rica</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,181.05,627.14,38.53,8.97"><forename type="first">J</forename><surname>Carranza</surname></persName>
							<email>jcarranza@itcr.ac.cr</email>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,360.54,15.42;1,89.29,106.66,179.61,15.42">Extreme Automatic Plant Identification Under Constrained Resources</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">244BBAED21FD2EC6ED4D66E8F1146DD9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Extreme Classification</term>
					<term>Hierarchical Softmax</term>
					<term>Automatic Plant Identification</term>
					<term>Taxonomy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The estimated amount of plant species in our planet Earth is calculated to be around 400, 000. In order to create an automatic plant identification system that aims to identify any plant species on Earth, machine learning techniques must scale to a high volume of images and species. This leads to Extreme Classification, an area of machine learning that aims to develop models that can classify among hundreds of thousands, or even millions of classes. This work depicts BioMachina's team participation in the PlantCLEF 2022 challenge. Our approach was based on deep learning techniques for constrained environments, where resources are scarce for the creation of large models to deal with the considerable amount of species of the challenge. Additionally to using several training techniques to alleviate resource consumption, we developed a 2-level hierarchical softmax. By simulating a small and inferred plant taxonomy, we allowed the model to learn a 2 level hierarchy of classes on its own, reducing model sizes significantly. Our implementation of hierarchical softmax resulted in position 4 of the overall PlantCLEF 2022 ranking, while keeping model sizes reasonably small and computationally efficient, with a 5.67x reduction of parameters compared to vanilla softmax.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Automatic plant identification based on images has evolved from a small amount of species <ref type="bibr" coords="1,89.29,474.78,11.58,10.91" target="#b0">[1]</ref>, to gradually thousands of species <ref type="bibr" coords="1,262.86,474.78,11.58,10.91" target="#b1">[2]</ref>. This year's PlantCLEF competition has increased the number of species to 80, 000 <ref type="bibr" coords="1,237.46,488.32,11.53,10.91" target="#b2">[3]</ref>, which is, to our knowledge, the biggest automatic plant identification dataset known to date. Such dataset enters the realm of Extreme Multi-label Classification (XMLC), that aims to classify instances among hundreds of thousands, to millions of categories <ref type="bibr" coords="1,149.06,528.97,11.49,10.91" target="#b3">[4]</ref>. As we approach the total number of plant species on Earth, calculated to be around 400, 000, the trend is to use bigger and more complex models to fit such large datasets <ref type="bibr" coords="1,89.29,556.07,11.28,10.91" target="#b4">[5]</ref>. Such progress is somehow similar to the Natural Language Processing (NLP) domain, where bigger and bigger transformer-based models have been the norm for a while now, such as BERT <ref type="bibr" coords="2,89.29,100.52,11.28,10.91" target="#b5">[6]</ref>. However, training such large models has environmental and financial costs associated with them <ref type="bibr" coords="2,115.05,114.06,11.50,10.91" target="#b6">[7]</ref>. In the plant identification domain, we believe there is a need to think beyond brute force and bigger models. There is also a need to decrease model sizes, complexity, and memory footprint, to save on computing and environmental costs. Additionally, making efficient models will have a positive impact on the deployment of automatic plant identification systems, such as Pl@ntNet <ref type="bibr" coords="2,149.56,168.26,11.58,10.91" target="#b7">[8]</ref>. Some techniques to reduce model sizes, such as knowledge distillation <ref type="bibr" coords="2,492.99,168.26,12.99,10.91" target="#b8">[9]</ref> and quantization <ref type="bibr" coords="2,165.89,181.81,16.10,10.91" target="#b9">[10]</ref>, have been explored in related machine learning domains. However, very little work has focused on reducing plant identification model sizes and memory footprints.</p><p>The plant domain offers a unique setting where a class hierarchy exists, known as the plant taxonomy. It has not been thoroughly studied yet in machine learning, although some work has been done in such domain <ref type="bibr" coords="2,225.09,236.01,16.34,10.91" target="#b10">[11,</ref><ref type="bibr" coords="2,244.16,236.01,12.26,10.91" target="#b11">12]</ref>. The intuition is that, guided by the taxonomy, one may be able to improve model performance, and maybe decrease model sizes. As an example, in the NLP domain, Language Model (LM) training exhibits similar problems, as the amount of human language tokens is large. Some work has been done towards training with reduced logits layers, by implementing a hierarchical softmax, reducing time complexity from linear to ğ’ª(ğ‘™ğ‘œğ‘”(ğ‘›)) for token prediction.</p><p>The present work depicts the participation of the BioMachina team in the PlantCLEF 2022 challenge. Our focus was to try to get the best possible results with limited resources, as we did not count on a large GPU cluster to deal with this year's amount of species. Therefore, our primary focus was on keeping models small to fit smaller computing resources. We applied a particular implementation of a 2-level hierarchical softmax, reducing by 5.67x the number of model parameters.</p><p>Our contributions are the following:</p><p>â€¢ The exploration of a different way to model the species probability distribution, such that fewer model parameters are needed, via a 2-level hierarchical softmax. â€¢ The experimental comparison of using normal softmax versus using hierarchical softmax, in an extreme plant classification challenge, in terms of Moving Average Mean Reciprocal Rank (MA-MRR), parameter size, and memory footprint. â€¢ Help democratize plant identification model development and raise awareness of the potential consequences of using brute force to deal with bigger plant datasets, and provide viable options to keep model sizes at bay under constrained computing environments.</p><p>The rest of the document is as follows: Section 2 contains the related work. Section 3 depicts the datasets, architectures used, and how we decreased models sizes. Section 4 contains a description of the experiments with their respective results, and discussion. Section 5 contains the conclusions we draw from our results, and lists potential future work in this line of research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>PlantCLEF 2013 included for the first time other organs beyond leaves, such as fruits and flowers <ref type="bibr" coords="2,89.29,659.98,11.58,10.91" target="#b0">[1]</ref>. By 2017, the PlantCLEF challenge's dataset had 10, 000 species, leading to this year's challenge with 80, 000 species <ref type="bibr" coords="3,226.97,86.97,11.44,10.91" target="#b2">[3]</ref>. Year by year, the challenge draws near to the total number of plants species on Earth.</p><p>Previous work on hierarchical loss functions has been studied in NLP, given the large number of tokens that particular human languages have <ref type="bibr" coords="3,309.62,127.61,16.41,10.91" target="#b12">[13]</ref>. In particular, hierarchical softmax has been widely used for training language models <ref type="bibr" coords="3,296.53,141.16,16.09,10.91" target="#b13">[14]</ref>. It is defined over a binary tree, where each node has only 2 children, and all language tokens are in the leaves of the tree. Clearly, this may not be well aligned with our taxonomy needs, as classes at a given level of the plant taxonomy, such as family or genera, may not have only 2 children. In order to approximate part of the plant taxonomy, a variation of the hierarchical softmax would be needed.</p><p>Several research has been done on how to use the plant taxonomy to inject additional knowledge into the learning process. In <ref type="bibr" coords="3,264.93,222.46,17.75,10.91" target="#b10">[11]</ref> the authors created 3 different architectures where a classifier is needed for family, genus and species. Each architecture connects the 3 classifiers in different fashions. Other work such as the Taxonomy Softmax <ref type="bibr" coords="3,360.95,249.56,17.76,10.91" target="#b14">[15]</ref> attempts to create a new loss function using a new type of softmax where probabilities are derived based on the taxonomy, however the results were not as good as expected. It also uses the taxonomy as ground truth, assuming the taxonomy is perfectly defined. In <ref type="bibr" coords="3,297.17,290.20,17.76,10.91" target="#b11">[12]</ref> the authors use 3 different models, each for family, genus and species, to improve the domain adaptation from herbarium to field images. They, however, did not aim to reduce model sizes, as they include 3 classifiers for each taxa, increasing the model size.</p><p>To our knowledge, no other work has been attempted to automatically learn a plant taxonomy to reduce model sizes. In this work, we borrow ideas from training big language models on NLP tasks, where hierarchical softmax is used to improve training times. We implement a 2-level hierarchical softmax, allowing the model to learn a 2-level taxonomy on its own, while keeping the number of parameters small compared to traditional softmax with cross entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Our methodology is based on the idea of keeping model sizes at bay, while trying to get the best results possible. The main technique used to reduce model size was a 2-level hierarchical softmax, but we also used Automatic Mixed Precision (AMP) to process bigger batches, as well as Gradient Accumulation. The following subsections depict the architectures, equipment, and software used, and this year's dataset, as well as the 2-level hierarchical softmax implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>PlantCLEF 2022 challenge provides a large dataset based on 80, 000 species. The trusted subset contains a total of 2.9 million images. Such trusted subset has been confirmed by human experts. A second subset is provided, called the web subset, which has 57, 314 species and 1, 065, 129 images. Such web images have not been fully confirmed by human experts <ref type="bibr" coords="3,420.65,601.80,11.32,10.91" target="#b2">[3]</ref>. This dataset, to our knowledge, is the biggest dataset for plant identification to date. Consequently, it converts this year's task in an extreme classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Architectures</head><p>Although Attention-based architectures, in particular, Visual Transformers <ref type="bibr" coords="4,437.62,107.54,18.07,10.91" target="#b15">[16]</ref> have been getting lots of attention, we used smaller known Convolutional Neural Network (CNN) architectures for our runs. This due to constrained resources. The models of choice were ResNet and EfficientNet. Both are relatively small compared to other models, and their performance is known to be good in generalistic challenges such as ImageNet.</p><p>ResNet Residual learning blocks were introduced by He et al. <ref type="bibr" coords="4,379.90,190.49,17.91,10.91" target="#b16">[17]</ref> in order to train deeper neural networks. The residual function tackles the degradation problems that arose with adding more layers. These type of networks learn residual functions that reference the layer's inputs as shortcuts, as shown in Figure <ref type="figure" coords="4,240.70,231.14,3.81,10.91">1</ref>. Formally, the building block is defined as shown in the Equation <ref type="formula" coords="4,132.01,244.69,3.72,10.91">1</ref>, where ğ‘¥ and ğ‘¦ are the input and output vectors of the layers, respectively, and the function â„± represents the residual mapping to be learned.</p><formula xml:id="formula_0" coords="4,251.17,285.34,254.82,11.36">ğ‘¦ = â„±(ğ‘¥, {ğ‘Š ğ‘– }) + ğ‘¥ (1)</formula><p>Figure <ref type="figure" coords="4,121.06,417.59,3.82,8.93">1</ref>: Residual learning building block <ref type="bibr" coords="4,265.74,417.64,14.92,8.87" target="#b16">[17]</ref>.</p><p>We used two variants of the Residual Networks, namely ResNet50 and ResNet101, referring to residual nets with a depth of 50 and 101 layers, respectively.</p><p>EfficientNet Scaling processes of a CNN are done to produce better performance, such as scaling by their depth (i.e. ResNet <ref type="bibr" coords="4,260.51,502.94,15.89,10.91" target="#b16">[17]</ref>). Tan and Le <ref type="bibr" coords="4,342.91,502.94,17.91,10.91" target="#b17">[18]</ref> proposed the compound scaling method, which scales up a CNN by using a constant ratio to scale each dimension of network width/depth/resolution.</p><p>A CNN can be defined as shown in the Equation <ref type="formula" coords="4,319.63,543.59,3.76,10.91" target="#formula_1">2</ref>, where â„± ğ¿ ğ‘– ğ‘– denotes layer ğ¹ ğ‘– is repeated ğ¿ ğ‘– times in stage ğ‘–, âŸ¨ğ» ğ‘– , ğ‘Š ğ‘– , ğ¶ ğ‘– âŸ© denotes the shape of input tensor ğ‘‹ of layer ğ‘– <ref type="bibr" coords="4,441.29,557.14,16.25,10.91" target="#b17">[18]</ref>.</p><formula xml:id="formula_1" coords="4,232.78,581.61,273.21,25.43">ğ’© = â¨€ï¸ ğ‘–=1...ğ‘  â„± ğ¿ ğ‘– ğ‘– (ğ‘‹ âŸ¨ğ» ğ‘– ,ğ‘Š ğ‘– ,ğ¶ ğ‘– âŸ© )<label>(2)</label></formula><p>Different from regular CNN designs, this scaling mechanism expands the network length (ğ¿ ğ‘– ), width (ğ¶ ğ‘– ), and resolution (ğ» ğ‘– , ğ‘Š ğ‘– ) without changing the â„± ğ‘– in the baseline. The approach can be formulated as an optimization problem, where the model accuracy is being maximized for any given resource constraints, which can be seen in the Equation <ref type="formula" coords="4,416.46,654.18,3.81,10.91">3</ref>, where the ğ‘¤, ğ‘‘, ğ‘Ÿ are coefficients for scaling the network width, depth and resolution and â„± ^ğ‘–, â„’ ^ğ‘–, â„‹ ^ğ‘–, ğ’² ^ğ‘–, ğ’ ^ğ‘– are predefined parameters. The result of this meta-learning optimization is the EfficienNets, which outperform most of the CNN models using a considerably less number of parameters <ref type="bibr" coords="5,470.30,100.52,16.25,10.91" target="#b17">[18]</ref>. (3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Hardware and Software</head><p>We worked in a fairly constrained environment proportionally to the size and complexity of the task. We used the cloud computing platform Vast.ai, with instances ranging from 1 to 2 NVIDIA RTX 3090 GPUs, with 16 to 32 GBs of RAM on average. This represents a good workstation, accessible to most people, but not a research cluster. Software-wise, we developed all models using Pytorch and Pytorch Lightning, and the code is publicly available here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Reducing output sizes</head><p>As the number of species for the task is 80, 000, normal softmax would require a logits output of the same size, leading to a linear increase of model parameters in the logits layers, regardless of the model used. Traditionally, image classifiers rely on cross-entropy loss for training, shown in Equation <ref type="formula" coords="5,145.65,368.02,3.81,10.91" target="#formula_3">5</ref>, where ğ‘¦ ğ‘ is the ground truth one-hot vector. Equation 4 depicts the softmax calculation, where ğ‘€ is the total of classes at hand. The input vector ğ‘§ represents the logits from a backbone model. Clearly, softmax has a linear time complexity ğ’ª(ğ‘€ ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ğœ(ğ‘§</head><formula xml:id="formula_2" coords="5,223.34,415.83,282.64,30.83">ğ‘– ) = ğ‘’ ğ‘§ ğ‘– âˆ‘ï¸€ ğ‘€ ğ‘—=1 ğ‘’ ğ‘§ ğ‘— ğ‘“ ğ‘œğ‘Ÿ ğ‘– = 1, 2, . . . , ğ‘€<label>(4)</label></formula><formula xml:id="formula_3" coords="5,253.23,459.50,252.75,11.36">â„’ = -ğ‘¦ ğ‘ log(ğœ(ğ‘§ ğ‘ ))<label>(5)</label></formula><p>The problem with extreme classification scenarios is that the value of ğ‘€ is large. In this case, ğ‘€ âˆˆ R 80,000 species, which causes the model to grow importantly, as the last layer of the neural network will be of size |ğ¿ (ğ‘›-1) | Ã— 80, 000, where |ğ¿ (ğ‘›-1) | is the size of the layer before logits. For perspective, EfficientNet B4 increases its size with 2, 048 Ã— 80, 000, summing up around 160 million parameters only for logits computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">Hierarchical Softmax</head><p>Aiming to reduce the amount of parameters related to the output of the model, we defined a 2-level hierarchical softmax. The implementation is in Pytorch and based on the implementation found here. Our 2-level hierarchical softmax differs to the original hierarchical softmax definition from <ref type="bibr" coords="5,112.79,623.70,17.76,10.91" target="#b12">[13]</ref> since it does not rely on a binary tree. The main reason is because the plant taxonomy cannot be implemented as a binary tree, as nodes may have more than 2 children. Instead, our implementation relies on 2 smaller layers, each one with its own softmax distribution, where the top layer leads to the bottom layer, and both probabilities are maximized. Each node of the top layer has more than 2 children, allowing to place sibling species under the same learned top layer node, effectively simulating a potential 2 layer taxonomy. The comparison of an EfficientNet B4 with and without 2-level hierarchical softmax is shown in Figure <ref type="figure" coords="6,445.33,114.06,3.68,10.91" target="#fig_1">2</ref>. Notice how the hierarchical softmax can be easily plugged into other models by reducing the increasingly bigger logits layer of the backbone. shows a traditional fine-tunning scenario, with a pre-logits layer's output of 2, 048 and a vanilla softmax of size ğ‘€ = 80, 000, forcing a same sized logits layer. To the right, a hierarchical softmax with logits ğ¿ (ğ‘›) layer is reduced to an arbitrary, smaller number than ğ‘€ . Then it connects to the 2 levels of the hierarchical softmax. The logits layer is reduced without a bottleneck effect.</p><p>The top layer ğœ (ğ‘¡) , depicted by Equation <ref type="formula" coords="6,273.96,517.60,3.66,10.91" target="#formula_4">6</ref>, simulates a higher taxonomic level of the taxonomy, although it does not have exactly the real amount of families or genera in the dataset. The input ğ‘§ is the actual output of the logits ğ¿ (ğ‘›) of a backbone model, which would normally be injected in a plain softmax. We reduce such logits size to ğ‘§ âˆˆ R 128 , which is controlled as a hyperparameter. Such size does not have a bottleneck effect since we distribute the ğ‘€ species through 2 levels. The weights ğ‘Š (ğ‘¡) âˆˆ R 128Ã—1,000 learn how to route ğ‘§ to the right node of the top layer, with ğ‘ (ğ‘¡) been its corresponding bias.</p><formula xml:id="formula_4" coords="6,246.59,624.23,259.39,12.68">ğœ (ğ‘¡) = ğœ(ğ‘§ğ‘Š (ğ‘¡) + ğ‘ (ğ‘¡) )<label>(6)</label></formula><p>The bottom layer ğœ (ğ‘) , shown in Equation <ref type="formula" coords="6,291.35,646.02,3.79,10.91" target="#formula_5">7</ref>, is another plain softmax of size 80. The input ğ‘§ is indexed by the biggest probability of ğœ (ğ‘¡) . The weights ğ‘Š (ğ‘) âˆˆ R 1,000Ã—128Ã—80 learn the mapping from one of the nodes in the top layer, to its children in the bottom layer. Again, the middle 128 size of the weights is another hyperparameter.</p><formula xml:id="formula_5" coords="7,226.95,125.85,279.04,14.57">ğœ (ğ‘) = ğœ(ğ‘§ ğ‘šğ‘ğ‘¥(ğœ (ğ‘¡) ) ğ‘Š (ğ‘) + ğ‘ (ğ‘) )<label>(7)</label></formula><p>We chose an arbitrary number of nodes for the top layer of ğœ (ğ‘¡) âˆˆ R 1,000 , where each top node is mapped to one of the ğœ (ğ‘) âˆˆ R 80 children in the bottom layer, accounting for all 80,000 species. Another potential distribution of nodes, for reference, would be 400 in the top layer, and 200 in the bottom layer.</p><p>The loss â„’ (â„) for the 2-level hierarchical softmax is shown in Equation <ref type="formula" coords="7,432.37,204.29,3.81,10.91" target="#formula_6">8</ref>. Both top and bottom layers route the right species ğ‘¦ ğ‘ by using as ground truth ğ‘¦ ğ‘¡ = ğ‘¦ ğ‘ /80 and ğ‘¦ ğ‘ = ğ‘¦ ğ‘ % 80 respectively. This effectively forces the hierarchical softmax to learn mappings of the right class index on both top and bottom layers. Additionally, log is used for numerical stability. Our 2-level hierarchical softmax reduces dramatically the number of parameters needed to predict the correct class, as each species does not have dedicated parameters, but they are shared between species.</p><formula xml:id="formula_6" coords="7,216.05,310.92,289.93,13.27">â„’ (â„) = -[ğ‘¦ ğ‘¡ log(ğœ (ğ‘¡) ) + ğ‘¦ ğ‘ log(ğœ (ğ‘) )]<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Additional techniques for constrained resources</head><p>Additional to the hierarchical softmax explained in Section 3.4.1, we also used several techniques to limit the amount of memory, time, and GPU usage needed to train such complex models, while helping with generalization for unseen samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automatic Mixed Precision (AMP)</head><p>AMP uses both single-and half-precision representations, instead of only single-precision format <ref type="bibr" coords="7,311.07,431.82,16.41,10.91" target="#b18">[19]</ref>. This strategy nearly halves memory requirements by accessing half the bytes, and speeds up math-intensive operations like linear and convolution layers. It enables training larger mini-batches while keeping the accuracy achieved with single precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Batch Accumulation</head><p>It is known that a small batch size usually results in performance degradation, especially in tasks where the number of classes is large <ref type="bibr" coords="7,404.42,514.77,16.41,10.91" target="#b19">[20]</ref>. An effective way to increase the batch size without compromising memory is the use of accumulated gradients. This method allows to run ğ¾ batches of size ğ‘ before doing the backward process, resulting in a large effective batch size of size ğ¾ğ‘¥ğ‘ . We used Pytorch Lightning <ref type="bibr" coords="7,401.90,555.42,18.07,10.91" target="#b20">[21]</ref> with a accumulate grad batches of 32.</p><p>Gradient Clipping Gradient clipping is used to solve the exploding gradients problem, by clipping the value of gradients to a certain threshold. We used Pytorch Lightning <ref type="bibr" coords="7,456.81,611.28,18.00,10.91" target="#b20">[21]</ref> with a gradient clip value of 9. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>Our experiments aimed to measure model performance under limited resources for a large number of species. We measured model and final layer parameter sizes, memory footprint, and MA-MRR using the official submission tool from the challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Efficacy of adding more layers</head><p>We experimented with different depths of ResNet to see how the model would scale up given the large amount of classes. In general, by just adding more layers from ResNet50 to ResNet101, the task at hand did not improve dramatically, as shown in Table <ref type="table" coords="8,385.99,428.38,3.81,10.91" target="#tab_0">1</ref>. This suggests that brute force may not be the best approach as the amount of species grows. We pre-trained both ResNet versions with the Web subset, improving the MA-MRR compared to runs without such pre-training, getting the best results we got for the challenge with 0.46 for ResNet50 and 0.45 for ResNet101. ResNet101, regardless of been a bigger model, did not provide better results. From this experiment, we also noticed the positive impact of using the Web subset for pre-training, with an increase in MA-MRR of 0.22 for ResNet50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Softmax versus hierarchical softmax</head><p>We compared the performance of the EfficientNet with and without hierarchical softmax. We used EfficientNet as it is smaller than ResNet, but also because it shows better results in the ImageNet competition proportionally to its size. We used the B4 version of EfficientNet, as it was reasonable for our computing resources. Table <ref type="table" coords="8,296.27,600.05,5.01,10.91" target="#tab_0">1</ref> shows the comparison of EfficientNet B4 with and without hierarchical softmax with respect to number of parameters, memory footprint and MA-MRR. Notice how the use of hierarchical softmax dramatically reduces the number of model parameters up to 5.67x in the case of EfficientNet B4, from 160M to 28.2M parameters, while not sacrificing MA-MRR. This suggests that hierarchical softmax could be used with different sized models to reduce the complexity of the last layers while keeping similar performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>The usage of techniques such as hierarchical softmax reduces dramatically the size of the final model in terms of parameters and memory footprint, with a 5.67x reduction in the case of the PlantCLEF 2022 challenge, without affecting performance. As the number of species in the challenge grows each year, similar techniques may be worth exploring to keep model sizes at bay. In consequence, training such smaller models requires less time and computation power, making training more accessible to researchers and engineers. It may also positively impact the deployment of more extensive plant identification systems, even aiming to place models on mobile devices while still identifying thousands of species. There are, however, additional lines of research worth exploring beyond this work. We developed a 2-level hierarchical softmax, but we would like to explore more depth in the hierarchy. We explored learning a small 2-level taxonomy, however it would be interesting to study how to leverage the existing plant taxonomy for additional supervision signal.</p><p>Additionally, applying other techniques such as knowledge distillation and quantization may further compress the memory footprint of such models. From the PlantCLEF challenge perspective, most likely transformers and other bigger backbone models may have provided better MA-MRR results. The techniques discussed here could also be applied to them, reducing their sizes while keeping high performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,274.18,136.24,20.30,9.57;5,274.62,145.60,19.18,6.99;5,305.39,136.24,101.55,9.57;5,188.33,164.94,80.44,9.57;5,275.90,161.59,16.48,6.98;5,271.80,180.04,24.68,6.99;5,298.30,162.18,21.39,12.33;5,314.69,157.76,7.66,6.99;5,307.22,169.65,2.88,6.99;5,323.35,164.94,13.28,9.57;5,336.63,164.94,70.31,12.79"><head></head><label></label><figDesc>max ğ‘‘,ğ‘¤,ğ‘Ÿ ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦(ğ’© (ğ‘‘, ğ‘¤, ğ‘Ÿ)) ğ‘ .ğ‘¡ ğ’© (ğ‘‘, ğ‘¤, ğ‘Ÿ) = â¨€ï¸ ğ‘–=1...ğ‘  â„± ^ğ‘‘â€¢ğ¿ ^ğ‘– ğ‘– (ğ‘‹ âŸ¨ğ‘Ÿâ€¢ğ» ^ğ‘–,ğ‘Ÿâ€¢ğ‘Š ^ğ‘–,ğ‘¤â€¢ğ¶ ^ğ‘–âŸ© )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,89.29,440.13,416.69,8.93;6,89.29,451.87,416.92,9.14;6,89.29,463.82,416.69,9.14;6,89.29,475.78,6.78,8.74;6,96.07,474.20,11.15,6.12;6,110.25,475.78,395.74,9.14;6,89.29,488.00,310.95,8.87;6,171.38,163.87,250.02,268.83"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architectural changes to add hierarchical softmax into EfficientNet B4. The left diagramshows a traditional fine-tunning scenario, with a pre-logits layer's output of 2, 048 and a vanilla softmax of size ğ‘€ = 80, 000, forcing a same sized logits layer. To the right, a hierarchical softmax with logits ğ¿(ğ‘›) layer is reduced to an arbitrary, smaller number than ğ‘€ . Then it connects to the 2 levels of the hierarchical softmax. The logits layer is reduced without a bottleneck effect.</figDesc><graphic coords="6,171.38,163.87,250.02,268.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,88.99,90.49,417.17,182.94"><head>Table 1</head><label>1</label><figDesc>Results of BioMachina's runs on the PlantCLEF 2022 challenge with their respective model sizes. Best results were achieved by a small ResNet50 model with traditional softmax, pretrained with the web subset. Notice, however, how the HEfficientNetB4 got similar results with a small fraction of other model's parameters.</figDesc><table coords="8,314.26,157.97,58.08,8.87"><row><cell>Size (Millions)</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Online Resources</head><p>The code of this research is available via â€¢ GitHub</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,386.69,395.17,10.91;9,112.66,400.24,393.33,10.91;9,112.66,413.79,395.17,10.91;9,112.66,427.34,395.01,10.91;9,112.66,440.89,155.44,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,458.70,386.69,49.12,10.91;9,112.66,400.24,120.81,10.91">The imageclef plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bakic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>BarthÃ©lÃ©my</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-F</forename><surname>Molino</surname></persName>
		</author>
		<idno type="DOI">10.1145/2509896.2509902</idno>
		<ptr target="https://doi.org/10.1145/2509896.2509902.doi:10.1145/2509896.2509902" />
	</analytic>
	<monogr>
		<title level="m" coord="9,275.44,400.24,230.55,10.91;9,112.66,413.79,246.13,10.91">Proceedings of the 2nd ACM International Workshop on Multimedia Analysis for Ecological Data, MAED &apos;13</title>
		<meeting>the 2nd ACM International Workshop on Multimedia Analysis for Ecological Data, MAED &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="23" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,454.44,393.32,10.91;9,112.66,467.99,393.33,10.91;9,112.66,481.54,394.62,10.91;9,112.66,495.09,201.94,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,245.16,454.44,260.82,10.91;9,112.66,467.99,184.54,10.91">Plant identification based on noisy web data: the amazing performance of deep learning (LifeCLEF</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<ptr target="https://hal.archives-ouvertes.fr/hal-01629183" />
	</analytic>
	<monogr>
		<title level="m" coord="9,349.10,467.99,156.88,10.91;9,112.66,481.54,260.63,10.91">CLEF: Conference and Labs of the Evaluation Forum, volume CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,508.64,393.33,10.91;9,112.66,522.18,393.32,10.91;9,112.66,535.73,57.08,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,237.09,508.64,268.90,10.91;9,112.66,522.18,60.82,10.91">Overview of PlantCLEF 2022: Image-based plant identification at global scale</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,195.99,522.18,309.99,10.91;9,112.66,535.73,26.38,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,549.28,393.33,10.91;9,112.66,562.83,393.32,10.91;9,112.66,576.38,394.53,10.91;9,112.66,589.93,397.48,10.91;9,112.66,605.92,43.94,7.90" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,227.26,549.28,278.73,10.91;9,112.66,562.83,55.03,10.91">Dismec: Distributed sparse machines for extreme multi-label classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Babbar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<idno type="DOI">10.1145/3018661.3018741</idno>
		<ptr target="https://doi.org/10.1145/3018661.3018741.doi:10.1145/3018661.3018741" />
	</analytic>
	<monogr>
		<title level="m" coord="9,189.99,562.83,315.99,10.91;9,112.66,576.38,125.36,10.91">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, WSDM &apos;17</title>
		<meeting>the Tenth ACM International Conference on Web Search and Data Mining, WSDM &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="721" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,617.03,394.53,10.91;9,112.66,630.58,394.53,10.91;9,112.66,644.13,393.33,10.91;10,112.66,86.97,393.33,10.91;10,112.66,100.52,353.54,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,198.52,644.13,307.47,10.91;10,112.66,86.97,247.50,10.91">Overview of lifeclef 2022: an evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>PlanquÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Å ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,383.00,86.97,122.99,10.91;10,112.66,100.52,280.38,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,114.06,393.33,10.91;10,112.66,127.61,317.86,10.91" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,353.43,114.06,152.55,10.91;10,112.66,127.61,181.08,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno>ArXiv abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,141.16,393.33,10.91;10,112.66,154.71,393.32,10.91;10,112.66,168.26,393.33,10.91;10,112.66,181.81,395.01,10.91;10,112.66,195.36,196.08,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,376.92,141.16,129.07,10.91;10,112.66,154.71,184.19,10.91">On the dangers of stochastic parrots: Can language models be too big?</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shmitchell</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442188.3445922</idno>
		<ptr target="https://doi.org/10.1145/3442188.3445922.doi:10.1145/3442188.3445922" />
	</analytic>
	<monogr>
		<title level="m" coord="10,320.95,154.71,185.03,10.91;10,112.66,168.26,261.75,10.91">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT &apos;21</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="610" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,208.91,394.53,10.91;10,112.33,222.46,393.65,10.91;10,112.66,236.01,394.52,10.91;10,112.66,249.56,230.29,10.91" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Affouard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chouet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Estopinan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gresse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-C</forename><surname>Lombardo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<ptr target="https://hal.inrae.fr/hal-03343235" />
		<title level="m" coord="10,326.66,222.46,179.32,10.91;10,112.66,236.01,128.05,10.91;10,263.95,236.01,162.90,10.91">Pl@ntNet, ten years of automatic plant identification and monitoring</title>
		<meeting><address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>IUCN</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>IUCN -CongrÃ¨s mondial de la nature</note>
</biblStruct>

<biblStruct coords="10,112.66,263.11,394.61,10.91;10,112.66,276.66,393.33,10.91;10,112.14,290.20,48.22,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,251.87,263.11,199.80,10.91">Distilling the knowledge in a neural network</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>arxiv:1503.02531</idno>
		<ptr target="http://arxiv.org/abs/1503.02531" />
	</analytic>
	<monogr>
		<title level="m" coord="10,393.77,276.66,112.22,10.91;10,112.14,290.20,42.86,10.91">NIPS 2014 Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,303.75,394.61,10.91;10,112.66,317.30,394.03,10.91;10,112.66,330.85,104.51,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,263.57,303.75,224.34,10.91">Model compression via distillation and quantization</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Polino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Alistarh</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1XolQbRW" />
	</analytic>
	<monogr>
		<title level="m" coord="10,112.66,317.30,235.80,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,344.40,393.33,10.91;10,112.66,357.95,394.53,10.91;10,112.28,371.50,393.70,10.91;10,112.66,385.05,393.33,10.91;10,112.66,398.60,397.48,10.91;10,112.36,414.59,146.82,7.90" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,396.87,344.40,109.12,10.91;10,112.66,357.95,235.24,10.91">Automated Identification of Herbarium Specimens at Different Taxonomic Levels</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carranza-Rojas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mata-Montero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-76445-0_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-76445-0_9.doi:10.1007/978-3-319-76445-0_9" />
	</analytic>
	<monogr>
		<title level="m" coord="10,253.07,371.50,252.91,10.91;10,112.66,385.05,284.88,10.91">Multimedia Tools and Applications for Environmental &amp; Biodiversity Informatics, Multimedia Systems and Applications</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Karatzas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Karppinen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="151" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,425.70,393.33,10.91;10,112.66,439.25,395.01,10.91;10,112.66,452.79,88.66,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,360.56,425.70,145.43,10.91;10,112.66,439.25,237.18,10.91">Domain adaptation in the context of herbarium collections a submission to plantclef 2020</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Villacis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mata-Montero</surname></persName>
		</author>
		<ptr target="https://www.imageclef.org/PlantCLEF2020" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,466.34,393.33,10.91;10,112.26,479.89,393.73,10.91;10,112.66,493.44,393.33,10.91;10,112.28,506.99,249.23,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,368.44,466.34,137.54,10.91;10,112.26,479.89,203.16,10.91">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,338.64,479.89,167.34,10.91;10,112.66,493.44,263.27,10.91;10,430.78,493.44,33.49,10.91">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>NIPS&apos;13</note>
</biblStruct>

<biblStruct coords="10,112.66,520.54,394.52,10.91;10,112.66,534.09,393.32,10.91;10,112.66,547.64,394.03,10.91;10,112.66,561.19,282.58,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,217.19,520.54,224.69,10.91">A scalable hierarchical distributed language model</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2008/file/1e056d2b0ebd5c878c550da6ac5d3724-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="s" coord="10,308.79,534.09,197.19,10.91;10,112.66,547.64,34.49,10.91">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2008">2008</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,574.74,395.00,10.91;10,112.66,588.29,394.01,10.91" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carranza-Rojas</surname></persName>
		</author>
		<ptr target="https://hdl.handle.net/2238/10341" />
		<title level="m" coord="10,195.46,574.74,281.10,10.91">Towards Multi-Level Classification in Deep Plant Identification</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>Instituto TecnolÃ³gico de Costa Rica</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="10,112.66,601.84,395.17,10.91;10,112.66,615.39,393.33,10.91;10,112.26,628.93,257.02,10.91" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="10,397.36,615.39,108.63,10.91;10,112.26,628.93,226.41,10.91">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,642.48,394.61,10.91;10,112.66,656.03,314.10,10.91" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="10,253.11,642.48,199.28,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1512.03385</idno>
		<ptr target="https://arxiv.org/abs/1512.03385.doi:10.48550/ARXIV.1512.03385" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,669.58,393.33,10.91;11,112.33,86.97,370.70,10.91" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="10,187.42,669.58,318.57,10.91">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1905.11946</idno>
		<ptr target="https://arxiv.org/abs/1905.11946.doi:10.48550/ARXIV.1905.11946" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,100.52,394.53,10.91;11,112.66,114.06,394.03,10.91;11,112.66,127.61,238.64,10.91" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="11,267.88,114.06,107.27,10.91">Mixed precision training</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1710.03740</idno>
		<ptr target="https://arxiv.org/abs/1710.03740.doi:10.48550/ARXIV.1710.03740" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,141.16,393.33,10.91;11,112.66,154.71,394.03,10.91;11,112.66,168.26,238.64,10.91" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="11,270.13,141.16,235.85,10.91;11,112.66,154.71,258.69,10.91">Micro batch streaming: Allowing the training of dnn models using a large batch size on small memory systems</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Synn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-K</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2110.12484</idno>
		<ptr target="https://arxiv.org/abs/2110.12484.doi:10.48550/ARXIV.2110.12484" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,181.81,395.01,10.91;11,112.66,195.36,340.81,10.91" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Falcon</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.3828935</idno>
		<ptr target="https://github.com/PyTorchLightning/pytorch-lightning.doi:10.5281/zenodo.3828935" />
		<title level="m" coord="11,163.24,181.81,220.21,10.91">The PyTorch Lightning team, PyTorch Lightning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
