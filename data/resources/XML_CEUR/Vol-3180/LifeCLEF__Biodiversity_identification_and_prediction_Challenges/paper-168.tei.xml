<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,364.38,15.42">Bird Species Classification: One Step at a Time</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,113.06,83.02,11.96"><forename type="first">Sidhart</forename><surname>Krishnan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<addrLine>450 Serra Mall</addrLine>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,184.95,113.06,86.86,11.96"><forename type="first">Priya</forename><surname>Khandelwal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<addrLine>450 Serra Mall</addrLine>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,302.80,113.06,66.33,11.96"><forename type="first">Rhythm</forename><surname>Garg</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<addrLine>450 Serra Mall</addrLine>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,364.38,15.42">Bird Species Classification: One Step at a Time</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">D38B5E96459E38403FFEA57BF12A9D55</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LSTM</term>
					<term>sequential prediction</term>
					<term>taxonomic training</term>
					<term>hierarchical structure</term>
					<term>bird species classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Long Short-Term Memory (LSTM) networks can learn long-range order dependencies across time steps, making them particularly well-suited for a variety of use-cases in Natural Language Processing (NLP), such as text generation. We believe that bird species classification can be framed as a sequence prediction problem of taxonomy, where the different ranks can be sequentially predicted to impose more structure on the output space of the model. In this paper, we explore the effectiveness of this novel framework by training and testing an LSTM network on the Xeno-Canto dataset. We compare our model against Multi-Layer Perceptron (MLP) baselines and existing works for audio-based bird classification. Our model outperformed the baselines, achieving 70% accuracy, which serves as a proof of concept that this architecture is expressive enough to be competitive for bird species classification. We conclude that with additional improvements, this method can achieve even more robust performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Long Short-Term Memory (LSTM) networks are particularly well-suited to making predictions based on sequences of data. While vanilla Recurrent Neural Networks (RNNs) suffer from the vanishing gradient problem, LSTMs can learn long-range order dependencies across time steps which is why LSTMs have become increasingly prevalent in language modeling <ref type="bibr" coords="1,440.29,420.23,14.26,10.91" target="#b0">[1]</ref>.</p><p>We are curious to see if LSTMs can also be applied to a different domain: bird sounds! All three of us love spending time in nature and want to build an app that can help us identify different birds. Most existing deployed bird classification tools use a semi-automated approach that requires ecologists to have extensive knowledge in signal processing, rendering them impractical <ref type="bibr" coords="1,140.56,487.98,11.28,10.91" target="#b1">[2]</ref>. Our model's objective is to take a bird sound audio file as input and then classify the correct species of the bird using extracted features from the audio file.</p><p>We believe that bird species classification can be framed as a sequence prediction problem of taxonomy, making the LSTM architecture a good model candidate as we can design architectures similar to sequential text generation models. In particular, the unique insight we believe we have is that the ranks (parts of a taxonomy) can be sequentially predicted to impose more structure on the output space of the model. We explore the effectiveness of this novel application of LSTMs and compare our model against ML baselines and existing works for audio-based bird classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Prior to the advent of deep learning, Support Vector Machines (SVMs) and Random Forests were the preferred approach to the bird audio classification task <ref type="bibr" coords="2,346.63,124.83,11.28,10.91" target="#b1">[2]</ref>. Now, many state-of-the-art solutions for audio-based bird classification are based on fully-supervised deep convolutional neural networks (CNNs) <ref type="bibr" coords="2,171.27,151.93,11.48,10.91" target="#b2">[3,</ref><ref type="bibr" coords="2,185.67,151.93,7.65,10.91" target="#b1">2]</ref>. These approaches typically treat the task as an image classification one by transforming the raw audio into spectrograms. Many researchers fine-tune pretrained ImageNet-based models on spectrograms of bird calls, though some models use more complex architectures like ResNet <ref type="bibr" coords="2,203.12,192.57,12.84,10.91" target="#b3">[4]</ref> or BirdNET <ref type="bibr" coords="2,272.77,192.57,11.43,10.91" target="#b4">[5]</ref>.</p><p>Many scholars have also used hybrid RNN-based approaches for the audio classification task. Liu et al. <ref type="bibr" coords="2,132.61,219.67,12.99,10.91" target="#b5">[6]</ref> added a Bi-directional Long Short-Term Memory (Bi-LSTM) neural network to DenseNet to extract the temporal correlation features of bird songs, allowing for superior bird song recognition with an average accuracy between 90% and 93%. There are also approaches that weakly label examples which allows them to be robust against background sounds in the audio <ref type="bibr" coords="2,116.77,273.87,11.43,10.91" target="#b6">[7]</ref>.</p><p>However, there did not seem to exist any works that leveraged the sequential structure or hierarchical relationships of taxonomy in bird audio classification, which we believe to be valuable information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset and Features</head><p>The Xeno-Canto dataset<ref type="foot" coords="2,197.67,392.36,3.71,7.97" target="#foot_0">1</ref> is a collection of 712,477 bird audio recordings over 10,314 different bird species. It is a widely used dataset for the bird audio species classification task and it was used as the primary dataset to train the state-of-the-art BirdNET model <ref type="bibr" coords="2,403.09,421.21,14.24,10.91" target="#b4">[5]</ref>.</p><p>We used 14850 samples across 152 species for our training and validation. These examples were provided through the BirdCLEF 2022 <ref type="bibr" coords="2,278.43,448.31,13.79,10.91" target="#b7">[8,</ref><ref type="bibr" coords="2,294.96,448.31,7.65,10.91" target="#b8">9]</ref>. The 14850 samples were split into a 60% -20% -20% train-test-validation set. Rather than using raw data as an input to the model, we denoise the audio and model the human hearing property at the feature extraction stage. Since humans perceive changes in low frequency sounds with more nuance than they do changes in high frequency sounds, we decided to use the Mel scale to map the audio data to the frequency that mimics what humans would perceive. Using the LibROSA package <ref type="bibr" coords="2,409.17,516.05,16.25,10.91" target="#b9">[10]</ref>, we extracted the Mel-frequency cepstral coefficients, the spectral centroid, short time Fourier transform, and Mel spectrogram for each audio file. By looking at the label histogram of the dataset shown below, it is clear that there is some heavy class imbalance across the species in this dataset. This motivates us to oversample the train set and val set so that we can train a model that has high precision on the minority species. This results in a completely flat species distribution for the train and val sets.</p><p>Additionally, for each model we trained, we worked to prevent overfitting using a validation set and saving the models that perform best on the validation set. We also used this validation set as the performance metric to tune each of the hyperparameters including learning rate, hidden sizes and dropout probability. Additionally, we had a patience mechanism where if the loss on the validation set increased for 50 consecutive epochs, then training would terminate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Baselines</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Multi-Layer Perceptron</head><p>The first baseline is a Multi-Layer Perceptron (MLP) model which we used to see if there is a relatively simple non-linear class boundary using the extracted features.</p><p>We trained a three-layer perceptron with hidden sizes 512, 128 using a ReLU non-linearity. This model was trained with Cross Entropy Loss, the Adam optimizer, and a learning rate of 0.001 for 1000 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Logit-Shifted Multi-Layer Perceptron</head><p>The second baseline consists of four Multi-Layer Perceptron models like in the first baseline -one for each taxonomic rank. Then starting from the highest rank -order -we use the corresponding MLP's prediction to boost the logits of the relevant labels outputted by the MLP for the next taxonomic rank, as shown in Figure <ref type="figure" coords="3,299.60,596.76,3.66,10.91" target="#fig_1">2</ref>. For example, the logits for families belonging to the order predicted by the order MLP would be increased by a large constant 𝐶 larger than any other logit in the vector. Then the family prediction would boost the logits for the genuses that belong to that family and so on.</p><p>We trained the four MLPs according to the previous section using the same loss, optimizer, hyperparameters and training epochs. However, the output layer sizes for the order, family, genus and species MLPs were 17, 41, 103 and 152 respectively. For inference, we loaded the best checkpoint for each rank's MLP and began the logit shifting process described above, starting by predicting order and ultimately predicting species. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">LSTM with Logit-Shifting</head><p>Our main model for this project is an LSTM model. A single LSTM cell is shown in Figure <ref type="figure" coords="4,500.81,368.62,5.17,10.91" target="#fig_2">3</ref> where the purpose of the cell is to extract key information about the each input while also saving key parameters over the entirety of the model. The model outputs this information in the hidden state ℎ and the cell state 𝑐 which are then passed to the next LSTM cell in the sequence.</p><p>In particular, if we look at the model architecture in Figure <ref type="figure" coords="4,362.14,422.81,3.70,10.91" target="#fig_2">3</ref>, we see that 𝑥 0 is the extracted features which is inputted into the first LSTM cell to product a hidden state and cell state ℎ 0 , 𝑐 0 . Then the hidden state is plugged into the linear layers with a dropout layer in between to get the predicted order (i.e. predict the highest taxonomic rank). The soft distribution over the order is then passed in as 𝑥 1 to the next LSTM cell which gives a new hidden state and cell state ℎ 1 , 𝑐 1 . Then we pass ℎ 1 into a seperate linear layer, dropout, linear layer architecture to predict logits over the family (i.e. the next taxonomic rank) and so on. We also apply logit shifting mechanism, described in four MLP baseline, to possible logits at each taxonomy and thus reduce the size of the prediction task.</p><p>We trained our LSTM using Cross Entropy Loss and the Adam optimizer with a learning rate of 0.001. The loss was calculated for each of the taxonomic ranks since given the true species label, we are able to backtrace to find the true genus, family and order labels to give us losses 𝐽 0 , 𝐽 1 , 𝐽 2 , 𝐽 3 . Then the overall loss of the model is</p><formula xml:id="formula_0" coords="4,273.31,608.44,232.67,33.71">𝐽 = 3 ∑︁ 𝑖=0 𝐽 𝑖 (1)</formula><p>Notably, during training of the model, we used teacher forcing <ref type="bibr" coords="4,378.58,653.13,19.13,10.91" target="#b10">[11]</ref>-a standard technique in text generation models-to mitigate the propagation of training errors forward. We implement this technique to minimize noisy gradients and better locate sources of error. We achieve teacher forcing by passing in the true probability distribution over the order, family and genus as the 𝑥 1 , 𝑥 2 and 𝑥 3 inputs in Figure <ref type="figure" coords="5,225.35,114.06,5.09,10.91" target="#fig_2">3</ref> regardless of the predicted distribution of the model. We also use the true distribution instead of the predicted distribution for logit shifting during training. In this way, each subsequent layer operates under the assumption that the previous layer was exactly correct in its prediction. However, the hidden state of the LSTM still gets passed through to the next layer without any direct tampering.</p><p>In terms of experimentation, we tried different model structures using the hidden state of each LSTM cell when predicting each rank. Initially, we started by just having no extra fullyconnected layers between the hidden state and the logits used for predicting the rank. We felt like the model was not expressive enough as it seemed to be underfitting (the validation loss was not decreasing), so we then decided to add an extra linear layer followed by dropout layer for robustness between the hidden state of each LSTM and output layer for that rank. The final model parameters we settled on was a hidden size of 512 for the LSTM which is fed into a 512 × 256 linear layer which undergoes dropout with 𝑝 = 0.4 and then is fed into a final linear layer of size 256 by the output size. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>Since our task is an imbalanced classification problem, we choose to evaluate our model on metrics besides just accuracy: precision, recall, and F1 score. The performance of each of the models over all of the metrics is shown in Table <ref type="table" coords="5,304.46,569.42,3.74,10.91" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Baselines</head><p>For the baseline Multi-Layer Perceptron, the model achieves a test accuracy of 0.57. The accuracy and loss curves begin to plateau around 1000 epochs and we found no significant improvement when trained for longer.</p><p>Then, when we compare the Logit-Shifted MLP to the simple MLP, we observed a 4.8</p><p>percentage point increase in accuracy, but a decrease in other metrics -namely precision, recall, F1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">LSTM with Logit-Shifting</head><p>As one can see, as the LSTM progresses to further ranks, the gap between train accuracy and validation tends to grow. This intuitively makes sense since the further along we go in the ranks, the more options there are. Here we also observe around epoch 1000 the negative downstream effects of incorrect order prediction, as can be seen in the train and validation loss downward spikes. We compare the MLP, logit-shifted four MLPs, and LSTM over the key metrics: Accuracy, Precision, Recall and F1 in Table <ref type="table" coords="6,341.57,429.68,5.12,10.91" target="#tab_0">1</ref> which shows that the LSTM model outperforms both baselines. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this study, the LSTM architecture outperformed all of the other baselines, likely due to the expressive power of the hidden cells combined with the hierarchical setup for the task. We conclude that this model is a potentially viable approach for the taxonomic sequence prediction task which should be explored further. More broadly, machine learning tasks like taxonomy classification, in which there is some hierarchical structure in the output space, can be interpreted under the same sequential prediction paradigm to reduce the scope of the prediction problem at each level of the hierarchy. Looking forward, we think that the model performance can be improved by adding the LSTM on top of other models; for example, logits extracted from BirdNET could be used as the features inputted to the LSTM. Another interesting result is with regard to the four MLPs paired with logit-shifting baseline as we saw that the accuracy increased compared to the standard MLP whereas the precision, recall and F1 decreased. This indicates that the four-MLP model is performing quite accurately on majority classes while performing much worse on minority bird species compared to the standard MLP. This could be because biases that occur in predictions at the higher levels of taxonomy get compounded over the course of the prediction. Thus the model's accuracy on minority bird species -which likely fall under minority bird genuses, families and orders -is worse compared to the standard MLP's performance. One future experiment could be to train four different bird classifiers like BirdNET at each taxonomy level and then concatenating them using the same logit shifting mechanism to determine whether these results are unique to the MLP architecture or occur over any bird classification model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,366.19,202.45,8.93;3,124.84,123.23,345.60,230.40"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Histogram of Species Labels in Dataset</figDesc><graphic coords="3,124.84,123.23,345.60,230.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,302.79,221.22,8.93;4,89.29,136.78,428.40,153.45"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of MLP Logit Shifting Baseline</figDesc><graphic coords="4,89.29,136.78,428.40,153.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,89.29,467.60,385.31,8.93;5,89.29,312.91,427.04,142.12"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: LSTM with Logit Shifting Architecture (including structure of single LSTM cell)<ref type="bibr" coords="5,458.14,467.65,16.46,8.87" target="#b11">[12]</ref> </figDesc><graphic coords="5,89.29,312.91,427.04,142.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,89.29,293.66,334.51,8.93;6,89.29,120.78,439.68,160.32"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: MLP Baseline Accuracy and Loss for prediction at each taxonomic rank</figDesc><graphic coords="6,89.29,120.78,439.68,160.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,89.29,349.34,215.21,8.93;7,214.30,84.19,291.68,239.79"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: LSTM with Logit-Shifting Training Curves</figDesc><graphic coords="7,214.30,84.19,291.68,239.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,88.99,472.24,416.99,91.57"><head>Table 1</head><label>1</label><figDesc>Model Metrics. The LSTM performs best and points to the sequential prediction paradigm as a viable framework for bird classification.</figDesc><table coords="6,154.33,513.59,284.13,50.22"><row><cell>Model</cell><cell cols="3">Accuracy Precision Recall</cell><cell>F1</cell></row><row><cell>MLP</cell><cell>0.5747</cell><cell>0.6286</cell><cell cols="2">0.8238 0.6880</cell></row><row><cell>Logit-Shifted Four MLPs</cell><cell>0.6229</cell><cell>0.6263</cell><cell cols="2">0.6421 0.6096</cell></row><row><cell>LSTM with Logit-Shifting</cell><cell>0.7040</cell><cell>0.7006</cell><cell cols="2">0.8828 0.7553</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,671.01,85.13,8.97"><p>https://xeno-canto.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Thanks to <rs type="institution">Stanford University</rs> for providing resources and mentorship through the CS 229 class.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,111.28,393.98,10.91;8,112.41,124.83,397.73,10.91;8,112.66,140.82,49.88,7.90" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,261.91,111.28,114.46,10.91">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="https://doi.org/10.1162/neco.1997.9.8.1735.doi:10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j" coord="8,391.60,111.28,74.22,10.91">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,151.93,393.32,10.91;8,112.66,165.48,394.04,10.91;8,112.41,179.03,50.45,10.91" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="8,336.50,151.93,169.48,10.91;8,112.66,165.48,246.15,10.91">Data-efficient classification of birdcall through convolutional neural networks transfer learning</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">B</forename><surname>Efremova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sankupellay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Konovalov</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1909.07526" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,192.57,393.33,10.91;8,112.66,206.12,394.04,10.91;8,112.66,219.67,172.59,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,269.01,192.57,236.97,10.91;8,112.66,206.12,128.36,10.91">Bird species identification using spectrogram based on multi-channel fusion of dcnns</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.3390/e23111507</idno>
		<ptr target="https://www.mdpi.com/1099-4300/23/11/1507.doi:10.3390/e23111507" />
	</analytic>
	<monogr>
		<title level="j" coord="8,249.40,206.12,35.39,10.91">Entropy</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,233.22,393.33,10.91;8,112.66,246.77,293.47,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,258.05,233.22,247.94,10.91;8,112.66,246.77,34.86,10.91">Bird call recognition using deep convolutional neural network</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sankupellay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Konovalov</surname></persName>
		</author>
		<idno type="DOI">10.13140/RG.2.2.31865.31847</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,155.23,246.77,26.14,10.91">resnet</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,260.32,393.53,10.91;8,112.66,273.87,394.04,10.91;8,112.26,287.42,394.92,10.91;8,112.36,303.41,168.57,7.90" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,324.46,260.32,181.73,10.91;8,112.66,273.87,123.08,10.91">Birdnet: A deep learning solution for avian diversity monitoring</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ecoinf.2021.101236</idno>
		<ptr target="https://doi.org/10.1016/j.ecoinf.2021.101236" />
	</analytic>
	<monogr>
		<title level="j" coord="8,252.72,273.87,103.34,10.91">Ecological Informatics</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">101236</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,314.52,393.33,10.91;8,112.66,328.07,393.33,10.91;8,112.66,341.62,376.28,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,244.44,314.52,261.55,10.91;8,112.66,328.07,35.56,10.91">Bird song classification based on improved bi-lstm-densenet network</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/RCAE53607.2021.9638962</idno>
	</analytic>
	<monogr>
		<title level="m" coord="8,174.66,328.07,331.33,10.91;8,112.66,341.62,88.12,10.91">2021 4th International Conference on Robotics, Control and Automation Engineering (RCAE)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="152" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,355.17,393.33,10.91;8,112.66,368.71,394.61,10.91;8,112.66,382.26,314.10,10.91" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="8,419.15,355.17,86.84,10.91;8,112.66,368.71,340.76,10.91">Weakly-supervised classification and detection of bird sounds in the wild. a birdclef 2021 solution</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">V</forename><surname>Conde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shubham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Agnihotri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">D</forename><surname>Movva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bessenyei</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2107.04878</idno>
		<ptr target="https://arxiv.org/abs/2107.04878.doi:10.48550/ARXIV.2107.04878" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,395.81,394.53,10.91;8,112.66,409.36,394.53,10.91;8,112.66,422.91,395.17,10.91;8,112.66,436.46,393.32,10.91;8,112.66,450.01,394.53,10.91;8,112.66,463.56,22.69,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,260.93,422.91,246.90,10.91;8,112.66,436.46,313.05,10.91">Overview of lifeclef 2022: an evaluation of machinelearning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Šulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,448.45,436.46,57.53,10.91;8,112.66,450.01,346.66,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,477.11,394.53,10.91;8,112.66,490.66,393.33,10.91;8,112.66,504.21,393.33,10.91;8,112.66,517.76,111.86,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,210.09,490.66,295.90,10.91;8,112.66,504.21,113.91,10.91">Overview of birdclef 2022: Endangered bird species recognition in soundscape recordings</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Planqué</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,236.87,504.21,269.11,10.91;8,112.66,517.76,79.94,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,531.30,394.62,10.91;8,112.28,544.85,393.70,10.91;8,112.66,558.40,122.28,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,474.12,531.30,33.16,10.91;8,112.28,544.85,185.47,10.91">librosa: Audio and music signal analysis in python</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mcvicar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,321.11,544.85,184.87,10.91;8,112.66,558.40,46.30,10.91">Proceedings of the 14th python in science conference</title>
		<meeting>the 14th python in science conference</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,571.95,393.33,10.91;8,112.66,585.50,388.73,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,217.88,571.95,288.11,10.91;8,112.66,585.50,39.20,10.91">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1989.1.2.270</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,160.68,585.50,91.81,10.91">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,599.05,394.04,10.91;8,112.66,612.60,145.91,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="8,163.12,599.05,142.22,10.91">Understanding lstm networks</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<ptr target="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
