<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,416.70,15.42;1,89.29,106.66,116.65,15.43">Solution for SnakeCLEF 2022 by Tackling Long-tailed Categorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,70.58,11.96"><forename type="first">Lingfeng</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,177.80,134.97,41.59,11.96"><forename type="first">Xiang</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Nankai University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,232.04,134.97,57.95,11.96"><forename type="first">Renjie</forename><surname>Song</surname></persName>
							<email>songrenjie@megvii.com</email>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,302.63,134.97,51.20,11.96"><forename type="first">Kexin</forename><surname>Zhu</surname></persName>
							<email>zhukexin@megvii.com</email>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,384.83,134.97,38.65,11.96"><forename type="first">Gang</forename><surname>Li</surname></persName>
							<email>gang.li@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,416.70,15.42;1,89.29,106.66,116.65,15.43">Solution for SnakeCLEF 2022 by Tackling Long-tailed Categorization</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">E3F4DF89B659CEF6E76032559B730CE2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>SnakeCLEF</term>
					<term>Fine-grained image classification</term>
					<term>Masked autoencoder</term>
					<term>Metadata</term>
					<term>Long-tailed distribution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>SnakeCLEF 2022 is a fine-grained image classification benchmark for snake identification. Recently, the masked autoencoder (MAE) has shown superior performance on fine-grained image classification tasks. As a result, we use the MAE pretrained ViT models and refine them on the SnakeCLEF 2022. Overall, the learning process contains two difficulties: 1) dealing with fine-grained species that are visually similar and 2) a long-tailed distribution. To address these issues, we propose using statistic-aware post-processing to process the metadata and refine image predictions. Next, we improve an effective logit adjustment loss (ELAL) to alleviate the classification bias toward the head class. Notably, we achieve 2nd place on the SnakeCLEF 2022 benchmark with a 0.84565 top F1 score. Codes and models are available at https://github.com/ylingfeng/snakeclef2022_fgvc9.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fine-grained visual categorization <ref type="bibr" coords="1,241.83,416.44,11.27,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,255.84,416.44,7.44,10.91" target="#b1">2,</ref><ref type="bibr" coords="1,266.02,416.44,7.44,10.91" target="#b2">3,</ref><ref type="bibr" coords="1,276.20,416.44,7.44,10.91" target="#b3">4,</ref><ref type="bibr" coords="1,286.37,416.44,7.44,10.91" target="#b4">5,</ref><ref type="bibr" coords="1,296.55,416.44,7.44,10.91" target="#b5">6,</ref><ref type="bibr" coords="1,306.73,416.44,8.91,10.91" target="#b6">7]</ref> is a popular task to identify fine categories out of coarse divisions. Recently, there is an increasing necessity to develop a fine-grained visual categorization algorithm for various species of snakes for biodiversity, conservation, and global health. The SnakeCLEF 2022 benchmark 1  <ref type="bibr" coords="1,307.86,457.09,12.91,10.91" target="#b7">[8]</ref> aims to tackle this requirement, which is held by LifeCLEF <ref type="bibr" coords="1,168.76,470.64,11.36,10.91" target="#b8">[9,</ref><ref type="bibr" coords="1,182.85,470.64,14.03,10.91" target="#b9">10]</ref> jointly with FGVC9 2 of the CVPR 2022.</p><p>The difficulty in fine-grained snake identification lies in the high intra-class and low interclass differences in appearance, and many species are visually similar to others. Moreover, the species distribution in terms of geographical location is irregular, and some countries (e.g., US) contain hundreds of species while some (e.g., Vatican) have only a few types. In addition, the dataset suffers from a severe long-tailed problem in which two-thirds of categories contain less than 100 instances.</p><p>In terms of the above problems, we propose to solve them individually. First, as for the visually similar samples which are confusing for image-only predictions, we utilize the metadata <ref type="bibr" coords="1,475.79,579.03,16.30,10.91" target="#b10">[11,</ref><ref type="bibr" coords="1,494.69,579.03,12.50,10.91" target="#b11">12,</ref><ref type="bibr" coords="2,89.04,86.97,12.59,10.91" target="#b12">13,</ref><ref type="bibr" coords="2,104.39,86.97,12.59,10.91" target="#b13">14,</ref><ref type="bibr" coords="2,119.74,86.97,12.59,10.91" target="#b14">15,</ref><ref type="bibr" coords="2,135.08,86.97,14.11,10.91" target="#b15">16]</ref> provided in the dataset to form a prior distribution of whole species. Different from previous multi-modal methods which embed the metadata to the feature space, we design a parameter-free post-processing structure to refine the predictions. To be specific, we record the number of occurrences of metadata corresponding to each species as the priors. More details can be found in Sec. 4.1. Secondly, in Sec. 4.2 we propose the effective logit adjustment loss (ELAL) to alleviate the prediction bias along with training the long-tailed samples by increasing the optimization weight of the tailed classes while reducing the head.</p><p>Our contributions can be summarized as:</p><p>â€¢ We improve a new way to process the metadata by recording statistics referring to each category and a post-processing algorithm is designed to refine the image predictions. â€¢ We propose the effective logit adjustment loss (ELAL) to alleviate the prediction bias resulting from the long-tailed dataset. â€¢ Based on our algorithm, we achieve 2nd place on the SnakeCLEF 2022 benchmark with a 0.84565 top F1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Fine-grained image classification: To deal with the fine-grained property which is hard to recognize merely through the visual clues, there are three workable solutions: 1) to detect the discriminative regions of an image and pass all parts through the networks for joint classification <ref type="bibr" coords="2,147.11,377.04,12.39,10.91" target="#b0">[1,</ref><ref type="bibr" coords="2,162.81,377.04,7.52,10.91" target="#b2">3,</ref><ref type="bibr" coords="2,173.64,377.04,7.52,10.91" target="#b3">4,</ref><ref type="bibr" coords="2,184.46,377.04,7.52,10.91" target="#b5">6,</ref><ref type="bibr" coords="2,195.29,377.04,7.65,10.91" target="#b6">7]</ref>. 2) Design a robust feature extraction architecture to capture the subtle representations from an image <ref type="bibr" coords="2,261.36,390.59,16.55,10.91" target="#b16">[17,</ref><ref type="bibr" coords="2,280.75,390.59,7.52,10.91" target="#b4">5,</ref><ref type="bibr" coords="2,291.11,390.59,7.52,10.91" target="#b1">2,</ref><ref type="bibr" coords="2,301.46,390.59,12.59,10.91" target="#b17">18,</ref><ref type="bibr" coords="2,316.89,390.59,12.42,10.91" target="#b18">19]</ref>. 3) Utilize the metadata (e.g., shooting date, latitude, longitude, country, and a brief description of the image) <ref type="bibr" coords="2,410.07,404.14,16.56,10.91" target="#b10">[11,</ref><ref type="bibr" coords="2,429.45,404.14,12.59,10.91" target="#b11">12,</ref><ref type="bibr" coords="2,444.86,404.14,12.59,10.91" target="#b12">13,</ref><ref type="bibr" coords="2,460.28,404.14,12.59,10.91" target="#b13">14,</ref><ref type="bibr" coords="2,475.69,404.14,12.59,10.91" target="#b14">15,</ref><ref type="bibr" coords="2,491.11,404.14,12.42,10.91" target="#b15">16]</ref>. However, the region detector and feature extractor are heavily designed and thus not suitable for our task. Meanwhile, the existing metadata fusion methods all deal with the multimodal feature by embedding them to higher semantic representations before interaction. Specifically, in SnakeCLEF 2022, the types of metadata are discrete (e.g., country, endemic, and code), which is different from the continuous latitude, longitude, or date hypothesized in the previous works.</p><p>To make use of this metadata, we calculate the existence label within a certain country for all country values in the metadata and form the prior matrix regarding all species.</p><p>Long-tailed distribution: In terms of the long-tailed classification, the data re-sampling <ref type="bibr" coords="2,473.86,512.54,16.30,10.91" target="#b19">[20,</ref><ref type="bibr" coords="2,492.03,512.54,13.95,10.91" target="#b20">21]</ref> seeks to change class sampling probability based on the number of samples to get a classbalanced dataset, which includes over-sampling and under-sampling. <ref type="bibr" coords="2,416.59,539.64,18.07,10.91" target="#b21">[22]</ref> develop a twostage paradigm to re-balanced the classifier in the second stage with a frozen backbone. Reweighting <ref type="bibr" coords="2,136.87,566.73,16.46,10.91" target="#b22">[23,</ref><ref type="bibr" coords="2,156.05,566.73,12.56,10.91" target="#b23">24,</ref><ref type="bibr" coords="2,171.33,566.73,12.56,10.91" target="#b24">25,</ref><ref type="bibr" coords="2,186.61,566.73,14.05,10.91" target="#b25">26]</ref> aims to assign the loss weight class-wise to reduce the optimization bias between head-tail classes. The logit adjustment loss <ref type="bibr" coords="2,359.13,580.28,18.07,10.91" target="#b22">[23]</ref> encourages a large relative margin between logits of rare versus dominant labels. Based on this work, we modify the margin coefficient and propose an effective logit adjustment loss (ELAL) to solve the long-tailed problem efficiently. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>The SnakeCLEF 2022 dataset <ref type="bibr" coords="3,221.10,442.18,11.43,10.91" target="#b7">[8,</ref><ref type="bibr" coords="3,235.27,442.18,7.50,10.91" target="#b8">9,</ref><ref type="bibr" coords="3,245.50,442.18,14.07,10.91" target="#b9">10]</ref> is based on observations of 187,129 snakes, containing 318,532 photographs, belonging to 1,572 snake species, observed in 208 countries. The data comes from the online biodiversity platform, iNaturalist. The provided dataset has a heavy long-tailed class distribution (see Fig. <ref type="figure" coords="3,263.12,482.83,3.65,10.91">1</ref>), where the most frequent species (Natrix natrix) is represented by 6,472 images and the least frequent species by just 5 samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Metric</head><p>The evaluation metric for this competition is Mean (Macro) F1-Score. The F1 score, commonly used in information retrieval, measures accuracy using the statistics precision (P) and recall (R).</p><p>The macro F1 score is not biased by class frequencies and is more suitable for the long-tailed class distributions observed in nature. This metric raises a higher requirement for classification accuracy on tailed categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Metadata-aware Post-processing</head><p>Given the metadata-label mapping, we count the instance number for all categories if it is attached to a certain metadata value. Then, we obtain the statistic in form of a metadata-wise category matrix P âˆˆ R ğ‘›Ã—ğ‘ , where ğ‘› is the value number within one type of metadata, and ğ‘ is the number of classes. Next, we transform P to a one-hot form P ğ‘œ , known as the prior statistic, which represents whether a specific category would appear in a certain place. Finally, P ğ‘œ is utilized to refine the prediction from the visual networks via the Hadamard product. The whole structure is illustrated in Fig. <ref type="figure" coords="4,220.59,213.54,3.74,10.91">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Effective Logit Adjustment Loss</head><p>In this section, we introduce our new effective logit adjustment loss (ELAL) function which addresses the performance drop resulting from the prediction bias brought by the long-tailed distribution. First, we give a brief review of the existing loss functions, and then we show how ELAL is developed based on them. The vanilla softmax cross-entropy can be derived by:</p><formula xml:id="formula_0" coords="4,201.26,331.21,304.72,37.32">â„“(ğ‘¦, ğ‘“ (ğ‘¥)) = log â› â 1 + âˆ‘ï¸ ğ‘¦ â€² Ì¸ =ğ‘¦ ğ‘’ ğ‘“ ğ‘¦ â€² (ğ‘¥)-ğ‘“ğ‘¦(ğ‘¥) â â  ,<label>(1)</label></formula><p>where ğ‘¦ denotes the ground-truth label. The logit adjustment loss <ref type="bibr" coords="4,383.64,372.94,17.87,10.91" target="#b22">[23]</ref> adds a label-dependent offset to each of the logits, and modifies Eq. 1 with the shifted coefficient ğ‘€ :</p><formula xml:id="formula_1" coords="4,191.44,400.24,314.55,37.32">â„“(ğ‘¦, ğ‘“ (ğ‘¥)) = log â› â 1 + âˆ‘ï¸ ğ‘¦ â€² Ì¸ =ğ‘¦ ğ‘€ â€¢ ğ‘’ ğ‘“ ğ‘¦ â€² (ğ‘¥)-ğ‘“ğ‘¦(ğ‘¥) â â  ,<label>(2)</label></formula><p>where ğ‘€ = ğœ‹ğ‘¦â€² ğœ‹ğ‘¦ , ğœ‹ ğ‘¦ = ğ‘ğ‘¦ âˆ‘ï¸€ ğ‘¦ â€² ğ‘ ğ‘¦ â€² âˆˆ (0, 1), and ğ‘ ğ‘¦ is the total number of instances in each class. Class-balanced Loss <ref type="bibr" coords="4,183.98,461.51,18.07,10.91" target="#b23">[24]</ref> proposes the concept of an effective number to replace the direct label-wise instance number to represent the volume of samples. The definition of the effective number is shown as:</p><formula xml:id="formula_2" coords="4,262.52,497.92,243.47,26.38">ğ¸ ğ‘¦ = 1 -ğ›½ ğ‘ğ‘¦ 1 -ğ›½ .<label>(3)</label></formula><p>Inspired by the conception of effective number, which is an improved representation of the vanilla number, we modify the logit adjustment loss by changing the shifted coefficient ğ‘€ to ğ‘€ = ğœ–ğ‘¦â€² ğœ–ğ‘¦ , ğœ– ğ‘¦ = ğ¸ğ‘¦ âˆ‘ï¸€ ğ‘¦ â€² ğ¸ ğ‘¦ â€² âˆˆ (0, 1) and propose ELAL. Notably, we set ğ›½ = 1ğ‘’ -6 by default in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we first elaborate on our experimental settings, then ablation studies are conducted to demonstrate the performance of each component. Finally, we list the top results of our methods and give a considerable analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Setup</head><p>In this paper, we use the Masked autoencoder (MAE) <ref type="bibr" coords="5,320.61,356.60,17.75,10.91" target="#b26">[27]</ref> pretrained ViT <ref type="bibr" coords="5,407.26,356.60,17.76,10.91" target="#b27">[28]</ref> models conducted on ImageNet-1K <ref type="bibr" coords="5,162.90,370.15,17.75,10.91" target="#b28">[29]</ref> training set for 800 epochs. The fine-tuning codes and checkpoints refer to the MAE repository <ref type="foot" coords="5,175.71,381.94,3.71,7.97" target="#foot_0">3</ref> . The ImageNet-1K dataset has 1.3M images with 1K categories for training and 50K images for validation. Notably, we do not use the larger ImageNet-22K (IN22K) dataset, which contains 14.2M images and 22K classes. Based on the MAE pretrained models, we finetune 50 epochs on the SnakeCLEF 2022 dataset, and the default setting is depicted in Table <ref type="table" coords="5,480.16,424.35,3.81,10.91" target="#tab_0">1</ref>. We randomly select 1/10 of the training dataset to form the validation set to update our algorithm, and a full set is used to train models which present the final submissions. To be specific, we set batch size per GPU to 2 to avoid exceeding the GPU memory. The effective learning rate is obtained following MAE: lr= base_lrÃ—globalbatchsize / 256. We apply random resizing/cropping, random horizontal flipping <ref type="bibr" coords="5,211.35,492.09,16.19,10.91" target="#b29">[30]</ref>, label-smoothing regularization <ref type="bibr" coords="5,373.45,492.09,16.19,10.91" target="#b30">[31]</ref>, Mixup <ref type="bibr" coords="5,427.81,492.09,16.18,10.91" target="#b31">[32]</ref>, CutMix <ref type="bibr" coords="5,486.95,492.09,16.19,10.91" target="#b32">[33]</ref>, RandomErasing <ref type="bibr" coords="5,165.20,505.64,16.41,10.91" target="#b33">[34]</ref>, and RandAug <ref type="bibr" coords="5,256.46,505.64,18.07,10.91" target="#b34">[35]</ref> as the standard data augmentations. Notably, all ablation studies are conducted under ViT-L for fair comparisons. The ViT-large and ViT-huge models are trained on eight NVIDIA TITAN Xp GPUs (12G) and eight GeForce RTX 3090 GPUs (24G), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Study</head><p>First, we compare the performance with different sets of metadata for post-processing. Table <ref type="table" coords="5,500.96,596.02,5.03,10.91" target="#tab_1">2</ref> shows that refining predictions with "endemic" and "code" metadata perform the best. Next, we conduct the ablation on two losses. Table <ref type="table" coords="5,338.41,623.11,5.17,10.91" target="#tab_2">3</ref> shows our ELAL achieves a higher F1 score under two sets of input resolution. To demonstrate the effectiveness of ELAL on tail   class and the potential side effect on head class, we calculate the validation accuracy on the top 10/50/100/500 class from the head and tail classes, respectively (Table <ref type="table" coords="6,399.75,571.92,3.57,10.91" target="#tab_3">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results</head><p>Based on the strong ViT-L and ViT-H <ref type="bibr" coords="6,255.39,621.65,16.11,10.91" target="#b27">[28]</ref>, we conduct experiments with an input resolution of 384/392/448 learned on a full training set. We adopt multi-crop <ref type="bibr" coords="6,361.02,635.20,17.76,10.91" target="#b35">[36]</ref> as post-processing strategies, which would crop the given image into four corners and the central crop plus the flipped version and average the predictions of whole crops. The model ensemble is an averaging operation over each prediction score after the softmax of the selected models. Our final submissions come from the ensemble of models w/o and w/ multi-crop, which receives 0.84409 and 0.84565 F1 scores on the private benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Analysis</head><p>We attempt to run a ViT-H with a 448 resolution, which is capable of reaching a higher accuracy theoretically, however, due to the resource limitation we only present the result of a 392 resolution. Also, we notice that the effect of post-processing on the private benchmark is not as significant as on the public benchmark. We suspect that there is distribution gap between the train and test set of metadata while the public benchmark is less affected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we give our solution to the Snake Recognition Competition (SnakeCLEF 2022) in FGVC9, which is challenging due to the fine-grained categorization and long-tailed classes. To deal with the difficulties, we utilize statistic-aware metadata to refile image predictions through post-processing and propose the effective logit adjustment loss (ELAL) to handle the long-tailed problem, respectively. Our team achieves the 2nd place on the private benchmark with a 0.84565 top F1 score.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,224.49,410.98,8.93"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Visualization of the instance number for each class sorted by number in descending order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,90.49,309.12,234.68"><head>Table 1</head><label>1</label><figDesc>Fine-tuning settings on the SnakeCLEF 2022 dataset.</figDesc><table coords="5,192.68,119.84,205.43,205.33"><row><cell>Config</cell><cell>Value</cell></row><row><cell>optimizer</cell><cell>AdamW</cell></row><row><cell>base learning rate</cell><cell>1e-4 (ViT-L), 1e-3 (ViT-H)</cell></row><row><cell>weight decay</cell><cell>0.05</cell></row><row><cell>optimizer momentum</cell><cell>ğ›½1, ğ›½2=0.9, 0.999</cell></row><row><cell>layer-wise lr decay</cell><cell>0.75 (ViT-L), 0.8 (ViT-H)</cell></row><row><cell cols="2">global batch size (over 8 GPUs) 16</cell></row><row><cell>batch size per GPU</cell><cell>2</cell></row><row><cell>accumulated iteration</cell><cell>4</cell></row><row><cell>learning rate schedule</cell><cell>cosine decay</cell></row><row><cell>warmup epochs</cell><cell>5</cell></row><row><cell>augmentation</cell><cell>RandAug (9, 0.5)</cell></row><row><cell>label smoothing</cell><cell>0.1</cell></row><row><cell>mixup</cell><cell>0.8</cell></row><row><cell>cutmix</cell><cell>1.0</cell></row><row><cell>random erase</cell><cell>0.25</cell></row><row><cell>drop path</cell><cell>0.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.98,90.49,386.44,97.51"><head>Table 2</head><label>2</label><figDesc>Ablation study on the performance of post-processing under different metadata combinations.</figDesc><table coords="6,204.66,119.76,189.19,68.23"><row><cell>code</cell><cell cols="3">endemic country val acc val F1 test F1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>80.470 0.758 0.755</cell></row><row><cell></cell><cell></cell><cell>âœ“</cell><cell>88.554 0.810 0.796</cell></row><row><cell>âœ“</cell><cell></cell><cell></cell><cell>88.613 0.856 0.834</cell></row><row><cell>âœ“</cell><cell>âœ“</cell><cell></cell><cell>89.949 0.873 0.864</cell></row><row><cell>âœ“</cell><cell>âœ“</cell><cell>âœ“</cell><cell>93.893 0.920 0.815</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,88.98,203.20,417.00,97.20"><head>Table 3</head><label>3</label><figDesc>Ablation study on the performance of the long-tailed loss. CE: Cross-entropy loss. ELAL: Effective logit adjustment loss.</figDesc><table coords="6,221.91,244.23,146.96,56.17"><row><cell cols="2">resolution loss</cell><cell>val acc val F1 test F1</cell></row><row><cell>224</cell><cell cols="2">CE ELAL 0.915 0.892 0.756 0.858 0.821 0.735</cell></row><row><cell>384</cell><cell cols="2">CE ELAL 0.939 0.920 0.815 0.889 0.859 0.792</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,88.98,315.73,418.54,99.81"><head>Table 4</head><label>4</label><figDesc>Ablation study on the performance of the head and tail class. We depict the accuracy of the top 10/50/100/500 from the head/tail classes. CE: Cross-entropy loss. ELAL: Effective logit adjustment loss.</figDesc><table coords="6,230.49,357.08,129.82,58.45"><row><cell>loss</cell><cell>class 10</cell><cell>50 100 500</cell></row><row><cell>CE</cell><cell cols="2">head 1.00 1.00 0.95 0.94 tail 0.30 0.46 0.56 0.79</cell></row><row><cell>ELAL</cell><cell cols="2">head 1.00 1.00 0.94 0.94 tail 0.90 0.82 0.88 0.93</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,88.99,431.94,312.86,106.53"><head>Table 5</head><label>5</label><figDesc>Performance of the final submissions on public/private benchmarks.</figDesc><table coords="6,188.94,461.41,212.92,77.06"><row><cell></cell><cell></cell><cell>center crop</cell><cell>multi crop</cell></row><row><cell>model</cell><cell cols="3">resolution public private public private</cell></row><row><cell>large</cell><cell>384</cell><cell cols="2">0.87134 0.81199 0.87996 0.81997</cell></row><row><cell>large</cell><cell>432</cell><cell cols="2">0.88375 0.82382 0.89173 0.83063</cell></row><row><cell>huge</cell><cell>392</cell><cell cols="2">0.89692 0.83662 0.89449 0.84057</cell></row><row><cell>ensemble</cell><cell>-</cell><cell cols="2">0.90245 0.84409 0.89822 0.84565</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="5,108.93,671.01,152.24,8.97"><p>https://github.com/facebookresearch/mae</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,112.66,402.99,393.61,10.91;7,112.66,416.54,116.38,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,315.15,402.99,191.13,10.91;7,112.66,416.54,39.31,10.91">Part-based r-cnns for fine-grained category detection</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,174.90,416.54,22.98,10.91">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,430.08,393.53,10.91;7,112.66,443.63,219.59,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,336.51,430.08,169.68,10.91;7,112.66,443.63,142.75,10.91">Evaluation of output embeddings for fine-grained image classification</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,277.96,443.63,23.09,10.91">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,457.18,393.33,10.91;7,112.66,470.73,394.62,10.91;7,112.66,484.28,54.28,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,340.67,457.18,165.31,10.91;7,112.66,470.73,373.56,10.91">The application of two-level attention models in deep convolutional neural network for fine-grained image classification</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,112.66,484.28,23.09,10.91">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,497.83,393.33,10.91;7,112.66,511.38,132.68,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,337.67,497.83,168.31,10.91;7,112.66,511.38,55.98,10.91">Learning to navigate for fine-grained classification</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,191.19,511.38,22.98,10.91">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,524.93,393.32,10.91;7,112.66,538.48,393.33,10.91;7,112.33,552.03,29.19,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,464.49,524.93,41.49,10.91;7,112.66,538.48,343.63,10.91">The devil is in the channels: Mutual-channel loss for fine-grained image classification</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Bhunia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,465.17,538.48,40.81,10.91">IEEE TIP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,565.58,393.53,10.91;7,112.66,579.13,225.77,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,268.62,565.58,237.57,10.91;7,112.66,579.13,147.56,10.91">Multi-branch and multi-scale attention learning for fine-grained visual categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,283.16,579.13,22.39,10.91">MMM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,592.68,393.54,10.91;7,112.66,606.22,323.87,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Behera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wharton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hewage</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06635</idno>
		<title level="m" coord="7,311.07,592.68,195.13,10.91;7,112.66,606.22,142.15,10.91">Context-aware attentional pooling (cap) for fine-grained visual classification</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,619.77,393.33,10.91;7,112.66,633.32,393.33,10.91;7,112.66,646.87,159.39,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,292.93,619.77,107.55,10.91;7,429.01,619.77,76.98,10.91;7,112.66,633.32,165.68,10.91">Automated snake species identification on a global scale</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>HrÃºz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,300.64,633.32,205.35,10.91;7,112.66,646.87,128.70,10.91">Working Notes of CLEF 2022 -Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Overview of SnakeCLEF</note>
</biblStruct>

<biblStruct coords="8,112.66,86.97,394.53,10.91;8,112.66,100.52,393.33,10.91;8,112.66,114.06,393.33,10.91;8,112.66,127.61,168.28,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,185.62,100.52,320.37,10.91;8,112.66,114.06,208.65,10.91">Lifeclef 2022 teaser: An evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,343.19,114.06,162.80,10.91;8,112.66,127.61,38.01,10.91">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="390" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,141.16,394.53,10.91;8,112.66,154.71,394.53,10.91;8,112.66,168.26,393.33,10.91;8,112.66,181.81,393.33,10.91;8,112.66,195.36,353.54,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,198.52,168.26,307.47,10.91;8,112.66,181.81,247.50,10.91">Overview of lifeclef 2022: an evaluation of machine-learning based species identification and species distribution prediction</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>PlanquÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Navine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Å ulc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,383.00,181.81,122.99,10.91;8,112.66,195.36,280.38,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,208.91,393.32,10.91;8,112.66,222.46,144.03,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,345.50,208.91,160.47,10.91;8,112.66,222.46,69.73,10.91">Improving image classification with location context</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,205.39,222.46,20.71,10.91">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,236.01,393.33,10.91;8,112.66,249.56,322.36,10.91" xml:id="b11">
	<analytic>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">C</forename><surname>Wittich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Seeland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>WÃ¤ldchen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rzanny</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>MÃ¤der</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,388.61,236.01,117.37,10.91;8,112.66,249.56,192.49,10.91">Recommending plant taxa for supporting on-site species identification</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,263.11,393.33,10.91;8,112.66,276.66,240.73,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,458.38,263.11,47.61,10.91;8,112.66,276.66,166.40,10.91">Geo-aware networks for fine-grained recognition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Potetz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,302.08,276.66,20.71,10.91">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,290.20,393.33,10.91;8,112.66,303.75,159.46,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,274.37,290.20,231.62,10.91;8,112.66,303.75,85.60,10.91">Presence-only geographical priors for fine-grained image classification</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,220.81,303.75,20.71,10.91">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,317.30,395.17,10.91;8,112.66,330.85,393.60,10.91;8,112.66,344.40,146.44,10.91" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="8,406.44,317.30,101.39,10.91;8,112.66,330.85,360.62,10.91">Dynamic mlp for finegrained image classification by leveraging geographical and temporal information</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.03253</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,357.95,393.53,10.91;8,112.66,371.50,288.50,10.91" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02751</idno>
		<title level="m" coord="8,307.65,357.95,198.53,10.91;8,112.66,371.50,106.31,10.91">Metaformer: A unified meta framework for fine-grained recognition</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,385.05,393.33,10.91;8,112.66,398.60,274.38,10.91" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03621</idno>
		<title level="m" coord="8,261.21,385.05,244.78,10.91;8,112.66,398.60,92.20,10.91">Learning deep bilinear transformation for fine-grained image representation</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,412.15,395.17,10.91;8,112.66,425.70,204.32,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="8,330.14,412.15,177.69,10.91;8,112.66,425.70,127.57,10.91">Channel interaction networks for finegrained image categorization</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,263.17,425.70,22.71,10.91">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,439.25,393.32,10.91;8,112.66,452.79,253.09,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="8,375.98,439.25,130.01,10.91;8,112.66,452.79,179.26,10.91">Grafit: Learning fine-grained image representations with coarse labels</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>JÃ©gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,314.44,452.79,20.71,10.91">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,466.34,393.33,10.91;8,112.66,479.89,261.82,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="8,292.05,466.34,213.94,10.91;8,112.66,479.89,184.51,10.91">Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z.-M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,320.19,479.89,23.09,10.91">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,493.44,393.60,10.91;8,112.66,506.99,327.58,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="8,385.99,493.44,120.27,10.91;8,112.66,506.99,108.46,10.91">Smote: synthetic minority over-sampling technique</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,229.67,506.99,178.65,10.91">Journal of artificial intelligence research</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,520.54,395.17,10.91;8,112.66,534.09,393.57,10.91;8,112.33,547.64,29.19,10.91" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09217</idno>
		<title level="m" coord="8,433.71,520.54,74.12,10.91;8,112.66,534.09,236.88,10.91">Decoupling representation and classifier for long-tailed recognition</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,561.19,393.33,10.91;8,112.66,574.74,252.90,10.91" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07314</idno>
		<title level="m" coord="8,411.38,561.19,94.60,10.91;8,112.66,574.74,70.44,10.91">Long-tail learning via logit adjustment</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,588.29,393.53,10.91;8,112.66,601.84,122.38,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="8,305.50,588.29,200.69,10.91;8,112.66,601.84,44.80,10.91">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,180.75,601.84,23.09,10.91">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,615.39,393.33,10.91;8,112.66,628.93,212.73,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="8,270.42,615.39,235.56,10.91;8,112.66,628.93,135.52,10.91">Equalization loss v2: A new gradient balance approach for long-tailed object detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,271.10,628.93,23.09,10.91">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,642.48,393.33,10.91;8,112.66,656.03,251.04,10.91" xml:id="b25">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02593</idno>
		<title level="m" coord="8,328.18,642.48,177.81,10.91;8,112.66,656.03,68.95,10.91">Equalized focal loss for dense long-tailed object detection</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,669.58,393.33,10.91;9,112.66,86.97,111.14,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="8,330.08,669.58,175.91,10.91;9,112.66,86.97,34.05,10.91">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,169.52,86.97,23.09,10.91">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,100.52,395.16,10.91;9,112.66,114.06,395.17,10.91;9,112.66,127.61,349.55,10.91" xml:id="b27">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m" coord="9,330.08,114.06,177.76,10.91;9,112.66,127.61,167.84,10.91">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,141.16,393.33,10.91;9,112.66,154.71,143.18,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="9,346.64,141.16,159.35,10.91;9,112.66,154.71,65.82,10.91">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,201.56,154.71,23.09,10.91">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,168.26,394.53,10.91;9,112.28,181.81,287.82,10.91" xml:id="b29">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<title level="m" coord="9,181.29,181.81,141.36,10.91">Going deeper with convolutions</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,195.36,393.33,10.91;9,112.66,208.91,163.25,10.91" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="9,344.36,195.36,161.63,10.91;9,112.66,208.91,86.21,10.91">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,221.63,208.91,23.09,10.91">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,222.46,395.17,10.91;9,112.66,236.01,197.93,10.91" xml:id="b31">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m" coord="9,331.02,222.46,176.81,10.91;9,112.66,236.01,16.17,10.91">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,249.56,393.32,10.91;9,112.66,263.11,257.70,10.91" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="9,329.08,249.56,176.90,10.91;9,112.66,263.11,183.62,10.91">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,319.06,263.11,20.71,10.91">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,276.66,394.53,10.91;9,112.66,290.20,22.69,10.91" xml:id="b33">
	<monogr>
		<title level="m" type="main" coord="9,303.70,276.66,152.68,10.91">Random erasing data augmentation</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,303.75,395.16,10.91;9,112.66,317.30,238.29,10.91" xml:id="b34">
	<monogr>
		<title level="m" type="main" coord="9,291.86,303.75,215.96,10.91;9,112.66,317.30,151.58,10.91">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,330.85,393.33,10.91;9,112.66,344.40,158.49,10.91" xml:id="b35">
	<monogr>
		<title level="m" type="main" coord="9,295.99,330.85,209.99,10.91;9,112.66,344.40,70.43,10.91">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
