<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,81.95,77.31,431.35,15.35;1,85.88,97.95,423.56,15.35;1,124.72,118.83,345.77,15.35">Caverns Detection and Caverns Report in Tuberculosis: lesion detection based on image using YOLO-V3 and median based multi-label multi-class classification using SRGAN</title>
				<funder ref="#_HfyuYkV #_aUY2Kum #_GSaFBMs">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.00,162.70,84.97,10.54"><forename type="first">Tetsuya</forename><surname>Asakawa</surname></persName>
							<email>asakawa@kde.cs.tut.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyohashi University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,166.97,162.70,66.99,10.54"><forename type="first">Riku</forename><surname>Tsuneda</surname></persName>
							<email>tsuneda.riku.am@tut.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyohashi University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,243.96,162.70,77.65,10.54"><forename type="first">Kazuki</forename><surname>Shimizu</surname></persName>
							<email>shimizu@heart-center.or.jp</email>
							<affiliation key="aff2">
								<orgName type="institution">Toyohashi Heart center</orgName>
								<address>
									<addrLine>21-1 Gobutori Tenpaku</addrLine>
									<settlement>Oyama, Toyohashi</settlement>
									<region>Aichi</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,331.61,162.70,90.32,10.54"><forename type="first">Takuyuki</forename><surname>Komoda</surname></persName>
							<email>komoda@heart-center.or.jp</email>
							<affiliation key="aff2">
								<orgName type="institution">Toyohashi Heart center</orgName>
								<address>
									<addrLine>21-1 Gobutori Tenpaku</addrLine>
									<settlement>Oyama, Toyohashi</settlement>
									<region>Aichi</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,449.50,162.70,64.99,10.54"><forename type="first">Masaki</forename><surname>Aono</surname></persName>
							<email>aono@tut.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Toyohashi University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Hibarigaoka Tenpaku</orgName>
								<address>
									<settlement>Toyohashi</settlement>
									<region>Aichi</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">CLEF</orgName>
								<address>
									<postCode>2022</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,81.95,77.31,431.35,15.35;1,85.88,97.95,423.56,15.35;1,124.72,118.83,345.77,15.35">Caverns Detection and Caverns Report in Tuberculosis: lesion detection based on image using YOLO-V3 and median based multi-label multi-class classification using SRGAN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C5386EB9F1B8E7C7C4E73A24FA4F4B3C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Tuberculosis</term>
					<term>Deep Learning</term>
					<term>Image Super Resolution</term>
					<term>Detection</term>
					<term>Multi-label and Multi-class classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ImageCLEF 2022 Tuberculosis Task is an example of a challenging research problem in the field of CT image analysis. The purpose of this research is to make lesion detection for tuberculosis and accurate estimates for the three labels. We describe the tuberculosis task and approach for chest CT image analysis, then perform lesion detection and multi-label classification in the CT image analysis using the task dataset. We propose a fine-tuning deep neural network model that uses inputs from multiple CNN features. In addition, this paper presents two approaches for applying mask data to the extracted 2D image data and for extracting a set of 2D projection images along multi-axis based on the 3D chest CT data. Our submissions on the task test dataset reached a mAP IOU value of about 19% at detection, reached a mean AUC value of about 66% and a minimum AUC value of about 32% in classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the spread of various diseases (e.g., tuberculosis (TB), COVID-19, and influenza), medical research has been performed to develop and implement the necessary treatments for viruses. However, there is no method currently available to identify such diseases early. An early diagnosis method is needed to provide the necessary treatment, develop specific medicines, and prevent the deaths of patients.</p><p>Accordingly, a significant amount of effort has been invested in medical image analysis research in recent years. In fact, a task dedicated to TB has been adopted as part of the ImageCLEF evaluation campaign for the five last years <ref type="bibr" coords="1,245.31,565.57,12.54,9.69" target="#b0">[1]</ref>[2][3][4] <ref type="bibr" coords="1,295.46,565.57,12.54,9.69" target="#b4">[5]</ref>. In ImageCLEF 2022 the main task <ref type="bibr" coords="1,507.66,565.57,11.74,9.69" target="#b5">[6]</ref>, "ImageCLEFmed Tuberculosis," is treated as a computed tomography (CT) report.</p><p>The goal of the first subtask (Caverns Detection) is to detect lung cavern regions in lung CT images associated with lung caverns characteristics. And the second subtask (Caverns Report) is to predict three binary features of caverns suggested by experienced radiologists.</p><p>In this paper, we employ a new fine-tuning neural network model that uses features extracted by pretrained convolutional neural network (CNN) models and Vision Transformer (ViT) as input. We propose a new fully connected two layers. The new contributions of this paper are the proposition of novel feature building techniques, the incorporation of features from the proposed CNN model, and the use of several forms of pre-processing to predict TB from the images. In Section 2, we describe the conducted task and the ImageCLEF2022 dataset. In Section 3, we introduce the image pre-processing, experimental settings, and features used in this study. In Section 4, we describe the experiments we performed. In Section 5, we provide our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">ImageCLEF 2022 Dataset</head><p>The TB task of the ImageCLEF 2022 Challenge included partial 3D patient chest CT images <ref type="bibr" coords="2,507.81,170.53,11.63,9.69" target="#b5">[6]</ref>. The task includes two subtasks: Caverns Detection, Caverns Report.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Caverns Detection</head><p>The task dataset contains 559 train and 140 test cases. In addition, participants may also use 60 training cases from the Caverns Report task. Any other public dataset usage is also welcome.</p><p>Each case includes the CT image, two versions of automatically extracted lung masks, and information on cavern area location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Caverns Report</head><p>The task dataset contains 60 train and 16 test cases. In this task, participants must generate automatic lung-wise reports based on CT image data. Each report should include probability scores (ranging from 0 to 1) for each of the three labels and for each of the lungs (resulting in three entries per CT). The resulting list of entries includes thick walls, calcification, and presence of foci. Table <ref type="table" coords="2,451.90,374.53,5.52,9.69">1</ref> lists the labels for the chest CT scan in the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Presence of labels for the chest CT scan in the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label</head><p>In Training set (total numbers) thick walls 49 calcification 34 foci presence 30</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>We propose detection and a multi-label analysis system to predict caverns characteristics from CT scan images. The first step is the input data pre-processing in both analyses. After pre-processing input data, we will describe our deep neural network model that enables the detection and multi-label outputs, given CT scan images. In addition, we add an optional step to the first step. We use a CT scan movie not CT scan images. We will detail our proposed system in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Input data pre-processing 3.1.1. Input data pre-processing for Caverns Detection</head><p>CT scans in the training and test datasets are provided in compressed Nifti format. We decompressed the files and extracted the slices along the z-axis of the 3D image, as shown in fig. <ref type="figure" coords="2,448.37,706.69,4.13,9.69" target="#fig_0">1</ref>. For each Nifti image, we obtained several slices, according to the dimensions, ranging from 110 to 250 images for the z-dimension. After extracting the slices along the z-axis, we filtered the slices of each patient using mask1 and mask2 data <ref type="bibr" coords="2,179.39,744.61,12.19,9.69" target="#b6">[7]</ref> <ref type="bibr" coords="2,191.57,744.61,12.19,9.69" target="#b7">[8]</ref>. The mask1 data provide more accurate masks but tend to miss large abnormal regions of the lungs in the most severe TB cases. The mask2 data provide more rough bounds but behave more stably in terms of including lesion areas. We extracted the filtered CT scan images. We noticed that all slices contain relevant information, including bone, space, fat, and skin, in addition to the lungs that could help classify the samples. Therefore, we added a step to the filter and selected several slices per patient. We call this data the Applying mask CT data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Input data pre-processing in Caverns Report</head><p>The 3D CT scans in the training and test datasets are provided in compressed Nifti format. We decompressed the files and extracted the slices along the z-axis of the 3D image, as shown in fig. <ref type="figure" coords="3,497.19,476.05,4.13,9.69" target="#fig_0">1</ref>. For each Nifti image, we obtained several slices, according to the dimensions, ranging from 110 to 250 images for the z-dimension. After extracting the slices along the z-axis, we filtered the slices of each patient using mask data <ref type="bibr" coords="3,176.98,513.97,11.68,9.69" target="#b7">[8]</ref>. The mask data provide more rough bounds but behave more stably in terms of including lesion areas. We extracted the filtered CT scan images. We noticed that all slices contain relevant information, including bone, space, fat, and skin, in addition to the lungs that could help classify the samples. Therefore, we added a step to the filter and selected several slices per patient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proposed deep neural network model 3.2.1. Proposed deep neural network model for Caverns Detection</head><p>To conduct this detection, we propose annotation-based YOLO-V3 <ref type="bibr" coords="3,393.99,626.77,12.84,9.69" target="#b8">[9]</ref> that allow inputs coming from medical images. We used Caverns Report Train cavern bounding boxes as input. Cavern area location information includes a cavern area bounding box and cavern area centroid. As illustrated in fig. <ref type="figure" coords="3,72.00,664.69,4.13,9.69" target="#fig_1">2</ref>, we annotate from the image, and predict cavern area using YOLO-V3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Proposed deep neural network model at Caverns Report</head><p>To solve our multi-label problem, we propose new combined neural network models which allow inputs coming from End-to-end (CNN) features.</p><p>We perform Super-Resolution using SRGAN(Generative Adversarial Network for Super-Resolution). The SRGAN allows the model to achieve an upscaling factor of almost 4x for most image visuals. Thus, the resolution pixel size of 512 X 512 of dataset provided by ImageCLEF has pixel size of 2048 X 2048.</p><p>Here, we divided the training dataset at random into training and validation datasets with a ratio of 8:2. The CNN features were extracted using pre-trained CNN-based neural networks, including EfficientNet B07. To deal with the above features, we propose a deep neural network architecture.</p><p>Our system incorporates CNN features, which can be extracted using deep CNNs pre-trained on ImageNet <ref type="bibr" coords="4,117.50,513.97,18.33,9.69" target="#b9">[10]</ref> such as EffcientNet B07 <ref type="bibr" coords="4,239.79,513.97,19.70,9.69" target="#b10">[11]</ref>. Because of the lack of datasets in visual sentiment analysis, we adopted transfer learning for the feature extraction to prevent overfitting. We decreased the dimensions of the fully connected layers used in the CNN models. In addition, we extracted the vector to 2048 dimensions.</p><p>We employ from three CNNs models. As illustrated in Fig. <ref type="figure" coords="4,357.86,564.61,4.13,9.69" target="#fig_1">2</ref>, the CNN feature is combined and represented by an integrated feature as a linearly weighted average, where weights are w3 for CNN features, respectively. CNN feature is passed out on "Fusion" processing to generate the integrated features, followed by "softmax" activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Annotation Object detection</head><p>We propose a method illustrated in Algorithm 1. The input is a collection of features extracted from each image with K kinds of sentiments, while the output is a K-dimensional multi-hot vector.</p><p>In Algorithm 1, we assume that the extracted CNN feature is represented by their probabilities. For each caverns characteristics, we sum up the features, followed by median of the result, which is denoted by ùëá ! " in Algorithm 1. In short, the vector ùëÜ " represents the output multi-hot vector. We repeat this computation until all the test (unknown) images are processed.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Submission at Caverns Detection</head><p>The training, validation, and test dataset consists of Applying mask1 and mask2 CT data. Here, we have divided the filtering data into training and validation datasets with a ratio of 8:2 to all slices from the same patient. We determined the following hyper-parameters: the optimizer function is SGD with a learning rate of 0.001 and a momentum of 0.9, decay of 0.0005, a batch size of 4, and the number of epochs is 200. We implement the "map_iou" as a loss. For the implementation, we employed PyTorch as our deep learning framework. These experiments were performed using PyTorch on Ubuntu 20.04. The workstation has an Intel Xeon 6242RXeon(20core/3.10GHz/TDP:205W) CPU with 16GB of 6 RAM and an NVIDIA RTX A6000 GPU.</p><p>For the evaluation of the Caverns Detection, Table2 shows the results. Finally, we employed YOLO-V3 for the training and validation datasets and the test data. The results are given in Section (4.2.1). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Submission for Caverns Report</head><p>Here, we have divided the filtering data into training and validation datasets with a ratio of 8:2. We determined the following hyper-parameters: the batch size is 256, the optimization function is stochastic gradient descent with a learning rate of 0.001 and a momentum of 0.9, and the number of epochs is 200 using early-stopping. For the implementation, we employed Tensorflow[12] as our deep learning framework. These experiments were performed using Tensorflow 2.6 on Ubuntu 20.04. The workstation has an Intel Xeon 6242RXeon(20core/3.10GHz/TDP:205W) CPU with 16GB of 6 RAM and an NVIDIA RTX A6000 GPU.</p><p>For the evaluation of the multi-label classification, we employed mean_auc and min_auc. Table <ref type="table" coords="6,517.67,194.53,5.52,9.69" target="#tab_2">3</ref> shows the results. Finally, we employed ViT-L/16, DenseNet201, EfficientNet B07 for the training and validation datasets and the test data. The results are given in Section (4.2.2). The results of the other participants' submissions with the map_iou are shown in Table <ref type="table" coords="6,471.82,423.97,4.13,9.69" target="#tab_4">4</ref>. Here, we compare the results in terms of the map_iou. For our team, KDE-lab, our proposed YOLO-V3 has the best score only this time. However, this is not the latest method.</p><p>We need to apply these datasets to other methods such as YOLO-X and YOLO-V5 in the future. The results achieved by our submissions are well ranked compared to those at the top of the list given in Table <ref type="table" coords="6,111.07,487.09,4.13,9.69" target="#tab_4">4</ref>. In terms of the map_ioc, our model ranks 3rd. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Results for the training and validation datasets and the test data using our proposed model at Caverns Report</head><p>We show the results in Table <ref type="table" coords="6,215.60,678.37,5.52,9.69">5</ref> with the terms of mean_auc and min_auc, and we compare the results in terms of the mean_auc and min_auc to the results of the other participants' submissions.</p><p>For our team, KDE-lab, our proposed CNN model has the best mean_auc and min_auc. The results achieved by our submissions are well ranked compared to those at the top of the list given in Table <ref type="table" coords="6,515.12,716.29,4.13,9.69">5</ref>. In terms of the mean_auc model ranks 2nd and in terms of the min_auc it ranks 3rd.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,72.00,392.86,276.80,9.95;3,72.15,135.24,451.00,253.70"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Pre-processing of the input data applying mask data.</figDesc><graphic coords="3,72.15,135.24,451.00,253.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,72.00,329.50,361.81,9.95;4,75.29,227.47,451.30,129.57"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Process for proposed deep neural network model at Caverns Detection.</figDesc><graphic coords="4,75.29,227.47,451.30,129.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,72.00,392.86,243.11,9.95;5,72.00,224.44,64.23,140.71"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Our proposed method for feature extraction.</figDesc><graphic coords="5,72.00,224.44,64.23,140.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,72.00,313.84,349.89,391.61"><head>Table 2</head><label>2</label><figDesc>Our submission for the Caverns Detection.</figDesc><table coords="5,181.00,666.46,240.89,38.99"><row><cell>Mask</cell><cell>Model</cell><cell>map_iou</cell></row><row><cell>Mask1</cell><cell>YOLO-V3</cell><cell>0.178</cell></row><row><cell>Mask2</cell><cell>YOLO-V3</cell><cell>0.185</cell></row></table><note coords="5,127.97,313.84,31.23,8.99;5,228.34,360.59,18.73,9.18"><p><p>SRGAN</p>Part</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,72.67,169.75,424.80,218.98"><head>of disease Whole image CNN (DNN) + FC (fully-connected layer) CNN (DNN) + FC (fully-connected layer) (512, 512) (2048, 2048) (2048, 2048) (512, 512) Concat (Fusion) Multi hot vector</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,72.00,245.74,406.68,126.88"><head>Table 3</head><label>3</label><figDesc>Our submission for the Caverns Report.</figDesc><table coords="6,72.00,275.02,406.68,97.60"><row><cell>Model</cell><cell>Parameter</cell><cell>mean_auc</cell><cell>min_auc</cell></row><row><cell>ViT-L/16</cell><cell>1000</cell><cell>0.598</cell><cell>0.508</cell></row><row><cell>DenseNet201</cell><cell>1920</cell><cell>0.559</cell><cell>0.460</cell></row><row><cell>EfficientNet B07</cell><cell>2560</cell><cell>0.658</cell><cell>0.317</cell></row><row><cell>4.2</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,72.00,360.08,451.26,47.10"><head>. The result of Caverns Detection and Report 4.2.1. Results for the training and validation datasets and the test data using our proposed model for Caverns Detection</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,72.00,513.10,349.89,97.07"><head>Table 4</head><label>4</label><figDesc>The best participants' runs submitted for the Caverns Detection.</figDesc><table coords="6,158.85,542.14,263.04,68.03"><row><cell>Group name</cell><cell>Rank</cell><cell>map_iou</cell></row><row><cell>CSIRO</cell><cell>1</cell><cell>0.504</cell></row><row><cell>SenticLab.UAIC</cell><cell>2</cell><cell>0.295</cell></row><row><cell>KDE-Lab</cell><cell>3</cell><cell>0.185</cell></row><row><cell>SDVA-UCSD</cell><cell>4</cell><cell>0.000</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">Acknowledgements</head><p>A part of this research was carried out with the support of the <rs type="grantName">Grant for Toyohashi Heart Center Smart Hospital Joint Research Course</rs> and the <rs type="grantName">Grant-in-Aid for Scientific Research</rs> (C) (issue numbers <rs type="grantNumber">22K12149</rs> and <rs type="grantNumber">22K12040</rs>)</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HfyuYkV">
					<orgName type="grant-name">Grant for Toyohashi Heart Center Smart Hospital Joint Research Course</orgName>
				</org>
				<org type="funding" xml:id="_aUY2Kum">
					<idno type="grant-number">22K12149</idno>
					<orgName type="grant-name">Grant-in-Aid for Scientific Research</orgName>
				</org>
				<org type="funding" xml:id="_GSaFBMs">
					<idno type="grant-number">22K12040</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this study, we proposed image pre-processing and a CNN model to detect lung cavern regions in lung CT images associated with lung caverns characteristics and to predict three binary features of caverns suggested by experienced radiologists.</p><p>We performed a lung CT image analysis in which we proposed a deep neural network model that enabled the inputs to be derived from the CNN features. To predict the three labels, we introduced a median-based multi-label prediction algorithm.</p><p>Specifically, after training our deep neural network using the pre-processed images, we were able to predict the categories of the three types of TB cases from unknown CT scan images.</p><p>The experimental results demonstrate that our proposed models out-perform some models in terms of the mean_auc and the min_auc. For the mean_auc and min_auc, our model achieved a good value. Therefore, we believe that using pre-processed images is effective.</p><p>In the future, given an arbitrary X-ray, CT, echo, or magnetic resonance imaging image might include the optimal weights for the neural networks. Moreover, we hope our proposed model will encourage further research into the early detection of diseases (such as TB, COVID-19, and influenza) or unknown diseases.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,88.95,578.77,434.36,9.69;7,86.20,591.25,437.19,9.69;7,86.20,603.97,403.55,9.69" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,405.50,578.77,117.82,9.69;7,86.20,591.25,295.92,9.69">Overview of ImageCLEFtuberculosis 2017 -predicting tuberculosis type and drug resistances</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">Dicente</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kalinovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="7,402.45,591.25,116.20,9.69">CLEF2017 Working Notes</title>
		<title level="s" coord="7,86.20,603.97,188.63,9.69">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.95,616.69,434.36,9.69;7,86.20,629.17,437.26,9.69;7,86.20,641.89,437.11,9.69;7,86.20,654.61,104.27,9.69" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,334.54,616.69,188.77,9.69;7,86.20,629.17,416.12,9.69">OverviewofImageCLEFtuberculosis2018detecting multi-drug resistance, classifying tuberculosis type, and assessing severity score</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">Dicente</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="7,86.20,641.89,119.84,9.69">CLEF2018 Working Notes</title>
		<title level="s" coord="7,214.83,641.89,192.53,9.69">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.95,667.33,434.36,9.69;7,86.20,679.81,437.16,9.69;7,86.20,692.53,437.12,9.69;7,86.20,705.25,218.42,9.69" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,447.33,667.33,75.98,9.69;7,86.20,679.81,437.16,9.69;7,86.20,692.53,37.75,9.69">Overview of Im-ageCLEFtuberculosis 2019 -Automatic CT-based Report Generation and Tuberculosis Severity Assessment</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">Dicente</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="7,157.26,692.53,128.64,9.69">CLEF2019 Working Notes</title>
		<title level="s" coord="7,299.12,692.53,205.95,9.69">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,88.95,717.73,434.29,9.69;7,86.20,730.45,437.12,9.69;7,86.20,743.17,429.21,9.69" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,448.39,717.73,74.85,9.69;7,86.20,730.45,294.03,9.69">Overview of Im-ageCLEFtuberculosis 2020 -automatic CT-based report generation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="7,401.62,730.45,116.96,9.69">CLEF2020 Working Notes</title>
		<title level="s" coord="7,86.20,743.17,188.63,9.69">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,88.95,75.01,434.36,9.69;8,86.20,87.49,437.19,9.69;8,86.20,100.21,393.46,9.69" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,397.12,75.01,126.20,9.69;8,86.20,87.49,258.84,9.69">Overview of ImageCLEFtuberculosis 2021 -CT-based tuberculosis type classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="8,366.35,87.49,120.12,9.69">CLEF 2021 Working Notes</title>
		<title level="s" coord="8,494.02,87.49,29.36,9.69;8,86.20,100.21,156.53,9.69">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,88.95,112.93,434.36,9.69;8,86.20,125.65,437.13,9.69;8,86.20,138.13,283.19,9.69" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,333.79,112.93,189.53,9.69;8,86.20,125.65,169.57,9.69">OverviewofImageCLEFtuberculosis2022 -CT-based caverns detection and report</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicentecid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="8,277.59,125.65,117.67,9.69">CLEF2022 Working Notes</title>
		<title level="s" coord="8,403.02,125.65,120.32,9.69;8,86.20,138.13,48.17,9.69">CEUR Workshop Proceedings, CEUR</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,88.95,150.85,434.59,9.69;8,86.20,163.57,437.11,9.69;8,86.20,176.05,437.19,9.69;8,86.20,188.77,435.87,9.69" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,396.00,150.85,127.54,9.69;8,86.20,163.57,178.04,9.69">Efficient and fully automatic segmentation of the lungs in ct volumes</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">Dicente</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">A</forename><surname>Jim√©nez Del Toro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Depeursinge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<ptr target="-WS.org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="8,214.60,176.05,308.79,9.69;8,86.20,188.77,45.40,9.69">Proceedings of the VISCERAL Anatomy Grand Challenge at the 2015 IEEE ISBI</title>
		<title level="s" coord="8,139.05,188.77,164.23,9.69">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">O</forename><surname>Goksel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Jim√©nez Del Toro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Foncubierta-Rodr√≠guez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</editor>
		<meeting>the VISCERAL Anatomy Grand Challenge at the 2015 IEEE ISBI</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,88.95,201.49,434.41,9.69;8,86.20,213.97,437.12,9.69;8,86.20,226.69,193.96,9.69" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,203.48,201.49,319.88,9.69;8,86.20,213.97,56.77,9.69">Imageclef 2017: Supervoxels and co-occurrence for tuberculosis ct image classification</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="8,171.08,213.97,124.11,9.69">CLEF2017 Working Notes</title>
		<title level="s" coord="8,306.14,213.97,198.95,9.69">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,88.95,239.41,434.40,9.69;8,86.20,252.13,166.84,9.69" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,166.44,239.41,356.91,9.69;8,86.20,252.13,17.13,9.69">YOLOv3: An Incremental Improvement, Computer Vision and Pattern Recognition</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ali</surname></persName>
		</author>
		<idno>arxiv:1804.02767</idno>
	</analytic>
	<monogr>
		<title level="m" coord="8,112.08,252.13,52.59,9.69">CVPR) 2018</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.00,264.61,415.31,9.69;8,86.20,277.33,437.03,9.69;8,86.20,290.05,335.46,9.69" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,308.83,277.33,214.41,9.69;8,86.20,290.05,22.15,9.69">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,115.52,290.05,208.53,9.69">International Journal of Computer Vision (IJCV</title>
		<imprint>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,108.00,302.53,415.31,9.69;8,86.20,315.25,96.21,9.69" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,199.34,302.53,319.18,9.69">Efficientnet: Rethinking model scaling for convo-lutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,86.20,315.25,27.53,9.69">ICML</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
