<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,91.52,75.47,412.25,16.98;1,77.00,96.23,441.29,16.98;1,156.80,116.99,281.61,16.98">CS_Morgan at ImageCLEFmedical 2022 Caption Task: Deep Learning Based Multi-Label Classification and Transformers for Concept Detection &amp; Caption Prediction</title>
				<funder ref="#_mxSqUXa">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,72.00,162.58,116.04,10.80"><forename type="first">Md</forename><forename type="middle">Mahmudur</forename><surname>Rahman</surname></persName>
							<email>md.rahman@morgan.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Morgan State University</orgName>
								<address>
									<addrLine>1700 East Cold Spring Ln</addrLine>
									<postCode>21251</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,215.36,162.58,76.33,10.80"><forename type="first">Oyebisi</forename><surname>Layode</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Morgan State University</orgName>
								<address>
									<addrLine>1700 East Cold Spring Ln</addrLine>
									<postCode>21251</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,91.52,75.47,412.25,16.98;1,77.00,96.23,441.29,16.98;1,156.80,116.99,281.61,16.98">CS_Morgan at ImageCLEFmedical 2022 Caption Task: Deep Learning Based Multi-Label Classification and Transformers for Concept Detection &amp; Caption Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FE449437C96AED61FE89517D09B36205</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>1 Medical imaging</term>
					<term>Image annotation</term>
					<term>Deep learning</term>
					<term>Caption prediction</term>
					<term>Concept detection</term>
					<term>Transformer</term>
					<term>Multi-label classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of Morgan_CS in both Concept Detection and Caption Prediction tasks under the ImageCLEFmedical 2022 Caption task. The task required participants to automatically identifying the presence and location of relevant concepts and composing coherent captions for the entirety of an image in a large corpus which is a subset of the extended Radiology Objects in COntext (ROCO) dataset. Our implementation is motivated by using encoder-decoder based sequence-to-sequence model for caption and concept generation using both pre-trained Text and Vision Transformers (ViTs). In addition, the Concept Detection task is also considered as a multi concept labels classification problem where several deep learning architectures with "sigmoid" activation are used to enable multilabel classification with Keras. We have successfully submitted eight runs for the Concept Detection task and four runs for the Caption Prediction task. For the Concept Detection Task, our best model achieved an F1 score of 0.3519 and for the Caption Prediction Task, our best model achieved a BLEU Score of 0.2549 while using a fusion of Transformers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Automatic caption and concept generation of biomedical images is a challenging AI problem which requires both techniques from Computer Vision to interpret the concepts of the image and understanding the relationship among concepts and techniques from natural language processing (NLP) to generate the textual description. During the past decade, immense progress has been made to automatically generate clinical reports and captions from medical images using Deep Learning to assist clinicians in accurate decision-making and speed up the diagnosis process <ref type="bibr" coords="1,349.36,576.40,8.29,9.88" target="#b0">[1]</ref><ref type="bibr" coords="1,357.65,576.40,4.15,9.88" target="#b1">[2]</ref><ref type="bibr" coords="1,361.79,576.40,8.29,9.88" target="#b2">[3]</ref>. Most of the existing literature in medical domain are based on a Convolutional Neural Network (CNN) -Recurrent Neural Network (RNN) framework where a pre-trained CNN is used to encode the images and a RNN is used to either encode the text sequence generated so far, and/or generate the next word in the sequence <ref type="bibr" coords="1,464.81,614.32,11.68,9.88" target="#b0">[1]</ref>.</p><p>Computer Vision, the medical imaging field has also witnessed growing interest for Transformers in segmentation, detection, classification, reconstruction, synthesis, registration, clinical report generation, and other tasks <ref type="bibr" coords="2,143.58,100.02,11.68,9.88" target="#b5">[6]</ref>. Recent works have shown that these Transformer modules can fully replace the standard convolutions in deep neural networks by operating on a sequence of image patches and capturing the global context of an input image, giving rise to ViTs <ref type="bibr" coords="2,369.22,125.36,11.73,9.88" target="#b6">[7]</ref>, which push the state-of-the-art in numerous computer vision tasks. Core components behind the success of ViTs that are self-attention which determines the relative importance of a single token (patch embedding) with respect to all other tokens in the sequence and multi-head self-attention consists of multiple self-attention blocks (heads) concatenated together channel-wise to model complex dependencies between different elements in the input sequence.</p><p>Inspired from this transition and motivated by the successful adaptation of Transformers in medical domain, we investigated encoder-decoder based Text and ViTs for sequence-to-sequence model generation in both the Concept Detection and Caption Prediction tasks based on our participation in the ImageCLEFmedical 2022 Caption task <ref type="bibr" coords="2,245.62,251.18,12.84,9.88" target="#b7">[8]</ref> under the ImageCLEF 2022 evaluation campaign <ref type="bibr" coords="2,479.10,251.18,11.69,9.88" target="#b8">[9]</ref>. These tasks requires participants to automatically identifying the presence and location of relevant concepts and composing coherent captions for the entirety of an image in a large corpus which is a subset of the extended ROCO dataset <ref type="bibr" coords="2,190.34,289.16,16.91,9.88" target="#b9">[10]</ref>. Especially, we investigated a seq2seq model for medical image captioning that employs both pre-trained CNN (e.g., CheXNet <ref type="bibr" coords="2,355.69,301.82,17.73,9.88" target="#b10">[11]</ref>) and ViT as the encoder and the pre-trained BERT as the decoder. In addition, the concept detection task is also considered as a multilabel concept classification problem where scikit-learn based MultiLabelBinarizer class is used with several CNN architectures with "sigmoid" activation at the end of the network architecture to enable us to perform multi-label classification with Keras <ref type="bibr" coords="2,283.96,352.40,16.88,9.88" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodologies</head><p>In this section, we describe the encoder-decoder based transformer architecture for sequence-tosequence modeling, the multi-label classification based on using different CNN models approach for concept detection and training and fusion strategies for the submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Transformer-based Encoder-Decoder Models</head><p>The captioning model utilized in this task follows the modular transformer architecture consists of the following three models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Encoder</head><p>The goal of the 'transformer' encoder used in this study was to transform visual features obtained from a pretrained CheXNet model into an attended projection representative of features of the image that are important towards the correct sequence generation. The CheXNet model consists of 5 conv blocks of multiple convolutional, ReLU activation, batch normalization, concatenation, and average pooling layers <ref type="bibr" coords="2,139.57,614.20,16.87,9.88" target="#b10">[11]</ref>. To obtain an attended projection of the image fed into the pretrained CheXNet model, features are fetched from the last layer of the 5 th conv block of the pretrained model. The features are reshaped from a <ref type="bibr" coords="2,162.88,639.46,68.58,9.88">[7 X 7 X 1024]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Vision Transformer 1 (ViT1)</head><p>The concept of ViT is an extension of the original concept of Transformers, the latter of which is described earlier in this article as text transformer <ref type="bibr" coords="3,296.65,394.16,11.68,9.88" target="#b6">[7]</ref>. It is only the application of Transformer in the image domain with slight modification in the implementation to handle the different data modality. More specifically, a ViT uses different methods for tokenization and embedding. However, the generic architecture remains the same.</p><p>A modified vision transformer architecture (ViT1) is utilized in this study where the operation of this architecture involves a patch encoder that transforms pretrained CheXNet embeddings into patch embeddings that also carry information about the position of the feature patches. The steps involved in the patch encoder are detailed below (Fig. <ref type="figure" coords="3,259.45,495.34,4.08,9.88" target="#fig_0">1</ref>):</p><p>1. Split features into feature patches. 2. Obtain higher dimensional embeddings from each feature patch. 3. Add positional embeddings to keep the feature order information.</p><p>The [49 X 1024] features obtained from the CheXNet pretrained model <ref type="bibr" coords="3,415.62,576.94,18.31,9.88" target="#b10">[11]</ref> are flattened into a 50176 dim feature representation of the image. The patch encoder splits this feature representation into [128 X 392] feature patches. The goal of this architecture was to apply the knowledge already gained from pretrained models in Vision Transformers. The final output of the patch encoder is a combination of the [128 X 392] feature projections and the [1 X 392] dim positional embeddings. The output of the patch encoder is fed into a ViT encoder which includes a multi-head attention layer, a skip connection and an intermediate dense layer that projects the visual feature representation into the specified dimension size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">Vision Transformer 2 (ViT2)</head><p>The set-up of the ViT2 encoder (Fig. <ref type="figure" coords="4,254.75,105.84,4.59,9.88" target="#fig_1">2</ref>) consists of a patch encoder that receives an input of 224 X 224 images and produces a dense projection of the patches and a positional embedding representative of each patch's locality. 16 X 16 X 3 overlapping patches are obtained from the image and are flattened into a 768-dimensional array and fed into a 512-neuron fully connected layer which results in a 512dimensional array. The patch projections and positional embeddings are fed into the encoder stack which is a layer of 8 encoders consisting of a multi-head attention layer that provides the attended representation of the features, a skip connection and an intermediate dense layer that projects the visual feature representation into the specified dimension size. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4.">Decoder</head><p>The textual decoder generates captions based on the visual representation provided by the encoder. The decoder feeds the image encodings alongside the positional encodings of the words in the generated captions into multiple attention heads that compute a scaled dot product of the query sequence to all other sequence entries. The decoder predicts auto-regressively the future track positions. The decoder query is compared against the encoder keys, value and against the previous decoder prediction.</p><p>The input of the decoder is an array consisting of the encoder output, a tokenized text sequence and the positional embeddings of the tokenized text sequence. The decoder consists of two masked multihead attention layers, feed forward neural modules and a classification head. The output of the decoder layer is a matrix [maximum sequence length -1 X vocabulary size], the output of the decoder is right shifted by ensuring the prediction for an input sequence length 1 is 29 for an example maximum sequence length 30. This enables a concatenation of the previous input with the classification at position 0 of the generated output. This concatenation can be used as the text token input for the next iteration of the prediction. This prediction loop continues until an end flag is predicted at position 0 of the generated output. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.5.">Fusion of Transformers</head><p>In the bid to utilize the varying information learned by all models, the trained models were combined in a late fusion. The fusion predictor working principle involved a sequence generation based on the most likely word from the words generated by the models. At each iteration of the sequence generation, the three models were fed with the same image and the attended embeddings or encodings were obtained as the results for each of the encoder models, the encodings and the text embeddings were passed into separate decoders to generate three possible words that could fit the current iteration of the sequence generation. The most likely word is selected from the generated words and appended to the generated sequence for the next iteration. In the prediction of the next word in the caption sequence, all models were fed with the same images and same texts. However, only the predictions with maximum probability were accepted by the fused model (Fig. <ref type="figure" coords="5,352.21,454.84,3.98,9.88" target="#fig_2">3</ref>). This results in the models working together to predict a single caption sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.6.">Training</head><p>The training set of the ImageCLEFmedical 2022 Caption task consists of 83,275 radiology images where image has associated ground truth caption and concepts. The validation and test sets comprise of 7,645, and 7,601 radiology images, respectively <ref type="bibr" coords="5,286.96,550.54,11.68,9.88" target="#b6">[7]</ref>. All models were trained with a SGD optimizer, a sparse categorical entropy loss, a learning rate set at 0.001 and batch size of 32. The encoder block of the transformer model was trained with a 512 embedding dense projection dimension and a feed forward dimension of 512. The encoder was also trained on 256 attention heads. The vision transformer_1 (ViT1) encoder was also trained with a 512 embedding dense projection set, a patch encoding dimension of 512 and 256 multi-attention heads. The vision transformer_2 (ViT2) had similar configurations as the vision transformer with a patch creator replacing the patch encoder. The decoder was trained on a vocabulary size of 20,000 while the maximum sequence length for the model was 30. All the text sequences exceeding a length of 30 were cut off at the 30th word due to the fact that most of the words relevant to the image appeared in the first 30 words of the caption sequence. The decoder was trained with 256 attention heads while the word embedding dimension was set as 512. All the models were trained for about 50 epochs and early stopping was applied to prevent overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-label Concept Classification</head><p>The concept detection task also considered as a multi-label classification problem which involves predicting zero or more concept labels (multiple mutually non-exclusive classes) to each image instance. We focused on flat (non-hierarchical) multi-label classification methods using three different CNN models: modified version of popular VGGNet <ref type="bibr" coords="6,309.75,143.78,18.48,9.88" target="#b13">[14]</ref> and GoogLeNet <ref type="bibr" coords="6,406.73,143.78,18.42,9.88" target="#b14">[15]</ref> and original AlexNet <ref type="bibr" coords="6,72.00,156.44,18.38,9.88" target="#b15">[16]</ref> architecture with following configuration:</p><p>• Number of nodes in the output layer matches the number of concept labels in the training set images. • We divided the detection task into a series of multiple binary classification problems by keeping activation function of the classification(output) layer in our model to "sigmoid".</p><p>The first CNN architecture used is a simplified version of the original VGGNet architecture <ref type="bibr" coords="6,502.16,241.52,17.01,9.88" target="#b13">[14]</ref>, which consists of two sets of CONV =&gt; RELU =&gt; CONV =&gt; RELU =&gt; POOL layers, followed by a set of FC =&gt; RELU =&gt; FC =&gt; SOFTMAX layers. The first two CONV layers learn 32 filters, each of size 3 X 3. The second two CONV layers learn 64 filters, again, each of size 3 X 3. The POOL layers perform max pooling over a 2 X 2 window with a 1 X 1 stride. We also inserted batch normalization layers after the activations along with dropout layers (DO) after the POOL and FC layers <ref type="bibr" coords="6,466.38,304.76,16.88,9.88" target="#b11">[12]</ref>. Also, a smaller version of original GoogLeNet <ref type="bibr" coords="6,306.74,330.08,18.50,9.88" target="#b14">[15]</ref> architecture is used using the Miniception module which consists of building blocks including a convolution module, Inception module, and Downsample module <ref type="bibr" coords="6,173.13,355.34,16.97,9.88" target="#b16">[17]</ref>. These modules are put together to form the overall architecture. The Mininception module performed two sets of convolutions -a 1 X 1 CONV and a 3 X 3 CONV. These two convolutions are performed in parallel, and the resulting features concatenated across the channel dimension. Next comes the downsample module, which is responsible for reducing the spatial dimensions of an input volume. The first Inception module learns 32 filters for both the 1 X 1 and 3 X 3 CONV layers. When concatenated, this module outputs a volume with K = 32 + 32 = 64 filters. The second Inception module learns 32, 1 X 1 filters and 48, 3 X 3 filters with output volume size is K = 32 + 48 = 80 after concatenation. The down sample module reduces our input volume sizes but keeps the same number of filters learned at 80. The four Inception modules are stacked on top of each other before applying a down sample, allowing GoogLeNet to learn deeper, richer features.</p><p>The original AlexNet <ref type="bibr" coords="6,179.61,494.50,18.41,9.88" target="#b15">[16]</ref> model was also used for experimentation where the first block applies 96, 11 X 11 kernels with a stride of 4 X 4, followed by a RELU activation and max pooling with a pool size of 3 X 3 and strides of 2 X 2, resulting in an output volume of size 55 X 55. A second CONV =&gt; RELU =&gt; POOL layer is then applied using 256, 5 X 5 filters with 1 X 1 strides. After applying max pooling again with a pool size of 3 X 3 and strides of 2 X 2 we are left with a 13 X 13 volume. Next, we apply (CONV =&gt; RELU) * 3 =&gt; POOL. The first two CONV layers learn 384, 3 X 3 filters while the final CONV learns 256, 3 X 3 filters. After another max pooling operation, we reach our two FC layers, each with 4096 nodes and ReLU activations in between. The final layer in the network is our "sigmoid" classifier for multi-label classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Concept Model Training</head><p>The Concept Unique Identifiers (CUI) as concept labels of each training images are extracted from a csv file and added in a list. More than 8K concept labels are found in the training set that could be predicted, and more than one can be true: hence the multi-label nature of the problem. In our implementation, all images are resized to 96 X 96 for space and computational efficiency and scaled the raw pixel intensities to the range [0, 1] and stored as NumPy arrays. A data list contains images stored as NumPy arrays and converted concept labels list (as a list of lists) to a NumPy array as well. After that, labels are binarized for multi-class classification by utilizing the scikit-learn library's MultiLabelBinarizer class, which actually transforms the CUI labels into a vector that encodes which concepts are present in the image <ref type="bibr" coords="6,248.51,754.68,16.87,9.88" target="#b11">[12]</ref>. The high volume of classification (CUI) labels (&gt;8K) and imbalance in the label frequency results in a huge bias towards the multi-label classification problem. Hence, data augmentation (scaling, rotation, flipping, etc.) is also applied while training as we have only a handful of images per concept class. All the above models are built by initializing the Adam optimizer <ref type="bibr" coords="7,116.96,112.68,18.31,9.88" target="#b12">[13]</ref> and compiled using binary cross-entropy rather than categorical cross-entropy to treat each output label as an independent Bernoulli distribution where the labels are not disjoint. After training is complete, the models and label binarizers are saved to disk (cloud storage) and loaded later during prediction in the test set.</p><p>High memory and computing systems with four NVIDIA ® T4 GPU drivers were procured from the cloud and trained in parallel using the TensorFlow 2.8 mirrored strategy. Training was also performed on the VertexAI workbench of the Google Cloud platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Run Descriptions and Results</head><p>We have submitted in total twelve <ref type="bibr" coords="7,244.04,260.91,18.31,9.98" target="#b11">(12)</ref> runs in this year's participation of the ImageCLEFmedical 2022 Caption task, where 8 runs were submitted for the Concept Detection task and the remaining 4 runs were submitted for the caption prediction task as shown in Table <ref type="table" coords="7,381.59,286.28,5.49,9.88" target="#tab_1">1</ref> and<ref type="table" coords="7,408.56,286.28,4.12,9.88" target="#tab_2">2</ref>. For transformer-based approaches, the run descriptions are very similar for both Concept Detection and Caption Prediction. For Concept Detection, first four runs in Table <ref type="table" coords="7,278.50,311.60,5.49,9.88" target="#tab_1">1</ref> are based on the multi-label classification approach as described in Section 2.2 and the other four runs are transformer based as described in Section 2.1. In Table <ref type="table" coords="7,99.77,336.92,4.12,9.88" target="#tab_1">1</ref>, a Secondary F1 score is also included in addition to the F1 score, which was calculated using a subset of manually validated concepts (anatomy and image modality) only. In Table <ref type="table" coords="7,467.23,349.52,4.12,9.88" target="#tab_2">2</ref>, additional metrics, such as ROUGE, METEOR, CIDEr, and BERTScore are included in addition to the BLEU score used for ranking.</p><p>As it is observed in Table <ref type="table" coords="7,202.42,400.16,5.49,9.88" target="#tab_1">1</ref> and 2, both the best accuracies for Concept Detection (F1 score: 0.3519) and Caption Prediction (BLEU Score: 0.2549) are achieved while fusing the outputs of both Text and Visual Transformers. In addition, we achieved a comparable accuracy (F1 score: 0.3165) for Concept Detection using multi-label classification scheme while using a smaller version of original VGGNet architecture and considering all the concept output probabilities with a threshold of 2% only. The best scores are in boldface for each metric in Table <ref type="table" coords="7,280.06,463.42,5.49,9.88" target="#tab_1">1</ref> and<ref type="table" coords="7,307.24,463.42,32.03,9.88" target="#tab_2">Table 2</ref>. Overall, we ranked 9th out of 11 groups for the Concept Detection task and ranked 8th out of 10 groups for the Caption Prediction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Description of the Runs</head><p>The submitted runs for both the Concept Detection and Caption Prediction tasks are described as follows:</p><p>1. minigooglenet_prob2_100: This run utilized a multi-label classification model by using scikit-learn MultiLabelBinarizer class with a smaller version of the original GoogLeNet architecture using the Miniception module as described in Section 2.2. For training the following parameters are used: EPOCHS = 25, initial learning rate (INIT_LR) = 1e-3, batch size (BS) = 32 and IMAGE_DIMS = (96, 96, 3). For this run, we only considered concept labels with a probability threshold of 2% for each test image.</p><p>2. alexnet_prob2_100: This run utilized a multi-label classification model by using scikitlearn MultiLabelBinarizer class with the original version of the AlexNet architecture. We considered the following hyper parameters for training: EPOCHS = 100, INIT_LR = 1e-3, BS = 32, IMAGE_DIMS = (96, 96, 3), For this run, we only consider concept labels with a probability threshold of 2% for each test image.</p><p>3. vggnet_top20: This run utilized a multi-label classification model by using scikit-learn MultiLabelBinarizer class with a mini version of the original VGGNet architecture as described in Section 2.2. To initialize the number of epochs to train for, initial learning rate, batch size, and image dimensions, the following hyper parameters are used: EPOCHS = 200, INIT_LR = 1e-3 (the default value for the Adam optimizer), BS = 32 and IMAGE_DIMS = (96, 96, 3). For this run, we only consider the top 20 concept labels based on probability scores for each test image.</p><p>4. vggnet_prob2_100: This run is almost similar to the previous run. Here, we only considered concept labels with a probability threshold of 2% for each test image.</p><p>5. vit_1_20000_concepts: This run involved the concept sequence generation using the model obtained from training the ViT1 architecture in the training dataset. The model was trained for 50 epochs with early stopping applied based on the validation loss obtained from the validation training dataset.</p><p>6. vit_2_20000_concepts: Utilizing the ViT2 architecture, this run involved training a model on the imageCLEF'2022 training dataset for 50 epochs. The model training was stopped early at the 44th epoch after a patience of 10 was applied over the validation loss obtained from the validation dataset. The model also learned a vocabulary of 8000 CUI and a maximum length of the sequence was 30 CUI. The text training input was cut off after 28 words and flags "start" and "end" were appended to the text sequence.</p><p>7. t4mr_20000_concepts: In this run, encoder was used to transform visual features obtained from a pretrained CheXNet model as described earlier. The architecture was trained for 50 epochs and was stopped early after the 30 th epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">fusion_concepts:</head><p>This run involved the fusion model that combines the ViT1, ViT2 and Transformer models by obtaining the probabilities of the next CUI predictions and choosing the concept with the highest probability to generate the concept sequence. This run utilized an image dimension equal to (224,224,3) and initial seed text "start" fed into the model runs for ViT1, ViT2 and Transformer architecture. 11. t4mr_20000_caption: This run involved training the Transformer architecture described earlier where visual features were obtained from a pretrained CheXNet model. The architecture was trained for 50 epochs and was stopped early after the 32 nd epoch. In all these runs for Caption Prediction, the models learned a vocabulary of 20,000 words and a maximum length of the sequence was 30 words. The text training input was cut off after 28 words and flags "start" and "end" were appended to the text sequence.</p><p>12. fusion_caption: This run involved the fusion model that combines the ViT1, ViT2 and Transformer models by obtaining the probabilities of the next word predictions and choosing the word with the highest probability to generate the caption sequence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>This article describes the strategies and results based on the participation of the Morgan_CS group in the ImageCLEFmedical 2022 Caption task. We submitted runs for both Concept Detection and Caption Prediction tasks under this benchmark evaluation. Our best results were achieved while using transformer-based approaches, especially when using fusion of different Transformers. Hence, in future we plan explore more domain specific Transformers based approaches with better fusion mechanism, which has already witnessed growing interest in medical field to capture global context compared to CNNs with local receptive fields.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,72.00,322.45,292.88,10.98;3,146.73,72.00,301.80,236.20"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Encoder block for the vision transformer model 1 (ViT1).</figDesc><graphic coords="3,146.73,72.00,301.80,236.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,72.00,434.85,292.88,10.98;4,188.07,204.28,237.13,228.25"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Encoder block for the vision transformer model 2 (ViT2).</figDesc><graphic coords="4,188.07,204.28,237.13,228.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,72.00,269.35,204.90,10.98;5,146.90,72.00,300.82,195.05"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Process diagram of the fusion model</figDesc><graphic coords="5,146.90,72.00,300.82,195.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,83.36,365.61,411.92,9.99;8,72.00,378.21,423.17,9.98;8,72.00,390.87,422.93,9.98;8,72.00,403.53,157.23,9.98;8,83.36,416.21,411.81,9.99;8,72.00,428.87,422.96,9.98;8,72.00,441.47,422.94,9.98;8,72.00,454.13,79.91,9.98"><head>9 .</head><label>9</label><figDesc>vit_1_20000_caption: This run involved the sequence generation of caption using the model obtained from training the ViT1 architecture in the training dataset. The model was trained for 50 epochs with early stopping applied at the 32nd epoch based on the validation loss obtained from the validation training dataset. 10. vit_2_20000_caption: Utilizing the ViT2 architecture, this run involved training a model on the imageCLEF'2022 training dataset for 50 epochs. The model training was stopped early at the 44th epoch after a patience of 10 was applied over the validation loss obtained from the validation dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,72.00,593.67,418.41,166.76"><head>Table 1</head><label>1</label><figDesc>Results for the Runs submitted to the Concept Detection task Submission Run</figDesc><table coords="8,85.10,622.05,405.31,138.38"><row><cell></cell><cell>Run Name</cell><cell>F1 Score</cell><cell>Secondary F1</cell></row><row><cell>181984</cell><cell>minigooglenet_prob2_100</cell><cell>0.1757</cell><cell>0.1426</cell></row><row><cell>181959</cell><cell>alexnet_prob2_100</cell><cell>0.2323</cell><cell>0.3046</cell></row><row><cell>181952</cell><cell>vggnet_top20</cell><cell>0.1648</cell><cell>0.1214</cell></row><row><cell>181957</cell><cell>vggnet_prob2_100</cell><cell>0.3165</cell><cell>0.3757</cell></row><row><cell>182091</cell><cell>vit_1_20000_concepts</cell><cell>0.3340</cell><cell>0.6064</cell></row><row><cell>182111</cell><cell>vit_2_20000_concepts</cell><cell>0.3307</cell><cell>0.3307</cell></row><row><cell>181964</cell><cell>t4mr_20000_concepts</cell><cell>0.3257</cell><cell>0.5848</cell></row><row><cell>182150</cell><cell>fusion_concepts</cell><cell>0.3519</cell><cell>0.6280</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,72.00,87.71,442.00,108.74"><head>Table 2</head><label>2</label><figDesc>Results for the Runs submitted to the Caption Prediction task SubmissionRun</figDesc><table coords="9,88.28,116.81,425.72,79.64"><row><cell></cell><cell>Run Name</cell><cell>BLEU</cell><cell>ROUGE</cell><cell>METEOR</cell><cell>CIDEr</cell><cell>BERTScore</cell></row><row><cell>182092</cell><cell cols="2">vit_1_20000_caption 0.2501</cell><cell>0.1411</cell><cell>0.0549</cell><cell>0.1382</cell><cell>0.5814</cell></row><row><cell>182115</cell><cell cols="2">vit_2_20000_caption 0.2404</cell><cell>0.1338</cell><cell>0.0516</cell><cell>0.1299</cell><cell>0.5745</cell></row><row><cell>181962</cell><cell cols="2">t4mr_20000_caption 0.2459</cell><cell>0.2459</cell><cell>0.0489</cell><cell>0.1143</cell><cell>0.5721</cell></row><row><cell>182238</cell><cell>fusion_caption</cell><cell>0.2549</cell><cell>0.1440</cell><cell>0.0559</cell><cell>0.1481</cell><cell>0.5834</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work is supported by an <rs type="funder">NSF</rs> <rs type="grantName">Grant</rs> (Award <rs type="grantNumber">ID. 2131207</rs>), entitled "<rs type="projectName">CISE-MSI: DP: IIS: III: Deep Learning Based Automated Concept and Caption Generation of Medical Images Towards Developing an Effective Decision Support System (DSS)</rs>" under the <rs type="programName">CISE MSI Research Expansion program</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_mxSqUXa">
					<idno type="grant-number">ID. 2131207</idno>
					<orgName type="grant-name">Grant</orgName>
					<orgName type="project" subtype="full">CISE-MSI: DP: IIS: III: Deep Learning Based Automated Concept and Caption Generation of Medical Images Towards Developing an Effective Decision Support System (DSS)</orgName>
					<orgName type="program" subtype="full">CISE MSI Research Expansion program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,93.32,504.10,429.73,9.88;9,93.32,516.76,429.66,9.88;9,93.32,529.42,429.97,9.88;9,93.32,542.08,137.21,9.88" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,135.32,516.76,348.53,9.88">Automatic medical image interpretation: State of the art and future directions</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ayesha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tariq</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Abrar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanaullah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rehman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F K</forename><surname>Niazi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hussain</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2021.107856</idno>
		<ptr target="https://doi.org/10.1016/j.patcog.2021.107856.doi:10.1016/j.patcog.2021.107856" />
	</analytic>
	<monogr>
		<title level="j" coord="9,491.87,516.76,31.11,9.88;9,93.32,529.42,51.85,9.88">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page">107856</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.32,554.74,429.77,9.88;9,93.32,567.34,430.09,9.88;9,93.32,580.00,430.05,9.88;9,93.32,592.66,285.71,9.88" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,309.93,554.74,193.24,9.88">A Survey on Biomedical Image Captioning</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-1803</idno>
		<ptr target="https://aclanthology.org/W19-1803.doi:10.18653/v1/W19-1803" />
	</analytic>
	<monogr>
		<title level="m" coord="9,93.32,567.34,371.02,9.88">Proceedings of the Second Workshop on Shortcomings in Vision and Language</title>
		<meeting>the Second Workshop on Shortcomings in Vision and Language<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="26" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.32,605.32,429.90,9.88;9,93.32,617.98,40.28,9.88;9,162.35,617.98,51.89,9.88;9,242.96,617.98,8.54,9.88;9,280.24,617.98,44.24,9.88;9,353.24,617.98,55.32,9.88;9,437.46,617.98,32.14,9.88;9,498.30,617.98,25.01,9.88;9,93.32,630.64,364.59,9.88" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,268.84,605.32,249.81,9.88">Deep learning in generating radiology reports: A survey</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M A</forename><surname>Monshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chung</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artmed.2020.101878</idno>
		<ptr target="https://doi.org/10.1016/j.artmed.2020.101878.doi:10.1016/j.artmed.2020.101878" />
	</analytic>
	<monogr>
		<title level="j" coord="9,93.32,617.98,40.28,9.88;9,162.35,617.98,51.89,9.88;9,242.96,617.98,8.54,9.88;9,280.24,617.98,39.32,9.88">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page">101878</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.32,643.24,429.87,9.88;9,93.32,655.90,430.04,9.88;9,93.32,668.56,100.56,9.88" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,329.52,643.24,193.67,9.88;9,93.32,655.90,188.74,9.88">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,293.68,655.90,62.81,9.88">NAACL-HLT</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.32,681.22,429.93,9.88;9,93.32,693.90,36.00,9.88;9,158.03,693.90,39.68,9.88;9,226.46,693.90,7.93,9.88;9,263.08,693.90,33.91,9.88;9,325.69,693.90,83.07,9.88;9,437.52,693.90,32.14,9.88;9,498.30,693.90,25.01,9.88;9,93.32,706.50,192.19,9.88" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Shamshad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2201.09873</idno>
		<idno type="arXiv">arXiv:2201.09873</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2201.09873" />
		<title level="m" coord="9,452.33,681.22,70.92,9.88;9,93.32,693.90,36.00,9.88;9,158.03,693.90,39.68,9.88;9,226.46,693.90,7.93,9.88;9,263.08,693.90,29.06,9.88">Transformers in Medical Imaging: A Survey</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.32,719.05,430.03,9.98;9,93.32,731.82,429.93,9.88;9,93.32,744.48,404.79,9.88" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,93.32,731.82,107.41,9.88">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1706.03762</idno>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1706.03762" />
	</analytic>
	<monogr>
		<title level="m" coord="9,223.13,731.82,295.43,9.88">Proceedings of Advances in neural information processing systems</title>
		<meeting>Advances in neural information processing systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.32,74.76,429.83,9.88;10,93.32,87.42,429.92,9.88;10,93.32,100.02,340.70,9.88;10,453.66,100.02,69.77,9.88;10,93.32,112.68,351.22,9.88" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,371.74,87.42,151.50,9.88;10,93.32,100.02,232.32,9.88">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2010.11929</idno>
		<idno type="arXiv">arXiv:2010.11929v2[cs.CV</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2010.11929" />
	</analytic>
	<monogr>
		<title level="m" coord="10,361.33,100.02,72.69,9.88;10,453.66,100.02,22.22,9.88">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.32,125.36,430.04,9.88;10,93.32,137.91,430.03,9.99;10,93.32,150.68,430.08,9.88;10,93.32,163.34,397.53,9.88" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,235.28,137.91,288.07,9.99;10,93.32,150.68,80.45,9.88">Overview of ImageCLEFmedical 2022 -Caption Prediction and Concept Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,192.60,150.68,312.16,9.88;10,93.32,163.34,220.61,9.88">Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">September 5-8, 2022</date>
		</imprint>
	</monogr>
	<note>Proceedings of CEUR Workshop (CEUR-WS.org</note>
</biblStruct>

<biblStruct coords="10,93.32,175.94,429.81,9.88;10,93.32,188.49,429.89,9.99;10,93.32,201.26,429.91,9.88;10,93.32,213.92,429.93,9.88;10,93.32,226.58,429.82,9.88;10,93.32,239.18,429.99,9.88;10,93.32,251.84,430.06,9.88;10,93.32,264.50,35.75,9.88" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,204.62,213.92,318.63,9.88;10,93.32,226.58,165.25,9.88">Overview of the ImageCLEF 2022: Multimedia Retrieval in Medical, social media and Nature Applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Ștefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,277.96,226.58,245.18,9.88;10,93.32,239.18,66.03,9.88;10,183.23,239.18,340.08,9.88;10,93.32,251.84,57.30,9.88">Proceedings of the 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<title level="s" coord="10,158.49,251.84,227.14,9.88">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting>the 13th International Conference of the CLEF Association (CLEF 2022)<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">September 5-8, 2022</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="10,93.32,277.16,429.91,9.88;10,93.32,289.82,430.00,9.88;10,93.32,302.42,430.01,9.88;10,93.32,315.08,429.87,9.88;10,93.32,327.74,430.06,9.88;10,93.32,340.40,24.77,9.88" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,347.84,277.16,175.38,9.88;10,93.32,289.82,430.00,9.88;10,93.32,302.42,294.56,9.88">Radiology Objects in COntext (ROCO): A Multimodal Image Dataset, Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01364-6_20</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,417.90,302.42,105.44,9.88;10,93.32,315.08,379.96,9.88;10,121.31,327.74,179.52,9.88">Proceedings of the 7th Joint International Workshop, CVII-STENT (2018) and Third International Workshop</title>
		<meeting>the 7th Joint International Workshop, CVII-STENT (2018) and Third International Workshop<address><addrLine>LABELS; Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Held in Conjunction with MICCAI 2018</note>
</biblStruct>

<biblStruct coords="10,93.32,353.06,429.64,9.88;10,93.32,365.72,429.92,9.88;10,93.32,378.32,25.01,9.88;10,138.85,378.32,33.61,9.88;10,192.97,378.32,19.52,9.88;10,233.03,378.32,23.13,9.88;10,276.70,378.32,42.43,9.88;10,339.70,378.32,80.29,9.88;10,440.46,378.32,37.20,9.88;10,498.22,378.32,25.01,9.88;10,93.32,390.98,189.22,9.88" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bagul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shpanskaya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1711.05225</idno>
		<idno type="arXiv">arXiv:1711.05225</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1711.05225" />
		<title level="m" coord="10,278.44,365.72,244.80,9.88;10,93.32,378.32,25.01,9.88;10,138.85,378.32,33.61,9.88;10,192.97,378.32,19.52,9.88;10,233.03,378.32,23.13,9.88;10,276.70,378.32,37.71,9.88">CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.32,403.64,430.00,9.88;10,93.32,416.32,111.12,9.88" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,93.32,403.64,164.53,9.88">Multi-label classification with Keras</title>
		<ptr target="https://pyimagesearch.com/2018/05/07/multi-label-classification-with-keras/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.32,428.98,429.99,9.88;10,93.32,441.58,64.69,9.88" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="10,243.64,428.98,173.99,9.88">A Method for Stochastic Optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.32,454.24,430.10,9.88;10,93.32,466.90,139.91,9.88" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,221.18,454.24,297.83,9.88">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arXiv 1409.1556</idno>
	</analytic>
	<monogr>
		<title level="j" coord="10,93.32,466.90,25.02,9.88">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.32,479.56,429.65,9.88;10,93.32,492.22,430.02,9.88;10,93.32,504.88,429.98,9.88;10,93.32,517.48,183.83,9.88" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,152.90,492.22,149.57,9.88">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1409.4842</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1409.4842" />
	</analytic>
	<monogr>
		<title level="m" coord="10,330.97,492.22,192.37,9.88;10,93.32,504.88,278.91,9.88">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.32,530.14,429.95,9.88;10,93.32,542.80,120.95,9.88;10,233.81,542.80,9.15,9.88;10,262.52,542.80,13.44,9.88;10,295.49,542.80,50.30,9.88;10,365.38,542.80,32.02,9.88;10,417.00,542.80,13.77,9.88;10,450.30,542.80,28.49,9.88;10,498.36,542.80,24.95,9.88;10,93.32,555.46,142.32,9.88" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,283.72,530.14,239.55,9.88;10,93.32,542.80,38.28,9.88">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1145/3065386</idno>
		<ptr target="https://doi.org/10.1145/3065386" />
	</analytic>
	<monogr>
		<title level="j" coord="10,139.16,542.80,75.11,9.88;10,233.81,542.80,9.15,9.88;10,262.52,542.80,13.44,9.88;10,295.49,542.80,20.82,9.88">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.32,568.12,429.92,9.88;10,93.32,580.72,430.10,9.88;10,93.32,593.38,170.09,9.88" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,357.62,568.12,165.62,9.88;10,93.32,580.72,159.71,9.88">Understanding Deep Learning (Still) Requires Rethinking Generalization</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chiyuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Samy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Oriol</surname></persName>
		</author>
		<idno type="DOI">10.1145/3446776</idno>
		<ptr target="https://doi.org/10.1145/3446776" />
	</analytic>
	<monogr>
		<title level="j" coord="10,261.98,580.72,131.91,9.88">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="107" to="115" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
