<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,85.46,75.44,424.43,17.04;1,178.85,96.20,237.79,17.04">ImageCLEFmedical Caption Task, Concept Detection, Finding Duplicates, SDVA-UCSD Approach</title>
				<funder ref="#_k85tKGG">
					<orgName type="full">Office of the Assistant Secretary of Defense for Health Affairs</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,72.02,129.10,80.92,10.80"><forename type="first">Amilcare</forename><surname>Gentili</surname></persName>
							<email>agentili@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">San Diego VA HCS</orgName>
								<address>
									<addrLine>3350 La Jolla Village Drive</addrLine>
									<postCode>92161</postCode>
									<settlement>San Diego</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">UCSD</orgName>
								<address>
									<addrLine>9300 Campus Point Drive #7756, La Jolla</addrLine>
									<postCode>92037-7756</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,85.46,75.44,424.43,17.04;1,178.85,96.20,237.79,17.04">ImageCLEFmedical Caption Task, Concept Detection, Finding Duplicates, SDVA-UCSD Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EBCB9F177AB3F26BB345B773888C7A81</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional neural network</term>
					<term>hashing</term>
					<term>caption detection 2</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ImageCLEFmedical Caption task, concept detection goal, was to detect relevant concepts in a large corpus of medical images. For this task, a subset of the extended Radiology Objects in COntext (ROCO) dataset was provided. We utilized 2 approaches to classify images: hashing similarities and convolutional neural networks. Hashing similarity was able to detect duplicate images in the dataset, but did poorly in classifying images. Neural networks did better in classifying images, but our model had problems with low frequency concepts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>ImageCLEF <ref type="bibr" coords="1,143.18,358.27,12.78,9.94" target="#b0">[1]</ref> is part of the Conference and Labs of the Evaluation Forum (CLEF) since 2003 and ImageCLEF has included medical tasks every year since 2004. Since 2017 it has a Caption Task <ref type="bibr" coords="1,507.94,370.87,11.70,9.94" target="#b1">[2]</ref>. Interpreting and classifying medical images such as radiology images is a time-consuming task that involves highly trained experts. Manual labeling images to input in machine learning pipeline is often a slow and expensive process. There is a considerable need for methods that can automatically label medical images. This year concept detection task was to detect relevant concepts in a large corpus of medical image. The images were labeled with concepts generated using a reduced subset of the UMLS 2020 AB release.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hashing</head><p>The ImageHash hashing library <ref type="bibr" coords="2,228.29,94.72,12.80,9.94" target="#b3">[4]</ref> written in Python was used for hashing images, of the available hashing algorithms provided by the ImageHash library, the following 4 were utilized:</p><p>• average hashing (aHash)</p><p>• perception hashing (pHash)</p><p>• difference hashing (dHash)</p><p>• wavelet hashing (wHash) Results of each hashing algorithm were combined and similarities among all image pairs were calculated <ref type="bibr" coords="2,119.42,186.40,11.70,9.94" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Neural Network</head><p>Fastai library <ref type="bibr" coords="2,146.54,237.07,12.78,9.94" target="#b5">[6]</ref> was used for multilabel classification. Each model was trained for 15 epochs, using an image size of 384, batch size of 16, gradient accumulation of four, and learning rate in the range of 0.1 and 0.001. Pretrained Resnet <ref type="bibr" coords="2,217.49,262.39,12.80,9.94" target="#b6">[7]</ref> and Densenet <ref type="bibr" coords="2,294.89,262.39,12.78,9.94" target="#b7">[8]</ref> architectures were tested and based on results on the validation data set, best models were chosen and ensembled for final submission. The outputs of the best models were the average of the probabilities that each concept was a label for the image. The label was considered present if the average probability was greater than 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Exploratory Data Analysis</head><p>8374 different concepts were used to label the 83,275 images in the training dataset. The most common concept, CUIS CU0040405 was used 25989 times, while the list commonly used concept CUIS C0004760 was used only twice. The top 25 concept account for one third of the labels Figure <ref type="figure" coords="2,512.94,409.29,4.16,9.94">1</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cumulative Frequency of Concepts</head><p>The number of concepts assigned to each image varied from 1 to 50, with most images being assigned 3 concepts. </p><note type="other">Figure 2</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hashing</head><p>Using hashing similarities <ref type="bibr" coords="3,205.36,523.65,12.80,9.94" target="#b4">[5]</ref> greater than 96%, 275 duplicate images were present in the training dataset, 16 duplicate images were present in the validation dataset. Of the 191 images duplicated in the train or validation dataset, 222 had different concepts or captions assigned. 196 images of the test set were present in either the train or validation set. We made several attempted to use hashing similarities to label the validation images. We assign concepts to the validation images using variable thresholds of similarities between 0.5 and 0.9 and using concepts in common in the images above the similarity threshold. We also used concept from the 1, 3 or 5 most similar images in the training set, when 3 or 5 most similar images were used, we used 2 strategies to combine the results, we assigned the concept to validation images either if it was present in any image or if it was present in the majority of the images. Despite these multiple strategies, the F1 on the validation set were all below 0.16 and these attempts were not submitted to the competition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion/Conclusion</head><p>Using hashing similarities, we were able to find duplicate images in the dataset but did not help in assign correct concepts to not identical images based on hash similarities. The dataset was very unbalanced, with top 25 concepts accounting for half of the labels. As we did not use any correction for the unbalanced data, our neural networks only learned how to recognize the most common concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Perspectives for Future Work</head><p>Hashing similarities can be used to recognize duplicate images, but not images with similar concepts. Neural networks, without correction for imbalanced data, did well in classifying common concepts but did poorly on rare concepts. Strategies to correct for imbalanced data, such as enriching dataset with images with rare concepts or weighting more images with rare concepts during training will be necessary to improve concept detection. Other options include using a different neural network architecture, such as Siamese neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,279.17,434.61,37.06,9.94"><head>Figure 1 Figure 1 .</head><label>11</label><figDesc>Figure 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,72.02,445.63,451.17,11.04;3,72.02,459.07,47.43,11.04"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Histogram of number of concepts assigned to each image in the validation and training set combined.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="3,72.02,681.14,12.52,8.96;3,100.34,681.14,69.08,8.96;3,86.18,700.80,436.96,9.94;3,72.02,713.40,451.44,9.94;3,72.02,726.12,451.22,9.94;3,72.02,738.74,451.37,9.94;3,72.02,751.46,451.51,9.94;3,163.63,381.93,4.56,9.00;3,149.95,358.17,18.24,9.00;3,149.95,334.39,18.26,9.02;3,149.95,310.67,18.24,9.00;3,149.95,286.91,18.24,9.00;3,145.39,263.15,22.80,9.00;3,145.39,239.39,22.80,9.00;3,145.39,215.66,22.80,9.00;3,145.39,191.90,22.80,9.00;3,145.39,168.14,22.80,9.00;3,178.18,393.64,289.20,9.00;3,132.19,283.40,9.96,33.33;3,132.19,272.83,9.96,8.31;3,132.19,241.84,9.96,28.77;3,257.88,409.07,127.32,9.96;3,254.02,145.57,87.27,14.04;4,72.02,74.68,451.50,9.94;4,72.02,87.28,247.25,9.94"><head></head><label></label><figDesc>Resnet 34, Resnet 50, Resnet 101, Resnet 152, Resnet 201, Densenet 121, Densenet 161, Densenet 169 were used, based on validation results, predictions from Resnet 101, Densenet 161 and Densenet 169 were ensembled. For the final submission, the concepts of the validation or training dataset with a similarity greater than 0.96 were used as labels of the test images, instead of the concepts derived by the ensemble of the neural networks. Our single submission achieved F1 Score of 0.307932 and Secondary F1 score of 0.552432. Review of the submission classification demonstrated that only top 10 concepts were recognized by our neural network.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The author was supported in part by the <rs type="funder">Office of the Assistant Secretary of Defense for Health Affairs</rs> through the <rs type="programName">Accelerating Innovation in Military Medicine Program</rs> under Award No. <rs type="grantNumber">W81XWH-20-1-0693</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_k85tKGG">
					<idno type="grant-number">W81XWH-20-1-0693</idno>
					<orgName type="program" subtype="full">Accelerating Innovation in Military Medicine Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="4,93.38,444.45,429.98,9.94;4,93.38,457.17,430.35,9.94;4,93.38,469.77,430.08,9.94;4,93.38,482.26,430.16,10.05;4,93.38,495.09,429.93,9.94;4,93.38,507.69,429.91,9.94;4,93.38,520.41,429.80,9.94;4,93.38,533.01,429.73,9.94;4,93.38,545.61,196.82,9.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,396.02,495.09,127.29,9.94;4,93.38,507.69,339.29,9.94">Overview of the ImageCLEF 2022: MultimediaRetrieval in Medical, Social Media and Nature Applications</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renaud</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louise</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raphael</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmad</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Serge</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liviu-Daniel</forename><surname>Ștefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Gabriel Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jérôme</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jon</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,450.56,507.69,72.73,9.94;4,93.38,520.41,429.80,9.94;4,93.38,533.01,197.30,9.94">Proceedings of the 13th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="4,324.83,533.01,198.29,9.94;4,93.38,545.61,24.69,9.94">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting>the 13th International Conference of the CLEF Association (CLEF<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-09-05">2022. September 5-8, 2022</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="4,93.38,558.33,429.94,9.94;4,93.38,570.96,429.89,9.94;4,93.38,583.57,429.99,10.05;4,93.38,596.28,429.97,9.94;4,93.38,608.88,299.44,9.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,93.38,583.57,409.60,10.05">Overview of ImageCLEFmedical 2022 -Caption Prediction and Concept Detection</title>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louise</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raphael</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmad</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,93.38,596.28,429.97,9.94;4,93.38,608.88,122.58,9.94">Experimental IR Meets Multilinguality, Multimodality, and Interaction. CEUR Workshop Proceedings (CEUR-WS.org</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">September 5-8, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,93.38,621.49,430.00,10.05;4,93.38,634.09,430.13,10.05;4,93.38,646.92,429.79,9.94;4,93.38,659.52,430.11,9.94;4,93.38,672.12,295.20,9.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,386.38,621.49,136.99,10.05;4,93.38,634.09,173.32,10.04">Radiology Objects in COntext (ROCO): A Multimodal Image Dataset</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01364-6_20</idno>
	</analytic>
	<monogr>
		<title level="m" coord="4,279.10,634.09,244.42,10.05;4,93.38,646.92,381.75,9.94">Proceedings of the MICCAI Workshop on Large-scale Annotation of Biomedical data and Expert Label Synthesis (MICCAI LABELS 2018)</title>
		<title level="s" coord="4,215.29,659.52,186.22,9.94">Lecture Notes in Computer Science (LNCS</title>
		<meeting>the MICCAI Workshop on Large-scale Annotation of Biomedical data and Expert Label Synthesis (MICCAI LABELS 2018)<address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2018-09-16">September 16, 2018. 2018</date>
			<biblScope unit="volume">11043</biblScope>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,93.38,684.84,410.84,9.94" xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Buchner</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Imagehash</surname></persName>
		</author>
		<ptr target="https://github.com/JohannesBuchner/imagehash" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,93.38,697.44,35.28,9.94;4,145.82,697.44,20.84,9.94;4,183.72,697.44,17.86,9.94;4,218.61,697.44,14.11,9.94;4,249.91,697.44,40.29,9.94;4,307.24,697.44,31.15,9.94;4,355.57,697.44,19.55,9.94;4,392.14,697.44,50.16,9.94;4,459.32,697.44,22.08,9.94;4,498.58,697.44,24.91,9.94;4,93.38,710.16,425.24,9.94" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="4,145.82,697.44,20.84,9.94;4,183.72,697.44,17.86,9.94;4,218.61,697.44,14.11,9.94;4,249.91,697.44,40.29,9.94;4,307.24,697.44,31.15,9.94;4,355.57,697.44,19.55,9.94;4,392.14,697.44,45.14,9.94">Let&apos;s find out duplicate images with imagehash</title>
		<ptr target="https://www.kaggle.com/code/appian/let-s-find-out-duplicate-images-with-imagehash/notebook" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Appian</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,93.38,722.76,430.16,9.94;4,93.38,735.38,146.43,9.94" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,256.61,722.76,198.00,9.94">fastai: A Layered API for Deep Learning</title>
		<author>
			<persName coords=""><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sylvain</forename><surname>Gugger</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2002.04688" />
	</analytic>
	<monogr>
		<title level="j" coord="4,465.34,722.76,27.52,9.94">CoRR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,93.38,74.68,430.00,9.94;5,93.38,87.28,334.76,9.94" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="5,359.83,74.68,163.56,9.94;5,93.38,87.28,51.92,9.94">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1512.03385</idno>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1512.03385" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,93.38,100.00,430.11,9.94;5,93.38,112.60,392.24,9.94" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="5,436.30,100.00,87.19,9.94;5,93.38,112.60,106.31,9.94">Densely Connected Convolutional Networks</title>
		<author>
			<persName coords=""><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1608.06993</idno>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1608.06993" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
