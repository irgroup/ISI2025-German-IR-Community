<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,374.35,15.42;1,89.29,106.66,265.51,15.42">Overview of ImageCLEFmedical 2022 -Caption Prediction and Concept Detection</title>
				<funder ref="#_pbCbq22">
					<orgName type="full">Research England</orgName>
				</funder>
				<funder>
					<orgName type="full">University of Essex GCRF QR Engagement Fund</orgName>
				</funder>
				<funder ref="#_9kMvqdY">
					<orgName type="full">DFG</orgName>
				</funder>
				<funder>
					<orgName type="full">University of Applied Sciences and Arts Dortmund (FH Dortmund), Germany</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.10,134.97,85.90,11.96"><forename type="first">Johannes</forename><surname>Rückert</surname></persName>
							<email>johannes.rueckert@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,187.64,134.97,89.25,11.96"><forename type="first">Asma</forename><forename type="middle">Ben</forename><surname>Abacha</surname></persName>
							<email>abenabacha@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>Washington</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,289.53,134.97,117.48,11.96"><forename type="first">Alba</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Essex</orgName>
								<address>
									<addrLine>Wivenhoe Park</addrLine>
									<postCode>CO4 3SQ</postCode>
									<settlement>Colchester</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,419.66,134.97,62.76,11.96"><forename type="first">Louise</forename><surname>Bloch</surname></persName>
							<email>louise.bloch@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute for Medical Informatics, Biometry and Epidemiology (IMIBE)</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,148.92,79.36,11.96"><forename type="first">Raphael</forename><surname>Brüngel</surname></persName>
							<email>raphael.bruengel@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute for Medical Informatics, Biometry and Epidemiology (IMIBE)</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,186.54,148.92,101.76,11.96"><forename type="first">Ahmad</forename><surname>Idrissi-Yaghir</surname></persName>
							<email>ahmad.idrissi-yaghir@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute for Medical Informatics, Biometry and Epidemiology (IMIBE)</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,306.18,148.92,80.51,11.96"><forename type="first">Henning</forename><surname>Schäfer</surname></persName>
							<email>henning.schaefer@uk-essen.de</email>
							<affiliation key="aff4">
								<orgName type="department">Institute for Transfusion Medicine</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<settlement>Essen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,399.27,148.92,76.76,11.96"><forename type="first">Henning</forename><surname>Müller</surname></persName>
							<email>henning.mueller@hevs.ch</email>
							<affiliation key="aff5">
								<orgName type="institution">University of Applied Sciences Western Switzerland (HES-SO)</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff14">
								<orgName type="laboratory">IUST_NLPLAB</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,162.87,111.78,11.96"><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
							<email>christoph.friedrich@fh-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Applied Sciences and Arts Dortmund</orgName>
								<address>
									<settlement>Dortmund</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute for Medical Informatics, Biometry and Epidemiology (IMIBE)</orgName>
								<orgName type="institution">University Hospital Essen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">CSIRO</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="department">Australian e-Health Research Centre</orgName>
								<orgName type="laboratory">CSIRO Data61</orgName>
								<orgName type="institution">Commonwealth Scientific and Industrial Research Organisation</orgName>
								<address>
									<settlement>Herston</settlement>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="department">Imaging and Computer Vision Group</orgName>
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<settlement>Pullenvale, Brisbane</settlement>
									<region>Queensland, Queensland</region>
									<country>Australia, Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff11">
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
								<address>
									<settlement>Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff12">
								<orgName type="institution">CMRE-UoG (fdallaserra</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff13">
								<orgName type="department">Canon Medical Research Europe</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<settlement>Edinburgh, Glasgow</settlement>
									<country>UK, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff15">
								<orgName type="department">School of Computer Engineering</orgName>
								<orgName type="institution">Iran University of Science and Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country>Islamic Republic Of Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff16">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="laboratory">KDE Laboratory</orgName>
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<settlement>Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,374.35,15.42;1,89.29,106.66,265.51,15.42">Overview of ImageCLEFmedical 2022 -Caption Prediction and Concept Detection</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">5E1962D81D3CFAABEF7B4A7A79BC47FF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Concept Detection</term>
					<term>Computer Vision</term>
					<term>ImageCLEF 2022</term>
					<term>Image Understanding</term>
					<term>Image Modality</term>
					<term>Radiology</term>
					<term>Caption Prediction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The 2022 ImageCLEFmedical caption prediction and concept detection tasks follow similar challenges that were already run from 2017-2021. The objective is to extract Unified Medical Language System (UMLS) concept annotations and/or captions from the image data that are then compared against the original text captions of the images. The images used for both tasks are a subset of the extended Radiology Objects in COntext (ROCO) data set which was used in ImageCLEFmedical 2020. In the caption prediction task, lexical similarity with the original image captions is evaluated with the BiLingual Evaluation Understudy (BLEU) score. In the concept detection task, UMLS terms are extracted from the original text captions, combined with manually curated concepts for image modality and anatomy, and compared against the predicted concepts in a multi-label way. The F1-score was used to assess the performance. The task attracted a strong participation with 20 registered teams. In the end, 12 teams submitted 157 graded runs for the two subtasks. Results show that there is a variety of techniques that can lead to good prediction results for the two tasks. Participants used image retrieval systems for both tasks, while multi-label classification systems were used mainly for the concept detection, and Transformer-based architectures primarily for the caption prediction subtask.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The caption task was first proposed as part of the ImageCLEFmedical <ref type="bibr" coords="2,401.32,111.28,12.86,10.91" target="#b0">[1]</ref> in 2016. In 2017 and 2018 <ref type="bibr" coords="2,112.05,124.83,11.28,10.91" target="#b1">[2,</ref><ref type="bibr" coords="2,126.07,124.83,8.91,10.91" target="#b2">3]</ref> the ImageCLEFmedical caption task comprised two subtasks: concept detection and caption prediction. In 2019 <ref type="bibr" coords="2,207.86,138.38,12.68,10.91">[4]</ref> and 2020 <ref type="bibr" coords="2,263.72,138.38,11.27,10.91" target="#b4">[5]</ref>, the task concentrated on extracting Unified Medical Language System ® (UMLS) Concept Unique Identifiers (CUIs) <ref type="bibr" coords="2,366.54,151.93,12.84,10.91" target="#b5">[6]</ref> from radiology images.</p><p>In 2021 <ref type="bibr" coords="2,134.56,165.48,11.29,10.91" target="#b6">[7]</ref>, both subtasks, concept detection and caption prediction, were running again due to participants demands. The focus in 2021 was on making the task more realistic by using fewer images which were all manually annotated by medical doctors. As additional data of similar quality is hard to acquire, the 2022 ImageCLEFmedical caption task continues with both subtasks albeit with an extended version of the Radiology Objects in COntext (ROCO) <ref type="bibr" coords="2,471.72,219.67,12.77,10.91" target="#b7">[8]</ref> data set used for both subtasks, which was already used in 2020 and 2019.</p><p>This paper sets forth the approaches for the caption task: automated cross-referencing of medical images and captions into predicted coherent captions implying UMLS concept detection in radiology images as a first step. This task is a part of the ImageCLEF benchmarking campaign, which has proposed medical image understanding tasks since 2003; a new suite of tasks is generated each subsequent year. Further information on the other proposed tasks at ImageCLEF 2022 can be found in Ionescu et al. <ref type="bibr" coords="2,245.14,314.52,11.43,10.91" target="#b8">[9]</ref>. This is the 6th edition of the ImageCLEFmedical caption task. Just like in 2016 <ref type="bibr" coords="2,450.93,328.07,11.44,10.91" target="#b0">[1]</ref>, 2017 <ref type="bibr" coords="2,491.93,328.07,11.44,10.91" target="#b1">[2]</ref>, 2018 <ref type="bibr" coords="2,112.36,341.62,11.44,10.91" target="#b2">[3]</ref>, and 2021 <ref type="bibr" coords="2,172.58,341.62,11.44,10.91" target="#b6">[7]</ref>, both subtasks of concept detection and caption prediction are included in ImageCLEFmedical Caption 2022. Like in 2020, an extended subset of the ROCO <ref type="bibr" coords="2,456.59,355.17,12.76,10.91" target="#b7">[8]</ref> data set is used to provide a much larger data set compared to 2021.</p><p>Manual generation of the knowledge of medical images is a time-consuming process prone to human error. As this process requires assistance for the better and easier diagnoses of diseases that are susceptible to radiology screening, it is important that we better understand and refine automatic systems that aid in the broad task of radiology-image metadata generation. The purpose of the ImageCLEFmedical 2022 caption prediction and concept detection tasks is the continued evaluation of such systems. Concept detection and caption prediction information is applicable to unlabelled and unstructured data sets and medical data sets that do not have textual metadata. The ImageCLEFmedical caption task focuses on the medical image understanding in the biomedical literature and specifically on concept extraction and caption prediction based on the visual perception of the medical images and medical text data such as medical caption or UMLS CUIs paired with each image (see Figure <ref type="figure" coords="2,301.58,517.76,3.57,10.91" target="#fig_0">1</ref>).</p><p>For the development data, an extended subset of the ROCO <ref type="bibr" coords="2,365.07,531.30,12.79,10.91" target="#b7">[8]</ref> data set from 2020 was used, with new images from the same source added for the validation and test sets.</p><p>This paper presents an overview of the ImageCLEFmedical caption task 2022 including the task and participation in Section 2, the data creation in Section 3, and the evaluation methodology in Section 4. The results are described in Section 5, followed by conclusion in Sections 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task and Participation</head><p>In 2022, the ImageCLEFmedical caption task consisted of two subtasks: concept detection and caption prediction.</p><p>The concept detection subtask follows the same format proposed since the start of the task in 2017. Participants are asked to predict a set of concepts defined by the UMLS CUIs <ref type="bibr" coords="3,465.75,100.52,12.74,10.91" target="#b5">[6]</ref> based on the visual information provided by the radiology images.</p><p>The caption prediction subtask follows the original format of the subtask used between 2017 and 2018. The task is running again since 2021 because of participant demand. This subtask aims to automatically generate captions for the radiology images provided.</p><p>In 2022, 20 teams registered and signed the End-User-Agreement that is needed to download the development data. 12 teams submitted 157 runs for evaluation (all 12 teams submitted working notes) attracting more attention than in 2021. Each of the groups was allowed a maximum of 10 graded runs per subtask.</p><p>Table <ref type="table" coords="3,126.93,222.46,5.05,10.91">1</ref> shows all the teams who participated in the task and their submitted runs. 11 teams participated in the concept detection subtask this year, 3 of those teams also participated in 2021. 10 teams submitted runs to the caption prediction subtask, 4 of those teams also participated in 2021. Overall, 9 teams participated in both subtasks, two teams participated only in the concept detection subtask and one team participated only in the caption prediction subtask.  In the previous edition, in an attempt to make the task more realistic, the data set contained a smaller number of real radiology images annotated by medical doctors which resulted in high-quality concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data Creation</head><p>Additional data of similar quality is hard to acquire and so it was decided to return to the data set already used in 2020 and 2019, which originates from biomedical articles of the PMC Open Access Subset<ref type="foot" coords="4,176.27,608.94,3.71,7.97" target="#foot_0">1</ref>  <ref type="bibr" coords="4,183.10,610.69,17.76,10.91" target="#b23">[24]</ref> and was extended with new images added since the last time the data set was updated. All captions were pre-processed by removing punctuation, numbers and words containing numbers. Additionally, lemmatization was applied using spaCy<ref type="foot" coords="5,379.08,85.21,3.71,7.97" target="#foot_2">2</ref> and the pre-trained model en_core_web_lg. Finally, all captions were converted to lower-case.</p><p>From the resulting captions, UMLS concepts were generated using a reduced subset of the UMLS 2020 AB release <ref type="foot" coords="5,186.94,125.86,3.71,7.97" target="#foot_3">3</ref> , which includes the sections (restriction levels) 0, 1, 2, and 9. To improve the feasibility of recognizing concepts from the images, concepts were filtered based on their semantic type. Concepts with very low frequency were also removed, based on suggestions from previous years.</p><p>Additional concepts were assigned to all images addressing their image modality. Six modality concepts were covered: x-ray, computer tomography (CT), magnetic resonance imaging (MRI), ultrasound, and positron emission tomography (PET) as well as modality combinations (e.g., PET/CT) as standalone concept. For images of the x-ray modality further concepts on the represented anatomy were assigned, covering specific anatomical body regions of the Image Retrieval in Medical Application (IRMA) <ref type="bibr" coords="5,264.53,249.56,17.76,10.91" target="#b24">[25]</ref> classification: cranium, spine, upper extremity/arm, chest, breast/mamma, abdomen, pelvis, and lower extremity/leg. Both of the described concept extensions were created performing a two-stage process, each. In the first stage predictions via classification models were created and assigned as annotations. For modality prediction for all images a model trained on the ROCO dataset <ref type="bibr" coords="5,322.96,303.75,11.48,10.91" target="#b7">[8]</ref>, and for anatomy prediction for x-ray modality images a model trained on an existing IRMA-annotated image dataset <ref type="bibr" coords="5,444.06,317.30,17.90,10.91" target="#b25">[26]</ref> was used. In the second stage, these annotations underwent manual quality control measures, involving correction of faulty predictions and filtering of images that did not represent one of the minded modality or anatomy concepts. Three annotators were involved. Each individual modality concept was processed by a single annotator due to the low complexity of this task part. Anatomy concepts of x-ray modality images were each, too, processed by a single annotator per concept. However, due to the complexity/ambiguity of this task, the one annotator most-experienced in anatomy classification re-evaluated the assessments of the other two. This re-evaluation resulted in very few adjustments, indicating high agreement between annotators.</p><p>The following subsets were distributed to the participants where each image has one caption and multiple concepts (UMLS-CUI):</p><p>• Training set including 83,275 radiology images and associated captions and concepts.</p><p>• Validation set including 7,645 radiology images and associated captions and concepts.</p><p>• Test set including 7,645 radiology images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation Methodology</head><p>In this year's edition, the performance evaluation is carried out in the same way as last year, with both subtasks being evaluated separately.</p><p>For the concept detection subtask, the balanced precision and recall trade-off were measured in terms of F1-scores. In addition, a secondary F1-score was introduced in this edition, where the score is computed using a subset of concepts that was manually curated and only contains x-ray anatomy and image modality concepts.</p><p>Caption prediction performance is evaluated based on the BiLingual Evaluation Understudy (BLEU) scores <ref type="bibr" coords="6,154.04,100.52,16.27,10.91" target="#b26">[27]</ref>, which a geometric mean of n-gram scores from 1 to 4. As a preprocessing step for the evaluation, all captions were lowercased and stripped of all punctuation and English stop words. Additionally, to increase coverage, lemmatization was applied using spaCy and the pre-trained model en_core_web_lg. BLEU values are then computed for each test image, treating the entire caption as one sentence, even though it may contain multiple sentences. The average of the BLEU values for all images is reported as the primary ranking score. Since evaluating generated text and image captioning is very challenging and should be based on a single metric, additional evaluation metrics were explored in this year's edition in order to find the metric that correlate well with human judgements for this task. First, the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) <ref type="bibr" coords="6,234.75,222.46,17.91,10.91" target="#b27">[28]</ref> score was adopted as a secondary metric that counts the number of overlapping units such as n-grams, word sequences, and word pairs between the generated text and the reference. Specifically, the ROUGE-1 (F-measure) score was calculated, which measures the number of matching unigrams between the model-generated text and a reference. All individual scores for each caption are then summed and averaged over the number of captions, resulting in the final score. In addition to ROUGE, the Metric for Evaluation of Translation with Explicit ORdering (METEOR) <ref type="bibr" coords="6,318.17,303.75,18.07,10.91" target="#b28">[29]</ref> was explored, which is a metric that evaluates the generated text by aligning it to reference and calculating a sentence-level similarity score. Furthermore, the Consensus-based Image Description Evaluation (CIDEr) <ref type="bibr" coords="6,438.44,330.85,17.76,10.91" target="#b29">[30]</ref> metric was also adopted. CIDEr is an automatic evaluation metric that calculates the weights of n-grams in the generated text and the reference text based on term frequency and inverse document frequency (TF-IDF), and then compares them based on cosine similarity. Another used metric is the Semantic Propositional Image Caption Evaluation (SPICE) <ref type="bibr" coords="6,367.94,385.05,16.27,10.91" target="#b30">[31]</ref>, which maps the reference and generated captions to semantic scene graphs through dependency parse trees and measures the similarity between the scene graphs for the evaluation. Finally, BERTScore <ref type="bibr" coords="6,443.25,412.15,17.95,10.91" target="#b31">[32]</ref> was used, which is a metric that computes a similarity score for each token in the generated text with each token in the reference text. It leverages the pre-trained contextual embeddings from BERT-based models and matches words by cosine similarity. In this work, the pre-trained model microsoft/deberta-xlarge-mnli<ref type="foot" coords="6,216.27,464.59,3.71,7.97" target="#foot_4">4</ref> was utilized, since it is the model that correlates best with human evaluation according to the authors <ref type="foot" coords="6,247.56,478.14,3.71,7.97" target="#foot_5">5</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>For the concept detection and caption prediction subtasks, Tables <ref type="table" coords="6,379.12,538.52,5.01,10.91" target="#tab_1">2</ref> and<ref type="table" coords="6,405.78,538.52,5.01,10.91" target="#tab_2">3</ref> show the best results from each of the participating teams. The results will be discussed in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results for the Concept Detection subtask</head><p>In 2022, 11 teams participated in the concept prediction subtask, submitting 85 runs. Table <ref type="table" coords="6,500.82,601.80,5.17,10.91" target="#tab_1">2</ref> presents the results achieved in the submissions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUEB-NLP-Group</head><p>Like in previous years, the AUEB-NLP-Group submitted the best performing result with a primary F1-score of 0.4511 <ref type="bibr" coords="7,322.38,344.08,18.07,10.91" target="#b9">[10]</ref> and a secondary F1-score of 0.7907.</p><p>The winning approach was an ensemble of two EfficientNetV2-B0 backbones followed by a single classification layer where the union of predicted concepts was used to form the ensemble. This solution outperformed their retrieval-based system which won last year's concept detection subtask <ref type="bibr" coords="7,234.46,398.27,16.25,10.91" target="#b32">[33]</ref>.</p><p>fdallaserra The second best system, with an only slightly worse primary F1-score of 0.4505 and a better secondary F1-score of 0.8222 <ref type="bibr" coords="7,302.27,434.34,17.76,10.91" target="#b12">[13]</ref> was proposed by CMRE-UoG (fdallaserra). Their best approach consisted of an image retrieval system which used an ensemble of five DenseNet-201, each of which retrieves 100 different images. Then CUIs appearing in at least 30% of the images are taken, and finally a union of each model's predicted CUIs is assigned to each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CSIRO</head><p>The CSIRO group reached a primary F1-score of 0.4471 <ref type="bibr" coords="7,391.90,511.05,18.07,10.91" target="#b10">[11]</ref> and a secondary F1score of 0.7936. They experimented with a range of different backbones for multi-label classification system, and their best approach is an ensemble of 43 DenseNet-161 with top-1% threshold optimisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>eecs-kth</head><p>The eecs-kth team reached a primary F1 score of 0.4360 <ref type="bibr" coords="7,396.74,574.21,18.07,10.91" target="#b11">[12]</ref> and a secondary F1 score of 0.8546. Their best approach utilized a multi-label classification system based on DenseNet161 with a single classification layer.</p><p>vcmi The VCMI (vcmi) team reached a primary F1-score of 0.4329 <ref type="bibr" coords="7,396.29,623.83,18.07,10.91" target="#b21">[22]</ref> and the best overall secondary F1-score of 0.8634. They combined a multi-label classification system based on DenseNet-121 with an information retrieval approach for their best approach, where the retrieval system is used if the classification did not assign any labels.</p><p>PoliMi The PoliMi team reached a primary F1-score of 0.4320 <ref type="bibr" coords="8,372.93,86.97,17.90,10.91" target="#b18">[19]</ref> and a secondary F1-score of 0.8512. They used a ResNext50-based multi-label classification system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SSNSheerinKavitha</head><p>The SSN MLRG (SSNSheerinKavitha) team reached a primary F1-score of 0.4184 <ref type="bibr" coords="8,149.31,136.58,17.76,10.91" target="#b20">[21]</ref> and a secondary F1-score of 0.6544. They employed DenseNet for multi-label classification and an information retrieval system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IUST_NLPLAB</head><p>The IUST_NLPLAB team reached a primary F1-score of 0.3981 <ref type="bibr" coords="8,459.49,172.65,18.07,10.91" target="#b13">[14]</ref> and a secondary F1-score of 0.6732. They used a multi-label classification model based on ResNet for their best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Morgan_CS</head><p>The CS_Morgan (Morgan_CS) team from Morgan State University (USA) reached a primary F1-score of 0.3520 <ref type="bibr" coords="8,244.75,235.81,17.76,10.91" target="#b17">[18]</ref> and a secondary F1-score of 0.6280. They used a fusion of Vision Transformers for their best approach, which outperformed their multi-label classification systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kdelab</head><p>The Kdelab team reached a primary F1-score of 0.3104 <ref type="bibr" coords="8,374.05,285.42,17.83,10.91" target="#b14">[15]</ref> and a secondary F1-score of 0.4120. They exclusively experimented with image retrieval systems and their best approach consisted of an ensemble of different backbone networks (DenseNet, EfficientNet, ResNet) using simple majority voting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SDVA-UCSD</head><p>The SDVA-UCSD team reached a primary F1-score of 0.3079 <ref type="bibr" coords="8,439.90,348.59,18.07,10.91" target="#b19">[20]</ref> and a secondary F1-score of 0.5524. They used a multi-label classification system with ResNet and DenseNet backbones.</p><p>To summarize, in the concept detection subtasks, the groups used primarily multi-label classification systems and image retrieval systems, much like in the 2021 challenge. Multilabel classification systems outperformed retrieval-based systems for most of the teams who experimented with both, and while the winner was a multi-label classification approach, the second placing team with an F1-score only 0.0006 less than the winning team, used a retrievalbased system for which they took last year's winning approach and tuned it to include more CUIs by reducing the threshold for the percentage of retrieved images in which the CUI had to appear from 50% to 30% <ref type="bibr" coords="8,203.65,496.03,16.25,10.91" target="#b12">[13]</ref>. This year's models for concept detection do not show an increased F1-score compared to last year, however due to the much larger data set and number of concepts used in this year's challenge, this is not surprising. Comparing it to the 2020 results, where a data set of similar size was used, the F1-scores show a clear improvement. There are no radically new approaches used in this year's concept detection subtask, but the teams experimented with, optimised and re-combined many different existing techniques and created competitive solutions using both multi-label classification systems and image retrieval systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results for the Caption Prediction subtask</head><p>In this sixth edition, the caption prediction subtask attracted 10 teams which submitted 72 runs. Table <ref type="table" coords="8,115.79,654.15,5.07,10.91" target="#tab_2">3</ref> presents the results of the submissions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IUST_NLPLAB</head><p>The IUST_NLPLAB team presented the best model for the caption prediction subtask. They reached a BLEU score of 0.4828, outperforming the competition by a large margin, and a ROUGE score of 0.1422 <ref type="bibr" coords="9,290.89,557.35,16.40,10.91" target="#b13">[14]</ref>. Additionally, they reached the overall best METEOR score of 0.0928. For their best run, they employed a multi-label classification system based on ResNet50 which treats every word as a label and assigns 26 words in the order of their probability to each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUEB-NLP-Group</head><p>The AUEB-NLP-Group submitted the second best performing result with a BLEU score of 0.3222 <ref type="bibr" coords="9,213.47,634.06,17.76,10.91" target="#b9">[10]</ref> and a ROUGE score of 0.1664. Their best approach utilizes the Show &amp; Tell model <ref type="bibr" coords="9,199.87,647.61,17.76,10.91" target="#b33">[34]</ref> consisting of a CNN-RNN encoder-decoder with an EfficientNetB0 backbone. While they were clearly behind the BLEU score of the winners, they outscore them in most of the other scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CSIRO</head><p>The CSIRO group reached a BLEU score of 0.3114 <ref type="bibr" coords="10,352.84,109.48,17.87,10.91" target="#b10">[11]</ref> and a ROUGE score of 0.1974. Additionally, they reached the overall best BERTScore of 0.6234. They experimented with different encoder-to-decoder models and achieved their best scores with CvT-21 as the encoder and DistilGPT2 as the decoder, warm-started with a MIMIC-CXR checkpoint with a penalty for n-grams of size 3 that are repeated.</p><p>vcmi The VCMI (vcmi) team reached a BLEU score of 0.3058 <ref type="bibr" coords="10,357.70,186.19,17.76,10.91" target="#b21">[22]</ref> and a ROUGE score of 0.1738. They used a vision encoder-to-decoder system for the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>eecs-kth</head><p>The eecs-kth team reached a BLEU score of 0.2917 <ref type="bibr" coords="10,357.25,222.26,17.76,10.91" target="#b11">[12]</ref> and a ROUGE score of 0.1157. They employed an information retrieval system based on AlexNet which summarizes the captions of a number of similar images using Pegasus.</p><p>fdallaserra The CMRE-UoG (fdallaserra) group reached a BLEU score of 0.2913 <ref type="bibr" coords="10,452.08,271.87,17.90,10.91" target="#b12">[13]</ref> and the overall best ROUGE score of 0.2012. They used a CNN Transformer approach with multi-modal (image + CUIs) input for their best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kdelab</head><p>The Kdelab team reached a BLEU score of 0.2782 <ref type="bibr" coords="10,352.00,321.49,17.94,10.91" target="#b15">[16]</ref> and a ROUGE score of 0.1584. Additionally, they reached the overall best CIDEr score of 0.4114 and overall best SPICE score of 0.0512. They used an image retrieval approach with an ensemble of different backbone networks for their best submission results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Morgan_CS</head><p>The CS_Morgan (Morgan_CS) team reached a BLEU score of 0.2549 <ref type="bibr" coords="10,460.94,384.65,17.98,10.91" target="#b17">[18]</ref> and a ROUGE score of 0.1441. They used a very similar approach as for the concept detection, namely a fusion of Vision Transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAI_ImageSem</head><p>The MAI_ImageSem team reached a BLEU score of 0.2211 <ref type="bibr" coords="10,426.63,434.26,17.76,10.91" target="#b16">[17]</ref> and a ROUGE score of 0.1847. For the best results, they use pre-trained BLIP (Bootstrapping Language-Image Pre-training), a pre-training framework for vision-language understanding consisting of a multi-modal encoder-decoder and a captioning and filtering module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SSNSheerinKavitha</head><p>The SSN MLRG (SSNSheerinKavitha) team reached a BLEU score of 0.1595 <ref type="bibr" coords="10,149.82,510.98,18.07,10.91" target="#b20">[21]</ref> and a ROUGE score of 0.0425. For their best run, they employed a Sparse Auto Encoder (SAE) with a Multi-Layer Perceptron (MLP) and a Gated Recurrent Unit (GRU).</p><p>To summarize, in the caption prediction subtask most teams experimented with Transformerbased architectures and image retrieval systems. Only one team used a multi-label classification approach, and it achieved by far the best BLEU score. However, it did not score as well on most of the other employed metrics, with the second placing team outscoring the winners in all but the BLEU and METEOR metrics, which highlights the difficulty of evaluating caption similarity. One metric to highlight especially is SPICE, which is specifically designed for the evaluation of image captions. The winners scored a value of 0.0072 in this metric with the rest of the field (except the last placing team) scoring between 0.0218 and 0.0512.</p><p>Transfer Learning has frequently been used for pre-training, from a variety of different data sets. As in the previous years, simpler architectures ended up yielding better results compared to more complex ones in many instances.</p><p>Similar to the concept detection, the BLEU scores in the caption prediction subtask are overall lower compared to last year, which can be explained by the larger and more complex data set and more varied captions. Since there was no caption prediction subtask running in 2020, no comparable scores for a similar data set exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This year's caption task of ImageCLEFmedical once again ran with both subtasks, concept detection and caption prediction. It returned to a larger, ROCO-based data set for both challenges after a smaller, manually annotated data set was used last year. It attracted 12 teams who submitted 157 runs overall, a stronger participation compared to last year. For the concept detection subtask, a secondary F1-score was introduced to distinguish manually curated concepts from automatically generated ones. For the caption prediction, a number of additional scores were added to better illustrate the difficulty of evaluating the quality of predicted captions. All but one team participated in the concept detection subtask, with only two teams choosing not to participate in the caption prediction subtask as well. Only one team used the generated concepts as the input for the caption prediction model, most teams approached the subtasks with separate systems. For the concept detection challenge, most teams employed multi-label classification systems or image retrieval systems, while the caption prediction challenge was predominantly approached using Transformer-based architectures and image retrieval systems, with only the winning team using a multi-label classification system.</p><p>The scores for both subtasks have not improved compared to the 2021 edition. However, the larger and more complex ROCO-based data set with more concepts and more varied captions make the scores difficult to compare. Looking at the 2020 edition, which used a similar data set, the concept detection scores have clearly increased (there was no caption prediction subtask).</p><p>For next year's ImageCLEFmedical Caption challenge, some possible improvements include adding more manually validated concepts like increased anatomical coverage and directionality information, reducing recurring captions, more fine-grained CUI filters, improving the caption pre-processing, and using a different primary score for the caption prediction challenge, since the BLEU score has some disadvantages which were highlighted by this year's caption prediction results.</p><p>What should also be addressed is how to deal with models that were pre-trained on PMC data, because strictly speaking they have seen the real captions and can have an advantage when some of these images appear in test data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,335.28,294.62,10.91;3,316.93,371.30,185.12,143.60"><head>Figure 1</head><label>1</label><figDesc>Figure 1 shows an example from the data set provided by the task.</figDesc><graphic coords="3,316.93,371.30,185.12,143.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,89.29,561.34,295.09,9.96;3,384.38,559.96,4.41,6.97;3,391.77,561.34,114.21,9.96;3,89.29,573.29,321.27,9.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of a radiology image with the corresponding UMLS ® CUIs and caption extracted from the 2022's ImageCLEFmedical caption task. CC-BY [Ali et al. (2020)] [23]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,88.99,90.49,417.24,213.49"><head>Table 2</head><label>2</label><figDesc>Performance of the participating teams in the ImageCLEFmedical 2022 Concept Detection subtask. Only the best run based on the achieved F1-score is listed for each team, together with the corresponding secondary F1-score based on manual annotations as well as the team rankings based on the primary and secondary F1-score</figDesc><table coords="7,137.83,157.03,319.61,146.95"><row><cell>Group Name</cell><cell>Best Run</cell><cell cols="3">F1 Secondary F1 Rank (secondary)</cell></row><row><cell>AUEB-NLP-Group</cell><cell cols="2">182358 0.4511</cell><cell>0.7907</cell><cell>1 (6)</cell></row><row><cell>fdallaserra</cell><cell cols="2">182324 0.4505</cell><cell>0.8222</cell><cell>2 (4)</cell></row><row><cell>CSIRO</cell><cell cols="2">182343 0.4471</cell><cell>0.7936</cell><cell>3 (5)</cell></row><row><cell>eecs-kth</cell><cell cols="2">181750 0.4360</cell><cell>0.8546</cell><cell>4 (2)</cell></row><row><cell>vcmi</cell><cell cols="2">182097 0.4329</cell><cell>0.8634</cell><cell>5 (1)</cell></row><row><cell>PoliMi-ImageClef</cell><cell cols="2">182296 0.4320</cell><cell>0.8512</cell><cell>6 (3)</cell></row><row><cell>SSNSheerinKavitha</cell><cell cols="2">181995 0.4184</cell><cell>0.6544</cell><cell>7 (8)</cell></row><row><cell>IUST_NLPLAB</cell><cell cols="2">182307 0.3981</cell><cell>0.6732</cell><cell>8 (7)</cell></row><row><cell>Morgan_CS</cell><cell cols="2">182150 0.3520</cell><cell>0.6281</cell><cell>9 (9)</cell></row><row><cell>kdelab</cell><cell cols="2">182346 0.3104</cell><cell>0.4120</cell><cell>10 (11)</cell></row><row><cell>SDVA-UCSD</cell><cell cols="2">181691 0.3079</cell><cell>0.5524</cell><cell>11 (10)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,88.99,90.49,417.24,201.54"><head>Table 3</head><label>3</label><figDesc>Performance of the participating teams in the ImageCLEF 2022 Caption Prediction subtask. Only the best run based on the achieved BLEU score is listed for each team, together with the corresponding secondary ROUGE score as well as the team rankings based on the primary BLEU and secondary ROUGE score. The best results are highlighted.</figDesc><table coords="9,126.28,157.03,342.71,134.99"><row><cell>Group Name</cell><cell>Best Run</cell><cell cols="3">BLEU Secondary ROUGE Rank (secondary)</cell></row><row><cell>IUST_NLPLAB</cell><cell cols="2">182275 0.4828</cell><cell>0.1422</cell><cell>1 (8)</cell></row><row><cell>AUEB-NLP-Group</cell><cell cols="2">181853 0.3222</cell><cell>0.1665</cell><cell>2 (5)</cell></row><row><cell>CSIRO</cell><cell cols="2">182268 0.3114</cell><cell>0.1974</cell><cell>3 (2)</cell></row><row><cell>vcmi</cell><cell cols="2">182325 0.3058</cell><cell>0.1738</cell><cell>4 (4)</cell></row><row><cell>eecs-kth</cell><cell cols="2">182337 0.2917</cell><cell>0.1157</cell><cell>5 (9)</cell></row><row><cell>fdallaserra</cell><cell cols="2">182342 0.2913</cell><cell>0.2012</cell><cell>6 (1)</cell></row><row><cell>kdelab</cell><cell cols="2">182351 0.2783</cell><cell>0.1584</cell><cell>7 (6)</cell></row><row><cell>Morgan_CS</cell><cell cols="2">182238 0.2549</cell><cell>0.1441</cell><cell>8 (7)</cell></row><row><cell>MAI_ImageSem</cell><cell cols="2">182105 0.2211</cell><cell>0.1847</cell><cell>9 (3)</cell></row><row><cell>SSNSheerinKavitha</cell><cell cols="2">182248 0.1595</cell><cell>0.0425</cell><cell>10 (10)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,88.99,314.12,416.99,189.58"><head>Table 4</head><label>4</label><figDesc>Performance of the participating teams in the ImageCLEF 2022 Caption Prediction subtask for additional metrics METEOR, CIDEr, SPICE, and BERTScore. These correspond to the best F1 score-based runs of each team, listed in Table3. The best results are highlighted.</figDesc><table coords="9,139.36,368.71,316.55,134.99"><row><cell>Group Name</cell><cell cols="3">Best Run METEOR CIDEr SPICE BERTScore</cell></row><row><cell>IUST_NLPLAB</cell><cell>182275</cell><cell>0.0928 0.0304 0.0072</cell><cell>0.5612</cell></row><row><cell>AUEB-NLP-Group</cell><cell>181853</cell><cell>0.0737 0.1902 0.0313</cell><cell>0.5989</cell></row><row><cell>CSIRO</cell><cell>182268</cell><cell>0.0841 0.2693 0.0462</cell><cell>0.6234</cell></row><row><cell>vcmi</cell><cell>182325</cell><cell>0.0746 0.2047 0.0358</cell><cell>0.6044</cell></row><row><cell>eecs-kth</cell><cell>182337</cell><cell>0.0624 0.1317 0.0218</cell><cell>0.5728</cell></row><row><cell>fdallaserra</cell><cell>182342</cell><cell>0.0819 0.2564 0.0464</cell><cell>0.6101</cell></row><row><cell>kdelab</cell><cell>182351</cell><cell>0.0735 0.4114 0.0512</cell><cell>0.6003</cell></row><row><cell>Morgan_CS</cell><cell>182238</cell><cell>0.0559 0.1481 0.0232</cell><cell>0.5835</cell></row><row><cell>MAI_ImageSem</cell><cell>182105</cell><cell>0.0675 0.2513 0.0393</cell><cell>0.6059</cell></row><row><cell>SSNSheerinKavitha</cell><cell>182248</cell><cell>0.0226 0.0169 0.0072</cell><cell>0.5451</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,108.93,671.01,243.52,8.97"><p>https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/ [last accessed:</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1" coords="4,355.06,671.01,40.49,8.97"><p>28.06.2022]   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="5,108.93,660.07,213.75,8.97"><p>https://spacy.io/api/lemmatizer/ [last accessed: 28.06.2022]</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3" coords="5,108.93,671.03,353.84,8.97"><p>https://www.nlm.nih.gov/pubs/techbull/nd20/nd20_umls_release.html [last accessed: 28.06.2022]</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4" coords="6,108.93,660.03,292.39,8.97"><p>https://huggingface.co/microsoft/deberta-xlarge-mnli [last accessed: 28.06.2022]</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5" coords="6,108.93,670.99,233.15,8.97"><p>https://github.com/Tiiiger/bert_score [last accessed: 28.06.2022]</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was partially supported by the <rs type="funder">University of Essex GCRF QR Engagement Fund</rs> provided by <rs type="funder">Research England</rs> (grant number <rs type="grantNumber">G026</rs>). The work of <rs type="person">Louise Bloch</rs> and <rs type="person">Raphael Brüngel</rs> was partially funded by a PhD grant from the <rs type="funder">University of Applied Sciences and Arts Dortmund (FH Dortmund), Germany</rs>. The work of <rs type="person">Ahmad Idrissi-Yaghir</rs> and <rs type="person">Henning Schäfer</rs> was funded by a PhD grant from the <rs type="funder">DFG</rs> <rs type="programName">Research Training Group 2535</rs> Knowledgeand data-based personalisation of medicine at the point of care (WisPerMed).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pbCbq22">
					<idno type="grant-number">G026</idno>
				</org>
				<org type="funding" xml:id="_9kMvqdY">
					<orgName type="program" subtype="full">Research Training Group 2535</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,112.66,172.69,393.59,10.91;12,112.66,186.24,394.53,10.91;12,112.66,199.79,80.57,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,380.21,172.69,126.04,10.91;12,112.66,186.24,77.03,10.91">Overview of the ImageCLEF 2016 medical task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,212.47,186.24,284.72,10.91">Working Notes of CLEF 2016 (Cross Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="219" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,213.34,393.58,10.91;12,110.82,226.89,395.17,10.91;12,112.66,240.44,394.53,10.91;12,112.66,253.99,377.05,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,344.91,213.34,161.33,10.91;12,110.82,226.89,331.44,10.91">Overview of ImageCLEFcaption 2017 -Image Caption Prediction and Concept Detection for Biomedical Images</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1866/invited_paper_7.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="12,466.22,226.89,39.77,10.91;12,112.66,240.44,313.33,10.91">Working Notes of CLEF 2017 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">September 11-14, 2017., 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,267.54,393.59,10.91;12,112.66,281.08,393.33,10.91;12,112.66,294.63,395.01,10.91;12,112.66,308.18,153.69,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,379.38,267.54,126.87,10.91;12,112.66,281.08,130.78,10.91">Overview of the ImageCLEF 2018 Caption Prediction Tasks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2125/invited_paper_4.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="12,266.33,281.08,239.66,10.91;12,112.66,294.63,92.04,10.91">Working Notes of CLEF 2018 -Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018., 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,321.73,393.33,10.91;12,112.66,335.28,394.52,10.91;12,112.14,348.83,395.05,10.91;12,112.66,362.38,395.16,10.91;12,111.60,375.93,282.09,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,361.85,321.73,144.14,10.91;12,112.66,335.28,129.60,10.91">Overview of the ImageCLEFmed 2019 Concept Detection Task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2380/paper_245.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="12,112.14,348.83,350.09,10.91">Working Notes of CLEF 2019 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="12,344.82,362.38,156.80,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">September 9-12, 2019. 2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,389.48,395.16,10.91;12,112.66,403.03,393.33,10.91;12,112.14,416.58,395.05,10.91;12,112.66,430.13,58.60,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,400.32,389.48,107.51,10.91;12,112.66,403.03,320.03,10.91">Overview of the Image-CLEFmed 2020 concept prediction task: Medical image understanding</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,460.63,403.03,45.35,10.91;12,112.14,416.58,64.41,10.91">CLEF2020 Working Notes</title>
		<title level="s" coord="12,253.54,416.58,165.70,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1166</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,443.67,393.32,10.91;12,112.66,457.22,377.26,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,186.64,443.67,319.34,10.91;12,112.66,457.22,52.45,10.91">The Unified Medical Language System (UMLS): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gkh061</idno>
	</analytic>
	<monogr>
		<title level="j" coord="12,173.80,457.22,103.68,10.91">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,470.77,394.53,10.91;12,112.66,484.32,394.53,10.91;12,112.66,497.87,394.53,10.91;12,112.66,511.42,135.21,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,167.43,484.32,335.54,10.91">Overview of the ImageCLEFmed 2021 concept &amp; caption prediction task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,127.66,497.87,323.53,10.91">CLEF2021 Working Notes, CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1101" to="1112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,524.97,393.33,10.91;12,112.33,538.52,393.65,10.91;12,112.66,552.07,395.17,10.91;12,112.41,565.62,394.78,10.91;12,112.66,579.17,394.53,10.91;12,112.66,592.72,361.01,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,370.24,524.97,135.74,10.91;12,112.33,538.52,393.65,10.91;12,112.66,552.07,395.17,10.91">Radiology Objects in COntext (ROCO): A Multimodal Image Dataset, in: Intravascular Imaging and Computer Assisted Stenting -and -Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01364-6_20</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,112.41,565.62,389.37,10.91;12,179.72,579.17,180.34,10.91">7th Joint International Workshop, CVII-STENT 2018 and Third International Workshop</title>
		<meeting><address><addrLine>LABELS; Granada, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Proceedings</publisher>
			<date type="published" when="2018-09-16">2018. September 16, 2018. 2018</date>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
	<note>Held in Conjunction with MICCAI 2018</note>
</biblStruct>

<biblStruct coords="12,112.66,606.27,395.01,10.91;12,112.66,619.81,395.17,10.91;12,112.39,633.36,394.80,10.91;12,112.66,646.91,394.62,10.91;12,112.66,660.46,393.33,10.91;13,112.66,86.97,395.17,10.91;13,112.66,100.52,393.54,10.91;13,112.66,114.06,170.14,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,352.50,646.91,154.78,10.91;12,112.66,660.46,300.10,10.91">Overview of the ImageCLEF 2022: Multimedia retrieval in medical, social media and nature applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,435.26,660.46,70.73,10.91;13,112.66,86.97,395.17,10.91;13,112.66,100.52,239.58,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<title level="s" coord="13,359.20,100.52,147.00,10.91;13,112.66,114.06,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,127.61,395.17,10.91;13,112.66,141.16,394.53,10.91;13,112.66,154.71,294.58,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,440.73,127.61,67.10,10.91;13,112.66,141.16,226.09,10.91">AUEB NLP group at ImageCLEFmed caption</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Charalampakos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zachariadis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Trakas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,386.92,141.16,120.26,10.91;13,112.66,154.71,175.50,10.91">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Androutsopoulos</note>
</biblStruct>

<biblStruct coords="13,112.66,168.26,395.17,10.91;13,112.66,181.81,394.53,10.91;13,112.66,195.36,157.21,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,431.73,168.26,76.10,10.91;13,112.66,181.81,81.41,10.91">CSIRO at Image-CLEFmed caption</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lebrat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nicolson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Belous</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Koopman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,243.98,181.81,263.20,10.91;13,112.66,195.36,38.13,10.91">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,208.91,393.33,10.91;13,112.14,222.46,367.10,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,238.66,208.91,202.54,10.91">Neuraldynamicslab at ImageCLEF medical 2022</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Moschovis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fransén</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,462.41,208.91,43.58,10.91;13,112.14,222.46,248.02,10.91">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,236.01,393.33,10.91;13,112.66,249.56,394.52,10.91;13,112.66,263.11,294.58,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,340.97,236.01,165.02,10.91;13,112.66,249.56,256.68,10.91">CMRE-UoG team at ImageCLEFmed caption 2022 task: Concept detection and image captioning</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">D</forename><surname>Serra1</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Deligianni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Q</forename><surname>O'neil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,392.13,249.56,115.05,10.91;13,112.66,263.11,175.50,10.91">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,276.66,394.53,10.91;13,112.66,290.20,394.53,10.91;13,112.66,303.75,294.58,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,112.66,290.20,220.76,10.91">IUST_NLPLAB at ImageCLEFmed caption tasks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hajihosseini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lotfollahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Nobakhtian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Javid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Omidi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Eetemadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,385.72,290.20,121.47,10.91;13,112.66,303.75,175.50,10.91">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,317.30,394.62,10.91;13,112.66,330.85,393.33,10.91;13,112.66,344.40,325.41,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,382.96,317.30,124.31,10.91;13,112.66,330.85,286.48,10.91">Kdelab at ImageCLEF 2022: Medical concept detection with image retrieval and code ensemble</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tsuneda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Asakawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Komoda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,421.74,330.85,84.24,10.91;13,112.66,344.40,23.42,10.91">CLEF2022 Working Notes</title>
		<title level="s" coord="13,143.49,344.40,175.50,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,357.95,395.17,10.91;13,112.66,371.50,394.53,10.91;13,112.66,385.05,157.21,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,369.22,357.95,138.61,10.91;13,112.66,371.50,118.46,10.91">Kdelab at ImageCLEF2022 medical caption prediction task</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tsuneda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Asakawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Komoda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,253.77,371.50,253.42,10.91;13,112.66,385.05,38.13,10.91">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,398.60,393.33,10.91;13,112.66,412.15,394.53,10.91;13,112.66,425.70,294.58,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,176.46,398.60,329.53,10.91;13,112.66,412.15,257.68,10.91">ImageSem Group at ImageCLEFmed Caption 2022 Task: Generating Medical Image Descriptions based on Visual-Language Pre-training</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,392.68,412.15,114.51,10.91;13,112.66,425.70,175.50,10.91">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,439.25,393.33,10.91;13,112.66,452.79,394.53,10.91;13,112.66,466.34,394.53,10.91;13,112.66,479.89,22.69,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,236.34,439.25,176.23,10.91;13,441.86,439.25,64.13,10.91;13,112.66,452.79,390.26,10.91">Deep learning based multilabel classification and transformers for concept detection &amp; caption prediction</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Layode</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,126.96,466.34,288.62,10.91">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>CS_Morgan at ImageCLEFmed caption</note>
</biblStruct>

<biblStruct coords="13,112.66,493.44,393.33,10.91;13,112.66,506.99,394.52,10.91;13,112.66,520.54,89.12,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,322.79,493.44,183.20,10.91;13,112.66,506.99,32.70,10.91">Polimi-imageclef group at ImageCLEFmed caption</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A M</forename><surname>Ghayyomnia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Carmana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,188.84,506.99,291.47,10.91">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,534.09,393.33,10.91;13,112.66,547.64,325.41,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="13,162.20,534.09,234.76,10.91">ImageCLEFmed concept detection, finding duplicates</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gentili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,419.83,534.09,86.16,10.91;13,112.66,547.64,206.33,10.91">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,561.19,393.32,10.91;13,112.66,574.74,394.61,10.91;13,112.66,588.29,394.52,10.91;13,112.66,601.84,22.69,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="13,230.58,561.19,275.41,10.91;13,112.66,574.74,374.66,10.91">SSN MLRG at ImageCLEF 2022: Medical concept detection and caption prediction using transfer learning and transformer based learning approaches</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">M S</forename><surname>Sitara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kavitha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,112.66,588.29,113.95,10.91">CLEF2022 Working Notes</title>
		<title level="s" coord="13,234.09,588.29,178.06,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,615.39,393.33,10.91;13,112.66,628.93,393.33,10.91;13,112.66,642.48,394.53,10.91;13,112.66,656.03,89.12,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="13,350.20,615.39,155.79,10.91;13,112.66,628.93,393.33,10.91;13,112.66,642.48,34.80,10.91">Detecting Concepts and Generating Captions from Medical Images: Contributions of the VCMI Team to ImageCLEFmed Caption</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Rio-Torto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Patrício</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Montenegro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gonçalves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,190.82,642.48,110.18,10.91">CLEF2022 Working Notes</title>
		<title level="s" coord="13,308.33,642.48,172.16,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,669.58,395.16,10.91;14,112.66,86.97,393.33,10.91;14,112.66,100.52,395.00,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="13,376.65,669.58,131.17,10.91;14,112.66,86.97,393.33,10.91;14,112.66,100.52,74.70,10.91">Pelvic Girdle Pain, Hypermo-bility Spectrum Disorder and Hypermobility-Type Ehlers-Danlos Syndrome: A Narrative Literature Review</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Andrzejowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">K</forename><surname>Kanakaris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">V</forename><surname>Giannoudis</surname></persName>
		</author>
		<idno type="DOI">10.3390/jcm9123992</idno>
	</analytic>
	<monogr>
		<title level="j" coord="14,195.93,100.52,122.36,10.91">Journal of Clinical Medicine</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">3992</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,114.06,393.33,10.91;14,112.66,127.61,395.01,10.91;14,112.66,141.16,143.58,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="14,176.64,114.06,264.55,10.91">PubMed Central: The GenBank of the published literature</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.98.2.381</idno>
	</analytic>
	<monogr>
		<title level="j" coord="14,450.78,114.06,55.20,10.91;14,112.66,127.61,309.87,10.91">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="381" to="382" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,154.71,393.33,10.91;14,112.66,168.26,394.61,10.91;14,112.66,181.81,395.01,10.91;14,112.66,195.36,119.84,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="14,391.06,154.71,114.93,10.91;14,112.66,168.26,134.85,10.91">The IRMA code for unique classification of medical images</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kohnen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">B</forename><surname>Wein</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.480677</idno>
	</analytic>
	<monogr>
		<title level="m" coord="14,409.45,168.26,97.82,10.91;14,112.66,181.81,337.98,10.91">Medical Imaging 2003: PACS and Integrated Medical Information Systems: Design and Evaluation</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Ratib</surname></persName>
		</editor>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,208.91,393.33,10.91;14,112.66,222.46,394.51,10.91;14,112.66,238.45,92.42,7.90" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="14,251.62,208.91,254.37,10.91">IRMA Bilder in 193 Kategorien für ImageCLEFmed</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deserno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ott</surname></persName>
		</author>
		<idno type="DOI">10.18154/RWTH-2016-06143</idno>
		<ptr target="https://publications.rwth-aachen.de/record/667225.doi:10.18154/RWTH-2016-06143" />
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">363</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,249.56,393.32,10.91;14,112.66,263.11,393.53,10.91;14,112.66,276.66,203.57,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="14,308.17,249.56,197.81,10.91;14,112.66,263.11,88.86,10.91">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,224.89,263.11,281.30,10.91;14,112.66,276.66,116.04,10.91">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,290.20,395.17,10.91;14,112.66,303.75,394.61,10.91;14,112.66,317.30,157.69,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="14,157.59,290.20,266.49,10.91">ROUGE: A Package for Automatic Evaluation of Summaries</title>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/W04-1013" />
	</analytic>
	<monogr>
		<title level="m" coord="14,448.01,290.20,59.81,10.91;14,112.66,303.75,291.76,10.91">Text Summarization Branches Out, Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,330.85,393.33,10.91;14,112.66,344.40,393.32,10.91;14,112.33,357.95,394.94,10.91;14,112.31,371.50,289.63,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="14,230.46,330.85,275.52,10.91;14,112.66,344.40,109.29,10.91">Meteor Universal: Language Specific Translation Evaluation for Any Target Language</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W14-3348</idno>
		<ptr target="http://aclweb.org/anthology/W14-3348.doi:10.3115/v1/W14-3348" />
	</analytic>
	<monogr>
		<title level="m" coord="14,245.52,344.40,260.46,10.91;14,112.33,357.95,50.10,10.91">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,385.05,393.33,10.91;14,112.66,398.60,393.33,10.91;14,112.33,412.15,395.33,10.91;14,112.66,425.70,167.31,10.91" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="14,302.99,385.05,203.00,10.91;14,112.66,398.60,45.51,10.91">CIDEr: Consensus-based image description evaluation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7299087</idno>
		<ptr target="http://ieeexplore.ieee.org/document/7299087/.doi:10.1109/CVPR.2015.7299087" />
	</analytic>
	<monogr>
		<title level="m" coord="14,188.48,398.60,317.51,10.91;14,112.33,412.15,30.92,10.91">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,439.25,393.33,10.91;14,112.66,452.79,394.53,10.91;14,112.66,466.34,394.53,10.91;14,112.66,479.89,244.99,10.91" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="14,336.55,439.25,169.44,10.91;14,112.66,452.79,84.34,10.91">SPICE: Semantic Propositional Image Caption Evaluation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46454-1_24</idno>
	</analytic>
	<monogr>
		<title level="m" coord="14,421.73,452.79,85.45,10.91;14,112.66,466.34,48.13,10.91">Computer Vision -ECCV 2016</title>
		<title level="s" coord="14,168.07,466.34,155.04,10.91">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,493.44,393.33,10.91;14,112.66,506.99,394.53,10.91;14,112.66,520.54,394.03,10.91;14,112.66,534.09,109.64,10.91" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="14,384.35,493.44,121.63,10.91;14,112.66,506.99,98.70,10.91">Bertscore: Evaluating text generation with BERT</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SkeHuCVFDr" />
	</analytic>
	<monogr>
		<title level="m" coord="14,238.03,506.99,269.16,10.91;14,112.66,520.54,43.84,10.91">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,547.64,393.33,10.91;14,112.66,561.19,393.33,10.91;14,112.14,574.74,360.61,10.91" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="14,478.16,547.64,27.83,10.91;14,112.66,561.19,201.88,10.91">AUEB NLP Group at ImageCLEFmed Caption Tasks</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Charalampakos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,358.70,561.19,147.29,10.91;14,112.14,574.74,167.82,10.91">CLEF2021 Working Notes, CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="1184" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,588.29,394.53,10.91;14,112.66,601.84,395.01,10.91;14,112.66,615.39,393.60,10.91;14,112.66,628.93,394.52,10.91;14,112.48,642.48,364.33,10.91" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="14,481.24,588.29,25.95,10.91;14,112.66,601.84,302.25,10.91">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v37/xuc15.html" />
	</analytic>
	<monogr>
		<title level="m" coord="14,161.68,615.39,344.58,10.91;14,112.66,628.93,17.79,10.91;14,317.36,629.95,185.51,9.72">Proceedings of the 32nd International Conference on Machine Learning, ICML 2015</title>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</editor>
		<meeting>the 32nd International Conference on Machine Learning, ICML 2015<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-11">6-11 July 2015. 2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
