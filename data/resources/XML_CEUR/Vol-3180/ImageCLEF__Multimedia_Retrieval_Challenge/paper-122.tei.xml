<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,105.69,416.59,15.42;1,88.78,127.60,35.64,15.43">Kdelab at ImageCLEFmedical 2022 Caption Prediction Task</title>
				<funder ref="#_WB7Usf3 #_zk9rFN7 #_6QC32CQ">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,155.92,66.70,11.96"><forename type="first">Riku</forename><surname>Tsuneda</surname></persName>
							<email>tsuneda@kde.cs.tut.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<settlement>Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,168.63,155.92,85.40,11.96"><forename type="first">Tetsuya</forename><surname>Asakawa</surname></persName>
							<email>asakawa@kde.cs.tut.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<settlement>Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.67,155.92,76.24,11.96"><forename type="first">Kazuki</forename><surname>Shimizu</surname></persName>
							<email>shimizu@heartcenter.or.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">Toyohashi Heart Center</orgName>
								<address>
									<addrLine>21-1 Gobutori, Oyama-cho, Toyohashi-shi</addrLine>
									<settlement>Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,355.56,155.92,89.81,11.96"><forename type="first">Takuyuki</forename><surname>Komoda</surname></persName>
							<email>komoda@heartcenter.or.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">Toyohashi Heart Center</orgName>
								<address>
									<addrLine>21-1 Gobutori, Oyama-cho, Toyohashi-shi</addrLine>
									<settlement>Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,169.87,64.67,11.96"><forename type="first">Masaki</forename><surname>Aono</surname></persName>
							<email>aono@kde.cs.tut.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<settlement>Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,105.69,416.59,15.42;1,88.78,127.60,35.64,15.43">Kdelab at ImageCLEFmedical 2022 Caption Prediction Task</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">6A8432367C6905FCF4071D015BABD910</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Captioning</term>
					<term>Deep Learning</term>
					<term>Medical Images</term>
					<term>Image Retrieval</term>
					<term>Concept Detection</term>
					<term>Natural Language Processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ImageCLEFmedical 2022 Caption Task is an example of a challenging research problem in the field of image captioning. The goal of this research is to automatically generate accurate captions describing a given medical image. We describe five approaches using image retrieval and Deep Learning . In this paper, we have adopted K-nn as image retrieval, X-VLM and Show, Attend and Tell as Deep Neural Network (DNN). Furthermore, we describe the effectiveness of a method that uses information from the CUI code as an input feature for DNN. We submitted 8 runs to the caption prediction task, and achieved the BLEU score of 0.278 and the ROUGE score of 0.158, which ranked 7th among the participating teams.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, multimodal processing of images and natural language has attracted much attention in the field of machine learning. Image Captioning is one of these representative tasks, which aims at proper captioning of input images. As these accuracies improve, it is expected that computers will not only be able to detect objects in images, but also to understand the relationships and behaviors between objects. Image captioning is also effective in the medical field. For example, interpreting and summarizing possible disease symptoms from a large number of radiology images (e.g. X-ray images and CT images) is a time-consuming task that can only be understood by highly knowledgeable specialists. If computers could understand medical images and generate accurate captions, it would help solve the world's growing shortage of medical doctors. However, there is still the bottleneck problem that few physicians are able to give accurate annotations.</p><p>The nature of medical images are quite different from general images such as MS-COCO <ref type="bibr" coords="1,493.22,576.47,12.76,10.91" target="#b0">[1]</ref> in many aspects.</p><p>A player readies for a swing during a tennis game .  In the following, we first describe related work on Image Captioning task and Medical Image Captioning in Section 2, followed by the description of the dataset provided for ImageCLEFmedical 2022 Caption Prediction <ref type="bibr" coords="2,220.82,414.63,13.00,10.91" target="#b1">[2]</ref> [3] dataset in Section 3. In Section 4, we describe details of the method we have applied, and then of our experiments we have conducted in Section 5. We finally conclude this paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we introduce previous studies related to this research. First, as representative studies of caption generation in general, Show and Tell by Vinyals et al. Vinyals' work uses a CNN (convolutional neural network) <ref type="bibr" coords="2,259.24,527.45,13.00,10.91" target="#b3">[4]</ref> as an image encoder and an RNN (recurrent neural network) as a decoder. Such a model is called an encoder-decoder model and is the basis of current caption generation.</p><p>Vinyals et al. achieved the highest accuracy at that time on the MS-COCO dataset. Xu et al. 's <ref type="bibr" coords="2,108.04,581.65,12.79,10.91" target="#b4">[5]</ref> work was based on Vinyals et al. 's work. By incorporating visual attention <ref type="bibr" coords="2,455.90,581.65,12.79,10.91" target="#b5">[6]</ref> into the caption generation model using CNN and LSTM (Long Short Term Memory) <ref type="bibr" coords="2,441.99,595.20,11.59,10.91" target="#b6">[7]</ref>, they were able to generate more descriptive captions. This study achieved the highest accuracy at the time of publication. Anderson et al. <ref type="bibr" coords="2,247.31,622.30,12.69,10.91" target="#b7">[8]</ref> demonstrated that combining both bottom-up attention with Faster R-CNN <ref type="bibr" coords="2,176.98,635.85,12.86,10.91" target="#b8">[9]</ref> and top-down attention with weighted averaging can be used for both Image Captioning and Visual Question Answering <ref type="bibr" coords="2,322.81,649.39,16.41,10.91" target="#b9">[10]</ref>. Anderson et al. achieved SOTA on both the Image Captioning and Visual Question Answering tasks by combining both bottom-up attention with Faster R-CNN and top-down attention with weighted average. In recent years, research on caption generation using transformers has achieved SOTA.</p><p>In 2019, Jing et al. <ref type="bibr" coords="3,177.87,114.06,17.76,10.91" target="#b10">[11]</ref> proposed a method for generating captions for chest X-ray images using an Encoder-Decoder model with Co-Attention <ref type="bibr" coords="3,302.49,127.61,16.41,10.91" target="#b11">[12]</ref>. In 2021, the Medical Caption Prediction Task was held at the international competition ImageCLEF 2021 <ref type="bibr" coords="3,372.75,141.16,16.14,10.91" target="#b12">[13]</ref>. Thus, caption generation in the medical field is a challenging research field that continues to attract attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset</head><p>For the ImageCLEFmedical 2022 Caption Prediction task, organizers have provided us with a training set of 83,275 radiology images with the same number of captions, a validation set of 7,645 radiology images with the same number of captions, and a test set of 7,601 radiology images with the same number of captions. These images are part of ROCO dataset <ref type="bibr" coords="3,452.29,253.99,16.10,10.91" target="#b13">[14]</ref>. We are supposed to use these as our datasets. Most of the images in the dataset are non-colored, and they potentially include non-essential logos, arrow symbols, numbers and texts. The image data set included multiple modalities such as CT, MRI, X-ray, ultrasound images, and angiographic images. The task participants have to generate automatic caption based on radiology image data.</p><p>According to our analysis, the top word frequencies were dominated by prepositions and words such as right and left that indicate position. The top 14 ranking words in terms of word frequency are summarized in Table <ref type="table" coords="3,251.50,362.38,3.74,10.91" target="#tab_0">1</ref>. For our experiments, we merged the provided training and validation sets and used 10% of the merged data as our validation set, and another 10% of the merged data as our development set in which we evaluated the performance of our models. The remaining 80% served as the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>In this section, we describe the approaches that were used in our submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image Preprocessing</head><p>Given that the images in the dataset are black and white images, we tried pseudo colorization to the images. Pseudo colorization is the assignment of a color map to an image. We used the Open-CV <ref type="bibr" coords="4,133.88,134.63,17.88,10.91" target="#b14">[15]</ref> JET colormap for the colormap. We show an example of the pseudo-coloring in Figure <ref type="figure" coords="4,123.09,148.18,3.74,10.91" target="#fig_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image Retrieval Approach</head><p>Image retrieval methods were one of the major methods in CLEF2021. Last year, AUEB-NLP Gloup <ref type="bibr" coords="4,119.57,211.46,17.98,10.91" target="#b15">[16]</ref> and PUC Chile Team <ref type="bibr" coords="4,237.02,211.46,17.98,10.91" target="#b16">[17]</ref> adopted this method and achieved top scores. Since the most medical images are grayscale images, retrieval methods may be more effective than DNN methods. We similarly tested the effectiveness of our image retrieval method. We illustrate our image retrieval method in Figure <ref type="figure" coords="4,237.13,252.11,3.66,10.91" target="#fig_3">3</ref>. We have tried an ensemble of image retrieval methods. The ensemble method is a majority voting of five images predicted using each feature extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Image</head><p>Pseudo Colorized Image First, we extracted features from all images using a several feature extractor. Next, we compute approximation based on the features using the Cosine similarity or Euclidean distance. Finally, we assign captions to test images from the retrieval results. We adopted DenseNet121 <ref type="bibr" coords="4,229.10,536.66,16.42,10.91" target="#b17">[18]</ref>, DenseNet201 <ref type="bibr" coords="4,322.36,536.66,16.41,10.91" target="#b17">[18]</ref>, ResNet-50 <ref type="bibr" coords="4,401.93,536.66,16.41,10.91" target="#b18">[19]</ref>, ResNet-152 <ref type="bibr" coords="4,486.67,536.66,16.41,10.91" target="#b18">[19]</ref>, EfficientNet-B0 <ref type="bibr" coords="4,158.56,550.21,16.09,10.91" target="#b19">[20]</ref>, EfficientNet-B7 <ref type="bibr" coords="4,250.49,550.21,16.09,10.91" target="#b19">[20]</ref>, Inception-V3 <ref type="bibr" coords="4,332.03,550.21,16.09,10.91" target="#b20">[21]</ref>, Xception <ref type="bibr" coords="4,396.43,550.21,17.48,10.91" target="#b21">[22]</ref>,inception ResNet-V2 <ref type="bibr" coords="4,89.29,563.76,17.91,10.91" target="#b22">[23]</ref> and Nas Net Large <ref type="bibr" coords="4,195.21,563.76,17.91,10.91" target="#b23">[24]</ref> as feature extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">DNN Approach</head><p>DNN methods were one of the orthodox methods in CLEF2021. We have adopted Show, Attend and Tell <ref type="bibr" coords="4,127.63,627.03,12.84,10.91" target="#b4">[5]</ref> and X-VLM <ref type="bibr" coords="4,197.18,627.03,17.91,10.91" target="#b24">[25]</ref> as DNN baseline methods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Show, Attend and Tell</head><p>This model is capable of highly accurate captioning without using object detection. Our team achived 3rd in CLEF2021 with this model and Effective Image Preprocessing. The architecture of the models is almost the same, while our model differs in that we employ ResNet-101 <ref type="bibr" coords="5,488.01,447.15,17.98,10.91" target="#b18">[19]</ref> instead of VGG16 <ref type="bibr" coords="5,172.18,460.70,12.76,10.91" target="#b3">[4]</ref> as the CNN encoder. Furthermore, we have adopted DenseNet201 as the CNN encoder. In decoder part, words are predicted by LSTM with Attention based on the image features. The output captions are the best alignment of the predicted words by Beam Search. We illustrate our image retrieval method in Figure <ref type="figure" coords="5,318.02,501.35,3.74,10.91" target="#fig_4">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">X-VLM</head><p>Most modern caption generation models use object detectors. X-VLM <ref type="bibr" coords="5,414.57,550.68,18.07,10.91" target="#b24">[25]</ref> achieved SOTA in the Image Captioning task without using an object detector. We have adopted this model directly to this task. We illustrate X-VLM method in Figure <ref type="figure" coords="5,352.87,577.78,3.67,10.91">5</ref>. The X-VLM consists of an image encoder, a text encoder, and a cross-modal encoder. All encoders are based on Transformer. The cross-modal encoder fuses visual and linguistic features through cross-attention at each layer. In image encoder part, after images (224×224) are divided into 32x32 patches (called patches), they are reshaped while retaining its positional information. In bounding box prediction part, the model predicts the box for the text and input CLS tokens from Cross-model Encoder into MLP. In contrastive learning part, Contrastive Learning is performed using cosine similarity  Adam <ref type="bibr" coords="6,357.42,467.15,16.46,8.87" target="#b25">[26]</ref> in vision-to-text and text-to-vision. Texts that are not paired are farther apart. In Matching Prediction, the visual and textual features are checked for a match. We have adopted a 16M parameter model (trained using CC-12M) as our pre-training model. We used this pre-trained model for downstream (Image Captioning) training. We used beam search, with the beam size for each step equal to 8. The hyperparameters we used for each model, after light tuning, can be seen in Table <ref type="table" coords="6,166.25,570.47,3.74,10.91" target="#tab_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Combination of Retrieval and DNN Approach</head><p>This method adds a mechanism to the baseline method to predict CUI (Concept Unique Identifier) codes contained in images and use them as new features. Concept detection is not performed during training, but only during inference. During training, the CUI codes assigned to images are directly used for training. Figure <ref type="figure" coords="6,263.50,660.84,5.17,10.91" target="#fig_6">6</ref> shows an overview of the proposed method during</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Encoder</head><p>Text, Text1, Text2, Text3 The procedure of creating CUI features is as follows. First, the number of predicted CUI codes is set to be 50 (the maximum number of CUI codes in the dataset), and the missing codes are compensated by assigning "None" to them. Next, for each CUI code (including "None" ), perform Word Embedding with Embedding Dim=32. Finally, the final CUI features are the flattened ones. Using this concatenated CUI and image features, Show, Attend and Tell performs caption prediction.</p><formula xml:id="formula_0" coords="7,192.90,184.50,205.51,62.89">I,V 1 ,V 2 ,V 3 T,T 1 ,T 2 ,T 3 [CLS] [CLS] Contrastive Learning I,V 1 ,V 2 ,V 3 T,T 1 ,T 2 ,T 3 T,T 1 ,T 2 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Submission and Results</head><p>We have submitted eight runs using the three methods and pre-processing described in the previous section. Since the official evaluation metric for caption prediction is BLEU-4, we have evaluate models using this metric in the development set to determine which models to submit (each participant was allowed a maximum of 10 submissions). Tables <ref type="table" coords="7,395.27,549.31,4.97,10.91" target="#tab_3">3</ref> and<ref type="table" coords="7,424.54,549.31,4.97,10.91" target="#tab_4">4</ref> shows the scores for the development set, and Table <ref type="table" coords="7,245.75,562.86,4.98,10.91" target="#tab_5">5</ref> shows the final scores for our model on the unknown test caption.</p><p>First, we describe our results and findings in the development set. In image retrieval methods, accuracy has turned out to improve when using ensembles with simple majority voting. Ensemble 1 has a higher BLEU score than Ensemble 2. Comparing Cosine similarity and Euclidean distance, Euclidean distance provides better retrieval accuracy. The DNN approach yields higher accuracy for Show, Attend and Tell than X-VLM. We speculate that this is because the MS-COCO dataset was used in the X-VLM pre-training. The combination of Image Retrieval and DNN method have may a negative effect on learning. On the other hand, Pseudo colorization is not effective, while it works well for X-VLM. Second, we describe our results and findings in the test set. We submitted to AIcrowd the systems that scored highly in each of the approaches in our development set. Overall, the results of the test set scored very much higher than the development set. The highest scoring submission has turned out to be an image retrieval system using Euclidean distance.</p><p>Finally, from organizer's evaluation, we have achieved a BLEU score of 0.278, a ROUGE score of 0.158 , a METEOR score of 0.073, a CIDEr score of 0.411, a SPICE score of 0.051and a BERT score of 0.600 in the ImageCLEFmedical 2022 Caption Prediction task, placing us 7th. Our submission ranked 7th in the BLEU score, but 1st in the CIDEr score. We achieved the highest CIDEr score by using image retrieval method to predict words that appear only in certain images (infrequent words).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have described our system with which we submitted to the ImageCLEFmedical 2022 Caption Prediction task. In our system, we have done our own data pre-processing, and have attempted to automatically generate caption with image retrieval, DNN and combination of retrieval and neural network.</p><p>The results demonstrate that some of experiment have improved the caption prediction accuracy of the image retrieval. Pseudo colorization and combination approach turns out to be ineffective in this task. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,326.12,287.79,141.87,9.17;2,326.12,295.18,129.77,9.17"><head></head><label></label><figDesc>chest xray posteroanterior view suggestive of a large opacity on the left side chest</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,89.29,326.00,416.70,8.93;2,89.29,338.01,417.87,8.87;2,88.97,349.03,216.07,9.96;2,323.00,116.49,149.21,149.21"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of general (left) and medical (right) Caption Prediction data&amp; left image : via MS-COCO, [CC BY 4.0](https://cocodataset.org/), right image:CC BY [Wadhwa et al. (2021)](https: //www.ncbi.nlm.nih.gov/pmc/articles/PMC8476187/).</figDesc><graphic coords="2,323.00,116.49,149.21,149.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,89.29,454.39,416.69,8.93;4,89.29,466.40,333.26,8.87;4,323.80,320.52,90.58,101.17"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of Original Image (left) and Pseudo Colorization (right), CC BY [CC BY-NC-ND [Peixoto et al. (2015)]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5580006/).</figDesc><graphic coords="4,323.80,320.52,90.58,101.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,89.29,326.00,418.22,8.93;5,89.29,337.07,417.34,9.96;5,89.29,349.03,417.79,9.96;5,89.29,361.92,385.02,8.87"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Image Retrieval System's Flow, via CC BY-NC [Hekmat et al. (2016)](https://www.ncbi.nlm.nih. gov/pmc/articles/PMC4835740/), CC BY [Abidi et al. (2015)](https://www.ncbi.nlm.nih.gov/pmc/articles/ PMC4769046/), CC BY [Apaydin et al. (2018)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6202798/), CC BY-NC-ND [Datta et al. (2018)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5925857/)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,89.29,325.16,418.23,8.93;6,89.29,336.23,167.34,9.96"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Model of Show, Attend and Tell [CC BY-NC-ND [Peixoto et al. (2015)]](https://www.ncbi. nlm.nih.gov/pmc/articles/PMC5580006/)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,89.29,331.14,214.41,8.93"><head>T 3 IFigure 5 :</head><label>35</label><figDesc>Figure 5: Pre-training model ardhitecture of X-VLM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="8,89.29,325.16,418.23,8.93;8,88.99,337.16,275.23,8.87"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Combination of Neural Network and Image Retrieval System [CC BY-NC-ND [Peixoto et al. (2015)]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5580006/)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.81,391.39,324.67,108.69"><head>Table 1</head><label>1</label><figDesc>Word frequency Ranking in Caption Dataset</figDesc><table coords="3,181.79,419.08,231.70,81.00"><row><cell cols="2">Rank Word</cell><cell>Freq</cell><cell>Rank</cell><cell>Word</cell><cell>Freq</cell></row><row><cell>1</cell><cell cols="2">show 41,364</cell><cell>7</cell><cell>scan</cell><cell>11,655</cell></row><row><cell>2</cell><cell cols="2">arrow 24,555</cell><cell>8</cell><cell cols="2">tomography 10,628</cell></row><row><cell>3</cell><cell cols="2">right 20,340</cell><cell>9</cell><cell>chest</cell><cell>10,052</cell></row><row><cell>4</cell><cell>ct</cell><cell>16,495</cell><cell>10</cell><cell>mass</cell><cell>9,192</cell></row><row><cell>5</cell><cell cols="2">image 14,703</cell><cell>11</cell><cell>view</cell><cell>8,580</cell></row><row><cell>6</cell><cell>left</cell><cell>12,752</cell><cell>12</cell><cell>radiograph</cell><cell>8,025</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,88.99,370.28,284.62,105.74"><head>Table 2</head><label>2</label><figDesc>The hyperparameters of X-VLM</figDesc><table coords="6,221.40,401.89,152.21,74.12"><row><cell>Name</cell><cell>Parameter</cell></row><row><cell>Epochs</cell><cell>120</cell></row><row><cell>Batch Size</cell><cell>32</cell></row><row><cell>Patch Size of ViT</cell><cell>32</cell></row><row><cell>Num of Decoder Layers</cell><cell>6</cell></row><row><cell>Optimizer</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,88.99,90.49,385.87,213.33"><head>Table 3</head><label>3</label><figDesc>The scores of our Image Retrieval systems on our development set</figDesc><table coords="9,120.41,122.10,354.46,181.72"><row><cell>ID</cell><cell>Approach</cell><cell>Caluculation</cell><cell>BLEU-4 score</cell></row><row><cell>ex01</cell><cell>Image Retrieval with DenseNet121</cell><cell>Cosine Similarity</cell><cell>0.025</cell></row><row><cell>ex02</cell><cell>Image Retrieval with EfficientNetB0</cell><cell>Cosine Similarity</cell><cell>0.027</cell></row><row><cell>ex03</cell><cell>Image Retrieval with EfficientNetB7</cell><cell>Cosine Similarity</cell><cell>0.026</cell></row><row><cell>ex04</cell><cell>Image Retrieval with DenseNet201</cell><cell>Cosine Similarity</cell><cell>0.028</cell></row><row><cell>ex05</cell><cell>Image Retrieval with ResNet-50</cell><cell>Cosine Similarity</cell><cell>0.026</cell></row><row><cell>ex06</cell><cell>Image Retrieval with ResNet-152</cell><cell>Cosine Similarity</cell><cell>0.026</cell></row><row><cell>ex07</cell><cell>Image Retrieval with Xception</cell><cell>Cosine Similarity</cell><cell>0.025</cell></row><row><cell cols="2">ex08 Image Retrieval with InceptionResNetV2</cell><cell>Cosine Similarity</cell><cell>0.023</cell></row><row><cell>ex09</cell><cell>Image Retrieval with NasNet Large</cell><cell>Cosine Similarity</cell><cell>0.022</cell></row><row><cell>ex10</cell><cell>Image Retrieval with InceptionV3</cell><cell>Cosine Similarity</cell><cell>0.026</cell></row><row><cell cols="2">ex11 Ensemble1 (ex01, ex02, ex04, ex05, ex06)</cell><cell>Cosine Similarity</cell><cell>0.031</cell></row><row><cell cols="2">ex12 Ensemble2 (ex01, ex02, ex03, ex04, ex07)</cell><cell>Cosine Similarity</cell><cell>0.029</cell></row><row><cell cols="3">ex13 Ensemble1 (ex01, ex02, ex04, ex05, ex06) Euclidean Distance</cell><cell>0.033</cell></row><row><cell cols="3">ex14 Ensemble2 (ex01, ex02, ex03, ex04, ex07) Euclidean Distance</cell><cell>0.031</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,88.99,326.08,374.27,141.60"><head>Table 4</head><label>4</label><figDesc>The scores of our approaches on our development set</figDesc><table coords="9,132.01,357.69,331.26,109.99"><row><cell>Approach</cell><cell cols="2">Image Preprocessing BLEU-4 score</cell></row><row><cell>Show, Attend and Tell with ResNet-101</cell><cell>None</cell><cell>0.092</cell></row><row><cell>Show, Attend and Tell with ResNet-101</cell><cell>Pseudo Colorization</cell><cell>0.090</cell></row><row><cell>Show, Attend and Tell with DenseNet201</cell><cell>None</cell><cell>0.095</cell></row><row><cell cols="2">Show, Attend and Tell with DenseNet201 Pseudo Colorization</cell><cell>0.091</cell></row><row><cell>X-VLM</cell><cell>None</cell><cell>0.055</cell></row><row><cell>X-VLM</cell><cell>Pseudo Colorization</cell><cell>0.061</cell></row><row><cell>Combination of Retrieval and DNN</cell><cell>None</cell><cell>0.056</cell></row><row><cell>Combination of Retrieval and DNN</cell><cell>Pseudo Colorization</cell><cell>0.054</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,88.99,90.49,416.96,141.60"><head>Table 5</head><label>5</label><figDesc>The scores of all of systems on ImageCLEFmedical 2022 Test set</figDesc><table coords="10,95.27,122.10,410.69,109.98"><row><cell>Approach</cell><cell cols="4">Image Preprocessing BLEU-4 ROUGE Run ID</cell></row><row><cell>Ensemble1 Retrieval with Cosine Similarity</cell><cell>None</cell><cell>0.268</cell><cell>0.153</cell><cell>181904</cell></row><row><cell>Ensemble2 Retrieval with Cosine Similarity</cell><cell>None</cell><cell>0.271</cell><cell>0.141</cell><cell>181908</cell></row><row><cell>Ensemble1 Retrieval with Euclidean Distance</cell><cell>None</cell><cell>0.278</cell><cell>0.158</cell><cell>182351</cell></row><row><cell>Show, Attend and Tell with ResNet-101</cell><cell>None</cell><cell>0.221</cell><cell>0.154</cell><cell>181901</cell></row><row><cell>Show, Attend and Tell with ResNet-101</cell><cell>Pseudo Colorization</cell><cell>0.218</cell><cell>0.150</cell><cell>181903</cell></row><row><cell>Show, Attend and Tell with DenseNet201</cell><cell>None</cell><cell>0.222</cell><cell>0.157</cell><cell>182347</cell></row><row><cell>X-VLM</cell><cell>Pseudo Colorization</cell><cell>0.191</cell><cell>0.154</cell><cell>181946</cell></row><row><cell>Combination of Retrieval and DNN</cell><cell>None</cell><cell>0.187</cell><cell>0.113</cell><cell>182165</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>A part of this research was carried out with the support of the <rs type="grantName">Grant for Toyohashi Heart Center Smart Hospital Joint Research Course</rs> and the <rs type="grantName">Grant-in-Aid for Scientific Research</rs> (C) (issue numbers <rs type="grantNumber">22K12149</rs> and <rs type="grantNumber">22K12040</rs>)</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_WB7Usf3">
					<orgName type="grant-name">Grant for Toyohashi Heart Center Smart Hospital Joint Research Course</orgName>
				</org>
				<org type="funding" xml:id="_zk9rFN7">
					<idno type="grant-number">22K12149</idno>
					<orgName type="grant-name">Grant-in-Aid for Scientific Research</orgName>
				</org>
				<org type="funding" xml:id="_6QC32CQ">
					<idno type="grant-number">22K12040</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,604.43,394.53,10.91;9,112.66,617.98,269.34,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,112.66,617.98,192.22,10.91">Microsoft coco: Common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,327.86,617.98,22.98,10.91">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,631.52,395.01,10.91;9,112.66,645.07,395.17,10.91;9,112.39,658.62,394.80,10.91;10,112.66,258.80,394.62,10.91;10,112.66,272.35,393.33,10.91;10,112.66,285.90,395.17,10.91;10,112.66,299.44,393.54,10.91;10,112.66,312.99,170.14,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,352.50,258.80,154.78,10.91;10,112.66,272.35,300.10,10.91">Overview of the ImageCLEF 2022: Multimedia retrieval in medical, social media and nature applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,435.26,272.35,70.73,10.91;10,112.66,285.90,395.17,10.91;10,112.66,299.44,239.58,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<title level="s" coord="10,359.20,299.44,147.00,10.91;10,112.66,312.99,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,326.54,395.17,10.91;10,111.81,340.09,395.37,10.91;10,112.66,353.64,393.33,10.91;10,112.66,367.19,216.46,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,326.22,340.09,180.96,10.91;10,112.66,353.64,177.87,10.91">Overview of ImageCLEFmedical 2022caption prediction and concept detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,313.28,353.64,192.71,10.91;10,112.66,367.19,97.38,10.91">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,380.74,393.33,10.91;10,112.66,394.29,181.11,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,242.31,380.74,263.68,10.91;10,112.66,394.29,51.39,10.91">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,407.84,394.53,10.91;10,112.66,421.39,393.33,10.91;10,112.66,434.94,395.17,10.91;10,112.30,448.49,233.39,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,481.24,407.84,25.95,10.91;10,112.66,421.39,305.83,10.91">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,441.14,421.39,64.85,10.91;10,112.66,434.94,395.17,10.91;10,164.20,448.49,35.59,10.91">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
	<note>ICML&apos;15</note>
</biblStruct>

<biblStruct coords="10,112.66,462.04,394.53,10.91;10,112.66,475.58,394.53,10.91;10,112.66,489.13,393.32,10.91;10,112.66,502.68,394.03,10.91;10,112.66,516.23,278.51,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,176.01,475.58,105.33,10.91">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="s" coord="10,314.47,489.13,191.52,10.91;10,112.66,502.68,34.49,10.91">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,529.78,393.98,10.91;10,112.41,543.33,48.96,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,250.98,529.78,114.99,10.91">Long Short-Term Memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,375.74,529.78,93.13,10.91">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,556.88,393.33,10.91;10,112.66,570.43,395.01,10.91;10,112.66,583.98,207.13,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="10,456.96,556.88,49.03,10.91;10,112.66,570.43,345.19,10.91">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00636</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,597.53,393.33,10.91;10,112.26,611.08,394.93,10.91;10,112.66,624.63,395.01,10.91;10,112.66,638.17,76.35,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,283.82,597.53,222.17,10.91;10,112.26,611.08,135.67,10.91">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/conf/nips/nips2015.html#RenHGS15" />
	</analytic>
	<monogr>
		<title level="m" coord="10,187.57,624.63,19.18,10.91">NIPS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,651.72,393.33,10.91;10,112.66,665.27,394.52,10.91;11,112.66,86.97,237.02,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,453.81,651.72,52.18,10.91;10,112.66,665.27,84.44,10.91">Vqa: Visual question answering</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.279</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,220.42,665.27,282.02,10.91">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,100.52,394.53,10.91;11,112.66,114.06,395.17,10.91;11,112.66,127.61,395.01,10.91;11,112.66,141.16,138.14,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,236.23,100.52,266.59,10.91">On the automatic generation of medical imaging reports</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p18-1240</idno>
		<ptr target="http://dx.doi.org/10.18653/v1/P18-1240.doi:10.18653/v1/p18-1240" />
	</analytic>
	<monogr>
		<title level="m" coord="11,112.66,114.06,395.17,10.91;11,112.66,127.61,34.46,10.91">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="11,112.66,154.71,393.33,10.91;11,112.66,168.26,393.33,10.91;11,112.33,181.81,286.88,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,263.50,154.71,242.49,10.91;11,112.66,168.26,44.03,10.91">Deep modular co-attention networks for visual question answering</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00644</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,203.26,168.26,302.72,10.91;11,112.33,181.81,30.22,10.91">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="6274" to="6283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,195.36,395.01,10.91;11,112.66,208.91,394.53,10.91;11,112.48,222.46,395.18,10.91;11,112.66,236.01,394.53,10.91;11,112.28,249.56,393.70,10.91;11,112.66,263.11,393.33,10.91;11,112.66,276.66,393.33,10.91;11,112.66,290.20,393.53,10.91;11,112.66,303.75,197.61,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,264.32,249.56,241.66,10.91;11,112.66,263.11,261.75,10.91">Overview of the ImageCLEF 2021: Multimedia retrieval in medical, nature, internet and social media applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tauteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,401.27,263.11,104.72,10.91;11,112.66,276.66,393.33,10.91;11,112.66,290.20,201.15,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="11,348.61,290.20,157.57,10.91;11,112.66,303.75,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,317.30,394.62,10.91;11,112.28,330.85,318.45,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,349.60,317.30,157.67,10.91;11,112.28,330.85,123.60,10.91">Radiology objects in context (roco): A multimodal image dataset</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,258.68,330.85,140.76,10.91">CVII-STENT/LABELS@MICCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,344.40,373.15,10.91" xml:id="b14">
	<monogr>
		<ptr target="https://github.com/itseez/opencv" />
		<title level="m" coord="11,112.66,344.40,192.17,10.91">Itseez, Open source computer vision library</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,357.95,393.33,10.91;11,112.66,371.50,394.62,10.91;11,112.66,385.05,174.71,10.91" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="11,478.16,357.95,27.83,10.91;11,112.66,371.50,205.93,10.91">AUEB NLP Group at ImageCLEFmed Caption Tasks</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Charalampakos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/#paper-96" />
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="1184" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,398.60,393.32,10.91;11,112.66,412.15,395.01,10.91;11,112.66,425.70,199.39,10.91" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="11,280.31,398.60,225.67,10.91;11,112.66,412.15,254.78,10.91">Puc chile team at caption prediction: Resnet visual encoding and caption classification with parametric relu</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lobel</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/#paper-95" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1174" to="1183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,439.25,393.33,10.91;11,112.66,452.79,394.53,10.91;11,112.66,466.34,237.02,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,359.69,439.25,146.30,10.91;11,112.66,452.79,38.41,10.91">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,196.50,452.79,305.75,10.91">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,479.89,393.32,10.91;11,112.66,493.44,395.00,10.91;11,112.66,506.99,137.64,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="11,254.59,479.89,207.56,10.91">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,112.66,493.44,307.90,10.91">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,520.54,394.52,10.91;11,112.66,534.09,393.33,10.91;11,112.66,547.64,394.52,10.91;11,112.66,561.19,299.32,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="11,178.09,520.54,324.20,10.91">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v97/tan19a.html" />
	</analytic>
	<monogr>
		<title level="m" coord="11,293.89,534.09,212.10,10.91;11,112.66,547.64,90.84,10.91;11,269.41,548.65,176.97,9.72">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="11,112.66,574.74,393.33,10.91;11,112.66,588.29,393.33,10.91;11,112.33,601.84,275.01,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="11,344.36,574.74,161.63,10.91;11,112.66,588.29,82.87,10.91">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.308</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,216.12,588.29,289.87,10.91;11,112.33,601.84,30.22,10.91">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,615.39,393.32,10.91;11,112.66,628.93,395.01,10.91;11,112.66,642.48,143.58,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="11,160.89,615.39,278.43,10.91">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.195</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,462.22,615.39,43.76,10.91;11,112.66,628.93,294.34,10.91">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,656.03,393.33,10.91;11,112.66,669.58,393.33,10.91;12,112.66,86.97,359.26,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="11,331.70,656.03,174.29,10.91;11,112.66,669.58,194.75,10.91">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,334.23,669.58,171.76,10.91;12,112.66,86.97,203.53,10.91">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI&apos;17</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence, AAAI&apos;17</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,100.52,393.33,10.91;12,112.66,114.06,393.33,10.91;12,112.66,127.61,307.68,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="12,298.90,100.52,207.09,10.91;12,112.66,114.06,81.43,10.91">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00907</idno>
	</analytic>
	<monogr>
		<title level="m" coord="12,248.63,114.06,257.35,10.91;12,112.66,127.61,51.39,10.91">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,141.16,393.33,10.91;12,112.26,154.71,280.29,10.91" xml:id="b24">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.08276</idno>
		<title level="m" coord="12,228.71,141.16,277.28,10.91;12,112.26,154.71,92.76,10.91">Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>pre-print</note>
</biblStruct>

<biblStruct coords="12,112.66,168.26,393.33,10.91;12,112.33,181.81,29.19,10.91" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="12,238.21,168.26,167.55,10.91">A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
