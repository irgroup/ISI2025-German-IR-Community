<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,416.69,15.42;1,89.29,106.66,401.86,15.42;1,89.29,128.58,256.01,15.43">ImageSem Group at ImageCLEFmedical Caption 2022 task: Generating Medical Image Descriptions based on Vision-Language Pre-training</title>
				<funder ref="#_feuw9j5">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_JfDhNSj">
					<orgName type="full">Beijing Natural Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.90,156.89,66.46,11.96"><forename type="first">Xuwen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Medical Information and Library</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Medical Sciences</orgName>
								<orgName type="institution" key="instit2">Peking Union Medical College</orgName>
								<address>
									<postCode>100020</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,168.00,156.89,31.12,11.96"><forename type="first">Jiao</forename><surname>Li</surname></persName>
							<email>li.jiao@imicams.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Medical Information and Library</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Medical Sciences</orgName>
								<orgName type="institution" key="instit2">Peking Union Medical College</orgName>
								<address>
									<postCode>100020</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,416.69,15.42;1,89.29,106.66,401.86,15.42;1,89.29,128.58,256.01,15.43">ImageSem Group at ImageCLEFmedical Caption 2022 task: Generating Medical Image Descriptions based on Vision-Language Pre-training</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">D79E5D4D9837857DD442186A183B451B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Caption prediction</term>
					<term>vision-language pre-training</term>
					<term>bootstrapping</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the work of ImageSem group in the ImageCLEFmedical Caption 2022 task. In the caption prediction subtask, we employed the bootstrapping language-image pre-training (BLIP) framework for generating medical image descriptions. We submitted 2 runs using exclusively the official training data and achieved the BLEU score of 0.2211, which ranked 9th among the participating teams. Despite the lower BLEU score, we achieved a relatively balanced performance across all other metrics, such as the ROUGE of 0.1847, the METEOR of 0.0675, the CIDEr of 0.2513, the SPICE of 0.0393 and the BERTScore of 0.6059, showing the potential of better verbal fluency via fine-tuning the medical caption generation task grounded by vision-language pre-training (VLP).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As a classic track of ImageCLEF benchmark <ref type="bibr" coords="1,273.60,412.46,14.79,10.91" target="#b0">[1]</ref>, the ImageCLEFmedical Caption task <ref type="bibr" coords="1,456.34,412.46,12.68,10.91" target="#b1">[2]</ref> consists of two subtasks, namely Concept Detection and Caption Prediction. On behalf of the Institute of Medical Information and Library, Chinese Academy of Medical Sciences, our Image Semantics group (MAI_ImageSem) participated in both of the two subtasks in the last 4 years <ref type="bibr" coords="1,441.27,453.11,12.75,10.91" target="#b2">[3]</ref>, <ref type="bibr" coords="1,460.57,453.11,11.28,10.91" target="#b3">[4]</ref>. In this year, we focus on the Caption Prediction subtask, which asks participants to generate coherent captions for the entirety of an image, and requires higher accuracy and semantic interpretability of expression. For predicting fluent medical image captions, we employed BLIP <ref type="bibr" coords="1,440.74,493.75,13.80,10.91" target="#b4">[5]</ref>, a recently released Bootstrapping Language-Image Pre-training framework that transfers flexibly to both vision-language understanding and generation tasks. This paper is organized as follows. Section 2 describes the dataset of the ImageCLEFmedical Caption 2022 task. Section 3 presents our methods for caption prediction. Section 4 lists our submitted runs. Section 5 makes a brief summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Dataset</head><p>The ImageCLEFmedical Caption 2022 track released an extended version of the ImageCLEF 2020 dataset, including 83,275 radiology images as training set, 7,645 radiology images as validation set, and 7,601 radiology images as test set. Each image is associated with UMLS (the Unified Medical Language System) <ref type="bibr" coords="2,209.48,151.93,12.72,10.91" target="#b5">[6]</ref> concepts and image captions originated from the PMC (PubMed Central) biomedical articles. For the caption prediction task, each caption is pre-processed, such as removing numbers and punctuation, lower-case, lemmatization, etc.</p><p>Table <ref type="table" coords="2,127.33,192.57,5.13,10.91" target="#tab_0">1</ref> shows the statistic of caption lengths (i.e. the number of words in a caption) in the official training set and validation set. It can be seen that the caption length of different images vary a lot, e.g., in training set, the caption length ranges from the minimum of 1 word to the max of 410, and in validation set, it ranges from 1 to 297. The mean caption length in training set is 16, while in validation set is 17. In addition to the influence of preprocessing operations, the various image context from original PMC articles lead to the significant differences in caption lengths, which is a challenge for accurate caption prediction. Clinically, captions containing only 1 word may provide poor semantic information, such as "angiography", "radiograph", "xray", etc. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>This section introduces our methods in the Caption Prediction subtask of ImageCLEFmedical Caption 2022 task. Figure <ref type="figure" coords="2,205.47,486.96,5.07,10.91">3</ref> shows our workflow, which is described in detail in section 3.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation</head><p>In the ImageCLEFmed Caption 2021 track, we employed two methods for caption prediction subtask. One was a pattern-based caption generation strategy, which combined the UMLS concepts identified from medical images with a predefined sentence pattern. The experimental results showed that the quality of generated sentences depended heavily on the accuracy of concept detection. The other method was an image-matching based model proposed by Zheng <ref type="bibr" coords="3,114.14,175.28,14.91,10.91" target="#b6">[7]</ref>, which utilized two convolutional neural networks <ref type="bibr" coords="3,366.26,175.28,15.14,10.91" target="#b7">[8]</ref> to learn visual and textual representations simultaneously. It was based on an unsupervised assumption that every image or text group can be viewed as one class, so each category is equivalent to 1+m samples (i.e. 1 image vs m descriptions). The experimental results showed that the weakly matched medical images and captions may produce noisy or irrelevant descriptions.</p><p>The high-quality datasets of medical image-text pairs released by ImageCLEF provide opportunities for researchers to validate different ideas and methods. In the ImageCLEFmedical Caption 2022 track, we attempt to use a recently released vision-language pre-training (VLP) framework <ref type="bibr" coords="3,141.07,283.68,12.84,10.91" target="#b4">[5]</ref> to generate coherent and expressive descriptions for medical images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Vision-Language Pre-training-based Caption Prediction</head><p>For generating fluent and reasonable medical image captions, we employed the BLIP (Bootstrapping Language-Image Pre-training) model proposed by Li et al. <ref type="bibr" coords="3,405.41,346.95,11.59,10.91" target="#b4">[5]</ref>, which is a unified pre-training framework for vision-language understanding and generation downstream tasks. One contribution of BLIP is introducing multimodal Mixture of Encoder-Decoder (MED), a multi-task model which can operate either as a unimodal encoder, or an image-grounded text encoder, or an image-grounded text decoder. The model is jointly pre-trained with three visionlanguage objectives, including image-text contrastive learning, image-text matching, and image conditioned language modeling. Another highlight of BLIP is CapFilt (Captioning and Filtering) <ref type="bibr" coords="3,89.29,441.80,11.58,10.91" target="#b4">[5]</ref>, a dataset bootstrapping method for learning from noisy image-text pairs. A pre-trained MED is fine-tuned into two modules, including a Captioner (based on the Image-grounded text decoder) to produce synthetic captions given images, and a Filter (based on the Image-grounded text encoder) to remove noisy captions. We also use this bootstrapping mechanism to expand our training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Data Preparation</head><p>We processed the official dataset according to the data format requirements of BLIP <ref type="bibr" coords="3,466.06,545.32,11.50,10.91" target="#b4">[5]</ref>. First, the file paths were filtered and image-caption samples were retained with complete image path. Second, the images from training set were resized to 224*224 pixels for pre-training, and images from validation set as well as test set were resized to 384*384 pixels for caption prediction. Third, to meet the requirement of BLIP's PyTorch frameswork that figure ID should be an integer without characters, we randomly converted original figure IDs to INT16 data type as the unique identification of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Experiments</head><p>In our work, we employed MED as the backbone framework. We tried two pre-training strategies. The first one performed vision-language pre-training from scratch based on the official training set (83,275 image-caption pairs) of ImageCLEFmedical Caption 2022, and then fine-tuned the caption prediction task based on the official validation set (7,645 image-caption pairs). The second one utilized the original BLIP model pre-trained on general datasets (with 14M images in total) <ref type="bibr" coords="4,127.56,175.28,12.95,10.91" target="#b4">[5]</ref> as initial parameters, and performed secondary pre-training based on the official training set. Then we also fine-tune the caption prediction task on the basis of validation set.</p><p>Due to limited computing resources, it took us a long time to complete all the pre-training processes. A preliminary analysis shows that the model pre-trained from scratch performed poorly on the downstream tasks, since it was hard to learn sufficient image-language association in the pre-training process, which subject to the limited scale of training data, as well as the diverse types of medical images and uneven captions. On the other hand, the secondary pretraining model with the BLIP initial parameters showed better performance on the downstream tasks, so we submitted the caption prediction results based on the secondary pre-training model to the subtask. We also applied the data bootstrapping process to predict synthetic captions for expanding training dataset. However, since the initial dataset contains a total of 83,275 image-text pairs, a limited number of high-quality captions were obtained by bootstrapping, which yield marginal improvement in the next round of caption prediction.</p><p>The experimental framework was implemented in PyTorch <ref type="bibr" coords="4,359.09,351.42,14.83,10.91" target="#b8">[9]</ref> and pre-trained on 8 NVIDIA V100 GPUs. We used ViT-B <ref type="bibr" coords="4,210.30,364.97,19.61,10.91" target="#b9">[10]</ref> as the image transformer, and the text transformer initialized from BERTbase <ref type="bibr" coords="4,152.49,378.52,19.73,10.91" target="#b10">[11]</ref>. Our model was pre-trained for 40 epochs using a batch size of 24. We used AdamW <ref type="bibr" coords="4,119.54,392.07,24.50,10.91" target="#b11">[12]</ref> optimizer with a weight decay of 0.05. The learning rate was warmed-up to 3e-5 and decayed linearly with a rate of 0.85. With this experimental setting, we fine-tuned caption prediction task on the validation set and achieved a best BLEU score of 0.2344.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Submitted runs</head><p>Table <ref type="table" coords="4,116.12,477.79,5.14,10.91" target="#tab_1">2</ref> shows our submissions on the caption prediction subtask. Our best run is ID_182105, which use the pre-trained BLIP as initial parameter and perform secondary pre-training on the official training set. The hyper parameters of this submission were consistent with experimental settings in section 3.2.2, and achieved a BLEU score of 0.2211, ranked 9th in the leaderboard. We also achieved a relatively balanced performance across all other metrics, such as the ROUGE of 0.1847, the METEOR of 0.0675, the CIDEr of 0.2513, the SPICE of 0.0393 and the BERTScore of 0.6059, showing the potential of better verbal fluency. The submission ID_182348 refers to the secondary pre-training method with only 20 training epochs using a batch size of 24. It has achieved a BLEU score of 0.2179, the ROUGE of 0.1791, the METEOR of 0.0661, the CIDEr of 0.2304, the SPICE of 0.0371 and the BERTScore of 0.6013.</p><p>Figure <ref type="figure" coords="4,130.96,613.29,5.02,10.91" target="#fig_1">2</ref> shows a few examples <ref type="bibr" coords="4,239.29,613.29,16.36,10.91" target="#b12">[13,</ref><ref type="bibr" coords="4,258.38,613.29,12.52,10.91" target="#b13">14,</ref><ref type="bibr" coords="4,273.62,613.29,12.52,10.91" target="#b14">15,</ref><ref type="bibr" coords="4,288.87,613.29,13.99,10.91" target="#b15">16]</ref> of our best run model on the validation set. It can be observed that, compared with Ground Truth, the predicted captions prefer to describe medical images from the global perspective. However, due to a lack of context or patient-related information, some descriptions are semantically incomplete or unclear, e.g. our model describes figure valid_084402 <ref type="bibr" coords="4,178.11,667.48,17.88,10.91" target="#b15">[16]</ref> as "computed tomography ct scan of the head and neck with contrast show a soft tissue", while the GT clearly state the nature of the lesion is "emphysema in the nasopharynx".  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This paper presents the work of ImageSem Group at the ImageCLEFmedical Caption 2022 task. We employed the bootstrapping language-image pre-training framework for the caption prediction subtask, and achieved a relatively balanced performance across all metrics. For further work, it would be helpful to multiple downstream tasks such as caption prediction by collecting more high-quality medical image-caption pairs, as well as introducing imaging diagnosis knowledge of specific diseases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,628.56,351.82,8.93;2,89.29,509.67,416.70,106.32"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Workflow of ImageSem Group in the ImageCLEFmedical Caption 2022 task</figDesc><graphic coords="2,89.29,509.67,416.70,106.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,89.29,460.83,416.70,8.93;5,89.29,472.83,417.29,8.87;5,89.29,227.31,416.69,226.93"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples from the official validation set with captions predicted by our best run model (using the pre-trained BLIP as initial parameter and perform secondary pre-training on the official training set)</figDesc><graphic coords="5,89.29,227.31,416.69,226.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,88.99,329.92,417.00,79.60"><head>Table 1</head><label>1</label><figDesc>Statistic of caption lengths in the training set and validation set of ImageCLEFmedical Caption 2022 task.</figDesc><table coords="2,134.65,371.27,325.98,38.25"><row><cell>Dataset</cell><cell cols="4">Image Caption Pairs Max Length Min Length Mean Length</cell></row><row><cell>Training</cell><cell>83,275</cell><cell>410</cell><cell>1</cell><cell>16</cell></row><row><cell>Validation</cell><cell>7,645</cell><cell>297</cell><cell>1</cell><cell>17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.99,129.52,401.69,69.87"><head>Table 2</head><label>2</label><figDesc>Submissions of MAI_ImageSem group in the caption prediction subtask</figDesc><table coords="5,104.59,161.14,386.09,38.25"><row><cell>Submission ID</cell><cell>BLEU</cell><cell>ROUGE METEOR</cell><cell>CIDEr</cell><cell>SPICE</cell><cell cols="2">BERTScore Rank</cell></row><row><cell>182105</cell><cell cols="4">0.221136 0.184723 0.067541 0.251316 0.039311</cell><cell>0.605873</cell><cell>9</cell></row><row><cell>182348</cell><cell cols="4">0.217935 0.179142 0.066132 0.230447 0.037081</cell><cell>0.601299</cell><cell>-</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has been supported by the <rs type="funder">National Natural Science Foundation of China</rs> (Grant No. <rs type="grantNumber">61906214</rs>), the <rs type="funder">Beijing Natural Science Foundation</rs> (Grant No. <rs type="grantNumber">Z200016</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_feuw9j5">
					<idno type="grant-number">61906214</idno>
				</org>
				<org type="funding" xml:id="_JfDhNSj">
					<idno type="grant-number">Z200016</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,112.66,183.45,395.01,10.91;6,112.66,197.00,395.17,10.91;6,112.39,210.55,394.80,10.91;6,112.66,224.10,394.62,10.91;6,112.66,237.65,393.33,10.91;6,112.66,251.20,395.17,10.91;6,112.66,264.75,393.54,10.91;6,112.66,278.30,170.14,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,352.50,224.10,154.78,10.91;6,112.66,237.65,311.34,10.91">Overview of the ImageCLEF 2022: Multimedia Retrieval in Medical, Social Media and Nature Applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,446.87,237.65,59.11,10.91;6,112.66,251.20,395.17,10.91;6,112.66,264.75,239.58,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<title level="s" coord="6,359.20,264.75,147.00,10.91;6,112.66,278.30,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,291.85,395.17,10.91;6,111.81,305.40,395.37,10.91;6,112.66,318.95,393.33,10.91;6,112.66,332.50,216.46,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,326.22,305.40,180.96,10.91;6,112.66,318.95,181.70,10.91">Overview of ImageCLEFmedical 2022 -Caption Prediction and Concept Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,316.25,318.95,189.74,10.91;6,112.66,332.50,97.38,10.91">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,346.05,393.33,10.91;6,112.66,359.59,393.33,10.91;6,112.66,373.14,246.28,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,255.12,346.05,250.87,10.91;6,112.66,359.59,154.44,10.91">Imagesem at imageclefmed caption 2019 task: a two-stage medical concept detection strategy</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,290.06,359.59,132.34,10.91">ImageClef2019 working notes</title>
		<title level="s" coord="6,429.58,359.59,76.40,10.91;6,112.66,373.14,97.38,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,386.69,393.33,10.91;6,112.66,400.24,394.53,10.91;6,112.66,413.79,168.75,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,256.86,386.69,249.13,10.91;6,112.66,400.24,93.64,10.91">Imagesem at imageclef 2018 caption task: Image retrieval and transfer learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,229.58,400.24,133.14,10.91">ImageClef2018 working notes</title>
		<title level="s" coord="6,369.93,400.24,137.26,10.91;6,112.66,413.79,38.13,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,427.34,393.33,10.91;6,112.39,440.89,395.27,10.91;6,112.41,454.44,27.76,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="6,239.04,427.34,266.95,10.91;6,112.39,440.89,210.81,10.91">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2201.12086" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,467.99,393.33,10.91;6,112.66,481.54,376.61,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,190.92,467.99,315.07,10.91;6,112.66,481.54,51.86,10.91">The unified medical language system (umls): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gkh061</idno>
	</analytic>
	<monogr>
		<title level="j" coord="6,173.16,481.54,103.68,10.91">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,495.09,395.17,10.91;6,112.66,508.64,393.33,10.91;6,112.66,522.18,380.01,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,369.00,495.09,138.83,10.91;6,112.66,508.64,147.30,10.91">Dual-path convolutional imagetext embedding with instance loss</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-D</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3383184</idno>
		<ptr target="https://doi.org/10.1145/3383184" />
	</analytic>
	<monogr>
		<title level="j" coord="6,267.67,508.64,238.32,10.91;6,112.66,522.18,118.52,10.91">Communications, and Applications ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,535.73,393.33,10.91;6,112.66,549.28,277.00,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="6,209.44,535.73,296.54,10.91;6,112.66,549.28,209.19,10.91">Convolutional networks for images, speech, and time series, The handbook of brain theory and neural networks</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,562.83,394.53,10.91;6,112.66,576.38,394.52,10.91;6,112.66,589.93,394.53,10.91;6,112.66,603.48,313.39,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,371.36,589.93,135.83,10.91;6,112.66,603.48,174.61,10.91">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,295.18,603.48,36.79,10.91">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,617.03,395.16,10.91;6,112.66,630.58,393.33,10.91;6,112.41,644.13,317.06,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<title level="m" coord="6,421.13,630.58,84.86,10.91;6,112.41,644.13,254.77,10.91">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,86.97,393.33,10.91;7,112.66,100.52,306.23,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,327.64,86.97,178.35,10.91;7,112.66,100.52,181.08,10.91">Bert: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,302.46,100.52,35.23,10.91">NAACL</title>
		<imprint>
			<biblScope unit="page" from="4171" to="4186" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,114.06,395.00,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,218.06,114.06,170.15,10.91">Decoupled weight decay regularization</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,396.39,114.06,79.80,10.91">Computer Science</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,127.61,393.71,10.91;7,112.66,141.16,395.01,10.91;7,112.66,154.71,199.75,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Adhikari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Manduva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">V</forename><surname>Malayala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Deepika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Koritala</surname></persName>
		</author>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8435398/" />
		<title level="m" coord="7,498.63,127.61,7.73,10.91;7,112.66,141.16,277.47,10.91">A rare case of vaping-induced spontaneous pneumomediastinum</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,168.26,393.33,10.91;7,112.66,181.81,395.01,10.91;7,112.66,195.36,141.48,10.91" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="7,320.73,168.26,185.26,10.91;7,112.66,181.81,213.41,10.91">A ct study of the femoral and sciatic nerve periacetabular moving in different hip positions</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Isin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Hapa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Kara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">I</forename><surname>Kilic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Balcı</surname></persName>
		</author>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7488403/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,208.91,393.33,10.91;7,112.66,222.46,394.03,10.91;7,112.66,236.01,63.53,10.91" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="7,371.99,208.91,133.99,10.91;7,112.66,222.46,148.11,10.91">Carcinoma cervix leading to ichthyosis uteri: A rare case report</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Vijaywargiya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kachhara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Chahwala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ruia</surname></persName>
		</author>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8440718/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,249.56,393.33,10.91;7,112.66,263.11,394.03,10.91;7,112.66,276.66,63.53,10.91" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Urushidani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kuriyama</surname></persName>
		</author>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8245747/" />
		<title level="m" coord="7,239.70,249.56,266.29,10.91;7,112.66,263.11,146.97,10.91">A sudden decrease in voice volume: A rare manifestation of spontaneous pneumomediastinum</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
