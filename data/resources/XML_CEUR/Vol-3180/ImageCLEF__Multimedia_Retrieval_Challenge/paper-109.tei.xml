<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,83.21,335.36,17.22">CSIRO at ImageCLEFmedical Caption 2022</title>
				<funder>
					<orgName type="full">CSIRO&apos;s Machine Learning and Artificial Intelligence Future Science Platform</orgName>
					<orgName type="abbreviated">FSP</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,115.31,51.99,11.96"><forename type="first">Leo</forename><surname>Lebrat</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Imaging and Computer Vision Group</orgName>
								<orgName type="institution">Commonwealth Scientific and Industrial Research Organisation</orgName>
								<address>
									<addrLine>Pullenvale</addrLine>
									<postCode>4069</postCode>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<postCode>4000</postCode>
									<settlement>Brisbane</settlement>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,167.34,115.31,76.54,11.96"><forename type="first">Aaron</forename><surname>Nicolson</surname></persName>
							<email>aaron.nicolson@csiro.au</email>
							<affiliation key="aff3">
								<orgName type="department">Australian e-Health Research Centre</orgName>
								<orgName type="institution">Commonwealth Scientific and Industrial Research Organisation</orgName>
								<address>
									<postCode>4006</postCode>
									<settlement>Herston</settlement>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,264.65,115.31,95.41,11.96"><forename type="first">Rodrigo</forename><forename type="middle">Santa</forename><surname>Cruz</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Imaging and Computer Vision Group</orgName>
								<orgName type="institution">Commonwealth Scientific and Industrial Research Organisation</orgName>
								<address>
									<addrLine>Pullenvale</addrLine>
									<postCode>4069</postCode>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<postCode>4000</postCode>
									<settlement>Brisbane</settlement>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,377.99,115.31,65.39,11.96"><forename type="first">Gregg</forename><surname>Belous</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Australian e-Health Research Centre</orgName>
								<orgName type="institution">Commonwealth Scientific and Industrial Research Organisation</orgName>
								<address>
									<postCode>4006</postCode>
									<settlement>Herston</settlement>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,129.26,80.27,11.96"><forename type="first">Bevan</forename><surname>Koopman</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Australian e-Health Research Centre</orgName>
								<orgName type="institution">Commonwealth Scientific and Industrial Research Organisation</orgName>
								<address>
									<postCode>4006</postCode>
									<settlement>Herston</settlement>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,200.55,129.26,71.58,11.96"><forename type="first">Jason</forename><surname>Dowling</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Australian e-Health Research Centre</orgName>
								<orgName type="institution">Commonwealth Scientific and Industrial Research Organisation</orgName>
								<address>
									<postCode>4006</postCode>
									<settlement>Herston</settlement>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Ôä∞ Equal contribution</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,83.21,335.36,17.22">CSIRO at ImageCLEFmedical Caption 2022</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">85E76CF10E1DD23A17ABD0CB38FF7827</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical concept detection</term>
					<term>Medical caption prediction</term>
					<term>Multimodal learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe the participation of team CSIRO in the ImageCLEFmedical Caption task of 2022. This task consisted of two subtasks: concept detection and caption prediction. Concept detection involved identifying medical concepts within a given medical image. To accomplish this, we employed an ensemble of DenseNets with threshold tuning. CSIRO placed third amongst the participating teams with an F1 score of 0.447. For caption prediction, the task was to compose a coherent caption for a given medical image. We employed an encoder-to-decoder model with the Convolutional vision Transformer (CvT) as the encoder and DistilGPT2 as the decoder. CSIRO placed third amongst the participating teams with a BLEU score of 0.311.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Interpreting medical images is a complex and labour-intensive task. To become proficient requires a significant amount of training. A radiologist must be able to interpret medical concepts and their interplay from the image. On top of this, the workload of radiologists has increased significantly over the last couple of decades -mostly due to increases in crosssectional imaging and under-staffing <ref type="bibr" coords="1,251.90,490.63,11.23,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,265.74,490.63,7.49,10.91" target="#b1">2]</ref>. This leads to the need for an automated system that can produce textual descriptions of medical images. Such a system could improve the efficiency of interpretation and report creation -potentially reducing the workload and improving patient care. Such a system could also increase the diagnostic accuracy of non-specialist clinicians who have a lower diagnostic confidence <ref type="bibr" coords="1,247.97,544.83,11.36,10.91" target="#b2">[3,</ref><ref type="bibr" coords="1,262.06,544.83,7.57,10.91" target="#b3">4]</ref>.</p><p>The ImageCLEFmedical Caption task of 2022 <ref type="bibr" coords="1,308.34,558.38,11.48,10.91" target="#b4">[5,</ref><ref type="bibr" coords="1,322.86,558.38,9.03,10.91" target="#b5">6]</ref> is a step in this direction with its two subtasks: concept detection and caption prediction. For these subtasks, participants were required to develop methods from the provided dataset. The dataset was formed from a largescale collection of figures from open access biomedical journal articles (PubMed Central). All images in the dataset were accompanied by a caption, which form the labels for the caption prediction task. Unified Medical Language System (UMLS) concepts were extracted from each caption, forming the labels for the concept detection task.</p><p>In this article, we detail the methodology of our submissions for these two subtasks (Subsections 4.2 and 5.2). For concept detection, we make use of an ensemble of deep convolutional models. For caption prediction, we leverage encoder-to-decoder models, where the encoder is a computer vision model and the decoder is a natural language model. The remainder of this article includes a description of the two subtasks (Section 3), followed by a description and analysis of the data (Section 3). Following this, we describe the methodology and discuss the results for the concept detection task (Subsections 4.2 and 4.3). We then describe the methodology and discuss the results for the caption prediction task (Subsections 5.2 and 5.3) before concluding the article (Section 6).</p><p>Figure <ref type="figure" coords="2,121.44,357.21,3.87,9.96">1</ref>: A sample of images extracted from the dataset. The dataset includes an extensive range of modalities and anatomical regions, as well as both high and poor quality images. The licenses for the images from left to right are CC BY <ref type="bibr" coords="2,237.54,381.12,10.51,9.96" target="#b6">[7]</ref>, CC BY-NC <ref type="bibr" coords="2,301.19,381.12,10.51,9.96" target="#b7">[8]</ref>, CC BY <ref type="bibr" coords="2,348.10,381.12,10.51,9.96" target="#b8">[9]</ref>, CC BY <ref type="bibr" coords="2,395.01,381.12,14.92,9.96" target="#b9">[10]</ref>, and CC BY-NC <ref type="bibr" coords="2,481.22,381.12,14.92,9.96" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task Description</head><p>Concept Detection Task. A precursor to automatic medical image captioning is the identification of concepts in medical images. <ref type="foot" coords="2,246.41,468.90,3.71,7.97" target="#foot_0">1</ref> This task is arduous given that the system must contend with 8 374 possible concepts. The concepts can be further applied for context-based image and information retrieval purposes. Caption Prediction Task. Building upon the concept detection task, a system must not only detect concepts from a medical image, but also understand their interplay. From this understanding, the system must then compose a coherent natural language caption, akin to what a radiologist might write to describe their interpretation of an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset Description &amp; Analysis</head><p>Here, we describe the dataset for the concept detection and caption prediction subtasks. The dataset is a subset of the extended Radiology Objects in COntext (ROCO) dataset <ref type="bibr" coords="2,453.45,624.12,16.31,10.91" target="#b11">[12]</ref>. ROCO   originates from biomedical articles of the PubMed Central Open Access subset. <ref type="foot" coords="3,436.57,602.91,3.71,7.97" target="#foot_1">2</ref> The images of the dataset were split into training (ùëõ = 83 275), validation (ùëõ = 7 645), and test (ùëõ = 7 601) sets. The concepts were generated using a reduced subset of the UMLS 2020 AB release for the concept prediction task, which includes the sections (restriction levels) 0, 1, 2, and 9. To  improve the feasibility of recognising concepts from the images, concepts were further filtered based on their semantic type. The images in this dataset are very diverse, as depicted in Figure <ref type="figure" coords="4,381.99,549.50,3.66,10.91">1</ref>. An average of 4.74 ¬± 2.72 concepts are present for each image (the maximum number of concepts present in a single image is 50, and the minimum is 1). The concept distribution within the training dataset is highly skewed, as depicted in Figure <ref type="figure" coords="4,262.55,590.15,3.81,10.91" target="#fig_1">2</ref>, 93.36% concepts appear in less than 0.15% of the images. Concepts are grouped into higher-level semantic types; for example, concept "C0024485: Magnetic Resonance Imaging" is of type "T060: Diagnostic Procedure". As a result of the skewed count distribution, there is also an imbalance across Type Unique Identifiers (TUIs), as displayed in Figure <ref type="figure" coords="4,131.72,644.35,3.71,10.91" target="#fig_2">3</ref>. To illustrate this imbalance, we provide examples of common and rare concepts in Tables <ref type="table" coords="4,120.04,657.90,5.07,10.91" target="#tab_0">1</ref> and<ref type="table" coords="4,146.99,657.90,30.56,10.91" target="#tab_1">Table 2</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Concept Detection</head><p>In this section, we describe our participation pertaining the concept detection task. First, we describe the evaluation metric. Following this, we describe the methodology, as well as the model development on the task's dataset. Finally, we discuss the performance of our best model versus those of the other participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation Metrics</head><p>The F1 score between the predicted and ground truth concepts was the primary metric for concept detection. This was calculated as follows:</p><p>1. The Python scikit-learn f1_score function (v0.17.1-2) <ref type="bibr" coords="5,370.09,238.52,18.07,10.91" target="#b12">[13]</ref> was used to compute the F1 score between the predicted and ground truth boolean arrays. The default 'binary' averaging method was used. 2. All F1 scores were summed and averaged over the number of elements in the test set <ref type="bibr" coords="5,116.24,294.07,28.14,10.91">(7 601)</ref>, giving the final score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Methodology &amp; Model Development</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Image Pre-processing</head><p>Each medical image ùëã ‚àà R ùê∂√óùëä √óùêª (where ùê∂, ùëä , and ùêª denote the number of channels, width, and height, respectively) had an 8-bit pixel depth and three channels (ùê∂ = 3). The image was first resized using bilinear interpolation to a size of R 3√ó224√ó224 and normalised according to the mean and standard deviation defined by the ImageNet checkpoints <ref type="bibr" coords="5,392.05,405.01,16.32,10.91" target="#b13">[14]</ref>. During training, the image was rotated at an angle sampled from ùí∞[-50 ‚àò , 50 ‚àò ] and randomly flipped horizontally or vertically with an independent probability of 0.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Architecture Selection</head><p>To select the architecture of the model for concept detection, we performed a grid search, as seen in Table <ref type="table" coords="5,148.63,494.99,3.66,10.91" target="#tab_2">3</ref>. For this, we considered DenseNet-121 (7.2M parameters) <ref type="bibr" coords="5,407.04,494.99,16.09,10.91" target="#b14">[15]</ref>, ResNet-18 (11.4M parameters) <ref type="bibr" coords="5,143.41,508.54,16.08,10.91" target="#b15">[16]</ref>, and VGG-16 (134.7M parameters) <ref type="bibr" coords="5,311.43,508.54,16.09,10.91" target="#b16">[17]</ref>. Each model was trained for thirty hours using early stopping with either the stochastic gradient descent optimiser or Adam optimiser <ref type="bibr" coords="5,89.29,535.63,16.09,10.91" target="#b17">[18]</ref>. We also considered several learning rates from 0.01 to 10 -5 , as detailed in Table <ref type="table" coords="5,465.10,535.63,3.66,10.91" target="#tab_2">3</ref>. Finally, we investigated three different initialisation procedures:</p><p>1. Freezing all the convolutional weights obtained from ImageNet and training only the last fully connected layers of the network (denoted by MLP in Table <ref type="table" coords="5,403.32,583.06,4.16,10.91" target="#tab_2">3</ref>) 2. Training all weights from scratch (denoted by scratch in Table <ref type="table" coords="5,396.16,597.96,4.16,10.91" target="#tab_2">3</ref>) 3. Initialising the weights with an ImageNet checkpoint and fine-tuning all of the network's weights (denoted by ImageNet in Table <ref type="table" coords="5,293.32,626.41,3.57,10.91" target="#tab_2">3</ref>).</p><p>We observed that the best results were produced for the Adam optimiser <ref type="bibr" coords="5,419.12,646.74,17.75,10.91" target="#b17">[18]</ref> with a learning rate in the range [10 -5 , 5‚Ä¢10 -4 ], and fine-tuning all the weights of DenseNet-161 initialised with an ImageNet checkpoint. We also benchmarked more recent architectures, such as EfficientNet-B7 <ref type="bibr" coords="6,104.15,397.02,18.07,10.91" target="#b18">[19]</ref> and RegNetY-8GF <ref type="bibr" coords="6,209.54,397.02,16.41,10.91" target="#b19">[20]</ref>, but they did not offer an improvement in performance over DenseNet-161.<ref type="foot" coords="6,154.11,408.81,3.71,7.97" target="#foot_2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Improving Performance on Underrepresented Concepts</head><p>The concept detection task is a multi-label classification problem. For this purpose, the final activation function of each model was set to the sigmoid function. Moreover, Binary Cross-Entropy (BCE) was used as the loss function. In order to improve the recall for the underrepresented classes in the training set described in Section 3, we experiment with three different approaches, namely, weighted BCE loss, preferential sampling, and threshold optimisation. We also investigated an ensemble of models.</p><p>Weighted BCE loss: Given a ground truth concept vector y ‚àà {0, 1} ùëÅ and a prediction vector y Àú‚àà [0, 1] ùëÅ , the p-weighted BCE loss is defined as:</p><formula xml:id="formula_0" coords="6,195.49,582.30,310.50,33.71">1 ùëÅ ùëÅ ‚àëÔ∏Å ùëñ=1 (Ô∏Å p ùëñ y ùëñ log(y Àúùëñ) + (1 -y ùëñ ) log(1 -y Àúùëñ) )Ô∏Å .<label>(1)</label></formula><p>We trialled different weighting vectors p, ranging from continuous weighting that is inversely proportional to the frequency of the concept within the training set, to quantified weighting for different percentiles of the count distribution that more heavily penalises mispredictions of rare concepts. However, we found through 10-fold cross-validated that weighted BCE loss did not consistently improve the F1 score.</p><p>Preferential sampling: For the second approach, we over-sampled training examples that contained rare concepts by providing them with a larger sampling probability. This was in an attempt to balance the number of times a model would observe each concept during training. However, we found through 10-fold cross-validation that preferential sampling did not consistently improve the F1 score.</p><p>Threshold optimisation: Given a model's prediction, one has to convert the vector of probabilities into a set of concepts. This is classically performed using thresholding (where a threshold of 0.5 is typically used for each class). In this challenge, we propose to fine-tune those thresholds based on the performance on a holdout dataset ùíü; this is done by solving the following optimisation problem:</p><formula xml:id="formula_1" coords="7,220.36,269.79,285.63,28.25">arg max t‚àà[0,1] ùëÅ ‚àëÔ∏Å y ùëñ ‚ààùíü F1-score (Ô∏Å 1 y Àúùëñ‚â•t , y ùëñ )Ô∏Å ,<label>(2)</label></formula><p>where 1 ‚Ä¢‚â•t is a piecewise comparison operator that returns 1 if the ùëñ-th entry of ‚Ä¢ is greater or equal to t ùëñ , else, 0 is returned. Empirically, we noticed that optimising the threshold for rare concepts did not generalise well ('100% threshold optimisation' in Table <ref type="table" coords="7,432.43,330.56,3.55,10.91" target="#tab_3">4</ref>), as a sufficient number of data points is required for a robust estimation. Following this, we instead optimised the threshold for concepts that appear sufficiently in the holdout dataset; for the concepts that occured in the Top-10% ('Top-10% threshold optimisation' in Table <ref type="table" coords="7,390.41,371.21,3.61,10.91" target="#tab_3">4</ref>), as well as the concepts that occur in the Top-1% ('Top-1% threshold optimisation' in Table <ref type="table" coords="7,380.65,384.76,3.50,10.91" target="#tab_3">4</ref>). We also compared setting the thresholds for each concept to a constant ('Fixed threshold' in Table <ref type="table" coords="7,411.10,398.31,3.57,10.91" target="#tab_3">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results &amp; Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Final architectures</head><p>The models for our concept detection submissions were ensembles of DenseNets (where each was a DenseNet-161 initialised with the weights of an ImageNet checkpoint) trained according to the following recipe. We first start by combining the training and validation sets, splitting that into a holdout set used for threshold fine-tuning (10%). We randomly split the remaining data into an 80%/20% split for training/validation. We train all the parameters of each DenseNet-161 using the Adam optimiser with a batch-size of 8 and a learning rate of 10 -5 , along with earlystopping. The monitored metric was the validation F1-score. The average training time was 90 hours, where the selected epoch for each model was ‚âà 210. We form an ensemble of these models using majority voting. Finally our submissions, which use different thresholding and ensemble approaches, are presented in Table <ref type="table" coords="7,290.59,593.97,3.74,10.91" target="#tab_3">4</ref>. Our best scoring submission was an ensemble of 43 DenseNet-161 models with Top-1% threshold optimisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Participant Rankings</head><p>The rankings amongst the participants of the concept detection task are shown in Table <ref type="table" coords="7,500.04,656.03,3.81,10.91" target="#tab_4">5</ref>.</p><p>We managed an F1 score of 0.447, placing us third amongst the participants. Despite our best  <ref type="table" coords=""></ref>and<ref type="table" coords="8,317.85,268.86,9.93,10.91" target="#tab_10">A2</ref>. Some of the most commonly predicted concepts included modality (e.g., 'X-Ray Computed Tomography', 'Plain x-ray', and 'Magnetic Resonance Imaging'), body location (e.g., 'Chest', 'Abdomen', and 'Neck'), body part (e.g., 'Bone structure of cranium', 'Lower Extremity', and 'Pelvis'), and colour ('Yellow color', 'Green color', and 'Blue color'). One research direction that could be explored is a mixture of experts <ref type="bibr" coords="8,487.91,323.06,18.07,10.91" target="#b20">[21]</ref> with models that focus on different diagnostic procedures (e.g., TUI T060). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Caption Prediction</head><p>In this section, we first describe the evaluation metrics and the methodology for the caption prediction tasks. We then discuss the results of our models and end with a discussion about the performance of our best model versus those of the other participants. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation Metrics</head><p>The metrics for evaluating the caption prediction task are shown in Table <ref type="table" coords="9,428.13,304.27,3.81,10.91" target="#tab_5">6</ref>. For the official metrics used by the competition organisers, which we designate as CLEF-*, the following formatting was applied to the predicted and ground truth captions before evaluation:</p><p>1. Lowercased: The caption was first converted to lower-case. 2. Remove punctuation: All punctuation was then removed and the caption was tokenized into its individual words. 3. Remove stopwords: Stopwords were then removed using NLTK's English stopword list (NLTK v3.2.2). 4. Lemmatization: Lemmatization was next applied using spaCy's Lemmatizer (with spaCy model en_core_web_lg).</p><p>For the remaining metrics, only the first two formatting steps were applied (lower-cased and remove punctuation), and only to the ground truth captions (the models were relied upon to generate captions with the correct formatting, as they were trained on ground truth captions with the same formatting). These non-official metrics were only applied to the validation set of the caption prediction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Methodology</head><p>First, we describe the image pre-processing, followed by the caption formatting and generation, the models, and model fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Caption Formatting and Generation</head><p>For the training and validation sets, the ground truth captions were converted to lower-case and had punctuation removed. When generating the captions during validation and testing, a beam search with a beam size of four and a maximum number of 128 subwords was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 7</head><p>Caption prediction validation scores for each of the encoder-to-decoder models using the non-official metrics. (The row in grey indicates that the ground truth captions were different during evaluation and are thus uncomparable with the other rows). A higher colour saturation indicates a better score. When comparing CvT2DistilGPT2 to CvT2DistilGPT2‚Ä¢MIMIC-CXR, it can be seen that warmstarting with the MIMIC-CXR checkpoint (a chest X-ray checkpoint) improved the validation score for each metric. As highlighted in Table <ref type="table" coords="12,306.28,508.09,3.81,10.91" target="#tab_0">1</ref>, 'X-ray' was the second most represented modality in the dataset. This gives one reason as to why warm-starting with the MIMIC-CXR checkpoint was beneficial. However, the gains experienced on the validation set did not translate to an improvement in the test scores, with CvT2DistilGPT2 and CvT2DistilGPT2‚Ä¢MIMIC-CXR performing similarly.</p><p>When observing the captions generated by CvT2DistilGPT2‚Ä¢MIMIC-CXR, for example, in the first row in Table <ref type="table" coords="12,164.13,589.38,3.66,10.91" target="#tab_7">9</ref>, there were repetitions of ùëõ-grams. This was evident in the generated captions of the other models as well. To mitigate this issue, we applied a penalty to the probabilities of the subword tokens in order to prevent an ùëõ-gram from being generated more than once, which is detailed in Subsection 5.2.2. It can be seen that an ùëõ-gram size of three successfully removed the repetitions. While this improved the validation and test CLEF-ROUGE-1 scores in Table <ref type="table" coords="12,499.69,643.58,3.74,10.91" target="#tab_6">8</ref>, alarmingly, it had a minimal impact on the CLEF-BLEU score. This highlights the fragility of BLEU -the metric did not penalise the score due to the repetitions. A more aggressive schema, i.e., an ùëõ-gram size of two, attained a test CLEF-ROUGE-1 score similarly to an ùëõ-gram size of three; however, the CLEF-BLEU score was reduced. Hence, CvT2DistilGPT2‚Ä¢MIMIC-CXR with no repetitions for an ùëõ-gram size of 3 was our best-performing caption prediction model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Participant Rankings</head><p>The rankings amongst the participants of the caption prediction task are shown in Table <ref type="table" coords="13,494.87,399.05,8.53,10.91" target="#tab_0">10</ref>.</p><p>IUST NLP LAB attained the highest CLEF-BLEU and CLEF-METEOR scores, placing them first amongst the participants. However, their system produced the second-worst CLEF-CIDEr and CLEF-SPICE scores and their mean ranking over all the metrics was 6.2. This indicates that their performance, while optimal for CLEF-BLEU, did not generalise to the remaining metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 10</head><p>Caption prediction scores for the best submission of each participant. The ranking is determined by the participants' CLEF-BLEU scores. The mean rank of each participant over all the metrics is given in the last column. A higher colour saturation indicates a better score. Comparing our results (CSIRO) to that of the other participants, we attained the third-highest CLEF-BLEU and CLEF-SPICE scores, the second-highest CLEF-ROUGE-1, CLEF-METEOR, and CLEF-CIDEr scores, and the highest CLEF-BERTScore. We also attained the highest mean ranking over all the metrics at 2.2. This suggests that our system, when considering all metrics, outperformed the system of IUST NLP LAB. The mean ranking of fdallserra suggests that, in fact, their system was second best. Moreover, their system attained the highest CLEF-ROUGE-1 score. This highlights the importance of considering multiple metrics when evaluating natural language generation systems, as purely relying on a single metric, for example, CLEF-BLEU, can be misleading. We thus commend the organisers of the caption prediction task for expanding on the number of metrics from previous years. However, it should be noted that the model for each team was selected based on the best CLEF-BLEU score, which could bias the mean rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we detailed our participation in the concept detection and caption prediction subtasks of ImageCLEFmedical Caption 2022. For concept detection, we demonstrate the effectiveness of the ensemble approach, as well as the performance gains from threshold tuning. Despite our efforts, only a small portion of the concepts were predicted on the test set. This could be due to the fact that a vast amount of the concepts are underrepresented in the training set. For caption prediction, the important role that processing word token probabilities during generation can play was highlighted. Here, we used a penalty to prevent ùëõ-gram repetitions, which dramatically increased our CLEF-ROUGE-1 score. In future work, we aim to improve performance on the caption prediction task by leveraging the concept detection task, following the aim of the ImageCLEFmedical Caption challenge.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,89.29,325.47,286.32,9.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Number of occurrences of the concepts in the training split.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,89.29,566.18,372.72,9.96"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Number of occurrences of the concepts grouped by Type Unique Identifier (TUI).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="19,476.80,303.15,3.64,7.84;19,114.83,312.56,254.28,7.84;19,476.80,312.56,3.64,7.84;19,114.83,321.96,234.56,7.84;19,476.80,321.96,3.64,7.84;19,114.83,331.37,132.92,7.84;19,318.78,331.37,47.62,7.84;19,476.80,331.37,3.64,7.84;19,114.83,340.77,123.28,7.84;19,318.78,340.77,82.77,7.84;19,476.80,340.77,3.64,7.84;19,114.83,350.18,238.58,7.84;19,476.80,350.18,3.64,7.84;19,114.83,359.58,221.79,7.84;19,476.80,359.58,3.64,7.84;19,114.83,368.99,299.78,7.84;19,476.80,368.99,3.64,7.84;19,114.83,378.39,313.58,7.84;19,476.80,378.39,3.64,7.84;19,114.83,387.80,89.34,7.84;19,318.78,387.80,20.87,7.84;19,476.80,387.80,3.64,7.84;19,114.83,397.21,258.32,7.84;19,476.80,397.21,3.64,7.84;19,114.83,406.61,280.51,7.84;19,476.80,406.61,3.64,7.84;19,114.83,416.02,116.16,7.84;19,318.78,416.02,60.19,7.84;19,476.80,416.02,3.64,7.84;19,114.83,425.42,89.34,7.84;19,318.78,425.42,74.80,7.84;19,476.80,425.42,3.64,7.84;19,114.83,434.83,238.24,7.84;19,476.80,434.83,3.64,7.84;19,114.83,444.23,245.35,7.84;19,476.80,444.23,3.64,7.84;19,114.83,453.64,89.34,7.84;19,318.78,453.64,45.40,7.84;19,476.80,453.64,3.64,7.84;19,114.83,463.04,89.34,7.84;19,318.78,463.04,81.72,7.84;19,476.80,463.04,3.64,7.84;19,114.83,472.45,218.46,7.84;19,476.80,472.45,3.64,7.84;19,114.83,481.85,129.93,7.84;19,318.78,481.85,46.89,7.84;19,476.80,481.85,3.64,7.84;19,114.83,491.26,240.30,7.84;19,476.80,491.26,3.64,7.84;19,114.83,500.67,226.04,7.84;19,476.80,500.67,3.64,7.84;19,114.83,510.07,244.42,7.84;19,476.80,510.07,3.64,7.84;19,114.83,519.48,182.41,7.84;19,318.78,519.48,84.32,7.84;19,476.80,519.48,3.64,7.84;19,114.83,528.88,254.14,7.84;19,476.80,528.88,3.64,7.84;19,114.83,538.29,277.36,7.84;19,476.80,538.29,3.64,7.84;19,114.83,547.69,257.07,7.84;19,476.80,547.69,3.64,7.84;19,114.83,557.10,269.87,7.84;19,476.80,557.10,3.64,7.84;19,114.83,566.50,220.26,7.84;19,476.80,566.50,3.64,7.84;19,114.83,575.91,287.45,7.84;19,476.80,575.91,3.64,7.84;19,114.83,585.31,182.41,7.84;19,318.78,585.31,112.65,7.84;19,476.80,585.31,3.64,7.84;19,114.83,594.72,182.41,7.84;19,318.78,594.72,84.68,7.84;19,476.80,594.72,3.64,7.84;19,114.83,604.13,276.46,7.84;19,476.80,604.13,3.64,7.84;19,114.83,613.53,240.16,7.84;19,476.80,613.53,3.64,7.84;19,114.83,622.94,243.07,7.84;19,476.80,622.94,3.64,7.84;19,114.83,632.34,116.16,7.84;19,318.78,632.34,24.46,7.84;19,476.80,632.34,3.64,7.84;19,114.83,641.75,140.20,7.84;19,318.78,641.75,89.85,7.84;19,476.80,641.75,3.64,7.84;19,114.83,651.15,144.28,7.84;19,318.78,651.15,34.61,7.84;19,476.80,651.15,3.64,7.84;19,114.83,660.56,116.16,7.84;19,318.78,660.56,59.37,7.84;19,476.80,660.56,3.64,7.84;19,114.83,669.96,261.20,7.84;19,476.80,669.96,3.64,7.84"><head></head><label></label><figDesc>7 C0032005 T023 Body Part, Organ, or Organ Component Pituitary Gland 6 C0024687 T023 Body Part, Organ, or Organ Component Mandible 5 C0032326 T047 Disease or Syndrome Pneumothorax 5 C1711105 T109 Organic Chemical b-Hexachlorocyclohexane 5 C0014876 T023 Body Part, Organ, or Organ Component Esophagus 5 C0018563 T023 Body Part, Organ, or Organ Component Hand 5 C0000962 T023 Body Part, Organ, or Organ Component Bone structure of acetabulum 5 C0024091 T023 Body Part, Organ, or Organ Component Bone structure of lumbar vertebra 5 C0221198 T033 Finding Lesion 5 C0025526 T023 Body Part, Organ, or Organ Component Metacarpal bone 4 C0226054 T023 Body Part, Organ, or Organ Component Right pulmonary artery 4 Part, Organ, or Organ Component Left breast 4 C0227613 T023 Body Part, Organ, or Organ Component Right kidney 4 Part, Organ, or Organ Component Left kidney 3 C0030647 T023 Body Part, Organ, or Organ Component Patella 2 C0005847 T023 Body Part, Organ, or Organ Component Blood Vessel 2 C0016642 T061 Therapeutic or Preventive Procedure Fracture Fixation, Internal 2 C0030288 T023 Body Part, Organ, or Organ Component Pancreatic duct 2 C0230431 T023 Body Part, Organ, or Organ Component Structure of right knee 2 C0021852 T023 Body Part, Organ, or Organ Component Intestines, Small 2 C0230461 T023 Body Part, Organ, or Organ Component Structure of left foot 2 C0022742 T023 Body Part, Organ, or Organ Component Knee 2 C0003956 T023 Body Part, Organ, or Organ Component Ascending aorta structure 2 C0040508 T061 Therapeutic or Preventive Procedure Total Hip Replacement (procedure) 2 C0013931 T061 Therapeutic or Preventive Procedure Embolization, Therapeutic 2 C0040184 T023 Body Part, Organ, or Organ Component Bone structure of tibia 2 C0013303 T023 Body Part, Organ, or Organ Component Duodenum 2 C0029130 T023 Body Part, Organ, or Organ Component Optic Nerve 1 Part, Organ, or Organ Component Right lobe of liver 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,89.60,388.23,185.95"><head>Table 1</head><label>1</label><figDesc>Exemplars of common concepts.</figDesc><table coords="4,99.30,120.76,377.92,154.79"><row><cell>Concept</cell><cell>Name</cell><cell>#</cell><cell>Concept</cell><cell>Name</cell><cell>#</cell></row><row><cell></cell><cell>T060: Diagnostic Procedure</cell><cell></cell><cell></cell><cell cols="2">T080: Qualitative Concept</cell></row><row><cell cols="3">C0040405 X-Ray Computed Tomography 25989</cell><cell cols="2">C0444706 Measured</cell><cell>1337</cell></row><row><cell cols="2">C1306645 Plain x-ray</cell><cell>24389</cell><cell cols="2">C0019409 Heterogeneity</cell><cell>958</cell></row><row><cell cols="2">C0024485 Magnetic Resonance Imaging</cell><cell>14622</cell><cell cols="2">C0392756 Reduced</cell><cell>831</cell></row><row><cell cols="2">C0041618 Ultrasonography</cell><cell>11147</cell><cell cols="2">C0442800 Enlarged</cell><cell>774</cell></row><row><cell></cell><cell>‚Ä¢ ‚Ä¢ ‚Ä¢</cell><cell></cell><cell></cell><cell>‚Ä¢ ‚Ä¢ ‚Ä¢</cell><cell></cell></row><row><cell></cell><cell>T082: Spatial Concept</cell><cell></cell><cell></cell><cell cols="2">T130: Indicator or Diagnostic Aid</cell></row><row><cell cols="2">C0205131 Axial</cell><cell>3187</cell><cell cols="2">C0009924 Contrast Media</cell><cell>1406</cell></row><row><cell cols="2">C0238767 Bilateral</cell><cell>2722</cell><cell cols="2">C0016911 gadolinium</cell><cell>289</cell></row><row><cell cols="2">C0205129 Sagittal</cell><cell>2012</cell><cell cols="2">C1522485 Tracer</cell><cell>104</cell></row><row><cell cols="2">C0205091 Left</cell><cell>1696</cell><cell cols="2">C0013343 Dyes</cell><cell>43</cell></row><row><cell></cell><cell>‚Ä¢ ‚Ä¢ ‚Ä¢</cell><cell></cell><cell></cell><cell>‚Ä¢ ‚Ä¢ ‚Ä¢</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,88.99,296.96,406.64,199.56"><head>Table 2</head><label>2</label><figDesc>Exemplars of rare concepts.</figDesc><table coords="4,94.73,327.69,400.90,168.82"><row><cell>Concept</cell><cell>Name</cell><cell>#</cell><cell>Concept</cell><cell>Name</cell><cell>#</cell></row><row><cell></cell><cell>T019: Congenital Abnormality</cell><cell></cell><cell></cell><cell cols="2">T131: Hazardous or Poisonous Substance</cell></row><row><cell cols="2">C0265905 Agenesis of pulmonary artery</cell><cell>3</cell><cell cols="2">C0037390 Snuff Tobacco</cell><cell>3</cell></row><row><cell cols="3">C2959359 Abnormal ventriculoarterial connection 3 C0266035 Enamel pearls 3 ‚Ä¢ ‚Ä¢ ‚Ä¢</cell><cell cols="2">C0142056 Asbestos, Serpentine C0556615 Paint thinners C0003947 Asbestos</cell><cell>3 3 6</cell></row><row><cell></cell><cell>T048: Mental or Behavioral Dysfunction</cell><cell></cell><cell cols="2">C0007018 carbon monoxide</cell><cell>10</cell></row><row><cell cols="2">C0006012 Borderline Personality Disorder C0016142 Firesetting Behavior C0686346 Gender Dysphoria ‚Ä¢ ‚Ä¢ ‚Ä¢ T122: Biomedical or Dental Material</cell><cell>3 3 3</cell><cell cols="3">‚Ä¢ ‚Ä¢ ‚Ä¢ T063: Molecular Biology Research Technique C0920677 Gene Delivery Systems 3 C4725722 Second-strand Library Sequencing 3 T073: Manufactured Object</cell></row><row><cell cols="2">C0181075 Bone graft -material C0011324 Dental Amalgam</cell><cell>3 3</cell><cell cols="2">C0183336 Sleeve T170: Intellectual Product</cell><cell>4</cell></row><row><cell></cell><cell>‚Ä¢ ‚Ä¢ ‚Ä¢</cell><cell></cell><cell cols="2">C0205442 Eighth</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,88.98,89.60,417.00,268.55"><head>Table 3</head><label>3</label><figDesc>Architecture selection for varied learning rates and optimisers. A higher colour saturation indicates a better score. Green indicates the F1 score while blue indicates recall. Did Not Finish (DNF) indicates that a model did not converge during training.</figDesc><table coords="6,93.75,143.66,407.78,214.48"><row><cell>Learning rate</cell><cell cols="2">0.01</cell><cell cols="2">0.005</cell><cell cols="2">0.001</cell><cell cols="2">0.0005</cell><cell cols="2">0.0001</cell><cell cols="2">0.00005</cell><cell cols="2">0.00001</cell></row><row><cell></cell><cell>F1</cell><cell>Recall</cell><cell>F1</cell><cell>Recall</cell><cell>F1</cell><cell>Recall</cell><cell>F1</cell><cell>Recall</cell><cell>F1</cell><cell>Recall</cell><cell>F1</cell><cell>Recall</cell><cell>F1</cell><cell>Recall</cell></row><row><cell>DenseNet MLP ADAM</cell><cell>0.2348</cell><cell>0.2096</cell><cell cols="2">0.2823 0.1993</cell><cell cols="2">0.3148 0.1992</cell><cell cols="2">0.3160 0.2000</cell><cell cols="2">0.3153 0.2000</cell><cell cols="2">0.3155 0.1990</cell><cell cols="2">0.3123 0.1949</cell></row><row><cell>DenseNet MLP SGD</cell><cell>0.2817</cell><cell>0.1674</cell><cell cols="2">0.2561 0.1487</cell><cell cols="2">0.1560 0.0850</cell><cell cols="2">0.0844 0.0442</cell><cell cols="2">0.0372 0.0229</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell></row><row><cell>DenseNet scratch ADAM</cell><cell>0.3106</cell><cell>0.1974</cell><cell cols="2">0.3076 0.1945</cell><cell cols="2">0.3191 0.2016</cell><cell cols="2">0.3215 0.2069</cell><cell cols="2">0.3210 0.2086</cell><cell cols="2">0.2255 0.1328</cell><cell cols="2">0.2255 0.1328</cell></row><row><cell>DenseNet scratch SGD</cell><cell>0.1654</cell><cell>0.0977</cell><cell cols="2">0.0980 0.0549</cell><cell cols="2">0.0988 0.0550</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell></row><row><cell cols="2">DenseNet ImageNet ADAM 0.3095</cell><cell>0.1966</cell><cell cols="2">0.3126 0.1974</cell><cell cols="2">0.3238 0.2143</cell><cell cols="2">0.3283 0.2123</cell><cell cols="2">0.3289 0.2147</cell><cell>0.3310</cell><cell>0.2173</cell><cell cols="2">0.3290 0.2142</cell></row><row><cell>DenseNet ImageNet SGD</cell><cell>0.3156</cell><cell>0.1949</cell><cell cols="2">0.2514 0.1452</cell><cell cols="2">0.0552 0.0304</cell><cell cols="2">0.0863 0.0480</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell></row><row><cell>ResNet MLP ADAM</cell><cell>0.2393</cell><cell>0.1901</cell><cell cols="2">0.2816 0.1840</cell><cell cols="2">0.2982 0.1900</cell><cell cols="2">0.2991 0.1898</cell><cell cols="2">0.3020 0.1887</cell><cell cols="2">0.3003 0.1872</cell><cell cols="2">0.2954 0.1828</cell></row><row><cell>ResNet MLP SGD</cell><cell>0.2691</cell><cell>0.1599</cell><cell cols="2">0.2437 0.1414</cell><cell cols="2">0.1421 0.0775</cell><cell cols="2">0.1161 0.0625</cell><cell cols="2">0.0303 0.0157</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell></row><row><cell>ResNet scratch ADAM</cell><cell>0.2812</cell><cell>0.1751</cell><cell cols="2">0.2957 0.1839</cell><cell cols="2">0.2720 0.1689</cell><cell>DNF</cell><cell>DNF</cell><cell cols="2">0.2957 0.1858</cell><cell cols="2">0.2319 0.1420</cell><cell cols="2">0.3205 0.2046</cell></row><row><cell>ResNet scratch SGD</cell><cell>0.2799</cell><cell>0.1730</cell><cell cols="2">0.2251 0.1371</cell><cell cols="2">0.0480 0.0256</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell cols="2">0.0010 0.4095</cell><cell>DNF</cell><cell>DNF</cell></row><row><cell>ResNet ImageNet ADAM</cell><cell>0.3142</cell><cell>0.1985</cell><cell cols="2">0.3109 0.1964</cell><cell cols="2">0.3229 0.2048</cell><cell cols="2">0.3243 0.2089</cell><cell cols="2">0.3258 0.2133</cell><cell cols="2">0.3273 0.2114</cell><cell cols="2">0.3267 0.2112</cell></row><row><cell>ResNet ImageNet SGD</cell><cell>0.3073</cell><cell>0.1889</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell cols="2">0.0330 0.0235</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell></row><row><cell>VGG MLP ADAM</cell><cell>0.3002</cell><cell>0.2046</cell><cell cols="2">0.3040 0.2062</cell><cell cols="2">0.3114 0.2045</cell><cell cols="2">0.3097 0.1986</cell><cell cols="2">0.3121 0.1973</cell><cell cols="2">0.3118 0.1972</cell><cell cols="2">0.3108 0.1954</cell></row><row><cell>VGG MLP SGD</cell><cell>0.2978</cell><cell>0.1834</cell><cell cols="2">0.2917 0.1781</cell><cell cols="2">0.2590 0.1540</cell><cell cols="2">0.2253 0.1302</cell><cell cols="2">0.1763 0.0990</cell><cell cols="2">0.0767 0.0412</cell><cell cols="2">0.0021 0.0558</cell></row><row><cell>VGG scratch ADAM</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell cols="2">0.2738 0.1665</cell><cell cols="2">0.2559 0.1550</cell><cell cols="2">0.2710 0.1669</cell></row><row><cell>VGG scratch SGD</cell><cell>0.2804</cell><cell>0.1742</cell><cell cols="2">0.2510 0.1551</cell><cell cols="2">0.1477 0.0825</cell><cell cols="2">0.0534 0.0279</cell><cell cols="2">0.0013 0.3967</cell><cell cols="2">0.0440 0.0308</cell><cell cols="2">0.1315 0.0789</cell></row><row><cell>VGG ImageNet ADAM</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell cols="2">0.3157 0.1967</cell><cell cols="2">0.3237 0.2046</cell><cell cols="2">0.3256 0.2069</cell><cell cols="2">0.3274 0.2088</cell></row><row><cell>VGG ImageNet SGD</cell><cell>0.3179</cell><cell>0.1992</cell><cell cols="2">0.3172 0.1978</cell><cell cols="2">0.3085 0.1920</cell><cell cols="2">0.2981 0.1844</cell><cell cols="2">0.1701 0.0945</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell><cell>DNF</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,88.99,89.60,416.99,190.17"><head>Table 4</head><label>4</label><figDesc>Submission history and associated F1 scores for each model. efforts to account for rare concepts, our best model only predicted 107 out of the 8 734 available concepts on the test set, as shown in Tables A1</figDesc><table coords="8,173.86,120.92,247.56,108.09"><row><cell>Model</cell><cell>Threshold selection</cell><cell>F1</cell></row><row><cell cols="2">Ensemble of 43 DenseNet-161 Top-1% threshold optimisation</cell><cell>0.447</cell></row><row><cell cols="2">Ensemble of 43 DenseNet-161 Fixed threshold: 0.30</cell><cell>0.446</cell></row><row><cell cols="2">Ensemble of 11 DenseNet-161 Fixed threshold: 0.30</cell><cell>0.445</cell></row><row><cell cols="2">Ensemble of 11 DenseNet-161 Fixed threshold: 0.25</cell><cell>0.444</cell></row><row><cell>Ensemble of 5 DenseNet-161</cell><cell>Fixed threshold: 0.3</cell><cell>0.442</cell></row><row><cell>Ensemble of 5 DenseNet-161</cell><cell>Top-10% threshold optimisation</cell><cell>0.407</cell></row><row><cell>Ensemble of 5 DenseNet-161</cell><cell>100% threshold optimisation</cell><cell>0.406</cell></row><row><cell>Single DenseNet-161</cell><cell>Fixed threshold: 0.30</cell><cell>0.437</cell></row><row><cell>Single DenseNet-161</cell><cell>Fixed threshold: 0.50</cell><cell>0.433</cell></row><row><cell>Single DenseNet-161</cell><cell>100% threshold optimisation</cell><cell>0.396</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,88.99,364.73,392.07,178.30"><head>Table 5</head><label>5</label><figDesc>Final ranking for the concept detection task. A higher colour saturation indicates a better score.</figDesc><table coords="8,181.79,396.29,226.71,146.73"><row><cell>Group Name</cell><cell cols="3">F1 Score Secondary F1 Rank</cell></row><row><cell>AUEB-NLP-Group</cell><cell>0.451</cell><cell>0.791</cell><cell>1</cell></row><row><cell>fdallaserra</cell><cell>0.451</cell><cell>0.822</cell><cell>2</cell></row><row><cell>CSIRO</cell><cell>0.447</cell><cell>0.794</cell><cell>3</cell></row><row><cell>eecs-kth</cell><cell>0.436</cell><cell>0.856</cell><cell>4</cell></row><row><cell>vcmi</cell><cell>0.433</cell><cell>0.863</cell><cell>5</cell></row><row><cell>PoliMi-ImageClef</cell><cell>0.432</cell><cell>0.851</cell><cell>6</cell></row><row><cell>SSNSheerinKavitha</cell><cell>0.418</cell><cell>0.654</cell><cell>7</cell></row><row><cell>IUST_NLPLAB</cell><cell>0.398</cell><cell>0.673</cell><cell>8</cell></row><row><cell>Morgan_CS</cell><cell>0.352</cell><cell>0.628</cell><cell>9</cell></row><row><cell>kdelab</cell><cell>0.310</cell><cell>0.412</cell><cell>10</cell></row><row><cell>SDVA-UCSD</cell><cell>0.308</cell><cell>0.552</cell><cell>11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,88.99,89.60,378.67,167.80"><head>Table 6</head><label>6</label><figDesc>Caption prediction metrics.</figDesc><table coords="9,127.62,120.92,340.04,136.49"><row><cell>Metric</cell><cell cols="2">Official metric Description</cell></row><row><cell>CLEF-BLEU</cell><cell>‚úì</cell><cell>Average score of BLEU-1, BLEU-2, BLEU-3, and BLEU-4 [22].</cell></row><row><cell>CLEF-ROUGE-1</cell><cell>‚úì</cell><cell>ROUGE-n with unigrams [23].</cell></row><row><cell>CLEF-METEOR</cell><cell>‚úì</cell><cell>METEOR v1.5 [24].</cell></row><row><cell>CLEF-CIDEr</cell><cell>‚úì</cell><cell>CIDEr [25].</cell></row><row><cell>CLEF-SPICE</cell><cell>‚úì</cell><cell>SPICE [26].</cell></row><row><cell>CLEF-BERTScore</cell><cell>‚úì</cell><cell>BERTScore with microsoft/deberta-xlarge-mnli [27].</cell></row><row><cell>BLEU-1</cell><cell>‚úó</cell><cell>BLEU-n with unigrams [22].</cell></row><row><cell>BLEU-2</cell><cell>‚úó</cell><cell>BLEU-n with bigrams [22].</cell></row><row><cell>BLEU-3</cell><cell>‚úó</cell><cell>BLEU-n with trigrams [22].</cell></row><row><cell>BLEU-4</cell><cell>‚úó</cell><cell>BLEU-n with four-grams[22].</cell></row><row><cell>METEOR</cell><cell>‚úó</cell><cell>METEOR v1.5 [24].</cell></row><row><cell>ROUGE-L</cell><cell>‚úó</cell><cell>ROUGE with longest common subsequence-based statistics [28].</cell></row><row><cell>CIDEr</cell><cell>‚úó</cell><cell>CIDEr [25].</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="12,88.99,144.83,417.30,309.86"><head>Table 8</head><label>8</label><figDesc>Caption prediction validation and test scores for each of the encoder-to-decoder models on the official metrics. Note that only the CLEF-BLEU and CLEF-ROUGE-1 scores were made available for each of the submissions, the remaining official metrics were only used with the best submission in Table10. A higher colour saturation indicates a better score. Yellow designates scores on the validation set, while blue indicates scores on the test set.</figDesc><table coords="12,111.74,144.83,371.80,309.86"><row><cell>Model</cell><cell cols="7">BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L CIDEr</cell></row><row><cell>ViT2BERT</cell><cell>0.223</cell><cell>0.124</cell><cell>0.066</cell><cell>0.038</cell><cell>0.091</cell><cell>0.202</cell><cell>0.192</cell></row><row><cell>remove stopwords</cell><cell>0.155</cell><cell>0.080</cell><cell>0.041</cell><cell>0.021</cell><cell>0.078</cell><cell>0.169</cell><cell>0.230</cell></row><row><cell>ViT2PubMedBERT</cell><cell>0.228</cell><cell>0.126</cell><cell>0.068</cell><cell>0.039</cell><cell>0.091</cell><cell>0.204</cell><cell>0.203</cell></row><row><cell>ViT2DistilGPT2</cell><cell>0.214</cell><cell>0.118</cell><cell>0.064</cell><cell>0.036</cell><cell>0.087</cell><cell>0.196</cell><cell>0.203</cell></row><row><cell>CvT2DistilGPT2</cell><cell>0.215</cell><cell>0.119</cell><cell>0.064</cell><cell>0.036</cell><cell>0.087</cell><cell>0.198</cell><cell>0.202</cell></row><row><cell>retain aspect ratio</cell><cell>0.215</cell><cell>0.119</cell><cell>0.065</cell><cell>0.037</cell><cell>0.088</cell><cell>0.198</cell><cell>0.208</cell></row><row><cell>CvT2DistilGPT2‚Ä¢MIMIC-CXR</cell><cell>0.221</cell><cell>0.122</cell><cell>0.067</cell><cell>0.039</cell><cell>0.090</cell><cell>0.201</cell><cell>0.213</cell></row><row><cell>no. repeat n-gram size: 2</cell><cell>0.197</cell><cell>0.110</cell><cell>0.061</cell><cell>0.035</cell><cell>0.090</cell><cell>0.204</cell><cell>0.233</cell></row><row><cell>no. repeat n-gram size: 3</cell><cell>0.204</cell><cell>0.115</cell><cell>0.063</cell><cell>0.037</cell><cell>0.092</cell><cell>0.205</cell><cell>0.229</cell></row><row><cell>Model</cell><cell></cell><cell cols="2">Validation Set</cell><cell></cell><cell>Test Set</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="7">CLEF-BLEU CLEF-ROUGE-1 CLEF-BLEU CLEF-ROUGE-1</cell></row><row><cell>ViT2BERT</cell><cell></cell><cell>0.004</cell><cell></cell><cell>0.182</cell><cell>0.311</cell><cell cols="2">0.181</cell></row><row><cell>remove stopwords</cell><cell></cell><cell>0.005</cell><cell></cell><cell>0.188</cell><cell>0.297</cell><cell cols="2">0.186</cell></row><row><cell>ViT2PubMedBERT</cell><cell></cell><cell>0.004</cell><cell></cell><cell>0.188</cell><cell>0.309</cell><cell cols="2">0.188</cell></row><row><cell>ViT2DistilGPT2</cell><cell></cell><cell>0.004</cell><cell></cell><cell>0.183</cell><cell>0.306</cell><cell cols="2">0.181</cell></row><row><cell>CvT2DistilGPT2</cell><cell></cell><cell>0.005</cell><cell></cell><cell>0.181</cell><cell>0.309</cell><cell cols="2">0.182</cell></row><row><cell>retain aspect ratio</cell><cell></cell><cell>0.005</cell><cell></cell><cell>0.183</cell><cell>0.310</cell><cell cols="2">0.181</cell></row><row><cell>CvT2DistilGPT2‚Ä¢MIMIC-CXR</cell><cell></cell><cell>0.006</cell><cell></cell><cell>0.188</cell><cell>0.310</cell><cell cols="2">0.181</cell></row><row><cell cols="2">no. repeat n-gram size: 2</cell><cell>0.005</cell><cell></cell><cell>0.195</cell><cell>0.308</cell><cell cols="2">0.197</cell></row><row><cell cols="2">no. repeat n-gram size: 3</cell><cell>0.006</cell><cell></cell><cell>0.194</cell><cell>0.311</cell><cell cols="2">0.197</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="13,88.99,154.28,416.99,193.76"><head>Table 9</head><label>9</label><figDesc>Generated captions for test example 092563. CvT2DistilGPT2‚Ä¢MIMIC-CXR would repeatedly generate the same six-gram if a penalty was not applied to the word token probabilities during generation.</figDesc><table coords="13,142.84,197.55,310.93,150.49"><row><cell>Model</cell><cell>Generated caption</cell></row><row><cell></cell><cell>angiogram of the left subclavian artery occlusion of the left</cell></row><row><cell></cell><cell>subclavian artery occlusion of the left common carotid artery</cell></row><row><cell></cell><cell>occlusion of the left subclavian artery occlusion of the left sub-</cell></row><row><cell>CvT2DistilGPT2‚Ä¢MIMIC-CXR</cell><cell>clavian artery occlusion of the left subclavian artery occlusion</cell></row><row><cell></cell><cell>of the left subclavian artery occlusion of the left subclavian</cell></row><row><cell></cell><cell>artery occlusion of the left subclavian artery occlusion of the</cell></row><row><cell></cell><cell>left subclavian artery</cell></row><row><cell>CvT2DistilGPT2‚Ä¢MIMIC-CXR</cell><cell>angiogram of the left subclavian artery after stent implanta-</cell></row><row><cell>no. repeat n-gram size: 2</cell><cell>tion</cell></row><row><cell>CvT2DistilGPT2‚Ä¢MIMIC-CXR</cell><cell>angiogram of the left subclavian artery occlusion of the proxi-</cell></row><row><cell>no. repeat n-gram size: 3</cell><cell>mal leave anterior descend artery</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="18,88.99,132.16,391.50,500.03"><head>Table A1</head><label>A1</label><figDesc>Concepts predicted by our final model on the test set.</figDesc><table coords="18,114.79,162.68,365.70,469.52"><row><cell>Concept</cell><cell>TUI</cell><cell>TUI Description</cell><cell>Name</cell><cell>#predictions</cell></row><row><cell cols="3">C0040405 T060 Diagnostic Procedure</cell><cell>X-Ray Computed Tomography</cell><cell>2804</cell></row><row><cell cols="3">C1306645 T060 Diagnostic Procedure</cell><cell>Plain x-ray</cell><cell>1997</cell></row><row><cell cols="3">C0024485 T060 Diagnostic Procedure</cell><cell>Magnetic Resonance Imaging</cell><cell>1504</cell></row><row><cell cols="3">C0041618 T060 Diagnostic Procedure</cell><cell>Ultrasonography</cell><cell>1097</cell></row><row><cell cols="3">C0817096 T029 Body Location or Region</cell><cell>Chest</cell><cell>1076</cell></row><row><cell cols="3">C0000726 T029 Body Location or Region</cell><cell>Abdomen</cell><cell>718</cell></row><row><cell cols="3">C0002978 T060 Diagnostic Procedure</cell><cell>angiogram</cell><cell>514</cell></row><row><cell cols="4">C0037303 T023 Body Part, Organ, or Organ Component Bone structure of cranium</cell><cell>306</cell></row><row><cell cols="4">C0023216 T023 Body Part, Organ, or Organ Component Lower Extremity</cell><cell>280</cell></row><row><cell cols="3">C0205129 T082 Spatial Concept</cell><cell>Sagittal</cell><cell>261</cell></row><row><cell cols="3">C0221205 T080 Qualitative Concept</cell><cell>Yellow color</cell><cell>240</cell></row><row><cell cols="4">C0030797 T023 Body Part, Organ, or Organ Component Pelvis</cell><cell>195</cell></row><row><cell cols="3">C0238767 T082 Spatial Concept</cell><cell>Bilateral</cell><cell>192</cell></row><row><cell cols="3">C0242485 T169 Functional Concept</cell><cell>Measurement</cell><cell>157</cell></row><row><cell cols="4">C1140618 T023 Body Part, Organ, or Organ Component Upper Extremity</cell><cell>134</cell></row><row><cell cols="4">C0037949 T023 Body Part, Organ, or Organ Component Vertebral column</cell><cell>107</cell></row><row><cell cols="3">C0046056 T109 Organic Chemical</cell><cell>fluorodeoxyglucose F18</cell><cell>97</cell></row><row><cell cols="3">C0205131 T082 Spatial Concept</cell><cell>Axial</cell><cell>96</cell></row><row><cell cols="4">C0225860 T023 Body Part, Organ, or Organ Component Left atrial structure</cell><cell>87</cell></row><row><cell cols="3">C0205143 T082 Spatial Concept</cell><cell>Angular</cell><cell>85</cell></row><row><cell cols="3">C1699633 T060 Diagnostic Procedure</cell><cell>PET/CT scan</cell><cell>84</cell></row><row><cell cols="4">C0225897 T023 Body Part, Organ, or Organ Component Left ventricular structure</cell><cell>82</cell></row><row><cell cols="4">C0226032 T023 Body Part, Organ, or Organ Component Anterior descending branch of left...</cell><cell>80</cell></row><row><cell cols="4">C0023884 T023 Body Part, Organ, or Organ Component Liver</cell><cell>80</cell></row><row><cell cols="4">C0225883 T023 Body Part, Organ, or Organ Component Right ventricular structure</cell><cell>76</cell></row><row><cell cols="3">C0243144 T039 Physiologic Function</cell><cell>Uptake</cell><cell>75</cell></row><row><cell cols="4">C0225844 T023 Body Part, Organ, or Organ Component Right atrial structure</cell><cell>73</cell></row><row><cell cols="3">C0035190 T201 Clinical Attribute</cell><cell>Residual volume</cell><cell>69</cell></row><row><cell cols="3">C0027530 T029 Body Location or Region</cell><cell>Neck</cell><cell>62</cell></row><row><cell cols="3">C0309093 T109 Organic Chemical</cell><cell>FLAIR (product)</cell><cell>62</cell></row><row><cell cols="4">C0006104 T023 Body Part, Organ, or Organ Component Brain</cell><cell>58</cell></row><row><cell cols="3">C0032743 T060 Diagnostic Procedure</cell><cell>Positron-Emission Tomography</cell><cell>58</cell></row><row><cell cols="3">C0012751 T081 Quantitative Concept</cell><cell>Distance</cell><cell>55</cell></row><row><cell cols="3">C0205132 T082 Spatial Concept</cell><cell>Linear</cell><cell>53</cell></row><row><cell cols="3">C0032227 T047 Disease or Syndrome</cell><cell>Pleural effusion disorder</cell><cell>49</cell></row><row><cell cols="3">C0444706 T080 Qualitative Concept</cell><cell>Measured</cell><cell>36</cell></row><row><cell cols="3">C0021102 T074 Medical Device</cell><cell>Implants</cell><cell>32</cell></row><row><cell cols="3">C1302256 T082 Spatial Concept</cell><cell>Apical four chamber view</cell><cell>31</cell></row><row><cell cols="3">C0332575 T033 Finding</cell><cell>Redness</cell><cell>29</cell></row><row><cell cols="4">C0034052 T023 Body Part, Organ, or Organ Component Pulmonary artery structure</cell><cell>27</cell></row><row><cell cols="4">C0005400 T023 Body Part, Organ, or Organ Component Bile duct structure</cell><cell>27</cell></row><row><cell cols="4">C0005682 T023 Body Part, Organ, or Organ Component Urinary Bladder</cell><cell>25</cell></row><row><cell cols="3">C0332583 T080 Qualitative Concept</cell><cell>Green color</cell><cell>24</cell></row><row><cell cols="4">C1261316 T023 Body Part, Organ, or Organ Component Right coronary artery structure</cell><cell>23</cell></row><row><cell cols="3">C1260957 T080 Qualitative Concept</cell><cell>Blue color</cell><cell>23</cell></row><row><cell cols="3">C0178602 T081 Quantitative Concept</cell><cell>Dosage</cell><cell>23</cell></row><row><cell cols="3">C0015965 T018 Embryonic Structure</cell><cell>Fetus</cell><cell>23</cell></row><row><cell cols="4">C0728985 T023 Body Part, Organ, or Organ Component Cervical spine</cell><cell>23</cell></row><row><cell cols="3">C3827002 T033 Finding</cell><cell>Ground-glass opacities</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="19,88.99,89.60,391.45,221.39"><head>Table A2 (</head><label>A2</label><figDesc>Continued) Concepts predicted by our final model on the test set.</figDesc><table coords="19,114.83,120.14,365.61,190.85"><row><cell>Concept</cell><cell>TUI</cell><cell>TUI Description</cell><cell>Name</cell><cell>#predictions</cell></row><row><cell cols="3">C0521530 T047 Disease or Syndrome</cell><cell>Lung consolidation</cell><cell>18</cell></row><row><cell cols="4">C0040578 T023 Body Part, Organ, or Organ Component Trachea</cell><cell>17</cell></row><row><cell cols="3">C0037775 T082 Spatial Concept</cell><cell>Spatial Distribution</cell><cell>15</cell></row><row><cell cols="3">C1302222 T082 Spatial Concept</cell><cell>Parasternal long axis view</cell><cell>15</cell></row><row><cell cols="3">C1881277 T081 Quantitative Concept</cell><cell>Isodose</cell><cell>14</cell></row><row><cell cols="4">C0003483 T023 Body Part, Organ, or Organ Component Aorta</cell><cell>13</cell></row><row><cell cols="3">C0031039 T047 Disease or Syndrome</cell><cell>Pericardial effusion</cell><cell>12</cell></row><row><cell cols="4">C0025584 T023 Body Part, Organ, or Organ Component Metatarsal bone structure</cell><cell>12</cell></row><row><cell cols="3">C0026266 T047 Disease or Syndrome</cell><cell>Mitral Valve Insufficiency</cell><cell>12</cell></row><row><cell cols="4">C0006141 T023 Body Part, Organ, or Organ Component Breast</cell><cell>11</cell></row><row><cell cols="3">C0454199 T081 Quantitative Concept</cell><cell>Planning target volume</cell><cell>10</cell></row><row><cell cols="4">C0030274 T023 Body Part, Organ, or Organ Component Pancreas</cell><cell>10</cell></row><row><cell cols="3">C0026608 T026 Cell Component</cell><cell>Motor Endplate</cell><cell>10</cell></row><row><cell cols="4">C0015813 T023 Body Part, Organ, or Organ Component Head of femur</cell><cell>10</cell></row><row><cell cols="3">C4331911 T169 Functional Concept</cell><cell>M-Mode Ultrasound Mode</cell><cell>8</cell></row><row><cell cols="3">C0025062 T047 Disease or Syndrome</cell><cell>Mediastinal Emphysema</cell><cell>8</cell></row><row><cell cols="3">C0038536 T046 Pathologic Function</cell><cell>Subcutaneous Emphysema</cell><cell>7</cell></row><row><cell cols="3">C3829578 T033 Finding</cell><cell>Hypoechoic Focus</cell><cell>7</cell></row><row><cell cols="4">C0024109 T023 Body Part, Organ, or Organ Component Lung</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,660.05,397.05,8.97;2,89.29,671.01,68.25,8.97"><p>Concepts are taken from some pre-defined medical terminology. In this case, the concepts are from the UMLS medical thesaurus.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,108.93,671.03,189.44,8.97"><p>https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,108.93,660.08,397.05,8.97;6,89.29,671.04,246.31,8.97"><p>Each implementation and checkpoint is from https://github.com/pytorch/vision/tree/main/torchvision/models and the input images are re-sized to the architecture's requirement.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work was partially funded by <rs type="funder">CSIRO's Machine Learning and Artificial Intelligence Future Science Platform (MLAI FSP)</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The encoder-to-decoder models investigated for caption prediction are listed below. The input to the encoder is a medical image. The output of the encoder is fed to the cross-attention module of the decoder, which then generates a caption in an autoregressive fashion. It should be noted that each model employs a linear layer that projects the last hidden state of the encoder to the hidden size of the decoder. It also should be noted that the image pre-processing that we used for caption prediction differs slightly to that used in Subsection 4.2.1. The image was instead resized to a size of R 3√ó384√ó384 . During training, the image was rotated at an angle sampled from ùí∞[-5 ‚àò , 5 ‚àò ] and no random horizontal or vertical flipping was applied.</p><p>ViT2BERT -ViT (86M parameters) is the encoder <ref type="bibr" coords="10,315.63,227.75,16.08,10.91" target="#b28">[29]</ref>. It was warm-started with a checkpoint pre-trained on ImageNet-22K (14M images, 21 843 classes) at a resolution of 224√ó224 and then additionally trained on ImageNet-1K (1M images, 1 000 classes) at resolution of 384√ó384. BERT (110M parameters) is the decoder, which is pre-trained on uncased BookCorpus <ref type="bibr" coords="10,175.86,281.95,18.06,10.91" target="#b29">[30]</ref> and Wikipedia articles using self-supervised learning <ref type="bibr" coords="10,441.03,281.95,16.41,10.91" target="#b30">[31]</ref>. Both ViT and BERT are 12 layers with a hidden size of 768.</p><p>ViT2BERT (remove stopwords) -Identical to ViT2BERT, except that stopwords are additionally removed from the ground truth captions of the training and validation sets.</p><p>ViT2PubMedBERT -Identical to ViT2BERT, except that PubMedBERT (110M parameters) is the decoder. Its main difference to BERT is the pre-training data: uncased abstracts from PubMed (4.5B words) and articles from PubMed Central (13.5B words).</p><p>ViT2DistilGPT2 -Identical to ViT2BERT, except that DistilGPT2 (82M parameters) is the decoder. It is pre-trained using knowledge distillation where DistilGPT2 was the student and GPT2 was the teacher. OpenWebText, a reproduction of OpenAI's WebText corpus, was used as the pre-training data <ref type="bibr" coords="10,264.62,444.21,16.12,10.91" target="#b31">[32]</ref>. DistilGPT2 includes 6 layers with a hidden size of 768.</p><p>CvT2DistilGPT2 -Identical to ViT2DistilGPT2, except that CvT-21 (32M parameters) is the encoder. CvT-21 was warm-started with an ImageNet-22K checkpoint with a resolution of 384√ó384 <ref type="bibr" coords="10,169.82,507.32,16.25,10.91" target="#b32">[33]</ref>. It has three stages, with a combined 21 layers.</p><p>CvT2DistilGPT2 (retain aspect ratio) -Identical to CvT2DistilGPT2, except that the image is first resized using bilinear interpolation so that its smallest side has 384 pixels and its largest side is set such that it maintained the aspect ratio. Next, the resized image is cropped to a size of R 3√ó384√ó384 . The crop location was random during training and centred during testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CvT2DistilGPT2</head><p>‚Ä¢MIMIC-CXR -This is CvT2DistilGPT2 warm-started with a MIMIC-CXR checkpoint <ref type="bibr" coords="10,168.30,620.01,16.39,10.91" target="#b33">[34,</ref><ref type="bibr" coords="10,187.42,620.01,12.29,10.91" target="#b34">35]</ref>. The checkpoint was not additionally fine-tuned with reinforcement learning on MIMIC-CXR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CvT2DistilGPT2</head><p>‚Ä¢MIMIC-CXR (no. repeat n-gram size: 2) -Identical to CvT2DistilGPT2‚Ä¢MIMIC-CXR, except that a penalty was applied during caption generation to the probability of tokens to prevent an n-gram from appearing more than once in a caption (the penalty was realised by setting a token's probability to zero). An n-gram size of two was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CvT2DistilGPT2‚Ä¢MIMIC-CXR (no. repeat n-gram size: 3) -</head><p>Identical to CvT2DistilGPT2‚Ä¢MIMIC-CXR (no. repeat n-gram size: 2), except that an n-gram size of three was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Fine-tuning</head><p>Teacher forcing was used for fine-tuning <ref type="bibr" coords="11,268.45,213.01,16.09,10.91" target="#b35">[36]</ref>. Each model was implemented in PyTorch version 1.10.1 and trained with 4√óNVIDIA P100 16GB GPUs. To reduce memory consumption, we employed PyTorch's automatic mixed precision (a combination of 16-bit and 32-bit floating-point variables). For fine-tuning, the following configuration was used: categorical cross-entropy as the loss function; a mini-batch size of 32; early stopping with a patience of 20 epochs and a minimum delta of 1ùëí -4; AdamW optimiser for gradient descent optimisation <ref type="bibr" coords="11,442.96,280.75,16.41,10.91" target="#b36">[37]</ref>; an initial learning rate of 1ùëí-5 and 1ùëí-4 for the encoder and all other parameters, respectively, following <ref type="bibr" coords="11,89.29,307.85,16.36,10.91" target="#b37">[38]</ref>. All other hyperparameters for AdamW were set to their defaults. A model's best epoch was selected using the highest validation BLEU-4 score. The epochs that were selected based on this criterion for each model were: epoch 5 for ViT2BERT and ViT2BERT (remove stopwords), epoch 7 for ViT2PubMedBERT and CvT2DistilGPT2, and epoch 8 for ViT2DistilGPT2 and CvT2DistilGPT2‚Ä¢MIMIC-CXR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results &amp; Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">Model Performance</head><p>Here, we evaluate the performance of each encoder-to-decoder model to determine our best submission for the caption prediction task. The results of each encoder-to-decoder model are presented in Tables 7 (auxiliary metrics) and 8 (official metrics). One important consideration for this task is the multiple formatting steps applied to the predicted and ground truth captions before evaluation is performed with the official metrics, as described in Subsection 5.1. Hence, we wanted to determine if training with formatted ground truth captions is advantageous. Removing stopwords from the ground truth captions during training of ViT2BERT improved the validation CLEF-BLEU and CLEF-ROUGE-1 scores, as well as the test CLEF-BLEU score. However, it drastically decreased the test CLEF-BLEU score. Due to this, we abandoned this formatting strategy. Multiple strategies had a negligible impact on performance. This included the choice of encoder (ViT vs. CvT), as well as maintaining the aspect ratio of the medical images during image pre-processing.</p><p>When examining the performance of the decoders, BERT attained the highest test CLEF-BLEU score, while PubMedBERT scored the highest on test CLEF-ROUGE-1 (i.e., ViT2PubMedBERT vs. ViT2BERT and ViT2DistilGPT2). This indicates that the natural language understanding pre-training tasks of BERT and PubMedBERT are more transferable to the caption prediction tasks than the natural language generation pre-training strategies of DistilGPT2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Predicted concepts</head><p>The 107 concepts predicted by our best model for the concept detection task are shown in Tables <ref type="table" coords="17,120.04,577.57,12.65,10.91">A1</ref> and<ref type="table" coords="17,154.57,577.57,10.04,10.91">A2</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="14,112.66,533.83,394.53,10.91;14,112.66,547.38,393.33,10.91;14,112.28,560.93,393.99,10.91;14,112.66,574.48,267.33,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,248.92,547.38,257.06,10.91;14,112.28,560.93,295.20,10.91">The Effects of Changes in Utilization and Technological Advancements of Cross-Sectional Imaging on Radiologist Workload</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Eckel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">E</forename><surname>Diehn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">J</forename><surname>Bartholmai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">J</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">F</forename><surname>Kallmes</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.acra.2015.05.007</idno>
	</analytic>
	<monogr>
		<title level="j" coord="14,416.51,560.93,89.76,10.91">Academic Radiology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1191" to="1198" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,588.02,393.33,10.91;14,112.66,601.57,394.51,10.91;14,112.66,617.57,104.78,7.90" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,389.56,588.02,116.43,10.91;14,112.66,601.57,148.00,10.91">Global Health Workforce Labor Market Projections for 2030</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Goryakin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Scheffler</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12960-017-0187-2</idno>
	</analytic>
	<monogr>
		<title level="j" coord="14,269.07,601.57,127.33,10.91">Human Resources for Health</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,628.67,393.33,10.91;14,112.66,642.22,393.53,10.91;14,112.66,655.77,394.51,10.91;14,112.66,671.76,109.22,7.90" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,389.21,628.67,116.78,10.91;14,112.66,642.22,393.53,10.91;14,112.66,655.77,31.74,10.91">Artificial Intelligence and Machine Learning in Radiology: Opportunities, Challenges, Pitfalls, and Criteria for Success</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Thrall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Brink</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jacr.2017.12.026</idno>
	</analytic>
	<monogr>
		<title level="j" coord="14,152.90,655.77,203.37,10.91">Journal of the American College of Radiology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="504" to="508" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,86.97,394.62,10.91;15,112.28,100.52,393.71,10.91;15,112.66,114.06,352.57,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,320.44,86.97,186.83,10.91;15,112.28,100.52,279.01,10.91">An Intelligent Future for Medical Imaging: A Market Outlook on Artificial Intelligence for Medical Imaging</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zurkiya</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jacr.2019.07.019</idno>
	</analytic>
	<monogr>
		<title level="j" coord="15,400.02,100.52,105.97,10.91;15,112.66,114.06,92.66,10.91">Journal of the American College of Radiology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="165" to="170" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,127.61,395.01,10.91;15,112.66,141.16,395.17,10.91;15,112.39,154.71,394.80,10.91;15,112.66,168.26,394.62,10.91;15,112.66,181.81,393.33,10.91;15,112.66,195.36,395.17,10.91;15,112.66,208.91,393.54,10.91;15,112.66,222.46,170.14,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,352.50,168.26,154.78,10.91;15,112.66,181.81,311.34,10.91">Overview of the ImageCLEF 2022: Multimedia Retrieval in Medical, Social Media and Nature Applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>≈ûtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,446.87,181.81,59.11,10.91;15,112.66,195.36,395.17,10.91;15,112.66,208.91,239.58,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<title level="s" coord="15,359.20,208.91,147.00,10.91;15,112.66,222.46,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,236.01,395.17,10.91;15,111.81,249.56,395.37,10.91;15,112.66,263.11,393.33,10.91;15,112.66,276.66,216.46,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="15,326.22,249.56,180.96,10.91;15,112.66,263.11,181.70,10.91">Overview of ImageCLEFmedical 2022 -Caption Prediction and Concept Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garc√≠a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,316.25,263.11,189.74,10.91;15,112.66,276.66,97.38,10.91">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,290.20,393.33,10.91;15,112.66,303.75,394.61,10.91;15,112.31,317.30,387.05,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="15,420.77,290.20,85.22,10.91;15,112.66,303.75,149.45,10.91">Arachnoid cyst in a patient with psychosis: Case report</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Talina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Carreiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guimar√£es</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Xavier</surname></persName>
		</author>
		<idno type="DOI">10.1186/1744-859X-6-16</idno>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1933420/.doi:10.1186/1744-859X-6-16" />
	</analytic>
	<monogr>
		<title level="j" coord="15,268.77,303.75,123.76,10.91">Annals of general psychiatry</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="16" to="16" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,330.85,393.33,10.91;15,112.66,344.40,394.62,10.91;15,112.66,357.95,381.14,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="15,379.39,330.85,126.60,10.91;15,112.66,344.40,158.95,10.91">A Rare Differential Diagnosis of an Adrenal Mass: A Case Report</title>
		<author>
			<persName coords=""><forename type="first">A.-K</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pless</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Soll</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hochuli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gubler</surname></persName>
		</author>
		<idno type="DOI">10.1159/000481501</idno>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5731155/.doi:10.1159/000481501" />
	</analytic>
	<monogr>
		<title level="j" coord="15,281.44,344.40,114.05,10.91">Case reports in oncology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="981" to="986" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,371.50,393.33,10.91;15,112.26,385.05,395.02,10.91;15,112.31,398.60,392.99,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="15,263.10,371.50,242.89,10.91;15,112.26,385.05,204.07,10.91">Outward bulging of the right parietal bone in connection with fibrous dysplasia in an infant: a case report</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Al Kaissi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Klaushofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Grill</surname></persName>
		</author>
		<idno type="DOI">10.1186/1757-1626-1-347</idno>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2611982/.doi:10.1186/1757-1626-1-347" />
	</analytic>
	<monogr>
		<title level="j" coord="15,323.28,385.05,58.38,10.91">Cases journal</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="347" to="347" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,412.15,393.33,10.91;15,112.66,425.70,395.01,10.91;15,112.66,439.25,363.85,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="15,187.00,412.15,318.99,10.91;15,112.66,425.70,122.86,10.91">Follicular thyroid carcinoma in a patient with myasthenia gravis and thymoma: a rare association</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Htet</surname></persName>
		</author>
		<idno type="DOI">10.3332/ecancer.2012.274</idno>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3469247/.doi:10.3332/ecancer.2012.274" />
	</analytic>
	<monogr>
		<title level="j" coord="15,243.76,425.70,100.72,10.91">Ecancermedicalscience</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="274" to="274" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,452.79,393.60,10.91;15,112.39,466.34,395.44,10.91;15,112.66,479.89,394.04,10.91;15,112.66,493.44,245.44,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="15,344.57,452.79,161.69,10.91;15,112.39,466.34,267.25,10.91">Robotic-assisted right hepatectomy via anterior approach for intrahepatic cholangiocarcinoma</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Goja</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Soin</surname></persName>
		</author>
		<idno type="DOI">10.14701/ahbps.2017.21.2.80</idno>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5449368/.doi:10.14701/ahbps.2017.21.2.80" />
	</analytic>
	<monogr>
		<title level="j" coord="15,391.56,466.34,116.27,10.91;15,112.66,479.89,85.98,10.91">Annals of hepato-biliarypancreatic surgery</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="80" to="83" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,506.99,393.33,10.91;15,112.33,520.54,395.50,10.91;15,112.66,534.09,394.53,10.91;15,112.66,547.64,393.53,10.91;15,112.66,561.19,393.33,10.91;15,112.66,574.74,392.28,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="15,370.24,506.99,135.74,10.91;15,112.33,520.54,171.65,10.91;15,448.67,547.64,57.52,10.91;15,112.66,561.19,393.33,10.91;15,112.66,574.74,118.52,10.91">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<editor>D. Stoyanov, Z. Taylor, S. Balocco, R. Sznitman, A. Martel, L. Maier-Hein, L. Duong, G. Zahnd, S. Demirci, S. Albarqouni, S.-L. Lee, S. Moriconi, V. Cheplygina, D. Mateus, E. Trucco, E. Granger, P. Jannin</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="180" to="189" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>Radiology Objects in COntext (ROCO): A Multimodal Image Dataset</note>
</biblStruct>

<biblStruct coords="15,112.66,588.29,394.53,10.91;15,112.66,601.84,393.33,10.91;15,112.48,615.39,261.79,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,307.23,601.84,176.03,10.91">Scikit-learn: Machine learning in python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,492.05,601.84,13.94,10.91;15,112.48,615.39,167.70,10.91">the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,628.93,393.33,10.91;15,112.66,642.48,394.53,10.91;15,112.66,656.03,103.61,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="15,345.70,628.93,160.29,10.91;15,112.66,642.48,67.28,10.91">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,228.08,642.48,274.55,10.91">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,669.58,393.33,10.91;16,112.66,86.97,394.53,10.91;16,112.66,100.52,195.59,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="15,350.55,669.58,155.44,10.91;16,112.66,86.97,39.90,10.91">Densely Connected Convolutional Networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.243</idno>
	</analytic>
	<monogr>
		<title level="m" coord="16,197.67,86.97,304.56,10.91">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,114.06,394.62,10.91;16,112.66,127.61,395.00,10.91;16,112.66,141.16,137.64,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="16,267.98,114.06,214.89,10.91">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m" coord="16,135.82,127.61,314.42,10.91">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,154.71,393.33,10.91;16,112.66,168.26,345.14,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="16,242.31,154.71,263.68,10.91;16,112.66,168.26,51.39,10.91">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,187.28,168.26,240.50,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,181.81,393.33,10.91;16,112.66,195.36,102.10,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m" coord="16,251.96,181.81,172.98,10.91">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="16,112.66,208.91,394.53,10.91;16,112.66,222.46,346.82,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="16,178.42,208.91,323.86,10.91">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,127.29,222.46,202.02,10.91">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,236.01,393.33,10.91;16,112.66,249.56,393.33,10.91;16,112.66,263.11,159.65,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="16,385.90,236.01,120.09,10.91;16,112.66,249.56,27.19,10.91">Designing network design spaces</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">P</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,165.27,249.56,340.72,10.91;16,112.66,263.11,51.39,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,276.66,393.33,10.91;16,112.66,290.20,116.47,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="16,247.13,276.66,160.21,10.91">Mixture of experts: a literature survey</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Masoudnia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ebrahimpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,414.31,276.66,91.68,10.91;16,112.66,290.20,32.53,10.91">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="275" to="293" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,303.75,393.33,10.91;16,112.66,317.30,393.33,10.91;16,112.66,330.85,394.53,10.91;16,112.66,344.40,395.01,10.91;16,112.66,357.95,155.44,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="16,304.38,303.75,201.60,10.91;16,112.66,317.30,92.87,10.91">Bleu: a Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
		<ptr target="https://www.aclweb.org/anthology/P02-1040.doi:10.3115/1073083.1073135" />
	</analytic>
	<monogr>
		<title level="m" coord="16,232.68,317.30,273.31,10.91;16,112.66,330.85,328.91,10.91">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,371.50,393.33,10.91;16,112.66,385.05,393.33,10.91;16,112.28,398.60,394.91,10.91;16,112.28,412.15,374.21,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="16,205.93,371.50,300.06,10.91;16,112.66,385.05,37.64,10.91">Automatic evaluation of summaries using N-gram co-occurrence statistics</title>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073445.1073465</idno>
	</analytic>
	<monogr>
		<title level="m" coord="16,172.74,385.05,333.25,10.91;16,112.28,398.60,391.31,10.91">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology -NAACL &apos;03</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology -NAACL &apos;03</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,425.70,393.33,10.91;16,112.66,439.25,395.17,10.91;16,112.66,452.79,394.53,10.91;16,112.28,466.34,395.00,10.91;16,112.66,479.89,157.69,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="16,210.22,425.70,295.77,10.91;16,112.66,439.25,162.15,10.91">METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/W05-0909" />
	</analytic>
	<monogr>
		<title level="m" coord="16,301.31,439.25,206.52,10.91;16,112.66,452.79,394.53,10.91;16,112.28,466.34,189.41,10.91">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, Association for Computational Linguistics</title>
		<meeting>the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, Association for Computational Linguistics<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,493.44,393.33,10.91;16,112.66,506.99,393.33,10.91;16,112.66,520.54,117.06,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="16,313.72,493.44,192.27,10.91;16,112.66,506.99,46.69,10.91">CIDEr: Consensus-Based Image Description Evaluation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,186.41,506.99,319.58,10.91;16,112.66,520.54,86.61,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,534.09,393.33,10.91;16,112.66,547.64,394.53,10.91;16,112.66,561.19,270.41,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="16,336.55,534.09,169.44,10.91;16,112.66,547.64,82.39,10.91">SPICE: Semantic Propositional Image Caption Evaluation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46454-1_24</idno>
	</analytic>
	<monogr>
		<title level="m" coord="16,217.85,547.64,133.50,10.91">Computer Vision -ECCV 2016</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,574.74,393.33,10.91;16,112.66,588.29,395.01,10.91;16,112.66,601.84,238.25,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="16,375.26,574.74,130.72,10.91;16,112.66,588.29,98.69,10.91">BERTScore: Evaluating Text Generation with BERT</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SkeHuCVFDr" />
	</analytic>
	<monogr>
		<title level="m" coord="16,235.57,588.29,241.90,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,615.39,393.33,10.91;16,112.66,628.93,393.33,10.91;16,112.66,642.48,394.53,10.91;16,112.66,656.03,393.94,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="16,191.48,615.39,314.50,10.91;16,112.66,628.93,224.32,10.91">Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<idno type="DOI">10.3115/1218955.1219032</idno>
		<ptr target="https://aclanthology.org/P04-1077.doi:10.3115/1218955.1219032" />
	</analytic>
	<monogr>
		<title level="m" coord="16,360.22,628.93,145.77,10.91;16,112.66,642.48,288.22,10.91">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)</title>
		<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="605" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,669.58,395.16,10.91;17,112.66,86.97,393.33,10.91;17,112.41,100.52,393.57,10.91;17,112.33,114.06,129.27,10.91" xml:id="b28">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De-Hghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929[cs.CV</idno>
		<idno>arXiv:2010.11929</idno>
		<title level="m" coord="17,418.93,86.97,87.06,10.91;17,112.41,100.52,269.72,10.91">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,127.61,393.32,10.91;17,112.66,141.16,393.33,10.91;17,112.66,154.71,393.33,10.91;17,112.33,168.26,58.77,10.91" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="17,466.25,127.61,39.73,10.91;17,112.66,141.16,393.33,10.91;17,112.66,154.71,62.29,10.91">Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,197.92,154.71,308.07,10.91;17,112.33,168.26,28.58,10.91">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,181.81,393.33,10.91;17,112.33,195.36,393.65,10.91;17,112.66,208.91,393.32,10.91;17,112.66,222.46,393.33,10.91;17,112.66,236.01,394.03,10.91;17,112.66,249.56,234.20,10.91" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="17,316.84,181.81,189.14,10.91;17,112.33,195.36,192.76,10.91">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1423.doi:10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m" coord="17,330.46,195.36,175.52,10.91;17,112.66,208.91,393.32,10.91;17,112.66,222.46,99.97,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="17,112.66,263.11,394.04,10.91;17,112.66,276.66,97.76,10.91" xml:id="b31">
	<monogr>
		<title level="m" type="main" coord="17,222.66,263.11,98.11,10.91">OpenWebText Corpus</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Cohen</surname></persName>
		</author>
		<ptr target="http://Skylion007.github.io/OpenWebTextCorpus" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,290.20,393.33,10.91;17,112.66,303.75,352.82,10.91" xml:id="b32">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808[cs.CV</idno>
		<idno>arXiv:2103.15808</idno>
		<title level="m" coord="17,371.26,290.20,134.73,10.91;17,112.66,303.75,101.12,10.91">CvT: Introducing Convolutions to Vision Transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,317.30,393.61,10.91;17,112.66,330.85,251.60,10.91" xml:id="b33">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nicolson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dowling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Koopman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.09405</idno>
		<title level="m" coord="17,293.62,317.30,212.64,10.91;17,112.66,330.85,119.60,10.91">Improving Chest X-Ray Report Generation by Leveraging Warm-Starting</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,344.40,393.53,10.91;17,112.66,357.95,229.54,10.91" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="17,285.58,344.40,220.61,10.91;17,112.66,357.95,70.12,10.91">Chest X-Ray Report Generation Checkpoints for CvT2</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nicolson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dowling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Koopman</surname></persName>
		</author>
		<idno type="DOI">10.25919/64WX-0950</idno>
	</analytic>
	<monogr>
		<title level="j" coord="17,112.66,357.95,70.12,10.91">DistilGPT</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,371.50,393.32,10.91;17,112.66,385.05,397.48,10.91;17,112.66,401.04,20.20,7.90" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="17,223.84,371.50,282.14,10.91;17,112.66,385.05,72.16,10.91">A Learning Algorithm for Continually Running Fully Recurrent Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1989.1.2.270</idno>
	</analytic>
	<monogr>
		<title level="j" coord="17,193.59,385.05,89.94,10.91">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,412.15,393.33,10.91;17,112.66,425.70,393.33,10.91;17,112.66,439.25,58.17,10.91" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="17,229.97,412.15,186.36,10.91">Decoupled Weight Decay Regularization</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bkg6RiCqY7" />
	</analytic>
	<monogr>
		<title level="m" coord="17,446.71,412.15,59.28,10.91;17,112.66,425.70,182.30,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,452.79,393.32,10.91;17,112.33,466.34,393.65,10.91;17,112.66,479.89,394.53,10.91;17,112.41,493.44,228.84,10.91" xml:id="b37">
	<analytic>
		<title level="a" type="main" coord="17,278.54,452.79,227.44,10.91;17,112.33,466.34,54.21,10.91">Generating Radiology Reports via Memory-driven Transformer</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.112</idno>
	</analytic>
	<monogr>
		<title level="m" coord="17,190.80,466.34,315.18,10.91;17,112.66,479.89,321.45,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1439" to="1449" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
