<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.38,75.32,450.58,17.04;1,81.14,96.08,433.15,17.04;1,154.82,116.84,285.58,17.04">SSN MLRG at ImageCLEFmedical Caption 2022: Medical Concept Detection and Caption Prediction using Transfer Learning and Transformer based Learning Approaches</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.02,149.50,67.61,10.80"><forename type="first">Sheerin</forename><surname>Sitara</surname></persName>
							<email>sheerinsitaran@ssn.edu</email>
						</author>
						<author>
							<persName coords="1,142.55,149.50,76.33,10.80"><forename type="first">Noor</forename><surname>Mohamed</surname></persName>
						</author>
						<author>
							<persName coords="1,245.81,149.50,91.64,10.80"><forename type="first">Kavitha</forename><surname>Srinivasan</surname></persName>
							<email>kavithas@ssn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">Sri Sivasubramaniya Nadar College of Engineering</orgName>
								<address>
									<postCode>-603110</postCode>
									<settlement>Kalavakkam</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.38,75.32,450.58,17.04;1,81.14,96.08,433.15,17.04;1,154.82,116.84,285.58,17.04">SSN MLRG at ImageCLEFmedical Caption 2022: Medical Concept Detection and Caption Prediction using Transfer Learning and Transformer based Learning Approaches</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CBA1B23122FE1D6FC4177D26DAA3BF06</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Concept Detection</term>
					<term>Caption Prediction</term>
					<term>Multi-label Classification</term>
					<term>Information Retrieval</term>
					<term>BERT</term>
					<term>SAE</term>
					<term>MLP</term>
					<term>GRU</term>
					<term>ResNet</term>
					<term>DenseNet</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The computer aided medical system for various applications is required now-a-days for an early and effective analysis. However most of the medical data are, publicly unavailable and exist in unstructured and unlabelled format are real challenges in developing the medical system. To address these issues, ImageCLEF forum is conducting many tasks on the medical domain from 2016 onwards. This year one of the tasks is medical concept detection and caption prediction. For this task, our team has proposed two concept detection techniques and caption prediction techniques. The concept detection models are developed using multi-label classification and information retrieval approaches resulted the F1-score and secondary F1score as 0.418 and 0.654 respectively. The caption prediction models are implemented using ResNet with Bidirectional Encoder Representations from Transformers (BERT) and, Sparse Auto Encoder (SAE) with Multi-Layer Perceptron (MLP) and Gated Recurrent Unit (GRU), which resulted a BLEU and BERT score of 0.160 and 0.545 respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The advancement in medical domain is trying to improvise the quality and quantity of medical data. These medical data are available in different formats such as medical images, clinical reports and doctor transcriptions. Among these, medical images play a vital role in diagnosis. The different types of medical images are X-Ray, Computed Tomography (CT), Magnetic Resonance Imaging (MRI), f-MRI, mammogram, ultrasonography, Positron Emission Tomography (PET), Single Photon Emission Computerized Tomography (SPECT) and thermography <ref type="bibr" coords="1,331.15,527.73,11.79,9.94" target="#b0">[1]</ref>. The ImageCLEF forum is conducting various tasks related to medical images such as caption prediction, concept detection, Tuberculosis (TB) type detection, Multi-Drug Resistant (MDR) detection, TB severity score calculation, CT report generation and Visual Question Answering (VQA) from 2016 onwards. In this, we have participated in concept detection and caption prediction tasks during the current year.</p><p>In concept detection and caption prediction tasks <ref type="bibr" coords="1,307.49,591.09,11.71,9.94" target="#b1">[2]</ref>, the dataset given by ImageCLEF consists of different modalities such as CT, XR, PET, angiogram and ultrasound images. The concepts and captions corresponds to these images are created by medical annotator from PubMED articles and Unified Medical Language System (UMLS) terms. Finally, these datasets are validated and verified by medical domain experts. These datasets are used to develop concept detection and caption prediction model using suitable techniques and evaluated performance metrics, are discussed in the following paragraphs.</p><p>The concept detection approaches are: multi-label classification approach using DenseNet <ref type="bibr" coords="1,507.71,666.96,11.86,9.94" target="#b2">[3]</ref>, information retrieval approach using DenseNet <ref type="bibr" coords="1,285.26,679.56,12.80,9.94" target="#b3">[4]</ref> and EfficientNet <ref type="bibr" coords="1,378.55,679.56,11.87,9.94" target="#b4">[5]</ref>, ensemble of classifier based on DenseNet and Feed-Forward Neural Network (FFNN) <ref type="bibr" coords="2,339.71,74.42,13.12,9.94" target="#b5">[6]</ref> and, Learned Perceptual Image Path Similarity (LPIPS) <ref type="bibr" coords="2,161.12,87.04,12.98,9.94" target="#b6">[7]</ref> based on VGGNet. Among these approaches, multi-label classification and information retrieval approaches along with DenseNet are used for this task execution. The multi-label classification approach (model 1) is chosen because it has an ability to find conditional dependencies between the labels and the independence between the labels are computed based on the specific condition. The information retrieval approach (model 2) is adapted because it effectively identifies the keyword in the query and retrieves the required answer from the dataset.</p><p>The caption prediction techniques are grouped into, (i). Techniques which performs both image and test processing like Visual Transformers <ref type="bibr" coords="2,252.89,175.60,11.79,9.94" target="#b7">[8]</ref>, (ii). Combination of techniques for image processing and text processing. The image processing can be performed by pre-trained models like DenseNet <ref type="bibr" coords="2,507.65,188.32,11.69,9.94" target="#b8">[9]</ref>, ResNet <ref type="bibr" coords="2,109.90,200.92,17.02,9.94" target="#b9">[10]</ref>, VGGNet <ref type="bibr" coords="2,181.75,200.92,18.41,9.94" target="#b10">[11]</ref> or autoencoder like Sparse Auto Encoder (SAE) <ref type="bibr" coords="2,439.90,200.92,18.44,9.94" target="#b11">[12]</ref> and, the text processing can be performed by attention based encoder-decoder <ref type="bibr" coords="2,375.19,213.52,16.90,9.94" target="#b12">[13]</ref>, Long Short Term Memory (LSTM) <ref type="bibr" coords="2,112.19,226.24,16.91,9.94" target="#b13">[14]</ref>, Gated Recurrent Unit (GRU) <ref type="bibr" coords="2,270.53,226.24,18.32,9.94" target="#b14">[15]</ref> or, transformer-based architectures like BERT <ref type="bibr" coords="2,502.27,226.24,16.88,9.94" target="#b15">[16]</ref>, GPT-2 <ref type="bibr" coords="2,106.09,238.84,16.99,9.94" target="#b16">[17]</ref>. Among the techniques, ResNet with BERT (model 3) and, SAE with MLP and GRU (model 4) are chosen for this task execution because, (i). The deep architecture and millions of trainable parameters in ResNet reduced the degradation problem and made it more suitable for image captioning problem. (ii). BERT predicts the context of the words from both left to right context and right to left context simultaneously. (iii). SAE supports dimensionality reduction and it has the capability to reconstruction the data from the latent space. (iv). GRU generates the next word in the sequence based on the previous sequence of words and it is better than LSTM in terms of memory and speed. (v). MLP has the capability to learn non-linear models.</p><p>The performance metrics given by ImageCLEF for evaluating concept detection and caption prediction tasks are: F1 Score, Bilingual Evaluation Understudy (BLEU) score, Recall-Oriented Understudy for Gisting Evaluation (ROUGE), Metric for Evaluation of Translation with Explicit Ordering (METEOR), Consensus-based Image Description Evaluation (CIDEr), Semantic Propositional Image Caption Evaluation (SPICE) and Bidirectional Encoder Representations from Transformers (BERT) Score <ref type="bibr" coords="2,200.57,403.39,16.99,9.94">[18]</ref>. The F1 Score is computed as the weighted average of precision and recall. It is more useful than accuracy especially if there is an uneven class distribution. The BLEU score compares the n-gram of the predicted caption with the n-gram of the reference caption to count the number of matches. The advantage of using BLEU over other score is, it provides the overall assessment of model quality. The ROUGE measures the longest common subsequence between predicted and reference caption. Like, BLEU score, ROUGE similarity value also lies between zero and one. It determines the overall quality of the predicted answer. The METEOR is based on harmonic mean of unigram recall and precision, with recall weighted higher than precision. It is used in a language-independent manner, and hence it has an ability to model features (especially synonyms, stem words and paraphrasing) of a specific language. The CIDEr measures the sentence similarity between the predicted and reference caption by inherently captures the notions of grammaticality, saliency, importance and accuracy. The SPICE measures the effectiveness of image captions in terms of recovered objects, attributes and the relationship between them. The BERT computes the similarity between each token in the predicted caption with each token in the reference caption. It overcomes the flaw of BLEU score by considering semantic and syntactic abilities.</p><p>The remaining part of the paper are discussed with following subsections. In Sect. 2, the concept detection and caption prediction datasets and the inference from these datasets are discussed. The design of the proposed system is explained in Section 3. A brief summary about the implementation, result and the evaluation of all runs for both tasks are given in Section 4 and, conclusion and future work are summarized at the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Dataset</head><p>The concept detection and caption prediction datasets comprises of training set, validation set and test set and it is represented in terms of number of images, concept IDs, concept names and captions in Table <ref type="table" coords="2,100.81,726.72,4.14,9.94" target="#tab_0">1</ref>. In concept detection dataset, each image corresponds to one or more concept IDs and each concept ID represents one concept names. In caption prediction dataset, each image corresponds to only one caption. Some of the interesting inferences from concept detection dataset is given in Table <ref type="table" coords="3,468.84,240.52,5.52,9.94" target="#tab_1">2</ref> and 3 and caption prediction dataset is shown in Figure <ref type="figure" coords="3,293.69,253.24,4.14,9.94">4</ref>. The maximum and minimum number of tags corresponds to the image along with the concept ID are mentioned in Table <ref type="table" coords="3,411.47,265.87,4.24,9.94" target="#tab_1">2</ref>. Because each image is mapped with one or more tags and each tag corresponds to one concept name only. The length of the concept name ranges from one (minimum) to twelve (maximum) and it is listed in Table <ref type="table" coords="3,464.07,291.19,4.18,9.94" target="#tab_1">2</ref>. In Table <ref type="table" coords="3,125.04,497.01,4.14,9.94" target="#tab_2">3</ref>, the maximum occurrence of Concept IDs in the dataset are listed along with its frequency and concept name for better understanding.  The Figure <ref type="figure" coords="4,137.74,284.83,5.52,9.94" target="#fig_0">1</ref> represents the radiology images corresponds to the minimum and maximum length of the caption. In the caption prediction dataset, the length of the caption varies from 1 (minimum) to 391 (maximum). The length of the caption and the words used in the caption varies for different human annotator, based on the annotator's domain knowledge and vocabulary skills and, the region of interest. The caption for the radiology images in Figure <ref type="figure" coords="4,282.62,335.47,5.52,9.94" target="#fig_0">1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">System Design</head><p>The system design of the proposed concept detection and caption prediction tasks are shown in Figure <ref type="figure" coords="4,103.45,431.11,5.52,9.94" target="#fig_2">2</ref> and 3. In Figure <ref type="figure" coords="4,184.21,431.11,4.14,9.94" target="#fig_2">2</ref>, two concept detection models are developed based on images, concept IDs and concept names in the training phase and, the generated model is validated by detecting the concept for the radiology images in the test set. In Figure <ref type="figure" coords="4,288.99,456.45,4.14,9.94" target="#fig_3">3</ref>, two caption prediction models are developed based on the radiology images and captions in the training phase. The generated caption prediction model is validated by predicting the caption for the medical images in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Concept Detection</head><p>The concept detection model is developed in two ways namely, multi-label classification (model 1) and information retrieval (model 2) using DenseNet. The multi-label classification system will predict all the suitable concepts which has probability value greater than criterion value for each image in the test set. The information retrieval system will retrieve the suitable concept from the training set based on the nearest neighbour and cosine similarity for each image in the test set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Model 1: Multi-label classification approach</head><p>In model 1, each image features are extracted using DenseNet and mapped with respective concepts in the training phase. For this, all layers except the last layer in DenseNet is freeze so weights remain same throughout the training and only the weights from added layers will update gradually. Then global average pooling, dense layer with sigmoid activation function are added after the last layer which predicts the probability value for each image. The global average pooling used in this model creation acts as a great alternative for CNN because it generates the one feature map for each corresponding concept category. The number of nodes in the dense layer is maintain to be equal to the number of concept names then only each node in this layer generates the probability value for each concept with respect to the image. Among the probability value, the concept which has probability value greater than criterion value is considered as the predicted concept. Moreover, the model is fine-tuned by minimizing the mean square error between the predicted and ground truth value. Then in the testing phase, the generated model is evaluated for radiology images in the test set which detects one or more concepts for each images. The concepts extracted for test set under different criteria are combined by (i). union of union (i.e., merging list of concepts from two results), (ii). Intersection of intersection (i.e., merging the common concepts from two results).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Model 2: Information retrieval approach</head><p>The model 2 generates feature vector for each image and mapped with concept name by DenseNet which is same as in multi-label classification approach. In the testing phase, the feature vector is extracted for each image, then the cosine similarity is computed between the test image and the training image feature vector. The training images which has the similarity score more than criterion value are retrieved. The respective concepts of these training images are considered as a resulted concept for the test images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Caption Prediction</head><p>The caption prediction system is developed using ResNet followed by BERT (model 3) and, SAE followed by MLP and GRU (model 4). In model 3, based on the given radiology image, the system will generate the caption based on the context of the image. The SAE in model 4, extract the significant information based on the dimension of the latent space and then GRU and MLP will generate the sequence of next word in the caption based on previous word. In both models, caption prediction model is fine-tuned by minimizing mean square error loss, cross entropy loss and KL-divergence loss in the training phase. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Model 3: ResNet with BERT</head><p>The model 3 takes images and the respective captions as input and generates the image and text feature vectors by ResNet and BERT. Before that, glove embedding is used to generate the vocabulary list and length of the captions are normalized based on the maximum caption length by padding. In the training phase, the caption is predicted word by word from the vocabulary list in the glove embedding based on the feature vector and the length of the caption is maintained within the maximum caption length. This model is fine-tuned by minimizing the error rate and the obtained model is used for caption prediction in the test phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Model 4: SAE with MLP and GRU</head><p>In this model, the images and the respective captions in the training phase is taken as input by the SAE, MLP and GRU. The SAE consists of seven layers, the middle layer represents the latent space, the layer before the latent space represents the encoder and the layer after that, represents the decoder. In the encoder part, the nodes in each layer are in increasing fashion (power of two). The latent space has the maximum dimension and in decoder part, the node distribution are in decreasing fashion which is also power of two. Even though, many nodes are available only the particular sequence of nodes are adapted by each sample and it differs from sample to sample. In the encoder part, convolution2D layer, maxpooling2D and leuky ReLU are used to generate flatten sequence of features and in the decoder part, flatten features are unflatten then used convolutionTranspose2D, unmaxpooling2D and leuky ReLU to generate image feature vector. The multi-layer perceptron and Gated Recurrent Unit are used to extract the text feature vector based on the image feature vector and it is further supported by word embedding. In the training phase, the model is fine-tuned by reducing the loss value which we aforementioned and based on this obtained model, the captions are predicted for test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation and Results</head><p>In this section, the proposed concept detection and caption prediction models are implemented and the results are compared and analysed using performance metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">System Specification</head><p>The hardware and software required for the implementation of concept detection and caption prediction model includes, (i). Intel i5 processor with NVIDIA graphics card, 4800M at 4.3GHZ clock speed, 16GB RAM, Graphical Processing Unit and 2TB disk space, (ii). Linux -Ubuntu 20.04 operating system, Python 3.7 package with required libraries like tensorflow, torch, sklearn, nltk, pickle, pandas, etc.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results of Concept Detection Models</head><p>The test set results obtained from concept detection model are measured by F1-score and secondary F1-score as given in Table <ref type="table" coords="7,191.52,118.12,4.16,9.94" target="#tab_3">4</ref>, the first six runs are based on model 1 and the last one is based on model 2. The description about the runs are as follows, (i). In run1, the model is implemented based on the features learned in 50 epochs and early stopped if the continuous three epochs having same accuracy values. The particular concept ID are detected only if the obtained probability value for the predicted concept ID with respect to the image is greater than 0.4. (ii). Run2 is same as run1, but the probability value is from 0.4 to 0.1. (iii). In run3, the early stopping value is updated from three to five. (iv). Run4 is the union of results obtained from run2 and run3. The results are merged based on the test set image name which acts as a primary key. (v). Same as run1, but the number of epochs are increased from 50 to 100. (vi). Like run4, run6 is the intersection of results obtained from run2 and run3. The concept ID which is common in both results are considered for evaluation. (vii). In run7, the number of epochs is fixed to be 50, early stopped within three epoch and, based on the cosine similarity value between the particular test set image and training set images and the nearest neighbour of one is considered for evaluation.</p><p>From the results, it has been observed that, selection of appropriate hyperparameters plays a significant role in improving the model performance. They are, (i). Early stopping at third epoch performed better than fifth epoch and also avoids the overfitting problem. (ii). Samples with more than one label identifies all the concept names by fixing the probability value as 0.1 rather than the recommended value of 0.4. as per many research papers. (iii). Union of two independent results gives better performance than the intersection of result (iv). Cosine similarity value retrieves the more relevant images for each test image than the Jaccard similarity value. Among the results, run2 and run4 obtained better performance value in terms of F1 score and secondary F1 score and it is italicized in Table <ref type="table" coords="7,279.16,562.89,4.14,9.94" target="#tab_3">4</ref>. From run2, it has been inferred that probability value greater than 0.1 gives considerably better result not only for this case but also for most of the cases. Moreover, the early stopping with a patience of three epochs also achieved better performance value than five epochs. The run4 exhibit the characteristics of run2 and run3 because run4 is the union of the results of these two runs. In concept detection task, totally 61 teams were registered, 11 teams were participated and they 104 submissions were recorded. Among this, we have submitted seven successful submissions and achieved seventh rank in ImageCLEF 2022 Concept Detection task. The overall ranking achieved by top 10 teams are listed in Table <ref type="table" coords="8,153.95,208.60,4.14,9.94" target="#tab_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results of Caption Prediction Models</head><p>The brief description about each run in terms of techniques used and the performance obtained for caption prediction model are listed in Table <ref type="table" coords="8,270.62,279.07,4.16,9.94" target="#tab_5">6</ref>. Among seven runs, model 3 is used in three runs and, model 4 is used in four runs. The description about each run are as follows. In run1, model 4 is used, in which the number of epochs is fixed to be 50, early stopped with three epochs, used adam optimizer and maintain learning rate to be 0.002. The run2 is same as run1, but the learning rate is reduced to 0.001 and used RMSProp optimizer. The model 3 is used in run3, in which the number of epochs is fixed to be 50, early stopping with three epochs, used "adam" optimizer and maintain learning rate to be 0.004. The run4 is same as run2, but the latent size dimension is doubled. In run 5, the batch size is increased to 64 and remaining is as same as run4. The batch size can't be increased further of memory insufficiency problem. The run6 is same as run3 but the optimizer here used is Stochastic Gradient Descent. In run7, the learning rate is reduced to 0.001, early stopped in fifth epoch and remaining is as same as run6.</p><p>The analysis of results, leads to some important observations: (ii). The learning rate value 0.004 gives better performance than 0.001 or 0.002 (ii). In terms of optimizer, adam optimizer improves the performance of the model because few parameters are required for tuning and reduces computation time (iii). Early stopping at the third epoch performs better than the fifth epoch and avoids overfitting problem (iv). The model is trained for the batch size of 64 only, since the higher batch size leads to data insufficiency problem. From the results, it has been inferred that run3 achieved better performance value and it is italicized in Table <ref type="table" coords="8,111.24,699.96,4.15,9.94" target="#tab_5">6</ref>. The overall results show that the lowest learning rate and early stopping gives better result. In caption prediction task, totally 43 teams were registered, 10 teams were participated and 81 submissions were recorded. Among this, we have made seven successful submissions and achieved tenth rank in ImageCLEF 2022 caption prediction task. The overall ranking achieved by top 10 teams are given in Table <ref type="table" coords="8,154.55,750.60,4.14,9.94" target="#tab_6">7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Works</head><p>This paper describes four different approaches to solve ImageCLEF 2022 medical concept detection and caption prediction tasks. The concept detection tasks are implemented using two approaches namely, multi-label classification and information retrieval approach using DenseNet pre-trained model. The caption prediction tasks are implemented in two ways as, ResNet followed by BERT and, SAE followed by MLP and GRU. From the results of these models, it has been inferred that multi-label classification gives better result for concept detection and, ResNet followed by BERT gives better results for caption prediction task. As compared with the best scores given by ImageCLEF, the proposed concept detection model lacks only by 0.033 and 0.131 in terms of F1-score and secondary F1-score respectively. And the proposed caption prediction model lacks only by 0.323 and 0.016 in terms of BLEU score and BERT score respectively.</p><p>In future work, the performance of the concept detection system can be improved by majority voting instead of union by union or intersection by intersection. For caption prediction system, the performance can be enhanced by Generative Pre-trained Transformer instead of BERT. The overall performance of the system can be improved by reducing the irrelevant samples, increasing the number of epochs and maintaining the minimum learning rate.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,72.02,258.41,304.17,11.04;4,72.00,97.32,451.20,158.88"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Minimum and maximum length caption's radiology images</figDesc><graphic coords="4,72.00,97.32,451.20,158.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,291.14,335.47,232.23,9.94;4,72.02,348.07,451.47,9.94;4,72.02,360.79,21.24,9.94"><head></head><label></label><figDesc>are, (a) and (b) shows unusual cause of a lung mass [20], (c) represents a sudden-onset facial oedema [21] and, (d) illustrates chikungunya virus infection [22].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,72.02,236.06,280.33,11.04;5,72.00,72.00,451.20,161.52"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: System design of proposed concept detection model</figDesc><graphic coords="5,72.00,72.00,451.20,161.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,72.02,217.10,281.06,11.04;6,72.00,72.00,451.20,143.04"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: System design of proposed caption prediction model</figDesc><graphic coords="6,72.00,72.00,451.20,143.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,72.02,86.66,428.19,136.20"><head>Table 1</head><label>1</label><figDesc>Dataset description for concept detection and caption prediction</figDesc><table coords="3,77.42,129.26,422.79,93.60"><row><cell></cell><cell>Datasets</cell><cell>Training Set</cell><cell>Validation Set</cell><cell>Test Set</cell></row><row><cell>Concept</cell><cell>No. of images</cell><cell>83275</cell><cell>7645</cell><cell>7601</cell></row><row><cell>Detection</cell><cell>No. of concept IDs</cell><cell>83275</cell><cell>7645</cell><cell>-</cell></row><row><cell>[19]</cell><cell>No. of concept name</cell><cell>8374</cell><cell>8374</cell><cell>-</cell></row><row><cell>Caption</cell><cell>No. of images</cell><cell>83275</cell><cell>7645</cell><cell>7601</cell></row><row><cell>Prediction</cell><cell>No. of captions</cell><cell>83275</cell><cell>7645</cell><cell>-</cell></row><row><cell>[19]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,72.02,316.13,445.53,162.50"><head>Table 2</head><label>2</label><figDesc>Brief inference from concept detection dataset</figDesc><table coords="3,77.78,358.61,439.77,120.02"><row><cell>Concept Detection</cell><cell>Value</cell><cell>Respective tags (or) concept name</cell></row><row><cell>Maximum number of tags</cell><cell>13</cell><cell>"C0002978;C0021102;C0085590;C0232180;C0205197;</cell></row><row><cell></cell><cell></cell><cell>C0887842;C0004704;C0021398;C0441127;C2698651;</cell></row><row><cell></cell><cell></cell><cell>C0333138;C0009924;C0036426"</cell></row><row><cell>Minimum number of tags</cell><cell>1</cell><cell>Most of the samples having only one tags</cell></row><row><cell>Maximum length of the concept</cell><cell>12</cell><cell>"Arterial Occlusive Disease, Progressive with</cell></row><row><cell></cell><cell></cell><cell>Hypertension, Heart Defects, Bone Fragility and</cell></row><row><cell></cell><cell></cell><cell>Brachysyndactyly"</cell></row><row><cell>Minimum length of the concept</cell><cell>1</cell><cell>Most of the concept size is one</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,72.02,534.67,401.88,189.27"><head>Table 3</head><label>3</label><figDesc>Top 10 concept ID with its frequency</figDesc><table coords="3,107.66,577.15,366.24,146.79"><row><cell>Concept ID</cell><cell>Concept Name</cell><cell>Frequency</cell></row><row><cell>C0040405</cell><cell>X-Ray Computed Tomography</cell><cell>28885</cell></row><row><cell>C1306645</cell><cell>Plain x-ray</cell><cell>26412</cell></row><row><cell>C0024485</cell><cell>Magnetic Resonance Imaging</cell><cell>15693</cell></row><row><cell>C0041618</cell><cell>Ultrasonography</cell><cell>12236</cell></row><row><cell>C0817096</cell><cell>Chest</cell><cell>8030</cell></row><row><cell>C0002978</cell><cell>Angiogram</cell><cell>6464</cell></row><row><cell>C0000726</cell><cell>Abdomen</cell><cell>6243</cell></row><row><cell>C0037303</cell><cell>Bone structure of cranium</cell><cell>5175</cell></row><row><cell>C0221198</cell><cell>Lesion</cell><cell>4094</cell></row><row><cell>C0205131</cell><cell>Axial</cell><cell>3528</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,72.02,383.45,436.48,149.06"><head>Table 4</head><label>4</label><figDesc>Brief description about each runs</figDesc><table coords="7,82.70,425.93,425.80,106.58"><row><cell>Run number</cell><cell>Approach/Techniques</cell><cell>F1 Score</cell><cell>Secondary F1 Score</cell></row><row><cell>1</cell><cell>Model 1</cell><cell>0.385</cell><cell>0.524</cell></row><row><cell>2</cell><cell>Model 1</cell><cell>0.418</cell><cell>0.654</cell></row><row><cell>3</cell><cell>Model 1</cell><cell>0.408</cell><cell>0.829</cell></row><row><cell>4</cell><cell>Model 1</cell><cell>0.418</cell><cell>0.654</cell></row><row><cell>5</cell><cell>Model 1</cell><cell>0.412</cell><cell>0.661</cell></row><row><cell>6</cell><cell>Model 1</cell><cell>0.406</cell><cell>0.614</cell></row><row><cell>7</cell><cell>Model 2</cell><cell>0.316</cell><cell>0.412</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,72.02,639.22,428.56,122.16"><head>Table 5</head><label>5</label><figDesc>Top 10 ranking of ImageCLEF 2022 concept detection task</figDesc><table coords="7,102.14,681.70,398.45,79.68"><row><cell>Rank</cell><cell>Team Name</cell><cell>F1 Score</cell><cell>Secondary F1</cell><cell>No. of runs</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Score</cell><cell>submitted</cell></row><row><cell>1</cell><cell>AUEB-NLP-Group</cell><cell>0.451</cell><cell>0.791</cell><cell>6</cell></row><row><cell>2</cell><cell>fdallaserra</cell><cell>0.450</cell><cell>0.822</cell><cell>5</cell></row><row><cell>3</cell><cell>CSIRO</cell><cell>0.447</cell><cell>0.794</cell><cell>10</cell></row><row><cell>4</cell><cell>eecs-kth</cell><cell>0.436</cell><cell>0.855</cell><cell>10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,72.02,507.19,442.14,162.51"><head>Table 6</head><label>6</label><figDesc>Brief description about each runs</figDesc><table coords="8,78.26,549.67,435.90,120.03"><row><cell>Run</cell><cell>Approach/</cell><cell cols="2">BLEU Rouge METEOR</cell><cell>CIDEr</cell><cell>SPICE</cell><cell>BERTScore</cell></row><row><cell>Number</cell><cell>Techniques</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>Model 4</cell><cell>0.159 0.042</cell><cell>0.023</cell><cell>0.017</cell><cell>0.007</cell><cell>0.545</cell></row><row><cell>2</cell><cell>Model 4</cell><cell>0.141 0.039</cell><cell>0.020</cell><cell>0.015</cell><cell>0.006</cell><cell>0.550</cell></row><row><cell>3</cell><cell>Model 3</cell><cell>0.160 0.043</cell><cell>0.023</cell><cell>0.017</cell><cell>0.007</cell><cell>0.545</cell></row><row><cell>4</cell><cell>Model 4</cell><cell>0.154 0.039</cell><cell>0.022</cell><cell>0.015</cell><cell>0.006</cell><cell>0.550</cell></row><row><cell>5</cell><cell>Model 4</cell><cell>0.153 0.040</cell><cell>0.021</cell><cell>0.015</cell><cell>0.007</cell><cell>0.552</cell></row><row><cell>6</cell><cell>Model 3</cell><cell>0.155 0.039</cell><cell>0.022</cell><cell>0.014</cell><cell>0.006</cell><cell>0.550</cell></row><row><cell>7</cell><cell>Model 3</cell><cell>0.142 0.038</cell><cell>0.020</cell><cell>0.014</cell><cell>0.006</cell><cell>0.549</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,72.02,99.38,442.14,189.39"><head>Table 7</head><label>7</label><figDesc>Top 10 ranking of ImageCLEF 2022 caption prediction task</figDesc><table coords="9,83.54,141.86,430.62,146.91"><row><cell>Rank</cell><cell>Team Name</cell><cell cols="2">BLEU Rouge METEOR</cell><cell>CIDEr</cell><cell>SPICE</cell><cell>BERTScore</cell></row><row><cell>1</cell><cell>IUST-NLPLAB</cell><cell>0.483 0.142</cell><cell>0.093</cell><cell>0.030</cell><cell>0.007</cell><cell>0.561</cell></row><row><cell>2</cell><cell>AUEB-NLP-Group</cell><cell>0.322 0.167</cell><cell>0.074</cell><cell>0.190</cell><cell>0.031</cell><cell>0.599</cell></row><row><cell>3</cell><cell>CSIRO</cell><cell>0.311 0.197</cell><cell>0.084</cell><cell>0.269</cell><cell>0.046</cell><cell>0.623</cell></row><row><cell>4</cell><cell>Vcmi</cell><cell>0.306 0.174</cell><cell>0.075</cell><cell>0.205</cell><cell>0.036</cell><cell>0.604</cell></row><row><cell>5</cell><cell>eecs-kth</cell><cell>0.292 0.116</cell><cell>0.062</cell><cell>0.132</cell><cell>0.022</cell><cell>0.573</cell></row><row><cell>6</cell><cell>Fdallaserra</cell><cell>0.291 0.201</cell><cell>0.082</cell><cell>0.256</cell><cell>0.046</cell><cell>0.610</cell></row><row><cell>7</cell><cell>Kdelab</cell><cell>0.278 0.158</cell><cell>0.074</cell><cell>0.411</cell><cell>0.051</cell><cell>0.600</cell></row><row><cell>8</cell><cell>Morgan_CS</cell><cell>0.255 0.144</cell><cell>0.056</cell><cell>0.148</cell><cell>0.023</cell><cell>0.583</cell></row><row><cell>9</cell><cell>MAI_ImageSem</cell><cell>0.221 0.185</cell><cell>0.068</cell><cell>0.251</cell><cell>0.039</cell><cell>0.606</cell></row><row><cell>10</cell><cell>SSNSheerinKavitha</cell><cell>0.160 0.043</cell><cell>0.023</cell><cell>0.017</cell><cell>0.007</cell><cell>0.545</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">Acknowledgements</head><p>Our profound gratitude to <rs type="institution">Sri Sivasubramaniya Nadar College</rs> <rs type="affiliation">of Engineering, Department of CSE</rs>, for allowing us to utilize the <rs type="institution">High Performance Computing Laboratory</rs> and GPU Server for the execution of this challenge successfully.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="9,93.26,669.48,430.03,9.94;9,93.26,682.08,430.23,9.94;9,93.26,694.68,13.80,9.94;9,267.40,694.68,25.05,9.94;9,452.86,694.68,70.76,9.94;9,93.26,707.40,359.64,9.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,331.51,669.48,191.79,9.94;9,93.26,682.08,45.34,9.94">A comparative study of medical imaging techniques</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kasban</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A M</forename><surname>El-Bendary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Salama</surname></persName>
		</author>
		<ptr target="https://ilearn.th-deg.de/pluginfile.php/480243/mod_book/chapter/8248/updated_JXIJSIS2015.pdf" />
	</analytic>
	<monogr>
		<title level="j" coord="9,146.54,682.08,299.34,9.94">International Journal of Information Science and Intelligent System</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="37" to="58" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,93.26,720.00,430.05,9.94;9,93.26,732.61,430.11,10.05;9,93.26,745.32,430.12,9.94;9,93.26,757.92,430.02,9.94;10,93.26,74.42,429.79,9.94;10,93.26,87.04,430.25,9.94;10,93.26,99.76,430.23,9.94;10,93.26,112.36,35.88,9.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,207.29,757.92,316.00,9.94;10,93.26,74.42,167.39,9.94">Overview of the ImageCLEF 2022: Multimedia Retrieval in Medical, Social Media and Nature Applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S D</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ștefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,279.03,74.42,244.02,9.94;10,93.26,87.04,64.65,9.94;10,178.77,87.04,344.74,9.94;10,93.26,99.76,30.60,9.94">Proceedings of the Thirteen International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="10,158.01,99.76,196.23,9.94">Springer Lecture Notes in Computer Science</title>
		<meeting>the Thirteen International Conference of the CLEF Association (CLEF<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>LNCS</publisher>
			<date type="published" when="2022-09-05">2022. September 5-8, 2022</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="10,93.26,125.08,430.14,9.94;10,93.26,137.68,430.16,9.94;10,93.26,150.28,24.84,9.94;10,147.49,150.28,13.80,9.94;10,190.56,150.28,52.37,9.94;10,272.33,150.28,25.05,9.94;10,326.71,150.28,196.79,9.94;10,93.26,163.00,197.22,9.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,372.79,125.08,150.62,9.94;10,93.26,137.68,38.35,9.94">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<ptr target="https://www.computer.org/csdl/proceedings-article/cvpr/2017/0457c261/12OmNBDQbld" />
	</analytic>
	<monogr>
		<title level="m" coord="10,156.08,137.68,362.93,9.94">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.26,175.60,430.23,9.94;10,93.26,188.32,429.69,9.94;10,93.26,200.92,430.28,9.94;10,93.26,213.52,337.97,9.94" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,304.49,175.60,219.00,9.94;10,93.26,188.32,410.12,9.94">Analysis and Implementation of the Bray-Curtis Distance-Based Similarity Measure for Retrieving Information from the Medical Repository</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bala</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-981-13-2354-6_14</idno>
		<ptr target="https://link.springer.com/chapter/10.1007/978-981-13-2354-6_14" />
	</analytic>
	<monogr>
		<title level="m" coord="10,93.26,200.92,315.39,9.94">International Conference on Innovative Computing and Communications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="117" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.26,226.24,429.97,9.94;10,93.26,238.84,430.14,9.94;10,93.26,251.56,38.61,9.94;10,154.92,251.56,42.34,9.94;10,220.25,251.56,47.47,9.94;10,290.65,251.56,24.72,9.94;10,338.40,251.56,24.84,9.94;10,386.27,251.56,13.80,9.94;10,422.98,251.56,52.44,9.94;10,498.45,251.56,25.05,9.94;10,93.26,264.19,200.43,9.94" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,160.34,226.24,330.71,9.94">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/tan19a.html" />
	</analytic>
	<monogr>
		<title level="m" coord="10,280.97,238.84,242.43,9.94;10,93.26,251.56,38.61,9.94;10,154.92,251.56,37.63,9.94">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.26,276.91,430.14,9.94;10,93.26,289.51,430.25,9.94;10,93.26,302.11,342.17,9.94" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,366.67,276.91,156.73,9.94;10,93.26,289.51,40.48,9.94">Densely Connected Convolutional Networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,156.20,289.51,362.60,9.94">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, USA</addrLine></address></meeting>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.26,314.83,430.11,9.94;10,93.26,327.43,430.23,9.94;10,93.26,340.15,289.85,9.94" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,348.91,314.83,174.46,9.94;10,93.26,327.43,133.58,9.94">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00068</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,274.19,327.43,249.31,9.94;10,93.26,340.15,51.90,9.94">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.26,352.75,430.00,9.94;10,93.26,365.35,430.25,9.94;10,93.26,378.07,380.24,9.94" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="10,408.55,365.35,114.97,9.94;10,93.26,378.07,224.05,9.94">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,93.26,390.67,430.14,9.94;10,93.26,403.39,429.94,9.94;10,93.26,415.99,339.41,9.94" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,366.67,390.67,156.73,9.94;10,93.26,403.39,40.48,9.94">Densely Connected Convolutional Networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,156.20,403.39,362.29,9.94">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, USA</addrLine></address></meeting>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.26,428.59,430.09,9.94;10,93.26,441.33,429.82,9.94;10,93.26,453.93,430.13,9.94;10,93.26,466.65,183.34,9.94" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,277.37,428.59,245.98,9.94;10,93.26,441.33,253.29,9.94">PUC Chile team at Caption Prediction: ResNet visual encoding and caption classification with Parametric ReLU</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lobel</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-95.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,369.48,441.33,153.60,9.94;10,93.26,453.93,174.45,9.94">CLEF2021 Working Notes, CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.26,479.25,430.09,9.94;10,93.26,491.85,430.23,9.94;10,93.26,504.46,430.35,10.05;10,93.26,517.17,83.45,9.94" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,243.53,479.25,279.82,9.94;10,93.26,491.85,196.55,9.94">ImageCLEF 2020: An approach for Visual Question Answering using VGG-LSTM for Different Datasets</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S N</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Srinivasan</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_94.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,319.51,491.85,203.99,9.94;10,93.26,504.57,51.88,9.94">CLEF (Working Notes). CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">September 22-25, 2020</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.26,529.89,430.14,9.94;10,93.26,542.49,403.52,9.94" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,366.19,529.89,157.21,9.94;10,93.26,542.49,247.38,9.94">Towards improving discriminative reconstruction via simultaneous dense and sparse coding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tasissa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Theodosis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Tolooshams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09534</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,93.26,555.09,430.12,9.94;10,93.26,567.81,430.04,9.94;10,93.26,580.41,430.35,9.94;10,93.26,593.13,134.50,9.94" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,298.46,555.09,224.92,9.94;10,93.26,567.81,202.54,9.94">Attention-based CNN-GRU model for automatic medical images captioning: ImageCLEF 2021</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R</forename><surname>Beddiar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Oussalah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Seppänen</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-94.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,318.96,567.81,204.34,9.94;10,93.26,580.41,123.70,9.94">CLEF2021 Working Notes, CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.26,605.73,430.08,9.94;10,93.26,618.48,430.20,9.94;10,93.26,630.97,430.35,10.05;10,93.26,643.68,140.02,9.94" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,230.93,605.73,292.41,9.94;10,93.26,618.48,255.91,9.94">SSN MLRG at VQA-MED 2021: An Approach for VQA to Solve Abnormality Related Queries using Improved Datasets</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">M S</forename><surname>Sitara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Srinivasan</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-110.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,377.23,618.48,146.23,9.94;10,93.26,631.08,103.70,9.94">CLEF (Working Notes) CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1329" to="1335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.26,656.40,430.25,9.94;10,93.26,669.00,430.22,9.94;10,93.26,681.72,152.62,9.94" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,190.37,656.40,261.88,9.94">Gate-variants of gated recurrent unit (GRU) neural networks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Salem</surname></persName>
		</author>
		<idno type="DOI">10.1109/MWSCAS.2017.8053243</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,475.90,656.40,47.62,9.94;10,93.26,669.00,285.99,9.94">IEEE sixty international midwest symposium on circuits and systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1597" to="1600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,93.26,694.32,430.08,9.94;10,93.26,706.92,332.93,9.94" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="10,342.43,694.32,180.91,9.94;10,93.26,706.92,176.93,9.94">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,93.26,719.64,430.07,9.94;10,93.26,732.24,234.85,9.94" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="10,370.03,719.64,153.31,9.94;10,93.26,732.24,77.11,9.94">Language models are unsupervised multitask learners</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>Open-AI</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="11,93.26,74.42,430.15,9.94;11,93.26,86.93,430.16,10.05;11,93.26,99.76,430.21,9.94;11,93.26,112.36,430.05,9.94;11,93.26,125.08,94.80,9.94" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,225.65,86.93,297.78,10.05;11,93.26,99.76,80.25,9.94">Overview of ImageCLEFmedical 2022 -Caption Prediction and Concept Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S D</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,194.81,99.76,310.35,9.94;11,93.26,112.36,344.94,9.94">Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">September 5-8, 2022</date>
		</imprint>
	</monogr>
	<note>CLEF2022 Working Notes, CEUR Workshop Proceedings (CEUR-WS.org</note>
</biblStruct>

<biblStruct coords="11,93.26,175.60,429.83,9.94;11,93.26,188.32,430.13,9.94;11,93.26,200.92,188.25,9.94" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="11,333.28,175.60,189.82,9.94;11,93.26,188.32,16.40,9.94">Overview of the ImageCLEF 2016 medical task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S D</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-1609/16090219.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="11,130.07,188.32,292.12,9.94">Working Notes of CLEF 2016 (Cross Language Evaluation Forum)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,93.26,213.52,430.26,9.94;11,93.26,226.24,430.10,9.94;11,93.26,238.84,143.17,9.94" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="11,194.81,213.52,280.37,9.94">A Study on Single and Multi-layer Perceptron Neural Network</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Banerjee</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCMC.2019.8819775</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,499.06,213.52,24.46,9.94;11,93.26,226.24,331.61,9.94">Third International Conference on Computing Methodologies and Communication</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,93.26,251.56,430.28,9.94;11,93.26,264.19,148.59,9.94" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Targ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lyman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08029</idno>
		<title level="m" coord="11,283.69,251.56,235.62,9.94">Resnet in resnet: Generalizing residual architectures</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,93.26,276.91,429.93,9.94;11,93.26,289.51,429.90,9.94;11,93.26,302.11,430.21,9.94;11,93.26,314.83,421.16,9.94" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="11,93.26,289.51,137.82,9.94">Supervised contrastive learning</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="11,126.11,302.11,234.19,9.94">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18661" to="18673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,93.26,327.43,430.23,9.94;11,93.26,340.15,55.02,9.94;11,169.30,340.15,43.15,9.94;11,233.33,340.15,38.62,9.94;11,292.97,340.15,39.73,9.94;11,353.59,340.15,16.56,9.94;11,391.14,340.15,29.37,9.94;11,441.58,340.15,35.76,9.94;11,498.34,340.15,25.05,9.94;11,93.26,352.75,190.59,9.94" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="11,217.25,327.43,306.25,9.94;11,93.26,340.15,55.02,9.94;11,169.30,340.15,38.35,9.94">Learning rules for multi-label classification: a stacking and a separateand-conquer approach</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Loza Mencía</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Janssen</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-016-5552-1</idno>
		<ptr target="https://doi.org/10.1007/s10994-016-5552-1" />
	</analytic>
	<monogr>
		<title level="j" coord="11,233.33,340.15,38.62,9.94;11,292.97,340.15,39.73,9.94">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="77" to="126" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
