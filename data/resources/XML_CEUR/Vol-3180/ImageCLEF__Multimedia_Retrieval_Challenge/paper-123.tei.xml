<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,105.69,412.15,15.42;1,89.29,127.60,404.23,15.42">Kdelab at ImageCLEFmedical 2022: Medical Concept Detection with Image Retrieval and Code Ensemble</title>
				<funder ref="#_gTfN8Aw #_x6VsmkE #_b9pEzwc">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,155.92,66.70,11.96"><forename type="first">Riku</forename><surname>Tsuneda</surname></persName>
							<email>tsuneda@kde.cs.tut.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<settlement>Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,168.63,155.92,85.40,11.96"><forename type="first">Tetsuya</forename><surname>Asakawa</surname></persName>
							<email>asakawa@kde.cs.tut.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<settlement>Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.67,155.92,76.24,11.96"><forename type="first">Kazuki</forename><surname>Shimizu</surname></persName>
							<email>shimizu@heartcenter.or.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">Toyohashi Heart Center</orgName>
								<address>
									<addrLine>21-1 Gobutori, Oyama-cho, Toyohashi-shi</addrLine>
									<settlement>Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,355.56,155.92,89.81,11.96"><forename type="first">Takuyuki</forename><surname>Komoda</surname></persName>
							<email>komoda@heartcenter.or.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">Toyohashi Heart Center</orgName>
								<address>
									<addrLine>21-1 Gobutori, Oyama-cho, Toyohashi-shi</addrLine>
									<settlement>Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,169.87,64.67,11.96"><forename type="first">Masaki</forename><surname>Aono</surname></persName>
							<email>aono@kde.cs.tut.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Toyohashi University of Technology</orgName>
								<address>
									<settlement>Aichi</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,105.69,412.15,15.42;1,89.29,127.60,404.23,15.42">Kdelab at ImageCLEFmedical 2022: Medical Concept Detection with Image Retrieval and Code Ensemble</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">EEF73131F49DC30BABBD8FC01DE61794</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical Images</term>
					<term>Image Retrieval</term>
					<term>Concept Detection</term>
					<term>Multi-Label Classification</term>
					<term>Concept Unique Identifier</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ImageCLEFmedical 2022 Concept Detection Task is an example of a challenging research problem in the field of image captioning. The goal of this research is to automatically generate accurate Medical Concept (CUI code) describing a given medical image. We describe three approaches toward the concept detection task:simple image retrieval, CUI code ensemble with retrieval, and modality classification. We submitted 10 runs to the concept detection task, and achieved the F1 score of 0.310 and the Secondary F1 score of 0.412, which ranked 10th among the participating teams. We describe in detail our ways on data analysis and three approaches, and conclude by discussing some ideas for future work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>ImageCLEF is an initiative aimed at advancing the field of image retrieval and improving the evaluation of technologies for annotation, indexing, and retrieval of visual data. ImageCLEF has been held annually since 2003, and since the second edition <ref type="bibr" coords="1,377.53,443.57,25.32,10.91">(2004)</ref>, tasks such as medical image retrieval have been included. Since the first and the second editions <ref type="bibr" coords="1,433.74,457.12,64.63,10.91">(2003 and 2004</ref>), ImageCLEF's Medical Challenge task group has integrated new ones that include medical images, and the Medical Caption task has been in place since 2017. This task consists of two subtasks : concept detection and caption prediction. Although the data used in the most recent version has changed, the goals of this task remain the same. Data has increased significantly over the last year. The goal of the task is to help physicians reduce the burden of manually translating visual medical information (e.g. radiology images) into textual descriptions. This document describes kdelab's participation in the ImageCLEF medical 2022 concept detection task. Our team placed 10th in this task. Our best submission was a system that visually encoded medical images with convolutional neural networks, performed K Nearest Neighbor (KNN) similarity search using Euclidean distance, and ensemble each result.</p><p>In the following, we first describe related work on Medical Concept Detection task in Section 2, followed by the description of the dataset provided for ImageCLEF medical 2022 Caption Task <ref type="bibr" coords="2,111.65,114.06,12.68,10.91" target="#b0">[1]</ref> [2] dataset in Section 3. In Section 4, we describe details of the method we have applied, and then of our experiments we have conducted in Section 5. We finally conclude this paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In previous competitions, the best participants used a variety of techniques based primarily on convolutional neural networks, natural language processing, K-Nearest Neighbors, and clustering. In the 2021 concept detection challenge <ref type="bibr" coords="2,324.79,226.89,11.58,10.91" target="#b2">[3]</ref>, the highest F1 score was 0.505. The winning approach <ref type="bibr" coords="2,174.90,240.44,12.99,10.91" target="#b3">[4]</ref> used a convolutional neural network (CNN) <ref type="bibr" coords="2,396.45,240.44,13.00,10.91" target="#b4">[5]</ref> as an image encoder, combined with an image retrieval module. This team also took first place in the competition. Second place went to NLIP-Essex-ITESM <ref type="bibr" coords="2,283.33,267.54,13.00,10.91" target="#b5">[6]</ref> with an F1 score of 0.469. This team adopted image retrieval methods using various distance calculations. The best distance calculation for this team was cosine similarity.</p><p>Looking at past years, the best submissions achieved F1 scores of 0.1108 in 2018 <ref type="bibr" coords="2,460.21,308.18,11.44,10.91" target="#b6">[7]</ref>, 0.2823 in 2019 <ref type="bibr" coords="2,126.63,321.73,11.43,10.91" target="#b7">[8]</ref>, 0.3940 in 2020 and 0.505 in 2021.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset</head><p>For the ImageCLEFmedical 2022 Concept Detection task, organizers have provided us with a training set of 83,275 radiology images with the same number of CUI codes, a validation set of 7,645 radiology images with the same number of CUI codes, and a test set of 7,601 radiology images with the same number of CUI codes. These images are part of ROCO dataset <ref type="bibr" coords="2,457.80,421.01,11.28,10.91" target="#b8">[9]</ref>. We are supposed to use these as our datasets. Most of the images in the dataset are non-colored, and they potentially include non-essential logos, arrow symbols, numbers and texts. The image data set included multiple modalities such as CT, MRI, X-ray, ultrasound images, and angiographic images. The task participants have to automatically predict CUI codes based on radiology image data.</p><p>The top 25 ranking concept names in terms of frequency are summarized in Table <ref type="table" coords="2,481.05,502.30,5.17,10.91" target="#tab_0">1</ref> and Figure <ref type="figure" coords="2,119.84,515.85,3.67,10.91" target="#fig_0">1</ref>. According to our analysis, the minimum number of CUI codes assigned was 1 and the maximum was 50. In addition, an average of 4.7 CUIs were assigned to each image.</p><p>For our experiments, we merged the provided training and validation sets and used 10% of the merged data as our validation set, and another 10% of the merged data as our development set in which we evaluated the performance of our models. The remaining 80% served as the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>In this section, we describe the three approaches that were used in our submissions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image Preprocessing</head><p>Since most of the images in the dataset are grayscale images, we attempted a pseudo-coloring on the images. For pseudo coloring, use the method of assigning a black and white color map to an RGB color map. We have used the Open-CV <ref type="bibr" coords="3,304.58,509.70,17.91,10.91" target="#b9">[10]</ref> JET colormap for the RGB colormap. We show an example of the pseudo-coloring in Figure <ref type="figure" coords="3,318.10,523.25,3.74,10.91" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image Retrieval Approach</head><p>Image retrieval methods were one of the major methods in CLEF2021. Last year, AUEB-NLP Gloup <ref type="bibr" coords="3,122.27,586.53,12.90,10.91" target="#b2">[3]</ref> and PUC Chile Team <ref type="bibr" coords="3,237.26,586.53,17.98,10.91" target="#b10">[11]</ref> adopted this method and achieved top scores. Since the most medical images are grayscale images, retrieval methods may be more effective than deep learning methods. In this section we describe our simple image retrieval and ensemble methods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Simple Image Retrieval</head><p>We similarly tested the effectiveness of our image retrieval method. We illustrate our image retrieval method in Figure <ref type="figure" coords="4,208.71,626.49,3.74,10.91" target="#fig_2">3</ref>. First, we extracted features from all images using a several feature extractor. Next, we compute approximation based on the features using the Cosine similarity or Euclidean distance. Finally, we assign concepts to test images from the retrieval results.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Ensemble with Majority Voting</head><p>We have attempted an ensemble system with multiple feature extractors. This system is based on a majority voting of the predictions in the five extractors. We have adopted DenseNet-121 <ref type="bibr" coords="5,487.02,445.22,16.13,10.91" target="#b11">[12]</ref>, DenseNet-201 <ref type="bibr" coords="5,153.98,458.77,16.19,10.91" target="#b11">[12]</ref>, ResNet-50 <ref type="bibr" coords="5,224.65,458.77,16.19,10.91" target="#b12">[13]</ref>, ResNet-152 <ref type="bibr" coords="5,300.36,458.77,16.19,10.91" target="#b12">[13]</ref>, EfficientNet-B0 <ref type="bibr" coords="5,393.65,458.77,16.19,10.91" target="#b13">[14]</ref>, EfficientNet-B7 <ref type="bibr" coords="5,486.95,458.77,16.19,10.91" target="#b13">[14]</ref>, Inception-V3 <ref type="bibr" coords="5,151.95,472.32,16.21,10.91" target="#b14">[15]</ref>, Xception <ref type="bibr" coords="5,217.45,472.32,16.21,10.91" target="#b15">[16]</ref>, inception ResNet-V2 <ref type="bibr" coords="5,336.85,472.32,17.88,10.91" target="#b16">[17]</ref> and Nas Net Large <ref type="bibr" coords="5,442.31,472.32,17.87,10.91" target="#b17">[18]</ref> as feature extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">CUI Code Ensemble</head><p>Unlike a simple ensemble, this method combines multiple CUI codes without determining a single predicted image. We illustrate our CUI Code Ensemble method in Figure <ref type="figure" coords="5,443.45,548.75,3.74,10.91">4</ref>.</p><p>First, as in the image retrieval method, each feature extractor is used to estimate the approximate image. Next, the CUI code assigned to the image is obtained. Finally, the CUIs are sorted by frequency of occurrence, and the CUIs up to top l are considered as results. We tried the variable l in two ways :   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Modality Classification Approach</head><p>This method is a combination of image classification and image retrieval. First, image modality classification is performed, followed by image retrieval among the predicted modalities and approximate image estimation. Approximate image estimation is the same as in section4.2. We tried ResNet-101 as modality classifiers. We trained these classifiers using the dataset Train, Validation. The classifiers are classified into 6 classes : CT, MRI, Plain X-ray, Angiorgam, Ultrasoundgraphy, and others. Class assignment is based on the CUI code that was assigned. The classes and the number of images corresponding to the classes are shown in the Table <ref type="table" coords="7,89.29,466.34,3.80,10.91" target="#tab_3">2</ref>. The training results of the classifiers are shown in the following Table <ref type="table" coords="7,419.40,466.34,3.80,10.91" target="#tab_4">3</ref>, which shows the results of our Modality Classification Approach using development set. As can be seen from the results, we failed to produce a highly accurate modality classifier. In fact, the best modality classifier in the Table <ref type="table" coords="7,187.11,506.99,5.11,10.91" target="#tab_4">3</ref> was used to search for CUI codes, resulting in a best F1 score of 0.034. This method was ineffective and we did not submit it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Submission and Results</head><p>We performed 10 submissions using Simple Image Retrieval, CUI Code Ensemble and preprocessing described in the previous section. Since the official evaluation metric for concept detection is F1 score, we evaluated models using this metric in the development set to determine which models to submit (each participant was allowed a maximum of 10 submissions). Table <ref type="table" coords="7,499.83,619.81,3.68,10.91" target="#tab_5">4</ref>, Table <ref type="table" coords="7,118.94,633.36,5.16,10.91" target="#tab_6">5</ref> shows the scores for the development set, and Table <ref type="table" coords="7,364.77,633.36,5.16,10.91" target="#tab_7">6</ref> shows the final scores for our model on the unknown test caption. First, we describe our results and findings in the development set. In simple image retrieval methods, accuracy was found to improve when using ensembles with simple majority voting. Ensemble 1 has a higher BLEU score than Ensemble2. Comparing Cosine similarity and Euclidean distance, Cosine Similarity provides better retrieval accuracy. Second, we describe our results and findings in the test set. We submitted to AIcrowd the systems that scored highly in each of the approaches in our development set. The highest scoring submission was simple image retrieval system using Euclidean distance. Finally, from organizer's evaluation, we have achieved a F1 score of 0.310 and a secondary F1 score of 0.412 in the ImageCLEF2022medical Concept Detection task, placing us 10th.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have described our systems with which we submitted to the ImageCLEFmedical 2022 Concept Detection task. In our system, we have done our own data pre-processing, and have attempted to automatically generate concepts with image retrieval.</p><p>The results demonstrate that some of experiment have improved the concept detection accuracy of the image retrieval. Pseudo colorization and code ensemble approach turns out to be ineffective in this task. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,318.03,342.32,8.93;4,329.03,383.46,108.69,121.40"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Frequency of CUI in ImageCLEFmedical 2022 Concept Detection Dataset</figDesc><graphic coords="4,329.03,383.46,108.69,121.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,542.80,416.70,8.93;4,89.29,554.80,284.03,8.87;4,157.54,383.46,108.69,121.40"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of Original Image (Left) and Pseudo Colorization(Right) [CC BY-NC-ND [Peixoto et al. (2015)]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5580006/)</figDesc><graphic coords="4,157.54,383.46,108.69,121.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,89.29,326.00,418.23,8.93;5,89.29,337.07,418.23,9.96;5,89.29,349.03,417.34,9.96;5,89.29,360.98,417.34,9.96;5,89.29,372.94,62.42,9.96"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of Image Retrieval System, [CC BY-NC [Hekmat et al. (2016)](https://www. ncbi.nlm.nih.gov/pmc/articles/PMC4835740/), CC BY [Abidi et al. (2015)](https://www.ncbi.nlm.nih. gov/pmc/articles/PMC4769046/), CC BY [Apaydin et al. (2018)](https://www.ncbi.nlm.nih.gov/pmc/ articles/PMC6202798/), CC BY-NC-ND [Datta et al. (2018)](https://www.ncbi.nlm.nih.gov/pmc/articles/ PMC5925857/)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,90.49,329.61,344.84"><head>Table 1</head><label>1</label><figDesc>CUI code frequency in dataset</figDesc><table coords="3,176.68,122.10,241.92,313.22"><row><cell>Rank</cell><cell>CUI</cell><cell>Freq UMLS defined Name</cell></row><row><cell>1</cell><cell cols="2">C0040405 28885 X-ray Computed Tomography</cell></row><row><cell>2</cell><cell cols="2">C1306645 26412 Plain x-ray</cell></row><row><cell>3</cell><cell cols="2">C0024485 15693 Magnetic Resonance Imaging</cell></row><row><cell>4</cell><cell cols="2">C0041618 12236 Ultrasoundgraphy</cell></row><row><cell>5</cell><cell cols="2">C0817096 8030 Chest</cell></row><row><cell>6</cell><cell cols="2">C0002978 6464 angiogram</cell></row><row><cell>7</cell><cell cols="2">C0000726 6243 Abdomen</cell></row><row><cell>8</cell><cell cols="2">C0037303 5175 Bone structure of cranium</cell></row><row><cell>9</cell><cell cols="2">C0221198 4094 Lesion</cell></row><row><cell></cell><cell cols="2">C0205131 3528 Axial</cell></row><row><cell></cell><cell cols="2">C0030797 3404 Pelvis</cell></row><row><cell></cell><cell cols="2">C0238767 3124 Bilateral</cell></row><row><cell></cell><cell cols="2">C0023216 2753 Lower Extremity</cell></row><row><cell></cell><cell cols="2">C0577559 2497 Mass of body structure</cell></row><row><cell></cell><cell cols="2">C0205129 2243 Sagittal</cell></row><row><cell></cell><cell cols="2">C0205091 1856 Left</cell></row><row><cell></cell><cell cols="2">C0205090 1665 Right</cell></row><row><cell></cell><cell cols="2">C0021102 1564 Implants</cell></row><row><cell></cell><cell cols="2">C0444706 1542 Measured</cell></row><row><cell></cell><cell cols="2">C0009924 1524 Contrast Media</cell></row><row><cell></cell><cell cols="2">C0006660 1412 Physiologic calcification</cell></row><row><cell></cell><cell cols="2">C0205095 1385 Dorsal</cell></row><row><cell></cell><cell cols="2">C0027651 1371 Neoplasms</cell></row><row><cell></cell><cell cols="2">C0023884 1339 Liver</cell></row><row><cell></cell><cell cols="2">C0037949 1339 Vertebral column</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,107.28,628.45,274.51,25.81"><head>•</head><label></label><figDesc>Average Length : Average length of predicted CUI codes • Max Length : Maximum length of the predicted CUI codes</figDesc><table coords="6,89.29,347.83,417.34,301.37"><row><cell></cell><cell></cell><cell></cell><cell cols="2">Concept Assignment</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Avg length</cell><cell>Top 4 (Rounded to the nearest 4.4) CUI</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Max Length</cell><cell>Top 5 CUI</cell></row><row><cell></cell><cell cols="2">Predicted CUI</cell><cell></cell></row><row><cell>Avg length</cell><cell>C1306645 C0817096 C1282910 C0025066</cell><cell>Max Length</cell><cell>C1306645 C0817096 C1282910 C0025066 C0332448</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>gov/pmc/articles/</cell></row><row><cell cols="5">PMC2939555/), CC BY [Naz et al. (2020)] (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7536294/)]</cell></row></table><note coords="6,89.29,580.36,417.87,8.93;6,88.97,591.43,418.55,9.96;6,89.29,603.38,417.34,9.96;6,89.29,615.34,417.34,9.96;6,89.29,627.29,347.17,9.96"><p>Figure 4: Example of CUI Code Ensemble method, [CC BY-NC [Hekmat et al. (2016)](https: //www.ncbi.nlm.nih.gov/pmc/articles/PMC4835740/), CC BY [Abidi et al. (2015)] (https://www.ncbi. nlm.nih.gov/pmc/articles/PMC4769046/), CC BY [Apaydin et al. (2018)] (https://www.ncbi.nlm.nih.gov/ pmc/articles/PMC6202798/), CC BY-NC-ND [Datta et al. (2018)] (https://www.ncbi.nlm.nih.gov/pmc/ articles/PMC5925857/), CC BY [Nouri-Majalan et al. (2010)] (https://www.ncbi.nlm.nih.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,88.99,90.49,291.95,117.69"><head>Table 2</head><label>2</label><figDesc>The Classes of Our Modality Classifier</figDesc><table coords="7,214.34,122.10,166.60,86.07"><row><cell>Modality Name</cell><cell>Quantity of Images</cell></row><row><cell>CT</cell><cell>28,885</cell></row><row><cell>X-Ray</cell><cell>26,412</cell></row><row><cell>MRI</cell><cell>15,693</cell></row><row><cell>Ultrasoundgraphy</cell><cell>12,236</cell></row><row><cell>Angiogram</cell><cell>6,464</cell></row><row><cell>Others</cell><cell>1,230</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,88.99,230.44,364.28,93.78"><head>Table 3</head><label>3</label><figDesc>The Validation Scores of our Training Modality Classifier on our Development Set</figDesc><table coords="7,142.00,262.05,311.27,62.16"><row><cell cols="5">Processing for Imbalanced Data Accuracy Precision Recall F1 score</cell></row><row><cell>None</cell><cell>0.675</cell><cell>0.332</cell><cell>0.416</cell><cell>0.367</cell></row><row><cell>Over Sampling</cell><cell>0.493</cell><cell>0.348</cell><cell>0.416</cell><cell>0.313</cell></row><row><cell>Under Sampling</cell><cell>0.179</cell><cell>0.215</cell><cell>0.202</cell><cell>0.085</cell></row><row><cell>Class Weighting</cell><cell>0.663</cell><cell>0.326</cell><cell>0.413</cell><cell>0.361</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,88.99,90.49,385.49,237.24"><head>Table 4</head><label>4</label><figDesc>The scores of our Image Retrieval systems on our pseudo colorized development set</figDesc><table coords="8,120.80,122.10,353.68,205.63"><row><cell>ID</cell><cell>Approach</cell><cell>Calculation</cell><cell>F1 score</cell></row><row><cell cols="2">exc01 DenseNet121</cell><cell>Cosine Similarity</cell><cell>0.269</cell></row><row><cell cols="2">exc02 EfficientNetB0</cell><cell>Cosine Similarity</cell><cell>0.266</cell></row><row><cell cols="2">exc03 EfficientNetB7</cell><cell>Cosine Similarity</cell><cell>0.256</cell></row><row><cell cols="2">exc04 DenseNet-201</cell><cell>Cosine Similarity</cell><cell>0.271</cell></row><row><cell cols="2">exc05 ResNet-50</cell><cell>Cosine Similarity</cell><cell>0.261</cell></row><row><cell cols="2">exc06 ResNet-152</cell><cell>Cosine Similarity</cell><cell>0.259</cell></row><row><cell cols="2">exc07 Xception</cell><cell>Cosine Similarity</cell><cell>0.253</cell></row><row><cell cols="2">exc08 inceptionResNetV2</cell><cell>Cosine Similarity</cell><cell>0.241</cell></row><row><cell cols="2">exc09 NasNetLarge</cell><cell>Cosine Similarity</cell><cell>0.232</cell></row><row><cell cols="2">exc10 inceptionV3</cell><cell>Cosine Similarity</cell><cell>0.251</cell></row><row><cell cols="2">exc11 Ensemble1 (exc01, exc02, exc04, exc05, exc06)</cell><cell>Cosine Similarity</cell><cell>0.286</cell></row><row><cell cols="2">exc12 Ensemble2 (exc01, exc02, exc03, exc04, exc07)</cell><cell>Cosine Similarity</cell><cell>0.284</cell></row><row><cell cols="3">exc13 Ensemble1 (exc01, exc02, exc04, exc05, exc06) Euclidean Distance</cell><cell>0.281</cell></row><row><cell cols="3">exc14 Ensemble2 (exc01, exc02, exc03, exc04, exc07) Euclidean Distance</cell><cell>0.276</cell></row><row><cell cols="2">exc15 CUI code Ensemble (average length)</cell><cell>Cosine Similarity</cell><cell>0.283</cell></row><row><cell cols="2">exc16 CUI code Ensemble (max length)</cell><cell>Cosine Similarity</cell><cell>0.281</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,88.99,90.49,372.66,237.24"><head>Table 5</head><label>5</label><figDesc>The scores of our Image Retrieval systems on our development set</figDesc><table coords="9,133.62,122.10,328.04,205.63"><row><cell>ID</cell><cell>Approach</cell><cell>Calculation</cell><cell>F1 score</cell></row><row><cell cols="2">ex01 DenseNet121</cell><cell>Cosine Similarity</cell><cell>0.277</cell></row><row><cell cols="2">ex02 EfficientNetB0</cell><cell>Cosine Similarity</cell><cell>0.276</cell></row><row><cell cols="2">ex03 EfficientNetB7</cell><cell>Cosine Similarity</cell><cell>0.261</cell></row><row><cell cols="2">ex04 DenseNet201</cell><cell>Cosine Similarity</cell><cell>0.280</cell></row><row><cell cols="2">ex05 ResNet-50</cell><cell>Cosine Similarity</cell><cell>0.273</cell></row><row><cell cols="2">ex06 ResNet-152</cell><cell>Cosine Similarity</cell><cell>0.272</cell></row><row><cell cols="2">ex07 Xception</cell><cell>Cosine Similarity</cell><cell>0.261</cell></row><row><cell cols="2">ex08 InceptionResNetV2</cell><cell>Cosine Similarity</cell><cell>0.253</cell></row><row><cell cols="2">ex09 NasNet Large</cell><cell>Cosine Similarity</cell><cell>0.226</cell></row><row><cell cols="2">ex10 Inception-V3</cell><cell>Cosine Similarity</cell><cell>0.264</cell></row><row><cell cols="2">ex11 Ensemble1 (ex01, ex02, ex04, ex05, ex06)</cell><cell>Cosine Similarity</cell><cell>0.311</cell></row><row><cell cols="2">ex12 Ensemble2 (ex01, ex02, ex03, ex04, ex07)</cell><cell>Cosine Similarity</cell><cell>0.308</cell></row><row><cell cols="3">ex13 Ensemble1 (ex01, ex02, ex04, ex05, ex06) Euclidean Distance</cell><cell>0.312</cell></row><row><cell cols="3">ex14 Ensemble2 (ex01, ex02, ex03, ex04, ex07) Euclidean Distance</cell><cell>0.290</cell></row><row><cell cols="2">ex15 CUI code Ensemble (average length)</cell><cell>Cosine Similarity</cell><cell>0.296</cell></row><row><cell cols="2">ex16 CUI code Ensemble (max length)</cell><cell>Cosine Similarity</cell><cell>0.295</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,88.99,349.99,433.60,117.69"><head>Table 6</head><label>6</label><figDesc>The scores of all of systems on our submission</figDesc><table coords="9,95.27,381.60,427.33,86.07"><row><cell>Approach</cell><cell>Image Preprocessing</cell><cell>F-1</cell><cell>Secondary F1 Run ID</cell></row><row><cell>Ensemble1 Retrieval with Cosine Similarity</cell><cell cols="2">Pseudo Colorization 0.309</cell><cell>0.409</cell></row><row><cell>Ensemble2 Retrieval with Cosine Similarity</cell><cell cols="2">Pseudo Colorization 0.290</cell><cell>0.396</cell></row><row><cell>Ensemble1 Retrieval with Cosine Similarity</cell><cell>None</cell><cell>0.309</cell><cell>0.409</cell></row><row><cell>Ensemble2 Retrieval with Cosine Similarity</cell><cell>None</cell><cell>0.296</cell><cell>0.409</cell></row><row><cell>DenseNet201 Retrieval with Cosine Similarity</cell><cell>None</cell><cell>0.310</cell><cell>0.408</cell></row><row><cell>DenseNet121 Retrieval with Cosine Similarity</cell><cell>None</cell><cell>0.309</cell><cell>0.409</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>A part of this research was carried out with the support of the <rs type="grantName">Grant for Toyohashi Heart Center Smart Hospital Joint Research Course</rs> and the <rs type="grantName">Grant-in-Aid for Scientific Research</rs> (C) (issue numbers <rs type="grantNumber">22K12149</rs> and <rs type="grantNumber">22K12040</rs>)</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gTfN8Aw">
					<orgName type="grant-name">Grant for Toyohashi Heart Center Smart Hospital Joint Research Course</orgName>
				</org>
				<org type="funding" xml:id="_x6VsmkE">
					<idno type="grant-number">22K12149</idno>
					<orgName type="grant-name">Grant-in-Aid for Scientific Research</orgName>
				</org>
				<org type="funding" xml:id="_b9pEzwc">
					<idno type="grant-number">22K12040</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Code Ensemble C1306645: 5 C0817096: 4 C1282910: 1 C0025066: 1 C0332448: 1 C0444598: 1 C0441994: 1 C0521530: 1 C0225758: 1 C0225730: 1 C0035222: 1 Code Ensemble (Average Length) None 0.292 0.406 Code Ensemble (Max Length) None 0.239 0.292 Ensemble1 Retrieval with Euclidean Distance None 0.310 0.412</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,554.57,395.01,10.91;9,112.66,568.12,395.17,10.91;9,112.39,581.67,394.80,10.91;9,112.66,595.21,394.62,10.91;9,112.66,608.76,393.33,10.91;9,112.66,622.31,395.17,10.91;9,112.66,635.86,393.54,10.91;9,112.66,649.41,170.14,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,352.50,595.21,154.78,10.91;9,112.66,608.76,300.10,10.91">Overview of the ImageCLEF 2022: Multimedia retrieval in medical, social media and nature applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,435.26,608.76,70.73,10.91;9,112.66,622.31,395.17,10.91;9,112.66,635.86,239.58,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<title level="s" coord="9,359.20,635.86,147.00,10.91;9,112.66,649.41,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,662.96,395.17,10.91;10,111.81,86.97,395.37,10.91;10,112.66,100.52,393.33,10.91;10,112.66,114.06,216.46,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,326.22,86.97,180.96,10.91;10,112.66,100.52,177.87,10.91">Overview of ImageCLEFmedical 2022caption prediction and concept detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,313.28,100.52,192.71,10.91;10,112.66,114.06,97.38,10.91">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,127.61,394.53,10.91;10,112.66,141.16,394.62,10.91;10,112.66,154.71,393.33,10.91;10,112.41,168.26,393.57,10.91;10,112.66,181.81,256.93,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,161.95,141.16,325.47,10.91">Overview of the ImageCLEFmed 2021 concept &amp; caption prediction task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,112.66,154.71,393.33,10.91;10,112.41,168.26,272.58,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="10,416.21,168.26,89.77,10.91;10,112.66,181.81,90.42,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,195.36,393.33,10.91;10,112.66,208.91,394.62,10.91;10,112.66,222.46,174.71,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,478.16,195.36,27.83,10.91;10,112.66,208.91,205.93,10.91">AUEB NLP Group at ImageCLEFmed Caption Tasks</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Charalampakos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/#paper-96" />
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="1184" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,236.01,393.33,10.91;10,112.66,249.56,395.17,10.91;10,112.66,263.11,394.03,10.91;10,112.66,276.66,387.22,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,295.99,236.01,209.99,10.91;10,112.66,249.56,68.43,10.91">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="s" coord="10,431.19,249.56,76.63,10.91;10,112.66,263.11,151.65,10.91">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<date type="published" when="2012">2012</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,290.20,394.53,10.91;10,112.66,303.75,393.33,10.91;10,112.66,317.30,394.52,10.91;10,112.66,330.85,297.91,10.91" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,112.66,303.75,393.33,10.91;10,112.66,317.30,350.62,10.91">Nlip-essex-itesm at imageclefcaption 2021 task : Deep learning-based information retrieval and multi-label classification towards improving medical image understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">P</forename><surname>Andrade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Compean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Papanastasiou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>Herrera</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/#paper-103" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1264" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,344.40,393.33,10.91;10,112.66,357.95,393.33,10.91;10,112.66,371.50,328.64,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,431.22,344.40,74.76,10.91;10,112.66,357.95,178.98,10.91">Overview of the ImageCLEF 2018 caption prediction tasks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="10,314.15,357.95,110.40,10.91">CLEF2018 Working Notes</title>
		<title level="s" coord="10,431.90,357.95,74.09,10.91;10,112.66,371.50,105.04,10.91">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,385.05,395.16,10.91;10,112.66,398.60,393.33,10.91;10,112.66,412.15,394.52,10.91;10,112.66,425.70,22.69,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,400.32,385.05,107.51,10.91;10,112.66,398.60,175.81,10.91">Overview of the Image-CLEFmed 2019 concept prediction task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="10,313.42,398.60,115.12,10.91">CLEF2019 Working Notes</title>
		<title level="s" coord="10,112.66,412.15,176.51,10.91">CEUR Workshop Proceedings, CEUR-WS.</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,439.25,394.62,10.91;10,112.28,452.79,318.45,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,349.60,439.25,157.67,10.91;10,112.28,452.79,123.60,10.91">Radiology objects in context (roco): A multimodal image dataset</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,258.68,452.79,140.76,10.91">CVII-STENT/LABELS@MICCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,466.34,373.15,10.91" xml:id="b9">
	<monogr>
		<ptr target="https://github.com/itseez/opencv" />
		<title level="m" coord="10,112.66,466.34,192.17,10.91">Itseez, Open source computer vision library</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,479.89,393.32,10.91;10,112.66,493.44,395.01,10.91;10,112.66,506.99,199.39,10.91" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="10,280.31,479.89,225.67,10.91;10,112.66,493.44,254.78,10.91">Puc chile team at caption prediction: Resnet visual encoding and caption classification with parametric relu</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lobel</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/#paper-95" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1174" to="1183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,520.54,393.33,10.91;10,112.66,534.09,394.53,10.91;10,112.66,547.64,237.02,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,359.69,520.54,146.30,10.91;10,112.66,534.09,38.41,10.91">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,196.50,534.09,305.75,10.91">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,561.19,393.32,10.91;10,112.66,574.74,395.00,10.91;10,112.66,588.29,137.64,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,254.59,561.19,207.56,10.91">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,112.66,574.74,307.90,10.91">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,601.84,394.52,10.91;10,112.66,615.39,393.33,10.91;10,112.66,628.93,394.52,10.91;10,112.66,642.48,299.32,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,178.09,601.84,324.20,10.91">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v97/tan19a.html" />
	</analytic>
	<monogr>
		<title level="m" coord="10,293.89,615.39,212.10,10.91;10,112.66,628.93,90.84,10.91;10,269.41,629.95,176.97,9.72">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="10,112.66,656.03,393.33,10.91;10,112.66,669.58,393.33,10.91;11,112.33,86.97,275.01,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,344.36,656.03,161.63,10.91;10,112.66,669.58,82.87,10.91">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.308</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,216.12,669.58,289.87,10.91;11,112.33,86.97,30.22,10.91">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,100.52,393.32,10.91;11,112.66,114.06,395.01,10.91;11,112.66,127.61,143.58,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,160.89,100.52,278.43,10.91">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.195</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,462.22,100.52,43.76,10.91;11,112.66,114.06,294.34,10.91">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,141.16,393.33,10.91;11,112.66,154.71,393.33,10.91;11,112.66,168.26,359.26,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,331.70,141.16,174.29,10.91;11,112.66,154.71,194.75,10.91">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,334.23,154.71,171.76,10.91;11,112.66,168.26,203.53,10.91">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI&apos;17</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence, AAAI&apos;17</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,181.81,393.33,10.91;11,112.66,195.36,393.33,10.91;11,112.66,208.91,307.68,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,298.90,181.81,207.09,10.91;11,112.66,195.36,81.43,10.91">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00907</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,248.63,195.36,257.35,10.91;11,112.66,208.91,51.39,10.91">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,222.46,395.01,10.91;11,112.66,236.01,394.53,10.91;11,112.48,249.56,395.18,10.91;11,112.66,263.11,394.53,10.91;11,112.28,276.66,393.70,10.91;11,112.66,290.20,393.33,10.91;11,112.66,303.75,393.33,10.91;11,112.66,317.30,393.53,10.91;11,112.66,330.85,197.61,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="11,264.32,276.66,241.66,10.91;11,112.66,290.20,261.75,10.91">Overview of the ImageCLEF 2021: Multimedia retrieval in medical, nature, internet and social media applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tauteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Moustahfid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,401.27,290.20,104.72,10.91;11,112.66,303.75,393.33,10.91;11,112.66,317.30,201.15,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="11,348.61,317.30,157.57,10.91;11,112.66,330.85,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
