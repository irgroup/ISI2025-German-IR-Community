<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,417.30,15.42">AUEB NLP Group at ImageCLEFmedical Caption 2022</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,116.63,109.91,5.42"><forename type="first">Foivos</forename><surname>Charalampakos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76, Patission Street</addrLine>
									<postCode>104 34</postCode>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,211.83,116.63,97.68,5.42"><forename type="first">Giorgos</forename><surname>Zachariadis</surname></persName>
							<email>geor.zachariadis@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76, Patission Street</addrLine>
									<postCode>104 34</postCode>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.14,116.63,84.75,5.42"><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76, Patission Street</addrLine>
									<postCode>104 34</postCode>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer and Systems Sciences</orgName>
								<orgName type="department" key="dep2">DSV</orgName>
								<orgName type="institution">Stockholm University</orgName>
								<address>
									<addrLine>Postbox 7003</addrLine>
									<postCode>SE-164 07</postCode>
									<settlement>Kista</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,424.81,116.63,75.75,5.42"><forename type="first">Vasilis</forename><surname>Karatzas</surname></persName>
							<email>basil.karatzas@outlook.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76, Patission Street</addrLine>
									<postCode>104 34</postCode>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,130.58,97.12,5.42"><forename type="first">Christoforos</forename><surname>Trakas</surname></persName>
							<email>christrakas7@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76, Patission Street</addrLine>
									<postCode>104 34</postCode>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,217.41,130.58,103.58,5.42"><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<addrLine>76, Patission Street</addrLine>
									<postCode>104 34</postCode>
									<settlement>Athens</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,417.30,15.42">AUEB NLP Group at ImageCLEFmedical Caption 2022</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">E54E767A1A79D5CDF27352DABA22D0B2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical Images</term>
					<term>Concept Detection</term>
					<term>Image Captioning</term>
					<term>Image Retrieval</term>
					<term>Multi-label Classification</term>
					<term>Convolutional Neural Network</term>
					<term>Natural Language Processing</term>
					<term>Machine Learning</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the methods AUEB's NLP Group used to participate in the annual ImageCLEFmedical Caption Task. The task comprises of the Concept Detection and the Caption Prediction sub-tasks. Concept Detection aims to automatically tag medical images with relevant medical concepts, while Caption Prediction generates draft diagnostic captions of medical images to help medical experts prepare diagnostic reports. For Concept Detection, we employ CNN image encoders, combined with a feedforward neural network classifier or a retrieval module, extending our previous work. For Caption Prediction, we also extend the retrieval approach of our previous work with a caption selection method; we also experiment with a state-of-the-art memory-enhanced caption generation method, a simpler CNN-RNN caption generation model, and a captions ensemble method, which combines predictions from our different models. We ranked 1st in Concept Detection and 2nd in Caption Prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>ImageCLEF <ref type="bibr" coords="1,143.42,420.70,12.85,4.94" target="#b0">[1]</ref> is an evaluation campaign held annually since 2003 as part of CLEF, revolving around image analysis and retrieval tasks. 1 ImageCLEFmedical, which was one of the four main ImageCLEF 2022 tasks, consists of a series of challenges associated with the study and processing of medical images. The ImageCLEFmedical Caption Task <ref type="bibr" coords="1,392.88,461.35,12.76,4.94" target="#b1">[2]</ref> ran for the 6th year in 2022. As in the previous year, it included a Concept Detection sub-task, where the goal was to perform multi-label classification of medical images by automatically assigning medical terms (called concepts) to each image. The concepts stem from the Unified Medical Language System (UMLS) <ref type="bibr" coords="1,125.57,515.55,11.39,4.94" target="#b2">[3]</ref>. 2 Selecting the appropriate medical terms can be a first step towards automatically generating image captions and/or assisting medical experts by reducing the time needed for a diagnosis <ref type="bibr" coords="1,134.67,542.64,11.58,4.94" target="#b3">[4]</ref>. Following the previous edition of the task, ImageCLEFmedical also included a Caption Prediction sub-task <ref type="bibr" coords="2,215.92,90.23,11.42,4.94" target="#b1">[2]</ref>. This sub-task aims to automatically generate draft diagnostic captions given medical images, which could potentially help medical experts prepare diagnostic reports and, more generally, help them analyze more efficiently large volumes of medical images (e.g., X-rays, MRI scans) they confront in their daily workflow <ref type="bibr" coords="2,367.52,130.88,11.43,4.94" target="#b3">[4]</ref>.</p><p>In this paper, we describe the systems that were submitted to the ImageCLEFmedical Caption 2022 sub-tasks by AUEB's NLP Group, which extend prior work on medical image understanding <ref type="bibr" coords="2,89.29,171.52,11.26,4.94" target="#b4">[5,</ref><ref type="bibr" coords="2,103.28,171.52,7.43,4.94" target="#b5">6,</ref><ref type="bibr" coords="2,113.45,171.52,7.43,4.94" target="#b6">7,</ref><ref type="bibr" coords="2,123.62,171.52,7.43,4.94" target="#b7">8,</ref><ref type="bibr" coords="2,133.79,171.52,7.50,4.94" target="#b3">4]</ref>. For the Concept Detection sub-task, our submissions were based on two methods. The first method extends our classification system <ref type="bibr" coords="2,307.85,185.07,11.23,4.94" target="#b4">[5,</ref><ref type="bibr" coords="2,321.34,185.07,7.42,4.94" target="#b5">6,</ref><ref type="bibr" coords="2,331.03,185.07,7.42,4.94" target="#b6">7,</ref><ref type="bibr" coords="2,340.72,185.07,7.49,4.94" target="#b7">8]</ref>, which uses a Convolutional Neural Network (CNN) image encoder <ref type="bibr" coords="2,234.77,198.62,12.99,4.94" target="#b8">[9]</ref> and a feed-forward neural network (FFNN) classifier on top. The second method was based on neural retrieval approaches of our previous work, again employing a CNN image encoder and a weighting scheme suitable for multi-label classification. For the Caption Prediction sub-task, our submissions were based on four methods. The first method was based on the neural retrieval approaches of our previous work, as in the Concept Detection sub-task, with a new mechanism <ref type="bibr" coords="2,286.74,266.37,18.03,4.94" target="#b9">[10]</ref> that uses the retrieved captions. The second method was based on a state-of-the-art caption generation model <ref type="bibr" coords="2,381.16,279.92,16.20,4.94" target="#b10">[11]</ref>, which employs a novel memory-driven Transformer that remembers previously generated captions. The third method was based on the well-known Show and Tell model <ref type="bibr" coords="2,328.02,307.02,16.41,4.94" target="#b11">[12]</ref>, which adopts a simple CNN-RNN architecture. The fourth and last method is an ensemble of our previous approaches, which selects its caption from the ones produced by the other three methods.</p><p>Following our previous successful entries in the competition <ref type="bibr" coords="2,369.14,347.66,11.28,4.94" target="#b4">[5,</ref><ref type="bibr" coords="2,383.16,347.66,7.44,4.94" target="#b5">6,</ref><ref type="bibr" coords="2,393.34,347.66,7.44,4.94" target="#b6">7,</ref><ref type="bibr" coords="2,403.52,347.66,7.52,4.94" target="#b7">8]</ref>, our best performing systems were ranked 1st among 11 participating teams in Concept Detection, and 2nd among the 10 participating teams in Caption Prediction, yet also 1st when ranked with the secondary evaluation metric. The remainder of this article first describes the dataset and our methods for the two subtasks, followed by our submissions and an experimental analysis. The article concludes with our findings and suggested future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data</head><p>The ImageCLEFmedical Caption 2022 dataset is a subset of the Radiology Objects in Context (ROCO) <ref type="bibr" coords="2,127.64,487.58,18.07,4.94" target="#b12">[13]</ref> dataset, which contains medical images extracted from open access biomedical journal articles of PubMed Central. <ref type="foot" coords="2,247.62,498.50,3.71,3.61" target="#foot_0">3</ref> The organisers state that this year's ImageCLEFmedical Caption dataset is an extended version of the dataset used in ImageCLEFmedical Caption 2020. There are images of several different modalities in the dataset (e.g., X-rays, CT-Scans), but no information was provided regarding the modality of each image. The same set of images is used for both Concept Detection and Caption Prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Concept Detection</head><p>All the images in the dataset are accompanied by relevant UMLS <ref type="bibr" coords="2,390.18,605.06,12.99,4.94" target="#b2">[3]</ref> medical concepts and, more specifically, by their unique UMLS identifiers (CUIs), which are the ground truth for the Concept Detection sub-task. These ground truth concepts, which are essentially medical terms, were obtained from the respective image captions using multiple concept extraction methods, and were then manually curated. An image can be associated with multiple CUIs (see Fig. <ref type="figure" coords="3,496.77,425.52,3.63,4.94" target="#fig_0">1</ref>). Table <ref type="table" coords="3,115.52,439.07,5.02,4.94" target="#tab_0">1</ref> shows the 5 most frequent ground truth concepts of the whole dataset (official training &amp; official validation splits), the corresponding UMLS terms, and how many training images they are assigned to. As in previous years, the dataset is highly imbalanced. For example, the concept x-ray computed tomography occurs 28,885 times in the whole dataset (Table <ref type="table" coords="3,468.76,479.72,3.50,4.94" target="#tab_0">1</ref>), while 1,149 other concepts appear only three times each (see the right long tail in Fig. <ref type="figure" coords="3,436.09,493.26,3.50,4.94" target="#fig_1">2</ref>). Furthermore, there is a large number of unique concepts (number of available classes), which increased from 1,585 last year <ref type="bibr" coords="3,154.01,520.36,12.71,4.94" target="#b4">[5]</ref> to 8,374 this year. The average number of concepts assigned to each image is 4.74, while the minimum number is 1. There are 4,316 images with only one assigned concept, and only one image with 50 concepts (the maximum number of assigned concepts).</p><p>An official training set of 83,275 images and an official validation set of 7,645 images were provided by the organisers. The official test set comprised 7,601 images and its gold truth concepts were hidden. For our experiments, we merged the provided (official) training and validation sets, and used 15% of the merged data as our validation set, and another 20% of the merged data as our development set; the former was used for hyper-parameter tuning, whereas the latter was used to evaluate the performance of our models. The remaining 65% served as our training set.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Caption Prediction</head><p>The same images that were used for the Concept Detection sub-task (Sec. 2.2, Fig. <ref type="figure" coords="4,471.82,490.10,4.25,4.94" target="#fig_0">1</ref>) were also used for the Caption Prediction sub-task, but now each of these was also accompanied by a gold caption (the most frequent captions in the dataset are shown in Table <ref type="table" coords="4,448.94,517.20,3.63,4.94" target="#tab_1">2</ref>). This year, approximately 97% (88,342 out of total 90,920) of the provided captions were unique (associated with only one image), whereas in last year's dataset only 65% of the captions were unique. This fact makes retrieval-based approaches less suitable for caption prediction this year. The maximum number of words (tokens) in a single caption was 391 (found in 1 image) and the average number of words per caption was 19.14. The histogram of the caption lengths (Fig. <ref type="figure" coords="4,498.29,584.95,4.17,4.94" target="#fig_2">3</ref>) indicates that the vast majority of captions are shorter than 50 words. As in previous years, BLEU <ref type="bibr" coords="4,223.33,612.05,17.84,4.94" target="#b16">[17]</ref> was the main evaluation measure of the Caption Prediction sub-task. This year, ROUGE-1 <ref type="bibr" coords="4,223.72,625.60,17.84,4.94" target="#b17">[18]</ref> was also employed as a secondary measure. The organisers announced that the following pre-processing steps would be followed before computing the evaluation scores:  • Captions are converted to lower-case.</p><p>• All punctuation is removed and captions are tokenised using a particular tokeniser. <ref type="foot" coords="5,486.22,498.98,3.71,3.61" target="#foot_1">4</ref>• Stopwords are removed using NLTK's 'english' stop-word list.</p><p>• Spacy's lemmatiser is applied. <ref type="foot" coords="5,249.95,528.79,3.71,3.61" target="#foot_2">5</ref>We decided not to follow these pre-processing steps during the training of our models, to avoid discarding or distorting any potentially important words in the gold and generated captions, and to try to produce captions close to those medical experts generate.</p><p>Similarly to the Concept Detection sub-task (Sec. 2.2), we merged the official training and validation sets, creating our own data splits. Specifically, we used 6,000 instances for validation and development, 3,000 for each, and the remaining 84,920 instances were used for training. Again, we used our validation set to tune the hyper-parameters of our models, and the develop- ment set for evaluation. In the training set, we applied a maximum threshold to remove the instances with captions longer than 80 words. This led to the removal of 517 training instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this section, we describe the systems that were used in our submissions to the Concept Detection and Caption Prediction sub-tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Concept Detection</head><p>We describe two Concept Detection systems, 2xCNN+FFNN, which follows the work of our past submissions <ref type="bibr" coords="6,166.19,395.39,11.27,4.94" target="#b4">[5,</ref><ref type="bibr" coords="6,180.17,395.39,7.44,4.94" target="#b5">6,</ref><ref type="bibr" coords="6,190.32,395.39,8.90,4.94" target="#b7">8]</ref> for the same task, and a retrieval-based system, dubbed CNN+wKNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Concept Detection System 1: 2xCNN+FFNN</head><p>This system is an ensemble of two members that share the same architecture: an image encoder (e.g., a CNN), followed by a single dense layer with sigmoid activations (for multi-label classification).</p><p>In 2020, our best performing submission was an ensemble consisting of two instances of a CNN+FFNN classifier that employed a DenseNet-121 <ref type="bibr" coords="6,329.05,498.91,17.97,4.94" target="#b18">[19]</ref> image encoder, pre-trained on Ima-geNet <ref type="bibr" coords="6,117.48,512.46,16.09,4.94" target="#b19">[20]</ref>. We fine-tuned five instances on the task's data and kept the two that performed best on validation data. The two instances were combined by using the union and the intersection of their predicted concepts. The ensemble that used the intersection was ranked 1st. In 2021, we employed ResNet-50 <ref type="bibr" coords="6,229.40,553.11,18.07,4.94" target="#b20">[21]</ref> pre-trained on ImageNet <ref type="bibr" coords="6,366.41,553.11,18.06,4.94" target="#b19">[20]</ref> as the image encoder, and we integrated an extra pre-training step using supervised contrastive learning <ref type="bibr" coords="6,440.97,566.66,16.30,4.94" target="#b21">[22]</ref>. Then, we again fine-tuned and combined two instances using the union and the intersection of the predicted concepts of each instance. We had submitted the union combination as it performed better on our own test split. This system was ranked 8th in 2021, as it was outperformed by our retrieval-based winning systems. This year's submissions employed an EfficientNetV2-B0 CNN <ref type="bibr" coords="6,387.82,634.40,11.58,4.94" target="#b8">[9]</ref>, as the image encoder, pre-trained on ImageNet <ref type="bibr" coords="6,201.60,647.95,16.22,4.94" target="#b19">[20]</ref>. We extracted image embeddings (feature vectors) using the last convolutional layer of the backbone of the image encoder, followed by Generalized-Mean (GeM) global pooling <ref type="bibr" coords="7,157.36,90.23,16.41,4.94" target="#b22">[23]</ref>. Given an input image, the output of the last convolutional layer is a 3D tensor 𝑋 of 𝑊 × 𝐻 × 𝐾 dimensions, where 𝐾 is the number of feature maps (channels) in the layer. Let 𝑋 𝑘 be the 𝑘-th feature map of dimensions 𝑊 × 𝐻 with 𝑘 ∈ {1, 2, . . . , 𝐾}. The GeM layer takes 𝑋 as an input and returns a vector 𝑣 as output:</p><formula xml:id="formula_0" coords="7,188.32,148.80,318.32,42.38">𝑣 = [𝑣 ˜1 . . . 𝑣 ˜𝑘 . . . 𝑣 ˜𝐾] 𝑇 , 𝑣 ˜𝑘 = ⎛ ⎝ 1 |𝑋 𝑘 | ∑︁ 𝑥∈𝑋 𝑘 𝑥 𝑝 ⎞ ⎠ 1 𝑝<label>(1)</label></formula><p>where 𝑥 is the value of the corresponding pixel of the 𝑘-th feature map 𝑋 𝑘 . 𝑣 is the produced image embedding. The pooling parameter 𝑝 can be trained along with the network or be manually set. The image embeddings were then passed through a dense layer with |𝐶| outputs and sigmoid activations, where 𝐶 is the set of all possible concepts, to produce a probability per label. The models were trained by minimizing the binary cross entropy loss of all concepts. We used Adam <ref type="bibr" coords="7,143.05,273.39,18.07,4.94" target="#b23">[24]</ref> as our optimizer and decreased the learning rate by a factor of 10 when the loss showed no improvement. We also used early stopping on the validation set, with patience of 3 epochs. For each instance of the system, a classification threshold for all the concepts was tuned by optimizing the F1 score on our validation data. Any concepts for which the respective model outputs exceeded that threshold, were assigned to the corresponding image during inference. We trained two instances of the same system by fine-tuning on the task's data and kept checkpoints from the two best (on validation data) epochs. Finally, in order to form the ensemble, we combined the two instances using the union and the intersection of their predicted concept sets. We call these models 2xCNN+FFNN@U and 2xCNN+FFNN@I, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Concept Detection System 2: CNN+wKNN</head><p>Following our previous work <ref type="bibr" coords="7,228.17,444.46,11.48,4.94" target="#b4">[5,</ref><ref type="bibr" coords="7,243.42,444.46,7.52,4.94" target="#b5">6,</ref><ref type="bibr" coords="7,254.71,444.46,7.52,4.94" target="#b7">8,</ref><ref type="bibr" coords="7,266.01,444.46,7.65,4.94" target="#b6">7]</ref>, this system employs a neural retrieval approach.</p><p>Intuitively, given a test image, the goal of the system is to retrieve similar images from the training set and select concepts from the retrieved neighbors. We used the image encoder of our fine-tuned CNN+FFNN system (see Sec. 3.1.1). We discarded the last dense layer of the classifier and used the last GeM pooling layer to extract embeddings (feature vectors) for all the training images. Given a test image, we used the same encoder to obtain its embedding (Fig. <ref type="figure" coords="7,477.58,512.20,4.25,4.94" target="#fig_4">4</ref>) and retrieved the (embeddings of the) 𝑘 training images with the highest cosine similarity with the (embedding of the) test image. We tuned the value of 𝑘 in the range from 5 to 100 with a step of 5 using our validation set, which led to 𝑘 = 10. The voting scheme that we used was introduced in <ref type="bibr" coords="7,100.88,566.40,17.91,4.94" target="#b24">[25]</ref> and can be described as follows. Given a test image (query) 𝑥, for each concept 𝑐 𝑖 ∈ 𝐶, we calculate the weighted sum of 𝑘 scores, from each of the 𝑘 neighbors of 𝑥:</p><formula xml:id="formula_1" coords="7,225.25,598.19,281.39,33.63">𝑓 𝑖 (𝑥) = ∑︀ 𝑘 𝑗=1 𝑤 𝑗 • 𝑦 𝑖 (𝑁 𝑘 (𝑥, 𝑗)) ∑︀ 𝑘 𝑗=1 𝑤 𝑗<label>(2)</label></formula><p>where 𝑦 𝑖 (𝑁 𝑘 (𝑥, 𝑗)) denotes the presence of the 𝑖-th concept in the multi-hot vector of the 𝑗-th nearest neighbor of 𝑥, and 𝑤 𝑗 is the weight assigned to the 𝑗-th nearest neighbor. The weights vector &lt;𝑤 1 , . . . , 𝑤 𝑘 &gt; can be learned or can be set manually with monotonically decreasing weights, e.g., the top-ranked neighbor can be given weight 𝑘 and the lowest-ranked neighbor weight 1. We used the latter simple linear assignment method in our experiments. We assigned concept 𝑐 𝑖 to test image 𝑥 by the rule:  </p><formula xml:id="formula_2" coords="8,237.52,148.27,35.90,10.63">ℎ 𝑖 (𝑥) =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Caption Prediction</head><p>In this subsection, we describe four Caption Prediction systems, Retrieval-based Approach, which follows the work of our past submissions <ref type="bibr" coords="8,310.59,474.50,11.49,4.94" target="#b4">[5,</ref><ref type="bibr" coords="8,325.04,474.50,7.52,4.94" target="#b5">6,</ref><ref type="bibr" coords="8,335.53,474.50,7.65,4.94" target="#b7">8]</ref>, R2Gen &amp; Image Clustering, which employs a memory-driven Transformer, CNN-RNN, an Encoder-Decoder model based on the Show&amp;Tell model <ref type="bibr" coords="8,171.40,501.60,17.95,4.94" target="#b25">[26]</ref> and a Captions Ensemble system, which utilize the generated captions of the aforementioned systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Caption Prediction System 1: Retrieval-based Approach</head><p>Since our previous work had led to top performance with retrieval-based methods in Caption Prediction <ref type="bibr" coords="8,138.21,578.00,11.41,4.94" target="#b4">[5,</ref><ref type="bibr" coords="8,152.34,578.00,7.49,4.94" target="#b5">6,</ref><ref type="bibr" coords="8,162.55,578.00,7.61,4.94" target="#b7">8]</ref>, we again explored retrieval-based methods, which are based on KNN. We used an image encoder, pre-trained on ImageNet <ref type="bibr" coords="8,304.50,591.55,16.09,4.94" target="#b19">[20]</ref>, to obtain the embedding of each training image. The embedding was extracted from the last average pooling layer of the network. We experimented with several encoders. Specifically we experimented with DenseNet-121 <ref type="bibr" coords="8,486.67,618.65,16.41,4.94" target="#b18">[19]</ref>, DenseNet-201 <ref type="bibr" coords="8,152.67,632.20,16.08,4.94" target="#b18">[19]</ref>, EfficientNetB0 <ref type="bibr" coords="8,240.40,632.20,16.09,4.94" target="#b26">[27]</ref>, ResNet50V2 <ref type="bibr" coords="8,317.75,632.20,16.09,4.94" target="#b27">[28]</ref>, InceptionResNetV2 <ref type="bibr" coords="8,426.49,632.20,17.76,4.94" target="#b28">[29]</ref> and CotNet50 <ref type="bibr" coords="8,89.29,645.75,16.23,4.94" target="#b29">[30]</ref>. During inference, given a test image, we generate its embedding using the same encoder, and we retrieve the 𝑘 most similar training images, based on the cosine similarity of their embeddings with the test image embedding. Following the Consensus Caption (CC) method of <ref type="bibr" coords="9,100.90,90.23,16.26,4.94" target="#b9">[10]</ref>, we then retrieve the captions of the 𝑘 most similar training images, creating the set 𝑆. Among the captions in 𝑆, we select the caption 𝑐 * with the highest textual similarity with the other captions in 𝑆:</p><formula xml:id="formula_3" coords="9,238.10,138.54,264.68,25.75">𝑐 * = argmax 𝑐∈𝑆 ∑︁ 𝑐 ′ ∈𝑆 cos(𝑐, 𝑐 ′ ) (<label>4</label></formula><formula xml:id="formula_4" coords="9,502.78,144.43,3.86,4.94">)</formula><p>where cos denotes the cosine similarity, calculated using the TF-IDF representations of the captions (the coefficients TF and DF were computed using only the retrieved captions). In effect, we select the caption closest to the centroid of the 𝑘 retrieved captions as the prediction for the test image. An illustration of this approach can been seen in Fig. <ref type="figure" coords="9,378.72,215.20,3.74,4.94" target="#fig_5">5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Caption Prediction System 2: R2Gen &amp; Image Clustering</head><p>This system is based on a memory-driven Transformer and on image clustering. The former, R2Gen <ref type="bibr" coords="9,121.50,463.28,16.18,4.94" target="#b10">[11]</ref>, is a captioning system that has been reported to achieve competitive performance on several medical captioning datasets. Originally, it employed a ResNet-101 <ref type="bibr" coords="9,439.37,476.83,16.41,4.94" target="#b20">[21]</ref>, to extract image (patch) embeddings. However, we substituted it with a DenseNet-121 <ref type="bibr" coords="9,420.74,490.38,16.08,4.94" target="#b18">[19]</ref>, pre-trained on ImageNet <ref type="bibr" coords="9,134.73,503.93,16.17,4.94" target="#b19">[20]</ref>, which showed better performance in preliminary experiments. The representations resulted per image are then passed to a Transformer encoder-decoder <ref type="bibr" coords="9,418.40,517.47,16.08,4.94" target="#b30">[31]</ref>, whose decoder is enhanced with a relational memory and a (memory-driven conditional) layer normalization.</p><p>As an extra step before training we clustered the embeddings of all the images, as these were generated by our DenseNet-121. We used 𝑘-Means, <ref type="foot" coords="9,319.15,555.49,3.71,3.61" target="#foot_3">6</ref> with 𝑘 = 8, defined by varying 𝑘 from 5 to 9 and evaluating each clustering with Silhouette <ref type="bibr" coords="9,316.85,571.67,16.20,4.94" target="#b31">[32]</ref>. For each cluster, we used the training images to fit a separate R2Gen instance (Fig. <ref type="figure" coords="9,290.40,585.22,3.60,4.94" target="#fig_6">6</ref>). Provided a test image, we retrieve the R2Gen instance trained on the cluster the test image belongs to, and we use it to generate a caption. We experimented both, with and without image clustering. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Caption Prediction System 3: CNN-RNN</head><p>This system is based on the Show&amp;Tell model <ref type="bibr" coords="10,302.13,284.24,16.41,4.94" target="#b11">[12]</ref>, which consists of a CNN-RNN encoderdecoder. For the CNN encoder, we experimented with DenseNet-121, DenseNet-201, Efficient-NetB0, ResNet50V2, InceptionResNetV2, and CotNet50. The RNN decoder is fed with the encoded image representation, and generates a caption word by word. In specific, the encoded image representation is concatenated with the hidden states of an encoding GRU <ref type="bibr" coords="10,455.06,338.44,16.33,4.94" target="#b32">[33]</ref>, which operates on the unfinished generated caption. The result is fed onto a decoding GRU, whose last hidden state is followed by a FFNN, in order to yield a probability distribution over the vocabulary and decide the next word to output.</p><p>We pre-processed each training caption by adding a start and end token, at the beginning and end of the text, respectively. Then we created a vocabulary, keeping only the words that appeared 3 or more times in the training captions. The out-of-vocabulary words (OOV) were replaced by an &lt;UNK&gt; token. The maximum length was set to 40 tokens, based on preliminary experiments. We experimented with both greedy decoding (selecting the vocabulary word with the highest predicted probability at each decoding step) and beam search decoding (searching for the most probable sequence of output words, as in machine translation) with different beam sizes (1, 2, 3, 5, 7). In addition, we implemented three different ensembles consisted of Show &amp; Tell models with different encoders (the same we used in Sec. 3.2.3), which all used greedy decoding.</p><p>For the first ensemble, dubbed Maximum Probability (MP), at each decoding step we select the single word with the highest probability from the 𝑘 probability distributions produced by the 𝑘 ensemble members. For the second ensemble, Maximum Voting Probability (MVP) <ref type="bibr" coords="10,486.79,659.29,16.32,4.94" target="#b33">[34]</ref>, at each decoding step we keep the most probable word from the probability distribution of each ensemble member as the word selected by that ensemble member, and we output the word that was selected by most ensemble members. For the third ensemble, Average Probability (AP) <ref type="bibr" coords="11,89.29,117.33,16.41,4.94" target="#b33">[34]</ref>, at each decoding step, each word is assigned the average of the probabilities assigned to that word by the ensemble members. We then output the word with the highest average probability. An extension of this approach could use weighted probability averaging, with weights reflecting how well each ensemble member performs on its own on validation data, so that better ensemble members would influence more the predictions of the ensemble than worse ones.</p><p>In our experiments, beam search performed better (specifically with beam size 3 and 5) than all three ensembles (which all used greedy decoding; we did not use beam search in the ensembles).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Caption Prediction System 4: Captions Ensemble</head><p>In this approach, we utilised the captions generated by each one of the previous systems for a given image, to create an ensemble model. Specifically, for each test image, we gathered all the captions assigned to it by the aforementioned systems, and we applied the Consensus Caption (CC) technique (see Sec. 3.2.1) to select one of the gathered captions. For this system, we used ClinicalBERT <ref type="bibr" coords="11,151.71,315.64,17.91,4.94" target="#b34">[35]</ref> to extract caption embeddings instead of TF-IDF representations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Submissions and results</head><p>In this Section, we provide the results of our experiments for both sub-tasks as well as some insight and comments upon those.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Concept Detection</head><p>We used our development set to evaluate all our models and submitted those that performed best. Two of our six submissions were the ensemble systems (2xCNN+FFNN@U &amp; 2xCNN+FFNN@I) described in Sec. 3.1.1. Additionally, we submitted a CNN+wKNN system (Sec. 3.1.2) as well as another ensemble system that employed two instances of our CNN+FFNN (Sec. 3.1.1) and an instance of the CNN+wKNN system. This ensemble used a majority voting rule to make the concept predictions. That is, for every test image, if a concept is predicted at least by two of the three systems, it is assigned to the image. The sixth assignment consisted of a CNN+FFNN that was trained using the Sharpness-Aware Minimization (SAM) algorithm <ref type="bibr" coords="11,488.03,632.20,17.95,4.94" target="#b35">[36]</ref> and the Gradient Centralization (GC) <ref type="bibr" coords="11,261.47,645.75,18.07,4.94" target="#b36">[37]</ref> optimization technique in order to achieve better generalization performance. In principle, SAM tries to find parameter values whose entire neighborhoods have uniformly low training loss, rather than seeking parameter values that simply have a low training loss. Meanwhile, GC centralizing the gradient vectors of weight matrices to have zero mean and acts as a regularizer <ref type="bibr" coords="12,329.63,103.78,16.41,4.94" target="#b36">[37]</ref>. We also experimented with a loss function that aimed to optimise 𝐹 1 , which was the official evaluation measure of the task. Similarly to <ref type="bibr" coords="12,147.39,130.88,16.41,4.94" target="#b37">[38]</ref>, we employed the 'soft 𝐹 1 score' (sF 1 ), a differentiable version of the 𝐹 1 measure that computes true positives, false positives, and false negatives using the output probability distributions of the model, without applying any threshold to round them to binary decisions. In practice, 1 -sF 1 is used for minimization and the total loss that was used is:</p><formula xml:id="formula_5" coords="12,250.32,193.43,252.46,11.41">𝐿 𝐹 1 = (1 -sF 1 ) • bce (<label>5</label></formula><formula xml:id="formula_6" coords="12,502.78,195.96,3.86,4.94">)</formula><p>where bce is the standard binary cross-entropy loss used in the multi-label setting (i.e., summed for all labels). As mentioned, the primary measure of the competition was 𝐹 1 , calculated by comparing the binary multi-hot vectors 𝑦 𝑡𝑟𝑢𝑒 (the ground truth) and 𝑦 𝑝𝑟𝑒𝑑 (the predicted concepts) of each test image and then macro-averaging over all test images. In addition to the primary measure, this year's task included a secondary 𝐹 1 score that was calculated using a subset of manually validated concepts (only anatomy and image modality ones). To generate the predictions for the official test set, we merged our training, validation and development data. We used a held-out set (15% of the merged data) as our final development set for hyper-parameter tuning and trained the models using the rest of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5</head><p>Scores and rankings of experiments on our final development and the official test sets. Systems that were not submitted do not have test scores and rankings available. cd3's development score is also not available, as its predictions were created using the output files of the ensemble's members. Table <ref type="table" coords="12,126.24,618.65,4.97,4.94">5</ref> presents the scores of our submitted systems on our final development and the official test sets, as well as their official rankings. 2xCNN+FFNN@U had the best results. The table also includes systems that were not submitted and are parts of further experiments that we conducted. We experimented with several image encoders ranging from pure CNNs <ref type="bibr" coords="12,475.76,659.29,18.07,4.94" target="#b38">[39]</ref> to Vision Transformers (ViTs) <ref type="bibr" coords="12,209.26,672.84,17.76,4.94" target="#b39">[40]</ref> and hybrid versions of the two <ref type="bibr" coords="12,363.73,672.84,16.08,4.94" target="#b40">[41]</ref>. Overall, pure convolutional encoders yielded better results and more particularly, EfficicientNetV2-B0 had the best score, so we only used this specific image encoder for our ensemble and CNN+wKNN systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID</head><p>We also experimented with tuning the decision thresholds of the classifiers (probability thresholds for assigning each concept or not). Instead of tuning a single decision threshold value (the same for all concepts), we tried to tune a different threshold value for each concept. Due to the large size of the concept set (|𝐶| = 8, 374) and the large search space for each threshold, we employed Bayesian Optimization <ref type="bibr" coords="13,252.85,171.52,17.81,4.94" target="#b41">[42]</ref> instead of a full parameter sweep and used the task's primary measure as the evaluation function.</p><p>Furthermore, we observed that, despite of their high performance in terms of precision, our systems yielded low recall scores, which can be explained by the fact that the number of the predicted concepts was very low compared to the total number of possible concepts (i.e., only a few hundreds vs. thousands). In order to alleviate this problem, we experimented with a retrieval-augmented classification system. We employed a simple CNN+FFNN model and added to its predicted concepts the concepts of a simple KNN system. That is, every test image was passed through CNN+FFNN and the predicted concepts were added (set union) to the concepts predicted by the KNN system. To generate the latter predictions, retrieved the top-𝑘 closest training images (w.r.t. cosine similarity computed on the image embeddings) of the test image and returned the 2 concepts that were most frequently assigned to the 𝑘 images. We used a 𝑘 of value 10. In spite of producing a higher recall score, the retrieval-augmented classification system scored much lower precision-wise leading to a worse 𝐹 1 score than our other systems and thus, it was not submitted. In general, we aimed to deal with the class imbalance of the dataset and experimented with additional loss functions suitable for imbalanced multi-label tasks, such as the ASL <ref type="bibr" coords="13,193.88,388.31,18.07,4.94" target="#b42">[43]</ref> and Focal <ref type="bibr" coords="13,261.66,388.31,18.07,4.94" target="#b43">[44]</ref> losses, but these experiments did not yield better results either.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Caption Prediction</head><p>For Caption Prediction, in addition to the BLEU measure <ref type="bibr" coords="13,346.68,451.59,18.01,4.94" target="#b16">[17]</ref> of previous years, this year the organisers added ROUGE-1 <ref type="bibr" coords="13,210.62,465.14,17.76,4.94" target="#b17">[18]</ref> as a secondary measure. For the former, the organizers clarified that BLEU is calculated for up to 4-grams, using uniform weights (this is called BLEU-4). For the latter, they used ROUGE-1, which considers the overlap of unigrams between the generated caption and the gold-truth caption. <ref type="foot" coords="13,248.41,503.15,3.71,3.61" target="#foot_4">7</ref> Therefore, we used these two measures to evaluate our models and decide which ones we were going to submit.</p><p>For the ensemble KNN model (Sec. 3.2.1), we combined our training and validation sets, to have more images to retrieve from. The 𝑘 hyper-parameter was tuned in the range <ref type="bibr" coords="13,457.58,546.43,12.33,4.94" target="#b0">[1,</ref><ref type="bibr" coords="13,471.73,543.90,20.22,9.57">100]</ref> on our development set, and the best 𝑘 was 18. All 𝑘 values can been seen in Table <ref type="table" coords="13,448.59,559.98,3.77,4.94" target="#tab_5">6</ref>. It is worth mentioning that this model did not yield the anticipated results for the secondary ROUGE-1 measure. For our first submission, we used an ensemble of KNNs (Caption Prediction System 1: Retrieval-based Approach Sec. 3.2.1) with different image encoders (the same ones we used in Sec. 3.2.3). During inference, for each test image, we collected the captions produced by each ensemble member (for the test image) and selected a single caption from them using the CC method (Sec. 3.2.1). For the R2Gen model without the image clustering step (see Sec. 3.2.2), we performed a K-fold Cross Validation (CV) on our sets. Then, we kept the best 3 models from the Fold-splits with the best BLEU scores, on their corresponding Fold-split development sets. Two of our submissions consisted of model instances that were trained on the training set of the two best fold-splits. One of our submissions is an ensemble that utilizes the models from the three best fold-splits, and other submissions (see Table <ref type="table" coords="14,236.61,552.77,5.00,4.94">7</ref> for more detail). It uses the Captions Ensemble method (see Sec. 3.2.4) to decide the final predictions. These submissions are shown in Table <ref type="table" coords="14,446.43,566.32,3.74,4.94">7</ref>.</p><p>For models based on the CNN-RNN system (Sec. 3.2.3), we only considered beam search decoding for the submissions (BS𝑚 for short, where 𝑚 is the beam size), as well as and ensembles with greedy decoding (MP, MVP, AP -see Sec. 3.2.3). An interesting point about CNN-RNN models is that whenever we observed an increase in the primary BLEU score, a decrease was detected in the secondary ROUGE-1 score. Consequently, we plan to further investigate the generated captions step by step to conduct an exploratory error analysis and shed more light on this phenomenon.</p><p>For the Captions Ensemble method (Sec. 3.2.4, Fig. <ref type="figure" coords="15,319.61,90.23,3.50,4.94" target="#fig_7">7</ref>), we used ClinicalBERT instead of TF-IDF, as already noted, which worked better. This may be due to the fact that ClinicalBERT is a BERT model <ref type="bibr" coords="15,148.15,117.33,18.00,4.94" target="#b44">[45]</ref> pre-trained on numerous medical texts. The Captions Ensemble models we submitted are listed in Table <ref type="table" coords="15,218.11,130.88,3.74,4.94">7</ref>.</p><p>Lastly, an interesting observation, is that encoders with complex architectures performed worse than encoders with fewer parameters. Hence, we did not use CotNet50 <ref type="bibr" coords="15,456.56,157.97,18.07,4.94" target="#b29">[30]</ref> as the backbone encoder in our systems and Ensemble models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 7</head><p>Our 9 submissions to the ImageCLEFmedical Caption Prediction sub-task, along with their rank on all submission runs. The development scores of submissions cp26, cp27 and cp28 are not available due to the fact that their caption predictions were created using other submission files. All these submissions are based on System 4 (Captions Ensemble method, Sec. Our team officially ranked 2nd among 10 teams in the Caption Prediction sub-task. Our best model was EfficientNetB0@CNN-RNN -BS5, which is based on System 3 (Sec. 3.2.3) and employed EfficientNet-B0 <ref type="bibr" coords="15,203.41,438.43,17.76,4.94" target="#b26">[27]</ref> as the image encoder. We also ranked 1st in the secondary metric (ROUGE-1) according to the official results, by using an ensemble of cp18, cp19, cp20 and cp27 with the Consensus Caption method (Sec. 3.2.4). The table with all official measures is provided in the Appendix A</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and future work</head><p>We described the submissions of AUEB's NLP Group to the 2022 ImageCLEFmedical Caption subtasks, Concept Detection and Caption Prediction. In Concept Detection, we ranked 1st amongst 11 teams. Our top system was an ensemble of two CNN+FFNN multi-label classifiers, which employed an EfficientNetV2-B0 <ref type="bibr" coords="15,234.45,578.00,12.97,4.94" target="#b8">[9]</ref> image encoder. Our submissions also included classifiers trained with different optimization techniques <ref type="bibr" coords="15,307.52,591.55,16.55,4.94" target="#b35">[36,</ref><ref type="bibr" coords="15,327.95,591.55,14.11,4.94" target="#b36">37]</ref> and objectives, as well as a neural retrieval approach that was again competitive, as in previous years. In Caption Prediction, we ranked 2nd amongst 10 teams, by using Show and Tell <ref type="bibr" coords="15,328.99,618.65,16.09,4.94" target="#b11">[12]</ref>, with EfficientNet-B0 <ref type="bibr" coords="15,443.86,618.65,17.76,4.94" target="#b26">[27]</ref> for image encoding and a GRU <ref type="bibr" coords="15,185.96,632.20,18.07,4.94" target="#b32">[33]</ref> for text decoding. Our analysis included experiments with R2Gen <ref type="bibr" coords="15,89.29,645.75,16.25,4.94" target="#b10">[11]</ref>, combined with image clustering, and with a neural retrieval approach that was based on prior work <ref type="bibr" coords="15,139.29,659.29,11.23,4.94" target="#b4">[5,</ref><ref type="bibr" coords="15,153.25,659.29,7.42,4.94" target="#b5">6,</ref><ref type="bibr" coords="15,163.41,659.29,7.49,4.94" target="#b7">8]</ref>. In future work, we aim to investigate more neural retrieval methods and to explore multi-modal approaches that incorporate information from both images and text <ref type="bibr" coords="15,486.76,672.84,16.25,4.94" target="#b45">[46]</ref>. shot text-to-image generation, in: International Conference on Machine Learning (ICML), Online, 2021, pp. 8821-8831.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Caption Prediction results on all official measures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 8</head><p>The submissions to the ImageCLEFmedical Caption Prediction Task, along with their results on all official measures. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,372.70,416.69,8.93;3,88.99,386.48,136.03,4.79;3,102.60,84.19,390.08,281.93"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Three images of the dataset [14, 15, 16](1st row) with their corresponding ground truth tags (2nd row) and captions (3rd row).</figDesc><graphic coords="3,102.60,84.19,390.08,281.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,274.18,417.79,8.93;4,89.29,287.95,342.43,4.79;4,161.49,84.19,272.30,183.40"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Number of images (vertical axis) tagged with each concept in the training data. To save space, the horizontal axis shows the index of each concept (class index), instead of its CUI.</figDesc><graphic coords="4,161.49,84.19,272.30,183.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,89.29,445.85,309.48,8.93;5,155.89,238.68,283.50,194.60"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Histogram of the length in tokens of all the official gold captions.</figDesc><graphic coords="5,155.89,238.68,283.50,194.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,276.45,136.62,8.79,6.98;8,285.24,140.66,71.32,10.63;8,285.24,156.92,71.32,10.63;8,495.06,150.80,11.57,4.94;8,89.29,181.89,416.70,10.63;8,89.29,197.97,61.05,4.94"><head>{︃ 1 ,</head><label>1</label><figDesc>𝑓 𝑖 (𝑥) ≥ 0.5 0, 𝑓 𝑖 (𝑥) &lt; 0.5 (3) yielding the predicted label set 𝐻(𝑥) = {𝑐 𝑖 |ℎ 𝑖 (𝑥) = 1} = {𝑐 𝑖 |𝑓 𝑖 (𝑥) ≥ 0.5}. We call this system CNN+wKNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,89.29,377.53,417.29,8.93;8,88.93,391.31,321.23,4.79;8,89.29,214.92,416.70,156.03"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of how the retrieval-based CNN+wKNN Concept Detection system (Sec. 3.1.2) works at inference time. The training image embeddings are computed offline.</figDesc><graphic coords="8,89.29,214.92,416.70,156.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,89.29,369.09,416.69,8.93;9,89.29,382.86,308.49,4.79;9,89.29,234.65,416.69,127.85"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Illustration of how the retrieval-based Caption Prediction method (Sec. 3.2.1) works at inference time. The training image embeddings are again computed offline.</figDesc><graphic coords="9,89.29,234.65,416.69,127.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="10,89.29,212.19,416.70,8.93;10,89.29,225.97,66.05,4.79;10,89.29,84.19,416.70,121.25"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Illustration of the R2Gen Caption Prediction method with image clustering added, at inference time (Sec. 3.2.2).</figDesc><graphic coords="10,89.29,84.19,416.70,121.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="11,89.29,384.38,416.69,8.93;11,89.29,398.16,144.10,4.79"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Illustration of the Captions Ensemble method of Caption Prediction, which utilises the Consensus Caption technique [10].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,319.28,418.65,108.38"><head>Table 1</head><label>1</label><figDesc>The 5 most frequent concepts (CUIs and corresponding UMLS terms) in the official dataset of Image-CLEFmedical Caption 2022 and how many images they are assigned to.</figDesc><table coords="4,184.43,359.24,226.42,68.42"><row><cell>CUI</cell><cell>UMLS term</cell><cell>Images</cell></row><row><cell cols="2">C0040405 X-Ray Computed Tomography</cell><cell>28,885</cell></row><row><cell>C1306645</cell><cell>Plain x-ray</cell><cell>26,412</cell></row><row><cell cols="2">C0024485 Magnetic Resonance Imaging</cell><cell>15,693</cell></row><row><cell>C0041618</cell><cell>Ultrasonography</cell><cell>12,236</cell></row><row><cell>C0817096</cell><cell>Chest</cell><cell>8,030</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.99,90.49,415.25,132.29"><head>Table 2</head><label>2</label><figDesc>Most common captions in the dataset.</figDesc><table coords="5,95.71,118.49,408.54,104.29"><row><cell>Caption</cell><cell>Occurrences</cell></row><row><cell>case a electrocardiogram with inferolateral early repolarization pattern with jpoint</cell><cell></cell></row><row><cell></cell><cell>261</cell></row><row><cell>elevation and qrs slur after hypothermia treatment red arrow</cell><cell></cell></row><row><cell>case telemetry tracing ventricular fibrillation precede by a ventricular extrasystole</cell><cell>179</cell></row><row><cell>case a a electrocardiogram with an aggressive inferiorlateral er pattern during</cell><cell></cell></row><row><cell>hypothermia treatment red arrow b the electrocardiogram be completely normalise</cell><cell>115</cell></row><row><cell>after adminbetration of beoproterenol infusion</cell><cell></cell></row><row><cell>the degenerative nuclear atypic area ancient modification he</cell><cell>61</cell></row><row><cell>chest xray</cell><cell>48</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,88.99,90.49,418.52,116.47"><head>Table 3</head><label>3</label><figDesc>Most common words found in the captions of the (whole) provided dataset, with and without stopwords. There are 19,217 words (w/ stopwords) with only 1 occurrence.</figDesc><table coords="6,95.00,130.28,405.29,76.68"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Most common words</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Word</cell><cell>the</cell><cell>of</cell><cell>show</cell><cell>and</cell><cell>a</cell><cell>in</cell><cell cols="2">with</cell><cell>be</cell><cell>arrow</cell><cell>right</cell></row><row><cell cols="12">Occurrences 129,758 84,428 41,364 40,003 35,811 34,437 32,688 24,649 24,555 20,340</cell></row><row><cell></cell><cell></cell><cell cols="6">Most common words (excluding stop-words)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Word</cell><cell>show</cell><cell>arrow</cell><cell>right</cell><cell>ct</cell><cell>image</cell><cell>left</cell><cell>scan</cell><cell cols="4">tomography chest mass</cell></row><row><cell cols="8">Occurrences 41,364 24,555 20,340 16,495 14,703 12,752 11,655</cell><cell></cell><cell>10,628</cell><cell cols="2">10,052 9,192</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,88.99,472.07,417.00,58.96"><head>Table 4</head><label>4</label><figDesc>Example of a text sequence input before and after the preprocessing used in the Caption Prediction System 3: CNN-RNN (see Sec. 3.2.3).</figDesc><table coords="10,105.51,513.89,384.26,17.15"><row><cell>Before</cell><cell>digital subtraction angiography of the left pedal vessel</cell></row><row><cell>After</cell><cell>startsequence digital subtraction angiography of the left &lt;UNK&gt; vessel endsequence</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="14,88.99,90.49,357.10,352.66"><head>Table 6</head><label>6</label><figDesc>The BLEU and ROUGE-1 scores from all of our experiments on our development set.</figDesc><table coords="14,149.19,118.49,296.91,324.66"><row><cell>ID</cell><cell>Approach</cell><cell cols="2">Development BLEU ROUGE-1</cell></row><row><cell>cp1</cell><cell>DenseNet121 KNN (best k=28)</cell><cell>0.3166</cell><cell>0.1117</cell></row><row><cell>cp2</cell><cell>DenseNet201 KNN(best k=22)</cell><cell>0.3019</cell><cell>0.1139</cell></row><row><cell>cp3</cell><cell>EfficientNetB0 KNN (best k=16)</cell><cell>0.3165</cell><cell>0.1276</cell></row><row><cell>cp4</cell><cell>ResNet50V2 KNN (best k=26)</cell><cell>0.3189</cell><cell>0.1226</cell></row><row><cell>cp5</cell><cell>InceptionResNetV2 KNN (best k=31)</cell><cell>0.2964</cell><cell>0.0981</cell></row><row><cell>cp6</cell><cell>DenseNet121@CNN-RNN -BS3</cell><cell>0.3029</cell><cell>0.1567</cell></row><row><cell>cp7</cell><cell>DenseNet201@CNN-RNN -BS3</cell><cell>0.3054</cell><cell>0.1578</cell></row><row><cell>cp8</cell><cell>EfficientNetB0@CNN-RNN -BS3</cell><cell>0.3109</cell><cell>0.1587</cell></row><row><cell>cp10</cell><cell>ResNet50V2@CNN-RNN -BS3</cell><cell>0.3002</cell><cell>0.1589</cell></row><row><cell cols="2">cp11 InceptionResNet50V2@CNN-RNN -BS3</cell><cell>0.2987</cell><cell>0.1382</cell></row><row><cell>cp12</cell><cell>DenseNet121@CNN-RNN -BS5</cell><cell>0.3189</cell><cell>0.1467</cell></row><row><cell>cp13</cell><cell>DenseNet201@CNN-RNN -BS5</cell><cell>0.3116</cell><cell>0.1598</cell></row><row><cell>cp14</cell><cell>EfficientNetB0@CNN-RNN -BS5</cell><cell>0.3280</cell><cell>0.1678</cell></row><row><cell>cp15</cell><cell>ResNet50V2@CNN-RNN -BS5</cell><cell>0.3145</cell><cell>0.1532</cell></row><row><cell cols="2">cp16 InceptionResNet50V2@CNN-RNN -BS5</cell><cell>0.3021</cell><cell>0.1243</cell></row><row><cell>cp17</cell><cell>ResNet101@R2Gen (Authors)</cell><cell>0.2885</cell><cell>0.1490</cell></row><row><cell>cp18</cell><cell>DenseNet121@R2Gen (Best split)</cell><cell>0.3089</cell><cell>0.1938</cell></row><row><cell>cp19</cell><cell>DenseNet121@R2Gen (2nd Best split)</cell><cell>0.3021</cell><cell>0.1939</cell></row><row><cell>cp20</cell><cell>DenseNet121@R2Gen (3rd Best Split)</cell><cell>0.3000</cell><cell>0.1998</cell></row><row><cell cols="3">cp21 DenseNet121@ImageClustering + R2Gen 0.3183</cell><cell>0.1892</cell></row><row><cell>cp22</cell><cell>Ensembles KNN (best k=18) (CC)</cell><cell>0.3196</cell><cell>0.1267</cell></row><row><cell>cp23</cell><cell>Ensembles Greedy Search (MP)</cell><cell>0.2938</cell><cell>0.1660</cell></row><row><cell>cp24</cell><cell>Ensembles Greedy Search (MVP)</cell><cell>0.2848</cell><cell>0.1629</cell></row><row><cell>cp25</cell><cell>Ensembles Greedy Search (AP)</cell><cell>0.2854</cell><cell>0.1647</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="15,94.40,246.11,409.69,132.76"><head></head><label></label><figDesc>3.2.4)    </figDesc><table coords="15,94.40,272.86,409.69,106.01"><row><cell>ID</cell><cell>Run ID</cell><cell>Approach</cell><cell cols="4">Development BLEU ROUGE-1 BLEU ROUGE-1 Test</cell><cell>Rank</cell></row><row><cell cols="2">cp14 181853</cell><cell>EfficientNetB0@CNN-RNN -BS5</cell><cell>0.3280</cell><cell>0.1678</cell><cell>0.3221</cell><cell>0.1664</cell><cell>11</cell></row><row><cell cols="2">cp26 182129</cell><cell>Ensemble of cp8, cp14, cp22, cp28 (CC)</cell><cell>-</cell><cell>-</cell><cell>0.3195</cell><cell>0.1817</cell><cell>12</cell></row><row><cell cols="3">cp27 182100 Ensemble of cp8, cp14, cp18, cp19, cp20, cp22 (CC)</cell><cell>-</cell><cell>-</cell><cell>0.3166</cell><cell>0.1991</cell><cell>13</cell></row><row><cell cols="2">cp22 181285</cell><cell>Ensemble KNN (best k=18) (CC)</cell><cell>0.3196</cell><cell>0.1267</cell><cell>0.3126</cell><cell>0.1177</cell><cell>14</cell></row><row><cell>cp8</cell><cell>181488</cell><cell>EfficientNetB0@CNN-RNN -BS3</cell><cell>0.3109</cell><cell>0.1587</cell><cell>0.3086</cell><cell>0.1741</cell><cell>21</cell></row><row><cell cols="2">cp28 182287</cell><cell>Ensemble of cp18, cp19, cp20, cp27 (CC)</cell><cell>-</cell><cell>-</cell><cell>0.3084</cell><cell>0.2062</cell><cell>22</cell></row><row><cell cols="2">cp18 182052</cell><cell>DenseNet121@R2Gen (Best split)</cell><cell>0.3089</cell><cell>0.1938</cell><cell>0.2960</cell><cell>0.2013</cell><cell>29</cell></row><row><cell cols="2">cp19 181357</cell><cell>DenseNet121@R2Gen (2nd Best split)</cell><cell>0.3021</cell><cell>0.1939</cell><cell>0.2895</cell><cell>0.2051</cell><cell>32</cell></row><row><cell cols="2">cp21 181536</cell><cell>DenseNet121@ImageClustering + R2Gen</cell><cell>0.3183</cell><cell>0.1892</cell><cell>0.2741</cell><cell>0.1760</cell><cell>42</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,108.93,673.71,131.01,4.06"><p>https://www.ncbi.nlm.nih.gov/pmc/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="5,108.93,662.75,346.22,4.06"><p>https://www.nltk.org/_modules/nltk/tokenize/punkt.html#PunktLanguageVars.word_tokenize.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="5,108.93,673.71,115.64,4.06"><p>https://spacy.io/api/lemmatizer.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="9,108.93,673.63,288.11,4.06"><p>https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4" coords="13,108.93,673.69,261.59,4.06"><p>https://github.com/google-research/google-research/tree/master/rouge.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="16,112.66,114.54,395.01,4.94;16,112.66,128.09,395.17,4.94;16,112.39,141.64,394.80,4.94;16,112.48,155.19,393.51,4.94;16,112.66,168.74,393.33,4.94;16,112.66,182.29,395.17,4.94;16,112.66,195.84,394.52,4.94;16,112.66,209.39,131.87,4.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="16,294.00,155.19,211.99,4.94;16,112.66,168.74,261.46,4.94">Overview of the ImageCLEF 2022: Multimedia retrieval in medical, social media and nature applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,401.20,168.74,104.79,4.94;16,112.66,182.29,395.17,4.94;16,112.66,195.84,198.31,4.94">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<title level="s" coord="16,318.04,195.84,184.66,4.94">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,222.94,395.17,4.94;16,111.81,236.48,395.37,4.94;16,112.66,250.03,393.33,4.94;16,112.66,263.58,216.46,4.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="16,326.22,236.48,180.96,4.94;16,112.66,250.03,177.87,4.94">Overview of ImageCLEFmedical 2022caption prediction and concept detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,313.28,250.03,192.71,4.94;16,112.66,263.58,97.38,4.94">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,277.13,393.32,4.94;16,112.66,290.68,212.52,4.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="16,186.64,277.13,319.34,4.94;16,112.66,290.68,52.45,4.94">The Unified Medical Language System (UMLS): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,173.80,290.68,98.78,4.94">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,304.23,393.70,4.94;16,112.66,316.96,157.28,7.90" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Papamichail</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.07299</idno>
		<title level="m" coord="16,393.98,304.23,112.39,4.94;16,112.66,317.78,27.24,4.94">Diagnostic captioning: A survey</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,331.33,393.33,4.94;16,112.66,344.88,393.33,4.94;16,112.14,358.43,360.61,4.94" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,478.16,331.33,27.83,4.94;16,112.66,344.88,201.88,4.94">AUEB NLP Group at ImageCLEFmed Caption Tasks</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Charalampakos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,358.70,344.88,147.29,4.94;16,112.14,358.43,167.82,4.94">CLEF2021 Working Notes, CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="1184" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,371.98,395.17,4.94;16,112.66,385.53,394.53,4.94;16,112.66,399.07,214.32,4.94" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="16,380.29,371.98,127.54,4.94;16,112.66,385.53,83.65,4.94">AUEB NLP Group at Image-CLEFmed Caption</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,244.95,385.53,262.24,4.94;16,112.66,399.07,59.92,4.94">CLEF2020 Working Notes, CEUR Workshop Proceedings, CEUR-WS.org</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,412.62,393.32,4.94;16,112.66,426.17,393.33,4.94;16,112.66,439.72,393.58,4.94;16,112.66,453.27,206.83,4.94" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="16,320.12,412.62,185.87,4.94;16,112.66,426.17,57.31,4.94">Medical Image Tagging by Deep Learning and Retrieval</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,192.79,426.17,313.20,4.94;16,112.66,439.72,393.58,4.94;16,112.66,453.27,21.62,4.94">Experimental IR Meets Multilinguality, Multimodality, and Interaction Proceedings of the Eleventh International Conference of the CLEF Association (CLEF 2020)</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="154" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,466.82,393.33,4.94;16,112.66,480.37,394.53,4.94;16,112.66,493.92,139.82,4.94" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="16,334.00,466.82,171.98,4.94;16,112.66,480.37,34.80,4.94">AUEB NLP Group at ImageCLEFmed Caption</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,190.82,480.37,289.68,4.94">CLEF2019 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,507.47,393.33,4.94;16,112.66,521.02,395.01,4.94;16,112.41,534.57,59.11,4.94" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="16,184.16,507.47,227.10,4.94">Efficientnetv2: Smaller models and faster training</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,438.33,507.47,67.65,4.94;16,112.66,521.02,252.99,4.94">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="10096" to="10106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,548.12,393.53,4.94;16,112.66,561.67,326.02,4.94" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04467</idno>
		<title level="m" coord="16,381.36,548.12,124.83,4.94;16,112.66,561.67,143.77,4.94">Exploring nearest neighbor approaches for image captioning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="16,112.66,575.21,393.33,4.94;16,112.66,588.76,393.33,4.94;16,112.66,602.31,268.58,4.94" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="16,288.49,575.21,217.49,4.94;16,112.66,588.76,51.35,4.94">Generating radiology reports via memory-driven transformer</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,189.11,588.76,316.87,4.94;16,112.66,602.31,134.49,4.94">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1439" to="1449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,615.86,394.53,4.94;16,112.66,629.41,394.53,4.94;16,112.66,642.96,173.95,4.94" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="16,296.75,615.86,206.04,4.94">Show and tell: A neural image caption generator</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,129.33,629.41,373.30,4.94">Proceedings of the IEEE conference on Computer Vision and pattern recognition</title>
		<meeting>the IEEE conference on Computer Vision and pattern recognition<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,656.51,393.33,4.94;16,112.33,670.06,393.65,4.94;17,112.33,90.23,117.93,4.94" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="16,369.94,656.51,136.04,4.94;16,112.33,670.06,165.16,4.94">Radiology Objects in COntext (ROCO): A Multimodal Image Dataset</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="16,350.28,670.06,155.70,4.94;17,112.33,90.23,24.73,4.94">Lecture Notes in Computer Science (LNCS</title>
		<imprint>
			<biblScope unit="volume">11043</biblScope>
			<biblScope unit="page" from="180" to="189" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,103.78,393.33,4.94;17,112.66,117.33,226.96,4.94" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="17,226.18,103.78,279.81,4.94;17,112.66,117.33,77.80,4.94">Surgical reconstruction of the left main coronary artery with patch-angioplasty</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Martinovic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Greve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,198.96,117.33,88.07,4.94">J Cardiothorac Surg</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,130.88,393.33,4.94;17,112.28,144.43,132.36,4.94" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="17,269.26,130.88,190.49,4.94">Ascariasis cholecystitis: An unusual cause</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">K</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,469.23,130.88,36.75,4.94;17,112.28,144.43,53.50,4.94">J Minim Access Surg</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="108" to="110" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,157.97,393.61,4.94;17,112.66,171.52,393.33,4.94;17,112.66,185.07,185.42,4.94" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="17,339.67,157.97,166.60,4.94;17,112.66,171.52,393.33,4.94;17,112.66,185.07,27.79,4.94">Cone Beam Computed Tomography Findings in Calcifying Cystic Odontogenic Tumor Associated with Odontome: A Case Report</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Phulambrikar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">V</forename><surname>Kant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kode</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Magar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,149.01,185.07,65.14,4.94">J Dent (Shiraz)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="374" to="379" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,198.62,393.33,4.94;17,112.66,212.17,393.53,4.94;17,112.66,225.72,354.13,4.94" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="17,310.87,198.62,195.12,4.94;17,112.66,212.17,88.86,4.94">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,224.89,212.17,281.30,4.94;17,112.66,225.72,116.04,4.94">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,239.27,395.17,4.94;17,112.66,252.82,231.60,4.94" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="17,156.04,239.27,254.16,4.94">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,433.54,239.27,74.29,4.94;17,112.66,252.82,75.47,4.94">Text Summarization Branches Iut</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,266.37,393.33,4.94;17,112.66,279.92,393.33,4.94;17,112.66,293.47,238.40,4.94" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="17,354.40,266.37,151.59,4.94;17,112.66,279.92,39.98,4.94">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,182.56,279.92,323.43,4.94;17,112.66,293.47,51.39,4.94">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,307.02,393.33,4.94;17,112.66,320.56,393.33,4.94;17,112.66,334.11,279.68,4.94" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="17,338.94,307.02,167.04,4.94;17,112.66,320.56,65.12,4.94">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,200.70,320.56,305.29,4.94;17,112.66,334.11,86.61,4.94">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Miami Beach, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,347.66,395.17,4.94;17,112.66,361.21,394.52,4.94;17,112.66,374.76,105.79,4.94" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="17,251.58,347.66,194.92,4.94">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,469.26,347.66,38.56,4.94;17,112.66,361.21,322.89,4.94">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,388.31,395.17,4.94;17,112.66,401.86,393.33,4.94;17,112.66,415.41,301.13,4.94" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="17,148.95,401.86,139.25,4.94">Supervised contrastive learning</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="17,311.19,401.86,194.80,4.94;17,112.66,415.41,33.92,4.94">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18661" to="18673" />
			<date type="published" when="2020">2020</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,428.96,393.33,4.94;17,112.66,442.51,393.98,4.94;17,112.41,456.06,48.96,4.94" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="17,277.26,428.96,228.72,4.94;17,112.66,442.51,47.25,4.94">Fine-tuning CNN image retrieval with no human annotation</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Radenović</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,169.31,442.51,293.82,4.94">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1655" to="1668" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,469.61,393.33,4.94;17,112.66,483.16,336.70,4.94" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="17,236.18,469.61,169.80,4.94">A Method for Stochastic Optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,429.48,469.61,76.50,4.94;17,112.66,483.16,211.47,4.94">3rd International Conference on Learning Representations (ICLR)</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,496.70,394.53,4.94;17,112.66,510.25,393.33,4.94;17,112.66,523.80,232.19,4.94" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="17,230.20,496.70,272.91,4.94">A ranking-based knn approach for multi-label classification</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,128.85,510.25,269.79,4.94">Proceedings of the Asian Conference on Machine Learning</title>
		<meeting>the Asian Conference on Machine Learning<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="81" to="96" />
		</imprint>
		<respStmt>
			<orgName>Singapore Management University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,537.35,394.53,4.94;17,112.66,550.90,393.32,4.94;17,112.66,564.45,298.19,4.94" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="17,480.15,537.35,27.04,4.94;17,112.66,550.90,311.83,4.94">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,447.46,550.90,58.52,4.94;17,112.66,564.45,141.18,4.94">International conference on machine learning</title>
		<meeting><address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,578.00,394.61,4.94;17,112.66,591.55,395.00,4.94" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="17,174.72,578.00,313.52,4.94">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,112.66,591.55,198.37,4.94">International conference on machine learning</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,605.10,393.33,4.94;17,112.66,618.65,305.52,4.94" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="17,249.53,605.10,191.77,4.94">Identity mappings in deep residual networks</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,463.99,605.10,41.99,4.94;17,112.66,618.65,142.26,4.94">European Conference on Computer Vision</title>
		<meeting><address><addrLine>Amsterdam, NL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,632.20,393.33,4.94;17,112.66,645.75,394.53,4.94;17,112.66,659.29,133.47,4.94" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="17,331.70,632.20,174.29,4.94;17,112.66,645.75,183.65,4.94">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,318.47,645.75,184.62,4.94">AAAI Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>San Fransicso, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,672.84,393.33,4.94;18,112.33,90.23,294.82,4.94" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="17,234.74,672.84,241.84,4.94">Contextual transformer networks for visual recognition</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,484.91,672.84,21.08,4.94;18,112.33,90.23,262.90,4.94">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,103.78,394.53,4.94;18,112.28,117.33,391.46,4.94" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="18,112.28,117.33,107.76,4.94">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,228.71,117.33,230.24,4.94">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,130.88,393.53,4.94;18,112.66,144.43,346.75,4.94" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="18,186.62,130.88,319.57,4.94;18,112.66,144.43,34.00,4.94">Silhouettes: a graphical aid to the interpretation and validation of cluster analysis</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="18,154.83,144.43,230.78,4.94">Journal of Computational and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="53" to="65" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,157.97,393.32,4.94;18,112.66,171.52,394.53,4.94;18,112.66,185.07,61.25,4.94" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="18,298.60,157.97,207.38,4.94;18,112.66,171.52,143.26,4.94">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,279.62,171.52,176.35,4.94">NIPS 2014 Workshop on Deep Learning</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,198.62,393.33,4.94;18,112.66,212.17,394.52,4.94;18,112.66,225.72,165.76,4.94" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="18,220.52,198.62,285.47,4.94;18,112.66,212.17,45.71,4.94">Ensemble learning on deep neural networks for image caption generation</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Katpally</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,204.48,212.17,298.20,4.94">IEEE 14th International Conference on Semantic Computing (ICSC)</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="61" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,239.27,394.53,4.94;18,112.66,252.82,393.33,4.94;18,112.66,266.37,355.46,4.94" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="18,112.66,252.82,194.37,4.94">Publicly available clinical BERT embeddings</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">H</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jindi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mcdermott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,330.71,252.82,175.27,4.94;18,112.66,266.37,139.36,4.94">Proceedings of the 2nd Clinical Natural Language Processing Workshop</title>
		<meeting>the 2nd Clinical Natural Language Processing Workshop<address><addrLine>Minneapolis, Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="72" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,279.92,393.61,4.94;18,112.66,293.47,395.01,4.94" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="18,311.05,279.92,195.22,4.94;18,112.66,293.47,108.20,4.94">Sharpness-aware minimization for efficiently improving generalization</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Foret</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,243.12,293.47,235.21,4.94">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,307.02,395.17,4.94;18,112.66,320.56,211.15,4.94" xml:id="b36">
	<monogr>
		<title level="m" type="main" coord="18,281.32,307.02,226.51,4.94;18,112.66,320.56,140.08,4.94">Gradient-centralization: A new optimization technique for deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="635" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,334.11,393.53,4.94;18,112.66,347.66,393.33,4.94;18,112.66,361.21,247.61,4.94" xml:id="b37">
	<analytic>
		<title level="a" type="main" coord="18,283.08,334.11,223.11,4.94;18,112.66,347.66,172.15,4.94">A Competitive Deep Neural Network Approach for the ImageCLEFmed Caption 2020 Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kalimuthu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nunnari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Sonntag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,308.25,347.66,113.82,4.94">CLEF2020 Working Notes</title>
		<title level="s" coord="18,429.56,347.66,76.43,4.94;18,112.66,361.21,97.38,4.94">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,374.76,393.33,4.94;18,112.66,387.49,198.20,7.90" xml:id="b38">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.03545</idno>
		<title level="m" coord="18,369.75,374.76,102.79,4.94">A convnet for the 2020s</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,401.86,395.16,4.94;18,112.66,415.41,393.33,4.94;18,112.41,428.96,393.58,4.94;18,112.66,442.51,241.74,4.94" xml:id="b39">
	<analytic>
		<title level="a" type="main" coord="18,421.13,415.41,84.86,4.94;18,112.41,428.96,246.86,4.94">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,380.46,428.96,125.52,4.94;18,112.66,442.51,176.57,4.94">9th International Conference on Learning Representations, ICLR 2021</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,456.06,393.33,4.94;18,112.66,469.61,393.33,4.94;18,112.28,483.16,163.92,4.94" xml:id="b40">
	<analytic>
		<title level="a" type="main" coord="18,265.87,456.06,240.12,4.94;18,112.66,469.61,41.98,4.94">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="18,177.84,469.61,235.80,4.94">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3965" to="3977" />
			<date type="published" when="2021">2021</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,496.70,393.33,4.94;18,112.66,510.25,393.33,4.94;18,112.33,523.80,99.37,4.94" xml:id="b41">
	<analytic>
		<title level="a" type="main" coord="18,276.34,496.70,229.65,4.94;18,112.66,510.25,46.75,4.94">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="18,185.52,510.25,237.68,4.94">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<date type="published" when="2012">2012</date>
			<pubPlace>Lake Tahoe, NV, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,537.35,395.17,4.94;18,112.66,550.90,393.33,4.94;18,112.66,564.45,171.01,4.94" xml:id="b42">
	<analytic>
		<title level="a" type="main" coord="18,478.56,537.35,29.27,4.94;18,112.66,550.90,179.89,4.94">Asymmetric loss for multi-label classification</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">B</forename><surname>Baruch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,327.40,550.90,178.58,4.94;18,112.66,564.45,110.09,4.94">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="82" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,578.00,393.58,4.94;18,112.66,591.55,344.23,4.94" xml:id="b43">
	<analytic>
		<title level="a" type="main" coord="18,322.44,578.00,155.68,4.94">Focal loss for dense object detection</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,112.66,591.55,263.02,4.94">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,605.10,393.33,4.94;18,112.66,618.65,393.33,4.94;18,112.66,632.20,393.32,4.94;18,112.66,645.75,393.33,4.94;18,112.66,659.29,281.85,4.94" xml:id="b44">
	<analytic>
		<title level="a" type="main" coord="18,323.06,605.10,182.92,4.94;18,112.66,618.65,186.91,4.94">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,327.87,618.65,178.11,4.94;18,112.66,632.20,393.32,4.94;18,112.66,645.75,99.97,4.94">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="18,112.66,672.84,395.17,4.94" xml:id="b45">
	<monogr>
		<title level="m" type="main" coord="18,482.91,672.84,24.92,4.94">Zero</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
