<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,345.36,15.42;1,89.29,106.66,374.35,15.42;1,89.29,128.58,74.99,15.43">AIMultimediaLab at ImageCLEFfusion 2022: DeepFusion Methods for Ensembling in Diverse Scenarios</title>
				<funder ref="#_8DnAb4T">
					<orgName type="full">AI4Media &quot;A European Excellence Centre for Media, Society and Democracy&quot;</orgName>
				</funder>
				<funder>
					<orgName type="full">Solutions Axis</orgName>
				</funder>
				<funder ref="#_Yvhhk7j">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_GJb8Vx4">
					<orgName type="full">UEFISCDI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,156.89,121.17,11.96"><forename type="first">Mihai</forename><forename type="middle">Gabriel</forename><surname>Constantin</surname></persName>
							<email>mihai.constantin84@upb.ro</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AI Multimedia Lab</orgName>
								<orgName type="institution">University &quot;Politehnica&quot; of Bucharest</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,222.50,156.89,92.56,11.96"><forename type="first">Liviu-Daniel</forename><surname>Åtefan</surname></persName>
							<email>liviu1_daniel.stefan@upb.ro</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AI Multimedia Lab</orgName>
								<orgName type="institution">University &quot;Politehnica&quot; of Bucharest</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,327.09,156.89,69.56,11.96"><forename type="first">Mihai</forename><surname>Dogariu</surname></persName>
							<email>mihai.dogariu@upb.ro</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AI Multimedia Lab</orgName>
								<orgName type="institution">University &quot;Politehnica&quot; of Bucharest</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,426.25,156.89,75.70,11.96"><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
							<email>bogdan.ionescu@upb.ro</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AI Multimedia Lab</orgName>
								<orgName type="institution">University &quot;Politehnica&quot; of Bucharest</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,345.36,15.42;1,89.29,106.66,374.35,15.42;1,89.29,128.58,74.99,15.43">AIMultimediaLab at ImageCLEFfusion 2022: DeepFusion Methods for Ensembling in Diverse Scenarios</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">EF4BC2D925D3DC35CAB6CBBE231048EE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>DeepFusion</term>
					<term>deep ensembles</term>
					<term>ImageCLEFfusion</term>
					<term>media interestingness</term>
					<term>result diversification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ImageCLEFfusion task addresses the challenging task of creating ensembling schemes and algorithms for fusing a large set of inducers in two particular scenarios: a regression scenario where the ground truth data consists of media interestingness annotations and a search result diversification scenario. We present our team's approach for these two scenarios, consisting of a simple statistical baseline followed by the use of DeepFusion architectures and the way these architectures must be adapted for each scenario. The DeepFusion methods tested for media interestingness and result diversification are represented by deep neural networks consisting of dense, attention, convolutional, and Cross-Space-Fusion layers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>While the development of deep neural networks has greatly improved system performance for many popular tasks, such as image recognition <ref type="bibr" coords="1,325.63,404.09,11.58,10.91" target="#b0">[1]</ref>, there still are some domains where the performances of single end-to-end systems are comparatively low. This can happen in various domains, however, those related to the human perception of multimedia data represent one of the areas widely known for data that is harder to classify and predict with the help of computer vision methods. This is presented throughout the literature with examples like media interestingness <ref type="bibr" coords="1,186.76,471.84,11.28,10.91" target="#b1">[2]</ref>, violence detection <ref type="bibr" coords="1,286.10,471.84,11.28,10.91" target="#b2">[3]</ref>, and emotional content classification <ref type="bibr" coords="1,462.97,471.84,11.28,10.91" target="#b3">[4]</ref>. Many reasons are given for this trend, including the inherent subjectivity of the concepts, their multimodality, the lack of distinctive or unique features that define and influence them, and the complexity and novelty of these concepts.</p><p>Ensemble systems, or late fusion systems, are defined as systems where, given a set of endto-end systems called inducers and their decisions on the training set, an ensembling engine is trained or programmed to join the decisions of the individual inducers in order to obtain a better set of predictions. These systems represent one of the most important approaches for increasing overall system performance and have been proven useful in many domains, including those related to the human perception of multimedia data <ref type="bibr" coords="1,322.21,593.78,11.43,10.91" target="#b4">[5]</ref>.</p><p>Given these aspects, the ImageCLEFfusion 2022 task <ref type="bibr" coords="2,329.47,86.97,11.27,10.91" target="#b5">[6]</ref>, which is part of the 2022 ImageCLEF evaluation campaign <ref type="bibr" coords="2,186.30,100.52,11.56,10.91" target="#b6">[7]</ref>, proposes two different tasks related to the subjective perception of multimedia data, namely media interestingness prediction and search result diversification. Participants are given a set of pre-defined inducers and their prediction outputs and are tasked with developing and adapting ensembling methods that are able to more accurately predict the ground truth for these two subjective concepts. This paper describes the methods employed by the AI Multimedia Lab team for this task. We propose starting from a simple statistical weighted method <ref type="bibr" coords="2,126.02,181.81,12.86,10.91" target="#b7">[8]</ref> for combining the inducer predictions and then using the more novel DeepFusion approach <ref type="bibr" coords="2,133.15,195.36,11.40,10.91" target="#b8">[9]</ref>, which has previously shown very good improvements over the performances of the inducers included in the system.</p><p>The rest of this work is structured as follows. Section 2 presents the datasets used for the experiments. The proposed methods are presented in Section 3, while their results on the ImageCLEFfusion tasks are presented in Section 4. Finally, the paper closes with Section 5, where we present the main conclusions of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The ImageCLEFfusion 2022 task</head><p>ImageCLEFfusion consists of two different tasks related to the human perception of multimedia data, namely the prediction of media interestingness and search result diversification. The organizers provide a set of pre-defined and pre-computed inducers for these two tasks that correspond to end-to-end systems used for the Interestingness10k <ref type="bibr" coords="2,363.64,362.38,12.68,10.91" target="#b1">[2]</ref> dataset during the MediaEval 2017 Predicting Media Interestingness Task<ref type="foot" coords="2,279.31,374.17,3.71,7.97" target="#foot_0">1</ref> , and to the systems used for MediaEval Retrieving Diverse Social Images task<ref type="foot" coords="2,208.73,387.72,3.71,7.97" target="#foot_1">2</ref>  <ref type="bibr" coords="2,215.65,389.48,16.34,10.91" target="#b9">[10]</ref>. Table <ref type="table" coords="2,266.35,389.48,5.13,10.91" target="#tab_0">1</ref> shows the main statistics of the two proposed tasks. As the table shows, a high number of inducers is available for both tasks, giving the opportunity of testing ensembling schemes that have an integrated learning component. The performance for interestingness prediction is validated with the MAP@10 metric, while the performance for result diversification is validated with the F1@20 primary metric and with Cluster Recall at 20 (CR@20) as the secondary metric. Given the different compositions of the inducer output files associated with these two tasks, different input processing schemes will be employed, which will allow our proposed methods to function and learn in an input-agnostic fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Methods</head><p>Two methods are proposed for handling these two challenges. The first one consists of a simple approach based on statistical weighted averages, where weights are assigned according to the pre-computed performance of the inducers on the development set. Thus, this statistical approach needs no training phase but only a grid-search type of approach. The second method is represented by the DeepFusion <ref type="bibr" coords="3,246.07,165.48,12.99,10.91" target="#b8">[9]</ref> approach, where dense, attention, convolutional, and Cross-Space-Fusion layers are employed, thus creating a set of architectures that are trained on the development set, allowing the networks to learn the way inducers correlate and interact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Statistical Approach</head><p>The statistical approach is based on creating a simple scheme, where weights are assigned to each inducer based on their pre-computed performance on the development set. After finding the optimal weights on the development set, these weights are applied to the testing set, thus obtaining the final predicted scores. In theory, given a set of inducers ğ¼ = [ğ‘– 1 , ğ‘– 2 , ..., ğ‘– ğ‘› ], and their performances on the development set, arranged in descending order ğ‘ƒ = [ğ‘ 1 , ğ‘ 2 , ..., ğ‘ ğ‘› ], the rank according to their performance ğ‘… = [ğ‘Ÿ 1 , ğ‘Ÿ 2 , ..., ğ‘Ÿ ğ‘› ], a set of weights can be applied to each inducer's prediction ğ‘Š = [ğ‘¤ 1 , ğ‘¤ 2 , ..., ğ‘¤ ğ‘› ], where weight ğ‘¤ 1 is the largest, as it belongs to the best performing inducer on the development set, ğ‘¤ 2 the second largest, as so on.</p><p>In order to calculate the weights according to the performance of the inducers we used equation 1, where we perform a grid search for parameter ğœ– taking the following values 0.01, 0.05, 0.1, 0.15, 0.20, 0.25. Following this step each inducer's prediction output is multiplied by the corresponding weight, obtaining a new set of inducers defined by equation 2. Of course, due to the high value of the ğ‘’ğ‘ğ‘ ğ‘–ğ‘™ğ‘œğ‘› parameter in some cases the worst performing inducers may end up being neutralized and populated with zeroes. For such cases, those inducers are neglected from the final ensembling scheme. Finally, in order to obtain the predictions on the testing set, the weighted prediction values of the inducers that have not been zeroed by the ğœ– parameter are averaged for each sample.</p><formula xml:id="formula_0" coords="3,262.82,486.19,243.17,11.36">ğ‘¤ ğ‘› = 1 -ğ‘Ÿ ğ‘› â€¢ ğœ–<label>(1)</label></formula><formula xml:id="formula_1" coords="3,228.42,519.76,277.57,11.97">ï¸€ ğ¼ = [ğ‘– 1 â€¢ ğ‘¤ 1 , ğ‘– 2 â€¢ ğ‘¤ 2 , ..., ğ‘– ğ‘› â€¢ ğ‘¤ ğ‘› ]<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">DeepFusion</head><p>The DeepFusion approach <ref type="bibr" coords="3,207.15,569.49,12.74,10.91" target="#b8">[9]</ref> has previously shown very good results in improving the performance of the input inducers. We propose to adapt and test these architectures for the two given tasks. The general setup of the architecture is presented in Figure <ref type="figure" coords="3,376.37,596.59,3.66,10.91" target="#fig_0">1</ref>, describing the four types of layers that compose it: dense, attention, convolutional, and Cross-Space-Fusion layers. Unlike the statistical approach, no inducers are zeroed out, and all of them are taken into account. It thus falls into the network's learning tasks to reduce as much as possible the inducers that actually harm the final results. The training process uses 50 epoch for each of the four proposed architectures.</p><p>... The Dense approach show in Figure <ref type="figure" coords="4,260.50,351.88,9.86,10.91" target="#fig_0">1a</ref> consists of a series of fully connected or dense neural network layers, with the traditional role of creating a simple MLP-like architecture that can accurately classify the given testset samples, based on the learning process that is carried out on the devset. We created several variants of this approach, featuring a varying number of dense layers (namely 5, 7, 10, 12, and 15), a varying number of neurons per layer (namely 25, 50, 100, 500, 1000), and activating or deactivating the batch normalization layers for this architecture.</p><note type="other">Input</note><p>The Attention architecture shown in Figure <ref type="figure" coords="4,299.33,433.17,10.57,10.91" target="#fig_0">1b</ref> starts its training from the best performing Dense architecture and augments it with soft attention layers that have the role of controlling the inducers according to the learning that this architecture performs on the development set. The soft attention vector takes the values ğ‘ğ‘¡ğ‘¡ğ‘› ğ‘– âˆˆ [0, 1], thus having the option to vary the weights corresponding to inducers but also to completely negate them.</p><p>For the following two architectures, the Convolutional and Cross-Space-Fusion, input decoration schemes as presented in <ref type="bibr" coords="4,238.14,514.47,16.56,10.91" target="#b10">[11,</ref><ref type="bibr" coords="4,257.67,514.47,9.03,10.91" target="#b8">9]</ref> are used. These decoration schemes allow the input to the networks to gain spatial correlations by decorating the prediction of each inducer with predictions from correlated inducers and the correlation scores. The correlation scores between inducers are calculated using the provided primary metrics for the two tasks.</p><p>The Convolutional architecture, shown in Figure <ref type="figure" coords="4,327.80,568.66,9.94,10.91" target="#fig_0">1c</ref> uses the two-dimensional decorated input and applies convolutional filters to this input in order to compute and learn the way correlated inducers may interact. The output of the convolutional filters is then pooled through average pooling operation, thus allowing it to be sent to the Dense architecture previously discovered. A similar operation is carried out for the Cross-Space-Fusion approach, which uses a three-dimensional decoration scheme. This time the decorated input is fed to a series of CSF filters, each one being different for each inducer. This allows the network to learn more than just one type of correlation processing scheme, learning one for each inducer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Input Normalization</head><p>We quickly learned, in the training phase, that certain operations are needed for each individual task in order to optimize the performance of the networks and help their learning process. While for the interestingness task, this process was quite easy and only consisted of normalizing the predictions of the inducers in a [0, 1] interval, the problem was more difficult to adapt to the predictions of the diversification task. Thus, a few observations have to be made about the inducer data from the diversification task. While the data from the interestingness task has a normal spread of values, many inducers from the diversification task start with a value of 1 for the most relevant image for a query and decrease in an exponential manner to the end of the relevance list. We theorize that this causes two possible problems: (i) The values at the top of the list are very high, thus making those images hard to re-rank at lower positions in case this is needed; (ii) The images at the middle, and of course at the end of the relevance lists have scores that are almost identical, making them easy to re-rank during the learning process, even though this may not be needed. We thus decided to recreate the scores associated with the relevance lists using the following custom normalization method. Given the rank ğ‘… = [ğ‘Ÿ 0 , ğ‘Ÿ 1 , ..., ğ‘Ÿ 49 ] = [0, 1, ..., 49] predicted by a single inducer for a single query, we create a new set of scores and overwrite the initial scores according to the equation:</p><formula xml:id="formula_2" coords="5,265.04,535.01,240.94,11.36">ğ‘  ğ‘– = 1 -ğ‘Ÿ ğ‘– â€¢ ğœ”<label>(3)</label></formula><p>In this case, we tested several values for the ğœ” parameter, namely 0.005, 0.01, 0.05, 0.1. Given that in some of these cases the ğœ” parameter may cause some scores to be zeroed out or go below zero, we neutralize those particular values. We performed a series of preliminary tests in order to obtain the best value for this parameter, and in the end we obtained ğœ” = 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and discussion</head><p>Results on the testing set are presented in Table <ref type="table" coords="5,307.10,654.31,3.78,10.91" target="#tab_1">2</ref>, where they are compared with the lowest, best and average performance of the inducers on the development set, as they are given by the task organizers. These three inducer performances should be used as a general baseline for comparison, as they do not reflect the actual performances on the testset. The actual performances of the inducers on the testset are not released yet, as this data will most likely be used in future versions of this competition.</p><p>First of all it is interesting to note that, while the statistical approach generally obtains good results, surpassing both the lowest and the average inducer performances, it does not surpass the best highest inducer performance. However, better performances are achieved by the DeepFusion approaches. Thus, even the basic DeepFusion-Dense architecture outperforms the top inducer, by a very large margin in the case of Interestingness (49.55% over the top inducer) and by a smaller margin in the case of Diversification (1.09% over the top inducer). The best results are achieved by two different approaches. For Interestingness, the best performing approach uses the DeepFusion-CSF method, and attains a MAP@10 of 0.2192, representing a 49.62% increase over the top inducer. On the other hand, the best performing approach for Diversification is represented by the DeepFusion-Convolutional approach, with a final F1@20 score of 0.6216, representing a 2.03% increase over the top inducer, and a CR@20 score of 0.4916, representing a 1.92% increase over the top inducer.</p><p>The clear observation from these results is the significant gap in performance gains between the two tasks. We theorize that one of the reasons for such a gap may be represented by the data itself. Thus, for Interestingness, the results were very low to begin with, thus making performance gains much easier to achieve. Furthermore, there is a much larger comparative gap between the best and lowest performing inducers in the two tasks. This may have given the networks clearer candidates for exclusion during the learning process for Interestingness when compared with Diversification. Also, it is worth noting that the inducer predictions had to be adapted by using a custom input normalization scheme for the Diversification task. While this scheme did help the networks and increased the results, significantly improved variants of this scheme may be developed in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this study we proposed and compared a set of ensembling methods, based on simple statistical weighted approaches and on the DeepFusion deep neural network-based fusion architecture. These approaches are tested and validated on the ImageCLEFfusion 2022 task, featuring a task related to Media Interestingness and one related to Result Diversification. Results on the testing set show a major increase in performance provided by the DeepFusion architecture on the Interestingness task, while a lower increase is provided for the Diversification task. We thus demonstrated that the DeepFusion architectures can be successfully adapted to tasks that are significantly different in their scope and type of approach with regards to data representation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,302.32,416.69,8.93;4,88.93,314.32,343.63,8.87"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1:The DeepFusion architecture. We present the four main types of architectures associated with this method, namely Dense, Attention, Convolutional and Cross-Space-Fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,88.99,481.71,418.65,110.42"><head>Table 1</head><label>1</label><figDesc>Main statistics of the ImageCLEFfusion 2022 tasks. ImageCLEFfusion-int represents the Media Interestingness task, while ImageCLEFfusion-div represents the Result Diversification task.</figDesc><table coords="2,162.17,481.71,270.93,69.04"><row><cell></cell><cell cols="2">ImafeCLEFfusion-int ImageCLEFfusion-div</cell></row><row><cell>No inducers</cell><cell>29</cell><cell>56</cell></row><row><cell>Samples devset</cell><cell>1826 samples</cell><cell>104 queries</cell></row><row><cell>Samples testset</cell><cell>609 samples</cell><cell>35 queries</cell></row><row><cell>Primary metric</cell><cell>MAP@10</cell><cell>F1@20</cell></row><row><cell>Secondary metric</cell><cell>-</cell><cell>CR@20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.99,86.02,417.00,157.84"><head>Table 2</head><label>2</label><figDesc>Main statistics of the results obtained on the Interestingness and Diversification tasks. The proposed approaches are compared with the performance statistics for the development set.</figDesc><table coords="5,101.27,86.02,392.73,116.47"><row><cell></cell><cell cols="2">Media Interestingness</cell><cell cols="3">Result Diversification</cell></row><row><cell></cell><cell cols="2">Run ID -int MAP@10</cell><cell cols="3">Run ID -div F1@20 CR@20</cell></row><row><cell>devset-inducer-low</cell><cell>-</cell><cell>0.022</cell><cell>-</cell><cell>0.4262</cell><cell>0.3103</cell></row><row><cell>devset-inducer-avg</cell><cell>-</cell><cell>0.0946</cell><cell>-</cell><cell>0.5313</cell><cell>0.4140</cell></row><row><cell>devset-inducer-high</cell><cell>-</cell><cell>0.1465</cell><cell>-</cell><cell>0.6092</cell><cell>0.4823</cell></row><row><cell>Statistical</cell><cell>183903</cell><cell>0.1406</cell><cell>183898</cell><cell>0.5971</cell><cell>0.4622</cell></row><row><cell>DeepFusion-Dense</cell><cell>183908</cell><cell>0.2191</cell><cell>183902</cell><cell>0.6159</cell><cell>0.4862</cell></row><row><cell>DeepFusion-Attention</cell><cell>183915</cell><cell>0.2116</cell><cell>183901</cell><cell>0.6182</cell><cell>0.4879</cell></row><row><cell>DeepFusion-Convolutional</cell><cell>183917</cell><cell>0.2103</cell><cell>183900</cell><cell cols="2">0.6216 0.4916</cell></row><row><cell>DeepFusion-CSF</cell><cell>183921</cell><cell>0.2192</cell><cell>183899</cell><cell>0.6188</cell><cell>0.4889</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,660.08,255.77,8.97"><p>http://www.multimediaeval.org/mediaeval2017/mediainterestingness/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,108.93,671.04,229.98,8.97"><p>http://www.multimediaeval.org/mediaeval2017/diverseimages/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was funded under projects <rs type="projectName">DeepVisionRomania "Identifying People in Video Streams using Silhouette Biometric</rs>", grant <rs type="grantNumber">28SOL/2021</rs>, <rs type="funder">UEFISCDI</rs>, <rs type="funder">Solutions Axis</rs>, and <rs type="funder">AI4Media "A European Excellence Centre for Media, Society and Democracy"</rs>, grant <rs type="grantNumber">951911</rs>, <rs type="grantNumber">H2020 ICT-48-2020</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_GJb8Vx4">
					<idno type="grant-number">28SOL/2021</idno>
					<orgName type="project" subtype="full">DeepVisionRomania &quot;Identifying People in Video Streams using Silhouette Biometric</orgName>
				</org>
				<org type="funding" xml:id="_8DnAb4T">
					<idno type="grant-number">951911</idno>
				</org>
				<org type="funding" xml:id="_Yvhhk7j">
					<idno type="grant-number">H2020 ICT-48-2020</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,112.66,159.14,394.53,10.91;7,112.28,172.69,393.70,10.91;7,112.66,186.24,397.48,10.91;7,112.36,202.23,134.45,7.90" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,321.59,172.69,184.39,10.91;7,112.66,186.24,43.19,10.91">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j" coord="7,164.58,186.24,217.80,10.91">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,213.34,393.33,10.91;7,112.66,226.89,393.33,10.91;7,112.48,240.44,222.55,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,478.57,213.34,27.42,10.91;7,112.66,226.89,326.74,10.91">Visual interestingness prediction: A benchmark framework and literature review</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>Åtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">Q</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-H</forename><surname>Demarty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>SjÃ¶berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,447.87,226.89,58.11,10.91;7,112.48,240.44,123.39,10.91">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="1526" to="1550" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,253.99,394.53,10.91;7,112.28,267.54,393.71,10.91;7,112.28,281.08,125.20,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,112.28,267.54,282.27,10.91">Affect in multimedia: Benchmarking violent scenes detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-H</forename><surname>Demarty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sjoberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schedl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,406.68,267.54,99.30,10.91;7,112.28,281.08,93.28,10.91">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,294.63,393.33,10.91;7,112.66,308.18,325.64,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,417.95,294.63,88.04,10.91;7,112.66,308.18,142.89,10.91">The mediaeval 2016 emotional impact of movies task</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>DellandrÃ©a</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Baveye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">V</forename><surname>SjÃ¶berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Chamaret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,278.24,308.18,129.93,10.91">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,321.73,393.33,10.91;7,112.39,335.28,394.80,10.91;7,112.66,348.83,70.43,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,297.23,321.73,208.76,10.91;7,112.39,335.28,143.35,10.91">Exploring deep fusion ensembling for automatic visual interestingness prediction</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>Åtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,278.70,335.28,180.86,10.91">Human Perception of Visual Information</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="33" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,362.38,393.33,10.91;7,112.66,375.93,395.17,10.91;7,112.66,389.48,394.53,10.91;7,112.66,403.03,89.12,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,354.95,362.38,151.04,10.91;7,112.66,375.93,395.17,10.91;7,112.66,389.48,40.05,10.91">Overview of the ImageCLEFfusion 2022 Task -Ensembling Methods for Media Interestingness Prediction and Result Diversification</title>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>Åtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,176.95,389.48,114.96,10.91">CLEF2022 Working Notes</title>
		<title level="s" coord="7,299.73,389.48,179.67,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,416.58,395.01,10.91;7,112.66,430.13,395.17,10.91;7,112.39,443.67,394.80,10.91;7,112.66,457.22,394.62,10.91;7,112.66,470.77,393.33,10.91;7,112.66,484.32,395.17,10.91;7,112.66,497.87,393.54,10.91;7,112.66,511.42,170.14,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,352.50,457.22,154.78,10.91;7,112.66,470.77,300.10,10.91">Overview of the ImageCLEF 2022: Multimedia retrieval in medical, social media and nature applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>RÃ¼ckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>BrÃ¼ngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>SchÃ¤fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>Åtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,435.26,470.77,70.73,10.91;7,112.66,484.32,395.17,10.91;7,112.66,497.87,239.58,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<title level="s" coord="7,359.20,497.87,147.00,10.91;7,112.66,511.42,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,524.97,393.33,10.91;7,112.66,538.52,269.08,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,293.25,524.97,110.36,10.91">On combining classifiers</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hatef</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,411.49,524.97,94.49,10.91;7,112.66,538.52,185.15,10.91">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="226" to="239" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,552.07,395.17,10.91;7,112.66,565.62,394.53,10.91;7,112.66,579.17,80.57,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,302.70,552.07,205.13,10.91;7,112.66,565.62,99.40,10.91">Deepfusion: Deep ensembles for domain independent system fusion</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>Åtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,235.02,565.62,224.10,10.91">International Conference on Multimedia Modeling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="240" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,592.72,393.33,10.91;7,112.66,606.27,393.33,10.91;7,112.33,619.81,68.33,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,411.51,592.72,94.48,10.91;7,112.66,606.27,223.52,10.91">Benchmarking image retrieval diversification techniques for social media</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rohm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Boteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>GÃ®nscÄƒ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,345.02,606.27,148.18,10.91">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="677" to="691" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,633.36,394.62,10.91;7,112.66,646.91,395.01,10.91;7,112.66,660.46,38.81,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,318.96,633.36,163.27,10.91">System fusion with deep ensembles</title>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>Åtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,112.66,646.91,345.12,10.91">Proceedings of the 2020 International Conference on Multimedia Retrieval</title>
		<meeting>the 2020 International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="256" to="260" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
