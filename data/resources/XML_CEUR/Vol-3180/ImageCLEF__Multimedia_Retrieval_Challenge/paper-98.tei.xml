<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,355.12,15.42">Overview of the ImageCLEF 2022 Aware Task</title>
				<funder ref="#_zFm3sCt">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,113.06,76.11,11.96"><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
							<email>adrian.popescu@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universit√© Paris-Saclay</orgName>
								<orgName type="institution" key="instit2">CEA</orgName>
								<orgName type="institution" key="instit3">LIST</orgName>
								<address>
									<postCode>F-91120</postCode>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,177.57,113.06,129.35,11.96"><forename type="first">J√©r√¥me</forename><surname>Deshayes-Chossart</surname></persName>
							<email>jerome.deshayes-chossart@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universit√© Paris-Saclay</orgName>
								<orgName type="institution" key="instit2">CEA</orgName>
								<orgName type="institution" key="instit3">LIST</orgName>
								<address>
									<postCode>F-91120</postCode>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,319.51,113.06,75.16,11.96"><forename type="first">Hugo</forename><surname>Schindler</surname></persName>
							<email>hugo.schindler@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universit√© Paris-Saclay</orgName>
								<orgName type="institution" key="instit2">CEA</orgName>
								<orgName type="institution" key="instit3">LIST</orgName>
								<address>
									<postCode>F-91120</postCode>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,425.39,113.06,76.56,11.96"><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
							<email>bogdan.ionescu@upb.ro</email>
							<affiliation key="aff1">
								<orgName type="laboratory">AI Multimedia Lab</orgName>
								<orgName type="institution">Politehnica University of Bucharest</orgName>
								<address>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,355.12,15.42">Overview of the ImageCLEF 2022 Aware Task</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">F0D283DCA7F64A3D871D8D55CB66C085</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>social networks</term>
					<term>photo sharing</term>
					<term>object detection</term>
					<term>use profile rating</term>
					<term>situation modeling</term>
					<term>ImageCLEF</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The paper presents the overview of the ImageCLEF 2022 Aware task whose final objective is to make users more aware about the consequences of posting information on social networks. This is important insofar as users are often unaware about the effects of personal data sharing. Focus is put on modeling the impact of sharing impactful real-life situations such as searching for a bank loan, an accommodation, or a job. Since photos are one of the main types of data shared online, the task is instantiated as a photographic user profile assessment. Participants receive a training and validation dataset which includes a set of photographic profiles which are manually rated for each situation. They are required to train algorithms which rate and then rank test profiles in each tested situation. The correlation between automatic and manual profile rankings is used to measure the performance of algorithms. The overview discusses the task settings, the dataset constitution process, and the approaches proposed this year.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Online social networks (OSNs) thrived on the promise to offer their users rich interactions. The personal data shared online is processed and structured into profiles. These profiles are used to personalize the content delivered to each user, and to fuel the OSNs' business activity, which basically consists of selling access to relevant user segments to interested third parties <ref type="bibr" coords="1,470.67,432.72,11.32,10.91" target="#b0">[1]</ref>. The success of this business model depends on the richness of user profiles which are available. Consequently, users are incentivized to share large amounts of data <ref type="bibr" coords="1,392.76,459.82,11.43,10.91" target="#b1">[2]</ref>.</p><p>One problem with this functioning model is the lack of control over the effects of data sharing. Users share data primarily to interact with their contacts, but these data are then potentially usable in other contexts which are unknown to them and where the interpretation of data might change compare to the original context <ref type="bibr" coords="1,294.74,514.01,11.39,10.91" target="#b2">[3]</ref>. In an early work, the authors of <ref type="bibr" coords="1,456.56,514.01,12.80,10.91" target="#b3">[4]</ref> showed that geolocation sharing can become a threat for users if available to malevolent third parties. A framework which claims to automatically detect potential insurance fraud was introduced in <ref type="bibr" coords="1,100.97,554.66,11.50,10.91" target="#b4">[5]</ref>. The sharing of the place of origin on OSNs was shown to lead to discrimination in the labor market <ref type="bibr" coords="1,149.65,568.21,11.49,10.91" target="#b5">[6]</ref>. These works indicate that sharing data which are seemingly innocuous can be detrimental for users. It is thus important to make them more aware of the impact of their sharing practices. Assuming that a range of reference user profiles were already rated in different situations, subfigure (a) presents the level of exposure of a photographic profile in different situations depending on its relative rating. This relative rating is then detailed in subfigure (b), where the user's profile is situated against the community of reference. A visual explanation of the rating is provided by showing the effect of individual photos in the current situation. Photo-level explanations about how ratings are obtained are proposed in subfigure (c). It depicts the objects which were automatically detected in the photo, gives details about its influence in each modeled situation. Importantly, a control mechanism is introduced since the user is able to remove the photo if it is considered problematic.</p><p>These effects are amplified by the use of artificial intelligence tools which extract actionable cues from raw data. For instance, photos, which represent one of the main types of data shared on OSNs, can be analyzed in order to predict their privacy status. An early example of such work was presented <ref type="bibr" coords="2,156.81,559.27,11.59,10.91" target="#b6">[7]</ref>, where the authors used hand-crafted visual features. Important progress in this task was obtained by the introductions of deep learning representations <ref type="bibr" coords="2,449.48,572.81,11.48,10.91" target="#b7">[8,</ref><ref type="bibr" coords="2,463.72,572.81,7.65,10.91" target="#b8">9]</ref>. While inferring privacy status of a photo is interesting, similar techniques can be used to automatically derive finer grained cues from images.</p><p>We built on the works discussed above to propose the Aware shared task. Its technical objective is to automatically score user photographic profiles in a series of impactful situations. Such profiles can then be used to give users feedback about the potential consequences of personal data sharing on their real lives. The long-term objective is to enable and improve awareness rising applications such as the YDSYO prototype<ref type="foot" coords="3,356.18,85.21,3.71,7.97" target="#foot_0">1</ref> , whose functioning is illustrated in Figure <ref type="figure" coords="3,133.48,100.52,3.81,10.91" target="#fig_0">1</ref>. The reminder of this paper is organized as follows: Section 2 presents the task, Section 3 the associated dataset, Section 4 discusses the evaluation methodology, Section 5 analyzes this year's results, and Section 6 discusses the current conclusions and future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task</head><p>The task aims to raise users' awareness about the effects of personal data sharing. Focus is put on photos since they potentially convey a lot of information which is usable by third parties if an appropriate analysis pipeline is deployed. In contrast with existing works which compute on the impact of single images <ref type="bibr" coords="3,235.93,226.74,11.49,10.91" target="#b7">[8,</ref><ref type="bibr" coords="3,251.06,226.74,7.52,10.91" target="#b8">9,</ref><ref type="bibr" coords="3,262.23,226.74,12.59,10.91" target="#b9">10,</ref><ref type="bibr" coords="3,278.47,226.74,7.65,10.91" target="#b6">7]</ref>, we hypothesize that sharing effects should be assessed primarily at a user profile level. This choice is made because the data sharing has a cumulative effect, and the final rating of a profile results from an assessment of the entire user profile. Since the same data can be interpreted differently depending on the context <ref type="bibr" coords="3,469.60,267.38,16.39,10.91" target="#b10">[11]</ref>, we model four real-life situations. The user is assumed to search for: (1) a bank loan, (2) a new accommodation, (3) a job in IT, and (4) a job as a waiter. The task implementation is based on three main components:</p><p>‚Ä¢ situation models -each situation is modeled as an array of visual objects which can be detected in images. Each of these objects has a situation-related rating which was obtained by crowdsourcing. ‚Ä¢ visual objects -can be automatically detected in images using object detectors, such as Faster-RCNN <ref type="bibr" coords="3,178.88,384.86,16.29,10.91" target="#b11">[12]</ref>. These detections are essential for the task since they are aggregated into user profiles. ‚Ä¢ photographic user profiles -a set of images which were shared by a user. They provide a raw representation of the user. The actual user representation includes the objects which are automatically detected in the images.</p><p>These three components are combined into a dataset which is provided to task participants. The constitution of this dataset is described in more details in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Situation Models</head><p>Given the objective of the task, actionable models of the four situations are needed. Focus is on visual objects and we propose a crowdsourcing approach to obtain these models. We start from an initial list of objects which are represented in three object detection datasets: OpenImages <ref type="bibr" coords="3,146.91,595.50,16.15,10.91" target="#b12">[13]</ref>, ImageNet LSVRC <ref type="bibr" coords="3,249.04,595.50,16.14,10.91" target="#b13">[14]</ref>, and COCO <ref type="bibr" coords="3,322.64,595.50,16.14,10.91" target="#b14">[15]</ref>. The combination of these datasets is important to obtain good coverage of object detectors. The objective is to obtain ratings which encode the influence of these objects in each modeled situation. This is done via a dedicated crowdsourcing interface which is illustrated in Figure <ref type="figure" coords="3,325.06,636.15,3.66,10.91" target="#fig_1">2</ref>. The name and a few illustrative images are presented for each object, along with the possible ratings for the the situation in which they are currently evaluated. Its rating is annotated using a 7-points Likert scale with values between -3 (strongly negative influence) to +3 (strongly positive influence). The final rating is obtained by averaging ratings per situation from 52 annotators (14 per situation) who took part in the experiment. The situation model includes the 269 visual objects for which the final rating is not null for at least one of the four situations. Inter-rater agreement, which is important for task which are prone to bias such as the one proposed here, is computed using the average deviation index (ùê¥ùê∑) <ref type="bibr" coords="4,187.08,418.54,16.20,10.91" target="#b15">[16]</ref>. The obtained ùê¥ùê∑ varies between 0.48 for ùêºùëá and 0.65 for ùëä ùê¥ùêºùëá . These values are well below ùê¥ùê∑ ‚â§ 1.2, the maximum acceptable value for a 7-points Likert scale defined in <ref type="bibr" coords="4,160.90,445.63,16.25,10.91" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Visual Objects Detection</head><p>As we mentioned, we merge three existing datasets: OpenImages <ref type="bibr" coords="4,395.73,495.36,16.41,10.91" target="#b12">[13]</ref>, ImageNet <ref type="bibr" coords="4,467.60,495.36,18.07,10.91" target="#b13">[14]</ref> and COCO <ref type="bibr" coords="4,122.11,508.91,16.41,10.91" target="#b14">[15]</ref>. Whenever an object is present in more than one dataset, we select images from each dataset in a balanced manner to reduce biases. The number of samples per object is variable across datasets, and we keep a maximum of 1,000 images per object to reduce imbalance. The resulting dataset includes 269 objects and 137,976 images. The average and standard deviation of the distribution are 513 and 305, respectively. We train object detectors by combining an Inception-ResNet-v2 <ref type="bibr" coords="4,198.81,576.66,18.04,10.91" target="#b17">[18]</ref> with atrous convolutions backbone with the well-known Faster RCNN module <ref type="bibr" coords="4,157.02,590.21,17.96,10.91" target="#b11">[12]</ref> for the detection step. More details about the detector are provided in the supplementary material of <ref type="bibr" coords="4,209.97,603.75,16.25,10.91" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Photographic User Profiles</head><p>The third core component of the dataset is the set of photographic user profiles which should be automatically rated in each situation. Photos were sampled from the YFCC dataset <ref type="bibr" coords="4,445.73,667.03,16.08,10.91" target="#b18">[19]</ref>, a dataset which includes only data shared under Creative Commons licenses. The labeling approach used here is similar to the one deployed for situation modeling. Visual profiles are evaluated using a 7-points Likert scale which goes from -3 (strongly unappealing) to +3 (strongly appealing). Each profile includes a total of 100 images which were sampled from the user's contribution to YFCC. The 2021 version of the dataset included rating collected from 9 annotators for a set of 500 users. In 2022, the dataset was enriched and it now includes 1,000 profiles. The newly includes profiles were labeled by up to 10 annotators. The annotation interface is illustrated in Figure <ref type="figure" coords="5,306.81,435.69,3.66,10.91" target="#fig_2">3</ref>. The images of each visual profile are shown on a single page, along with the possible ratings in all four situations. Participants are asked to look at all the photos and provide a global rating of the user profile in each situation. The order in which profiles were presented to annotators was randomized to avoid any ordering bias. Similar to situation modeling, the inter-rater agreement was measured in each situation using the ùê¥ùê∑ index <ref type="bibr" coords="5,180.59,503.44,16.22,10.91" target="#b15">[16]</ref>. The obtained values are below 0.9, which is within the acceptability bounds for a 7-points Likert scale (ùê¥ùê∑ ‚â§ 1.2) <ref type="bibr" coords="5,299.80,516.99,16.41,10.91" target="#b16">[17]</ref>. The disagreement is higher when rating profiles compared to object rating. This is intuitive insofar profile rating is a more complex task which involves simultaneous evaluation of a set of 100 images for each profile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Dataset Distribution</head><p>Automatic object detection (Section 3.2) is applied to photographic user profiles (Section 3.3) to create usable representations of the profiles. Profile representation are an aggregation of image-related vectors, with each vector being composed of all objects detected in the image, along with their position and detection confidence scores.</p><p>The data included in the dataset is personal, and even sensitive according to Article 9 of GDPR<ref type="foot" coords="6,119.50,85.21,3.71,7.97" target="#foot_1">2</ref> . It is important to distribute the dataset so as to minimize any deanonymization risks. Images themselves are not provided, but only their representative vectors. In addition, the user, image and visual object names are anonymized in the distributed version of the dataset. All data were shared using a JSON format to facilitate their parsing by participants. The profiles subset was split into three parts which include 600, 200 and 200 profiles intended for training, validating and testing algorithms, respectively. Training and validation data were provided along with their associated manual ratings. The dataset equally includes situation models (i.e., visual object ratings), which provide complementary information to that available in the profiles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation Methodology</head><p>The performance of the task submissions is measured by their ability to produce profile ratings which are similar to those provided by human annotators. The similarity between automatic and manual rankings is measured using the Pearson correlation coefficient, which is a normalized measure of the covariance of two variables. Its values range from -1 (inverse correlation) to 1 (perfect correlation), with 0 standing for no linear dependency between variables. Correlation is measured for each of the four situations and the submissions are evaluated using the average of individual values. The obtained scores can be analyzed using Cohen's interpretation of the Pearson correlation coefficient <ref type="bibr" coords="6,226.08,348.83,16.14,10.91" target="#b19">[20]</ref>. Correlation is considered weak for values between 0.1 and 0.3, moderate between 0.3 and 0.5 and strong above 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>Three teams submitted a total of 9 runs, with 5 submitted by SSNCSE_KS_NA_AKR_CB <ref type="bibr" coords="6,478.09,421.01,16.14,10.91" target="#b20">[21]</ref>, 2 from JBTTM <ref type="bibr" coords="6,147.99,434.55,17.78,10.91" target="#b21">[22]</ref> and 2 from ssnce-cse-JT (no working notes provided). An overview of results is presented in Table <ref type="table" coords="6,183.85,448.10,3.74,10.91" target="#tab_0">1</ref>.</p><p>JBTTM <ref type="bibr" coords="6,134.90,461.65,17.75,10.91" target="#b21">[22]</ref> tested approaches based on random forest regressors and dense neural networks (NNs). Preprocessing was the same for both approaches. It consisted in creating a stacked matrix per user which included the location and confidence scores per situation were concatenated. They compared a random forest regressor and an extra tree regressor to combine predictions from individual prediction trees. A 7-layers deep NN was tested and the authors noted that performance for dense NNs is suboptimal. They explain this finding by an insufficient amount of data which is available for training the neural net. The reported results are similar for regressor and NN approaches since PCC reaches 0.139.</p><p>SSNCSE_KS_NA_AKR_CB <ref type="bibr" coords="6,220.60,570.05,17.79,10.91" target="#b20">[21]</ref> tested random forest regressors with different preprocessing of the data, and also explored the effect of fine-grained parameter tuning. Their baseline run (179994) used user profile descriptions made of a combination of average confidence score and situation impact scores for each detected object. Focus was put on optimizing the number of estimator used by the regressor. The range from 10 to 1000 was explored and the best results were obtained with 650 estimator. The correlation score reported for this run is 0.288. The second run (182709) improved over the first by adding an object-confidence score matrix to the input. This addition is highly effective since the PCC score of this run is 0.544. The third run (182888) focused on a fine-tuning of model parameters. The team varied parameters such as bootstrapping, max number of features, max depth, max samples per leaf, number of estimator. This exploration did not lead to a performance improvement compared to the second run since PCC reached 0.542. The last run described by the team added the area of bounding boxes which delimit the object detection as a proxy for object importance. The use of the area did not prove useful since PCC dropped to 0.519. The results submitted this year show that both the machine learning methods and an appropriate preprocessing of the data are important to obtain good quality profile ratings. This is notably underlined by the experiments run by team SSNCSE_KS_NA_AKR_CB <ref type="bibr" coords="7,461.81,438.98,16.41,10.91" target="#b20">[21]</ref>, with run performance almost doubling when input data are appropriately prepared. Globally, the obtained scores show that the task is doable, since good a good correlation level is reported following Cohen's interpretation of PCC <ref type="bibr" coords="7,271.26,479.63,16.17,10.91" target="#b19">[20]</ref>. However, the correlation is still far from perfect and the task cannot be considered as solved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Participation was comparable between 2021 and 2022, and so where the best scores reported. The low participation might be explained by a combination of factors which include: the novelty of the task, its niche orientation, and the effectiveness of the communication effort made to advertise it. Of these three dimensions, the latter one can be improved by more widespread and early diffusion of the call for participation and, potentially, by adding a financial incentive in order to stimulate participation. From a technical perspective, we will try to: (1) expand the dataset with new user profiles to the dataset to make it more robust, (2) add a more recent object detection, such as EfficientDet <ref type="bibr" coords="7,257.77,646.65,16.33,10.91" target="#b22">[23]</ref>, which improves the intrinsic quality of detections, or Detic <ref type="bibr" coords="7,127.90,660.20,16.25,10.91" target="#b23">[24]</ref>, which scales-up the number of detectable objects with image-level supervision.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,385.37,416.69,8.93;2,89.29,397.38,416.70,8.87;2,89.29,409.34,416.70,8.87;2,89.29,421.29,416.70,8.87;2,89.29,433.25,416.70,8.87;2,89.29,445.20,416.70,8.87;2,89.29,457.16,416.94,8.87;2,89.29,469.11,416.69,8.87;2,89.29,481.07,402.09,8.87;2,97.63,84.19,408.36,293.76"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Illustration of the type of feedback which is enabled by the algorithms tested as part of the ImageCLEFaware task. Assuming that a range of reference user profiles were already rated in different situations, subfigure (a) presents the level of exposure of a photographic profile in different situations depending on its relative rating. This relative rating is then detailed in subfigure (b), where the user's profile is situated against the community of reference. A visual explanation of the rating is provided by showing the effect of individual photos in the current situation. Photo-level explanations about how ratings are obtained are proposed in subfigure (c). It depicts the objects which were automatically detected in the photo, gives details about its influence in each modeled situation. Importantly, a control mechanism is introduced since the user is able to remove the photo if it is considered problematic.</figDesc><graphic coords="2,97.63,84.19,408.36,293.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,299.64,355.49,8.93;4,97.63,84.19,408.35,202.88"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Interface for rating the influence of visual objects in each modeled situation.</figDesc><graphic coords="4,97.63,84.19,408.35,202.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,89.29,303.24,355.49,8.93;5,97.63,84.18,408.36,206.49"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Interface for rating the influence of visual objects in each modeled situation.</figDesc><graphic coords="5,97.63,84.18,408.36,206.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,88.98,86.02,418.10,190.77"><head>Table 1</head><label>1</label><figDesc>Aware task result measured using the Pearson Correlation Coefficient (PCC). Higher values indicate better performance. A short description of each the method is proposed whenever available. Details for SSNCSE_KS_NA_AKR_CB and JBTTM teams are available in their working notes papers (<ref type="bibr" coords="7,452.46,255.97,16.32,8.87" target="#b20">[21]</ref> and<ref type="bibr" coords="7,488.62,255.97,14.77,8.87" target="#b21">[22]</ref>, respectively).</figDesc><table coords="7,132.66,86.02,329.95,122.74"><row><cell>Team</cell><cell>Run ID</cell><cell>Method</cell><cell>PCC</cell></row><row><cell cols="2">SSNCSE_KS_NA_AKR_CB 179994</cell><cell>random forest regressor</cell><cell>0.288</cell></row><row><cell cols="4">SSNCSE_KS_NA_AKR_CB 182709 179994 + object detection matrix 0.544</cell></row><row><cell cols="2">SSNCSE_KS_NA_AKR_CB 182888</cell><cell>182709 + parameter tuning</cell><cell>0.542</cell></row><row><cell cols="2">SSNCSE_KS_NA_AKR_CB 182890</cell><cell>-</cell><cell>0.540</cell></row><row><cell cols="4">SSNCSE_KS_NA_AKR_CB 182892 182888 + object bounding boxes 0.519</cell></row><row><cell>JBTTM</cell><cell>181730</cell><cell>random forest regressors</cell><cell>0.139</cell></row><row><cell>JBTTM</cell><cell>181665</cell><cell>dense neural network</cell><cell>0.139</cell></row><row><cell>ssnce-cse-JT</cell><cell>182457</cell><cell>-</cell><cell>0.0</cell></row><row><cell>ssnce-cse-JT</cell><cell>182300</cell><cell>-</cell><cell>0.0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,108.93,671.04,63.48,8.97"><p>https://ydsyo.app</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,108.93,671.01,142.72,8.97"><p>https://gdpr-text.com/fr/read/article-9/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The ImageCLEFaware task was supported under the <rs type="programName">H2020 AI4Media "A European Excellence Centre for Media, Society and Democracy"</rs> project, contract #<rs type="grantNumber">951911</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zFm3sCt">
					<idno type="grant-number">951911</idno>
					<orgName type="program" subtype="full">H2020 AI4Media &quot;A European Excellence Centre for Media, Society and Democracy&quot;</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,183.45,393.33,10.91;8,112.66,197.00,176.31,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,274.82,183.45,110.50,10.91">Advertising on facebook</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Temple</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,397.26,183.45,108.72,10.91;8,112.66,197.00,107.60,10.91">International Journal of E-business development</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="26" to="33" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,210.55,393.61,10.91;8,112.66,224.10,393.33,10.91;8,112.66,237.65,149.87,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,243.10,210.55,263.17,10.91;8,112.66,224.10,318.45,10.91">What makes users share content on facebook? compatibility among psychological incentive, social capital focus, and content type</title>
		<author>
			<persName coords=""><forename type="first">P.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-J</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,443.06,224.10,62.92,10.91;8,112.66,237.65,76.08,10.91">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="23" to="32" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,251.20,393.32,10.91;8,112.66,264.75,395.17,10.91;8,112.66,278.30,393.61,10.91;8,112.33,291.85,394.86,10.91;8,112.66,305.40,395.01,10.91;8,112.66,318.95,155.44,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,252.07,251.20,253.91,10.91;8,112.66,264.75,126.95,10.91">Social comparison and facebook: Feedback, positivity, and opportunities for comparison</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Gant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><forename type="middle">R</forename><surname>Bernhaupt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376482</idno>
		<ptr target="https://doi.org/10.1145/3313831.3376482.doi:10.1145/3313831.3376482" />
	</analytic>
	<monogr>
		<title level="m" coord="8,142.08,291.85,296.89,10.91">CHI &apos;20: CHI Conference on Human Factors in Computing Systems</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Verweij</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Andres</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Mc-Grenere</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Cockburn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Avellino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Goguey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bj√∏n</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Zhao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Samson</surname></persName>
		</editor>
		<editor>
			<persName><surname>Kocielnik</surname></persName>
		</editor>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">April 25-30, 2020. 2020</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,332.50,393.33,10.91;8,112.66,346.05,394.04,10.91;8,112.66,359.59,261.02,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,218.76,332.50,287.23,10.91;8,112.66,346.05,54.53,10.91">Semantic computing and privacy: a case study using inferred geo-location</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.1142/S1793351X11001171</idno>
		<ptr target="https://doi.org/10.1142/S1793351X11001171.doi:10.1142/S1793351X11001171" />
	</analytic>
	<monogr>
		<title level="j" coord="8,177.34,346.05,123.78,10.91">Int. J. Semantic Computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="79" to="93" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,373.14,393.33,10.91;8,112.66,386.69,395.01,10.91;8,112.41,400.24,48.96,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,323.91,373.14,182.08,10.91;8,112.66,386.69,25.02,10.91">Investigating insurance fraud using social media</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Diaz-Granados</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Diaz-Montes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Parashar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,163.82,386.69,268.33,10.91">2015 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1344" to="1349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,413.79,393.79,10.91;8,112.66,427.34,393.98,10.91;8,112.66,440.89,38.81,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,258.79,413.79,247.66,10.91;8,112.66,427.34,141.26,10.91">Can social media lead to labor market discrimination? evidence from a field experiment</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Manant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pajak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Souli√©</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,262.55,427.34,202.48,10.91">Journal of Economics &amp; Management Strategy</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="225" to="246" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,454.44,393.33,10.91;8,112.66,467.99,393.33,10.91;8,112.28,481.54,394.91,10.91;8,112.28,495.09,395.39,10.91;8,112.41,508.64,257.80,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,328.21,454.44,177.78,10.91;8,112.66,467.99,26.59,10.91">Privacy-aware image classification and search</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zerr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Siersdorfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Demidova</surname></persName>
		</author>
		<idno type="DOI">10.1145/2348283.2348292</idno>
		<ptr target="https://doi.org/10.1145/2348283.2348292.doi:10.1145/2348283.2348292" />
	</analytic>
	<monogr>
		<title level="m" coord="8,405.70,467.99,100.29,10.91;8,112.28,481.54,391.05,10.91">The 35th International ACM SIGIR conference on research and development in Information Retrieval, SIGIR &apos;12</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Maarek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</editor>
		<meeting><address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">August 12-16, 2012. 2012</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,522.18,393.33,10.91;8,112.66,535.73,394.53,10.91;8,112.66,549.28,395.01,10.91;8,112.66,562.83,340.74,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,264.30,522.18,241.68,10.91;8,112.66,535.73,145.17,10.91">Towards a visual privacy advisor: Understanding and predicting privacy risks in images</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Orekondy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.398</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2017.398.doi:10.1109/ICCV.2017.398" />
	</analytic>
	<monogr>
		<title level="m" coord="8,280.54,535.73,226.65,10.91;8,112.66,549.28,44.30,10.91">IEEE International Conference on Computer Vision, ICCV 2017</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017">October 22-29, 2017. 2017</date>
			<biblScope unit="page" from="3706" to="3715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,576.38,393.33,10.91;8,112.66,589.93,393.33,10.91;8,112.66,603.48,394.53,10.91;8,112.66,617.03,397.48,10.91;8,112.36,633.02,121.09,7.90" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,448.44,576.38,57.55,10.91;8,112.66,589.93,155.95,10.91">Personalized privacy-aware image classification</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Spyromitros-Xioufis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kompatsiaris</surname></persName>
		</author>
		<idno type="DOI">10.1145/2911996.2912018</idno>
		<ptr target="https://doi.org/10.1145/2911996.2912018.doi:10.1145/2911996.2912018" />
	</analytic>
	<monogr>
		<title level="m" coord="8,293.41,589.93,212.58,10.91;8,112.66,603.48,210.97,10.91">Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval, ICMR &apos;16</title>
		<meeting>the 2016 ACM on International Conference on Multimedia Retrieval, ICMR &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,644.13,393.33,10.91;8,112.33,657.68,218.36,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,222.43,644.13,246.49,10.91">Image privacy prediction using deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tonge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Caragea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,482.10,644.13,23.89,10.91;8,112.33,657.68,149.64,10.91">ACM Transactions on the Web (TWEB)</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,86.97,393.33,10.91;9,112.66,100.52,114.70,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V.-K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
		<title level="m" coord="9,327.14,86.97,178.85,10.91;9,112.66,100.52,33.49,10.91">Unveiling real-life effects of online photo sharing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2898" to="2908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,114.06,393.33,10.91;9,112.66,127.61,393.33,10.91;9,112.33,141.16,393.65,10.91;9,112.66,154.71,394.53,10.91;9,112.66,168.26,108.99,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,268.81,114.06,237.18,10.91;9,112.66,127.61,108.44,10.91">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,143.15,141.16,362.83,10.91;9,112.66,154.71,202.33,10.91">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">December 7-12, 2015. 2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,181.81,394.53,10.91;9,112.66,195.36,395.17,10.91;9,112.66,208.91,393.33,10.91;9,112.33,222.46,296.49,10.91" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="9,298.01,195.36,209.82,10.91;9,112.66,208.91,293.72,10.91">The open images dataset V4: unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1811.00982" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,236.01,394.53,10.91;9,112.28,249.56,393.70,10.91;9,112.66,263.11,394.62,10.91;9,112.31,276.66,335.55,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,325.23,249.56,180.75,10.91;9,112.66,263.11,41.03,10.91">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
		<ptr target="https://doi.org/10.1007/s11263-015-0816-y.doi:10.1007/s11263-015-0816-y" />
	</analytic>
	<monogr>
		<title level="j" coord="9,166.15,263.11,191.79,10.91">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,290.20,395.17,10.91;9,112.66,303.75,393.33,10.91;9,112.33,317.30,394.85,10.91;9,112.66,330.85,393.33,10.91;9,112.66,344.40,395.01,10.91;9,112.66,357.95,193.04,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,492.35,290.20,15.48,10.91;9,112.66,303.75,180.87,10.91">Microsoft COCO: common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10602-1_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-10602-1_48.doi:10.1007/978-3-319-10602-1\_48" />
	</analytic>
	<monogr>
		<title level="m" coord="9,142.66,317.30,265.23,10.91">Computer Vision -ECCV 2014 -13th European Conference</title>
		<title level="s" coord="9,386.78,331.87,119.21,9.72;9,112.66,345.42,29.41,9.72">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">September 6-12, 2014. 2014</date>
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,371.50,393.33,10.91;9,112.66,385.05,317.91,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,307.79,371.50,198.20,10.91;9,112.66,385.05,89.95,10.91">On average deviation indices for estimating interrater agreement</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">M</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Dusig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,211.48,385.05,150.37,10.91">Organizational Research Methods</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="49" to="68" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,398.60,393.33,10.91;9,112.66,412.15,326.57,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,235.17,398.60,270.82,10.91;9,112.66,412.15,91.90,10.91">Estimating interrater agreement with the average deviation index: A user&apos;s guide</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Dunlap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,212.97,412.15,147.40,10.91">Organizational research methods</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="159" to="172" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,425.70,393.33,10.91;9,112.66,439.25,394.53,10.91;9,112.66,452.79,393.61,10.91;9,112.41,466.34,394.86,10.91;9,112.31,479.89,262.19,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,346.45,425.70,159.53,10.91;9,112.66,439.25,214.56,10.91">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<ptr target="http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14806" />
	</analytic>
	<monogr>
		<title level="m" coord="9,112.66,452.79,344.24,10.91">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Markovitch</surname></persName>
		</editor>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017">February 4-9, 2017. 2017</date>
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,493.44,394.53,10.91;9,112.34,506.99,368.17,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,112.34,506.99,216.00,10.91">YFCC100M: the new data in multimedia research</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,336.70,506.99,70.03,10.91">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,520.54,394.53,10.91;9,112.66,534.09,22.69,10.91" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="9,154.65,520.54,227.33,10.91">Statistical power analysis for the behavioral sciences</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Academic press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,547.64,393.33,10.91;9,112.66,561.19,393.32,10.91;9,112.66,574.74,394.53,10.91;9,112.66,588.29,157.21,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="9,387.51,547.64,118.47,10.91;9,112.66,561.19,393.32,10.91;9,112.66,574.74,118.07,10.91">Ssn cse at imageclefaware 2022: Contextual job search feedback score based on photographic profile using a random forest regression technique</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nunna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Rathinasapabathi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B P K</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,253.83,574.74,111.86,10.91">CLEF2022 Working Notes</title>
		<title level="s" coord="9,373.08,574.74,134.10,10.91;9,112.66,588.29,38.13,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,601.84,393.54,10.91;9,112.66,615.39,393.32,10.91;9,112.66,628.93,216.46,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="9,384.43,601.84,121.77,10.91;9,112.66,615.39,178.87,10.91">Multi regressor based user rating predictor for imageclef aware 2022</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G M</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">R A</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,314.37,615.39,110.28,10.91">CLEF2022 Working Notes</title>
		<title level="s" coord="9,431.98,615.39,74.00,10.91;9,112.66,628.93,97.38,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,642.48,394.53,10.91;9,112.66,656.03,393.33,10.91;9,112.66,669.58,394.53,10.91;10,112.66,86.97,393.86,10.91;10,112.66,100.52,397.48,10.91;10,112.36,116.51,150.76,7.90" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="9,259.76,642.48,242.97,10.91">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.01079</idno>
		<ptr target="https://openaccess.thecvf.com/content_CVPR_2020/html/Tan_EfficientDet_Scalable_and_Efficient_Object_Detection_CVPR_2020_paper.html.doi:10.1109/CVPR42600.2020.01079" />
	</analytic>
	<monogr>
		<title level="m" coord="9,154.90,656.03,351.09,10.91;9,112.66,669.58,18.52,10.91">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2020-06-13">2020. June 13-19, 2020. 2020</date>
			<biblScope unit="page" from="10778" to="10787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,127.61,393.32,10.91;10,112.66,141.16,313.18,10.91" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kr√§henb√ºhl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02605</idno>
		<title level="m" coord="10,353.51,127.61,152.47,10.91;10,112.66,141.16,130.93,10.91">Detecting twenty-thousand classes using image-level supervision</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
