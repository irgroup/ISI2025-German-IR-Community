<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.08,75.05,426.14,16.83;1,72.08,96.05,319.94,16.83">Dual Convolutional Neural Networks and Regression Modelbased Coral Reef Annotation and Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.08,115.90,74.17,12.00"><forename type="first">Rohit</forename><forename type="middle">Raj</forename><surname>Gunti</surname></persName>
							<email>rgunti@albany.edu</email>
						</author>
						<author>
							<persName coords="1,155.55,115.90,69.85,12.00"><forename type="first">Abebe</forename><surname>Rorissa</surname></persName>
							<email>arorissa@albany.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Tennessee</orgName>
								<address>
									<settlement>Knoxville</settlement>
									<region>UTK</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.08,75.05,426.14,16.83;1,72.08,96.05,319.94,16.83">Dual Convolutional Neural Networks and Regression Modelbased Coral Reef Annotation and Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">35568E209B8C63FB5160566F4DEAF89B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Classification</term>
					<term>Coral Reef</term>
					<term>Convolution Neural Network</term>
					<term>Regression</term>
					<term>Annotation and Localization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There has been an enormous interest in using deep learning to classify underwater substrate images to identify various objects like fishes, plankton, coral reefs, seagrass, submarines, and gestures of sea divers. This classification is essential for measuring water bodies' health and quality and protecting endangered species. In the previous study, we examined the effectiveness and flexibility of the CNN model using online individual coral an d grouped datasets. Our observations from the previous study showed that our model predicted the substrates correctly. However, certain miscalculations led to lower recall scores. This year, we introduced a new approach and changed the platform to build a system that will improve the scores as well as predictions in an efficient manner. For the current study, we trained our model using high-level APIs of Keras and TensorFlow python libraries and produced improved predictions and accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In recent years, we have witnessed a massive growth in the interest in processing underwater im ages. Studying of the well-being and extent of corals and animals is beneficial marine biology, the economy, and biodiversity management. It can help in analyzing the differences in species and protecting endangered species. For example, plankton have a high sensitivity to changes in surroundings and t h e environment. Hence, the study of their well-being can detect any impending clim at ic even t s su ch as pollution and global warming. They are a crucial link in the ocean food chain and connect the ocean to the atmosphere. Plankton produce more than 80% of the world's oxygen. Hence, a low level of plankton is harmful. At the same time, an excessive number of plankton leads to toxins. Therefore, the level of plankton needs to be carefully controlled. Similarly, Posidonia Oceanica lives only in clean water an d contributes to biodiversity, reduces erosion of beaches, and enhances water quality. Studying the wellbeing of coral substrates can help analyze the impact of global warming and excessive human activit y on the water bodies and marine life <ref type="bibr" coords="1,255.82,581.67,12.51,9.72" target="#b0">[1]</ref> <ref type="bibr" coords="1,268.33,581.67,16.68,9.72" target="#b9">[10]</ref>, thus guiding preservation campaigns. Image processing can complement other techniques such as physiochemical analysis of water and sonar-based detection in achieving those goals. Therefore, the main objectives of the current study are: 1  • To test the Neural Network logic on the TensorFlow platform using Python programming and compare the results with those of our previous study <ref type="bibr" coords="2,377.77,94.70,12.60,9.72" target="#b1">[2]</ref> which were produced using MATLAB.</p><p>• To improve the performance, efficiency, and effectiveness of our predictions.</p><p>• To increase the accuracy of the Classification results.</p><p>• To effectively recognize plant images by using deep learning algorithms that improve the accuracy, kappa coefficient, and Jaccard coefficient values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Approaches</head><p>In a previous study <ref type="bibr" coords="2,177.18,285.65,11.49,9.72" target="#b1">[2]</ref>, we implemented the Convolutional Neural Network deep learnin g algorit h m <ref type="bibr" coords="2,76.88,300.04,25.22,9.72">[3][4]</ref> through MATLAB and annotated the images uncategorized. Though the labeling part was done manual, the predictions and accuracy were impressive. However, unsupervised learning (the approach we took in the current study) promises to produce much-improved results compared to last year's sem isupervised approach <ref type="bibr" coords="2,166.96,343.89,14.41,9.72" target="#b1">[2]</ref> which had a number of downsides, including the inability to handle a large volume of data, incorrect classification results, and lack of adequate GPU memory <ref type="bibr" coords="2,442.63,358.29,11.49,9.72" target="#b4">[5]</ref>.</p><p>On the other hand, this year, we followed two different models i.e., CNN Model <ref type="bibr" coords="2,449.85,387.72,25.20,9.72">[3][4]</ref> for predict in g and Regression Model <ref type="bibr" coords="2,181.22,402.12,12.59,9.72" target="#b5">[6]</ref> for bounding boxes. The former model is rather a more straightforward categorical approach for Predicting, while the latter one is the complicated object detecting approach. Initially, we started training the images with ten coral substrates with ten categories. We assumed that instead of training just ten categories with hundreds of images, as most of them are online, we could t rain each image in ImageClef 2022 training dataset <ref type="bibr" coords="2,277.76,460.37,13.25,9.72" target="#b0">[1]</ref>[10] with a label associated with a number of subst rat es present in that image and categorize each label with the provided annotations in "imageCLEFcoral2022_annotations_boxes_training" CSV file <ref type="bibr" coords="2,359.75,489.19,12.50,9.72" target="#b0">[1]</ref> <ref type="bibr" coords="2,372.25,489.19,16.67,9.72" target="#b9">[10]</ref>. As there were more than two hundred combinations of coral classification in the annotations file, each image is in a respective directory with the manually created label of its respective annotation. Therefore, we categorized 228 distinct categories with the training images. For example, for the image ID "2018_0712_073450_057" there are nine different substrates annotated in the imageCLEFcoral2022_annotations_boxes_training CSV file. Hence, a directory with a label "Hard_Coral_Branching_and_Hard_Coral_Submassive_and_Hard_Coral_Boulder_and_Hard_Coral_ Foliose_and_Hard_Coral_Mushroom_and_Soft_Coral_and_Soft_Coral_Gorgonian_and_Sponge_and_Al gae_Macro_or_Leaves" is created with a training image '2018_0712_073450_057.'</p><p>Similarly, the images in the Clef Coral 2022 training dataset were divided into 228 labels with each consisting of training images ranging from 1 to 98. The highest directory with the ninety-eight training images is categorized with the label in the combination of four substrates "Hard_Coral_Branching_and_Hard_Coral_Boulder_and_Soft_Coral_and_Sponge."</p><p>Since there are 228 categories of labels created manually with a significantly less number of train in g images in most of the directories, every image in the labeled directory required copying several tim es u n t il it matched the total number of categories i.e., 228. For example, the training image 2018_0712_073450_057 which is labeled as "Hard_Coral_Branching_and_Hard_Coral_Submassive_and_Hard_Coral_Boulder_and_Hard_Coral_Folio se_and_Hard_Coral_Mushroom_and_Soft_Coral_and_Soft_Coral_Gorgonian_and_Sponge_and_Algae_ Macro_or_Leaves" is replicated 228 times to match the total number of labels created.</p><p>Next, to train it on the TensorFlow platform using high-level neural network libraries from Keras, the proposed model is introduced to overcome all the disadvantages that arose in the existing system. The proposed model is hoped to increase the accuracy of the results by classifying and predicting the precise bounding cox coordinates. It could enhance the performance and reliability of the overall classification results, identify underwater corals, and improve accuracy. The only downside to the proposed model is that the computational speed for training the images is slow for large datasets. However, we could min im ize the number of samples for batch training for each epoch (one training iteration).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Dataset</head><p>There are three datasets used in this study. <ref type="bibr" coords="3,388.53,288.05,13.15,9.72" target="#b0">[1]</ref>[10] 2. ImageClef Coral Annotation and Localization Testing Dataset <ref type="bibr" coords="3,383.73,302.44,13.15,9.72" target="#b0">[1]</ref>[10] 3. Online coral substrate images from Google Initially, the system was trained on online Images considering 11 substrates for training. The CNN model was tested for 11 substrates labeled as each category containing a minimum of eleven images and a maximum of hundred training images. To calculate the CNN model efficiency the convention al way was trained and tested. However, after the ImageClef training dataset was made available for download, the image dataset was analyzed to design the CNN Model and regression model. After a thorough analysis, the 228 distinct categories of substrate combinations were spotted in the training dataset. Therefore, the dataset was created for each category with a given number of training images. In order to train the model, the syst em requires a minimum of 228 images in each category. Since to test the accuracy of the Model first the images have been replicated in every category of the dataset and used for training purposes. For training the model, we have created 228 labels with several replicates equivalent to the test data set created. Out of 228, we detected that only 96 categories are required to train the model for the predictions. Hence, the training dataset was eventually trained in 96 categories for predictions and confidence scores for 200 CoralClef Test dataset images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">ImageClef Coral Annotation and Localization Training Dataset</head><p>We saved and divided all the predictions into twenty-one unique predictions together with respective test image IDs. We divided the 96 categories into twenty-one datasets for bounding boxes and coordinates and removed the replications inside each category. This means that each category was trained simultaneously with respective annotations provided in the training dataset. Finally, the classified test dataset was used to calculate the Bounding boxes for each image ID in all twenty-one folder.        Image Data pre-processing is the process of getting rescale data from the dataset. It involves obtaining the data and resizing images in the dataset that involves rescaling the size of the remote sensing scene dataset images to 50. Categorical data is defined as variables with a finite set of rescaled values. That most deep learning algorithms require an array of input and output variables. In the current study, resizing was don e mainly for the initial phase, where we trained individual images with more than fifty images for each substrate. Hence there were different datasets involved before downloading the actual Coral 2022 dataset .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>The resizing for every image with different shapes and dimensions is monitored explicitly as shown in Figure <ref type="figure" coords="9,103.27,147.54,4.07,9.72" target="#fig_7">9</ref>.</p><p>Data splitting is the act of partitioning available data into two portions, usually for cross-validat e pu rposes. A portion of the data is used for developing a predictive model while the other portion is used to evaluate the model's performance. Separating image data into training and testing sets is also an import an t part of evaluating image processing models. Typically, when one separates a data set into a training set and testing set, most of the image data is used for training, and a smaller portion of the data is used for testing.</p><p>When training most of the substrates individually, the training is set to 80 percent of t h e dat aset an d t h e labels of each substrate. The remaining 20 percent is validated for testing as demonstrated in Figure <ref type="figure" coords="9,527.17,267.02,9.00,9.72" target="#fig_1">10</ref>.</p><p>As each substrate contained images in the range from fifty to one hundred fifty, our results were very encouraging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 10:</head><p>The validation for the ImageClef dataset was divided into eighty percent for train in g an d twenty percent for testing. All the categories were split into four datasets depending on different locations from Indonesia, Seychelles, and the Caribbean. The model was programmed to t rain wit h an equal or a greater number of images in each folder with a total number of labels. Since there are 228 combinations of annotations from all locations in the training dataset, we have six locations partitioned training process into six rounds where we trained N categorical folders with each containing more than or equal to N image samples. However, the test set is from a single location K1, and we primarily trained 96 categorical folders with each containing more than or equal to 96 samples. Since some folders have very few sample images, we duplicated the samples to make the total sample images in the folder add up to at least 96. The above figure displays the validation set when classifying the ten substrates initially. The right side shows the Boolean values trainY array t h at stores the true labels wherever the predicted labels, for all 96 categories, match the true labels and False everywhere else. On the left, we have the number of correct predicted labels for each iteration through which we generate the classification report and calculate the Cohen Kappa Coefficient, Jaccard similarity Coefficient, and confusion matrix</p><p>In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks most commonly applied to analysing visual imagery <ref type="bibr" coords="10,350.67,97.70,27.60,9.72">[3][4]</ref>. They are used in image and video recognition applications, recommender systems, image classification, medical image analysis, natural language processing, brain-computer interfaces, and financial time series <ref type="bibr" coords="10,410.43,126.52,24.26,9.72">[3][4]</ref>. CNNs are regularized versions of multilayer perceptron. Multilayer perceptron is usually a fully connected network, that is, each neuron in one layer is connected to all neurons in the next layer <ref type="bibr" coords="10,360.95,155.95,23.93,9.72">[3][4]</ref>. The "full connectedness" of these networks makes them prone to overfitting data. Typical ways of regularization include addin g som e form of magnitude measurement of weights to the loss function. CNNs take a different approach toward regularization: they take advantage of the hierarchical pattern in data and assemble more complex patterns using smaller and simpler patterns. Therefore, on the scale of connectedness and complexit y, CNN's fall on the lower scale. Convolutional networks were inspired by biological processes in that the connect ivit y pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neuron s respond to stimuli only in a restricted visual field region known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.</p><p>Prediction is a process of recognizing the substrate from the dataset by using a deep learning model. In ou r project, we attempted to effectively predict the classification of images from the dataset by enhancin g t h e performance of the overall prediction system. To do that, we programmed two models-one for prediction and another for bounding box visualization. In the prediction model, out of 200 test dat a im ages, the prediction model used a batch size of 32 samples with 25 epochs (iterations). This model was compiled using a categorical cross-entropy loss evaluator and Adam optimizer and trained with 96 pre-processed class labels with NumPy categorical to convert the array of labeled data to vector as shown in figure <ref type="figure" coords="10,515.13,397.92,9.00,9.72" target="#fig_8">11</ref>. After testing with 200 image samples in the Clef Coral dataset, the predictions were classified into twenty-one unique labels as shown in figure <ref type="figure" coords="11,268.92,97.70,9.10,9.72" target="#fig_9">12</ref>. For each label, a new bounding box regression model <ref type="bibr" coords="11,526.30,97.70,14.21,9.72" target="#b5">[6]</ref> was compiled using an mse loss evaluator and Adagrad optimizer and trained with VGG16 network ensuring the head FC layers. All the predicted values were then scaled on image dimensions to draw the bounding boxes. However, there has been an overlap between the substrates for some images and some images predicted only single substrates. We assigned eleven different colors for eleven substrates. We have encountered some images with a couple of mismatchings as shown in figure <ref type="figure" coords="11,451.52,431.54,9.09,9.72" target="#fig_10">13</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head><p>We generated the Final Result <ref type="bibr" coords="12,205.22,107.32,18.78,9.72" target="#b9">[10]</ref> based on overall classification and prediction. The performance of this proposed approach is evaluated using some measures such as accuracy, precision, recall, F1measure, Kappa Coefficient, and Jaccard Coefficient <ref type="bibr" coords="12,300.83,132.52,13.25,9.72" target="#b6">[7]</ref>[8] as shown in figure <ref type="figure" coords="12,413.67,132.52,10.80,9.72" target="#fig_12">14</ref>    Our initial plan was to submit as many runs as possible, considering the previous run scores and observations. Therefore, each unique prediction was trained again for the boxing visualization with the training dataset and annotations with the same category taken from ninety-six image categories of the training dataset and training annotation file. We used the callback function for every iteration to get the maximum number of predictions with a difference in accuracy to get the most accurate predict ion <ref type="bibr" coords="13,506.13,399.72,13.57,9.72" target="#b7">[ 8]</ref>. As saving results of the predictions in the developed Model from the callbacks require high GPU memory <ref type="bibr" coords="13,518.71,414.15,16.14,9.72" target="#b4">[5]</ref>, the bounding box model was considered training on lower batch levels and higher epochs(iterations). This model was developed to take input image shapes to resize and reshape them into four-dim en sion al 224 x 224 image sizes and then gives the most desirable predictions from the VGG16 trained model. As mentioned earlier, the dataset itself trained in this model belongs to the similar category of the input image that predicts most predictions from the trained model to improve the recall and precision. To calculate t h e coordinates, the NumPy array value in test data is multiplied by the exact dimensions of t h e in pu t im age for each saved NumPy array prediction and visualized the coordinates onto the input image.</p><p>To submit the runs, the task was divided into two parts. One for the prediction and confidence score usin g the prediction Model followed by the bounding cox coordinates using the bounding box model. The precision and recall were tested in the first run by training the system without the prediction callbacks and fewer iterations with 32 batch sizes for every unique dataset. In the second run, the system was developed, and the boxing regression model was retrained with more iterations, a batch size equal to the total number of training dataset images, and iterations equal to t he total number of trained annotations. Hence, the observations from the first run gave a lead in improving the model, and the second ru n resu lt s were improved. Though we planned to retrain the system to submit more runs to increase t h e scores, we decided to postpone it for future campaigns. Though we wanted to retrain both models with more accurate results, we only managed to train Bounding Box Regression Model <ref type="bibr" coords="13,365.29,676.55,15.48,9.72" target="#b5">[6]</ref> for the next run. However, one can see the difference in the scores.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Concluding Remarks</head><p>In this study, the deep learning classifier analyzed the different types of coral substrates. Two models were developed for different purposes, and the coral images were taken as input data and applied to the preprocessing method. The images were resized and converted into an array in the pre-processing method. Then it was processed into the feature selection method; in this method, the dataset is split into a t rain in g dataset and a testing dataset. However, the partitioning of data was prioritized differently in both Models. Since the first model was developed for predictions and confidence scores, it was partitioned into categorical labels. In contrast, the second model was developed to calculate the bounding box coordin at es and visualize them on the input test image. All the images are resized and converted into an array in bot h models. Finally, the first model that demonstrates the deep learning algorithm of CNN was im plem en t ed and predicted the result based on accuracy, precision, recall f1-measure, kappa coefficient, and Jaccard coefficient <ref type="bibr" coords="15,123.04,240.02,11.48,9.72" target="#b6">[7]</ref>. In contrast, the second model was the bounding box regression model, which visualizes and enables predict continuous values.</p><p>Our findings indicate that it will be possible to improve prediction accuracy as we increase the nu m ber of iterations and submit the annotations in the required format. Since all the configuration ch an ges are on ly made on the Bounding Box Regression Model <ref type="bibr" coords="15,271.14,306.07,15.29,9.72" target="#b5">[6]</ref> for the second run, we see the improvement. We strongly believe that retraining both models for this study would improve the Accuracy and Precision as we m igh t see the difference in prediction levels. We utilized and built an unsupervised system that can be used in any field of science. The unsupervised training algorithm trains the images to detect the sensin g scen e. It will also enhance Graphical User Interface to find the underwater coral type from the image. This approach of training will provide valid predictions, and it will enhance the performance and increase t h e accu racy, and visualizes multiple annotations <ref type="bibr" coords="15,225.97,393.72,13.49,9.72" target="#b8">[9]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,243.85,256.03,70.54,9.18;4,240.85,338.30,75.86,9.18;4,394.60,281.26,80.04,9.18;4,410.80,224.20,46.40,9.18;4,404.20,167.75,59.59,9.18;4,245.05,148.56,67.38,9.18;4,262.45,160.56,33.95,9.18;4,72.08,79.70,275.03,9.72;4,90.10,100.09,467.80,9.72;4,90.10,113.22,478.52,9.83;4,90.10,125.92,243.44,9.72"><head></head><label></label><figDesc>ARCHITECTURE AND FLOW DIAGRAMAs the model is built on Python programming the System architecture, flow diagram, and several forms of UML diagrams are a better way to translate the code. The below diagrams, figures 1 -7, help to analyze and fathom the structure and communication of the system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,57.05,411.63,517.99,10.80;4,57.05,425.55,229.46,9.72"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The above architecture of the system describes the five modules of the Convolutional Neural Network Model to classify an image into different substrates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,57.05,711.97,517.03,9.72;4,57.05,724.57,516.86,9.72;4,57.05,737.20,176.84,9.72"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: The above Flow Diagram of the CNN Model demonstrates the details of the process in each portrayed in Figure 1. Module One Select and View dataset and Module two Data preprocess are further divided into submodu les as illustrated in the above flow diagram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,57.05,479.60,513.12,9.72"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The above ER Diagram visualizes the data model design to demonstrate the requirements of each module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,21.03,759.88,551.44,10.80"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The above Class Diagram defines the structure of the model by defining the Module's classes and their operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,57.05,630.29,514.57,9.72;6,57.05,649.52,513.52,9.72;6,57.05,668.75,518.27,9.72;6,57.05,687.95,469.02,9.72;6,57.05,706.57,63.85,9.72"><head>Figure 6 :Figure 7 : 3 .</head><label>673</label><figDesc>Figure 6: The above Sequence diagram describes the model design. The relationships are shown in figure 3 between the system and its modules, and the communication &amp; interaction between the system and its modules are detailed in the above sequence diagram. All the interactions are depicted in chronological order. This diagram represents a better version of the architecture of the CNN Model (figure 1) and how different modules are interconnected and communicate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="8,90.10,410.42,287.04,10.80;8,113.35,239.95,223.15,144.55"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: A sample prediction based on an online Google dataset</figDesc><graphic coords="8,113.35,239.95,223.15,144.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="8,90.10,651.80,449.86,10.80;8,90.10,667.43,13.22,10.80;8,337.10,509.90,171.40,126.50"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Unique categorical labels on the left and a sample image from the Clef Coral 2022 training set</figDesc><graphic coords="8,337.10,509.90,171.40,126.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="10,72.08,658.40,373.02,10.80;10,72.00,433.90,271.55,201.55"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Convolutional Neural Network Model for Clef Coral training Dataset 2022</figDesc><graphic coords="10,72.00,433.90,271.55,201.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="11,72.08,395.40,502.82,10.80;11,72.08,409.34,316.93,9.72;11,72.00,190.00,477.10,191.20"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: A sample of 200 test dataset prediction categories/labels classified into twenty-one unique categories. The number of images per category is demonstrated in the above figure.</figDesc><graphic coords="11,72.00,190.00,477.10,191.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="11,72.08,670.43,466.62,10.80;11,76.80,442.92,216.00,213.60"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Measuring errors and overlapping in Bounding Box Visualization for two samples in test dataset</figDesc><graphic coords="11,76.80,442.92,216.00,213.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="12,427.47,132.52,61.70,9.72"><head></head><label></label><figDesc>and figure 15.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="12,72.08,661.40,502.67,10.80;12,72.08,675.95,502.14,9.72;12,72.08,688.55,501.12,9.72;12,72.08,701.17,113.20,9.72;12,75.00,416.75,485.40,224.85"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Above figure demonstrates the Accuracy (on left), precision(on right), recall(on right), f1-score(on right), and confusion matrix(on bottom). The confusion matrix portrayed above is for the initial dataset downloaded online. It portrays the number of true labels for each coral substrate throughout th e t rain ing dat aset and their predicted labels.</figDesc><graphic coords="12,75.00,416.75,485.40,224.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="13,72.08,294.52,439.00,10.80;13,290.85,151.17,200.80,128.00"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Performance Measures for ImageClef Coral Annotation and Localization training dataset</figDesc><graphic coords="13,290.85,151.17,200.80,128.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14" coords="14,72.08,74.90,193.97,9.72;14,72.08,97.60,321.82,9.83;14,72.08,120.52,353.34,9.72;14,72.08,143.24,304.40,9.83;14,72.08,164.95,359.66,9.72;14,72.08,188.37,208.97,9.72;14,72.08,202.77,467.80,9.72;14,72.08,217.79,467.50,9.72;14,72.08,232.20,468.44,9.72;14,72.08,246.62,468.45,9.72;14,72.08,261.02,84.84,9.72;14,72.08,337.89,467.11,9.72;14,72.08,351.69,317.01,9.72;14,72.08,366.70,58.21,9.72;14,72.08,381.12,292.43,9.72;14,72.08,469.37,278.52,9.72;14,72.08,534.70,456.14,10.80"><head></head><label></label><figDesc>Primary run: Recall: 0.001; Precision: 0.001 Soft Coral -200 test dataset images with an average of 2000 annotations Hard Coral Boulder-197 test dataset images with an average of 985 annotations Sponge -187 test dataset images with an average of 561 annotations Hard Coral Branching -186 test dataset images with an average of 558 annotation Secondary Run: Recall: 0.002; Precision: 0.003 Even though there is an increase for the second run, the score was not satisfying. So, to verify and validat e the score the submitted predictions were converted into XML file to visualize the coordinates on the images. We see a huge overlapping of images when we visualized the annotations as shown in figure 16. The actual reason is because the displacement of (Xmin1, Ymin1) coordinates with (width, height) in the submission format. Above is the actual submission formation that should be for ClefCoral 2022. However, all our coordinates [Xmin1,1] and [ymin1,1] are misplaced with [width1,1] and [height1,1] For example: Our actual result for a test dataset sample while running on Spyder Below are our coordinates misplaced in the submission text file When we visualized the correct coordinates by converting them into an XML file below are the results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15" coords="14,72.08,725.17,467.64,9.72;14,72.08,739.60,114.71,9.72;14,72.00,548.10,414.00,149.05"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: left image displays the actual annotations, while the right image displays the submitted annotations in the text file</figDesc><graphic coords="14,72.00,548.10,414.00,149.05" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="15,118.93,437.57,447.39,9.72;15,118.93,450.07,415.58,9.82;15,118.93,462.77,443.74,9.72;15,118.93,475.39,173.08,9.72" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="15,200.34,450.07,324.58,9.82">Overview of the ImageCLEFcoral 2022 Annotation and Localisation Task</title>
		<author>
			<persName coords=""><forename type="first">Jon</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jessica</forename><forename type="middle">P</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis</forename><forename type="middle">G</forename><surname>Clift</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Seco</forename><surname>De Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,118.93,462.77,388.30,9.72">Proceedings of the 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<meeting>the 13th International Conference of the CLEF Association (CLEF 2022)<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-09">September 2022</date>
		</imprint>
	</monogr>
	<note>CEUR-WS.org</note>
</biblStruct>

<biblStruct coords="15,118.93,495.79,455.65,9.72;15,118.93,509.02,302.97,9.72" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,232.83,495.79,341.76,9.72;15,118.93,509.02,50.98,9.72">A convolutional based neural networks based coral reef annotation and localization</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Gunti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rorissa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,207.78,509.02,131.79,9.72">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2021">2021. 2936</date>
			<biblScope unit="page" from="1229" to="1238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,118.93,529.42,454.57,9.72;15,118.93,542.04,455.86,9.72;15,118.93,554.65,456.42,9.72;15,118.93,567.24,217.15,9.72" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,156.10,542.04,418.68,9.72;15,118.93,554.65,207.79,9.72">Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Nogues</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mollura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Su M M Ers</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2016.2528162</idno>
		<ptr target="https://doi.org/10.1109/TMI.2016.2528162" />
	</analytic>
	<monogr>
		<title level="j" coord="15,335.97,554.65,173.34,9.72">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1285" to="1298" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,118.93,587.57,456.13,9.83;15,118.93,600.87,455.02,9.72;15,118.93,613.49,31.52,9.72" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,344.13,587.67,230.93,9.72;15,118.93,600.87,59.47,9.72">Multi-column Deep Neural Networks for Image Classification</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,202.86,600.87,371.09,9.72">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,118.93,633.90,456.28,9.72;15,118.93,646.52,456.35,9.72;15,118.93,659.12,90.37,9.72" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,308.58,633.90,266.63,9.72;15,118.93,646.52,75.37,9.72">Performance and Scalability of GPU-Based Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Strigl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kofler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Podlipnig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,209.48,646.52,365.80,9.72">Euromicro Conference on Parallel, Distributed, and Network-Based Processing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="317" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,118.93,679.54,455.13,9.72;15,118.93,692.75,456.45,9.72;15,118.93,705.37,160.88,9.72" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rosebrock</surname></persName>
		</author>
		<ptr target="https://pyimagesearch.com/2020/10/05/object-detection-bounding-box-regression-with-keras-tensorflow-and-deep-learning/" />
		<title level="m" coord="15,184.90,679.54,389.17,9.72">Object detection: Bounding box regression with Keras, TensorFlow, and Deep Learnin g</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,118.93,725.77,456.21,9.72;15,118.93,738.40,456.45,9.72;15,118.93,751.00,291.84,9.72" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Brownlee</surname></persName>
		</author>
		<ptr target="https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/" />
		<title level="m" coord="15,177.18,725.77,397.96,9.72;15,118.93,738.40,170.45,9.72">Machine Learning Mystery: How to Calculate Precision, Recall, F1, and More for Deep Learning Models?, Deep Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,118.93,71.87,455.72,9.72;16,118.93,84.49,56.35,9.72;16,198.71,84.49,12.59,9.72;16,234.73,84.49,20.40,9.72;16,278.54,84.49,29.86,9.72;16,332.11,84.49,37.79,9.72;16,393.32,84.49,20.98,9.72;16,437.72,84.49,34.18,9.72;16,495.33,84.49,31.70,9.72;16,550.75,84.49,24.61,9.72;16,118.93,97.09,401.72,9.72" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="16,168.70,71.87,405.96,9.72;16,118.93,84.49,56.35,9.72;16,198.71,84.49,12.59,9.72;16,234.73,84.49,20.40,9.72;16,278.54,84.49,25.59,9.72">Custom Callback for transformers: How to quantify F1-Score when fine-tuning pre-t rain ed transformers for NLP tasks?</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hossini</surname></persName>
		</author>
		<ptr target="https://towardsdatascience.com/custom-callback-functions-for-transformers-ae65e30c094f" />
	</analytic>
	<monogr>
		<title level="j" coord="16,332.11,84.49,37.79,9.72;16,393.32,84.49,20.98,9.72;16,437.72,84.49,34.18,9.72">Towards Data Science</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,118.93,117.42,455.99,9.83;16,118.93,130.72,427.90,9.72" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="16,227.06,117.52,347.86,9.72;16,118.93,130.72,59.47,9.72">Flexible, High-Performance Convolutional Neural Networks for Image Classification</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cireşan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,200.48,130.72,245.85,9.72">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1237" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,129.13,151.15,445.61,9.72;16,118.93,163.74,456.08,9.72;16,118.93,176.27,456.27,9.83;16,118.93,188.97,454.94,9.72;16,118.93,201.57,455.71,9.72;16,118.93,214.20,455.72,9.72;16,118.93,226.79,455.96,9.72;16,118.93,239.42,455.38,9.72;16,118.93,252.62,119.80,9.72" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,349.77,201.57,224.87,9.72;16,118.93,214.20,308.19,9.72">Overview of the ImageCLEF 2022: Multimedia Retrieval in Medical, Social Media and Nature Applications</title>
		<author>
			<persName coords=""><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renaud</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louise</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raphael</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmad</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Serge</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yashin</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassili</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liviu -Daniel</forename><surname>Ștefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Gabriel Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihai</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jérôme</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Schin Dler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jon</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,458.91,214.20,115.73,9.72;16,118.93,226.79,455.96,9.72;16,118.93,239.42,135.52,9.72">Proceedings of the 13th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="16,291.02,239.42,233.48,9.72">Springer Lecture Notes in Computer Science LNCS</title>
		<meeting>the 13th International Conference of the CLEF Association (CLEF<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-09-05">2022. September 5-8, 2022</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
