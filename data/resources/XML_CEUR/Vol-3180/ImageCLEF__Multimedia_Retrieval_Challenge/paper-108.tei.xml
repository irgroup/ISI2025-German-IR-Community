<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,345.81,15.42">Monitoring Coral Reefs Using Faster R-CNN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,113.06,56.43,11.96"><forename type="first">Felix</forename><surname>Kerlin</surname></persName>
							<email>kerlin@hhu.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Heinrich-Heine-Universit√§t D√ºsseldorf (Germany)</orgName>
								<address>
									<addrLine>Universit√§tsstra√üe 1</addrLine>
									<postCode>40225</postCode>
									<settlement>D√ºsseldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,158.36,113.06,84.36,11.96"><forename type="first">Kirill</forename><surname>Bogomasov</surname></persName>
							<email>bogomasov@hhu.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Heinrich-Heine-Universit√§t D√ºsseldorf (Germany)</orgName>
								<address>
									<addrLine>Universit√§tsstra√üe 1</addrLine>
									<postCode>40225</postCode>
									<settlement>D√ºsseldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.72,113.06,69.75,11.96"><forename type="first">Stefan</forename><surname>Conrad</surname></persName>
							<email>stefan.conrad@hhu.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Heinrich-Heine-Universit√§t D√ºsseldorf (Germany)</orgName>
								<address>
									<addrLine>Universit√§tsstra√üe 1</addrLine>
									<postCode>40225</postCode>
									<settlement>D√ºsseldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,345.81,15.42">Monitoring Coral Reefs Using Faster R-CNN</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">F0BDDC4A634A5B42E470C94655EAD8E5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Computer Vision</term>
					<term>Object Detection</term>
					<term>Neural Networks</term>
					<term>Coral Reefs Detection</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Monitoring coral reefs is an important procedure to protect the persistence of many marine species. The imageCLEFcoral 2022 Challenge aims to identify and annotate corals on underwarter images. These images vary in terms of quality and are therefore of a high complexity. While our investigation, we focused on the data set and searched for ways to improve the image quality. To be specific, we minimized the impact of color casts, and erratic annotations by a color balancing strategy, as well as combining the prediction results of the trained deep learning architectures on preprocessed and original images. Object detection was handled by deep learning entirely. In particular, faster R-CNN with a ResNet+FPN backbone network was the architecture of the choice. The merging strategy is based by a Non-maximum Suppression (NMS) and reduces therefore overlapping predictions. Additionally, we analyzed the impact of the depth of the chosen backbone network. We have identified a connection between increasing network depth and increasing accuracy for underwater imaging. Overall, our best approach achieved a MAP0.5 value of 0.396.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The CLEF Initiative's <ref type="bibr" coords="1,189.20,401.50,13.00,10.91" target="#b0">[1]</ref> imageCLEFcoral 2022 Challenge <ref type="bibr" coords="1,356.52,401.50,13.00,10.91" target="#b1">[2]</ref> addresses the issue of the destruction of coral reefs due to climate change and human activities. The reefs and the entire surrounding ecosystems are threatened with extinction within the next 30 years <ref type="bibr" coords="1,464.80,428.60,11.58,10.91" target="#b2">[3]</ref>. This would lead not only to the end of many marine species, but also to a humanitarian crisis of global proportions, as many regions depend on coral reefs. A quick change in the near future is essential. By this reason, an invention is indispensable. An appropriate intervention, in terms of environmental protection, can only take place if it is known which steps need to be taken. These are depending on the current state of the coral landscape, i.e. coral distribution, stocks and many more. For this reason, the entire area of coral reefs needs to be analyzed and regularly monitored subsequently. Manual monitoring by experts such as marine biologists is expensive and not feasible at all, keeping in mind the total area of 255 000 ùëòùëö 2 covered by corals <ref type="bibr" coords="1,492.22,536.99,11.58,10.91" target="#b3">[4]</ref>. Therefore automation is necessary. Our aim is to investigate how well we can locate and classify corals. For this purpose, we use efficient technologies from the field of deep learning. In the following chapters we will describe procedures used for the submissions to the challenge in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The annually occurring Coral reef image annotation and localization task is taking place for the fourth time in a row. The results in the recent past have always shown potential for future developments. Last years' winning team achieved a MAP 0.5 value of just 0.121 in 2021 <ref type="bibr" coords="2,466.09,138.38,11.28,10.91" target="#b4">[5]</ref>. Even though the data sets have been revised this year and therefore the results cannot be directly compared, these are the results that can best serve as a benchmark for our work. Over the years, we have seen different approaches in submissions to the challenge, both classic feature engineering <ref type="bibr" coords="2,144.92,192.57,11.37,10.91" target="#b5">[6]</ref>, as was commonly used in computer vision, and newer deep learning methods <ref type="bibr" coords="2,89.29,206.12,11.48,10.91" target="#b6">[7,</ref><ref type="bibr" coords="2,103.55,206.12,7.52,10.91" target="#b7">8,</ref><ref type="bibr" coords="2,113.84,206.12,7.65,10.91" target="#b8">9]</ref>. Our preliminary work, compared and combined both <ref type="bibr" coords="2,372.33,206.12,16.56,10.91" target="#b9">[10,</ref><ref type="bibr" coords="2,391.66,206.12,12.42,10.91" target="#b10">11]</ref>. A key lesson learned from previous investigation is the need of balancing the highly unbalanced data, which is not trivial and necessity of improving the quality of underwater images according to typical issues such as cloudiness and shifted color distribution. This year we want to benefit from these findings once again. Additionally, as proposed in <ref type="bibr" coords="2,308.92,260.32,16.25,10.91" target="#b9">[10]</ref>, we will build our investigations mainly around regions with CNN features. Although, <ref type="bibr" coords="2,301.39,273.87,13.00,10.91" target="#b8">[9]</ref> already experimented with Faster R-CNN and achieved a MAP 0.5 value of 0.13996, we see potentials for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data set</head><p>The data set was provided by the CLEF initiative. The training data set consisted of 1,374 images from a total of four different locations with a total of 31,517 annotations and 13 different classes. Additionally, the evaluation data for the submissions consisted of 200 images from one location and was not available for own investigations until the final submissions.</p><p>Regarding the quality of the data, it was varying and therefore challenging. Many images had a severe color shift, some images were blurry. Another complexity in a multi-label classification task is the number of objects. The number of corals in an image varied from 1 to 116. In addition, in groups of multiple corals of the same coral species, the corals are sometimes annotated in one bounding box and sometimes in multiple boxes as shown in Figure <ref type="figure" coords="2,408.34,454.44,3.74,10.91" target="#fig_2">2</ref>.</p><p>As can be seen in Table <ref type="table" coords="2,211.66,467.99,3.81,10.91" target="#tab_0">1</ref>, the individual classes are distributed very unbalanced, i.e. the substrate type "c_soft_coral" alone comprising 24.65% of the annotations. The three most frequent classes account for 67.37% of the annotations, while the three least frequent classes account for just 1.28% of the annotations.</p><p>To give an example: Figure <ref type="figure" coords="2,218.38,522.18,4.97,10.91" target="#fig_0">1</ref> illustrates four images of the data set with visualized annotations. Noticeable are, the color and quality differences of the images that can be easily observed. In particular, while images b) and c) have a strong blue cast, image a) is blurred. Especially in figure <ref type="figure" coords="2,118.60,562.83,3.78,10.91">d</ref>), the problem of delineating identical corals within an assemblage becomes clear. To be specific: the corals of the type "c_hard_coral_branching" are divided into a total of three annotations. Contrarily, in the same image the three large corals of type "c_hard_coral_table" in the lower right corner are combined into one annotation, although they are clearly separable.</p><p>Another example is shown in Figure <ref type="figure" coords="2,267.88,617.03,3.81,10.91" target="#fig_2">2</ref>. In image a), many corals of type "c_soft_coral" are annotated by a single bounding box per coral. In image b) a group of corals of type "c_soft_coral" is annotated by one large bounding box. Remembering the main evaluation metric MAP 0.5 , the impact of varying strategies while annotating becomes clear. For example, splitting a group </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Approach</head><p>The core strategy used for object detection uses the state-of-the-art convolutional neural network Faster R-CNN <ref type="bibr" coords="3,199.74,404.30,16.41,10.91" target="#b11">[12]</ref>. It is built with different ResNet backbone networks and the framework detectron2 <ref type="bibr" coords="3,191.09,417.85,17.90,10.91" target="#b12">[13]</ref> for PyTorch <ref type="bibr" coords="3,267.02,417.85,16.24,10.91" target="#b13">[14]</ref>. We will first observe the effect of network depth on coral detection and secondly try to compensate for the previously mentioned weaknesses of the dataset through image enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Network architecture</head><p>For the network, we chose Faster R-CNN as described in the related work section. As a backbone network, we have chosen ResNet+FPN <ref type="bibr" coords="3,269.02,521.77,16.41,10.91" target="#b14">[15]</ref>. This approach achieved the best results on the COCO-dataset <ref type="bibr" coords="3,158.73,535.32,18.07,10.91" target="#b15">[16]</ref> in the FPN paper <ref type="bibr" coords="3,262.96,535.32,18.07,10.91" target="#b14">[15]</ref> and in detectron2's Model Zoo baseline <ref type="bibr" coords="3,470.56,535.32,16.41,10.91" target="#b12">[13]</ref>. In addition to the commonly used ResNet-50 and ResNet-101 and according to He et al. <ref type="bibr" coords="3,465.94,548.87,17.91,10.91" target="#b16">[17]</ref> who showed that residual networks gain precision by increasing depth, we included ResNet-152.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training hyperparameter</head><p>Because of the small data set, we divided it into 90% training data and 10% validation data.</p><p>To monitor overfitting, we calculated the evaluation metric of the challenge MAP 0.5 in small intervals of 250 iterations (‚âà 104 epochs).</p><p>Figure <ref type="figure" coords="3,130.70,652.79,4.97,10.91" target="#fig_9">7</ref> shows the total loss of the training process and the MAP 0.5 of the checkpoints as an example. Although the training loss decreased over the complete 100.000 iterations (‚âà 41727 epochs), the network started to overfit at 70.000 iterations (‚âà 29209 epochs) and the MAP 0.5 decreased from there on. Therefore, in the end, we chose the checkpoint with the best MAP 0.5 on the evaluation data for the final submission.</p><p>For the learning process, we chose a base learning rate of 0.0005 for the first 25,000 iterations (‚âà 10431 epochs). After that, we lowered the learning rate to 0.0001 for the next 25,000 iterations (‚âà 10431 epochs) and to 0.00005 after 50,000 iterations (‚âà 20863 epochs).</p><p>Figure <ref type="figure" coords="4,132.27,573.84,5.17,10.91" target="#fig_10">8</ref> shows a comparison of the different batch sizes. The ResNet-50 and ResNet-101 networks were trained with batch sizes 32, 64, 128, 256 and 512 each. In both cases, the networks with larger batch sizes performed better than those with smaller batch sizes. Therefore, we chose a batch size of 512 for the final submissions.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Color balancing</head><p>To counteract the problem of color casts in the images, a function was written that removes blue and green casts. For this purpose, the average values for red, green and blue were calculated for each image. Then the image was shifted slightly into the red range until the average red value reached a certain threshold. Since the termination criterion depends only on the red component of the image, both images with a blue cast and images with a green cast can be improved in this way. <ref type="bibr" coords="6,112.58,175.28,17.91,10.91" target="#b17">[18]</ref> A comparison of a random image before and after color balancing, including the corresponding histograms, is shown in Figure <ref type="figure" coords="6,231.02,202.38,3.80,10.91" target="#fig_3">3</ref>. Contrary to the histogram of the original image, the green channel value of the postprocessed image is much less dominant. Its value was shifted by the image enhancement. Because of that, the histogram of the processed image shows a much more balanced distribution for all three channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Dual network approach</head><p>Both image variations are used for training subsequently. For a better comparison, all tested networks share the same settings and hyper-parameters. The setup for the training is explained in Figure <ref type="figure" coords="6,132.23,319.85,3.76,10.91" target="#fig_4">4</ref>. The finally submitted predictions were then calculated for both types of data and combined to to a common result. This process is illustrated in Figure <ref type="figure" coords="6,397.75,333.40,3.74,10.91" target="#fig_5">5</ref>.</p><p>All potential duplicates were removed using Non-maximum Suppression. NMS iteratively removes boxes with a lower confidence for all overlapping boxes that have an IoU greater than 0.8 and keeps the box with the highest confidence. If two boxes of different classes with an IoU greater than 0.8 overlap, the box with smaller confidence is also discarded. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Submissions</head><p>Nine runs were submitted to the "Coral reef image annotation and localisation" task. These consist of a combination of the backbone construction i.e. depth variation and the image type that was used for training. Each run configuration is explained in Table <ref type="table" coords="6,418.22,600.61,3.81,10.91" target="#tab_1">2</ref>. According to the challenge organization its main evaluation metric is mean average precision. Furthermore we added the submissions' mean average recall for a better comparison.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results and discussion</head><p>The results of the submitted runs are shown in Table <ref type="table" coords="7,342.51,501.28,3.81,10.91" target="#tab_1">2</ref>. The best result according to the Challenge's evaluation metric was the run with the ID 183919. It achieved an MAP 0.5 of 0.396. In view of increasing precision in connection with increasing depth, we can assume, that precision can be improved using an even deeper backbone. The same applies to recall.</p><p>With the combined method using both the original and color balanced images, the recall could be improved significantly. For example the MAR 0.5 for the ResNet-152+FPN network with combined images was 0.393 while the MAR 0.5 of the network with the original images was 0.292.</p><p>The results of all three image-type approaches used can be seen in Figure <ref type="figure" coords="7,425.00,609.67,3.68,10.91" target="#fig_7">6</ref>. In total, 5 corals could be identified in the exemplary image section by combining using NMS. In comparison, the network on the original images found only 3 corals and the network on the enhanced images found only 4 corals. The duplicate annotations found on both images were correctly sorted out by NMS. Using the data for the MAR 0.5 from Table <ref type="table" coords="7,318.02,663.87,3.74,10.91" target="#tab_1">2</ref>, it can be seen that this method was able to find more corals for all 3 network architectures by combining the predictions on the original images and the predictions on the enhanced images.  The average precision per substrate for the ResNet-152 approach for the original images and the enhanced images given in Table <ref type="table" coords="8,254.49,323.77,3.81,10.91" target="#tab_2">3</ref>. For most coral species the difference is less than 0.01, but the species "c_hard_coral_boulder", "c_hard_coral_mushroom" and "c_hard_coral_foliose" could be detected much better by the network with the enhanced images. In contrast, it was significantly worse especially with the species "c_soft_coral_gorgonian". Because both networks had strengths and weaknesses for certain coral species, the combined network was able to benefit from the strengths of both. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and Perspective</head><p>Overall, the results of the challenge are satisfying and the improvements we made to the models had the desired effects. Some image quality issues, as described in section 2, were improved. However, the quality of the annotation data should be addressed in future versions of the challenge. For instance, it is a bad starting point to have inaccurate bounding boxes containing in some cases a set of individual corals, and in other cases grouping these objects to a single annotation. It appears to make much more sense to have more precise annotations, similar to the "coral reef image pixel-wise parsing" subtask. Furthermore, dense coverage of ground surface as well as fluctuating image quality, makes it difficult to distinguish the different substrate types. However, a deeper network seems to be more capable of handling these difficulties. The image enhancements made have not produced better results on their own, but have found different corals than the network with the original images. By combining the bounding boxes of both approaches and applying non-maximum suppression, the best overall MAR 0.5 was obtained.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Figures</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,454.94,298.97,8.93;4,91.78,276.31,208.35,156.26"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Different images from the data set with visualized annotations</figDesc><graphic coords="4,91.78,276.31,208.35,156.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,89.29,324.87,340.77,8.93;5,91.78,94.15,208.35,208.35"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Different annotation strategies for assemblages of the same coral species</figDesc><graphic coords="5,91.78,94.15,208.35,208.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,89.29,639.31,257.42,8.93;5,300.13,512.29,166.68,104.65"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Training image before and after image enhancement</figDesc><graphic coords="5,300.13,512.29,166.68,104.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,89.29,498.79,216.88,8.93"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Training process of the combined Network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="7,89.29,264.97,164.95,8.93"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Use of the combined Network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="8,107.84,263.13,253.34,8.97;8,382.12,263.13,90.74,8.97"><head></head><label></label><figDesc>(a) predictions on original image (b) predictions on enhanced image (c) combined predictions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="8,89.29,280.57,344.07,8.93;8,105.14,133.19,125.01,125.01"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Predictions for original image, enhanced image and combined predictions</figDesc><graphic coords="8,105.14,133.19,125.01,125.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="11,89.29,486.62,167.11,9.43;11,193.47,322.83,208.35,141.42"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Total training loss and MAP 0.5</figDesc><graphic coords="11,193.47,322.83,208.35,141.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="12,89.29,586.56,214.41,8.93;12,91.78,374.99,416.70,178.25"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Total training loss for different batch sizes</figDesc><graphic coords="12,91.78,374.99,416.70,178.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,90.49,416.99,252.55"><head>Table 1</head><label>1</label><figDesc>Distribution of the individual classes in the training data set</figDesc><table coords="3,153.91,122.10,287.47,169.76"><row><cell>Class</cell><cell cols="2">absolute occurrence relative occurrence</cell></row><row><cell>c_soft_coral</cell><cell>7769</cell><cell>24.65%</cell></row><row><cell>c_hard_coral_boulder</cell><cell>7373</cell><cell>23.39%</cell></row><row><cell>c_sponge</cell><cell>6091</cell><cell>19.33%</cell></row><row><cell>c_hard_coral_branching</cell><cell>3132</cell><cell>9.94%</cell></row><row><cell>c_hard_coral_submassive</cell><cell>2637</cell><cell>8.37%</cell></row><row><cell>c_algae_macro_or_leaves</cell><cell>1870</cell><cell>5.93%</cell></row><row><cell>c_hard_coral_table</cell><cell>920</cell><cell>2.92%</cell></row><row><cell>c_sponge_barrel</cell><cell>606</cell><cell>1.92%</cell></row><row><cell>c_hard_coral_encrusting</cell><cell>380</cell><cell>1.21%</cell></row><row><cell>c_hard_coral_mushroom</cell><cell>335</cell><cell>1.06%</cell></row><row><cell>c_hard_coral_foliose</cell><cell>233</cell><cell>0.74%</cell></row><row><cell>c_soft_coral_gorgonian</cell><cell>171</cell><cell>0.54%</cell></row><row><cell>c_fire_coral_millepora</cell><cell>0</cell><cell>0.00%</cell></row></table><note coords="3,89.29,318.57,416.69,10.91;3,89.29,332.12,130.25,10.91"><p>of the same coral species among more or fewer annotations in the submission would have a negative impact on the score.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,88.99,298.12,345.59,152.14"><head>Table 2</head><label>2</label><figDesc>Results of the submission runs</figDesc><table coords="7,160.69,327.46,273.89,122.80"><row><cell>Run-ID</cell><cell>Backbone</cell><cell>Image type</cell><cell cols="2">Precision Recall</cell></row><row><cell>183911</cell><cell></cell><cell>default</cell><cell>0.365</cell><cell>0.269</cell></row><row><cell>183912</cell><cell>ResNet-50+FPN</cell><cell>color balanced</cell><cell>0.318</cell><cell>0.256</cell></row><row><cell>183913</cell><cell></cell><cell>combined</cell><cell>0.297</cell><cell>0.337</cell></row><row><cell>183914</cell><cell></cell><cell>default</cell><cell>0.371</cell><cell>0.246</cell></row><row><cell>183916</cell><cell>ResNet-101+FPN</cell><cell>color balanced</cell><cell>0.305</cell><cell>0.270</cell></row><row><cell>183918</cell><cell></cell><cell>combined</cell><cell>0.291</cell><cell>0.344</cell></row><row><cell>183919</cell><cell></cell><cell>default</cell><cell>0.396</cell><cell>0.292</cell></row><row><cell>183920</cell><cell>ResNet-152+FPN</cell><cell>color balanced</cell><cell>0.366</cell><cell>0.292</cell></row><row><cell>183922</cell><cell></cell><cell>combined</cell><cell>0.336</cell><cell>0.393</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,88.98,420.52,414.57,201.38"><head>Table 3</head><label>3</label><figDesc>Average precision per substrate on validation data</figDesc><table coords="8,95.27,452.14,408.28,169.76"><row><cell>Class</cell><cell cols="4">AP original images AP enhanced images difference difference(%)</cell></row><row><cell>c_soft_coral</cell><cell>0.237</cell><cell>0.233</cell><cell>-0.004</cell><cell>-1,7%</cell></row><row><cell>c_hard_coral_boulder</cell><cell>0.220</cell><cell>0.242</cell><cell>+0.022</cell><cell>+10%</cell></row><row><cell>c_sponge</cell><cell>0.121</cell><cell>0.117</cell><cell>-0.004</cell><cell>-3,3%</cell></row><row><cell>c_hard_coral_branching</cell><cell>0.208</cell><cell>0.199</cell><cell>-0.009</cell><cell>-4,3%</cell></row><row><cell>c_hard_coral_submassive</cell><cell>0.197</cell><cell>0.197</cell><cell>0</cell><cell>0%</cell></row><row><cell>c_algae_macro_or_leaves</cell><cell>0.049</cell><cell>0.057</cell><cell>+0.008</cell><cell>+16,3%</cell></row><row><cell>c_hard_coral_table</cell><cell>0.318</cell><cell>0.316</cell><cell>-0.002</cell><cell>-0,6%</cell></row><row><cell>c_sponge_barrel</cell><cell>0.354</cell><cell>0.326</cell><cell>-0.028</cell><cell>-7,9%</cell></row><row><cell>c_hard_coral_encrusting</cell><cell>0.461</cell><cell>0.467</cell><cell>+0.006</cell><cell>+1,3%</cell></row><row><cell>c_hard_coral_mushroom</cell><cell>0.251</cell><cell>0.316</cell><cell>+0.065</cell><cell>+25,9%</cell></row><row><cell>c_hard_coral_foliose</cell><cell>0.103</cell><cell>0.184</cell><cell>+0.081</cell><cell>+78,6%</cell></row><row><cell>c_soft_coral_gorgonian</cell><cell>0.254</cell><cell>0.142</cell><cell>-0.112</cell><cell>-44,1%</cell></row><row><cell>c_fire_coral_millepora</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,332.50,395.01,10.91;9,112.66,346.05,395.17,10.91;9,112.39,359.59,394.80,10.91;9,112.66,373.14,394.62,10.91;9,112.66,386.69,393.33,10.91;9,112.66,400.24,395.17,10.91;9,112.66,413.79,393.54,10.91;9,112.66,427.34,170.14,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,352.50,373.14,154.78,10.91;9,112.66,386.69,300.10,10.91">Overview of the ImageCLEF 2022: Multimedia retrieval in medical, social media and nature applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>P√©teri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>≈ûtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,435.26,386.69,70.73,10.91;9,112.66,400.24,395.17,10.91;9,112.66,413.79,239.58,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<title level="s" coord="9,359.20,413.79,147.00,10.91;9,112.66,427.34,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,440.89,394.61,10.91;9,112.66,454.44,395.17,10.91;9,112.66,467.99,393.32,10.91;9,112.66,481.54,394.52,10.91;9,112.66,495.09,89.12,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,409.80,440.89,97.48,10.91;9,112.66,454.44,202.24,10.91">ImageCLEFcoral task: Coral reef image annotation and localisation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,340.39,454.44,167.44,10.91;9,112.66,467.99,393.32,10.91;9,112.66,481.54,152.73,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<title level="s" coord="9,272.53,481.54,186.62,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,508.64,394.53,10.91;9,112.66,522.18,394.04,10.91;9,112.66,535.73,290.37,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Hoegh-Guldberg</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-94-007-0114-4_22</idno>
		<ptr target="https://doi.org/10.1007/978-94-007-0114-4_22.doi:10.1007/978-94-007-0114-4_22" />
		<title level="m" coord="9,215.42,508.64,286.76,10.91">The Impact of Climate Change on Coral Reef Ecosystems</title>
		<meeting><address><addrLine>Netherlands; Dordrecht</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="391" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,549.28,393.33,10.91;9,112.41,562.83,81.21,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,223.07,549.28,227.65,10.91">New estimates of global and regional coral reef areas</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Spalding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Grenfell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,458.48,549.28,47.51,10.91">Coral reefs</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="225" to="230" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,576.38,393.33,10.91;9,112.66,589.93,82.33,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Soukup</surname></persName>
		</author>
		<title level="m" coord="9,166.43,576.38,339.56,10.91;9,112.66,589.93,50.41,10.91">Automatic coral reef annotation, localization and pixel-wise parsing using mask r-cnn</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,603.48,393.33,10.91;9,112.66,617.03,187.91,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,250.46,603.48,255.53,10.91;9,112.66,617.03,32.59,10.91">Automatic classification of coral images using color and textures</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Caridade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,171.95,617.03,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,630.58,393.33,10.91;9,112.66,644.13,303.66,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,227.39,630.58,278.60,10.91;9,112.66,644.13,149.05,10.91">Coral reef annotation, localisation and pixel-wise classification using mask-rcnn and bag of tricks</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>R√≠ha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,287.71,644.13,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,657.68,393.33,10.91;10,112.66,86.97,394.52,10.91;10,112.66,100.52,22.69,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,388.53,657.68,117.46,10.91;10,112.66,86.97,266.29,10.91">Deep segmentation: using deep convolutional networks for coral reef pixel-wise parsing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Steffens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ravenscroft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hagras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,405.65,86.97,97.13,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,114.06,393.33,10.91;10,112.66,127.61,203.94,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,303.91,114.06,202.07,10.91;10,112.66,127.61,48.47,10.91">Coral reef annotation and localization using faster r-cnn</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jaisakthi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mirunalini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Aravindan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,187.99,127.61,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,141.16,395.17,10.91;10,112.66,154.71,352.19,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,273.13,141.16,234.70,10.91;10,112.66,154.71,200.27,10.91">A two-staged approach for localization and classification of coral reef structures and compositions</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bogomasov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Grawe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Conrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,336.23,154.71,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,168.26,393.33,10.91;10,112.66,181.81,275.31,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,274.36,168.26,231.63,10.91;10,112.66,181.81,123.39,10.91">Enhanced localization and classification of coral reef structures and compositions</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bogomasov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Grawe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Conrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,259.35,181.81,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,195.36,393.33,10.91;10,112.66,208.91,395.01,10.91;10,112.66,222.46,127.84,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="10,268.81,195.36,237.18,10.91;10,112.66,208.91,114.22,10.91">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1506.01497" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,236.01,394.04,10.91;10,112.66,249.56,156.24,10.91" xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
	</analytic>
	<monogr>
		<title level="j" coord="10,361.15,236.01,48.12,10.91">Detectron</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,263.11,394.53,10.91;10,112.66,276.66,394.52,10.91;10,112.66,290.20,394.53,10.91;10,112.66,303.75,394.53,10.91;10,112.66,317.30,395.17,10.91;10,112.66,330.85,394.03,10.91;10,112.66,344.40,357.13,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,371.36,290.20,135.83,10.91;10,112.66,303.75,178.35,10.91">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,292.30,317.30,215.52,10.91;10,112.66,330.85,31.56,10.91">Advances in Neural Information Processing Systems 32</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alch√©-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,357.95,393.33,10.91;10,112.66,371.50,397.48,10.91;10,112.36,387.49,61.75,7.90" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="10,391.04,357.95,114.95,10.91;10,112.66,371.50,85.53,10.91">Feature pyramid networks for object detection</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1612.03144</idno>
		<ptr target="https://arxiv.org/abs/1612.03144.doi:10.48550/ARXIV.1612.03144" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,398.60,394.52,10.91;10,112.66,412.15,394.62,10.91;10,112.31,425.70,277.84,10.91" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="10,221.53,412.15,199.65,10.91">Microsoft coco: Common objects in context</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1405.0312</idno>
		<ptr target="https://arxiv.org/abs/1405.0312.doi:10.48550/ARXIV.1405.0312" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,439.25,394.61,10.91;10,112.66,452.79,314.10,10.91" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="10,253.11,439.25,199.28,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1512.03385</idno>
		<ptr target="https://arxiv.org/abs/1512.03385.doi:10.48550/ARXIV.1512.03385" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,466.34,394.04,10.91;10,112.66,479.89,187.34,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">B</forename><surname>Andersen</surname></persName>
		</author>
		<ptr target="https://github.com/nikolajbech/underwater-image-color-correction" />
		<title level="m" coord="10,193.71,466.34,158.81,10.91">underwater-image-color-correction</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
