<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,334.46,15.42;1,89.29,106.66,369.46,15.42;1,89.29,128.58,289.67,15.43">Overview of ImageCLEFfusion 2022 Task -Ensembling Methods for Media Interestingness Prediction and Result Diversification</title>
				<funder ref="#_4yccsvb">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,156.89,92.56,11.96"><forename type="first">Liviu-Daniel</forename><surname>≈ûtefan</surname></persName>
							<email>liviu1_daniel.stefan@upb.ro</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AI Multimedia Lab</orgName>
								<orgName type="institution">Politehnica University of Bucharest</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,193.89,156.89,121.16,11.96"><forename type="first">Mihai</forename><forename type="middle">Gabriel</forename><surname>Constantin</surname></persName>
							<email>mihai.constantin84@upb.ro</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AI Multimedia Lab</orgName>
								<orgName type="institution">Politehnica University of Bucharest</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,327.09,156.89,69.56,11.96"><forename type="first">Mihai</forename><surname>Dogariu</surname></persName>
							<email>mihai.dogariu@upb.ro</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AI Multimedia Lab</orgName>
								<orgName type="institution">Politehnica University of Bucharest</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,426.25,156.89,75.70,11.96"><forename type="first">Bogdan</forename><surname>Ionescu</surname></persName>
							<email>bogdan.ionescu@upb.ro</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AI Multimedia Lab</orgName>
								<orgName type="institution">Politehnica University of Bucharest</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,334.46,15.42;1,89.29,106.66,369.46,15.42;1,89.29,128.58,289.67,15.43">Overview of ImageCLEFfusion 2022 Task -Ensembling Methods for Media Interestingness Prediction and Result Diversification</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">E0DD18997EF0517B940E7040EBB40AD5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Late fusion</term>
					<term>Ensembling</term>
					<term>Fusion benchmarking</term>
					<term>Visual interestingness prediction</term>
					<term>Image search results diversification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The 2022 ImageCLEFfusion task is the first edition of this task, targeting the creation of late fusion or ensembling methods in two different scenarios: (i) the prediction of media visual interestingness, and (ii) social media image search results diversification. The objective proposed to participants is to train and test their proposed fusion schemes on a set of pre-computed inducers, without creating or bringing inducers from the outside. The two scenarios correspond to a regression scenario in the case of media interestingness, where performance is measured via the mean average precision at 10 (MAP@10) metric, and to a retrieval scenario in the case of result diversification, where performance is measured via the F1-score and Cluster Recall at 20 (F1@20, CR@20). Overall 6 teams registered for ImageCLEFfusion, 5 of them submitting runs, while only one team submitted runs to both the interestingness and diversification tasks. A total of 39 runs were received, and an analysis of the proposed methods shows a great diversity among them, ranging from statistical weighted approaches, weighted approaches that use learning stages for creating the weights, machine learning approaches that join the inducer predictions like SVM or KNN, deep learning approaches, and even fusion schemes that join the results of other fusion schemes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The current landscape of computer vision tasks seems to be dominated by end-to-end deep neural networks that take a media sample as input and output a prediction by processing the images or videos through the network layers. However, in several domains, the performance of single network systems reaches a plateau where performance increases are marginal, and performances can be considered comparatively low. This phenomenon may impede the adoption of such AI solutions, as companies and users may be dissatisfied with the results.</p><p>One of the main methods researchers use to enhance the performance of models is the use of late fusion (or ensembling) systems. These systems use a collection of individual prediction systems, called inducers, and join their results through fusion schemes. The usefulness of such approaches is proven in several scenarios, in traditional computer vision tasks, like action recognition in videos <ref type="bibr" coords="2,189.96,86.97,11.59,10.91" target="#b0">[1]</ref>, but more pervasive in tasks related to the human understanding of multimedia data. These types of tasks generally show lower performance for end-to-end systems when compared with traditional computer vision tasks, a phenomenon commonly attributed to their inherent subjectivity and multi-modality, as well as difficulties in creating reliable ground-truth annotations <ref type="bibr" coords="2,239.84,141.16,11.28,10.91" target="#b1">[2,</ref><ref type="bibr" coords="2,253.86,141.16,7.52,10.91" target="#b2">3]</ref>. Examples from the current literature that support this trend show fusion systems achieving top performance in several benchmarking competitions related to the processing of subjective multimedia data, including but not limited to media interestingness <ref type="bibr" coords="2,159.38,181.81,11.43,10.91" target="#b3">[4]</ref>, memorability <ref type="bibr" coords="2,239.97,181.81,11.43,10.91" target="#b4">[5]</ref>, and violent scene detection <ref type="bibr" coords="2,382.12,181.81,11.43,10.91" target="#b5">[6]</ref>.</p><p>Given these factors, the ImageCLEFfusion task, currently in its initial edition at ImageCLEF 2022 <ref type="bibr" coords="2,111.94,208.91,11.29,10.91" target="#b6">[7]</ref>, asks participants to create ensembling schemes that can accurately join the prediction outputs of a pre-computed set of inducers for two scenarios. The first scenario represents a regression task, applied to the media interestingness data associated with the Interesting-ness10k dataset <ref type="bibr" coords="2,160.48,249.56,11.34,10.91" target="#b2">[3]</ref>, while the second scenario represents a retrieval task, applied to the results diversification data related to the Retrieving Diverse Social Images dataset <ref type="bibr" coords="2,422.40,263.11,11.43,10.91" target="#b7">[8]</ref>.</p><p>This paper presents an overview of the 2022 ImageCLEFfusion task and is structured as follows. Section 2 presents the data used in this task, while Section 3 shows details with regards to the participation. Results and their analysis are presented in Section 4, and the paper ends with the main conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data Description</head><p>As we mentioned, the first edition of the ImageCLEFfusion task presents participants with two different scenarios: (i) ImageCLEFfusion-int, a regression task using media interestingness data, and (ii) ImageCLEFfusion-div, a retrieval task using search result diversification data. Our philosophy in these scenarios is that we want to compare the performance of ensembling engines in similar setups for all participating teams. Therefore, we provide the set of inducers that participants will use, and the addition of external inducers is not allowed. In a general sense, given a set of ùëÄ media samples ùëÜ ‚àà {ùë† 1 , ùë† 2 , ...ùë† ùëÄ }, and a set of ùëÅ computer vision algorithms ùê¥ ‚àà {ùëé 1 , ùëé 2 , ...ùëé ùëÅ }, we will provide all the prediction outputs, i.e., for each sample ùë† ùëñ , we will provide a set of predictions ùë¶ ùëñ = {ùë† ùëñ,1 , ùë† ùëñ,2 , ...ùë† ùëñ,ùëÅ }. This process is presented in Figure <ref type="figure" coords="2,120.63,497.87,3.77,10.91" target="#fig_1">1</ref>. Participants are tasked with creating the ensembling function ‚Ñ±, that is able to join the predictions from individual inducers and create better predictions.</p><p>For the ImageCLEFfusion-int task, we provide data extracted from the image prediction task from the 2017 MediaEval Predicting Media Interestingness task <ref type="bibr" coords="2,384.11,538.52,11.58,10.91" target="#b8">[9]</ref>. We use the prediction outputs from the 29 systems submitted during MediaEval. We then split the available data into 1,877 images contained in the training set, and 558 images in the testing set. For the ImageCLEFfusion-div task, we provide data extracted from the DIV150 challenge associated with the Retrieving Diverse Social Images dataset <ref type="bibr" coords="2,317.85,592.72,11.58,10.91" target="#b7">[8]</ref>. The prediction outputs of 56 systems inducer systems is provided, with 60 retrieval queries included in the training set and 63 queries included in the testing set. These details are presented in Table <ref type="table" coords="2,366.92,619.81,3.66,10.91" target="#tab_0">1</ref>. Participants are free to create the validation sets as they choose, and can do this by splitting the training set according to their individual needs. In order to encourage a careful selection of the proposed fusion methods, participants are only allowed a maximum of 10 runs for each of the two tasks.   Evaluation is carried out using mean average precision at 10 (MAP@10) for the interestingness task and F1-score as the primary metric and Cluster Recall as the secondary one, at 20 (F1@20 and CR@20) for the diversification task. These metrics correspond to the metrics used on the respective datasets for each of the two tasks. Both these metrics place greater importance to selecting and presenting the most appropriate media samples to users, 10 in the case of interestingness and 20 in the case of diversification. For ImageCLEFfusion-int we provide the trec_eval tool developed by NIST <ref type="foot" coords="3,235.11,564.92,3.71,7.97" target="#foot_0">1</ref> , that can compute several metrics, including MAP@10, while for ImageCLEFfusion-div we provide the div_eval tool, specially developed and designed for the DIV competitions.</p><p>For each of the two training sets we provide ground truth data, inducer prediction outputs, inducer performance with regards to the main metrics, and scripts necessary for performance calculation. On the other hand, for the testing sets we only provide the inducer prediction outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Participation</head><p>Participation is satisfactory for the first edition of this task. Overall, 7 groups completed their registration to ImageCLEFfusion, 5 submitted runs, and 4 completed the competition by also submitting working notes describing their methods. For the interestingness task, 3 teams submitted a total of 14 runs, while 3 teams submitted a total of 25 runs for the diversification task. Only one of these teams chose to participate in both tasks. An overview of the submitting teams is presented in Table <ref type="table" coords="4,212.76,179.03,3.74,10.91">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Groups that participated with runs to the ImageCLEFfusion tasks. We present the institutions represented by these teams, the number of runs for interestingness and diversification, as well as references to their paper, where submitted. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>The results for the participating teams for interestingness and diversification are presented in Tables <ref type="table" coords="4,120.80,475.83,5.17,10.91">3</ref> and<ref type="table" coords="4,148.52,475.83,3.81,10.91">4</ref>. In both cases we present the results of the participating team in comparison with baseline runs that consist of the average performance of all the provided inducers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results for the interestingness task</head><p>Three teams submitted 14 runs in total for the ImageCLEFfusion-int task, with the highest performance being a MAP@10 value of 0.2192, representing an improvement of 131% over the baseline of 0.0946. It is interesting to note that all teams scored runs above the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AIMultimediaLab</head><p>The best performing run from the AIMultimediaLab team achieved a MAP@10 score of 0.2192 <ref type="bibr" coords="4,233.57,605.26,16.42,10.91" target="#b9">[10]</ref>. This team proposed two types of runs. While the first type is based on a simple weighted approach, where weights are determined through a grid-search approach, the second one is based on DeepFusion <ref type="bibr" coords="4,411.33,632.36,18.07,10.91" target="#b13">[14]</ref> DNN structures, that use Dense, Attention, Convolutional and Cross-Space-Fusion approaches. The best performing method from this team uses the DeepFusion-CSF model.</p><p>ssn_it The best performing run from the ssn_it team achieved a MAP@10 score of 0.1106. Unfortunately, no working notes paper was submitted for this run.</p><p>UECORK The best performing run from UECORK attained a MAP@10 score of 0.1097 <ref type="bibr" coords="5,487.27,123.03,16.31,10.91" target="#b12">[13]</ref>.</p><p>The authors propose several approaches to weighted fusion. A baseline weighting method computes the average for all inducer outputs, while complex approaches use different methods for determining and optimizing the weights, like Genetic Algorithms, Nelder Mead algorithm <ref type="bibr" coords="5,192.85,177.23,16.41,10.91" target="#b14">[15]</ref>, Truncated Newton optimization <ref type="bibr" coords="5,367.13,177.23,16.41,10.91" target="#b15">[16]</ref>, and Particle Swarm Optimization <ref type="bibr" coords="5,159.80,190.78,16.41,10.91" target="#b16">[17]</ref>. The best performing method from this was recorded by two different approaches: PSO and TNC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>Results for the ImageCLEFfusion-int task. We present the results only for the best performing proposed system for each team according to the MAP@10 metric. We also compare these results with the a baseline that consists of the average performance of all the provided inducers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results for the diversification task</head><p>Three teams were involved in the diversification task too, submitting 25 runs in total. The highest performance shows a F1@20 score of 0.6216, representing an improvement of 17% over the baseline value of 0.5313. For the secondary metric, CR@20, the improvement of the corresponding system is almost equal, at 18%. Again we are happy to note that all teams presented systems that score above the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AIMultimediaLab</head><p>The best performing run from the AIMultimediaLab team achieved a F1@20 score of 0.6216, and a CR@20 score of 0.4916 <ref type="bibr" coords="5,353.86,500.21,16.35,10.91" target="#b9">[10]</ref>. The same approaches as presented in Section 4.1 were used for the diversification task as well. For the diversification task, the best performing method from this team used the DeepFusion-Convolutional approach.</p><p>klssncse The best performing run from the klssncse team shows a F1@20 score of 0.5634 and a CR@20 score of 0.4414 <ref type="bibr" coords="5,249.99,576.92,16.42,10.91" target="#b10">[11]</ref>. The authors analyzed fusion models based on KNN Regressors, Classification and Regression Trees <ref type="bibr" coords="5,334.48,590.47,18.07,10.91" target="#b17">[18]</ref> and SVR <ref type="bibr" coords="5,397.11,590.47,16.41,10.91" target="#b18">[19]</ref>. All the submission from this team are represented by CART approaches, as these provided the best results in the preliminary studies.</p><p>shreya_sriram The best performing run from the shreya_sriram team shows a F1@20 score of 0.5604 and a CR@20 performance of 0.4373. The paper studies several methods, based on neural networks, namely MLP regressor, Ridge regressor with Grid Search and KerasRegressor <ref type="bibr" coords="6,211.28,86.97,16.41,10.91" target="#b19">[20]</ref>. The results of all these fusion models are combined by a voting regressor, thus obtaining a meta-estimator that uses late fusion as inputs. All the submissions for this team are represented by different setups for the voting regressor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>Results for the ImageCLEFfusion-div task. We present the results only for the best performing proposed system for each team according to the F1@20 metric, as well as the corresponding CR@20 metric. We also compare these results with the a baseline that consists of the average performance of all the provided inducers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This first edition of the ImageCLEFfusion task attracted a total of 5 teams that submitted runs, with 4 of them completing their submissions by creating a working notes paper. Two tasks were proposed to the participants, a regression-based task that uses media interestingness data and a retrieval task that uses search result diversification data. In total, 39 runs were submitted by the teams, 14 for media interestingness and 25 for diversification. Only one group chose to participate in both tasks. We compared the submitted runs against a baseline composed of the average performance of all the inducers in the testing set. All the participant teams show performances above this baseline. For the interestingness task, the best result is a MAP@10 score of 0.2192, representing an improvement of 131% over the baseline. On the other hand, for the diversity task, the improvement is lower, 17% for the F1@20 metric and 18% for the CR@20 metric, corresponding to performances of 0.6216 and 0.4916, respectively. Thus, while the results for diversity are higher, from the percentual improvement standpoint, interestingness represents a higher success. We consider that this difference may be the result of the complexity of the inducer output data -interestingness data has a lower complexity and, therefore, perhaps easier to handle by the proposed methods. Also, it may be possible that the higher inducer performances for the diversification tasks leave little room for improvement compared with the inducers associated with the interestingness task.</p><p>Regarding the fusion methods proposed by the participants, we are happy to report a high degree of diversity among them. Proposed fusion schemes include simple statistical weighted approaches, approaches that use learning methods for creating the weights, algorithms that utilize both traditional (kNN, SVM) and deep learning (DeepFusion, KerasRegressor) models for combining the inducer prediction, and even a method that uses a voting regressor for combining the outputs of several other fusion schemes. This is indeed encouraging are we are looking forward to developments in future editions of the ImageCLEFfusion task.</p><p>Future editions of this task must, first of all, follow the same two datasets with the purpose of monitoring if and how the performances of the proposed systems increased. Also, we plan to add other tasks in the future based on a different machine learning task, whether it is as simple as classification or more complex multi-label or multi-class regressions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,89.29,354.65,416.87,8.93;3,89.29,366.66,380.67,8.87"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: General presentation of ensembling systems. Samples are represented with blue color, inducer algorithms with green, prediction outputs with yellow and the ensembling function with red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,399.76,345.56,61.27"><head>Table 1</head><label>1</label><figDesc>Data composition for the ImageCLEFfusion-int and ImageCLEFfusion-div tasks.</figDesc><table coords="3,160.73,427.85,273.82,33.18"><row><cell>Task</cell><cell cols="3">Training set Testing set No. inducers</cell></row><row><cell cols="3">ImageCLEFfusion-int 1,877 images 558 images</cell><cell>29</cell></row><row><cell>ImageCLEFfusion-div</cell><cell>60 queries</cell><cell>63 queries</cell><cell>56</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,108.93,671.04,109.62,8.97"><p>https://trec.nist.gov/trecùëíùë£ùëéùëô/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work is supported under the <rs type="programName">H2020 AI4Media "A European Excellence Centre for Media, Society and Democracy"</rs> project, contract #<rs type="grantNumber">951911</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4yccsvb">
					<idno type="grant-number">951911</idno>
					<orgName type="program" subtype="full">H2020 AI4Media &quot;A European Excellence Centre for Media, Society and Democracy&quot;</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,112.66,258.42,394.62,10.91;7,112.66,271.96,394.53,10.91;7,112.66,285.51,90.72,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,273.67,258.42,213.76,10.91">Gate-shift networks for video action recognition</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,112.66,271.96,389.80,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1102" to="1111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,299.06,394.53,10.91;7,112.28,312.61,393.71,10.91;7,112.28,326.16,125.20,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,112.28,312.61,281.84,10.91">Affect in multimedia: benchmarking violent scenes detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-H</forename><surname>Demarty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sjoberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schedl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gravier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,406.54,312.61,99.44,10.91;7,112.28,326.16,93.28,10.91">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,339.71,393.33,10.91;7,112.66,353.26,393.32,10.91;7,112.48,366.81,222.55,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,478.57,339.71,27.42,10.91;7,112.66,353.26,326.29,10.91">Visual interestingness prediction: a benchmark framework and literature review</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>≈ûtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">Q</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-H</forename><surname>Demarty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sj√∂berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,447.46,353.26,58.52,10.91;7,112.48,366.81,123.39,10.91">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="1526" to="1550" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,380.36,394.53,10.91;7,112.66,393.91,393.33,10.91;7,112.66,407.46,394.53,10.91;7,112.66,421.01,45.01,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,256.86,380.36,245.45,10.91">Video interestingness prediction based on ranking model</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,127.19,393.91,378.80,10.91;7,112.66,407.46,365.33,10.91">Proceedings of the joint workshop of the 4th workshop on affective social multimedia computing and first multi-modal affective computing of large-scale multimedia data</title>
		<meeting>the joint workshop of the 4th workshop on affective social multimedia computing and first multi-modal affective computing of large-scale multimedia data</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="55" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,434.55,393.60,10.91;7,112.66,448.10,269.61,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,365.08,434.55,141.18,10.91;7,112.66,448.10,100.75,10.91">Predicting media memorability using ensemble models</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Azcona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Moreu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">E</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,222.21,448.10,129.93,10.91">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,461.65,393.33,10.91;7,112.66,475.20,394.62,10.91;7,112.66,488.75,135.89,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,386.28,461.65,119.71,10.91;7,112.66,475.20,368.57,10.91">Fudan-huawei at mediaeval 2015: Detecting violent scenes and affective impact in movies with deep learning</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R.-W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,112.66,488.75,44.40,10.91">MediaEval</title>
		<imprint>
			<biblScope unit="volume">1436</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,502.30,395.01,10.91;7,112.66,515.85,395.17,10.91;7,112.39,529.40,394.80,10.91;7,112.66,542.95,394.62,10.91;7,112.66,556.50,393.33,10.91;7,112.66,570.05,395.17,10.91;7,112.66,583.60,393.54,10.91;7,112.66,597.15,170.14,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,352.50,542.95,154.78,10.91;7,112.66,556.50,311.34,10.91">Overview of the ImageCLEF 2022: Multimedia Retrieval in Medical, Social Media and Nature Applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>≈ûtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,446.87,556.50,59.11,10.91;7,112.66,570.05,395.17,10.91;7,112.66,583.60,239.58,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<title level="s" coord="7,359.20,583.60,147.00,10.91;7,112.66,597.15,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,610.69,393.33,10.91;7,112.66,624.24,393.33,10.91;7,112.33,637.79,68.33,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,411.51,610.69,94.48,10.91;7,112.66,624.24,223.52,10.91">Benchmarking image retrieval diversification techniques for social media</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rohm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Boteanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>G√ÆnscƒÉ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,345.02,624.24,148.18,10.91">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="677" to="691" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,651.34,393.57,10.91;7,112.66,664.89,309.16,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C.-H</forename><surname>Demarty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sj√∂berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Duong</surname></persName>
		</author>
		<title level="m" coord="7,435.63,651.34,70.60,10.91;7,112.66,664.89,164.62,10.91;7,299.97,664.89,91.24,10.91">Mediaeval 2017 predicting media interestingness task</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>MediaEval workshop</note>
</biblStruct>

<biblStruct coords="8,112.66,86.97,393.33,10.91;8,112.66,100.52,393.33,10.91;8,112.66,114.06,325.41,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,348.05,86.97,157.94,10.91;8,139.34,100.52,254.94,10.91">Deepfusion methods for ensembling in diverse scenarios</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>≈ûtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,418.05,100.52,87.94,10.91;8,112.66,114.06,206.33,10.91">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Ai multimedia lab at imagecleffusion</note>
</biblStruct>

<biblStruct coords="8,112.66,127.61,394.53,10.91;8,112.66,141.16,395.17,10.91;8,112.66,154.71,394.53,10.91;8,112.66,168.26,157.21,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,208.93,141.16,298.90,10.91;8,112.66,154.71,113.90,10.91">A fusion approach for web search result diversification using machine learning algorithms</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kalinathan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Balsundaram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Munees</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Venkatakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,249.91,154.71,257.28,10.91;8,112.66,168.26,38.13,10.91">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,181.81,393.33,10.91;8,112.66,195.36,393.33,10.91;8,112.66,208.91,216.46,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,311.50,181.81,194.48,10.91;8,112.66,195.36,167.78,10.91">Ensembled approach for web search result diversification using neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Balasundaram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kalinathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,305.92,195.36,115.04,10.91">CLEF2022 Working Notes</title>
		<title level="s" coord="8,428.83,195.36,77.16,10.91;8,112.66,208.91,97.38,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,222.46,393.61,10.91;8,112.26,236.01,393.73,10.91;8,112.66,249.56,325.41,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,401.68,222.46,104.59,10.91;8,112.26,236.01,280.78,10.91">A late fusion framework with multiple optimization methods for media interestingness</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shoukat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Said</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Uzzaman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ahmad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,417.87,236.01,88.12,10.91;8,112.66,249.56,23.42,10.91">CLEF2022 Working Notes</title>
		<title level="s" coord="8,143.49,249.56,175.50,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,263.11,395.17,10.91;8,112.66,276.66,394.53,10.91;8,112.66,290.20,80.57,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,302.70,263.11,205.13,10.91;8,112.66,276.66,99.40,10.91">Deepfusion: Deep ensembles for domain independent system fusion</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>≈ûtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,235.02,276.66,224.10,10.91">International Conference on Multimedia Modeling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="240" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,303.75,318.10,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,200.27,303.75,101.80,10.91">Nelder-mead algorithm</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,310.61,303.75,57.40,10.91">Scholarpedia</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">2928</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,317.30,395.00,10.91;8,112.41,330.85,38.81,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="8,164.91,317.30,193.37,10.91">Deep learning via hessian-free optimization</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,385.87,317.30,22.37,10.91">ICML</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="735" to="742" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,344.40,393.98,10.91;8,112.66,357.95,28.67,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="8,256.74,344.40,121.24,10.91">Particle swarm optimization</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Blackwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,386.26,344.40,83.81,10.91">Swarm intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="33" to="57" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,371.50,394.52,10.91;8,112.66,385.05,72.53,10.91" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="8,350.39,371.50,152.74,10.91">Classification and regression trees</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Routledge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,398.60,393.33,10.91;8,112.66,412.15,318.43,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="8,387.71,398.60,118.27,10.91;8,112.66,412.15,39.59,10.91">Support vector regression machines</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,161.13,412.15,230.24,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,425.70,303.23,10.91" xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
