<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,418.89,15.42;1,89.29,106.66,324.67,15.42">CMRE-UoG team at ImageCLEFmedical Caption 2022: Concept Detection and Image Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,105.55,11.96"><forename type="first">Francesco</forename><surname>Dalla Serra</surname></persName>
							<email>francesco.dallaserra@mre.medical.canon</email>
							<affiliation key="aff0">
								<orgName type="institution">Canon Medical Research Europe</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<settlement>Glasgow</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,212.77,134.97,74.97,11.96"><forename type="first">Fani</forename><surname>Deligianni</surname></persName>
							<email>fani.deligianni@glasgow.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<settlement>Glasgow</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,300.39,134.97,68.19,11.96"><forename type="first">Jeffrey</forename><surname>Dalton</surname></persName>
							<email>jeff.dalton@glasgow.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<settlement>Glasgow</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,399.58,134.97,77.94,11.96"><forename type="first">Alison</forename><forename type="middle">Q</forename><surname>O'neil</surname></persName>
							<email>alison.oneil@mre.medical.canon</email>
							<affiliation key="aff0">
								<orgName type="institution">Canon Medical Research Europe</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,418.89,15.42;1,89.29,106.66,324.67,15.42">CMRE-UoG team at ImageCLEFmedical Caption 2022: Concept Detection and Image Captioning</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">ACF970A8B3B7C38D2B8A1B69BB91B54A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>Concept Detection</term>
					<term>Image Captioning</term>
					<term>Image Retrieval</term>
					<term>Medical Imaging</term>
					<term>UMLS</term>
					<term>Convolutional Neural Network</term>
					<term>Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work presents the proposed solutions of our team for the ImageCLEFmedical Caption 2022 task <ref type="bibr" coords="1,130.96,238.04,9.39,8.97" target="#b0">[1]</ref>. This task is structured as two subtasks: (1) Concept Detection subtask -which consists of detecting Concept Unique Identifiers (CUIs) from the Unified Medical Language System (UMLS) [2] attributed to each image; and (2) the Caption Prediction subtask -which involves generating an accurate description of the content of the image, based on the concepts detected in the first subtask. For both subtasks, the dataset corresponds to a subset of the Radiology Objects in the COntext (ROCO) dataset <ref type="bibr" coords="1,455.21,281.87,9.39,8.97" target="#b2">[3]</ref>.</p><p>In the Concept Detection subtask, we experiment with two different strategies: a) supervised learning -we train a Convolutional Neural Network (CNN) <ref type="bibr" coords="1,345.97,303.79,9.23,8.97" target="#b3">[4,</ref><ref type="bibr" coords="1,357.31,303.79,7.30,8.97" target="#b4">5]</ref> to classify the full set of CUIs; b) image retrieval -we retrieve the top ùêæ most "similar" images from the training set based on the cosine similarity score between the image representations (extracted from the last average pooling layer), and combine the associated CUIs using a soft majority voting approach, similar to the ImageCLEFmed Caption 2021 winning approach [6]. Our best submission consists of the second image retrieval approach, for which we used an ensemble of five different CNNs. This approach ranked 2nd with an F1 score equal to 0.451, with a margin of approximately 5 √ó 10 -4 from the 1st position.</p><p>In the Caption Prediction subtask, we adopt an image encoder-decoder Transformer model <ref type="bibr" coords="1,494.28,380.50,9.52,8.97" target="#b6">[7]</ref>, which takes as input the image representation -generated using a CNN image encoder -and generates a text caption describing the image. Furthermore, we considered a multimodal encoder-decoder Transformer model, which differs from the previous by taking as an additional input the CUIs extracted from the previous subtask alongside an image representation. Our multimodal approach ranked 6th, with a BLEU score [8] of 0.291, and ranked 1st place in terms of ROUGE [9]  (the secondary metric for this subtask), with a score of 0.201.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The ImageCLEFmedical Caption 2022 challenge <ref type="bibr" coords="1,312.84,557.99,12.99,10.91" target="#b0">[1]</ref> is the 6th edition of the Caption Task, organized by the CLEF initiative 1 and part of ImageCLEF 2022 <ref type="bibr" coords="1,378.39,571.54,16.41,10.91" target="#b9">[10]</ref>. This challenge aims at promoting research in the field of medical image captioning, which consists of describing the content of a medical image in the form of free text. This is normally a time-consuming task performed by highly specialized experts. Therefore, this could aid radiologists by speeding up the diagnosis and treatment workflow, while reducing human errors, due to extensive working hours or distractions. This year's challenge was divided into two subtasks: Concept Detection and Caption Prediction. Concept Detection consists in detecting the Concept Unique Identifiers (CUIs) from the Unified Medical Language System (UMLS) <ref type="bibr" coords="2,243.50,181.81,12.77,10.91" target="#b1">[2]</ref> relevant to each image. This is considered to be the first step towards the Caption Prediction task, which consists in generating a textual description of the image based on the detected CUIs.</p><p>In these working notes, we present the different methods applied by our team in both subtasks. For the Concept Detection task, we considered a supervised learning approach, comparing two different Convolutional Neural Network (CNN) architectures <ref type="bibr" coords="2,379.24,249.56,11.28,10.91" target="#b3">[4,</ref><ref type="bibr" coords="2,393.24,249.56,7.63,10.91" target="#b4">5]</ref>; and an image retrieval approach, similar to last year's winning solution <ref type="bibr" coords="2,301.31,263.11,11.28,10.91" target="#b5">[6]</ref>, where the predicted CUIs are selected from the top ùêæ most similar images. For the Caption Prediction subtask, we applied a Transformer encoder-decoder architecture <ref type="bibr" coords="2,221.79,290.20,11.38,10.91" target="#b6">[7]</ref>, using the images alone or the image-CUI pairs as input. Our best approaches ranked 2nd out of 11 participating teams for the Concept Detection subtask (with a small margin in terms of F1 score from the 1st team) and 6th out of 10 participating teams for the Caption Prediction subtask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Dataset</head><p>The data released for the ImageCLEFmedical Caption 2022 task correspond to a subset of the Radiology Objects in the COntext (ROCO) dataset <ref type="bibr" coords="2,335.88,403.03,11.58,10.91" target="#b2">[3]</ref>. This multimodal dataset contains image-caption pairs collected from open access biomedical journal articles in PubMedCentral<ref type="foot" coords="2,501.08,414.82,3.71,7.97" target="#foot_0">2</ref> . This dataset comprises several different medical imaging modalities, including Computed Tomography, Ultrasound, X-Ray, Fluoroscopy, Positron Emission Tomography, Mammography, Magnetic Resonance Imaging, Angiography and PET-CT. However, in this edition of the Caption Task, the modality information was not available to participants.</p><p>The dataset used for this challenge consists of 83,275 radiology images in the training set, 7,645 radiology images in the validation set, and 7,601 radiology images in the test set. Each image is paired with the associated caption and the set of extracted UMLS CUIs. The original split of the dataset is used throughout all our experiments and no additional sources are considered for both subtasks.</p><p>For the Concept Detection subtask, the set of CUIs assigned to each radiology image was extracted from the caption texts, based on the UMLS 2020 AB release. The organizers filtered them based on their semantic type and removed those with very low occurrences. This results in a total of 8,347 CUIs.</p><p>For the Caption Prediction subtask, the original captions of each image were pre-processed by the organizers as follows: (1) removing numbers and words containing numbers; (2) removing the punctuation; (3) applying lemmatization; and (4) converting the text to lower-case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this section we describe the methods implemented by our team for both ImageCLEFmedical Caption 2022 subtasks: Concept Detection and Caption Prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Concept Detection</head><p>We participated in the Concept Detection task with four different submissions. These can be categorized into two main approaches, as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Supervised Learning</head><p>Two different CNN architectures were considered and fine-tuned on the full set of CUIs contained in the training set. Namely, ResNet-152 <ref type="bibr" coords="3,259.66,250.98,12.69,10.91" target="#b3">[4]</ref> and DenseNet-201 <ref type="bibr" coords="3,355.63,250.98,12.69,10.91" target="#b4">[5]</ref> are the two CNN considered for this task. The task was framed as a multi-label classification task, where each label corresponds to a different CUI, resulting in a total of 8, 374 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Image Retrieval</head><p>Following last year's winning method <ref type="bibr" coords="3,258.86,327.41,11.34,10.91" target="#b5">[6]</ref>, we considered an image retrieval approach, where a set of CUIs were assigned to each query image (corresponding to an image in the validation and test set) based on the top ùëÅ most similar images from the training set and their associated CUIs.</p><p>More specifically, this was achieved by considering the following three steps. (1) A single DenseNet-201 was fine-tuned with supervised learning, as described in 3.1.1. (2) After discarding the final fully-connected layer, the CNN was used as image encoder, and the top ùëÅ most similar images to a query image were retrieved based on the cosine similarity between image embeddings. (3) An aggregation step was performed to select the set of CUIs to associate to each query image. This consisted in a soft majority voting, similar to the method proposed in <ref type="bibr" coords="3,89.29,449.35,11.52,10.91" target="#b5">[6]</ref>, where a CUI is attributed to the query image if it appears in at least 30% of the retrieved images. The proposed pipeline is shown in Figure <ref type="figure" coords="3,310.13,462.90,3.68,10.91" target="#fig_0">1</ref>. Differently from <ref type="bibr" coords="3,395.01,462.90,11.31,10.91" target="#b5">[6]</ref>, which considers only the CUIs appearing in at least 50% of the retrieved images, we aim to assign also less frequent CUIs by taking those that appear in at least 30% of retrieved images. Another difference from <ref type="bibr" coords="3,89.29,503.55,12.96,10.91" target="#b5">[6]</ref> and our method, is that (for simplicity) we only consider a single DenseNet-201 model to retrieve ùëÅ different images.</p><p>Alternatively, an ensemble of five DenseNet-201 models was considered, each fine-tuned using a different seed. The three steps described above were performed individually for each model. Finally, for each image, we assigned the union of each model's predicted CUIs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Caption Prediction</head><p>Our team participated to the Caption Prediction task with six submissions, all based on a CNN-Transformer approach, similar to previous works in radiology report generation <ref type="bibr" coords="3,472.15,621.02,16.40,10.91" target="#b12">[13,</ref><ref type="bibr" coords="3,491.27,621.02,12.30,10.91" target="#b13">14]</ref> the low frequency terms from the vocabulary, and c) the text post-processing -which consists of removing repeating words.</p><p>The chosen architecture consists of a ResNet-101 image encoder followed by a vanilla Transformer encoder-decoder <ref type="bibr" coords="4,203.40,372.00,11.59,10.91" target="#b6">[7]</ref>, composed of 3 attention layers (for both the encoder and the decoder), 8 heads, 512 hidden units and no pre-trained weights initialization, similar to the <ref type="bibr" coords="4,89.29,399.10,18.03,10.91" target="#b12">[13]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments &amp; Results</head><p>In this section, we describe the implementation details of the four submitted solutions for the Concept Detection subtask, and the six submissions for the Caption Prediction subtask. For each subtask, we highlight our best performing approach, and we show how it compares with other participating teams' solutions by presenting the final ranking.</p><p>Transformer Encoder-Decoder  ). The four CUIs in this example correspond to "X-Ray Computed Tomography", "Brain", "Abscess" and "Left frontal lobe structure", respectively.</p><formula xml:id="formula_0" coords="5,172.09,248.85,306.20,10.00">I 1 I 2 I N [SEP] W 1 W 2 W M I 3 W 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Concept Detection</head><p>The first two submitted solutions -#182230 and #182232 -consist of a DenseNet-201 and a ResNet-152 architecture, respectively. These are initialized with ImageNet <ref type="bibr" coords="5,433.55,433.86,18.07,10.91" target="#b15">[16]</ref> pre-trained weights and fine-tuned using a binary cross entropy loss, on the 8, 374 CUIs. The same set of hyperparameters was used for both submissions. The two models were trained for 20 epochs, and a batch size of 64 on a single NVIDIA RTX A5000 GPU. The initial learning rate was set to 10 -3 and was reduced by a factor of 0.5 when the F1 score, computed on the validation set, was not improving for 3 consecutive epochs. The input images are resized by matching the smaller edge to 224 pixels and maintaining the original aspect ratio. We applied the following data augmentation techniques: random horizontal flipping; and random crop of 224 √ó 224 pixels.</p><p>The third and fourth submissions -#182260 and #182324 -apply the image retrieval approach. Submission #182260 is a single DenseNet-201 fine-tuned with the same set of hyperparameters described above, discarding the final fully-connected layer. We then computed the cosine similarity between the image embedding in the test set and the training set (the training set is considered as our database), extracted from the last average pooling layer of DenseNet-201. The CUIs associated with the 50 images in the database with the highest similarity score were retrieved. Finally, an aggregation step consists of selecting only the CUIs which appeared in at least 30% of the retrieved images, which we named soft majority voting. Following these steps, we predicted which CUIs to attribute to each image in the test set. Similarly, submission #182324 consists of an ensemble of five different DenseNet-201, fine-tuned using different seeds. Differently to #182260, we retrieved 100 different images for each of the five networks. Next, the soft majority voting step was applied to each of them. Finally, we assigned the union of each model's predicted CUIs to each image. The results on the test set are shown in Table We can notice that our best submission was an image retrieval approach, by ensembling five different DenseNet-201 models. This follows the trend of last year's winning solution <ref type="bibr" coords="6,270.87,500.44,11.43,10.91" target="#b5">[6]</ref>.</p><p>We compare our top submission with other teams' submissions, considering also a Secondary F1 score, computed on a manually curated concept subset, corresponding to anatomy and image modality only. The results are shown in Table <ref type="table" coords="6,308.95,541.09,3.81,10.91" target="#tab_1">2</ref>. Our best submission ranked 2nd in this challenge, with a very small margin from the 1st position, in terms of F1 score <ref type="foot" coords="6,438.45,552.88,3.71,7.97" target="#foot_2">3</ref> . Furthermore, we note that our best method achieved the 4th best Secondary F1 score (the best among the top 3 ranked solution), meaning that it effectively managed to predict the anatomy and image modality corresponding CUIs along with other types of CUIs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Caption Prediction</head><p>We participated to the Caption Prediction task with six different submissions. These can be grouped into image-only (#182349, #182255 and #182273) and image + CUIs (#182350, #182342 and #182344), as described in section 3.2. For all models we considered the same set of hyperparameters. Specifically, we trained each model for 15 epochs using a batch size equal to 16 and Adam optimizer with weight decay <ref type="bibr" coords="7,269.48,307.67,16.31,10.91" target="#b16">[17]</ref>, with the initial learning rate set to 5 √ó 10 -5 for the visual encoder (ResNet-101) and 10 -4 for the remaining parameters. The learning rates were reduced by a factor of 0.8 when the BLEU-1 score <ref type="bibr" coords="7,333.10,334.77,11.33,10.91" target="#b7">[8]</ref>, computed on the validation set, did not improve for 3 consecutive epochs. Apart from the visual encoder, which was initialized using ImageNet pre-trained weights, the remaining parameters were initialized from a random uniform distribution. Another difference among the submissions is found in the text pre-processing stage of the target captions. This matches the post-processing steps performed on the predicted caption before computing the different caption metrics:</p><p>‚Ä¢ the caption text is converted to lower-case (this step is applied to all submissions); ‚Ä¢ all punctuation is removed; ‚Ä¢ stopwords are removed; ‚Ä¢ lemmatization is applied.</p><p>Finally, we applied a post-processing stage by noticing that our model suffers a commonly reported repetition problem -where a language model tends to unnecessarily repeat chunks of text (e.g. "... connect center femoral head center femoral head center femoral head..."). This is simply addressed by removing repeating words in the predicted captions.</p><p>The BLEU scores for each of our submissions are shown in Table <ref type="table" coords="7,406.59,561.19,3.81,10.91" target="#tab_2">3</ref>. It can be seen that the multimodal approach (image -CUIs) generally yields higher BLEU scores. Moreover, the model benefits from the pre-processing stage but not from removing repeating words. This last observation can be attributed to the fact that there are captions where the same word is repeated multiple times in the ground-truth. Therefore, a less naive approach should be considered to overcome the repetition problem. Overall, our best submission is #182342, which is a multimodal architecture where the pre-processing stage is applied but no post-processing.</p><p>Table <ref type="table" coords="7,127.55,656.03,5.17,10.91" target="#tab_3">4</ref> shows the final ranking of the Caption Prediction task. This is based on the BLEU score, and our best submission ranked 6th. However, by looking at the other caption metrics we notice that we ranked 1st in terms of ROUGE score <ref type="bibr" coords="8,339.20,300.04,13.00,10.91" target="#b8">[9]</ref> and 2nd when considering SPICE <ref type="bibr" coords="8,89.29,313.59,17.77,10.91" target="#b17">[18]</ref> and the BERTScore <ref type="bibr" coords="8,196.76,313.59,16.09,10.91" target="#b18">[19]</ref>. This shows how problematic it is to evaluate captioning methods, and it is usually good practice to keep track of multiple metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This manuscript presents our proposed solutions for the ImageCLEFmedical Caption 2022 task.</p><p>In particular, we applied well known techniques in the field of image classification (supervised learning and image retrieval) and image captioning, and showed how they yield promising results in the medical domain. Our best submission ranked 2nd in the Concept Detection subtask, showing a good balance between detecting the full set of CUIs (determined using the F1 score) and the CUIs associated with the image modality and the anatomy (determined from the Secondary F1 score). Furthermore, we ranked 6th in the Caption Prediction subtask, based on the BLEU score, but we highlighted how the ranking can vary considerably depending on the metric.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,257.85,416.70,8.93;4,88.93,269.86,418.71,8.87;4,89.29,281.81,416.70,8.87;4,88.99,293.72,407.66,8.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Diagram of the proposed image retrieval pipeline, using a single DenseNet-201 model, which was first fine-tuned on the full set of CUIs. The datatbase images correspond to the full ImageCLEFmedical Caption 2022 training set. The images are taken from the ImageCLEFmedical Caption 2022 dataset (top image: CC BY [Shimoyama et al. (2017)] [11]; bottom image: CC BY [Alnofal et al. (2021)] [12]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,110.04,399.10,397.62,10.91;4,89.29,412.65,416.69,10.91;4,88.96,426.20,417.30,10.91;4,89.29,439.75,416.69,10.91;4,89.29,453.29,416.69,10.91;4,89.02,466.84,418.64,10.91;4,88.91,480.39,417.07,10.91;4,89.02,493.94,418.81,10.91;4,89.29,507.49,417.10,10.91;4,89.29,521.04,416.69,10.91;4,89.29,534.59,416.69,10.91;4,89.29,548.14,82.47,10.91"><head></head><label></label><figDesc>baseline method. The image encoder extracts a 49 √ó 2048 feature map from each image. Each of the ùëÅ = 49 latent vectors is considered to be an image token which is inputted to the Transformer. In the multimodal setup, where the CUIs extracted in the Concept Detection task are considered as additional input, the extracted CUIs are concatenated into a single string and tokenized into ùëÄ text tokens (one for each CUI). The textual tokens are based on a custom-built vocabulary, where each token corresponds to a word or a CUI appearing in the training set. A [ùëÜùê∏ùëÉ ] token is used to separate the two input modalities. Both the textual (CUIs) and the visual inputs are projected into the input embedding space and summed with the related positional and segment embedding; the segment embedding is then available to the model to allow distinction of textual and visual inputs. The model is treated as a language model, following the sequence-to-sequence paradigm. The multimodal Encoder-Decoder Transformer architecture is shown in Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,172.70,219.98,4.94,8.25;5,202.31,219.98,4.94,8.25;5,231.93,219.98,4.94,8.25;5,457.97,220.51,26.71,7.56;5,172.57,234.79,5.19,8.25;5,202.19,234.79,5.19,8.25;5,231.80,234.79,5.19,8.25;5,291.03,234.79,5.19,8.25;5,320.65,234.79,5.19,8.25;5,350.26,234.79,5.19,8.25;5,379.88,234.79,5.19,8.25;5,409.49,234.79,5.19,8.25;5,468.72,234.79,5.19,8.25;5,172.70,190.36,4.94,8.25;5,202.31,190.36,4.94,8.25;5,350.39,190.36,4.94,8.25;5,468.85,190.36,4.94,8.25;5,231.93,190.36,4.94,8.25;5,380.01,190.36,4.94,8.25;5,409.62,190.36,4.94,8.25;5,291.16,190.36,4.94,8.25;5,320.78,190.36,4.94,8.25;5,172.57,205.17,5.19,8.25;5,202.19,205.17,5.19,8.25;5,231.80,205.17,5.19,8.25;5,320.65,205.17,5.19,8.25;5,350.26,205.17,5.19,8.25;5,379.88,205.17,5.19,8.25;5,291.03,205.17,5.19,8.25;5,409.49,205.17,5.19,8.25;5,468.72,205.17,5.19,8.25;5,98.92,190.25,44.45,10.31;5,98.92,219.87,39.51,10.31;5,98.92,249.48,29.64,10.31;5,134.12,88.93,326.44,8.25;5,290.42,219.98,6.42,8.25;5,314.97,219.98,105.39,8.25;5,322.67,289.57,171.42,8.25;5,210.70,290.31,47.41,8.25"><head></head><label></label><figDesc>of the brain show abscess in the left frontal lobe arrow N N+1 N+2 N+3 N+4 C0040405;C0006104;C0000833;C0228194 ResNet-101</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,89.29,326.28,416.69,8.93;5,89.29,338.28,416.69,8.87;5,89.29,350.24,416.69,8.87;5,89.29,362.19,95.21,8.87"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Multimodal Encoder-Decoder Transformer. The image, CUIs and caption are taken from the ImageCLEFmedical Caption 2022 dataset (CC BY [Wang et al. (2021)] [15]). The four CUIs in this example correspond to "X-Ray Computed Tomography", "Brain", "Abscess" and "Left frontal lobe structure", respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,88.99,90.49,338.29,92.87"><head>Table 1</head><label>1</label><figDesc>F1 scores on the test set, for each of our submitted solutions.</figDesc><table coords="6,167.99,121.19,259.30,62.17"><row><cell>Run</cell><cell>Approach</cell><cell>Network</cell><cell>F1 Score</cell></row><row><cell cols="2">#182230 Supervised Learning</cell><cell>DenseNet-201</cell><cell>0.443</cell></row><row><cell cols="2">#182232 Supervised Learning</cell><cell>ResNet-152</cell><cell>0.440</cell></row><row><cell>#182260</cell><cell>Image Retrieval</cell><cell>DenseNet-201</cell><cell>0.446</cell></row><row><cell>#182324</cell><cell>Image Retrieval</cell><cell>5√ó DenseNet-201</cell><cell>0.451</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.99,205.61,417.22,200.38"><head>Table 2</head><label>2</label><figDesc>F1 and Secondary F1 scores on the test set for each team's best solution. The ranking is based on the F1 Score. For both metrics, we highlight in bold the best score and underline the second best score.</figDesc><table coords="6,162.07,249.18,271.14,156.81"><row><cell>Team</cell><cell>Run</cell><cell cols="3">F1 Score Secondary F1 Rank</cell></row><row><cell cols="2">AUEB-NLP-Group #182358</cell><cell>0.451</cell><cell>0.791</cell><cell>1</cell></row><row><cell>Ours</cell><cell>#182324</cell><cell>0.451</cell><cell>0.822</cell><cell>2</cell></row><row><cell>CSIRO</cell><cell>#182343</cell><cell>0.447</cell><cell>0.794</cell><cell>3</cell></row><row><cell>eecs-kth</cell><cell>#181750</cell><cell>0.436</cell><cell>0.855</cell><cell>4</cell></row><row><cell>vcmi</cell><cell>#182097</cell><cell>0.433</cell><cell>0.863</cell><cell>5</cell></row><row><cell>PoliMi-ImageClef</cell><cell>#182296</cell><cell>0.432</cell><cell>0.851</cell><cell>6</cell></row><row><cell cols="2">SSNSheerinKavitha #181995</cell><cell>0.418</cell><cell>0.654</cell><cell>7</cell></row><row><cell>IUST_NLPLAB</cell><cell>#182307</cell><cell>0.398</cell><cell>0.673</cell><cell>8</cell></row><row><cell>Morgan_CS</cell><cell>#182150</cell><cell>0.352</cell><cell>0.628</cell><cell>9</cell></row><row><cell>kdelab</cell><cell>#182346</cell><cell>0.310</cell><cell>0.412</cell><cell>10</cell></row><row><cell>SDVA-UCSD</cell><cell>#181691</cell><cell>0.308</cell><cell>0.552</cell><cell>11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,88.99,90.49,351.26,116.93"><head>Table 3</head><label>3</label><figDesc>BLEU scores on the test set, for each our submitted solutions.</figDesc><table coords="7,155.02,121.19,285.24,86.24"><row><cell>Run</cell><cell>Input</cell><cell cols="3">Pre-Processing Post-Processing BLEU</cell></row><row><cell>#182349</cell><cell>Image</cell><cell></cell><cell></cell><cell>0.271</cell></row><row><cell>#182255</cell><cell>Image</cell><cell>‚úì</cell><cell></cell><cell>0.280</cell></row><row><cell>#182273</cell><cell>Image</cell><cell>‚úì</cell><cell>‚úì</cell><cell>0.276</cell></row><row><cell cols="2">#182350 Image + CUIs</cell><cell></cell><cell></cell><cell>0.278</cell></row><row><cell cols="2">#182342 Image + CUIs</cell><cell>‚úì</cell><cell></cell><cell>0.291</cell></row><row><cell cols="2">#182344 Image + CUIs</cell><cell>‚úì</cell><cell>‚úì</cell><cell>0.285</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,88.99,90.49,418.52,182.99"><head>Table 4</head><label>4</label><figDesc>Caption scores computed on the test set for each teams' best solution. The ranking is based on the BLEU score. For all the metrics, we highlight in bold the best score and underline the second best score.</figDesc><table coords="8,95.05,133.85,405.18,139.62"><row><cell>Team</cell><cell>Run</cell><cell cols="7">BLEU ROUGE METEOR CIDEr SPICE BERTScore Rank</cell></row><row><cell>IUST_NLPLAB</cell><cell cols="2">#182275 0.483</cell><cell>0.142</cell><cell>0.093</cell><cell>0.030</cell><cell>0.007</cell><cell>0.561</cell><cell>1</cell></row><row><cell cols="3">AUEB-NLP-Group #181853 0.322</cell><cell>0.166</cell><cell>0.074</cell><cell>0.190</cell><cell>0.031</cell><cell>0.599</cell><cell>2</cell></row><row><cell>CSIRO</cell><cell cols="2">#182268 0.311</cell><cell>0.197</cell><cell>0.084</cell><cell>0.269</cell><cell>0.046</cell><cell>0.623</cell><cell>3</cell></row><row><cell>vcmi</cell><cell cols="2">#182325 0.306</cell><cell>0.174</cell><cell>0.075</cell><cell>0.205</cell><cell>0.036</cell><cell>0.604</cell><cell>4</cell></row><row><cell>eecs-kth</cell><cell cols="2">#182337 0.292</cell><cell>0.116</cell><cell>0.062</cell><cell>0.132</cell><cell>0.022</cell><cell>0.573</cell><cell>5</cell></row><row><cell>Ours</cell><cell cols="2">#182342 0.291</cell><cell>0.201</cell><cell>0.082</cell><cell>0.256</cell><cell>0.046</cell><cell>0.610</cell><cell>6</cell></row><row><cell>kdelab</cell><cell cols="2">#182351 0.278</cell><cell>0.158</cell><cell>0.074</cell><cell cols="2">0.411 0.051</cell><cell>0.600</cell><cell>7</cell></row><row><cell>Morgan_CS</cell><cell cols="2">#182238 0.255</cell><cell>0.144</cell><cell>0.056</cell><cell>0.148</cell><cell>0.023</cell><cell>0.583</cell><cell>8</cell></row><row><cell>MAI_ImageSem</cell><cell cols="2">#182105 0.221</cell><cell>0.185</cell><cell>0.067</cell><cell>0.251</cell><cell>0.039</cell><cell>0.606</cell><cell>9</cell></row><row><cell cols="3">SSNSheerinKavitha #182248 0.160</cell><cell>0.043</cell><cell>0.027</cell><cell>0.017</cell><cell>0.007</cell><cell>0.545</cell><cell>10</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,108.93,671.00,185.08,8.97"><p>https://www.ncbi.nlm.nih.gov/pmc/ [last accessed:</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1" coords="2,296.63,671.00,40.49,8.97"><p>30.06.2022]   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,108.93,649.05,397.29,8.97;6,89.29,660.01,416.69,8.97;6,89.29,670.97,299.97,8.97"><p>All the scores shown in our tables are rounded to the third decimal place, therefore, two submissions may appear to have the same score. A more detailed list of the best scores of each team can be seen on the challenge web page: https://www.imageclef.org/2022/medical/caption [last accessed: 30.06.2022].</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,552.78,395.17,10.91;8,111.81,566.33,395.37,10.91;8,112.66,579.88,393.33,10.91;8,112.66,593.43,216.46,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,326.22,566.33,180.96,10.91;8,112.66,579.88,181.70,10.91">Overview of ImageCLEFmedical 2022 -Caption Prediction and Concept Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garc√≠a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,316.25,579.88,189.74,10.91;8,112.66,593.43,97.38,10.91">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,606.98,393.33,10.91;8,112.66,620.53,258.51,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,188.99,606.98,316.99,10.91;8,112.66,620.53,51.86,10.91">The unified medical language system (UMLS): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,173.16,620.53,98.78,10.91">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="D270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,634.08,393.33,10.91;8,112.33,647.63,393.66,10.91;9,112.66,86.97,394.53,10.91;9,112.66,100.52,123.33,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,370.24,634.08,135.74,10.91;8,112.33,647.63,163.41,10.91">Radiology Objects in COntext (ROCO): a multimodal image dataset</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,298.74,647.63,207.25,10.91;9,112.66,86.97,389.95,10.91">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,114.06,395.17,10.91;9,112.66,127.61,395.01,10.91;9,112.41,141.16,38.81,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,259.74,114.06,203.38,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,488.38,114.06,19.45,10.91;9,112.66,127.61,347.24,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,154.71,395.17,10.91;9,112.66,168.26,393.33,10.91;9,112.66,181.81,147.08,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,377.68,154.71,130.16,10.91;9,112.66,168.26,67.72,10.91">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,203.84,168.26,302.14,10.91;9,112.66,181.81,49.16,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,195.36,393.33,10.91;9,112.66,208.91,393.33,10.91;9,112.14,222.46,295.31,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,478.16,195.36,27.83,10.91;9,112.66,208.91,198.10,10.91">AUEB NLP group at ImageCLEFmed caption tasks</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Charalampakos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,356.31,208.91,114.73,10.91">CLEF2021 Working Notes</title>
		<title level="s" coord="9,478.71,208.91,27.27,10.91;9,112.14,222.46,149.86,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,236.01,395.17,10.91;9,112.66,249.56,393.33,10.91;9,112.33,263.11,29.19,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,484.04,236.01,23.79,10.91;9,112.66,249.56,143.41,10.91">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">≈Å</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,264.71,249.56,228.49,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Polosukhin</note>
</biblStruct>

<biblStruct coords="9,112.66,276.66,393.32,10.91;9,112.66,290.20,393.53,10.91;9,112.66,303.75,203.57,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,308.17,276.66,197.81,10.91;9,112.66,290.20,88.86,10.91">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,224.89,290.20,281.30,10.91;9,112.66,303.75,116.04,10.91">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,317.30,395.17,10.91;9,112.66,330.85,153.57,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,156.44,317.30,254.62,10.91">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,434.41,317.30,73.43,10.91;9,112.66,330.85,76.13,10.91">Text summarization branches out</title>
		<imprint>
			<biblScope unit="page" from="74" to="81" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,344.40,395.01,10.91;9,112.66,357.95,395.17,10.91;9,112.39,371.50,394.80,10.91;9,112.66,385.05,394.62,10.91;9,112.66,398.60,393.33,10.91;9,112.66,412.15,395.17,10.91;9,112.66,425.70,393.54,10.91;9,112.66,439.25,170.14,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,352.50,385.05,154.78,10.91;9,112.66,398.60,311.34,10.91">Overview of the ImageCLEF 2022: Multimedia Retrieval in Medical, Social Media and Nature Applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>≈ûtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,446.87,398.60,59.11,10.91;9,112.66,412.15,395.17,10.91;9,112.66,425.70,239.58,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<title level="s" coord="9,359.20,425.70,147.00,10.91;9,112.66,439.25,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,452.79,393.33,10.91;9,112.66,466.34,393.98,10.91;9,112.41,479.89,18.52,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,437.48,452.79,68.51,10.91;9,112.66,466.34,261.41,10.91">Bacillus cereus pneumonia in an immunocompetent patient: a case report</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shimoyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Umegaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Agui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kadono</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Minami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,382.35,466.34,86.45,10.91">JA Clinical Reports</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,493.44,393.32,10.91;9,112.66,506.99,393.32,10.91;9,112.66,520.54,112.88,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,310.85,493.44,195.13,10.91;9,112.66,506.99,393.32,10.91;9,112.66,520.54,27.79,10.91">Spontaneous Subcutaneous Emphysema and Pneumomediastinum Associated With Influenza B Virus in a Young Male Adult: A Case Report</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename><surname>Alnofal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Alshadely</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Khatib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,149.01,520.54,31.73,10.91">Cureus</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,534.09,393.33,10.91;9,112.66,547.64,393.33,10.91;9,112.66,561.19,394.53,10.91;9,112.66,574.74,65.30,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,289.07,534.09,216.91,10.91;9,112.66,547.64,51.35,10.91">Generating radiology reports via memory-driven transformer</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,189.11,547.64,316.87,10.91;9,112.66,561.19,329.57,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1439" to="1449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,588.29,393.33,10.91;9,112.66,601.84,393.53,10.91;9,112.30,615.39,245.57,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,268.78,588.29,237.21,10.91;9,112.66,601.84,133.18,10.91">Exploring and distilling posterior and prior knowledge for radiology report generation</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,267.78,601.84,238.41,10.91;9,112.30,615.39,137.31,10.91">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13753" to="13762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,628.93,395.17,10.91;9,112.66,642.48,393.57,10.91;9,112.33,656.03,29.19,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,325.02,628.93,182.81,10.91;9,112.66,642.48,132.69,10.91">Invasive Aspergillus rhinosinusitis complicated with cerebral abscess</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">V</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>Radhakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,254.01,642.48,239.08,10.91">Revista da Sociedade Brasileira de Medicina Tropical</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,669.58,393.33,10.91;10,112.66,86.97,394.53,10.91;10,112.66,100.52,103.61,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,346.64,669.58,159.35,10.91;10,112.66,86.97,67.28,10.91">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,228.08,86.97,274.55,10.91">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,114.06,394.83,10.91" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<title level="m" coord="10,234.17,114.06,163.28,10.91">A method for stochastic optimization</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Poster</note>
</biblStruct>

<biblStruct coords="10,112.66,127.61,393.33,10.91;10,112.66,141.16,395.01,10.91;10,112.66,154.71,38.81,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,336.97,127.61,169.01,10.91;10,112.66,141.16,83.01,10.91">SPICE: Semantic propositional image caption evaluation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,224.73,141.16,188.94,10.91">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,168.26,393.33,10.91;10,112.66,181.81,390.74,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,377.87,168.26,128.12,10.91;10,112.66,181.81,96.10,10.91">BERTScore: Evaluating text generation with BERT</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,232.88,181.81,240.50,10.91">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
