<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,373.73,15.42;1,89.29,106.66,381.13,15.42;1,89.29,128.58,305.13,15.43">CSIRO at the ImageCLEFmed 2022 Tuberculosis Caverns Detection Challenge: A 2D and 3D Deep Learning Detection Network Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,159.15,54.23,11.96"><forename type="first">Bowen</forename><surname>Xin</surname></persName>
							<email>bowen.xin@csiro.au</email>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution Australian e-Health Research Centre</orgName>
								<orgName type="institution">Commonwealth Scientific and Industrial Research Organisation</orgName>
								<address>
									<postCode>4006</postCode>
									<settlement>Herston</settlement>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,159.01,159.15,49.39,11.96"><forename type="first">Hang</forename><surname>Min</surname></persName>
							<email>hang.min@csiro.au</email>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution Australian e-Health Research Centre</orgName>
								<orgName type="institution">Commonwealth Scientific and Industrial Research Organisation</orgName>
								<address>
									<postCode>4006</postCode>
									<settlement>Herston</settlement>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,223.88,159.15,84.98,11.96"><forename type="first">Ashley</forename><forename type="middle">G</forename><surname>Gillman</surname></persName>
							<email>ashley.gillman@csiro.au</email>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution Australian e-Health Research Centre</orgName>
								<orgName type="institution">Commonwealth Scientific and Industrial Research Organisation</orgName>
								<address>
									<postCode>4006</postCode>
									<settlement>Herston</settlement>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,320.12,159.15,77.11,11.96"><forename type="first">Bevan</forename><surname>Koopman</surname></persName>
							<email>bevan.koopman@csiro.au</email>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution Australian e-Health Research Centre</orgName>
								<orgName type="institution">Commonwealth Scientific and Industrial Research Organisation</orgName>
								<address>
									<postCode>4006</postCode>
									<settlement>Herston</settlement>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,409.49,159.15,71.58,11.96"><forename type="first">Jason</forename><surname>Dowling</surname></persName>
							<email>jason.dowling@csiro.au</email>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution Australian e-Health Research Centre</orgName>
								<orgName type="institution">Commonwealth Scientific and Industrial Research Organisation</orgName>
								<address>
									<postCode>4006</postCode>
									<settlement>Herston</settlement>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,88.87,173.10,76.54,11.96"><forename type="first">Aaron</forename><surname>Nicolson</surname></persName>
							<email>aaron.nicolson@csiro.au</email>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution Australian e-Health Research Centre</orgName>
								<orgName type="institution">Commonwealth Scientific and Industrial Research Organisation</orgName>
								<address>
									<postCode>4006</postCode>
									<settlement>Herston</settlement>
									<region>Queensland</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,373.73,15.42;1,89.29,106.66,381.13,15.42;1,89.29,128.58,305.13,15.43">CSIRO at the ImageCLEFmed 2022 Tuberculosis Caverns Detection Challenge: A 2D and 3D Deep Learning Detection Network Approach</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">0531D72574039112F8E3AFF2A3821A9B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Tuberculosis caverns detection</term>
					<term>Retina U-Net</term>
					<term>YOLO</term>
					<term>Computed Tomography</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tuberculosis (TB) is one of the leading causes of death worldwide. Automated detection of lung caverns associated with TB in Computed Tomography (CT) could help clinicians optimise treatment. However, caverns detection on 3D CT data is challenging due to the curse of dimensionality, thus requiring larger training data and more computational resource. Our team (AEHRC CSIRO) participated in ImageCLEFmed TB caverns detection 2022 to address this challenge, by developing a 2D YOLO-based model (TBdet-2D), and an efficient 3D Retina-U-Net-based model (TBdet-3D). Both networks were trained on 559 CT data with data augmentation and tested on 140 data provided by the challenge. The results show that TBdet-3D (mAP_IoU 0.504) outperformed TBdet-2D model (mAP_IoU 0.308) on testing data, indicating that employing a 3D approach instead of a 2D approach is more appropriate for the task. Our team placed first among the participating teams in this challenge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Tuberculosis (TB) is one of the leading causes of death globally, leading to approximately 1.7 million deaths in 2016, according to a World Health Organisation report <ref type="bibr" coords="1,421.85,488.04,11.58,10.91" target="#b0">[1]</ref>. It is a bacteria disease caused by a germ named Mycobacterium tuberculosis, which can cause lung tissue inflammation and subsequently lead to lung caverns <ref type="bibr" coords="1,327.24,515.14,11.52,10.91" target="#b1">[2]</ref>. Precise detection of lung caverns in Computed Tomography (CT) imaging is an important clinical task, because it contributes to the diagnoses of the TB sub-type. This helps with optimising treatment, which is different for each TB sub-type. Even after successful treatment, the detection of lung cavern regions is of importance because the cavern may contain colonies of Mycobacterium tuberculosis that could result in unpredictable disease relapse. Detection of these lung cavern regions, is generally performed by expert radiologists through manual measurements often on 2D planes <ref type="bibr" coords="2,120.88,114.06,11.64,10.91" target="#b2">[3]</ref>; however, this manual process can be time-consuming, experience-dependant, and with limited reproducibility due to inter-observer variability. Thus, there is an urgent clinical demand to develop an automated detection algorithm for lung caverns associated with TB on CT imaging.</p><p>There have been advancements in automated object detection for natural images, mostly for 2D images. These methods can be generally categorised into two-stage and one-stage detection mechanisms. Two-stage mechanisms (e.g., R-CNN <ref type="bibr" coords="2,312.68,195.36,11.98,10.91" target="#b3">[4]</ref>) generate a sparse set of object locations in the first stage, and classify each candidate location as either foreground or background in the second stage. Algorithms along this line include fast R-CNN <ref type="bibr" coords="2,390.30,222.46,13.00,10.91" target="#b4">[5]</ref> and faster R-CNN <ref type="bibr" coords="2,492.22,222.46,11.58,10.91" target="#b5">[6]</ref>. One-stage mechanisms aim to simplify the process while retaining the accuracy through dense sampling of object scales, locations and aspect ratios. Such examples, including YOLO <ref type="bibr" coords="2,481.46,249.56,11.48,10.91" target="#b6">[7,</ref><ref type="bibr" coords="2,495.70,249.56,7.65,10.91" target="#b7">8]</ref>, SSD <ref type="bibr" coords="2,110.02,263.11,12.74,10.91" target="#b8">[9]</ref> and RetinaNet <ref type="bibr" coords="2,191.12,263.11,16.16,10.91" target="#b9">[10]</ref>, are relatively faster with matching performance to their two-stage counterparts.</p><p>Cross-sectional imaging, such as Computed Tomography (CT), is an important tool in radiology that makes use of not only length and width, but also depth. This captures comprehensive spatial information (such as shape, geometry and location) for the detection task; however, the limitation is that processing such 3D data can be computationally expensive and data hungry due to the curse of dimensionality <ref type="bibr" coords="2,242.53,344.40,16.17,10.91" target="#b10">[11]</ref>. An alternative way to process 3D data is to operate on individual 2D slices with a 2D detection network. However, this ignores the contextual information between slices. This issue could be mitigated by considering 2D slices along different planes (e.g., sagittal and coronal planes). Furthermore, recent advancements in 3D one-stage detection networks, such as Retina U-net <ref type="bibr" coords="2,272.22,398.60,16.13,10.91" target="#b11">[12]</ref>, also bring potential to achieve higher speed and accuracy at the same time.</p><p>In this work, we developed a 2D model (TBdet-2D) and a 3D model (TBdet-3D) for TB caverns detection on CT images in the imageCLEF 2022 challenge <ref type="bibr" coords="2,356.77,439.25,16.41,10.91" target="#b12">[13]</ref>. For the 2D model, YOLOv5 networks <ref type="bibr" coords="2,133.96,452.79,17.96,10.91" target="#b13">[14]</ref> were trained on axial and coronal slices of the 3D CT image. The 2D bounding boxes were then merged into 3D bounding boxes. For the 3D model, we trained a 3D Retina U-Net network <ref type="bibr" coords="2,156.72,479.89,16.09,10.91" target="#b11">[12]</ref>, which leverages both the model simplicity of RetinaNet and the supervision segmentation signal from U-Net. In addition, we reduced the false positive rate by plane-based bounding box merging. Comparison of the 2D and 3D models was carried out on the TB-CT dataset provided by the Image-CLEF 2022 Caverns Detection challenge <ref type="bibr" coords="2,407.79,520.54,16.26,10.91" target="#b14">[15]</ref>. In this challenge, our 3D model achieved the 1st place with a mean averaged precision (mAP) of 0.504 (intersection over union (IoU) âˆˆ [0.40, 0.75]) on the testing data. Our 2D model achieved the second best mAP of 0.308.</p><p>We summarised our contributions as follows:</p><p>â€¢ We developed TBdet-2D, a 2D TB caverns region detector using YOLOv5 and an efficient 2D bounding box merging technique based on connected component labelling <ref type="bibr" coords="2,467.16,610.80,16.25,10.91" target="#b15">[16]</ref>. â€¢ We developed TBdet-3D, a 3D TB cavern region detector based on Retina U-Net, equipped with a false positive reduction module that uses plane-based bounding box (PBB) merging. â€¢ We provide a comparison between the 2D and 3D approaches on the ImageCLEFmed TB Caverns Detection 2022 challenge. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data</head><p>The dataset for ImageCLEFmed TB Caverns Detection 2022 contains 559 3D CT images for training and 140 3D CT images for testing. Each CT image has 512Ã—512 pixels per slice and an average of approximately 100 slices, as well as its associated metadata, including the dimensions of the image, voxel spacing, etc. The raw voxel intensities for each image are stored in Housfield units. The lung masks were automatically segmented by a three-stage technique <ref type="bibr" coords="3,465.15,448.27,16.41,10.91" target="#b16">[17]</ref>, and provided for each CT image by the challenge organisers. The targets in the challenge were bounding boxes of cavern regions within the CT images. The target 3D bounding boxes were extracted using manual segmentation masks delineated by radiologists. Data examples are shown in Figure <ref type="figure" coords="3,164.29,502.46,3.74,10.91" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Metrics</head><p>The detection task was evaluated using the mean average precision (mAP) over a series of IoU thresholds. The IoU between a predicted bounding box (predBB) and the ground truth bounding box (gtBB) is defined as:</p><formula xml:id="formula_0" coords="3,240.17,591.17,265.81,24.43">ğ¼ğ‘œğ‘ˆ = ğ‘ğ‘Ÿğ‘’ğ‘‘ğµğµ âˆ© ğ‘”ğ‘¡ğµğµ ğ‘ğ‘Ÿğ‘’ğ‘‘ğµğµ âˆª ğ‘”ğ‘¡ğµğµ<label>(1)</label></formula><p>For each IoU threshold t, the average precision (AP) is computed using the formula:</p><formula xml:id="formula_1" coords="3,216.67,645.99,289.32,24.43">ğ´ğ‘ƒ (ğ‘¡) = ğ‘‡ ğ‘ƒ (ğ‘¡) ğ‘‡ ğ‘ƒ (ğ‘¡) + ğ¹ ğ‘ƒ (ğ‘¡) + ğ¹ ğ‘ (ğ‘¡)<label>(2)</label></formula><p>where a true positive (TP) is counted if the computed IoU is greater than the threshold t (ğ¼ğ‘œğ‘ˆ &gt; ğ‘¡); a false positive (FP) is counted if a predBB has no associated gtBB with ğ¼ğ‘œğ‘ˆ &gt; ğ‘¡; a false negative (FN) is counted if a gtBB has no associated predBB with ğ¼ğ‘œğ‘ˆ &gt; ğ‘¡. The final mean average precision was defined as the mean of ğ´ğ‘ƒ (ğ‘¡) sweeping over a range of IoU thresholds ğ‘¡ where ğ‘¡ âˆˆ (0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75) in this task. The evaluator was implemented in python and provided at https://github.com/SergeKo/clef2022-caverns-detection-metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this work, we consider two separate methodologies and associated network topologies. The first, TBdet-2D presented in Â§3.1, is based on the YOLO 2D object detection network, which frames the 3D detection task as a series of 2D detection tasks to be subsequently aggregated.</p><p>The second approach, TBdet-3D presented in Â§3.2, instead is built upon an efficient 3D one-stage network, Retina U-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">TBdet-2D</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">YOLOv5 network</head><p>The TBdet-2D approach adopted the recent version of the YOLO <ref type="bibr" coords="4,379.27,337.83,12.87,10.91" target="#b6">[7]</ref> object detection network YOLOv5. The general concept of YOLO networks is as follows: The network applies a single convolution network on the image and divides the image into grid cells. Each grid cell predicts the object class probabilities and a number of bounding boxes. Each bounding box is represented by parameters including the confidence score of object presence, centre coordinates, height and width. Finally, the network utilises non-maximum suppression to remove overlapping bounding boxes with low confidence scores. Compared with the original YOLO structure, YOLOv5 incorporated the cross stage partial network <ref type="bibr" coords="4,340.79,432.68,18.06,10.91" target="#b17">[18]</ref> into the backbone to addresses the duplicate gradient issue, and the path aggregation network <ref type="bibr" coords="4,381.20,446.22,18.07,10.91" target="#b18">[19]</ref> as the neck for feature fusion to improve information flow and localisation accuracy. The loss function of YOLOv5 is a combination of bounding box regression loss, binary cross entropy for object presence and cross entropy for classification loss. YOLOv5 has a model ensembling feature which allows the aggregation of predictions from multiple base models to improve generalisation on unseen data. YOLOv5 has a few variants of different levels of complexity and size. For this challenge, we chose the YOLOv5s6 network, which is a small YOLOv variant with 12.6M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Input image pre-processing</head><p>For the TBdet-2D method, the axial and coronal slices were used for caverns detection. The pre-processing stage is illustrated in Figure <ref type="figure" coords="4,285.66,590.40,3.77,10.91" target="#fig_1">2</ref>. The 2D slices were cropped with the bounding box of the lung masks provided by the organisers. Since the YOLO network accepts 3-channel RGB images as input, we created a pseudo-RGB input which consists of the target slice in the middle and the two adjacent slices in the other two channels. The input image is linearly normalised to 8bit between 0 and 255 via min-max normalisation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Network training</head><p>The training process of the YOLO networks is outlined in Figure <ref type="figure" coords="5,394.72,390.25,3.81,10.91" target="#fig_2">3</ref>. To train the YOLOv5 network, a 5-fold cross validation was carried out on the training data, where the cases were randomly partitioned into 5 folds with 1 fold for validation and the rest folds for training in each iteration. The YOLO networks were trained on the axial and coronal slices separately.</p><p>Given the large number of axial and coronal slices in the CT images, we only selected a subset of the slices for training. The sampling strategy is as follows: for slices intersecting with the cavern ground truth bounding box, only the middle 1 4 to 3 4 slices were used; for slices which do not intersect with the cavern bounding box, 10 samples evenly spaced across all slices were selected. During each training iteration, the model achieving the best performance metric (a weighted sum of precision, recall, mAP (ğ¼ğ‘œğ‘ˆ = 0.5) and mAP (ğ¼ğ‘œğ‘ˆ âˆˆ [0.5, 0.95])) on the validation set was saved. Within the 5-fold cross validation, five YOLO-axial and YOLO-coronal models were generated for caverns detection on axial and coronal slices respectively. Each YOLO network was trained for 200 epochs with a batch size of 12 and an image size of 1280 Ã— 1280. Augmentations including translation, scaling, flip and mosaic augmentation <ref type="bibr" coords="5,462.89,566.39,18.06,10.91" target="#b19">[20]</ref> were applied during training. Stochastic gradient descent (SGD) was used as the optimiser with an initial learning rate (LR) of 0.01, a momentum of 0.937 and a weight decay of 0.0005. The one-cycle LR scheduler <ref type="bibr" coords="5,195.12,607.04,17.91,10.91" target="#b20">[21]</ref> was also applied with a maximum and minimum LR of 0.001 and 0.001 Ã— 0.001 respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4.">Cavern detection on testing data</head><p>After the YOLO networks were established on the training data, the networks were applied on the testing data to generate 2D detections as shown in Figure <ref type="figure" coords="6,370.47,314.07,10.26,10.91" target="#fig_4">4a</ref> and then the 2D detections were merged into 3D detections through connected component labelling as shown in Figure <ref type="figure" coords="6,494.87,327.62,8.41,10.91" target="#fig_4">4b</ref>.</p><p>YOLO detection on testing data When using the YOLO models to perform caverns detection, the ensemble model (as described in section 3.1.1) of the 5 YOLO-axial models from the 5-fold cross validation were firstly applied onto the axial slices within the lung masks from the testing images. Then the ensemble model of the 5 YOLO-coronal models was applied onto the coronal slices that intersect with the axial detections as shown in Figure <ref type="figure" coords="6,377.23,410.53,8.31,10.91" target="#fig_4">4a</ref>. The confidence threshold for both steps was set as 0.1. The detections generated on the cropped slices (with the lung region) were finally matched back to the original full-size slices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Merging 2D detections into 3D detections</head><p>After the detections were generated on the axial and coronal slices, the 2D detections were merged into 3D ones via connected component labelling. The general merging process is as follows: 1) Given a set of 2D detections ğ· = {ğ‘‘ ğ‘› |ğ‘› = 1, ..., ğ‘ } on slices ğ‘† = {ğ‘  ğ‘› |ğ‘› = 1, ..., ğ‘ } with prediction probabilities of ğ‘ƒ = {ğ‘ ğ‘› |ğ‘› = 1, ..., ğ‘ } and a 3D binary mask ğ‘€ to capture the merged detections, the voxels within the bounding box represented by ğ‘‘ ğ‘› on slice ğ‘  ğ‘› were set as 1 on ğ‘€ if ğ‘ ğ‘› â‰¥ ğ‘‡ â„1 (ğ‘‡ â„1 is the slice probability threshold). 2) A connected component analysis was carried out on ğ‘€ resulting in a set of connected components ğ¶ = {ğ‘ ğ‘˜ |ğ‘˜ = 1, ..., ğ¾} and the bounding boxes ğµğ‘ğ‘œğ‘¥ = {ğ‘ğ‘ğ‘œğ‘¥ ğ‘˜ |ğ‘˜ = 1, ..., ğ¾} of these components. 3) If a connected region ğ‘ ğ‘˜ consists of a subset of slices ğ· â€² âŠ† ğ· which is associated with the corresponding probabilities ğ‘ƒ â€² âŠ† ğ‘ƒ , the probability for the bounding box of this connected region ğ‘ğ‘ğ‘œğ‘¥ ğ‘˜ is set as the maximum value within ğ‘ƒ â€² (ğ‘ƒ _ğ‘ğ‘ğ‘œğ‘¥ ğ‘˜ = ğ‘šğ‘ğ‘¥(ğ‘ƒ â€² )). 4) If ğ‘ƒ _ğ‘ğ‘ğ‘œğ‘¥ ğ‘˜ â‰¥ ğ‘‡ â„2 (ğ‘‡ â„2 is the bounding box probability threshold), the bounding box ğ‘ğ‘ğ‘œğ‘¥ ğ‘˜ is included in the final detection.</p><p>Two types of merging strategies were used as shown in Figure <ref type="figure" coords="6,366.14,642.48,8.51,10.91" target="#fig_4">4b</ref>: one is to merge only the axial detections and the other one is to merge both axial and coronal detections. For axial-only merging, the general process described above was followed. For axial-coronal merging, the axial and  coronal detections were firstly merged into separate 3D masks ğ‘€ ğ‘ğ‘¥ğ‘–ğ‘ğ‘™ and ğ‘€ ğ‘ğ‘œğ‘Ÿğ‘œğ‘›ğ‘ğ‘™ following the step 1) and the two masks were further fused into a single mask ğ‘€ = ğ‘šğ‘ğ‘¥(ğ‘€ ğ‘ğ‘¥ğ‘–ğ‘ğ‘™ , ğ‘€ ğ‘ğ‘œğ‘Ÿğ‘œğ‘›ğ‘ğ‘™ ). Then, the rest of the steps were carried out to generate the final set of bounding boxes. The slice probability threshold ğ‘‡ â„1 and bounding box probability threshold ğ‘‡ â„2 were set empirically based on the out-of-fold (OOF) validation performance within the 5-fold cross validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">TBdet-3D</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">TBdet-3D architecture</head><p>As shown in Figure <ref type="figure" coords="7,181.70,641.79,3.81,10.91" target="#fig_5">5</ref>, the network architecture of TBdet-3D network is based on Retina U-Net <ref type="bibr" coords="7,107.68,655.34,16.11,10.91" target="#b11">[12]</ref>. Specifically, it contains two major modules, including a Unet-shaped Feature Pyramid Network (U-FPN) feature extractor and a detection head. Pre-processing is conducted before inputting 3D CT data into the network and post-processing is implemented before the output bounding boxes.</p><p>U-FPN feature extractor U-FPN is a variant of FPN with a Unet-shape network, which is used to extract multi-scale 3D image features, while enabling semantic segmentation supervision. As shown in Figure <ref type="figure" coords="8,176.59,482.05,3.66,10.91" target="#fig_5">5</ref>, it consists of two parts, including 1) bottom-up feature encoder (coloured in blue) and 2) top-down feature decoder (in which detection blocks are coloured in red and segmentation blocks are coloured in green).</p><p>â€¢ Bottom-up feature encoder is a feed-forward 3D convolutional network with a hierarchy of multi-scale computation units with a scaling step of 2. In total, the encoder has six pyramid levels where each level consists of plain 3D convolution, ReLU, and instance normalisation blocks. â€¢ Top-down feature decoder generates a set of higher resolution features by upsampling spatially coarser but semantically stronger feature maps using another set of 6-level pyramids with a scaling step of 2. These higher resolution features are further enhanced with features from bottom-up encoders (with the same size but more accurate localised activation) using lateral connections. â€¢ As suggested by <ref type="bibr" coords="8,192.28,656.03,16.35,10.91" target="#b11">[12]</ref>, two additional segmentation blocks P0 and P1 (coloured in green) are added to conventional FPN (coloured in blue and red) for the symmetry of the encoder and the decoder. This enables fully semantic segmentation supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection head for bounding box classification and regression</head><p>The detection head is designed for one-stage dense detection. It consists of a classification sub-network for convolutional object classification, and a regression sub-network for convolutional bounding box regression. Both sub-networks use feature outputs from pyramid level 2-5 (P2-P6) from the top-down decoder as input.</p><p>â€¢ Classification sub-network is designed to predict the object label at each spatial position for each anchor. The classifier is a small fully convolutional network (FCN) with three convolutions with group norm, attached to P2-P6 of the decoder, with parameters shared across these pyramid levels. Along with the classifier is a regression sub-network designed to regress the offset from each anchor box to a nearby ground-truth object if it exists. â€¢ Regression sub-network shares a similar network design as a small FCN except that it outputs 4 linear outputs per spatial location per anchor for prediction of relative offset between the anchor and ground-truth bounding box. â€¢ Anchor matching is achieved with adaptive training sample selection <ref type="bibr" coords="9,430.89,303.54,18.07,10.91" target="#b21">[22]</ref> to deal with varying object sizes across datasets. It is not required to have the centre of anchor boxes in the ground truth box to prevent from removing positive anchors for small objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">TBdet-3D loss</head><p>The TBdet-3D loss includes both detection loss and segmentation loss for better detection performance.</p><formula xml:id="formula_2" coords="9,238.95,407.07,267.04,11.50">ğ¿ ğ‘‡ ğµğ‘‘ğ‘’ğ‘¡-3ğ· = ğ¿ ğ‘‘ğ‘’ğ‘¡ + ğ¿ ğ‘ ğ‘’ğ‘”<label>(3)</label></formula><p>Detection loss In the detection loss, hard negative mining is used to select 1/3 positive and 2/3 negative anchors in order to balance positive and negative anchors. Then, Binary Cross-Entropy loss is used for label classification, while Generalised IoU loss <ref type="bibr" coords="9,373.14,462.92,18.07,10.91" target="#b22">[23]</ref> is used for bounding box regression.</p><formula xml:id="formula_3" coords="9,245.35,503.57,260.64,11.51">ğ¿ ğ‘‘ğ‘’ğ‘¡ = ğ¿ ğµğ¶ğ¸ + ğ¿ ğºğ¼ğ‘œğ‘ˆ<label>(4)</label></formula><p>Segmentation loss In the segmentation loss, Dice loss and pixel-wise cross-entropy loss are used to differentiate foreground and background pixels. A soft Dice loss, in addition to cross-entropy loss, is applied because it could stabilise training for segmentation task with class imbalance <ref type="bibr" coords="9,137.28,572.98,16.25,10.91" target="#b23">[24]</ref>.</p><formula xml:id="formula_4" coords="9,197.42,584.35,304.71,32.28">ğ¿ ğ‘ ğ‘’ğ‘” = ğ¿ ğ¶ğ¸ - 2 |ğ¾| âˆ‘ï¸ ğ‘˜âˆˆğ¾ âˆ‘ï¸€ ğ‘–âˆˆğ¼ ğ‘¢ ğ‘˜ ğ‘– ğ‘£ ğ‘˜ ğ‘– âˆ‘ï¸€ ğ‘–âˆˆğ¼ ğ‘¢ ğ‘˜ ğ‘– + âˆ‘ï¸€ ğ‘–âˆˆğ¼ ğ‘£ ğ‘˜ ğ‘– (<label>5</label></formula><formula xml:id="formula_5" coords="9,502.13,593.55,3.86,10.91">)</formula><p>where ğ‘¢ is the softmax output of the network (P0 logits), and ğ‘£ is the one-hot encoding of the ground-truth segmentation map. Both u and v share the same shape ğ¼ * ğ¾ where ğ¼ denotes the number of pixels in the training batch, and ğ¾ is the number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">TBdet-3D false positive reduction</head><p>False positive reduction is a big challenge in object detection tasks. To reduce false positives, we proposed a plane-based bounding box merging (PBB merging) to combine small bounding boxes belonging to a larger one. The motivation of this technique is that we found a considerable number of false positive bounding boxes actually corresponded to a ground truth with larger size, even after the conventional post-processing pipeline has been implemented ( Non-maximum suppression (NMS), weighted box clustering <ref type="bibr" coords="10,286.72,175.28,16.16,10.91" target="#b11">[12]</ref>, small-size object removal, and adjustment of prediction confidence). To address this issue, we first tried merging all overlapped bounding boxes, but it led to decreased overall performance due to significantly increased false negatives. Then, we addressed this issue by only merging bounding boxes sharing a 2D plane with a large proportion of overlapped areas.</p><p>The algorithm of PBB merging includes three steps. First, we define the connectivity of two predicted bounding boxes b1 and b2 if 1) they share an overlapping area (IoU &gt; 0) and 2) there is a large proportion of overlap between the two boxes on any of the 2D planes (xy, yz or xz plane) with threshold 0.8. Secondly, we establish a undirected graph setting each bounding box as a node and the computed connectivity as an edge. Thirdly, we merge all connected nodes into a larger box using graph-based breadth first search. The merged box is the bounding box of all previous small boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Discussion</head><p>In this section, we summarised the 5-fold validation results of TBdet-2D and TBdet-3D in Â§4.1 and Â§4.2, respectively. In Â§4.3, we compared the performance between TBdet-2D and TBdet-3D, and the performance of other teams on the testing dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">TBdet-2D results</head><p>By choosing different 2D detection merging strategies, slice and bounding box thresholds ğ‘‡ â„1 and ğ‘‡ â„2, five different detection results were generated from the TBdet-2D approach. For the training and validation process, the OOF prediction performance on the validation sets is presented in Table <ref type="table" coords="10,171.22,500.42,3.66,10.91" target="#tab_0">1</ref>. The YOLO-axial model with Th1=0.1 and Th2=0.8 achieved the best overall OOF prediction on the validation sets. During testing, the 5 YOLO models from the 5-fold cross validation were fused to generate predictions on the testing data. The testing results of all 5 submissions using the 2D approach are listed in Table <ref type="table" coords="10,332.15,541.07,3.75,10.91">3</ref>. It can be observed that the ensemble YOLO-axial model with Th1=0.1 and Th2=0.8 also achieved the best mAP (0.308) among the TBdet-2D results on the testing data, which is higher than the mAP (0.295) of the runner-up team.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">TBdet-3D experiments and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Experimental implementation</head><p>All experiments were performed on a Linux server with Tesla P100-SXM2 GPU with 16gb RAM. Experiments were conducted using the Python language, and TBdet-3D was based on Pytorch-lightning and implemented with a self-configuring framework named nnDetection <ref type="bibr" coords="11,487.56,222.93,16.08,10.91" target="#b24">[25]</ref>.</p><p>In the preprocessing stage, we used techniques including lung masking, resampling, normalisation and data augmentation. 1) In lung masking, we extracted the lung region of CT images using provided lung masks using a three-stage segmentation algorithm <ref type="bibr" coords="11,408.29,263.58,16.24,10.91" target="#b16">[17]</ref>, while setting the remaining voxels to 0. 2) To address the heterogenous voxel spacing in CT data (especially in the z-axis), we used image resampling via third-order spline interpolation following <ref type="bibr" coords="11,473.89,290.68,16.41,10.91" target="#b25">[26]</ref>. 3) normalisation was performed based on mean and standard deviation on the training data. 4) To fit data into memory, images were cropped into patches with size [z, x, y] = [112, 160, 160], which was determined by gradually reducing the patch size until the memory constraints were fulfilled. The batch size was fixed to 4. Following <ref type="bibr" coords="11,305.31,344.87,16.08,10.91" target="#b25">[26]</ref>, we applied a series of data augmentation techniques, including rotations, scaling, Gaussian noise, Gaussian bur, brightness, contrast, gamma correlation and mirroring on the fly during training.</p><p>In the training stage, the network is trained for 60 epochs with 2500 mini-batches, and half of the batch is constrained to have at least one object. For the optimisation, SGD with Nesterov momentum 0.9 is used. In the first 50 epochs, the warm-up learning rate was linear rampped up from 1e-6 to 1e-2 over 4000 iterations to reduce early overfitting, followed by polynomial decay until 50 epochs. In the last 10 epochs, we used cyclic learning rate fluctuating from 1e-3 to 1e-6 during every epoch, in which model weights were snapshot for stochastic weight averaging. 5-fold validation was performed on training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">TBdet-3D main results</head><p>We investigated the TBdet-3D model with different post-processing techniques on the validation datasets. The evaluation was mAP_IoU provided by the challenge. TBdet-3D (conf_0.5) is the baseline Retina U-Net with a default prediction confidence threshold as 0.5. TBdet-3D (conf_0.7) is the Retina U-Net with prediction confidence threshold of 0.7. TBdet-3D (PBB merging) applied PBB merging technique to the TBdet-3D (conf_0.7).</p><p>Table <ref type="table" coords="11,128.57,583.89,5.17,10.91" target="#tab_1">2</ref> summarises the 5-fold validation results of these three models. TBdet-3D (PBB merging) achieved the best results in all folds, followed by TBdet-3D (conf_0.7), and TBdet-3D (conf_0.5). In the results from consolidating models from 5 folds, the highest mAP_IoU acheived by TBdet-3D (PBB merging) was 0.623. The results indicate both post-processing techniques (adjustment of prediction confidence, and PBB merging) were effective. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Effect of PBB merging</head><p>We further investigated the effect of PBB merging as shown in Figure <ref type="figure" coords="12,397.14,229.32,3.68,10.91" target="#fig_6">6</ref>. Figure <ref type="figure" coords="12,438.40,229.32,9.92,10.91" target="#fig_6">6a</ref> and<ref type="figure" coords="12,469.98,229.32,10.30,10.91" target="#fig_6">6b</ref> show the prediction results without and with the PBB merging technique while Figure <ref type="figure" coords="12,448.90,242.87,9.69,10.91" target="#fig_6">6c</ref> shows the ground truth for this case. The IoU between the prediction and ground truth improves from 0.25 to 0.70, and it can be seen that two false positive cases closely attached to the bigger bounding boxes by plane were successfully merged into the bigger boxes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Submission results on testing data</head><p>Table <ref type="table" coords="12,115.19,523.15,4.97,10.91">3</ref> shows the testing results of our model compared with other teams from the leaderboard as well as a comparison of our own submission.</p><p>Comparison of TBdet-3D and TBdet-2D in our submissions Table <ref type="table" coords="12,428.82,565.45,5.17,10.91">3</ref> shows that our TBdet-3D outperformed TBdet-2D model on the testing dataset. The mAP_IoU results of TBdet-3D range from 0.504 to 0.479, and the one consolidating models from all 5 folds achieved the highest performance. Comparatively, the results of TBdet-2D range from 0.308 to 0.219 with the TBdet-2D (Ensemble YOLO-axial, Th1-0.1, Th2-0.2) achieving the best performance. The improved performance of the 3D model could be due to its capacity to better exploit 3D spatial information. <ref type="table" coords="13,353.77,86.97,5.14,10.91">3</ref> shows that our TBdet-3D model achieved the highest evaluation score (mAP_IoU 0.504) among all participating teams (Senti-cLab.UAIC: 0.295 and KDE LAB: 0.185).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with other teams in leaderboard Table</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>Results for submissions of ours (left) and other teams in the leaderboard (right). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we described our participation in the ImageCELFmed TB caverns detection challenge 2022. We developed and compared a 2D YOLO-based model (TBdet-2D) and a 3D Retina U-Net-based model (TBdet-3D). The results show TBdet-3D outperforms the TBdet-2D on the testing data, potentially due to the better capability of the 3D detection model in capturing 3D spatial information. We further demonstrated that postprocessing techniques such as PBB merging significantly improved the mAP_IoU score on both the validation and testing data.</p><p>In future work, we aim to improve the performance by further leveraging the complementary advantages of both 2D and 3D models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,317.08,416.69,8.93;3,89.29,329.09,65.13,8.87;3,108.88,123.20,187.51,169.36"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of CT tuberculosis lung caverns (from the ImageCLEFmed TB Caverns Detection challenge 2022).</figDesc><graphic coords="3,108.88,123.20,187.51,169.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,89.29,320.13,416.69,8.93;5,89.29,332.13,40.38,8.87;5,127.56,84.19,340.17,229.35"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Pre-processing stage generating the pseudo-RGB axial and coronal slices for the TBdet-2D approach.</figDesc><graphic coords="5,127.56,84.19,340.17,229.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,89.29,242.53,418.22,8.93;6,155.91,84.19,283.47,145.77"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The training process of YOLO networks on the pseudo-RGB slices for the TBdet-2D approach.</figDesc><graphic coords="6,155.91,84.19,283.47,145.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,226.11,247.23,142.75,9.96;7,246.56,427.57,101.85,9.96"><head></head><label></label><figDesc>(a) YOLO detection on testing data. (b) 2D detection merging</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,89.29,446.05,418.36,8.93;7,89.29,458.06,416.69,8.87;7,89.29,470.01,416.69,8.87;7,89.29,481.97,23.97,8.87;7,193.47,259.29,208.35,161.41"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Cavern detection on testing data. The YOLO ensemble models were applied on the pseudo-RGB slices to generated detections on axial and coronal slices as in subfigure (a). Then the 3D detection results were generated by either merging the detections solely on axial slices or on both axial and coronal slices.</figDesc><graphic coords="7,193.47,259.29,208.35,161.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,89.29,316.82,416.69,8.93;8,89.29,328.83,416.69,8.87;8,89.29,340.78,416.69,8.87;8,89.29,352.74,416.69,8.87;8,89.29,364.69,416.70,8.87;8,89.29,376.65,223.92,8.87;8,89.29,84.19,416.69,225.21"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: TBdet-3D network. The network consists of two major modules, including U-FPN feature extractor and a detection head. U-FPN is composed of a bottom-up encoder (coloured in blue) and a top-down decoder (coloured in red and green). Detection signals from P2-P5 in the decoder are fed to the detection head, while segmentation signals from P1-P2 in the decoder are fed back to the encoder. The detection head has a classification sub-network (class subnet) for label classification and a regression sub-network (box subnet) for bounding box regression.</figDesc><graphic coords="8,89.29,84.19,416.69,225.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="12,89.29,447.59,416.69,8.93;12,89.29,459.59,23.32,8.87;12,89.29,306.23,416.71,133.66"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: A case study comparing of TBdet-3D prediction with/without PBB merging and the ground truth.</figDesc><graphic coords="12,89.29,306.23,416.71,133.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="13,233.62,173.88,246.79,7.09;13,195.06,188.78,119.01,7.14;13,332.65,188.78,18.35,7.14;13,412.41,188.78,24.42,7.14;13,455.43,188.78,18.35,7.14;13,206.02,198.29,108.05,7.09;13,333.55,198.29,16.58,7.09;13,385.35,198.29,51.49,7.09;13,456.30,198.29,16.58,7.09;13,240.72,207.75,73.35,7.09;13,333.55,207.75,16.58,7.09;13,405.85,207.75,30.98,7.09;13,456.30,207.75,16.58,7.09;13,127.97,217.18,186.10,7.14;13,332.66,217.18,18.35,7.14;13,144.82,226.68,169.24,7.09;13,333.55,226.68,16.58,7.09;13,144.82,236.15,169.24,7.09;13,333.55,236.15,16.58,7.09;13,114.87,245.61,199.20,7.09;13,333.55,245.61,16.58,7.09;13,114.87,255.08,199.20,7.09;13,333.55,255.08,16.58,7.09"><head></head><label></label><figDesc>(Ensemble YOLO-axial, Th1=0.5, Th2=0.7) 0.281 TBdet-2D (Ensemble YOLO-axial, Th1=0.6, Th2=0.7) 0.272 TBdet-2D (Ensemble YOLO-axial&amp;coronal, Th1=0.6, Th2=0.7) 0.226 TBdet-2D (Ensemble YOLO-axial&amp;coronal, Th1=0.5, Th2=0.7) 0.219</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="11,88.99,90.49,403.19,105.74"><head>Table 1</head><label>1</label><figDesc>Out-of-fold prediction on the validation sets within 5-fold validation using TBdet-2D.</figDesc><table coords="11,103.09,122.10,389.10,74.12"><row><cell>Methods</cell><cell cols="5">Fold 0 Fold 1 Fold 2 Fold 3 Fold4</cell></row><row><cell>TBdet-2D (YOLO-axial, Th1=0.1, Th2=0.8)</cell><cell>0.345</cell><cell>0.398</cell><cell>0.350</cell><cell>0.355</cell><cell>0.385</cell></row><row><cell>TBdet-2D (YOLO-axial, Th1=0.5, Th2=0.7)</cell><cell>0.311</cell><cell>0.342</cell><cell>0.274</cell><cell>0.279</cell><cell>0.302</cell></row><row><cell>TBdet-2D (YOLO-axial, Th1=0.6, Th2=0.7)</cell><cell>0.300</cell><cell>0.316</cell><cell>0.265</cell><cell>0.266</cell><cell>0.290</cell></row><row><cell cols="2">TBdet-2D (YOLO-axial&amp;coronal, Th1=0.6, Th2=0.7) 0.282</cell><cell>0.328</cell><cell>0.244</cell><cell>0.261</cell><cell>0.349</cell></row><row><cell cols="2">TBdet-2D (YOLO-axial&amp;coronal, Th1=0.5, Th2=0.7) 0.270</cell><cell>0.326</cell><cell>0.244</cell><cell>0.268</cell><cell>0.353</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="12,88.99,90.49,417.17,91.57"><head>Table 2</head><label>2</label><figDesc>Results of TBdet-3D in 5-fold validation. "5 folds" column shows the results of consolidated model over all 5 folds. Bold text indicates best results in each column.</figDesc><table coords="12,129.37,131.84,336.54,50.22"><row><cell></cell><cell cols="6">Fold 0 Fold 1 Fold 2 Fold 3 Fold 4 5 folds</cell></row><row><cell>TBdet-3D (conf_0.5)</cell><cell>0.580</cell><cell>0.520</cell><cell>0.490</cell><cell>0.589</cell><cell>0.512</cell><cell>0.561</cell></row><row><cell>TBdet-3D (conf_0.7)</cell><cell>0.650</cell><cell>0.570</cell><cell>0.539</cell><cell>0.632</cell><cell>0.570</cell><cell>0.609</cell></row><row><cell>TBdet-3D (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="12,180.88,173.13,280.99,8.93"><head>PBB merging) 0.665 0.585 0.554 0.637 0.595 0.623</head><label></label><figDesc></figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="13,112.66,479.56,384.58,10.91" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">H</forename><surname>Organization</surname></persName>
		</author>
		<title level="m" coord="13,203.87,479.56,136.90,10.91">Global tuberculosis report 2013</title>
		<imprint>
			<publisher>World Health Organization</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,493.11,393.33,10.91;13,112.66,506.66,240.46,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,297.54,493.11,208.45,10.91;13,112.66,506.66,100.99,10.91">Cavitary pulmonary tuberculosis: the holy grail of disease transmission</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Yoder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lamichhane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>Bishai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,222.15,506.66,70.05,10.91">Current science</title>
		<imprint>
			<biblScope unit="page" from="74" to="81" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,520.21,393.33,10.91;13,112.66,533.76,393.32,10.91;13,112.66,547.31,247.94,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,372.34,520.21,133.65,10.91;13,112.66,533.76,393.32,10.91;13,112.66,547.31,105.91,10.91">Nontuberculous mycobacterial pulmonary infection in immunocompetent patients: comparison of thin-section CT and histopathologic findings</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-J</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">J</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,226.78,547.31,44.81,10.91">Radiology</title>
		<imprint>
			<biblScope unit="volume">231</biblScope>
			<biblScope unit="page" from="880" to="886" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,560.86,393.33,10.91;13,112.66,574.41,393.53,10.91;13,112.39,587.95,220.92,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,311.82,560.86,194.16,10.91;13,112.66,574.41,159.37,10.91">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,294.40,574.41,211.79,10.91;13,112.39,587.95,133.15,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,601.50,394.92,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,168.14,601.50,49.51,10.91">Fast R-CNN</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,241.94,601.50,235.57,10.91">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,615.05,393.32,10.91;13,112.66,628.60,394.92,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,261.39,615.05,244.59,10.91;13,112.66,628.60,111.59,10.91">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,233.07,628.60,229.80,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,642.15,393.33,10.91;13,112.66,655.70,393.33,10.91;13,112.66,669.25,136.93,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,330.35,642.15,175.63,10.91;13,112.66,655.70,68.89,10.91">You only look once: Unified, real-time object detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,204.50,655.70,301.49,10.91;13,112.66,669.25,49.16,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,86.97,393.33,10.91;14,112.66,100.52,341.92,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,219.45,86.97,152.44,10.91">YOLO9000: better, faster, stronger</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,396.50,86.97,109.48,10.91;14,112.66,100.52,244.01,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,114.06,393.33,10.91;14,112.66,127.61,395.01,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="14,432.53,114.06,73.46,10.91;14,112.66,127.61,78.74,10.91">SSD: Single Shot MultiBox Detector</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,213.59,127.61,177.11,10.91">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,141.16,394.61,10.91;14,112.66,154.71,395.01,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,327.53,141.16,159.84,10.91">Focal loss for dense object detection</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,112.66,154.71,299.48,10.91">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,168.26,393.32,10.91;14,112.66,181.81,220.31,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="14,415.31,168.26,90.68,10.91;14,112.66,181.81,109.78,10.91">3D deep learning on medical images: a review</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>GulyÃ¡s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,231.01,181.81,34.15,10.91">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">5097</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,195.36,395.17,10.91;14,112.66,208.91,393.53,10.91;14,112.66,222.46,395.01,10.91;14,112.41,236.01,38.81,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,140.63,208.91,365.56,10.91;14,112.66,222.46,108.82,10.91">Retina U-Net: Embarrassingly simple exploitation of segmentation supervision for medical object detection</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bickelhaupt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Kuder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-P</forename><surname>Schlemmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,246.34,222.46,179.47,10.91">Machine Learning for Health Workshop</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="171" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,249.56,394.53,10.91;14,112.66,263.11,395.01,10.91;14,112.66,276.66,394.53,10.91;14,112.48,290.20,393.51,10.91;14,112.66,303.75,393.33,10.91;14,112.66,317.30,393.33,10.91;14,112.66,330.85,393.53,10.91;14,112.66,344.40,170.14,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="14,294.00,290.20,211.99,10.91;14,112.66,303.75,261.46,10.91">Overview of the ImageCLEF 2022: Multimedia retrieval in medical, social media and nature applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>PÃ©teri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>RÃ¼ckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>BrÃ¼ngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>SchÃ¤fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>Åtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,401.20,303.75,104.79,10.91;14,112.66,317.30,393.33,10.91;14,112.66,330.85,227.21,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<title level="s" coord="14,348.61,330.85,157.57,10.91;14,112.66,344.40,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,357.95,394.53,10.91;14,112.66,371.50,394.53,10.91;14,112.66,385.05,394.53,10.91;14,112.48,398.60,393.50,10.91;14,112.66,412.15,395.00,10.91;14,112.66,425.70,149.51,10.91" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="14,254.68,398.60,251.30,10.91;14,112.66,412.15,162.86,10.91">ultralytics/yolov5: v6.1 -TensorRT, TensorFlow Edge TPU and OpenVINO Export and Inference</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jocher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Stoken</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Borovec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Nanocode012</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Taoxie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lorna</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nadar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Laughing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Skalski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fati</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mammana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename><surname>Alexwang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Yiwei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hajek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Diaconu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Minh</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.6222936</idno>
		<ptr target="https://doi.org/10.5281/zenodo.6222936.doi:10.5281/zenodo.6222936" />
		<imprint>
			<date type="published" when="1900">1900. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,439.25,393.33,10.91;14,112.66,452.79,393.33,10.91;14,112.14,466.34,365.75,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,341.66,439.25,164.33,10.91;14,112.66,452.79,210.62,10.91">Overview of ImageCLEFtuberculosis 2022 -CT-based caverns detection and report</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dicente Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<ptr target=".org&lt;http://ceur-ws.org&gt;" />
	</analytic>
	<monogr>
		<title level="m" coord="14,353.08,452.79,116.72,10.91">CLEF2022 Working Notes</title>
		<title level="s" coord="14,478.71,452.79,27.27,10.91;14,112.14,466.34,147.97,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,479.89,393.32,10.91;14,112.66,493.44,393.33,10.91;14,112.66,506.99,246.61,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="14,227.34,479.89,278.65,10.91;14,112.66,493.44,136.99,10.91">Efficient component labeling of images of arbitrary dimension represented by linear bintrees</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tamminen</surname></persName>
		</author>
		<idno type="DOI">10.1109/34.3918</idno>
	</analytic>
	<monogr>
		<title level="j" coord="14,262.38,493.44,243.61,10.91;14,112.66,506.99,51.98,10.91">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="579" to="586" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,520.54,393.33,10.91;14,112.66,534.09,395.01,10.91;14,112.66,547.64,28.67,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="14,374.54,520.54,131.45,10.91;14,112.66,534.09,186.19,10.91">Efficient and fully automatic segmentation of the lungs in CT volumes</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">A J</forename><surname>Del Toro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Depeursinge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,330.90,534.09,128.65,10.91">VISCERAL Challenge@ ISBI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,561.19,393.73,10.91;14,112.66,574.74,393.59,10.91;14,112.66,588.29,383.25,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="14,435.49,561.19,70.89,10.91;14,112.66,574.74,239.37,10.91">CSPNet: A new backbone that can enhance learning capability of CNN</title>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-W</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I.-H</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,376.70,574.74,129.55,10.91;14,112.66,588.29,294.83,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="390" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,601.84,394.61,10.91;14,112.66,615.39,395.01,10.91;14,112.66,628.93,48.96,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="14,257.31,601.84,229.90,10.91">Path aggregation network for instance segmentation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,112.66,615.39,349.32,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,642.48,393.33,10.91;14,112.66,656.03,326.69,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="14,188.78,642.48,249.46,10.91">Improved Mosaic: Algorithms for more Complex Images</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,461.65,642.48,44.34,10.91;14,112.66,656.03,117.77,10.91">Journal of Physics: Conference Series</title>
		<imprint>
			<biblScope unit="volume">1684</biblScope>
			<biblScope unit="page">12094</biblScope>
			<date type="published" when="2020">2020</date>
			<publisher>IOP Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,669.58,393.33,10.91;15,112.66,86.97,393.33,10.91;15,112.66,100.52,394.53,10.91;15,112.66,114.06,74.01,10.91" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="14,215.55,669.58,290.43,10.91;15,112.66,86.97,87.23,10.91;15,225.36,86.97,280.63,10.91;15,112.66,100.52,104.24,10.91">Artificial intelligence and machine learning for multi-domain operations applications</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Topin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>International Society for Optics and Photonics</publisher>
			<biblScope unit="volume">11006</biblScope>
			<biblScope unit="page">1100612</biblScope>
		</imprint>
	</monogr>
	<note>Super-convergence: Very fast training of neural networks using large learning rates</note>
</biblStruct>

<biblStruct coords="15,112.66,127.61,395.17,10.91;15,112.66,141.16,393.58,10.91;15,112.66,154.71,341.92,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="15,281.37,127.61,226.46,10.91;15,112.66,141.16,236.25,10.91">Bridging the gap between anchor-based and anchorfree detection via adaptive training sample selection</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,373.30,141.16,132.94,10.91;15,112.66,154.71,244.01,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9759" to="9768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,168.26,393.33,10.91;15,112.66,181.81,393.33,10.91;15,112.66,195.36,378.95,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="15,399.73,168.26,106.26,10.91;15,112.66,181.81,279.69,10.91">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,420.23,181.81,85.76,10.91;15,112.66,195.36,291.18,10.91">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,208.91,394.53,10.91;15,112.33,222.46,393.65,10.91;15,112.66,236.01,269.98,10.91" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zimmerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wasserthal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Norajitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wirkert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10486</idno>
		<title level="m" coord="15,244.17,222.46,261.81,10.91;15,112.66,236.01,87.44,10.91">nnU-Net: Self-adapting framework for U-Net-Based medical image segmentation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,112.66,249.56,393.33,10.91;15,112.66,263.11,393.33,10.91;15,112.66,276.66,344.45,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="15,366.98,249.56,139.01,10.91;15,112.66,263.11,164.72,10.91">nndetection: A self-configuring method for medical object detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">F</forename><surname>JÃ¤ger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,305.36,263.11,200.63,10.91;15,112.66,276.66,213.94,10.91">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="530" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,290.20,393.33,10.91;15,112.66,303.75,393.98,10.91;15,112.66,317.30,38.81,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="15,386.04,290.20,119.95,10.91;15,112.66,303.75,275.37,10.91">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,396.01,303.75,69.79,10.91">Nature methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
