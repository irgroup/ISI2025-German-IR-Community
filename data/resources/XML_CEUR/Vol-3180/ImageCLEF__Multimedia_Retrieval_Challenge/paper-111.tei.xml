<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,368.37,15.42;1,89.29,106.66,138.86,15.42">Polimi-ImageClef Group at ImageCLEFmedical Caption task 2022</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,139.46,11.96"><forename type="first">Seyyed</forename><forename type="middle">Ali</forename><surname>Mir Ghayyomnia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Politecnico di Milano Piazza Leonardo da Vinci</orgName>
								<address>
									<postCode>32, 20133</postCode>
									<settlement>Milano</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,241.39,134.97,55.60,11.96"><forename type="first">Kai</forename><surname>De Gast</surname></persName>
							<email>kai.degast@mail.polimi.it</email>
							<affiliation key="aff1">
								<orgName type="institution">Politecnico di Milano Piazza Leonardo da Vinci</orgName>
								<address>
									<postCode>32, 20133</postCode>
									<settlement>Milano</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,327.99,134.97,77.54,11.96"><forename type="first">Mark</forename><forename type="middle">J</forename><surname>Carman</surname></persName>
							<email>mark.carman@polimi.it</email>
							<affiliation key="aff2">
								<orgName type="institution">Politecnico di Milano Piazza Leonardo da Vinci</orgName>
								<address>
									<postCode>32, 20133</postCode>
									<settlement>Milano</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,368.37,15.42;1,89.29,106.66,138.86,15.42">Polimi-ImageClef Group at ImageCLEFmedical Caption task 2022</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C283D90F0065FB4E50AE8F3349501A17</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical Images</term>
					<term>Concept Detection</term>
					<term>Multi-label Classification</term>
					<term>Deep Learning</term>
					<term>Vision Transformer</term>
					<term>Encoders</term>
					<term>CEUR-WS</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the models that PoliMi-ImageClef group developed to participate in ImageCLEFmedical Caption task <ref type="bibr" coords="1,180.56,238.04,9.52,8.97" target="#b0">[1]</ref>. The goal of this task is to identify medical concepts present in medical images with different imaging modalities, which is a milestone in automatically generating medical reports. We participated with different systems, using encoders (ResNet-50 [2], Resnext-50 [3] and Swin-Transformer [4] ) combined with a feed-forward neural network to predict concepts. During development process we compared the performances of the trained models, by using a part of provided data as a test set, and the model utilizing  had the best performance. However submission results proved that the model based on Resnext-50 encoder [3] had the best performance on the competition test set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We present the participation experience of the PoliMi-ImageClef group in ImageCLEFmedical Caption task <ref type="bibr" coords="1,148.32,429.07,12.81,10.91" target="#b0">[1]</ref> 1 . The Image Captioning is one of these research tasks, which is composed of two sub-tasks: Concept Detection and Caption Prediction. The Concept Detection task includes developing a multi-label classifier, intended for medical images, by identifying medical concepts. These concepts are assigned in terms of Unified Medical Languages System (UMLS) <ref type="bibr" coords="1,449.89,469.72,15.35,10.91" target="#b4">[5]</ref> 2 to each image. The Caption Prediction task comprises of generation of captions, which is essential in interpreting the medical images.</p><p>This paper discusses the models that were used in Concept Detection sub-task by PoliMi-ImageClef team. Our best run was ranked 6th in the competition. In this model we used a Resnext-50-32x4d encoder <ref type="bibr" coords="1,211.48,537.47,12.99,10.91" target="#b2">[3]</ref> to acquire image embeddings used for classification. Another model which showed a promising potential in validation phase, used a Swin-Transformer [4] to extract the image features. Combined with a feed-forward neural network, it slightly outperformed the Resnext-50 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data</head><p>The dataset provided for ImageCLEFmedical Caption task 2022 is an extended version of Radiology Objects in COntext (ROCO) dataset <ref type="bibr" coords="2,305.13,400.51,11.58,10.91" target="#b5">[6]</ref>. The dataset originates from biomedical articles of the PMC OpenAccess subset <ref type="bibr" coords="2,264.41,414.06,11.38,10.91" target="#b6">[7]</ref>. A similar dataset was used for the ImageCLEFmed 2020 concept detection task <ref type="bibr" coords="2,214.78,427.61,11.43,10.91" target="#b7">[8]</ref>.</p><p>The dataset in comprised of 83,275 radiology images in training set, 7,645 images as the validation set and 7,601 images as the test set. The total number of UMLS concepts present in the dataset is 8,374. The maximum number of concepts present in a single image is 100. In development process, we combined the training set and validation set and spliced it into training set, validation set and test set with (0.6, 0.2, 0.2) ratio. This test set was used to compare the performance of models in development process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this section we discuss different method used in development phase. In total we trained and tested 6 models. Some of these models used different encoders such as ResNet-50 <ref type="bibr" coords="2,488.39,581.08,12.99,10.91" target="#b1">[2]</ref> , DenseNet-121 <ref type="bibr" coords="2,155.82,594.63,16.42,10.91" target="#b12">[12]</ref>, Resnext-50 <ref type="bibr" coords="2,231.96,594.63,11.58,10.91" target="#b2">[3]</ref>, whereas others used variations of Swin-Transformer <ref type="bibr" coords="2,492.22,594.63,11.58,10.91" target="#b3">[4]</ref>. All these models were pretrained on ImageNet1k or ImageNet22k <ref type="bibr" coords="2,389.76,608.18,18.07,10.91" target="#b13">[13]</ref> in case of Swin-B <ref type="bibr" coords="2,492.22,608.18,11.59,10.91" target="#b3">[4]</ref>. Following this, we rescaled the images to 224√ó224 and normalized with the mean and standard deviation of ImageNet(224√ó224) <ref type="bibr" coords="2,231.55,635.28,16.11,10.91" target="#b13">[13]</ref>. ResNext <ref type="bibr" coords="2,293.71,635.28,12.71,10.91" target="#b2">[3]</ref> and Swin-Transformer <ref type="bibr" coords="2,410.76,635.28,12.71,10.91" target="#b3">[4]</ref> models performed remarkably well during the development phase of the model. In the following we discuss the structure of ResNext <ref type="bibr" coords="2,183.41,662.38,12.84,10.91" target="#b2">[3]</ref> encoder and Swin-Transformer <ref type="bibr" coords="2,340.53,662.38,12.84,10.91" target="#b3">[4]</ref> and their novelties. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Common Attributes of training process:</head><p>To avoid repetition, in this section we note common attributes of the training procedure for all models. We tackle this task as a multi-label classification; the target to be classified is an array of labels <ref type="foot" coords="3,350.66,361.23,3.71,7.97" target="#foot_0">3</ref> that can be present in each image. This narrows our choice for the choice of activation function <ref type="bibr" coords="3,359.82,376.54,17.82,10.91" target="#b14">[14]</ref> to Sigmoid function for each output layer node. In addition, we need to use Binary Cross-Entropy loss function <ref type="bibr" coords="3,448.52,390.09,17.76,10.91" target="#b15">[15]</ref> to fit the model. To acquire a computationally efficient with fewer parameters, we used Adam optimizer <ref type="bibr" coords="3,89.29,417.18,16.25,10.91" target="#b16">[16]</ref>.</p><p>For each encoder, we experimented with two structures :</p><p>‚Ä¢ With no hidden layer.</p><p>‚Ä¢ With one hidden layer : with 2048 nodes, ReLU activation function <ref type="bibr" coords="3,410.40,463.17,17.76,10.91" target="#b17">[17]</ref> and Dropout rate of 0.2 <ref type="bibr" coords="3,143.44,476.72,16.25,10.91" target="#b18">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Resnext-50-based Classification</head><p>In this model, we used a Resnext-50-32x4d encoder <ref type="bibr" coords="3,326.46,525.84,11.58,10.91" target="#b2">[3]</ref>, a CNN<ref type="foot" coords="3,379.12,524.08,3.71,7.97" target="#foot_1">4</ref> with 48 layers. A ResNext repeats a building block that aggregates a set of transformations with the same topology <ref type="foot" coords="3,501.01,537.63,3.71,7.97" target="#foot_2">5</ref> . Compared to a ResNet <ref type="bibr" coords="3,197.37,552.94,13.00,10.91" target="#b1">[2]</ref> , it exposes a new dimension, cardinality (the size of the set of transformations) C, as an essential factor in addition to the dimensions of depth and width.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">ResNext building block</head><p>A ResNext Block <ref type="bibr" coords="3,165.15,615.20,12.69,10.91" target="#b2">[3]</ref> is a type of residual block used as part of the ResNext CNN architecture. It uses a "split-transform-merge" <ref type="bibr" coords="3,225.43,628.75,17.79,10.91" target="#b19">[19]</ref> strategy (branched paths within a single module) similar to an Inception module<ref type="foot" coords="4,181.88,378.45,3.71,7.97" target="#foot_3">6</ref>  <ref type="bibr" coords="4,188.74,380.21,16.09,10.91" target="#b19">[19]</ref>, i.e. it aggregates a set of transformations. The effect of this strategy on the performance is further discussed in "Aggregated Residual Transformations for Deep Neural Networks" <ref type="bibr" coords="4,172.46,407.31,11.43,10.91" target="#b2">[3]</ref>.</p><p>In our validation process we experimented with different variations of FFNN for classification task, however the simplest model, with only one hidden layer, had the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Swin-Transformer</head><p>The Swin-Transformer <ref type="bibr" coords="4,194.81,484.13,12.97,10.91" target="#b3">[4]</ref> is a type of Vision Transformer<ref type="foot" coords="4,354.38,482.38,3.71,7.97" target="#foot_4">7</ref>  <ref type="bibr" coords="4,361.31,484.13,18.04,10.91" target="#b20">[20]</ref> that constructs hierarchical feature maps by merging image patches in deeper layers. It can thus serve as a generalpurpose backbone for feature extraction which can be used for a variety of tasks including Image Classification, Semantic Segmentation and Dense Recognition. The architecture of Swin-Transformer and its differences with respect to previous generation of Vision Transformers, is discussed in detail in "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows" <ref type="bibr" coords="4,138.18,565.43,11.43,10.91" target="#b3">[4]</ref>.</p><p>In our development we used an iteration of a Swin-Transformer <ref type="bibr" coords="4,388.75,578.98,12.88,10.91" target="#b3">[4]</ref> that was pretrained on ImageNet22K <ref type="bibr" coords="4,150.73,592.53,16.09,10.91" target="#b13">[13]</ref>. The features extracted from the images were then fed to an FFNN. Thorough experimentation we chose an iteration in which the FFNN had 1 hidden layers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>In this section we further explain the details of each run:</p><p>ResNet-50 RUNS : In this runs we used the ResNet-50 encoder <ref type="bibr" coords="5,376.26,260.02,12.69,10.91" target="#b1">[2]</ref> pretrained on ImageNet1K <ref type="bibr" coords="5,89.29,273.57,17.76,10.91" target="#b13">[13]</ref> and with 48 convolutional layers and 23M parameters. The network is trained on the training set for 5 epochs. We trained two instances of this model with learning rates of 1ùëí -3 and 1ùëí -4.</p><p>The model trained with learning 1ùëí -4 had the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet-152 RUNS :</head><p>In this runs we used the ResNet-152 encoder <ref type="bibr" coords="5,405.92,329.43,13.00,10.91" target="#b1">[2]</ref> pretrained on Ima-geNet1K <ref type="bibr" coords="5,129.75,342.98,17.80,10.91" target="#b13">[13]</ref> and with 150 convoltional layers and 58M parameters. The encoder coupled with an FFNN with one hidden layer was trained on training set for 5 epochs. The learning rate for this training procedure was set to 1ùëí -3 after experimenting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DenseNet-121 RUNS :</head><p>For this run we used the DenseNet-121 encoder <ref type="bibr" coords="5,424.65,398.84,17.94,10.91" target="#b12">[12]</ref> pretrained on ImageNet1K <ref type="bibr" coords="5,147.16,412.39,17.91,10.91" target="#b13">[13]</ref> and with 120 convolutional layers and 6M parameters. The encoder coupled with an FFNN with one hidden layer was trained on training set for 5 epochs with learning rate of 1ùëí -3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNext-50 RUNS :</head><p>In this series of runs we experimented with ResNext encoder [3] capabilities. We used the pretrained encoder on ImageNet1K <ref type="bibr" coords="5,355.89,481.79,17.76,10.91" target="#b13">[13]</ref> with 22M parameters. For the two structures mentioned previously we experimented with 2 values of learning rates; 1ùëí -3, 1ùëí -4. The models were trained for 5 epochs. The structure with no hidden layers and learning rate of 1ùëí -4 produced the best F1-Score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Swin-Transformer RUNS :</head><p>In this series of runs we experimented with Swin Transofrmer <ref type="bibr" coords="5,89.29,564.75,12.81,10.91" target="#b3">[4]</ref> capabilities. The structure used for the FFNN with one hidden layer. We used 2 versions of the Swin-Transformer :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>‚Ä¢ Swin-T [4] , tiny version pretrained on ImageNet1K ‚Ä¢ Swin-B [4] , base version pretrained on ImageNet22K</head><p>Learning the rate for these runs was set to 1ùëí -4 after experimenting. The structures were trained for 5 epochs. The Swin-B model had the best the results between these runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance Evaluation:</head><p>The performance index proposed in the Caption Detection subtask is F1-score <ref type="bibr" coords="6,160.87,100.52,16.33,10.91" target="#b21">[21]</ref>. The F1-score is calculated in "binary" averaging method for each image. Then all F1-scores are summed and averaged over the number of elements in the test set <ref type="bibr" coords="6,476.22,114.06,11.61,10.91" target="#b6">(7,</ref><ref type="bibr" coords="6,487.83,114.06,15.48,10.91">601)</ref>, giving the final score 8 . Table reports the details of the models used and their performances during development and testing. The F1-score during development was computed by processing the result of the predictions over the test set generated in development process from splicing the merged set of training data and validation data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future work</head><p>We investigated the performance of Resnext-50 <ref type="bibr" coords="6,303.01,226.89,12.87,10.91" target="#b2">[3]</ref> and Swin-Transformer <ref type="bibr" coords="6,422.34,226.89,12.87,10.91" target="#b3">[4]</ref> in a multi-label classification task. In our development phase as seen in table Swin-Transformer outperformed Resnext-50 by slight advantage. In future work, we aim to further investigate the potential of Swin-Transformers, in Concept Detection and Caption Prediction contexts, to improve the performance of medical image captioning systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,264.67,418.23,9.96;2,88.99,276.62,351.75,9.96;2,89.29,84.19,416.72,174.62"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Training data and their corresponding captions. From left to right, CC BY [Lambelin et al. (2014)] [9], CC BY-NC [Park et al. (2010)][10], CC BY-NC [√É-zt√É¬ºrk et al. (2015)][11]</figDesc><graphic coords="2,89.29,84.19,416.72,174.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,89.29,300.00,349.77,9.96;3,89.29,84.19,416.69,204.13"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison between structures of ResNet [2] Block and ResNext Block [3]</figDesc><graphic coords="3,89.29,84.19,416.69,204.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,89.29,341.72,329.55,9.96;4,89.29,84.19,416.70,245.85"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of the feature maps in ViT<ref type="bibr" coords="4,294.06,341.72,16.46,9.96" target="#b20">[20]</ref> and Swin-Transformer<ref type="bibr" coords="4,407.02,341.72,11.83,9.96" target="#b3">[4]</ref> </figDesc><graphic coords="4,89.29,84.19,416.70,245.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,90.49,411.00,91.04"><head>Table 1</head><label>1</label><figDesc>Performance results of our best systems</figDesc><table coords="5,95.29,119.76,404.70,61.76"><row><cell>Model Name</cell><cell>Structure</cell><cell>Learning rate</cell><cell>Weights</cell><cell cols="3">Parameters F1-Score(Dev) F1-Score(Test)</cell></row><row><cell>Resnext-50</cell><cell>Resnext-50-32x4d + FFNN(0 hidden layer)</cell><cell>1ùëí -4</cell><cell>ImageNet1k</cell><cell>22M</cell><cell>0.401</cell><cell>0.432</cell></row><row><cell>Swin-Base</cell><cell>Swin-B-224 + FFNN(1 hidden layer)</cell><cell>1ùëí -4</cell><cell>ImageNet22k</cell><cell>88M</cell><cell>0.403</cell><cell>0.428</cell></row><row><cell>ResNet-50</cell><cell>ResNet-50 + FFNN(0 hidden layer)</cell><cell>1ùëí -4</cell><cell>ImageNet1k</cell><cell>23M</cell><cell>0.399</cell><cell>0.425</cell></row><row><cell>Swin-Tiny</cell><cell>Swin-T-224 + FFNN(1 hidden layer)</cell><cell>1ùëí -4</cell><cell>ImageNet1k</cell><cell>28M</cell><cell>0.396</cell><cell>0.426</cell></row><row><cell>DenseNet-121</cell><cell>DenseNet-121 + FFNN(1 hidden layer)</cell><cell>1ùëí -3</cell><cell>ImageNet1k</cell><cell>6M</cell><cell>0.393</cell><cell>0.423</cell></row><row><cell>ResNet-152</cell><cell>ResNet-152 + FFNN(1 hidden layer)</cell><cell>1ùëí -3</cell><cell>ImageNet1k</cell><cell>58M</cell><cell>0.391</cell><cell>0.420</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="3,108.93,649.12,64.68,8.97"><p>Array size is 8374</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="3,108.93,660.08,113.81,8.97"><p>Convolutional Neural Network</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="3,108.93,671.04,185.72,8.97"><p>https://paperswithcode.com/method/resnext-block</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="4,108.93,660.01,200.89,8.97"><p>https://paperswithcode.com/method/inception-module</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4" coords="4,108.93,670.97,205.00,8.97"><p>https://paperswithcode.com/method/vision-transformer</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,112.66,339.71,395.17,10.91;6,111.81,353.26,395.37,10.91;6,112.66,366.81,393.33,10.91;6,112.66,380.36,216.46,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,326.22,353.26,180.96,10.91;6,112.66,366.81,181.70,10.91">Overview of ImageCLEFmedical 2022 -Caption Prediction and Concept Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garc√≠a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,316.25,366.81,189.74,10.91;6,112.66,380.36,97.38,10.91">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,393.91,393.33,10.91;6,112.66,407.46,365.41,10.91" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,263.86,393.91,205.14,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1512.03385" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,421.01,393.53,10.91;6,112.66,434.55,395.00,10.91;6,112.66,450.55,97.35,7.90" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,322.24,421.01,183.94,10.91;6,112.66,434.55,93.83,10.91">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1611.05431" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,461.65,393.33,10.91;6,112.39,475.20,395.28,10.91;6,112.66,488.75,187.21,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,370.89,461.65,135.10,10.91;6,112.39,475.20,179.70,10.91">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="2103.14030" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,502.30,393.33,10.91;6,112.66,515.85,374.29,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,190.81,502.30,315.18,10.91;6,112.66,515.85,51.86,10.91">The unified medical language system (umls): Integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gkh061</idno>
	</analytic>
	<monogr>
		<title level="j" coord="6,173.16,515.85,98.78,10.91">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,529.40,393.33,10.91;6,112.33,542.95,393.98,10.91;6,112.66,556.50,393.33,10.91;6,112.66,570.05,394.51,10.91;6,112.66,586.04,123.08,7.90" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Friedrich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01364-6_20</idno>
		<title level="m" coord="6,366.84,529.40,139.15,10.91;6,112.33,542.95,393.98,10.91;6,112.66,556.50,170.10,10.91;6,354.54,556.50,151.45,10.91;6,112.66,570.05,18.10,10.91">Radiology Objects in COntext (ROCO): A Multimodal Image Dataset: 7th Joint International Workshop, CVII-STENT 2018 and Third International Workshop</title>
		<meeting><address><addrLine>LABELS; Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09-16">2018. September 16, 2018. 2018</date>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
	<note>Held in Conjunction with MICCAI 2018</note>
</biblStruct>

<biblStruct coords="6,112.66,597.15,394.62,10.91;6,112.31,610.69,207.63,10.91" xml:id="b6">
	<monogr>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/" />
		<title level="m" coord="6,112.66,597.15,116.89,10.91">PMC Open Access Subset</title>
		<meeting><address><addrLine>Bethesda (MD</addrLine></address></meeting>
		<imprint>
			<publisher>National Library of Medicine</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,624.24,394.53,10.91;6,112.66,637.79,394.53,10.91;6,105.65,669.44,2.78,5.98;6,108.93,671.01,178.84,8.97" xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>P√©teri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>Herrera</surname></persName>
		</author>
		<ptr target="https://www.imageclef.org/2022/medical/caption" />
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.30,86.97,395.53,10.91;7,112.66,100.52,394.53,10.91;7,112.66,114.06,393.33,10.91;7,112.66,127.61,393.33,10.91;7,112.66,141.16,393.33,10.91;7,112.66,154.71,393.32,10.91;7,112.66,168.26,249.02,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,303.77,114.06,202.22,10.91;7,112.66,127.61,294.88,10.91">Overview of the ImageCLEF 2020: Multimedia retrieval in medical, lifelogging, nature, and internet applications</title>
		<author>
			<persName coords=""><forename type="first">V.-T</forename><surname>Ninh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D.-T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fichou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Berari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Brie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>≈ûtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,432.20,127.61,73.78,10.91;7,112.66,141.16,234.23,10.91;7,428.88,142.18,77.11,9.72;7,112.66,155.73,284.79,9.72">Proceedings of the 11th International Conference of the CLEF Association (CLEF 2020)</title>
		<title level="s" coord="7,404.37,154.71,101.61,10.91;7,112.66,168.26,78.83,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting>the 11th International Conference of the CLEF Association (CLEF 2020)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12260</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="7,128.86,181.81,378.33,10.91;7,112.66,195.36,284.00,10.91" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="7,246.01,181.81,256.47,10.91">National Institutes of Health&apos;s National Library of Medicine</title>
		<author>
			<persName coords=""><surname>By ; Lambelin</surname></persName>
		</author>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4083777/" />
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,208.91,394.53,10.91;7,112.66,222.46,284.00,10.91" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="7,245.63,208.91,256.84,10.91">National Institutes of Health&apos;s National Library of Medicine</title>
		<author>
			<persName coords=""><forename type="first">By-Nc [</forename><surname>Cc</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Park</surname></persName>
		</author>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2966712/" />
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,236.01,393.33,10.91;7,112.66,249.56,330.01,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="7,278.45,236.01,227.54,10.91;7,112.66,249.56,38.48,10.91">National Institutes of Health&apos;s National Library of Medicine</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>By-Nc [√£-Zt√£¬ºrk</surname></persName>
		</author>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5779162/" />
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,263.11,393.33,10.91;7,112.66,276.66,365.41,10.91" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="7,277.19,263.11,193.94,10.91">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1608.06993" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,290.20,393.33,10.91;7,112.66,303.75,395.17,10.91;7,112.66,317.30,394.04,10.91;7,112.66,330.85,259.41,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,346.64,290.20,159.35,10.91;7,112.66,303.75,68.43,10.91">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
		<ptr target="https://ieeexplore.ieee.org/abstract/document/5206848/.doi:10.1109/CVPR.2009.5206848" />
	</analytic>
	<monogr>
		<title level="m" coord="7,237.23,303.75,270.60,10.91;7,112.66,317.30,58.32,10.91">IEEE Conference on Computer Vision and Pattern Recognition(CVPR)</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="volume">00</biblScope>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,344.40,393.33,10.91;7,112.66,357.95,395.01,10.91;7,112.66,373.94,97.35,7.90" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="7,172.78,344.40,333.21,10.91;7,112.66,357.95,73.00,10.91">Review and comparison of commonly used activation functions for deep neural networks</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Szandala</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="2010.09458" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,385.05,395.17,10.91;7,112.66,398.60,393.33,10.91;7,112.66,412.15,242.64,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="7,213.28,385.05,294.55,10.91;7,112.66,398.60,31.63,10.91">Binary cross entropy with deep learning technique for image classification</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Ruby</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Yendapalli</surname></persName>
		</author>
		<idno type="DOI">10.30534/ijatcse/2020/175942020</idno>
	</analytic>
	<monogr>
		<title level="j" coord="7,152.17,398.60,353.82,10.91">International Journal of Advanced Trends in Computer Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,425.70,395.01,10.91;7,112.66,439.25,393.33,10.91;7,112.66,452.79,339.71,10.91" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<idno>arxiv:1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<title level="m" coord="7,233.49,425.70,164.04,10.91">A method for stochastic optimization</title>
		<meeting><address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014. 2015</date>
		</imprint>
	</monogr>
	<note>Published as a conference paper at the 3rd International Conference for Learning Representations</note>
</biblStruct>

<biblStruct coords="7,112.66,466.34,393.33,10.91;7,112.66,479.89,107.17,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Agarap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08375</idno>
		<title level="m" coord="7,191.43,466.34,228.34,10.91">Deep learning using rectified linear units (relu)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,493.44,393.33,10.91;7,112.26,506.99,393.73,10.91;7,112.41,520.54,91.35,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="7,424.65,493.44,81.34,10.91;7,112.26,506.99,215.02,10.91">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,335.32,506.99,170.66,10.91">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,534.09,393.33,10.91;7,112.66,547.64,394.04,10.91;7,112.41,561.19,150.53,10.91" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="7,314.75,534.09,191.24,10.91;7,112.66,547.64,137.11,10.91">A survey of the recent architectures of deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sohail</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Zahoora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Qureshi</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1901.06032" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,574.74,395.16,10.91;7,112.66,588.29,393.33,10.91;7,112.41,601.84,395.25,10.91;7,112.66,615.39,268.82,10.91" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="7,421.13,588.29,84.86,10.91;7,112.41,601.84,258.93,10.91">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="2010.11929" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,628.93,394.53,10.91;7,112.66,642.48,353.19,10.91" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="7,304.59,628.93,198.33,10.91">Thresholding classifiers to maximize f1 score</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1402.1892</idno>
		<ptr target="https://arxiv.org/abs/1402.1892.doi:10.48550/ARXIV.1402.1892" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
