<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,375.74,15.42">NeuralDynamicsLab at ImageCLEFmedical 2022</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,116.63,99.04,5.42"><forename type="first">Georgios</forename><surname>Moschovis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
								<address>
									<addrLine>LindstedtsvÃ¤gen 5</addrLine>
									<postCode>114 28</postCode>
									<settlement>Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,206.26,116.63,62.13,5.42"><forename type="first">Erik</forename><surname>FransÃ©n</surname></persName>
							<email>erikf@kth.se</email>
							<affiliation key="aff0">
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
								<address>
									<addrLine>LindstedtsvÃ¤gen 5</addrLine>
									<postCode>114 28</postCode>
									<settlement>Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Science for Life (SciLife) Laboratory</orgName>
								<address>
									<addrLine>TomtebodavÃ¤gen 23A, Gamma 6</addrLine>
									<postCode>171 65</postCode>
									<settlement>Solna</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,375.74,15.42">NeuralDynamicsLab at ImageCLEFmedical 2022</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">78103BB91D3FB7FD53C356AFA72946F2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Neural networks</term>
					<term>Speech and language technology</term>
					<term>Natural Language Processing (NLP)</term>
					<term>Deep learning</term>
					<term>Generative deep networks</term>
					<term>Convolutional neural networks (CNN)</term>
					<term>Text generation</term>
					<term>Information retrieval</term>
					<term>Diagnostic captioning</term>
					<term>Image captioning</term>
					<term>concept prediction</term>
					<term>classification</term>
					<term>image encoders</term>
					<term>transformers</term>
					<term>Encoder-Decoder architecture</term>
					<term>abstractive summarization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Diagnostic Captioning is described as the automatic text generation from a collection of X-RAY images and it can assist inexperienced doctors and radiologists to reduce clinical errors or help experienced professionals to increase their productivity. Therefore, tools that would help doctors and radiologists produce higher quality reports in less time could be of high interest for medical imaging departments, as well as significantly impact deep learning research within the biomedical domain. With our participation in ImageCLEFmedical 2022 Caption evaluation campaign, we have attempted to address both concept detection and caption prediction tasks by developing baselines based on Deep Neural Networks; including image encoders, classifiers and text generators. Our group, NeuralDynamicsLab at KTH Royal Institute of Technology, within the school of Electrical Engineering and Computer Science, ranked 4 th in the former and 5 th in the latter task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>One of the most exciting technological aspects nowadays is Machine Learning's impressive potential in transforming the world we live in, primarily due to its exciting resurgence through Deep Learning (DL). The increasing size of biomedical data has allowed researchers demonstrate the evolving capabilities of Deep Learning in biomedical applications, through the development of advanced computing and imaging systems in biomedical engineering, machine learning-based biomedical data mining algorithms <ref type="bibr" coords="1,242.95,508.45,12.68,4.94" target="#b0">[1]</ref> and baselines for Diagnostic Captioning that has recently attracted researchers' attention, towards the goal of reducing the time required by a doctor or radiologist to produce medical texts and the amount of clinical errors, but also increasing the throughput of medical imaging departments <ref type="bibr" coords="1,289.19,549.10,11.43,4.94" target="#b1">[2]</ref>.</p><p>In this work, we attempted to develop Diagnostic Captioning baselines, based on novel Deep Learning approaches, to investigate to what extent deep networks are capable of automatically generating a diagnostic text from a set of medical images and how much their interpretation of medical images can assist doctors and radiologists produce better quality diagnoses; also at an increased throughput <ref type="bibr" coords="2,188.47,117.33,11.50,4.94" target="#b1">[2]</ref>. Towards this objective, the first step is concept detection that boils down to predicting relevant tags for X-RAY images, while the end goal is caption generation. In ImageCLEFmedical 2022 evaluation campaign, we experimented with addressing both concept detection and caption prediction tasks in order to get a quantitative measure of our proposed architectures' performance <ref type="bibr" coords="2,212.20,171.52,11.43,4.94" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Dataset</head><p>In this section, we describe the data provided in ImageCLEFmedical 2022 evaluation campaign. Precisely, we provide details about the ImageCLEFmedical 2022 concept detection and caption prediction datasets that include images from different radiological image modalities but without including imaging modality information.</p><p>The dataset provided for both subtasks of ImageCLEFmedical 2022 evaluation campaign <ref type="bibr" coords="2,493.17,284.35,12.81,4.94" target="#b3">[4]</ref> consists of 90920 images that constitute a subset of the extended Radiology Objects in COntext (ROCO) dataset <ref type="bibr" coords="2,159.83,311.44,11.28,4.94" target="#b4">[5]</ref>, without imaging modality information. As in previous editions, the dataset originates from biomedical articles of the PMC OpenAccess subset. After merging the initially provided train and validation data, we shuffle them after manually setting the seeds to eliminate randomness in consecutive runs while tuning our hyperparameters and then keep 80% as our training set, 10% as our validation set used for hyperparameter tuning and the remaining 10% as our development set used for model selection. Since the dataset is large we perform neither cross-validation nor data-augmentation. We experimented with adding noise to the images, in the form of random rotations and translations, which however did not provide any additional benefit in our baselines' quantitative evaluation.</p><p>Regarding the concept detection subtask, there are 8374 tags of concepts that are assigned to the X-RAY images, while each image in any of the training, validation or development set is assigned 5 tags on average. Regarding the caption prediction subtask, the total number of captions in the training set is 72736, the total number of unique captions is 70879 and the average caption length is 108 words, including 28 unique words. In the validation set the total number of captions is 9092, the total number of unique captions is 8984, the average caption length is 107 words, including 26 unique words. In the development set the total number of captions is 9092, the total number of unique captions is 8977 and the average caption length is 108 words, including 28 unique words. These counts verify that the aforementioned sets are balanced in terms of their statistics.</p><p>"The concepts were generated using a reduced subset of the Unified Medical Language System (UMLS) 2020 AB release, which includes the sections (restriction levels) 0, 1, 2, and 9". <ref type="bibr" coords="2,473.16,582.43,12.84,4.94" target="#b3">[4]</ref> The UMLS is a set of files and software that collects multiple health and biomedical vocabularies and standards to enable interoperability between computer systems. To improve the feasibility of recognizing concepts from the images, concepts were filtered based on their semantic type and concepts with very low frequency were removed. In each caption, tokens containing numbers and all punctuation were removed, captions were converted to lower-case and lemmatization was applied using spaCy toolkit <ref type="bibr" coords="2,233.86,663.72,11.43,4.94" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods and results</head><p>In this section, we describe the core components of the methods utilized to encode the X-RAYs with dense embeddings in our work and explain in detail the baseline networks that we proposed in ImageCLEFmedical 2022 evaluation campaign, in order of performance, for both subtasks that are based on the aforementioned core components that rely on pre-trained architectures, extremely popular in computer vision.</p><p>Precisely, we provide details about the ImageCLEFmedical 2022 concept detection and caption prediction datasets and on how we designed backbone networks as generic image encoders that rely on Convolutional Neural Networks (CNN) architectures that are popular for vision tasks on generic images, such as classification and semantic segmentation, while they are shared within all baselines, in both ImageCLEFmedical Caption tasks. Furthermore, we describe the components of each model and give details on the selected hyper-parameters. For all our models, we have set in advance all the random seeds equal to 0, the CUDNNs backends as deterministic and disabled the CUDNNs backends benchmark to ensure consistency of the aforementioned splits in consecutive runs for hyper-parameter selection. This procedure has been applied for both subtasks of the evaluation campaign.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Backbone Networks: image encoders</head><p>One of the principal components in the proposed architectures that is shared for both subtasks includes the image encoders. They constitute existing state-of-the-art architectures, pretrained on ImageNet classification dataset <ref type="bibr" coords="3,238.95,381.06,11.28,4.94" target="#b5">[6]</ref>, which are obtained from torchvision models library to perform inference, while any additional components such as a multi-label classification head or a caption generation architecture are appended to the output of the image encoder; in this content these models are referred to as "backbone networks". The goal of these networks is to encode the images into dense numerical representations. Since Deep Learning became popular and what is called the Deep Learning Community was given birth, different initialization strategies for the weights and the biases were proposed. We used Glorot initialization shown below <ref type="bibr" coords="3,481.81,462.35,12.69,4.94" target="#b6">[7]</ref> to initialize the weights of the classification heads and experimented with non-pretrained image encoders that we initialized using the same strategy and fully-finetuned them, their performance however was inferior in concept prediction.</p><formula xml:id="formula_0" coords="3,188.02,522.88,219.23,26.12">Glorot: ğ‘Š ğ‘–,ğ‘— âˆ¼ ğ’° (ï¸‚ - âˆšï¸‚ 6 ğ‘“ in + ğ‘“ out , âˆšï¸‚ 6 ğ‘“ in + ğ‘“ out )ï¸‚</formula><p>Some Convolutional Neural Network (CNN) encoders that have been attempted to use include variants of AlexNet <ref type="bibr" coords="3,178.88,578.00,11.45,4.94" target="#b7">[8]</ref>, ResNet <ref type="bibr" coords="3,231.17,578.00,11.46,4.94" target="#b8">[9]</ref>, DenseNet <ref type="bibr" coords="3,295.53,578.00,16.28,4.94" target="#b9">[10]</ref>, VGG <ref type="bibr" coords="3,343.08,578.00,17.93,4.94" target="#b10">[11]</ref> and EfficientNet <ref type="bibr" coords="3,438.73,578.00,16.27,4.94" target="#b11">[12]</ref>, which are obtained from torchvision models library as mentioned above. We also experimented with another architectural choice that is Vision Transformers (ViT) <ref type="bibr" coords="3,367.97,605.10,16.25,4.94" target="#b12">[13]</ref>, the performance obtained was poor however compared to CNN encoders. That outcome is in line with the observation in <ref type="bibr" coords="3,100.88,632.20,17.90,4.94" target="#b13">[14]</ref> that Vision Transformers and "Hybrid-ViT architectures are inferior to the CNN-based ones". The above summarize the first step in the design of image encoders that is model selection based on their performance on a development set.</p><p>Moreover, model selection shall be followed by a model collaboration design principle, based on ensemble learning. In this case, we have used the aforementioned models as members of the ensemble or weak learners in a pool of encoders trained with different parameter values (e.g. learning rates, decision thresholds for the positive class, number of epochs), as well as based on different architectures, to seek for diversity and exploit the "Wisdom of the crowd" <ref type="bibr" coords="4,455.91,144.43,17.82,4.94" target="#b14">[15]</ref> for the fine-tuned models. In this context, we take into consideration the "votes" of all the different CNNs by averaging their outputs to make decisions on the generated tags or make guesses on the assigned captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Concept prediction subtask</head><p>As mentioned in section 3.1 "backbone networks" refer to image encoders, which are state-ofthe-art architectures, pretrained on ImageNet classification dataset <ref type="bibr" coords="4,381.61,248.21,11.28,4.94" target="#b5">[6]</ref>, shared for both subtasks. In the case of concept prediction, an additional classification head that is either a Perceptron or a Multi-layered Perceptron was added on top of these "backbone networks" and its weights were initialized using Glorot initialization strategy <ref type="bibr" coords="4,316.94,288.86,11.43,4.94" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Pre-trained DenseNet161 with fine-tuned classification head, learning rate 10 -3 , Adam optimizer and gradient clipping</head><p>The first two models correspond to a DenseNet161 convolutional network that is pretrained on ImageNet classification dataset and its head is a Perceptron, which is further fine-tuned on the ImageCLEFmedical 2022 data using sigmoid activation function in the output units that equal the number of concepts -thus 8374 nodes, a constant learning rate equal to 10 -3 and the negative ğ¹ 1 score as a minimization criterion. For each image, we assign it the concepts that have predicted probabilities above 50%, while the tags obtain their numerical IDs in their order of appearance before shuffling them. Furthermore, we clip the gradients computed during training to be in [-1, 1], to increase numerical stability. When performing stochastic or minibatch Gradient Descent, and the loss changes quickly at one direction and slowly at another, Gradient Descent will progress slowly along the shallow dimension and jitter along the steep one. To overcome this issue, we used Adam optimizer <ref type="bibr" coords="4,487.08,487.09,16.08,4.94" target="#b15">[16]</ref>, so that progress along steep directions is damped and meanwhile progress along flat directions is accelerated. Adam uses exponentially decaying average to discard history but also momentum as an estimate of the first-order gradient. It has bias corrections for first-order and second-order moments and converges rapidly after finding a local convex bowl. If ğ‘¡ represents the current time step, Adam updates are equal to:</p><formula xml:id="formula_1" coords="4,196.18,573.92,202.41,67.67">w (ğ‘¡+1) = w (ğ‘¡) -ğœ– v (ğ‘¡) ğ›¿ + âˆš r (ğ‘¡) , ğ›¿, ğœ– âˆˆ R + v (ğ‘¡+1) = ğœŒ 1 v (ğ‘¡) + (1 -ğœŒ 1 )g (ğ‘¡) , ğœŒ 1 âˆˆ R + r (ğ‘¡+1) = ğœŒ 2 r (ğ‘¡) + (1 -ğœŒ 2 ) (ï¸ g (ğ‘¡) )ï¸ 2 , ğœŒ 2 âˆˆ R +</formula><p>Our best performing model (with submission ID 181750) is an instance of the aforementioned architecture trained in all the provided data, thus after merging again the training, validation and development sets that are described in section 2 and achieves ğ¹ 1 = 0.43601. The next model corresponds to the same network architecture but is trained only in training set (with submission ID 181715) and achieves a score ğ¹ 1 = 0.43567. For the latter case, where we have measured performance in all sets, we present plots with the evolution of ğ¹ 1 score and accuracy during training in Figure <ref type="figure" coords="5,202.79,144.43,3.77,4.94" target="#fig_1">1</ref>(a). The next model corresponds to another DenseNet161 convolutional network that is pretrained on ImageNet classification dataset and its head is a Perceptron, which is further fine-tuned on the ImageCLEFmedical 2022 data using sigmoid activation function in the output units that equal the number of concepts -thus 8374 nodes, a constant learning rate equal to 5 Ã— 10 -4 and the negative ğ¹ 1 score as a minimization criterion. For each image, we assign it the concepts that have predicted probabilities above 50%, while the tags obtain their numerical IDs in their order of appearance before shuffling them. Furthermore, we clip the gradients computed during training to be in [-1, 1], to ensure numerical stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Pre-trained</head><p>In this occasion we have used an improved version of Adam optimizer, called AdamW <ref type="bibr" coords="5,486.85,315.70,16.27,4.94" target="#b16">[17]</ref>, where weight decay is performed only after controlling the parameter-wise step size and thus yields models that generalize much better. Compared to Adam optimizer that we discussed in section 3.2.1, as well as other adaptive gradient algorithms, where the potential benefit of weight decay regularization is limited because "the weights do not decay multiplicatively but by an additive constant factor" <ref type="bibr" coords="5,225.62,383.44,16.09,4.94" target="#b16">[17]</ref>, AdamW optimizer may overcome this issue, while training much faster than stochastic or minibatch Gradient Descent.</p><p>Our model is an instance of the aforementioned network architecture, it is trained only in training set (with submission ID 181753) and achieves a score ğ¹ 1 = 0.43558, although we would expect training with AdamW to perform better. Since the gain of re-training the model after merging all the splits is almost negligible, as we already noticed in section 3.2.1, the remaining models are not re-trained in the entire dataset. Once again, we present plots with the evolution of ğ¹ 1 score and accuracy in Figure <ref type="figure" coords="5,245.35,478.29,3.90,4.94" target="#fig_1">1</ref>(b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Pre-trained DenseNet161 with fine-tuned classification head, learning rate 5 Ã— 10 -4 and Adam optimizer</head><p>The subsequent model is yet another DenseNet161 convolutional network that is pretrained on ImageNet classification dataset and its head is a Perceptron, which is further fine-tuned on the ImageCLEFmedical 2022 data using sigmoid activation function in the output units that equal the number of concepts -thus 8374 nodes, a constant learning rate equal to 5 Ã— 10 -4 and the negative ğ¹ 1 score as a minimization criterion. For each image, we assign it the concepts that have predicted probabilities above 50%, while the tags obtain their numerical IDs in their order of appearance before shuffling them and train the network using Adam optimizer; as we have excessively described in section 3.2.1.</p><p>Our model is an instance of the aforementioned network architecture (with submission ID 182152) and achieves a score ğ¹ 1 = 0.43539, however, in this baseline we omit clipping the gradients, in contrast with the models described above in sections 3.2.1 and 3.2.2. Furthermore, as for both previous best-performing models we present plots with the evolution of ğ¹ 1 score and accuracy below in Figure <ref type="figure" coords="6,222.82,117.33,11.07,4.94" target="#fig_1">1(c</ref>). 3. We observe that the classifications heads, which we finetune on ImageCLEFmedical 2022 data, appear to be sufficiently regularized (thus there is no overfitting) and to have used their maximum capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Ensemble of pre-trained DenseNet CNNs with fine-tuned classification heads</head><p>The proceeding model and the best performing mixture of individual networks corresponds to the 10 best performing DenseNet CNNs, including instances of DenseNet161 and DenseNet121 architectures, and indicates our quest for diversity and to consequently exploit the "Wisdom of the crowd" <ref type="bibr" coords="6,140.25,413.14,17.91,4.94" target="#b14">[15]</ref> notion.</p><p>In this context, we take into account the "votes" of all the different CNNs to make decisions on the assigned tags. The voting scheme consists of averaging the probabilities computed by the different weak learners before assigning to each image the concepts that have average predicted probabilities above 50%, while the tags as usual obtain their numerical IDs in their order of appearance before shuffling them. We also experimented with using alternative voting policies, such as computing the union or intersection of the assigned tags by each weak learner, where assignments are defined by the predicted probabilities being above 50%, in the pool of finetuned networks, but they performed poorly.</p><p>Table <ref type="table" coords="6,126.86,535.09,5.04,4.94" target="#tab_0">1</ref> summarizes the architecture of all individual networks in the pool of encoders. This includes the type of Backbone Network, the optimizer, the value of learning rate and whether it is decaying per epoch, the batch size and the submission ID of the individual network, for the aforementioned weak learners in sections 3.2.1, 3.2.2, 3.2.3 that performed better than the ensemble altogether and thus were submitted individually. Note that the classification head is always a Perceptron which is further fine-tuned in the ImageCLEFmedical 2022 data using sigmoid activation function in the output units that equal the number of concepts. Moreover, when linear decay is applied, the learning rate is updated by: ğœ‚ ğ‘¡+1 = ğœ‚ 0 Ã— 1-ğ‘¡ ğ‘‡ where ğ‘¡ represents the current time step, ğ‘‡ the total number of epochs and ğœ‚ 0 is the learning rate at the beginning of training procedure. The performance of this mixture of experts (with submission ID 182338) equals ğ¹ 1 = 0.43496. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5.">Ensemble of pre-trained DenseNet CNNs with fine-tuned classification heads</head><p>Although Dense Convolutional Networks (DenseNet CNNs) appear to outperform other network architectures, which is in line with their extensive use in biomedical applications that include X-RAYs processing <ref type="bibr" coords="7,163.82,321.48,16.09,4.94" target="#b17">[18]</ref>, we also experimented with a plethora of CNNs backbone networks as we have mentioned in section 3.1. Consequently, the ensuing three models constitute ensembles that include different architectures within their members, with varying hyperparameter values to encourage diversity of training regimes. During the voting process we average the probabilities computed by the softmax layer of all different week learners before assigning to each image the tags that have average predicted probabilities above 50%. Our three following mixtures of experts (with submission IDs 181546, 182155, 182154) and achieve a score ğ¹ 1,1 = 0.43404, ğ¹ 1,2 = 0.43130, ğ¹ 1,3 = 0.42957 respectively. Tables <ref type="table" coords="8,479.45,538.71,7.62,4.94" target="#tab_1">2,</ref><ref type="table" coords="8,490.26,538.71,3.81,4.94" target="#tab_2">3</ref>, 4 summarize the architecture of all individual networks in each pool of encoders. Their format is identical to that used in section 3.2.4 and consequently they also refer to the hyper-parameter values for each of the weak learners.</p><p>Note that the classification head is always a Perceptron which is further fine-tuned in the ImageCLEFmedical 2022 data using sigmoid activation function in the output units that equal the number of concepts. Moreover, when linear decay is applied, the learning rate is updated by: ğœ‚ ğ‘¡+1 = ğœ‚ 0 Ã— 1-ğ‘¡ ğ‘‡ where ğ‘¡ represents the current time step, ğ‘‡ the total number of epochs and ğœ‚ 0 is the initial learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.6.">Fully fine-tuned DenseNet161 with cyclical learning rate and AdamW optimizer</head><p>The succeeding model corresponds to a DenseNet161 convolutional network that is now fullyfinetuned on the ImageCLEFmedical 2022 data using sigmoid activation function in the output units that equal the number of concepts -thus 8374 nodes, scheduled learning rate <ref type="bibr" coords="9,452.67,137.90,17.78,4.94" target="#b18">[19]</ref> and the negative ğ¹ 1 score as a minimization criterion. For each image, we assign it the concepts that have predicted probabilities above 50%, while the tags obtain their numerical IDs in their order of appearance before shuffling them.</p><p>One important aspect of minibatch or stochastic gradient descent relates to the choice of the learning rate ğœ‚ that controls the size of the update, which will occur to the gradients in every iteration. Constant learning rates have been traditionally used to train Deep Neural Networks based on back-propagation algorithm, although do not guarantee optimal convergence rate according to the Stochastic Approximation Theory <ref type="bibr" coords="9,311.20,246.29,16.09,4.94" target="#b19">[20]</ref>, precisely the network parameters hover around a minimum at an average distance proportional to the learning rate and to a variance that is dependent on the objective function and the exemplar set <ref type="bibr" coords="9,389.46,273.39,16.42,4.94" target="#b20">[21]</ref>. To this end, cyclical learning rates have been proposed as a new method for setting the learning rate by cyclically varying its value between reasonable boundary values, which increases classification accuracy when training CNNs with generic images <ref type="bibr" coords="9,276.67,314.04,16.25,4.94" target="#b21">[22]</ref>. A high value of ğœ‚ will make the network make large steps above the minimum of the error function but never converge to it, as illustrated in Figure <ref type="figure" coords="9,351.79,535.02,15.36,4.94" target="#fig_2">2(a)</ref>. A small value of ğœ‚ will delay convergence, preventing the network to find a minimum of the error function if the number of epochs is limited. A cyclical learning rate linearly ranges between two values ğœ‚ min and ğœ‚ max . One maximization of the learning rate followed by a minimization is called a cycle. In Figure <ref type="figure" coords="9,89.29,589.22,4.19,4.94" target="#fig_2">2</ref>(b) hereunder we present an example of cyclical learning rate, where ğœ‚ min = 0.01, ğœ‚ max = 0.30, ğ‘› ğ‘  = 2 and we denote as 2ğ‘› ğ‘  the time required for a cycle of our learning rate to complete. In our model we set ğœ‚ min = 10 -5 , ğœ‚ max = 0.1, ğ‘› ğ‘  = 4 for the first 80 epochs and then set it to a constant value ğœ‚ = 10 -3 for 30 additional epochs.</p><p>This network (with submission ID 182156) achieves a score ğ¹ 1 = 0.31687, which is a rather lower score compared to the pre-trained models on ImageNet classification dataset <ref type="bibr" coords="9,446.56,656.96,11.28,4.94" target="#b5">[6]</ref>, achieving more than 10% higher ğ¹ 1 results on the test set. Moreover, we present plots with the evolution of ğ¹ 1 score and accuracy per training epoch of the model in Figure <ref type="figure" coords="10,392.65,90.23,4.07,4.94" target="#fig_2">2</ref>(c) that is quite unstable while varying the learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.7.">Nearest Neighbours Baseline</head><p>The ensuing model is a generalization of the 1-NN baseline proposed in <ref type="bibr" coords="10,406.42,153.11,16.14,4.94" target="#b22">[23]</ref>. We further either remind or inform the reader that for every image in the test set, the 1-NN baseline assigns the tags of the visually most similar image from the training set as the output and consequently for every image, ğ‘¥ ^, in the test set, the 1-NN baseline will output the set of concepts, say ğ‘¦ * , of the most similar image, say ğ‘¥ * , from the training set as output <ref type="bibr" coords="10,351.61,207.30,11.42,4.94" target="#b1">[2]</ref>. Therefore, if we denote by e(.) the output of the employed image encoder among those mentioned in section 3.1, 1-NN predicts (ğ‘¥ ^, ğ‘¦ ^) = (ğ‘¥ ^, ğ‘¦ * ) that satisfies (ğ‘¥ * , ğ‘¦ * ) = arg min ğ‘¥ * cos (e(ğ‘¥ ^), e(ğ‘¥ * )). Our generalized Nearest Neighbours baseline takes into account ğ‘˜ âˆˆ Z + neighbours instead and not necessarily only the one with closest representation. Our model (with submission ID 182331) uses ğ‘˜ = 1 with a VGG-16 encoder pre-trained on ImageNet classification dataset and achieves only ğ¹ 1 = 0.25061 that indicates the importance of fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Performance summary</head><p>Table <ref type="table" coords="10,114.73,338.32,4.97,4.94" target="#tab_4">5</ref> below summarizes several characteristics of the proposed baselines for concept detection, in order of performance with respect to ğ¹ 1 scores, together with their respective submission IDs. We observe that DenseNet161 image encoders with finetuned classification heads are the top performing configurations and outperform other CNN architectures, which is in accordance with their extensive use X-RAYs processing <ref type="bibr" coords="10,282.26,392.52,16.13,4.94" target="#b17">[18]</ref>, while fully finetuning the backbone networks and using retrieval based heuristics that capture representations' similarities, such as the 1-NN baseline <ref type="bibr" coords="10,128.14,419.62,16.25,4.94" target="#b22">[23]</ref>, achieve lower scores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Caption generation subtask</head><p>In ImageCLEFmedical 2022 evaluation campaign, "the first step to automatic image captioning and scene understanding boils down to identifying the presence and location of relevant concepts within a large corpus of medical images that is followed by caption generation in captioning.</p><p>Based on medical images content, the concept prediction task provides the building blocks for scene understanding by identifying the individual components, referred to as image tags, from which captions are composed. The assigned concepts can be further applied for context-based image and information retrieval purposes" <ref type="bibr" coords="11,280.66,144.43,11.43,4.94" target="#b2">[3]</ref>. "On the basis of the vocabulary ğ’± identified during concept prediction task, as well as the visual information of their interaction in the image, caption generation task refers to composing coherent captions for each entire image. For the medical captioning task, rather than the mere coverage of visual concepts, detecting the interplay of visible elements can be crucial for strong performance" <ref type="bibr" coords="11,150.72,212.17,11.28,4.94" target="#b2">[3]</ref>. In the following, we describe our proposed models for Diagnostic Captioning, in which the generalized Nearest Neighbours baseline that we introduced in section 3.2.7 has a crucial role despite it performing poorly as is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">(1 + ğ‘˜)-NN image retriever with Pegasus summarizer</head><p>Our best performing models extend the Nearest Neighbours baseline for caption generation. Precisely, 1-NN <ref type="bibr" coords="11,158.75,302.15,17.76,4.94" target="#b22">[23]</ref> constitutes one of the model components, where for every image in the test set, it will produce the diagnostic text of the visually most similar image from the training set as the output and consequently it will assign the corresponding caption, say ğ‘¦ * , of the most similar image, say ğ‘¥ * , from the training set as output <ref type="bibr" coords="11,293.83,342.79,11.42,4.94" target="#b1">[2]</ref>. Thus, if we denote by e(.) the output of the employed image encoder among those mentioned in section 3.1, 1-NN predicts (ğ‘¥ ^, ğ‘¦ ^) = (ğ‘¥ ^, ğ‘¦ * ) that satisfies (ğ‘¥ * , ğ‘¦ * ) = arg min ğ‘¥ * cos (e(ğ‘¥ ^), e(ğ‘¥ * )). This prediction constitutes the first part of the models' generated caption.</p><p>In the generalized baseline however, apart from the neighbour with the closest representation, we retrieve the top-(ğ‘˜ + 1) nearest neighbours, concatenate their outputs, excluding that of the most similar image and feed them as input to an abstractive summarizer; Pegasus <ref type="bibr" coords="11,467.85,424.09,17.83,4.94" target="#b23">[24]</ref> that is based on the transformer architecture <ref type="bibr" coords="11,272.76,437.64,16.38,4.94" target="#b24">[25]</ref>, one idea that revolutionized Natural Language Processing and is trained with a Masked Language Modelling objective, which became popular within the research community though BERT <ref type="bibr" coords="11,294.68,464.74,16.25,4.94" target="#b25">[26]</ref>.</p><p>For our models we employed a pre-trained AlexNet CNN on ImageNet classification dataset as our image encoder and merged our training, validation and development sets that are described in section 2, in order to benefit from an extensive set of train data to compute similarities with the test images. For each of them we keep the caption of the visually most similar image, concatenate the captions of the ğ‘˜ proceeding ones and give them as input to Pegasus summarizer, which we allow to produce a summary of maximum length ğ‘› tokens to eliminate repetitions. We exclude phrases as "All images are copyrighted. " and "Images courtesy of AFP, EPA, Getty" that were probably included in Pegasus' training set from our generated summaries. The predicted captions constitute the concatenation of 1-NN baseline and Pegasus summarizer outputs. Table <ref type="table" coords="11,89.29,600.23,5.01,4.94" target="#tab_5">6</ref> below presents all configurations' hyper-parameter values, namely ğ‘˜ and ğ‘›, their submission IDs and BLEU scores in decreasing order <ref type="bibr" coords="11,273.27,613.78,16.25,4.94" target="#b26">[27]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2.">ğ‘˜-NN image retriever with Retrieval Augmented Generation</head><p>It has been impressive to researchers how nowadays general-purpose sequence-to-sequence models are getting really powerful, they manage to capture the world knowledge in parameters, they achieve strong results on loads of tasks and are applicable for almost everything. However, they often hallucinate, may usually struggle to access, and apply knowledge and are difficult to update. On the other hand, modern Information Retrieval (IR) is great as well, as externally reviewed knowledge can be useful for a huge variety of NLP tasks. Modern IR provides a precise and accurate knowledge access mechanism, it is trivial to update, whereas by "modern" IR we refer to dense retrieval that starts to outperform traditional IR. On the negative side though, it still needs retrieval supervision or heuristics such as BM25, as well as some -task specific-way to integrate into downstream tasks. The goal of Retrieval Augmented Generation (RAG) <ref type="bibr" coords="12,323.65,382.06,16.09,4.94" target="#b27">[28]</ref>, which was used as model component, pretrained on Wikipedia with a FAISS index <ref type="bibr" coords="12,283.43,395.60,17.75,4.94" target="#b28">[29]</ref> built on 42% of PubMed 2022 including recent publications related to the fields of neuroscience and computational biology; is to combine the strengths of sequence-to-sequence models and explicit knowledge retrieval. Obviously, RAG is also blended with the 1-NN baseline; namely its outputs are concatenated with the caption of the visually most similar image from the training set to produce caption predictions. This model uses either a pre-trained AlexNet or VGG-16 CNN on ImageNet classification dataset as backbone network and, despite it containing a non-parametric memory, additional to storing information in the parameters of a sequence-to-sequence generative model that is a Bidirectional Auto-Regressive Transformers (BART) generator <ref type="bibr" coords="12,313.13,504.00,16.39,4.94" target="#b29">[30]</ref>, after merging our training, validation and development sets that are described in section 2 to take advantage of more input-output pairs (ğ‘¥, ğ‘¡), achieves a lower BLEU score than its predecessors described in 3.4.1 according to Table <ref type="table" coords="12,115.31,544.65,4.97,4.94" target="#tab_6">7</ref> below. These results could possibly improve if we store extracts from patients' previous diagnoses instead of biomedical articles. In the RAG approach <ref type="bibr" coords="13,198.77,90.23,16.41,4.94" target="#b27">[28]</ref>, dual memory components are pre-trained and pre-loaded with extensive knowledge to encapsulate information via the representations without further training; the generator ğ‘ ğœƒ acts as a parametric memory, with the retriever ğ‘ ğœ‚ embodying a non-parametric memory in the query encoder q(.), while also including a Dense Passage Retriever (DPR) <ref type="bibr" coords="13,487.40,130.88,16.21,4.94" target="#b30">[31]</ref>. To train the retriever ğ‘ ğœ‚ and generator ğ‘ ğœƒ end-to-end, we can treat the retrieved document as a latent variable ğ‘§, while the embedding of the closest document representation is represented as d(ğ‘§)). The Maximum Inner Product Search (MIPS) algorithm <ref type="bibr" coords="13,360.67,171.52,17.79,4.94" target="#b31">[32]</ref> is used to compute the top ğ‘˜ retrieved documents with respect to ğ‘ ğœ‚ (ğ‘§|ğ‘¥). Finally, the generated caption ğ‘¦ is produced by marginalizing over the predictions.</p><formula xml:id="formula_2" coords="13,232.82,217.13,129.64,13.13">ğ‘ ğœ‚ (ğ‘§|ğ‘¥) = exp (ï¸€ d(ğ‘§) ğ‘‡ q(ğ‘¥) )ï¸€</formula><p>The generator ğ‘ ğœƒ is a sequence-to-sequence model, a BART <ref type="bibr" coords="13,371.61,245.70,18.07,4.94" target="#b29">[30]</ref> instance precisely, which conditions on the latent documents ğ‘§ together with each input ğ‘¥ to generate each output. As an overall component, it produces ğ‘ ğœƒ (ğ‘¦ ğ‘– |ğ‘¥, ğ‘§, ğ‘¦ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3.">1-NN image retrieval baseline</head><p>Last but not least, we attempted using the 1-NN baseline <ref type="bibr" coords="13,349.70,443.49,12.99,4.94" target="#b1">[2]</ref> as is to generate the diagnostic text within the captions, which however achieved a lower score than all the aforementioned approaches. Although at first, one could interpret this as RAG models, in which the generator acts as a parametric memory, whereas the retriever ğ‘ ğœ‚ embodies a non-parametric memory in the query encoder q examined in section 3.4.2, perform better than solely the 1-NN baseline; when the latter is combined with abstractive summarization techniques for the diagnostic texts of ğ‘˜ additional visually similar images from the training set, where ğ‘˜ âˆˆ Z + , it may perform better as it is indicated in section 3.4.1 and Table <ref type="table" coords="13,310.92,538.34,3.80,4.94" target="#tab_5">6</ref>. Our models use a pre-trained AlexNet or VGG-16 CNN on ImageNet classification dataset as image encoder, our training, validation and development sets that are described in section 2 merged together and achieve a BLEU score according to Table <ref type="table" coords="13,174.15,578.99,3.74,4.94" target="#tab_8">8</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Directions for future work</head><p>In this work, we developed CNN-based image encoders trained end-to-end for tags assignment or combined with heuristics such as the 1-NN baseline for either concept prediction or caption generation, which although is really simple performs rather well if combined with abstractive summarization algorithms, as highlighted in section 3.4.1 as well as the study in <ref type="bibr" coords="14,442.00,155.19,11.29,4.94" target="#b1">[2]</ref>, where this baseline itself performs well for the Indiana University chest X-ray Collection <ref type="bibr" coords="14,444.57,168.74,18.07,4.94" target="#b32">[33]</ref> (IU chest X-ray dataset). Future work could focus on the use of task-specific models for summarization, such as Bio-BERT <ref type="bibr" coords="14,172.30,195.84,16.36,4.94" target="#b33">[34]</ref>, further fine-tuning on the number of neighbours ğ‘˜ and the summary maximum length ğ‘› in section 3.4.1 and consideration of potential associations between the two subtasks during 1-NN baseline extension.</p><p>Furthermore, although higher quantitative accuracy is most often better, there are categorical differences of the DC methods as well, which relate to their qualitative evaluation and indicate their practical usefulness. It is an open question how we may obtain practical information about the quality of the generated captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ethical considerations</head><p>Development of Diagnostic Captioning systems based on novel DL architectures could have both positive and negative societal impacts. My proposed work, for example, may be used for analyzing medical image data in undeveloped regions or countries under development. This is related to the 3 rd goal of United Nations Sustainability Goals (UNSG) about ensuring good health and well-being and the 10 th goal about reduced inequalities. On the other hand, privacy issues might arise from the use of medical data and "concerns over the sensitive information security and privacy" <ref type="bibr" coords="14,187.18,417.05,17.96,4.94" target="#b34">[35]</ref> that may also be related to the General Data Protection Regulation (GDPR) and EU legislation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,178.77,170.89,309.16,9.77;5,119.62,184.20,29.70,9.57;5,149.32,182.25,10.82,6.99;5,160.63,184.44,206.86,9.77"><head></head><label></label><figDesc>DenseNet161 with fine-tuned classification head, learning rate 5 Ã— 10 -4 , AdamW optimizer and gradient clipping</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,89.29,267.72,416.70,9.65;6,89.29,281.72,416.69,4.79;6,89.29,293.68,416.70,4.79;6,89.29,305.63,156.12,4.79;6,95.13,141.76,133.35,100.01"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: ğ¹ 1 and accuracy scores plots per epoch for the models described (a) in section 3.2.1, (b) in section 3.2.2, as well as (c) in section 3.2.3. We observe that the classifications heads, which we finetune on ImageCLEFmedical 2022 data, appear to be sufficiently regularized (thus there is no overfitting) and to have used their maximum capacity.</figDesc><graphic coords="6,95.13,141.76,133.35,100.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,89.29,464.65,416.69,8.93;9,89.29,476.39,121.00,8.74;9,210.29,481.35,11.20,3.35;9,224.76,476.39,37.86,8.74;9,262.62,481.35,12.16,3.35;9,278.04,476.39,228.13,9.65;9,89.29,490.38,189.12,4.79;9,222.63,346.51,141.67,91.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) Schematic illustration of the error landscape with a high learning rate, (b) example plot of a cyclical learning rate with ğœ‚ min = 0.01, ğœ‚ max = 0.30, ğ‘› ğ‘  = 2 and (c) ğ¹ 1 and accuracy scores plots per epoch for the model described in section 3.2.6.</figDesc><graphic coords="9,222.63,346.51,141.67,91.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,88.99,90.49,422.36,155.71"><head>Table 1</head><label>1</label><figDesc>Summary of weak learners' architecture and training regime in model 182338 Backbone Net. Optimizer Learning Rate Linear Decay Batch size Epochs</figDesc><table coords="7,461.93,119.95,38.56,4.79"><row><cell>Subm. ID</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,88.99,414.97,422.36,203.53"><head>Table 2</head><label>2</label><figDesc>Summary of weak learners' architecture and training regime in model 181546</figDesc><table coords="7,95.27,444.44,416.08,174.07"><row><cell cols="5">Backbone Net. Optimizer Learning Rate Linear Decay Batch size Epochs</cell><cell>Subm. ID</cell></row><row><cell>AlexNet</cell><cell>AdamW</cell><cell>10 -4</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>AlexNet</cell><cell>AdamW</cell><cell>5 Ã— 10 -5</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>DenseNet121</cell><cell>AdamW</cell><cell>5 Ã— 10 -4</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>DenseNet121</cell><cell>AdamW</cell><cell>10 -3</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>DenseNet121</cell><cell>AdamW</cell><cell>10 -4</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>DenseNet161</cell><cell>Adam</cell><cell>10 -3</cell><cell>False</cell><cell>120</cell><cell>181750, 181715</cell></row><row><cell>DenseNet161</cell><cell>AdamW</cell><cell>10 -3</cell><cell>True</cell><cell>120</cell><cell>-</cell></row><row><cell>DenseNet161</cell><cell>Adam</cell><cell>5 Ã— 10 -4</cell><cell>False</cell><cell>120</cell><cell>-</cell></row><row><cell>DenseNet161</cell><cell>Adam</cell><cell>5 Ã— 10 -4</cell><cell>False</cell><cell>120</cell><cell>181753</cell></row><row><cell>DenseNet161</cell><cell>AdamW</cell><cell>5 Ã— 10 -4</cell><cell>False</cell><cell>120</cell><cell>182152</cell></row><row><cell>ResNet50</cell><cell>AdamW</cell><cell>10 -4</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>ResNet101</cell><cell>AdamW</cell><cell>10 -4</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>VGG-13</cell><cell>AdamW</cell><cell>10 -4</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>VGG-16</cell><cell>AdamW</cell><cell>10 -4</cell><cell>False</cell><cell>60</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,88.99,90.49,422.36,203.53"><head>Table 3</head><label>3</label><figDesc>Summary of weak learners' architecture and training regime in model 182155</figDesc><table coords="8,95.27,119.95,416.08,174.07"><row><cell cols="5">Backbone Net. Optimizer Learning Rate Linear Decay Batch size Epochs</cell><cell>Subm. ID</cell></row><row><cell>AlexNet</cell><cell>AdamW</cell><cell>10 -4</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>AlexNet</cell><cell>AdamW</cell><cell>5 Ã— 10 -5</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>DenseNet121</cell><cell>AdamW</cell><cell>5 Ã— 10 -4</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>DenseNet121</cell><cell>AdamW</cell><cell>10 -3</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>DenseNet161</cell><cell>Adam</cell><cell>10 -3</cell><cell>False</cell><cell>120</cell><cell>181750, 181715</cell></row><row><cell>DenseNet161</cell><cell>AdamW</cell><cell>10 -3</cell><cell>True</cell><cell>120</cell><cell>-</cell></row><row><cell>ResNet50</cell><cell>AdamW</cell><cell>10 -4</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>ResNet50</cell><cell>AdamW</cell><cell>5 Ã— 10 -5</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>ResNet101</cell><cell>AdamW</cell><cell>10 -4</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>ResNet101</cell><cell>AdamW</cell><cell>5 Ã— 10 -4</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>VGG-13</cell><cell>AdamW</cell><cell>10 -4</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>VGG-13</cell><cell>AdamW</cell><cell>5 Ã— 10 -5</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>VGG-16</cell><cell>AdamW</cell><cell>10 -4</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>VGG-16</cell><cell>AdamW</cell><cell>5 Ã— 10 -5</cell><cell>False</cell><cell>60</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,88.99,325.88,422.36,179.62"><head>Table 4</head><label>4</label><figDesc>Summary of weak learners' architecture and training regime in model 182154</figDesc><table coords="8,95.27,355.34,416.08,150.16"><row><cell cols="5">Backbone Net. Optimizer Learning Rate Linear Decay Batch size Epochs</cell><cell>Subm. ID</cell></row><row><cell>AlexNet</cell><cell>AdamW</cell><cell>10 -4</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>AlexNet</cell><cell>AdamW</cell><cell>5 Ã— 10 -5</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>DenseNet121</cell><cell>AdamW</cell><cell>5 Ã— 10 -4</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>DenseNet121</cell><cell>AdamW</cell><cell>10 -3</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>DenseNet121</cell><cell>AdamW</cell><cell>10 -4</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>DenseNet161</cell><cell>Adam</cell><cell>10 -3</cell><cell>False</cell><cell>120</cell><cell>181750, 181715</cell></row><row><cell>DenseNet161</cell><cell>AdamW</cell><cell>10 -3</cell><cell>True</cell><cell>120</cell><cell>-</cell></row><row><cell>DenseNet161</cell><cell>Adam</cell><cell>5 Ã— 10 -4</cell><cell>False</cell><cell>120</cell><cell>-</cell></row><row><cell>ResNet50</cell><cell>AdamW</cell><cell>10 -4</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>ResNet101</cell><cell>AdamW</cell><cell>10 -4</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>VGG-13</cell><cell>AdamW</cell><cell>10 -4</cell><cell>False</cell><cell>60</cell><cell>-</cell></row><row><cell>VGG-16</cell><cell>AdamW</cell><cell>10 -4</cell><cell>False</cell><cell>60</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,88.99,444.89,401.98,155.71"><head>Table 5</head><label>5</label><figDesc>Summary of our configurations' characteristics and statistics</figDesc><table coords="10,104.30,474.36,386.68,126.25"><row><cell cols="2">Backbone Network Section described</cell><cell>Type of model</cell><cell cols="2">F1 scores Submission ID</cell></row><row><cell>DenseNet161</cell><cell>Section 3.2.1</cell><cell>Deep Network Head</cell><cell>0.43601</cell><cell>181750</cell></row><row><cell>DenseNet161</cell><cell>Section 3.2.1</cell><cell>Deep Network Head</cell><cell>0.43601</cell><cell>181750</cell></row><row><cell>DenseNet161</cell><cell>Section 3.2.2</cell><cell>Deep Network Head</cell><cell>0.43558</cell><cell>181753</cell></row><row><cell>DenseNet161</cell><cell>Section 3.2.3</cell><cell>Deep Network Head</cell><cell>0.43539</cell><cell>182152</cell></row><row><cell>DenseNet variants</cell><cell>Section 3.2.4</cell><cell cols="2">Ensemble of Networks 0.43496</cell><cell>182338</cell></row><row><cell>Various networks</cell><cell>Section 3.2.5</cell><cell cols="2">Ensemble of Networks 0.43404</cell><cell>181546</cell></row><row><cell>Various networks</cell><cell>Section 3.2.5</cell><cell cols="2">Ensemble of Networks 0.43130</cell><cell>182155</cell></row><row><cell>Various networks</cell><cell>Section 3.2.5</cell><cell cols="2">Ensemble of Networks 0.42957</cell><cell>182154</cell></row><row><cell>DenseNet161</cell><cell>Section 3.2.6</cell><cell>Deep Network (full)</cell><cell>0.31687</cell><cell>182156</cell></row><row><cell>VGG-16</cell><cell>Section 3.2.7</cell><cell>Nearest Neighbour</cell><cell>0.25061</cell><cell>182331</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,88.99,90.49,368.02,107.89"><head>Table 6</head><label>6</label><figDesc>Summary of our configurations' hyper-parameters and statisticsBackbone Network Captions ğ‘˜ Tokens ğ‘› BLEU scores Submission ID</figDesc><table coords="12,161.05,129.86,280.18,68.51"><row><cell>AlexNet</cell><cell>ğ‘˜ = 9</cell><cell>ğ‘› = 15</cell><cell>0.29166</cell><cell>182337</cell></row><row><cell>AlexNet</cell><cell>ğ‘˜ = 4</cell><cell>ğ‘› = 15</cell><cell>0.28343</cell><cell>182286</cell></row><row><cell>AlexNet</cell><cell>ğ‘˜ = 3</cell><cell>ğ‘› = 15</cell><cell>0.27855</cell><cell>182284</cell></row><row><cell>AlexNet</cell><cell>ğ‘˜ = 2</cell><cell>ğ‘› = 15</cell><cell>0.27007</cell><cell>182285</cell></row><row><cell>AlexNet</cell><cell>ğ‘˜ = 4</cell><cell>ğ‘› = 5</cell><cell>0.25521</cell><cell>182271</cell></row><row><cell>AlexNet</cell><cell>ğ‘˜ = 3</cell><cell>ğ‘› = 5</cell><cell>0.25334</cell><cell>182272</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="12,88.99,583.94,315.04,60.07"><head>Table 7</head><label>7</label><figDesc>Summary of our configurations' image encoders and statistics</figDesc><table coords="12,191.24,613.40,212.79,30.61"><row><cell cols="3">Backbone Network BLEU scores Submission ID</cell></row><row><cell>AlexNet</cell><cell>0.25127</cell><cell>181712</cell></row><row><cell>VGG-16</cell><cell>0.23958</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="13,89.29,270.26,417.90,132.87"><head></head><label></label><figDesc>1:ğ‘–-1 ) to create a Language Model (LM) over the tokens vocabulary ğ’± given as input the latent documents ğ‘§ and queries ğ‘¥, which are the outputs of 1-NN baseline. During training, we treat questions-answers as input-output pairs i.e. (ğ‘¥, ğ‘¡) and train RAG-token by directly minimizing the negative marginal log-likelihood of generating output sequences ğ‘¦ on input sequences ğ‘¥. If ğ’Ÿ = {ğ‘¥ ğ‘— , ğ‘¡ ğ‘— } ğ‘— is the complete dataset, our training objective is: ğ‘™ cross (ğ‘¥, ğ‘¡; ğœƒ, ğœ‚) = -log ğ‘(ğ‘¦|ğ‘¥; ğœƒ, ğœ‚) âˆ‘ï¸ ğ‘— ğ‘™ cross (ğ‘¥ ğ‘— , ğ‘¡ ğ‘— ; ğœƒ, ğœ‚) = âˆ‘ï¸</figDesc><table /><note coords="13,305.10,396.15,3.42,6.99;13,316.74,381.05,86.54,10.63"><p><p>ğ‘—</p>-log ğ‘(ğ‘¦ ğ‘— |ğ‘¥ ğ‘— ; ğœƒ, ğœ‚)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="13,88.99,604.41,315.04,60.07"><head>Table 8</head><label>8</label><figDesc>Summary of our configurations' image encoders and statistics</figDesc><table coords="13,191.24,633.87,212.79,30.61"><row><cell cols="3">Backbone Network BLEU scores Submission ID</cell></row><row><cell>AlexNet</cell><cell>0.24064</cell><cell>181711</cell></row><row><cell>VGG-16</cell><cell>0.22757</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="15,112.66,114.54,394.53,4.94;15,112.66,128.09,394.53,4.94;15,112.48,141.64,394.70,4.94;15,112.66,155.19,394.04,4.94;15,112.66,168.74,397.48,4.94;15,112.66,181.47,228.04,7.90" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="15,253.73,141.64,248.71,4.94">Machine learning for metabolic engineering: A review</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">E</forename><surname>Lawson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>MartÃ­</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Radivojevic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">V R</forename><surname>Jonnalagadda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gentz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Hillson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Peisert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">A</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Petzold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">W</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Tanjore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Garcia Martin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ymben.2020.10.005</idno>
		<ptr target="https://doi.org/10.1016/j.ymben.2020.10.005" />
	</analytic>
	<monogr>
		<title level="m" coord="15,179.12,182.29,156.84,4.94">Strategies of Metabolic Engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="34" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,195.84,395.17,4.94;15,112.66,209.39,69.57,4.94;15,197.70,209.39,309.47,4.94;15,112.66,222.12,110.72,7.90" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,417.13,195.84,90.70,4.94;15,112.66,209.39,64.93,4.94">Diagnostic captioning: a survey</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Papamichail</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10115-022-01684-7</idno>
	</analytic>
	<monogr>
		<title level="j" coord="15,197.70,209.39,176.62,4.94">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,236.48,395.17,4.94;15,111.81,250.03,395.37,4.94;15,112.66,263.58,393.33,4.94;15,112.66,277.13,216.46,4.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,326.22,250.03,180.96,4.94;15,112.66,263.58,177.87,4.94">Overview of ImageCLEFmedical 2022caption prediction and concept detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>RÃ¼ckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>GarcÃ­a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>BrÃ¼ngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>SchÃ¤fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,313.28,263.58,192.71,4.94;15,112.66,277.13,97.38,4.94">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,290.68,394.53,4.94;15,112.66,304.23,395.01,4.94;15,112.66,317.78,394.53,4.94;15,112.48,331.33,393.51,4.94;15,112.66,344.88,393.33,4.94;15,112.66,358.43,393.33,4.94;15,112.66,371.98,393.53,4.94;15,112.66,385.53,170.14,4.94" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,294.00,331.33,211.99,4.94;15,112.66,344.88,261.46,4.94">Overview of the ImageCLEF 2022: Multimedia retrieval in medical, social media and nature applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>RÃ¼ckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>BrÃ¼ngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>SchÃ¤fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,401.20,344.88,104.79,4.94;15,112.66,358.43,393.33,4.94;15,112.66,371.98,227.21,4.94">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<title level="s" coord="15,348.61,371.98,157.57,4.94;15,112.66,385.53,31.10,4.94">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,399.07,393.33,4.94;15,112.33,412.62,395.50,4.94;15,112.66,426.17,394.52,4.94;15,112.66,439.72,393.53,4.94;15,112.66,453.27,393.33,4.94;15,112.66,466.82,393.33,4.94;15,112.66,480.37,394.52,4.94;15,112.66,491.67,394.41,9.72;15,112.66,505.22,395.00,9.72;15,112.66,521.02,193.04,4.94" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,370.24,399.07,135.74,4.94;15,112.33,412.62,171.56,4.94;15,448.67,439.72,57.52,4.94;15,112.66,453.27,393.33,4.94;15,112.66,466.82,263.92,4.94">Intravascular Imaging and Computer Assisted Stenting -and -Large-Scale Annotation of Biomedical Data and Expert Label Synthesis -7th Joint International Workshop</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>RÃ¼ckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01364-6_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01364-6_20.doi:10.1007/978-3-030-01364-6\_20" />
	</analytic>
	<monogr>
		<title level="m" coord="15,383.99,466.82,121.99,4.94;15,112.66,480.37,102.28,4.94;15,287.18,480.37,173.04,4.94">CVII-STENT 2018 and Third International Workshop</title>
		<title level="s" coord="15,371.63,491.67,135.44,9.72;15,112.66,505.22,17.17,9.72">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Taylor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Balocco</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Sznitman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Duong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Zahnd</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Demirci</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Moriconi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Cheplygina</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Mateus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Trucco</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Granger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Jannin</surname></persName>
		</editor>
		<meeting><address><addrLine>LABELS; Granada, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-09-16">2018. September 16, 2018. 2018</date>
			<biblScope unit="volume">11043</biblScope>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
	<note>Held in Conjunction with MICCAI 2018</note>
</biblStruct>

<biblStruct coords="15,112.66,534.57,394.53,4.94;15,112.28,548.12,395.55,4.94;15,112.66,561.67,394.51,4.94;15,112.66,574.39,104.78,7.90" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="15,329.30,548.12,178.53,4.94;15,112.66,561.67,65.47,4.94">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j" coord="15,192.14,561.67,193.12,4.94">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,588.76,393.33,4.94;15,112.66,602.31,393.33,4.94;15,112.66,613.61,393.32,9.72;15,112.66,627.16,394.61,9.72;15,112.66,642.96,213.95,4.94" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="15,210.80,588.76,295.18,4.94;15,112.66,602.31,38.41,4.94">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v9/glorot10a.html" />
	</analytic>
	<monogr>
		<title level="m" coord="15,317.06,602.31,188.93,4.94;15,112.66,615.86,227.00,4.94;15,402.73,613.61,103.25,9.72;15,112.66,627.16,79.11,9.72">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Titterington</surname></persName>
		</editor>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>PMLR, Chia Laguna Resort, Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="15,112.66,656.51,393.33,4.94;15,112.66,669.24,349.33,7.90" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="15,183.33,656.51,287.16,4.94">One weird trick for parallelizing convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1404.5997" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,90.23,393.33,4.94;16,112.66,103.78,394.53,4.94;16,112.66,117.33,237.02,4.94" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="16,356.26,90.23,149.73,4.94;16,112.66,103.78,39.90,4.94">Densely Connected Convolutional Networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.243</idno>
	</analytic>
	<monogr>
		<title level="m" coord="16,197.71,103.78,304.54,4.94">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,130.88,393.32,4.94;16,112.66,144.43,395.00,4.94;16,112.66,157.97,137.64,4.94" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,254.59,130.88,207.56,4.94">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m" coord="16,112.66,144.43,307.90,4.94">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,171.52,393.33,4.94;16,112.66,185.07,159.81,4.94" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="16,247.99,171.52,258.00,4.94;16,112.66,185.07,49.16,4.94">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arXiv 1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,198.62,395.17,4.94;16,112.66,212.17,65.03,4.94;16,194.92,212.17,312.74,4.94;16,112.66,224.90,97.35,7.90" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="16,207.66,198.62,300.17,4.94;16,112.66,212.17,59.84,4.94">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1905.11946" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,239.27,395.16,4.94;16,112.66,252.82,393.33,4.94;16,112.41,266.37,393.57,4.94;16,112.66,279.92,393.08,4.94" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="16,419.65,252.82,86.34,4.94;16,112.41,266.37,259.29,4.94">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m" coord="16,394.36,266.37,111.63,4.94;16,112.66,279.92,125.91,4.94">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,293.47,393.33,4.94;16,112.33,307.02,395.33,4.94;16,112.66,320.56,238.63,4.94" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="16,297.83,293.47,208.16,4.94;16,112.33,307.02,121.30,4.94">Weakly-Supervised Semantic Segmentation via Transformer Explainability</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Athanasiadis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Moschovis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tuoma</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rcEDhGX3AY" />
	</analytic>
	<monogr>
		<title level="m" coord="16,257.42,307.02,160.12,4.94">ML Reproducibility Challenge 2021</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Fall Edition</note>
</biblStruct>

<biblStruct coords="16,112.66,334.11,235.22,4.94" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="16,176.17,334.11,102.98,4.94">The Wisdom of Crowds</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Surowiecki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Anchor</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,347.66,393.32,4.94;16,112.66,361.21,160.04,4.94" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="16,190.45,347.66,196.84,4.94">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,395.87,347.66,110.12,4.94;16,112.66,361.21,128.12,4.94">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,374.76,394.61,4.94;16,112.31,388.31,179.94,4.94" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rk6qdGgCZ" />
		<title level="m" coord="16,220.12,374.76,201.88,4.94">Fixing Weight Decay Regularization in Adam</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,401.86,394.53,4.94;16,112.66,415.41,393.33,4.94;16,112.66,428.96,394.61,4.94;16,112.31,441.69,218.89,7.90" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="16,322.01,415.41,183.98,4.94;16,112.66,428.96,210.62,4.94">CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bagul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">S</forename><surname>Shpanskaya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1711.05225" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,456.06,393.33,4.94;16,112.33,469.61,394.82,4.94;16,112.66,483.16,395.01,4.94;16,112.66,496.70,179.18,4.94" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="16,280.86,456.06,225.13,4.94;16,112.33,469.61,210.54,4.94">Comparison of Various Learning Rate Scheduling Techniques on Convolutional Neural Network</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Konar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tripathi</surname></persName>
		</author>
		<idno type="DOI">10.1109/SCEECS48394.2020.94</idno>
	</analytic>
	<monogr>
		<title level="m" coord="16,376.46,469.61,130.69,4.94;16,112.66,483.16,321.31,4.94">IEEE International Students&apos; Conference on Electrical,Electronics and Computer Science (SCEECS)</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,510.25,393.33,4.94;16,112.66,523.80,123.64,4.94" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="16,221.12,510.25,161.93,4.94">A stochastic approximation method</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,395.54,510.25,110.45,4.94;16,112.66,523.80,39.71,4.94">Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="400" to="407" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,537.35,394.53,4.94;16,112.66,550.90,64.50,4.94" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="16,229.67,537.35,272.77,4.94">Note on Learning Rate Schedules for Stochastic Optimization</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Darken</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">E</forename><surname>Moody</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,564.45,395.01,4.94;16,112.66,577.18,264.57,7.90" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="16,168.32,564.45,205.06,4.94">No More Pesky Learning Rate Guessing Games</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1506.01186" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,591.55,394.53,4.94;16,112.66,605.10,394.62,4.94;16,112.66,617.83,239.89,7.90" xml:id="b22">
	<monogr>
		<title level="m" type="main" coord="16,112.66,605.10,231.71,4.94">Clinically Accurate Chest X-Ray Report Generation</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">B A</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1904.02633" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,632.20,393.33,4.94;16,112.66,645.75,393.32,4.94;16,112.66,659.29,199.65,4.94" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="16,269.46,632.20,236.53,4.94;16,112.66,645.75,135.13,4.94">PEGASUS: Pre-Training with Extracted Gap-Sentences for Abstractive Summarization</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,271.41,645.75,234.57,4.94;16,112.66,659.29,169.27,4.94">Proceedings of the 37th International Conference on Machine Learning, ICML&apos;20, JMLR.org</title>
		<meeting>the 37th International Conference on Machine Learning, ICML&apos;20, JMLR.org</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,672.84,394.53,4.94;17,112.66,90.23,394.53,4.94;17,112.66,103.78,393.32,4.94;17,112.66,117.33,394.03,4.94;17,112.66,130.88,278.51,4.94" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="17,175.33,90.23,108.17,4.94">Attention is All you Need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="s" coord="17,314.47,103.78,191.52,4.94;17,112.66,117.33,34.49,4.94">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,144.43,393.33,4.94;17,112.66,157.97,393.33,4.94;17,112.66,171.52,393.32,4.94;17,112.66,185.07,393.33,4.94;17,112.66,198.62,394.03,4.94;17,112.66,212.17,185.51,4.94" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="17,323.15,144.43,182.83,4.94;17,112.66,157.97,186.91,4.94">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://aclanthology.org/N19-1423.doi:10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m" coord="17,327.87,157.97,178.11,4.94;17,112.66,171.52,393.32,4.94;17,112.66,185.07,99.97,4.94">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="17,112.66,225.72,393.33,4.94;17,112.66,239.27,393.33,4.94;17,112.66,252.82,394.53,4.94;17,112.66,266.37,397.48,4.94;17,112.66,279.10,121.09,7.90" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="17,318.69,225.72,187.29,4.94;17,112.66,239.27,100.78,4.94">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
		<ptr target="https://aclanthology.org/P02-1040.doi:10.3115/1073083.1073135" />
	</analytic>
	<monogr>
		<title level="m" coord="17,237.08,239.27,268.90,4.94;17,112.66,252.82,133.28,4.94">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,293.47,395.01,4.94;17,112.34,307.02,395.48,4.94;17,112.66,320.56,393.33,4.94;17,112.33,334.11,395.50,4.94;17,112.66,347.66,394.04,4.94;17,112.66,361.21,208.79,4.94" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="17,287.14,307.02,220.69,4.94;17,112.66,320.56,93.32,4.94">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>KÃ¼ttler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>-T. Yih</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>RocktÃ¤schel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="17,143.82,334.11,239.70,4.94">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9459" to="9474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,374.76,395.17,4.94;17,112.66,388.31,156.41,4.94" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="17,255.31,374.76,179.93,4.94">Billion-scale similarity search with GPUs</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>JÃ©gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,444.73,374.76,63.11,4.94;17,112.66,388.31,77.55,4.94">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="535" to="547" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,401.86,395.17,4.94;17,112.66,415.41,395.17,4.94;17,112.66,428.96,393.33,4.94;17,112.66,442.51,395.17,4.94;17,112.66,456.06,395.01,4.94;17,112.66,469.61,191.55,4.94" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="17,148.38,415.41,359.46,4.94;17,112.66,428.96,180.48,4.94">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
		<ptr target="https://aclanthology.org/2020.acl-main.703.doi:10.18653/v1/2020.acl-main.703" />
	</analytic>
	<monogr>
		<title level="m" coord="17,320.04,428.96,185.95,4.94;17,112.66,442.51,395.17,4.94;17,112.66,456.06,32.81,4.94">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,483.16,393.33,4.94;17,112.66,496.70,393.33,4.94;17,112.66,510.25,393.33,4.94;17,112.66,523.80,394.03,4.94;17,112.66,537.35,303.46,4.94" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="17,477.87,483.16,28.12,4.94;17,112.66,496.70,258.10,4.94">Dense Passage Retrieval for Open-Domain Question Answering</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.550</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-main.550.doi:10.18653/v1/2020.emnlp-main.550" />
	</analytic>
	<monogr>
		<title level="m" coord="17,397.16,496.70,108.82,4.94;17,112.66,510.25,393.33,4.94;17,112.66,523.80,129.01,4.94">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6769" to="6781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,550.90,394.62,4.94;17,112.66,564.45,393.33,4.94;17,112.66,575.75,394.52,9.72;17,112.66,591.55,394.04,4.94;17,112.66,605.10,84.11,4.94" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="17,224.27,550.90,263.02,4.94">Learning and Inference via Maximum Inner Product Search</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mussmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v48/mussmann16.html" />
	</analytic>
	<monogr>
		<title level="m" coord="17,282.12,564.45,223.87,4.94;17,112.66,578.00,96.25,4.94;17,281.10,575.75,187.60,9.72">Proceedings of The 33rd International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting>The 33rd International Conference on Machine Learning<address><addrLine>PMLR, New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2587" to="2596" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="17,112.66,618.65,394.53,4.94;17,112.66,632.20,394.62,4.94;17,112.48,645.75,393.50,4.94;17,112.66,659.29,393.33,4.94;17,112.28,672.84,393.70,4.94;18,112.66,90.23,79.64,4.94" xml:id="b32">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rosenman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shooshan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Thoma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="DOI">10.1093/jamia/ocv080</idno>
		<idno>Copyright: Â© 2015</idno>
	</analytic>
	<monogr>
		<title level="j" coord="17,236.08,632.20,271.19,4.94;17,112.48,645.75,31.69,4.94">Journal of the American Medical Informatics Association : JAMIA</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="304" to="310" />
			<date type="published" when="2015">2016. 2015</date>
			<publisher>American Medical Informatics Association</publisher>
		</imprint>
	</monogr>
	<note>This work is written by US Government employees and is in the public domain in the US</note>
</biblStruct>

<biblStruct coords="18,112.66,103.78,393.33,4.94;18,112.66,117.33,394.53,4.94;18,112.41,130.88,216.47,4.94" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="18,355.95,103.78,150.04,4.94;18,112.66,117.33,251.54,4.94">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz682</idno>
	</analytic>
	<monogr>
		<title level="j" coord="18,372.87,117.33,64.30,4.94">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,144.43,395.17,4.94;18,112.39,157.97,394.88,4.94;18,112.66,171.52,397.48,4.94;18,112.66,184.25,393.33,7.90;18,112.66,198.62,395.17,4.94;18,112.66,212.17,393.33,4.94;18,112.66,225.72,223.74,4.94" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="18,386.51,144.43,121.32,4.94;18,112.39,157.97,138.22,4.94">Big data security and privacy in healthcare: A Review</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Abouelmehdi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Beni-Hssane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Khaloufi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Saadi</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.procs.2017.08.292</idno>
		<ptr target="https://doi.org/10.1016/j.procs.2017.08.292" />
	</analytic>
	<monogr>
		<title level="m" coord="18,395.74,185.07,110.25,4.94;18,112.66,198.62,395.17,4.94;18,112.66,212.17,393.33,4.94;18,112.66,225.72,218.49,4.94">Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2017) / The 7th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2017) / Affiliated Workshops</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
