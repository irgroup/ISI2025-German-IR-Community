<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,396.50,15.42;1,89.29,106.66,407.30,15.42;1,89.29,128.58,259.02,15.43">Detecting Concepts and Generating Captions from Medical Images: Contributions of the VCMI Team to ImageCLEFmedical 2022 Caption</title>
				<funder ref="#_PKprDSU">
					<orgName type="full">ERDF -European Regional Fund</orgName>
				</funder>
				<funder ref="#_3XqYjH4">
					<orgName type="full">Portuguese Foundation for Science and Technology -FCT</orgName>
				</funder>
				<funder ref="#_ACjF2JF #_zRknPGt">
					<orgName type="full">CMU -Portugal International Partnership</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,156.89,76.14,11.96"><forename type="first">Isabel</forename><surname>Rio-Torto</surname></persName>
							<email>isabel.riotorto@inesctec.pt</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Departamento de Ciência de Computadores</orgName>
								<orgName type="department" key="dep2">Faculdade de Ciências</orgName>
								<orgName type="institution">Universidade do Porto</orgName>
								<address>
									<addrLine>Rua do Campo Alegre s/n</addrLine>
									<postCode>4169-007</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">INESC TEC</orgName>
								<address>
									<addrLine>Campus da FEUP Rua Dr. Roberto Frias s/n</addrLine>
									<postCode>4200-465</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,183.31,156.89,84.25,11.96"><forename type="first">Cristiano</forename><surname>Patrício</surname></persName>
							<email>cristiano.p.patricio@inesctec.pt</email>
							<affiliation key="aff1">
								<orgName type="laboratory">INESC TEC</orgName>
								<address>
									<addrLine>Campus da FEUP Rua Dr. Roberto Frias s/n</addrLine>
									<postCode>4200-465</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Departamento de Informática</orgName>
								<orgName type="institution">Universidade da Beira Interior</orgName>
								<address>
									<addrLine>Rua Marquês de Ávila e Bolama</addrLine>
									<postCode>6201-001</postCode>
									<settlement>Covilhã</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,285.44,156.89,95.54,11.96"><forename type="first">Helena</forename><surname>Montenegro</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">INESC TEC</orgName>
								<address>
									<addrLine>Campus da FEUP Rua Dr. Roberto Frias s/n</addrLine>
									<postCode>4200-465</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Faculdade de Engenharia</orgName>
								<orgName type="institution">Universidade do Porto</orgName>
								<address>
									<addrLine>Rua Dr. Roberto Frias s/n</addrLine>
									<postCode>4200-465</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,416.93,156.89,79.62,11.96"><forename type="first">Tiago</forename><surname>Gonçalves</surname></persName>
							<email>tiago.f.goncalves@inesctec.pt</email>
							<affiliation key="aff1">
								<orgName type="laboratory">INESC TEC</orgName>
								<address>
									<addrLine>Campus da FEUP Rua Dr. Roberto Frias s/n</addrLine>
									<postCode>4200-465</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Faculdade de Engenharia</orgName>
								<orgName type="institution">Universidade do Porto</orgName>
								<address>
									<addrLine>Rua Dr. Roberto Frias s/n</addrLine>
									<postCode>4200-465</postCode>
									<settlement>Porto</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,396.50,15.42;1,89.29,106.66,407.30,15.42;1,89.29,128.58,259.02,15.43">Detecting Concepts and Generating Captions from Medical Images: Contributions of the VCMI Team to ImageCLEFmedical 2022 Caption</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">CFB28C066BA584EA3DA34636A9DD7B2A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>concept retrieval</term>
					<term>contrastive learning</term>
					<term>image captioning</term>
					<term>medical concept detection</term>
					<term>multi-label classification</term>
					<term>natural language generation</term>
					<term>vision transformers</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the main contributions of the VCMI Team to the ImageCLEFmedical 2022 Caption task. We addressed both the concept detection and caption prediction tasks. Regarding concept detection, our team employed three different strategies: multi-label classification, in which a convolutional neural network aims to simultaneously predict all the concepts from an image considering only the 100 most frequent concepts; concept retrieval, in which a model learns to map concepts and images into a common latent space where images are closer to the concepts they contain; and semantic-based multi-label classification, which consists of training several models, each one specialised in predicting concepts from a given semantic type, and an aggregation operation to obtain the final prediction. Our best submission attained an F1-score of 0.433, placing 5th among 11 teams, and the best Secondary F1-score (0.863). Regarding the caption prediction task, our team designed two different approaches: a Vision Encoder-Decoder Transformer, that receives the input images as a sequence of 16 × 16 patches and is trained for next token prediction; and a modified Object-Semantics Aligned Pre-training for Vision-and-Language Tasks (OSCAR) model, i.e. an encoder-only Transformer, trained for masked language modelling, and modified to receive as input a sequence of image patches and the image concepts, besides the caption. Our best submission, the Vision Encoder-Decoder, attained a Bilingual Evaluation Understudy (BLEU) score of 0.306 and ranked 4th among 10 teams.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>ImageCLEF 2022 <ref type="bibr" coords="2,164.29,111.28,12.69,10.91" target="#b0">[1]</ref> is an evaluation campaign organised as part of the CLEF Initiative<ref type="foot" coords="2,464.38,109.52,3.71,7.97" target="#foot_0">1</ref> (Conference and Labs of the Evaluation Forum, formerly known as Cross-Language Evaluation Forum). The 2022 edition included four tasks related to different application domains (i.e., Internet, Medical, Nature, and Social Media). These challenges encompass the common objective of promoting the evaluation of technologies for annotation, indexing and retrieval of visual data, while contributing to the access to extensive collections of images in various usage scenarios and application domains.</p><p>Our team, composed by four members of the Visual Computing and Machine Intelligence (VCMI) Research Group of the Institute for Systems and Computer Engineering, Technology and Science (INESC TEC) from Porto, Portugal, participated in the ImageCLEFmedical 2022 Caption task <ref type="bibr" coords="2,148.84,246.77,12.89,10.91" target="#b1">[2]</ref> wherein the main motivation is to develop algorithms that can interpret and summarise insights gained from medical images. This challenge consisted of two independent, but complementary, tasks: concept detection, which aims to identify the presence of relevant concepts in a large corpus of medical images; and caption prediction, which aims to generate coherent textual descriptions of a medical image.</p><p>We addressed both the concept detection and caption prediction tasks. For the concept detection task, we developed three different approaches: multi-label classification, in which a convolutional neural network (CNN) aims to simultaneously predict all the concepts from an image; concept retrieval, in which a model learns to map concepts and images into a common latent space where images are closer to the concepts they contain; and semantic-based multilabel classification, which consists of training several models, each one specialised in predicting concepts from a given semantic type, and an aggregation operation to obtain the final prediction. Our best submission attained an F1-score of 0.433, ranking 5th among 11 teams (the best team achieved an F1-score of 0.451), and the best Secondary F1-score (0.863).</p><p>For the caption prediction task, we designed two different approaches: a Vision Encoder-Decoder Transformer, that receives the input images as sequences of 16x16 patches and is trained for next token prediction; and a modified Object-Semantics Aligned Pre-training for Vision-and-Language Tasks (OSCAR) model <ref type="bibr" coords="2,288.77,477.11,11.45,10.91" target="#b2">[3]</ref>, i.e. an encoder-only Transformer, trained for masked language modelling, and modified to receive as input a sequence of image patches and the image concepts, besides the caption. Our best submission, the Vision Encoder-Decoder, attained a Bilingual Evaluation Understudy (BLEU) score of 0.306 and ranked 4th among 10 teams (the best team achieved a BLEU score of 0.483).</p><p>The remainder of this paper is organised as follows: section 2 provides an overview of the data provided by the organisation to address the selected tasks and describes our exploratory data analysis; section 3 details the different proposals developed to solve the aforementioned tasks; section 4 presents the results and their discussion; and section 5 concludes this paper and recommends future work directions. The code related to this paper is publicly available in a GitHub repository 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Overview</head><p>The data set provided in this competition is a subset of the extended Radiology Objects in COntext (ROCO) data set <ref type="bibr" coords="3,205.00,145.80,11.46,10.91" target="#b3">[4]</ref>. As in previous editions, the data set originates from biomedical articles of the PMC OpenAccess subset <ref type="bibr" coords="3,264.33,159.35,11.39,10.91" target="#b1">[2]</ref>. The version provided to the participants is already divided into train (83,275 radiology images), validation (7,645 radiology images) and test (7,601 radiology images) sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Exploratory Data Analysis</head><p>We considered it important to do an exploratory data analysis step before delving into the development of technical strategies to tackle both the Concept Detection and Caption Prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Concept Detection Task</head><p>For the concept detection task, we analysed the data from two different perspectives: imagebased, where we computed the average, minimum and maximum number of concepts per image, and the 𝑚 most (Top-𝑚) and the 𝑙 least (Bottom-𝑙) frequent concepts (see Table <ref type="table" coords="3,476.05,339.70,3.62,10.91">1</ref>); and concept-based, where we computed the total number of concepts, the average, minimum and maximum number of images where each concept is present, the concepts that appear in 𝑛 images or less, and the number of concepts that do not appear in any image (see Table <ref type="table" coords="3,475.76,380.34,3.57,10.91" target="#tab_1">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Exploratory data analysis for the concept detection task, from the image-based perspective. Note: "Avg. ", "Min." and "Max." stand for "Average", "Minimum" and "Maximum" number of concepts per image, respectively. Top-3 corresponds to the 3 most frequent concepts, while Bottom-3 corresponds to the 3 least frequent concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subset</head><p>Total Avg. Min.   <ref type="table" coords="4,128.24,86.97,5.17,10.91">1</ref> shows that every image has at least one concept and that there is an average of around 5 concepts per image. Furthermore, in the Top-3 column, we observe that two of the most predominant concepts in the training data are also the most common in the validation data. Figures <ref type="figure" coords="4,152.63,127.61,5.17,10.91">6</ref> and<ref type="figure" coords="4,181.21,127.61,5.17,10.91">7</ref> in the Appendix also show that 21 of the 31 concepts exposed as the most predominant in the training data are also the most common in the validation data. An important outcome from this analysis is that the most common concepts are present in both training and validation sets. Intuitively, and assuming that the distribution of concepts is similar on the test set, this observation allows us to use the Top-𝑚 (most frequent) concepts for the concept prediction task without losing too much information. Additionally, 83,110 (99.80%) of the training images and 7,617 (99.63%) of the validation images contain at least one of the Top-100 most frequent concepts, which further supports the hypothesis that removing the least-frequent concepts leads to a small loss of information, which might have little impact on the results of a model designed to predict these concepts.</p><p>Table <ref type="table" coords="4,126.56,263.11,4.98,10.91" target="#tab_1">2</ref> shows that 58.76% of the concepts available in the data appears only in 10 (0.012%) of the training images or less. These results suggest that the concept prediction task, interpreted as a multi-label classification task where multiple labels can be assigned to the same image, is highly imbalanced in the training data. Furthermore, we observe that, out of the 8,374 existing concepts, 7,842 (93.65%) are reflected in less than 10 (0.13%) of the validation images, of which 4,017 (47.97% out of all concepts) are not reflected in the validation data at all. These observations suggest not only that the validation data is imbalanced, but also that the validation set has a very limited capacity to verify the quality and the generalisation power of any model regarding the detection of those concepts. Please refer to Figures <ref type="figure" coords="4,334.59,371.50,5.07,10.91" target="#fig_7">8</ref> and<ref type="figure" coords="4,361.54,371.50,5.07,10.91" target="#fig_9">9</ref> for additional details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Caption Prediction Task</head><p>For the caption prediction task, we analysed the length of the captions, i.e. the number of tokens obtained after tokenization <ref type="foot" coords="4,212.97,432.62,3.71,7.97" target="#foot_2">3</ref> , thus extracting the average, minimum, and maximum caption lengths (see Table <ref type="table" coords="4,175.08,447.93,3.65,10.91" target="#tab_2">3</ref>). The minimum length is 3 in both training and validation sets, while the maximum length corresponds to 577 in the training set and 339 in the validation set. We can observe that the vast majority of the images have description lengths close to the average number of tokens. In fact, 89.4% of the training images have less than 50 tokens and 99,1% have less than 100 tokens. The same tendency can be verified in the validation set, where 85.8% of the images have less than 50 tokens and 98.4% have less than 100 tokens. This analysis is further complemented by Figures <ref type="figure" coords="4,206.60,529.22,10.15,10.91" target="#fig_10">10</ref> and<ref type="figure" coords="4,238.61,529.22,8.36,10.91" target="#fig_0">11</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>This section describes all the strategies we used to tackle both the Concept Detection and Caption Prediction tasks, as well as the data processing steps applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Processing</head><p>All images were first resized to have 224 pixels of height, while keeping their aspect ratios, and were converted to greyscale. During training of the several approaches, random square crops of 224×224 were used. In the multi-label-based concept detection proposals the images were also rotated by at most 45 ∘ with 50% probability. Following the conclusions derived in the exploratory data analysis phase, all captions were tokenized using the distil-gpt2 tokenizer from the Hugging Face Transformers library and truncated or padded to 100 tokens in the Vision Encoder-Decoder approach and to 50 tokens in the modified OSCAR architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Concept Detection Task</head><p>The concept detection task consists of a multi-label classification problem, i.e., there are more than two classes and each data point may be labelled with more than one non-mutually exclusive class. To solve this task, we employed three different strategies: multi-label classification, concept retrieval, and semantic-based multi-label classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Multi-label Classification</head><p>A straightforward method to solve the task of concept detection is to use a multi-label classification model, since a single image can have multiple non-mutually exclusive concepts associated with it. The concepts are defined according to the Unified Medical Language System (UMLS) <ref type="bibr" coords="5,493.30,436.20,12.68,10.91" target="#b4">[5]</ref> 2020AB release, in which each concept has a unique identifier (CUI). Table <ref type="table" coords="5,417.12,449.75,4.98,10.91" target="#tab_3">4</ref> presents the Top-3 most frequent concepts in the training set and their frequency in both training and validation sets. Based on the exploratory data analysis, we adopted two strategies for predicting the concepts: (i) train the model to predict all the 8,374 concepts, and (ii) train the model to predict only the 𝑚 most frequent concepts, where 𝑚 = 100. For this, a multi-label classification-based model was developed to predict the associated concepts for each image. Specifically, we adapted the DenseNet-121 <ref type="bibr" coords="6,156.40,86.97,12.99,10.91" target="#b5">[6]</ref> by modifying the classification layer to have 𝑁 outputs, where 𝑁 is the number of concepts, and 𝑁 = 8, 374 or 𝑁 = 100. Regarding the training process, the model was trained using the binary cross-entropy loss and the adaptive moment estimation (Adam) <ref type="bibr" coords="6,295.02,376.68,13.00,10.91" target="#b7">[8]</ref> optimiser with its default hyperparameters during 100 epochs with a learning rate of 10 -3 . Concretely, we adopted three strategies for training the model: (i) we fine-tuned the classification layer of the model and kept the remaining layers frozen ("Frozen Backbone"), (ii) we trained the whole model with all layers unfrozen ("Whole Network"), and (iii) we froze the backbone layers for 5 epochs and then unfroze them for the remaining epochs ("2 Phases"). The model with the best validation loss was used for the testing phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Concept Retrieval</head><p>As a second approach, we attempted to solve the concept detection task through concept retrieval: images and concepts are mapped into a common latent space where the images are expected to be closer to the concepts they contain. Using this model, we retrieve the closest concepts to the images. Figure <ref type="figure" coords="6,227.40,547.64,5.07,10.91" target="#fig_1">2</ref> presents an overview of the model.</p><p>The model is composed of an image encoder and a concept encoder, each responsible for translating images and concepts into their latent representations. The input concepts are represented as one-hot encodings. The image encoder is a CNN composed of four blocks of convolutional layers with max pooling and batch normalisation. The concept encoder is a multilayer perceptron composed of two fully-connected layers with LeakyReLU and Tanh activations, respectively.</p><p>During training, the model uses a contrastive loss <ref type="bibr" coords="6,334.31,642.48,13.00,10.91" target="#b8">[9]</ref> to minimise the distance between images and their corresponding concepts while maximising the distance between images and concepts they do not contain. The contrastive loss function used to train the model is expressed in Equation <ref type="formula" coords="7,143.35,296.51,3.70,10.91" target="#formula_0">1</ref>, where 𝐷 is the distance between the input concept and image, and 𝑦 is a binary value that represents the existence of the concept in the image. On separate experiments, we use two different distance measures: Euclidean distance and Cosine Similarity.</p><formula xml:id="formula_0" coords="7,176.29,359.40,329.69,13.13">ℒ 𝑐𝑜𝑛𝑡𝑟𝑎𝑠𝑡𝑖𝑣𝑒 = 𝑦 × 𝐷 2 + (1 -𝑦)[max(0, 1 -𝐷)] 2<label>(1)</label></formula><p>During training, on each batch, the network receives as input a set of 𝑘 existing concepts and 𝑘 images. For each batch, the network only needs to process each concept and image once. As a result, for each training batch, the network predicts a square distance matrix where each line corresponds to an image, each column to a single one-hot encoded concept, and each cell represents the distance between the respective image and concept. This approach is more computationally efficient than providing independent image-concept pairs to the network, as, in that approach, the same concept and image would have to be processed multiple times in the same iteration.</p><p>During inference, we retrieve the concepts that are closer to the image and whose distance to the image is smaller than a threshold. This threshold corresponds to the average distance between an image and the closest concept on the validation data.</p><p>Using this methodology, we performed three separate experiments: (i) train the concept retrieval model using Euclidean distance and representing all 8, 374 concepts in the network's latent space; (ii) train the model using Euclidean distance and mapping only the 100 most frequent concepts to the latent space; and (iii) train the model using Cosine Similarity and considering only the 100 most frequent concepts. We trained the models using the Adam optimiser <ref type="bibr" coords="7,133.44,601.84,11.28,10.91" target="#b7">[8]</ref>, with a learning rate of 10 -6 . During the training process, we used a subset of the training data (15%) for validation, to obtain the epoch at which each model obtained the best results. As such, the models for the three experimental settings were trained for 1550, 703, and 279 epochs, respectively.</p><p>We also perform experiments where we merge the results of the concept retrieval and multi-label classification models, in an attempt to improve the respective results. In the first experiment, the images are labelled according to the multi-label classification model. In cases where the images are not assigned any label by the multi-label classification model, we retrieve its closer concepts according to the concept retrieval model (we call this the "Ensemble (NaN)" model). In the second experiment, we merge the predictions of the two models using an OR operation, assigning to the image all the concepts that were predicted by either the multi-label classification model or the concept retrieval model (we refer to this as the "Ensemble (OR)" model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Semantic-based Multi-label Classification</head><p>To address this task from a "divide-and-conquer" perspective, we started with the conversion of each concept into its correspondent semantic type. We used the UMLS <ref type="bibr" coords="8,396.63,231.14,12.68,10.91" target="#b4">[5]</ref> Terminology Services REST API <ref type="foot" coords="8,134.24,242.93,3.71,7.97" target="#foot_3">4</ref> to map each concept into a high-level semantic type. Afterwards, we computed the frequency of each high-level semantic type on the Top-100 concepts, thus getting to the following types: Body Part, Organ, or Organ Component; Spatial Concept; Finding; Pathologic Function; Qualitative Concept; Diagnostic Procedure; Body Location or Region; Functional Concept and Miscellaneous Concepts (i.e., the remaining semantic types which have lower frequencies). For each of these types, we trained a ResNet18 <ref type="bibr" coords="8,306.28,312.43,18.07,10.91" target="#b9">[10]</ref> on the images and their multi-labels (if present). To predict the final set of concepts per image, we run each of the previous models for the entire data set and perform an aggregation operation (i.e., the union). Regarding the training process, these models were trained during 10 epochs, using the Adam optimiser with a learning rate of 10 -4 . These models also used the "2 Phases" strategy, where we froze the backbone layers for 5 epochs and then unfroze them for the remaining epochs. The best model is saved based on the lowest validation loss. Figure <ref type="figure" coords="8,318.38,393.73,5.07,10.91" target="#fig_2">3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Caption Prediction Task</head><p>The caption prediction task consists of a natural language generation problem, more specifically, an image captioning problem where a textual description of the images must be generated. To tackle this problem we developed two strategies, both based on the Transformer architecture <ref type="bibr" coords="9,486.99,134.63,16.22,10.91" target="#b10">[11]</ref>: a Vision Encoder-Decoder Transformer and a modified OSCAR Transformer <ref type="bibr" coords="9,422.55,148.18,11.43,10.91" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Vision Encoder-Decoder</head><p>The Vision Encoder-Decoder architecture combines the original Transformer <ref type="bibr" coords="9,446.04,196.59,18.06,10.91" target="#b10">[11]</ref> with the Vision Transformer (ViT) <ref type="bibr" coords="9,205.84,210.13,16.33,10.91" target="#b11">[12]</ref>, i.e. while it keeps the original Transformer's encoder-decoder structure, the encoder receives as input an image divided into patches of 16×16 pixels. The decoder receives the ground-truth caption as input (i.e., training is done using teacher forcing) and the encoder hidden states as inputs to the cross attention layers. The model is trained autoregressively for next token prediction using causal (or unidirectional) self-attention, which means that a given token can only attend to previous tokens. Figure <ref type="figure" coords="9,391.72,277.88,5.01,10.91" target="#fig_3">4</ref> depicts this architecture.</p><p>The model was implemented using the Vision Encoder Decoder class from the Hugging Face Transformers library and we chose a tiny Data-efficient image Transformer (DeiT) <ref type="bibr" coords="9,487.93,304.98,18.05,10.91" target="#b12">[13]</ref> pretrained on ImageNet for the encoder. For the decoder we leverage pretrained weights from the Distilled-GPT2, a distilled version of the GPT-2 architecture <ref type="bibr" coords="9,381.15,332.08,16.42,10.91" target="#b13">[14]</ref>. We trained this model initially for 20 epochs, and then for an additional 20 epochs starting from the checkpoint with the lowest validation loss. We used the AdamW <ref type="bibr" coords="9,305.32,359.18,17.94,10.91" target="#b14">[15]</ref> optimiser with an initial learning rate of 5 × 10 -5 , linearly decayed. Due to limitations of the computational resources available we were not able to fine tune the model using self-critical sequence training <ref type="bibr" coords="9,390.38,386.27,16.25,10.91" target="#b15">[16]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Modified OSCAR</head><p>We hypothesised that leveraging the information present in the concepts might aid in generating the captions. Therefore, we developed a modified OSCAR <ref type="bibr" coords="10,342.79,121.08,12.68,10.91" target="#b2">[3]</ref> architecture, as depicted in Figure <ref type="figure" coords="10,89.29,134.63,3.75,10.91" target="#fig_5">5</ref>. Since the original architecture uses object tags and region features obtained from an object detector (e.g. Faster R-CNN) and we do not have access to bounding box annotations for our data, we modified OSCAR to receive as input the image divided into 16×16 patches similarly to what is done in the ViT model. Furthermore, instead of object detection tags the model receives the ground-truth concepts for each image.</p><p>Contrary to the Vision Encoder-Decoder model, OSCAR is trained for masked language modelling, so the objective is to predict the masked input tokens. We adopted the same masking strategy as in the original OSCAR <ref type="bibr" coords="10,234.69,229.48,11.28,10.91" target="#b2">[3]</ref>. However, instead of using bidirectional attention like in the original Bidirectional Encoder Representations from Transformers (BERT) encoder model <ref type="bibr" coords="10,486.96,243.03,16.18,10.91" target="#b16">[17]</ref>, OSCAR (as well as our modified version) adopts causal self-attention, since our target goal is text generation. Therefore, when predicting the masked token, it can attend to every concept and image patch, but it can only attend to previous tokens from the caption.</p><p>During inference on the test set, we obtained the concepts from our best concept detection model, i.e. the "Ensemble (NaN)" model. We start by passing a MASK token as the textual input and the model then generates the rest of the sequence autoregressively.</p><p>We leveraged weights from an OSCAR model pretrained on the MSCOCO Captions data set <ref type="bibr" coords="10,106.30,351.42,16.42,10.91" target="#b17">[18]</ref>. The model was fine tuned on the competition data set for 20 epochs, with the AdamW <ref type="bibr" coords="10,128.05,364.97,17.81,10.91" target="#b14">[15]</ref> optimiser and a learning rate of 10 -4 , linearly decayed. Maximum caption length was defined as 50, while the maximum length for the sequence of concepts was 10. As before, we planned on fine tuning the model using self-critical sequence training <ref type="bibr" coords="10,411.66,392.07,16.09,10.91" target="#b15">[16]</ref>, but that was not possible due to limitations in computational resources.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Discussion</head><p>This section presents and discusses the results of the conducted experiments in the Concept Detection and Caption Prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Concept Detection Task</head><p>The evaluation of the concept detection task is conducted in terms of the example-based F1score between the predicted and ground truth concepts. Additionally, a variant of the F1-score (Secondary F1-score) was computed using only a subset of manually validated concepts related to anatomy and image modality.  In the multi-label classification model, a concept is associated with the image if its predicted score is greater than the defined decision threshold value (0.5). As evidenced by the results presented in Table <ref type="table" coords="11,175.19,563.73,5.16,10.91" target="#tab_5">5</ref> concerning the multi-label classification model, the best performance is obtained when the model is trained to predict only the Top-100 concepts, achieving an F1-score of 0.3740 when using the Multi-label ("Frozen Backbone") approach, and 0.3947 when using the Multi-label ("Whole Network") model. On the other hand, when the model is trained to predict the 8, 734 concepts, performance slightly decreases (0.3740 to 0.3710). We can also conclude that training the whole network improves the results in terms of F1-score compared with the results obtained when freezing the weights of the feature extraction layers of the DenseNet-121 (0.3740 to 0.3947).</p><p>Alternatively, when adopting the "2 Phases" strategy, we verify a marginal decrease of the F1-score on the validation set (0.3937) compared to the best result (0.3947) obtained with the "Whole Network" strategy. However, the F1-score on the test set is marginally higher (0.431) than the result obtained with the "Whole Network" strategy (0.430).</p><p>Regarding the retrieval task using contrastive learning, we verify that the model that uses Euclidean distance in the contrastive loss yields better results than the model that uses Cosine Similarity. We also observe that training the model to recognise only the Top-100 concepts leads to higher F1-score than when all the existing concepts are used, which is consistent with the results obtained with the multi-label approach. Despite the Euclidean Retrieval model trained with the Top-100 concepts achieving higher F1-scores than the multi-label classification model on the validation set, we verify that its results are considerably worse in the test set. Nevertheless, when we merge the results of the multi-label classification and the Euclidean retrieval models in the ensemble model referred to as Ensemble (NaN), the results in the test set improve slightly. As such, we were able to improve the results of the multi-label classification model by using retrieval to detect concepts for images where the multi-label classification network failed to detect any concepts. Interestingly, the Ensemble with the OR operation is not able to surpass the previous Ensemble.</p><p>Regarding the semantic-based multi-label classification, we highlight the proximity of the scores obtained by this approach to the previous ones. However, it is important to mention that the results do not confirm our initial intuition that using prior knowledge (i.e., in this case, optimising different models for a specific semantic type of concepts) would improve the predictive performance.</p><p>Although our best submission ranked 5th (see Table <ref type="table" coords="12,349.23,385.05,4.25,10.91" target="#tab_5">5</ref>) in terms of F1-score among all submissions from 11 teams, the difference between the winner's F1-score and ours is relatively small (0.018). On the other hand, we note the results obtained on the Secondary F1-score, which is computed with a specific subset of manually validated concepts (anatomy and image modality). In this metric, our team achieved the best score (0.863) of the whole competition and by a considerable margin compared to the result of the task winners (0.791).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Caption Prediction Task</head><p>The caption prediction task is evaluated in terms of natural language generation metrics. The BLEU score was chosen as the primary competition metric, but ROUGE, METEOR, CIDEr, SPICE and BERTScore are also computed. Table <ref type="table" coords="12,271.07,529.62,4.97,10.91" target="#tab_6">6</ref> presents the results obtained in the competition test set.</p><p>Our best performing model was, surprisingly, the vanilla Vision Encoder-Decoder trained for 40 epochs, achieving a BLEU score of 0.306, and placing in 4th place out of 10 participating teams, while the modified OSCAR only achieved 0.230. This might be due to the fact that the modified OSCAR was trained on ground-truth concepts, but during inference the model used the predicted concepts from our best concept detection model. Perhaps it would have been beneficial to introduce some uncertainty during training by alternating between feeding ground-truth concepts and feeding predicted concepts from that same concept detection model, in an effort to teach the Transformer the bias introduced by the concept detection model.</p><p>Furthermore, had the time permitted it, the Vision Encoder-Decoder could have been trained</p><p>for more epochs, since the training loss was still decreasing, which is corroborated by the fact that training an additional 20 epochs improved the BLEU score from 0.300 to 0.306. Finally, it is interesting to note that although in terms of BLEU score there is a considerable difference between our best performing model (0.306) and the task winners (0.483), our model largely outperforms the winner in terms of CIDEr (by 0.175), a natural language generation metric that tries to better correlate with human judgement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>This paper described the work developed by the VCMI team in the ImageCLEFmedical 2022 Caption task. Regarding the concept detection task, three different strategies were adopted: (i) a multi-label classification model, (ii) a retrieval-based approach, and (iii) a semantic-based multi-label classification model. The experimental results indicated that merging the multi-label classification model with the retrieval approach (ensemble model) was the best strategy for the concept detection task, achieving the highest F1-score (0.433) on the test set among all our submissions, and ranking 5th among all the 11 participating teams. In terms of Secondary F1-score, we achieved the best value (0.863) among all the participating teams. Concerning the caption prediction task, we explored two strategies based on Transformer architectures, a Vision Encoder-Decoder Transformer and a modified OSCAR. Our best submission, the Vision Encoder-Decoder, obtained a BLEU score of 0.306, thus achieving the 4th place among all the 10 participating teams. Future work should be devoted to improving the developed methods for both the concept detection and caption prediction tasks. Regarding the first task, we believe we could improve our results by building the ensemble model with the "2 Phases" multi-label classification model. In terms of improving the generation of captions, we would start by training the Vision Encoder-Decoder for more epochs, as the training loss was still decreasing. Furthermore, we also believe that improving the concept detection phase would boost the performance of the modified OSCAR approach and that including non ground-truth concepts during training would teach the Transformer the bias introduced by the concept detection model and, consequently, be more adapted to the inference scenario. Finally, we would also like to perform an ablation study and compare both Transformer approaches more directly, by training the modified OSCAR without the concepts.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Exploratory data analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Caption prediction</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,89.29,298.27,416.69,8.93;6,89.29,310.28,416.69,8.87;6,89.29,321.96,416.69,9.14;6,89.29,334.19,299.03,8.87"><head>Figure 1 Figure 1 :</head><label>11</label><figDesc>Figure 1: Diagram of the multi-label classification model. The input image is fed to the model, a DenseNet-121 [6], and the final output layer where the sigmoid activation function is employed predicts 𝑁 values corresponding to the 𝑁 concepts. A concept is assigned to the image if its predicted score is greater than a predefined decision threshold. Example image: CC BY [7].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,89.29,223.42,416.70,8.93;7,89.29,235.42,416.69,8.87;7,89.29,247.38,416.70,8.87;7,89.29,259.33,152.93,8.87"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Diagram of the concept retrieval model trained using contrastive learning. The images and concepts are encoded into a common latent space. In the represented latent space, blue circles correspond to latent representations of images, while yellow diamonds correspond to latent representations of concepts. Example image: CC BY [7].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,89.29,592.77,416.70,8.93;8,89.29,604.77,416.70,8.87;8,89.29,616.73,416.69,8.87;8,89.29,628.68,416.70,8.87;8,89.29,640.64,74.43,8.87"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Diagram of the semantic-based multi-label classification model. Each model is trained on a set of concepts of the same semantic type. During inference, the input image is given to all the models and an aggregation operation (i.e., union) is performed. An example of the aggregation of the concepts detected by different modules of the network is shown by the different coloured concepts. Example image: CC BY [7].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,89.29,628.97,416.69,8.93;9,89.29,640.98,416.69,8.87;9,89.29,652.93,402.16,8.87;9,164.25,568.70,60.98,50.78"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Diagram of the Vision Encoder-Decoder captioning model. The encoder receives the input image divided into patches of 16×16 pixels, while the decoder receives the ground-truth caption and the encoder hidden states, and predicts the next word in the sentence. Example image: CC BY [7].</figDesc><graphic coords="9,164.25,568.70,60.98,50.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,280.68,468.80,36.03,8.98;10,94.51,517.41,184.16,5.84;10,294.57,516.92,117.07,5.84;10,121.98,430.35,70.52,7.41"><head>Encoder[</head><label></label><figDesc>CLS] computed [MASK] ct show cerebellar tonsil herniation [SEP] Cerebellum Computed Tomography [SEP]Masked Token Loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="10,89.29,576.99,416.69,8.93;10,89.29,589.00,417.79,8.87;10,89.29,600.95,418.22,8.87;10,89.29,612.91,111.88,8.87;10,420.81,512.48,79.80,56.10"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Diagram of the modified OSCAR captioning model, trained for masked language modelling using causal self-attention layers. Instead of receiving object regions and tags from an object detector, this modified version takes the image concepts and 16×16 patches as input alongside the caption. Example image: CC BY [7].</figDesc><graphic coords="10,420.81,512.48,79.80,56.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="16,162.88,405.59,269.52,9.77"><head>A. 1 .Figure 6 :UFigure 7 :</head><label>167</label><figDesc>Figure 6: Top-31 most frequent concepts in the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="17,169.21,617.95,256.85,9.77"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Distribution of the concepts in the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="18,164.67,321.07,265.93,9.77;18,130.96,368.26,333.37,250.03"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Distribution of the concepts in the validation set.</figDesc><graphic coords="18,130.96,368.26,333.37,250.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="18,134.81,630.52,325.66,9.77"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Distribution of the lengths of the captions in the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="19,130.27,346.45,334.73,9.77;19,130.96,84.19,333.37,250.03"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Distribution of the lengths of the captions in the validation set.</figDesc><graphic coords="19,130.96,84.19,333.37,250.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,88.15,545.79,418.93,106.36"><head>Table 2</head><label>2</label><figDesc>Exploratory</figDesc><table coords="3,130.96,613.36,327.29,38.79"><row><cell>Subset</cell><cell cols="5">Total Avg. Min. Max. In 10 or less images In 0 images</cell></row><row><cell>Training</cell><cell>8,374 47.2</cell><cell>2</cell><cell>25,989</cell><cell>4,923</cell><cell>0</cell></row><row><cell cols="2">Validation 4,357 4.3</cell><cell>0</cell><cell>2,896</cell><cell>7,842</cell><cell>4,017</cell></row></table><note coords="3,140.99,557.80,366.08,8.87;3,88.15,569.75,418.93,8.87;3,89.29,581.71,416.70,8.87;3,89.29,593.66,297.16,8.87"><p>data analysis for the concept detection task, from the concept-based perspective. Note: "Avg. ", "Min. " and "Max. " stand for "Average", "Minimum" and "Maximum" number of images per concept, respectively. The last two columns refer to the number of concepts that appear in 10 or less images and to the number of concepts that do not appear in any image, respectively.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,88.99,558.23,417.00,81.83"><head>Table 3</head><label>3</label><figDesc>Exploratory data analysis for the caption prediction task. We present the average, minimum and maximum number of tokens for the captions on the training and validation subsets.</figDesc><table coords="4,200.99,601.80,193.29,38.25"><row><cell>Subset</cell><cell cols="3">Average Minimum Maximum</cell></row><row><cell>Training</cell><cell>29.73</cell><cell>3</cell><cell>577</cell></row><row><cell>Validation</cell><cell>32.37</cell><cell>3</cell><cell>339</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,88.99,503.41,416.99,93.78"><head>Table 4</head><label>4</label><figDesc>Top-3 most frequent concepts (CUIs) in the training set alongside their correspondent UMLS term, and their respective frequency in the training and validation sets.</figDesc><table coords="5,201.69,546.98,189.41,50.21"><row><cell>CUI</cell><cell cols="3">UMLS Term Train Validation</cell></row><row><cell>C0040405</cell><cell>X-Ray CT</cell><cell>25989</cell><cell>2896</cell></row><row><cell cols="3">C1306645 Plain X-Ray 24389</cell><cell>2023</cell></row><row><cell>C0024485</cell><cell>MRI</cell><cell>14622</cell><cell>1071</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,88.89,215.20,417.10,51.56"><head></head><label></label><figDesc>Table 5 presents the results obtained in the experiments relative to this task. In this table, the first two columns refer to the model used and to the concepts that were considered during the respective model's training. The table also includes the results in terms of F1-score in the validation and test sets and Secondary F1-score in the test set.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="11,88.99,284.80,416.99,233.02"><head>Table 5</head><label>5</label><figDesc>Results of the concept detection task in terms of F1-score and Secondary F1-score computed on a subset of manually validated concepts. "Top-100" and "All" refer to the (sub)set of concepts used to train the models.</figDesc><table coords="11,111.59,338.10,372.10,179.71"><row><cell>Model</cell><cell>Concepts</cell><cell>F1-score (Validation)</cell><cell cols="2">F1-score Secondary F1-score (Test) (Test)</cell></row><row><cell cols="2">Multi-label (Frozen Backbone) All</cell><cell>0.3710</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Multi-label (Frozen Backbone) Top-100</cell><cell>0.3740</cell><cell>-</cell><cell></cell></row><row><cell>Multi-label (Whole Network)</cell><cell>Top-100</cell><cell>0.3947</cell><cell>0.430</cell><cell>0.861</cell></row><row><cell>Multi-label (2 Phases)</cell><cell>Top-100</cell><cell>0.3937</cell><cell>0.431</cell><cell>0.856</cell></row><row><cell>Euclidean Retrieval</cell><cell>All</cell><cell>0.3367</cell><cell>-</cell><cell>-</cell></row><row><cell>Euclidean Retrieval</cell><cell>Top-100</cell><cell>0.3973</cell><cell>0.368</cell><cell>0.778</cell></row><row><cell>Cosine Similarity Retrieval</cell><cell>Top-100</cell><cell>0.3184</cell><cell>-</cell><cell>-</cell></row><row><cell>Ensemble (NaN)</cell><cell>Top-100</cell><cell>0.3959</cell><cell>0.433</cell><cell>0.863</cell></row><row><cell>Ensemble (OR)</cell><cell>Top-100</cell><cell>0.3956</cell><cell>-</cell><cell>-</cell></row><row><cell>Semantic</cell><cell>Top-100</cell><cell>-</cell><cell>0.418</cell><cell>0.838</cell></row><row><cell>Task Winners</cell><cell>-</cell><cell>-</cell><cell>0.451</cell><cell>0.791</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="13,88.99,183.72,418.09,108.99"><head>Table 6</head><label>6</label><figDesc>Results of the caption prediction task on the test set in terms of BLEU, ROUGE, METEOR, CIDEr, SPICE, and BERTScore.</figDesc><table coords="13,95.27,225.07,410.73,67.64"><row><cell>Model</cell><cell cols="6">BLEU ROUGE METEOR CIDEr SPICE BERTScore</cell></row><row><cell cols="2">Vision Encoder-Decoder (20 epochs) 0.300</cell><cell>0.172</cell><cell>0.073</cell><cell cols="2">0.210 0.039</cell><cell>0.604</cell></row><row><cell cols="2">Vision Encoder-Decoder (40 epochs) 0.306</cell><cell>0.174</cell><cell>0.075</cell><cell>0.205</cell><cell>0.036</cell><cell>0.604</cell></row><row><cell>Modified OSCAR</cell><cell>0.230</cell><cell>0.111</cell><cell>0.047</cell><cell>0.088</cell><cell>0.023</cell><cell>0.551</cell></row><row><cell>Task Winners</cell><cell>0.483</cell><cell>0.142</cell><cell>0.0928</cell><cell>0.030</cell><cell>0.007</cell><cell>0.561</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,660.03,154.70,8.97"><p>http://www.clef-initiative.eu (accessed on:</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1" coords="2,266.23,660.03,42.09,8.97;2,105.65,669.42,2.78,5.98;2,108.93,670.98,156.43,8.97"><p>26-05-2022) 2 https://github.com/icrto/ImageClefMedical</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,108.93,660.07,397.64,8.97;4,89.29,671.03,169.55,8.97"><p>We used the distil-gpt2 tokenizer of the Hugging Face Transformers library (https://huggingface.co/docs/ transformers/index -accessed on: 27-05-2022).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="8,108.93,671.03,280.70,8.97"><p>https://www.nlm.nih.gov/research/umls/index.html (accessed on 26-05-2022)</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work, developed within the scope of the project "<rs type="projectName">TAMI -Transparent Artificial Medical Intelligence"</rs> (<rs type="grantNumber">NORTE-01-0247-FEDER-045905</rs>), is co-financed by <rs type="funder">ERDF -European Regional Fund</rs> through the <rs type="programName">Operational Program for Competitiveness and Internationalisation -COMPETE2020</rs>, the <rs type="programName">North Portugal Regional Operational Program</rs> -<rs type="grantNumber">NORTE 2020</rs> and by the <rs type="funder">Portuguese Foundation for Science and Technology -FCT</rs> under the <rs type="funder">CMU -Portugal International Partnership</rs> and the Ph.D. grants "<rs type="grantNumber">2020.06434.BD</rs>" and "<rs type="grantNumber">2020.07034.BD</rs>".</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_PKprDSU">
					<idno type="grant-number">NORTE-01-0247-FEDER-045905</idno>
					<orgName type="project" subtype="full">TAMI -Transparent Artificial Medical Intelligence&quot;</orgName>
					<orgName type="program" subtype="full">Operational Program for Competitiveness and Internationalisation -COMPETE2020</orgName>
				</org>
				<org type="funding" xml:id="_3XqYjH4">
					<idno type="grant-number">NORTE 2020</idno>
					<orgName type="program" subtype="full">North Portugal Regional Operational Program</orgName>
				</org>
				<org type="funding" xml:id="_ACjF2JF">
					<idno type="grant-number">2020.06434.BD</idno>
				</org>
				<org type="funding" xml:id="_zRknPGt">
					<idno type="grant-number">2020.07034.BD</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="14,112.66,237.65,395.01,10.91;14,112.66,251.20,395.17,10.91;14,112.39,264.75,394.80,10.91;14,112.66,278.30,394.62,10.91;14,112.66,291.85,393.33,10.91;14,112.66,305.40,395.17,10.91;14,112.66,318.95,393.54,10.91;14,112.66,332.50,170.14,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,352.50,278.30,154.78,10.91;14,112.66,291.85,300.10,10.91">Overview of the ImageCLEF 2022: Multimedia retrieval in medical, social media and nature applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,435.26,291.85,70.73,10.91;14,112.66,305.40,395.17,10.91;14,112.66,318.95,239.58,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<title level="s" coord="14,359.20,318.95,147.00,10.91;14,112.66,332.50,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,346.05,395.17,10.91;14,111.81,359.59,395.37,10.91;14,112.66,373.14,393.33,10.91;14,112.66,386.69,216.46,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,326.22,359.59,180.96,10.91;14,112.66,373.14,177.87,10.91">Overview of ImageCLEFmedical 2022caption prediction and concept detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,313.28,373.14,192.71,10.91;14,112.66,386.69,97.38,10.91">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,400.24,394.61,10.91;14,112.66,413.79,393.33,10.91;14,112.66,427.34,312.10,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,479.64,400.24,27.64,10.91;14,112.66,413.79,288.65,10.91">Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,424.31,413.79,81.68,10.91;14,112.66,427.34,223.63,10.91">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,440.89,393.33,10.91;14,112.33,454.44,393.65,10.91;14,112.66,467.99,394.53,10.91;14,112.66,481.54,55.16,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,370.24,440.89,135.74,10.91;14,112.33,454.44,166.38,10.91">Radiology Objects in COntext (ROCO): A Multimodal Image Dataset</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,301.70,454.44,204.28,10.91;14,112.66,467.99,365.26,10.91">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,495.09,393.32,10.91;14,112.66,508.64,264.05,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,186.64,495.09,319.34,10.91;14,112.66,508.64,52.45,10.91">The Unified Medical Language System (UMLS): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,173.80,508.64,103.68,10.91">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="D270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,522.18,393.33,10.91;14,112.66,535.73,393.33,10.91;14,112.66,549.28,185.09,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,356.26,522.18,149.73,10.91;14,112.66,535.73,41.53,10.91">Densely Connected Convolutional Networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,183.61,535.73,322.38,10.91;14,112.66,549.28,86.61,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,562.83,393.33,10.91;14,112.66,576.38,393.33,10.91;14,112.66,589.93,267.11,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,348.47,562.83,157.52,10.91;14,112.66,576.38,308.95,10.91">Dialysis Disequilibrium Syndrome and Cerebellar Herniation with Successful Reversal Using Mannitol</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<idno type="DOI">10.1155/2020/8850850</idno>
	</analytic>
	<monogr>
		<title level="j" coord="14,433.41,576.38,72.57,10.91;14,112.66,589.93,52.95,10.91">Case Reports in Nephrology</title>
		<imprint>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,603.48,393.33,10.91;14,112.33,617.03,394.86,10.91;14,112.66,630.58,275.84,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,233.70,603.48,162.64,10.91">A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,142.34,617.03,309.71,10.91">3rd International Conference on Learning Representations, ICLR 2015</title>
		<title level="s" coord="14,224.49,630.58,133.88,10.91">Conference Track Proceedings</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,644.13,393.33,10.91;15,112.66,86.97,393.33,10.91;15,112.66,100.52,249.94,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="14,267.77,644.13,238.21,10.91;15,112.66,86.97,36.51,10.91">Dimensionality Reduction by Learning an Invariant Mapping</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,172.78,86.97,333.21,10.91;15,112.66,100.52,105.89,10.91">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,114.06,394.62,10.91;15,112.66,127.61,394.53,10.91;15,112.66,141.16,80.57,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="15,267.98,114.06,214.89,10.91">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,112.66,127.61,389.59,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,154.71,395.17,10.91;15,112.66,168.26,393.33,10.91;15,112.66,181.81,42.06,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="15,484.04,154.71,23.79,10.91;15,112.66,168.26,149.47,10.91">Attention Is All You Need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,271.16,168.26,234.82,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Polosukhin</note>
</biblStruct>

<biblStruct coords="15,112.66,195.36,395.16,10.91;15,112.66,208.91,393.33,10.91;15,112.14,222.46,393.84,10.91;15,112.66,236.01,241.36,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="15,398.46,208.91,107.53,10.91;15,112.14,222.46,229.21,10.91">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,363.95,222.46,142.03,10.91;15,112.66,236.01,211.47,10.91">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,249.56,393.33,10.91;15,112.66,263.11,393.32,10.91;15,112.66,276.66,289.37,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,408.51,249.56,97.48,10.91;15,112.66,263.11,227.35,10.91">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,362.79,263.11,143.19,10.91;15,112.66,276.66,180.91,10.91">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,290.20,393.33,10.91;15,112.66,303.75,259.68,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="15,407.45,290.20,98.53,10.91;15,112.66,303.75,145.68,10.91">Language Models are Unsupervised Multitask Learners</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,266.84,303.75,57.98,10.91">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,317.30,393.33,10.91;15,112.66,330.85,394.53,10.91;15,112.66,344.40,125.94,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="15,225.12,317.30,177.47,10.91">Decoupled weight decay regularization</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,428.80,317.30,77.19,10.91;15,112.66,330.85,236.63,10.91">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">May 6-9, 2019. 2019</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct coords="15,112.66,357.95,393.33,10.91;15,112.66,371.50,393.33,10.91;15,112.66,385.05,220.45,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="15,365.72,357.95,140.27,10.91;15,112.66,371.50,94.02,10.91">Self-critical Sequence Training for Image Captioning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,230.17,371.50,275.82,10.91;15,112.66,385.05,121.97,10.91">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7008" to="7024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,398.60,393.33,10.91;15,112.33,412.15,393.65,10.91;15,112.28,425.70,393.71,10.91;15,112.33,439.25,311.76,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="15,321.08,398.60,184.91,10.91;15,112.33,412.15,183.29,10.91">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,317.85,412.15,188.13,10.91;15,112.28,425.70,393.71,10.91;15,112.33,439.25,56.35,10.91">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="15,112.66,452.79,393.33,10.91;15,112.66,466.34,394.91,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m" coord="15,432.87,452.79,73.11,10.91;15,112.66,466.34,213.43,10.91">Microsoft COCO Captions: Data Collection and Evaluation Server</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
