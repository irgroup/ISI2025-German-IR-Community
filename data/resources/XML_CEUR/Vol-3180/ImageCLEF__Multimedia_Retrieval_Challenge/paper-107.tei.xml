<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,327.38,15.42;1,89.29,106.66,405.61,15.42">A Fusion Approach for Web Search Result Diversification Using Machine Learning Algorithms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,97.94,11.96"><forename type="first">Lekshmi</forename><surname>Kalinathan</surname></persName>
							<email>lekshmik@ssn.edu.</email>
						</author>
						<author>
							<persName coords="1,199.87,134.97,122.51,11.96"><forename type="first">Prabavathy</forename><surname>Balsundaram</surname></persName>
							<email>prabavathyb@ssn.edu.in</email>
						</author>
						<author>
							<persName coords="1,335.02,134.97,75.75,11.96"><forename type="first">Yogesh</forename><surname>Munees</surname></persName>
						</author>
						<author>
							<persName coords="1,89.29,148.92,54.08,11.96"><forename type="first">Shrijith</forename><surname>Mr</surname></persName>
						</author>
						<author>
							<persName coords="1,156.02,148.92,150.68,11.96"><forename type="first">Shrikeshavinee</forename><surname>Ramachandran</surname></persName>
							<email>shrikeshavinee2010573@ssn.edu.</email>
						</author>
						<author>
							<persName coords="1,319.35,148.92,65.87,11.96"><forename type="first">Shruti</forename><surname>Sriram</surname></persName>
						</author>
						<author>
							<persName coords="1,89.29,162.87,149.59,11.96"><forename type="first">Ramdhanush</forename><surname>Venkatakrishnan</surname></persName>
							<email>ramdhanush2010105@ssn.edu</email>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,327.38,15.42;1,89.29,106.66,405.61,15.42">A Fusion Approach for Web Search Result Diversification Using Machine Learning Algorithms</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">FAD5CED65D480AF2B407B232A0860E0C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Voting Regressor</term>
					<term>Ensembling approach</term>
					<term>CART</term>
					<term>Fusion methods</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Result diversification provides a broader view of a search topic while also increasing the possibilities of finding relevant information. It has shown to increase user satisfaction in recommender systems and web search. Many different approaches have been proposed in the related literature for the diversification problem. Since the web search result is huge in size, it is essential to have an efficient fusion approach. Hence, objective of this paper is to propose the implementation of fusion model based on KNN, CART and SVR regressors. This fusion model aims to improve the accuracy and reduce the error value of the result that is generated.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Currently, the clients of search engines are interested in retrieving pieces of information that cover many aspects of their information needs. Consequently, result diversification has attracted considerable attention in order to improve user satisfaction. For example, if a user wants to search for car of a type, a diverse result containing various brands, models with different horsepower and other technical characteristics is intuitively more informative than a result that contains a homogeneous result containing only the cars with similar features.</p><p>Diversification is also helpful to offset the influence of personalization. Personalization aims at tailoring results to meet the preferences of each specific individual. However, this can leads to excessive restriction of search results. Diversification can complement preferences and provide personalization systems with the means to retrieve more satisfying results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Jiang et al <ref type="bibr" coords="2,136.59,111.28,12.90,10.91" target="#b0">[1]</ref> have proposed a learning framework for explicit result diversification in which subtopics are explicitly modeled. Recurrent Neural Networks, attention mechanism and Max Pooling are used to instantiate the framework. The framework is flexible to model query intent in a flat list or a hierarchy. Regardless of diversity modeling and optimization methods, all these solutions inherit the spirit of MMR, which is an implicit approach and disregards intent.</p><p>Göynük et al <ref type="bibr" coords="2,161.59,192.57,12.99,10.91" target="#b1">[2]</ref> have proposed R-LTR, a supervised learning approach for textual data and modified to allow tuning the weights of visual and textual features separately. The experiments using a benchmark dataset v with 153 queries and 45K images reveal that the proposed method significantly outperforms various ad hoc diversification approaches in terms of the sub-topic recall metric. Furthermore, certain variants of R-LTR are superior to the original method and provide additional (relative) gains of up to 2.2%.</p><p>Lu et al <ref type="bibr" coords="2,134.30,287.42,13.00,10.91" target="#b2">[3]</ref> proposed a multidimensional evaluation mechanism for analysing the results of diversification in image retrieval. Experiments were carried out to evaluate three different semantic distance algorithms (WordNet, Google Distance, and ESA) combined with three re-ranking algorithms (MMR, xQuAD, and Score Difference) on image diversification retrieval based on a subset of the NUS-WIDE image dataset. It allowed a comparison of these algorithms on social tags and visual tags to understand their strengths and weaknesses, and a comparison of visual distance algorithms to prove the effectiveness of semantic information in result diversification. MacAvaney et al <ref type="bibr" coords="2,177.09,409.36,13.00,10.91" target="#b3">[4]</ref> proposed a paper which introduces a new distributional causal language modelling objective and a representation replacement strategy to better handle ambiguous queries.It is found that IntenT5 excels at handling faceted queries and improves ColBERT's performance for fully specified queries. Given that ambiguous queries appear to be difficult to handle, it investigates Distributional Causal Language Modeling for overcoming this problem.</p><p>Maxwell et al <ref type="bibr" coords="2,155.95,504.21,12.99,10.91" target="#b4">[5]</ref> proposed that, the impact of diversification is explored where searchers undertake complex search tasks using Interactive Information Retrieval (IIR). The goal of the system is to help the searcher learn about a topic and the number of aspects that the searcher finds indicates how much they learned during the process. This finding suggested that the participants seemed largely ambivalent to the difference in the performance of the systems.</p><p>Omer Sagi et al <ref type="bibr" coords="2,173.88,585.50,13.00,10.91" target="#b5">[6]</ref> have studied various ensembling techniques like AdaBoost, Bagging, Random Forest, Gradient Boosting Machines, Rotation Forest, Extremely Randomized Trees and Deep Neural Networks. They suggested that these existing ensemble methods certainly improve predictive performance by avoiding overfitting, decreasing the risk of obtaining local minima and expanding the search space. The research direction in this survey includes the refinement of popular algorithms suitable for big data and distribution of ensembling algorithms across multiple machines. DONG et al <ref type="bibr" coords="3,150.45,114.06,12.24,10.91" target="#b6">[7]</ref> grouped ensemble learning into four categories: classification based supervised and semi-supervised ensemble methods, clustering based supervised and semi-supervised ensemble methods. It presented challenges and possible research directions for each mainstream approach of ensemble learning. Further, it also suggests that the performance of ensemble method can be improvised by fusing it with the deep learning and reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task and Dataset Description</head><p>The dataset used for this task is from ImageCLEF 2022 <ref type="bibr" coords="3,329.49,226.89,11.28,10.91" target="#b7">[8]</ref>. Result diversification fusion task <ref type="bibr" coords="3,493.29,226.89,12.70,10.91" target="#b8">[9]</ref> focuses on identifying relevant outputs and optimizing them, given a query. An inducer is a model which predicts images related to a query. The similarity scores and ranks are calculated and written into a text file during run time. However, a single inducer cannot be worked with, as it might have low precision or performance. Hence an extended version containing many inducers are used to make the predictions more precise. This is called ensembling technique. The performance given by the ensembling technique even tops the performance of the highest single inducer. The data for this task is obtained from the Retrieving Diverse Social Images Task dataset [Ionescu2020]. The outputs of 56 inducers, each stored in a separate text file, representing a total of 123 queries (topics). Each entry or row in these files is of the format as given below in the Table <ref type="table" coords="3,133.04,375.93,3.74,10.91" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodologies used</head><p>In order to understand the evaluation of the similarity scores by the inducers, three predictors namely, KNN Regressor, Classification and Regression Tree and Support Vector Regressor were explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">KNN Regressor</head><p>KNN (K -Nearest Neighbors) is a supervised machine learning model which classifies a data point based on feature similarity. It is called a lazy learner as not much training is required. KNN is applied to both classification and regression problems. It gives accurate results when the dataset is small and when it is properly labeled. First, the Euclidean distances of the new data point are calculated from all the training points and K of its closest neighbors are considered. Then, out of the K neighbors, the number of points in each class are counted. The new data point is classified into the class in which a majority of its neighbors lie. KNN regressor uses the same distance functions as KNN classification. After having the distances calculated, the samples of k smaller distances were found and the target values are averaged to obtain the predicted value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Classification and Regression Trees</head><p>While classification attempts to predict a class label, regression predicts a probabilistic value corresponding to a class label. The CART algorithm <ref type="bibr" coords="4,336.70,292.75,18.07,10.91" target="#b9">[10]</ref> works by searching for the best homogeneity for the subnodes in the decision tree with the help of the Gini Index criterion. The root node is taken as the training set and is split into two by considering the best attribute and threshold value. Further, the subsets are also split using the same logic. This continues until the last pure subset is found or the maximum number of leaves possible in that growing tree. Each leaf node represents a class label where decisions are taken after computing all features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Support Vector Machine Regressor</head><p>Support Vector Regression is a regression algorithm <ref type="bibr" coords="4,342.41,423.78,18.06,10.91" target="#b10">[11]</ref> that supports both linear and non-linear regressions. This method works on the principle of the Support Vector Machine, i.e., it finds a hyperplane in an N-dimensional space that distinctly classifies the data points. SVR differs from SVM in the way that SVM is a classifier that is used for predicting discrete categorical labels while SVR is a regressor that is used for predicting continuous ordered variables. In simple regression, the idea is to minimize the error rate while in SVR the idea is to fit the error inside a certain threshold which means, work of SVR is to approximate the best value within a given margin called -tube. The objective of SVR is to fit as many data points as possible without violating the margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation</head><p>The dataset is split into 90%-10% where the 90% is used for training and the remaining 10% is used for validation data. The similarity score is normalized and extracted to ensure the same data distribution. The training data is used as the input while the output is reflected on the validation data. Three predictor models 𝑃 1 , 𝑃 2 and 𝑃 3 were built to study how similarity scores are assigned.</p><p>The predictor 𝑃 1 was implemented using KNeighborsRegressor function of sklearn<ref type="foot" coords="5,455.48,85.21,3.71,7.97" target="#foot_0">1</ref> .neighbors library. Regressor parameters n_neighbors, weights, metric as 5, uniform and l2_distance respectively are initialized. The regressor is fitted on the training dataset X and y is obtained as their responses. The K-neighbors of a data point are found. Further, the weighted graph for k neighbors is calculated for the data points in X. With the model obtained, target is predicted for the given test data. The predictor 𝑃 2 was implemented using DecisionTreeRegressor function of sklearn.tree library. Regressor parameters criterion, splitter, min_samples_split and min_samples_leaf as squared_error, best, 2 and 1 respectively are initialized. These parameter values lead to fully grown and unpruned trees for this dataset. In addition, it also helps in the reduction of memory consumption. Decision tree regressor observes similarity scores of each inducer and trains a model to predict the data in future. The predictor 𝑃 3 was implemented using sklearn.svm with the following parameter values. SVR finds the curve for the given input similarity scores.</p><p>Parameter:Value Kernel:rbf, degree:3, gamma:scale, Coef:0.0, epsilon:0.1, shrinking:True, Cache_size:200, max_iter:-1 However, instead of using the curve as a decision boundary, it uses the curve to find the match between the support vector and position of the curve. SVR acknowledges the presence of non-linearity in the input similarity score and provides a proficient prediction model. The predictors are trained with 90% of the training dataset and the remaining set is validated. The data is split into batches of 5, and each batch is used to predict the similarity score of the next input for each predictor. Then a comparison between the actual and predicted values is done and the deviation between the two models is calculated as error. Ranks have been assigned for the models 𝑃 1 , 𝑃 2 and 𝑃 3 based on the error values. The ranks of the base regressors and the predictor models are used to construct the voting regressor. It is trained using the entire training data set. The output of each batch predicts the output for the next input. Error values are calculated by comparing the actual and the predicted values of the model. It is then tested on the test data and the similarity score predicted by the model replaces the original similarity score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results and Analysis</head><p>Voting regressor is used to predict the similarity scores of the models present in the validation dataset. The predicted and actual values of similarity scores were plotted in figures 1, 2, 3 and 4. From the figures 1, 2 and 3, it can be inferred that 𝑃 2 predicts better similarity scores as compared to 𝑃 1 and 𝑃 3 . Furthermore on analyzing figures 2 and 4, it can be seen that the predictor 𝑃 2 predicts better values as compared to the voting regressor. Table <ref type="table" coords="5,429.49,598.29,4.97,10.91" target="#tab_1">2</ref> shows the MAE and rank values for the base and voting regressor models.   Given the superior performance of the CART model, we chose it for creating our submission runs. It has been tested with CLEF test data using metrics F1 measure and cluster recall. Different clusters from many cluster labels are assessed under the cluster recall metric and the harmonic mean of cluster recall and precision equates to the F1 measure.</p><p>The CART model is used to predict the updated similarity score values for the test data. The test data contains 175,591 predicted values which are divided into 56 text files with around 3150 entries each. Ten different CART models were built by varying the paramater, iteration size.   Each variation of the CART model was tested with a given test data and the results obtained were submitted. For each submission run, we have obtained the F1 and CR scores and it has been tabulated in the Table <ref type="table" coords="7,215.42,563.16,3.81,10.91" target="#tab_2">3</ref>. A cluster recall of 0.4414 and a F1 measure of 0.5634 has been obtained for the 10 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>To improve the accuracy of the results of the inducers, three regressors were implemented in the voting regressor. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,89.29,205.83,297.81,8.93;6,90.10,84.19,415.08,109.08"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Actual versus Predicted similarity scores using KNN Regressor</figDesc><graphic coords="6,90.10,84.19,415.08,109.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,89.29,366.29,301.61,8.93;6,90.10,244.64,415.08,109.08"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Actual versus Predicted similarity scores using CART Regressor</figDesc><graphic coords="6,90.10,244.64,415.08,109.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,89.29,526.74,294.54,8.93;6,90.10,405.10,415.08,109.08"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Actual versus Predicted similarity scores using SVR Regressor</figDesc><graphic coords="6,90.10,405.10,415.08,109.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,89.29,205.83,304.03,8.93;7,90.10,84.19,415.08,109.08"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Actual versus Predicted similarity scores using Voting Regressor</figDesc><graphic coords="7,90.10,84.19,415.08,109.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.98,402.49,282.05,106.86"><head>Table 1</head><label>1</label><figDesc>Attributes of inducer file</figDesc><table coords="3,224.24,428.31,146.79,81.05"><row><cell>Fields</cell><cell>Representation</cell></row><row><cell>query_id</cell><cell>Unique id of the query</cell></row><row><cell>inter</cell><cell>Ignored value</cell></row><row><cell>photo_id</cell><cell>Unique photo id</cell></row><row><cell>rank</cell><cell>Photo rank</cell></row><row><cell>sim</cell><cell>Similarity score</cell></row><row><cell cols="2">run_name Name of inducer</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,88.99,250.94,295.79,85.18"><head>Table 2</head><label>2</label><figDesc>Performance of base and voting regressors</figDesc><table coords="7,210.49,278.98,174.29,57.14"><row><cell>Models</cell><cell>MAE Rank</cell></row><row><cell cols="2">P1 (K-Nearest Neighbors) 0.004 2</cell></row><row><cell>P2 (CART)</cell><cell>0.003 1</cell></row><row><cell>P3 (SVR)</cell><cell>0.085 3</cell></row><row><cell>Voting Regressor</cell><cell>0.017 -</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,88.99,357.02,277.36,154.68"><head>Table 3</head><label>3</label><figDesc>F1 and Cluster Recall scores for 10 runs of CART model</figDesc><table coords="7,228.92,382.84,137.43,128.87"><row><cell cols="3">Run No. F1 score CR score</cell></row><row><cell>1</cell><cell>0.5029</cell><cell>0.3925</cell></row><row><cell>2</cell><cell>0.5223</cell><cell>0.4087</cell></row><row><cell>3</cell><cell>0.5352</cell><cell>0.4036</cell></row><row><cell>4</cell><cell>0.5521</cell><cell>0.4295</cell></row><row><cell>5</cell><cell>0.5628</cell><cell>0.4407</cell></row><row><cell>6</cell><cell>0.5489</cell><cell>0.4297</cell></row><row><cell>7</cell><cell>0.5634</cell><cell>0.4414</cell></row><row><cell>8</cell><cell>0.5489</cell><cell>0.4297</cell></row><row><cell>9</cell><cell>0.4983</cell><cell>0.3778</cell></row><row><cell>10</cell><cell>0..4948</cell><cell>0.3734</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,89.29,648.88,416.70,24.46"><head></head><label></label><figDesc>The model was trained on data from 56 different inducers, containing 167,139 training values and tested on data from 56 inducers, containing 175,591 testing values. The base regressors obtained MAE values of 0.004 for KNN, 0.003 for CART and 0.085 for SVR. The voting regressor yielded a MAE of 0.017. Among the implemented regressors, CART provided the optimised result. Of the 10 best submissions, the best F1 score and CR score are 0.5634 and 0.4414 respectively.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,108.93,671.01,109.19,8.97"><p>https://scikit-learn.org/stable/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,186.24,393.33,10.91;8,112.39,199.79,394.25,10.91;8,112.41,213.34,36.52,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,333.53,186.24,172.46,10.91;8,112.39,199.79,95.44,10.91">Supervised search result diversification via subtopic attention</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,214.86,199.79,247.20,10.91">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1971" to="1984" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,226.89,395.17,10.91;8,112.66,240.44,96.05,10.91" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="8,162.35,226.89,255.94,10.91">Supervised learning for image search result diversification</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Göynük</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>Middle East Technical University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,253.99,393.33,10.91;8,112.66,267.54,358.37,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,372.48,253.99,133.50,10.91;8,112.66,267.54,159.12,10.91">Result diversification in image retrieval based on semantic distance</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,278.83,267.54,91.71,10.91">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">502</biblScope>
			<biblScope unit="page" from="59" to="75" />
			<date type="published" when="2019-10-01">2019 Oct 1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,281.08,393.33,10.91;8,112.66,294.63,341.78,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,342.01,281.08,163.98,10.91;8,112.66,294.63,135.43,10.91">IntenT5: Search Result Diversification using Causal Language Models</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mcavaney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Murray-Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.04026.2021</idno>
		<imprint>
			<date type="published" when="2009-08">Aug 9</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="8,112.66,308.18,393.33,10.91;8,112.66,321.73,362.18,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,294.91,308.18,211.07,10.91;8,112.66,321.73,120.87,10.91">The impact of result diversification on search behaviour and performance</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Moshfeghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,241.17,321.73,129.45,10.91">Information Retrieval Journal</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2019-10">2019 Oct</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,335.28,393.32,10.91;8,112.66,348.83,217.08,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,208.56,335.28,126.26,10.91">Ensemble learning: A survey</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Sagi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Rokach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,342.09,336.30,163.90,9.72;8,112.66,349.84,144.47,9.72">Wiley Interdisciplinary Reviews: Data Mining And Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1249</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,362.38,393.33,10.91;8,112.66,375.93,161.46,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,305.98,362.38,140.12,10.91">A survey on ensemble learning</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,453.54,363.39,52.45,9.72;8,112.66,376.94,74.59,9.72">Frontiers Of Computer Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="241" to="258" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,389.48,394.53,10.91;8,112.66,403.03,394.53,10.91;8,112.66,416.58,394.53,10.91;8,112.66,430.13,395.17,10.91;8,112.66,443.67,393.32,10.91;8,112.66,457.22,251.82,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,305.75,430.13,202.08,10.91;8,112.66,443.67,285.70,10.91">Overview of the ImageCLEF 2022: Multimedia Retrieval in Medical, Social Media and Nature Applications</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Brüngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deshayes-Chossart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,406.16,444.69,99.82,9.72;8,112.66,458.24,203.26,9.72">Experimental IR Meets Multilinguality, Multimodality, And Interaction</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,470.77,393.33,10.91;8,112.33,484.32,395.33,10.91;8,112.66,497.87,155.61,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,346.54,470.77,159.45,10.91;8,112.33,484.32,391.16,10.91">Overview of ImageCLEFfusion 2022 Task -Ensembling Methods for Media Interestingness Prediction and Result Diversification</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ştefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,112.66,498.89,106.79,9.72">CLEF2022 Working Notes</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,511.42,394.53,10.91;8,112.66,524.97,224.18,10.91" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="8,317.23,511.42,144.97,10.91">Classification and regression trees</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Wadsworth. International Group</publisher>
			<biblScope unit="volume">432</biblScope>
			<biblScope unit="page">9</biblScope>
			<pubPlace>Belmont, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,538.52,393.33,10.91;8,112.66,552.07,310.77,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,388.63,538.52,117.36,10.91;8,112.66,552.07,39.59,10.91">Support vector regression machines</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,159.93,553.08,221.04,9.72">Advances In Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
