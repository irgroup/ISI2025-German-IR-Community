<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,403.70,15.42;1,89.29,106.66,35.39,15.43">IUST_NLPLAB at ImageCLEFmedical Caption Tasks 2022</title>
				<funder ref="#_yMr7TFN">
					<orgName type="full">Simorgh Supercomputer -Amirkabir University of Technology</orgName>
				</funder>
				<funder>
					<orgName type="full">School of Computer Engineering of Iran University of Science and Technology and Iran&apos;s National Elites Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,97.42,11.96"><forename type="first">Malihe</forename><surname>Hajihosseini</surname></persName>
							<email>m_hajihosseini@comp.iust.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department">Student at School of Computer Engineering</orgName>
								<orgName type="institution">Iran University of Science and Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country>Islamic Republic Of Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,199.36,134.97,93.42,11.96"><forename type="first">Yasaman</forename><surname>Lotfollahi</surname></persName>
							<email>y_lotfollahi@comp.iust.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department">Student at School of Computer Engineering</orgName>
								<orgName type="institution">Iran University of Science and Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country>Islamic Republic Of Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,305.42,134.97,93.60,11.96"><forename type="first">Melika</forename><surname>Nobakhtian</surname></persName>
							<email>m_nobakhtian@comp.iust.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department">Student at School of Computer Engineering</orgName>
								<orgName type="institution">Iran University of Science and Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country>Islamic Republic Of Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,148.92,120.09,11.96"><forename type="first">Mohammad</forename><forename type="middle">Mahdi</forename><surname>Javid</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Student at School of Computer Engineering</orgName>
								<orgName type="institution">Iran University of Science and Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country>Islamic Republic Of Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,222.03,148.92,68.53,11.96"><forename type="first">Fateme</forename><surname>Omidi</surname></persName>
							<email>f_omidi97@comp.iust.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department">Student at School of Computer Engineering</orgName>
								<orgName type="institution">Iran University of Science and Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country>Islamic Republic Of Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,321.55,148.92,80.85,11.96"><forename type="first">Sauleh</forename><surname>Eetemadi</surname></persName>
							<email>sauleh@iust.ac.ir</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Assistant Professor of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Computer Engineering</orgName>
								<orgName type="institution">Iran University of Science and Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country>Islamic Republic Of Iran</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,403.70,15.42;1,89.29,106.66,35.39,15.43">IUST_NLPLAB at ImageCLEFmedical Caption Tasks 2022</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">7D05EC597EF931D4F342AF089922F8B4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical Image Captioning</term>
					<term>Concept Detection</term>
					<term>Caption Prediction</term>
					<term>Deep Learning</term>
					<term>Multi-label Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present models implemented by the IUST_NLPLAB group for ImageCLEFmedical Caption Task 2022. This task contains two subtasks: Concept Detection and Caption Prediction. Under the first subtask, the model should extract medical concepts contained in radiology images. These concepts can be used for context-based image and information retrieval. Under the second subtask, the model predicts the caption for a medical image. This can be used for improving the diagnosis and treatment of diseases by saving time, money and helping physicians. We used Retrieval Learning, Ensemble Learning, Multi Label Classification and Deep Learning techniques to rank 1st in the caption prediction subtask with 16 BLEU points over the second ranked group. We also ranked 8th in the concept detection task with a 5 percent gap from the top ranked group in F1 score.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>ImageCLEF <ref type="bibr" coords="1,138.32,437.34,16.34,10.91" target="#b0">[1]</ref> is part of CLEF 1 . ImageCLEF was launched in 2003 and added a medical task in 2004. Although it started with four participants, in 2020 was able to attract more than one hundred and ten participants from all around the world to participate in the competition. ImageCLEF includes various sections that retrieve and classify visual information using textual and visual data and their combinations.</p><p>In recent years, ImageCLEF has used the AIcrowd 2 platform to publish datasets and receive submissions. In 2022, one person from each group had to register with AIcrowd, then access the dataset and submit results on specified dates. Each group could register up to 10 successful submissions for each task. Five unsuccessful submissions for each group in each task were also allowed.</p><p>In ImageCLEFmedical 2022, two tasks were proposed: Image Captioning and Tuberculosis CT analysis. We selected the Image Captioning task from the ImageCLEFmedical section to participate in the competition. ImageCLEF medical Image Captioning task in 2022 contained two subtasks: Concepts Detection and Caption Prediction. These tasks have many uses, but their most important usage is to help physicians make accurate diagnoses and provide automatic descriptive reports of medical images which saves physicians's time. Each group could participate in one or both subtasks. In this paper, we present the methods our group, IUST_NLPLAB, from the Iran University of Science and Technology<ref type="foot" coords="2,300.03,220.70,3.71,7.97" target="#foot_0">3</ref> , School of Computer Engineering <ref type="foot" coords="2,461.98,220.70,3.71,7.97" target="#foot_1">4</ref> , Natural Language Pocessing Laboratory<ref type="foot" coords="2,232.86,234.25,3.71,7.97" target="#foot_2">5</ref> used in both subtasks. This is our first time participating in the ImageCLEF competition. We participated in both subtasks and registered ten successful submissions in the concept detection and caption prediction subtasks <ref type="bibr" coords="2,397.29,263.11,11.35,10.91" target="#b2">[2]</ref>. We were able to win first place in the caption prediction task with a margin of 16 BLEU points from the second group. Also, in the concept detection task, we were able to win the eighth place in the competition with a gap of about five percent in F1 measure from the first ranked group. In the following sections, we will describe the datasets used, models developed, and the results we achieved in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task description</head><p>This year the ImageCLEF evaluation campaign hosted the 6th edition of the medical image caption task. Unlike some of the previous editions which only contained the caption prediction task (e.g., 2016 <ref type="bibr" coords="2,156.04,416.58,12.00,10.91" target="#b3">[3]</ref>) or only the concept detection task (e.g., 2019 <ref type="bibr" coords="2,373.85,416.58,11.03,10.91" target="#b4">[4]</ref>), the 6th edition contained both subtasks as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Concept Detection</head><p>The goal for this task is to train a model based on the training data provided for extraction of UMLS 6  <ref type="bibr" coords="2,134.15,493.40,13.00,10.91" target="#b5">[5]</ref> Concept Unique Identifiers (CUIs) from medical images. This helps to better understand the medical concepts contained in medical images and can be used in other jobs such as caption generation. Table <ref type="table" coords="2,239.43,520.50,5.02,10.91" target="#tab_0">1</ref> lists the top 15 most frequent concepts in the training data.</p><p>The 2022 dataset includes 8374 medical concepts, which is a significant increase compared to 2021.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Caption Prediction</head><p>The goal of caption prediction is to train a model based on the training data provided to predict a suitable caption for medical images. It is essential for the model to correctly diagnose and extract sufficient information from medical images to be able to correctly predict the appropriate caption. This task is inherently more complex since it requires combining image processing and natural language processing techniques to generate captions for medical images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data</head><p>The dataset introduced for the ImageCLEFmedical Caption 2022 is a subset of the Radiology Objects in COntext (ROCO) <ref type="bibr" coords="3,210.44,441.76,16.14,10.91" target="#b6">[6]</ref> dataset. In this version of the dataset, imaging modality information is not mentioned. Also, as in previous versions, the dataset originates from biomedical articles of the PMC OpenAccess <ref type="bibr" coords="3,233.83,468.86,15.79,10.91" target="#b7">[7]</ref> subset. This dataset is used for both subtasks: Concept Detection and Caption Prediction. The published dataset consists of train, validation, and test images. Also, five Excel files were attached, including the names of concepts, concepts per train image, concepts per valid image, caption per train image, and caption per valid image. This dataset includes 83275 radiology images as training set, 7645 radiology images as validation set, and 7601 radiology images as test set. Figure <ref type="figure" coords="3,287.10,536.60,4.98,10.91" target="#fig_0">1</ref> compares the data size presented in the last four years for the Medical Image Captioning task at the ImageCLEF <ref type="bibr" coords="3,366.46,550.15,15.78,10.91" target="#b8">[8,</ref><ref type="bibr" coords="3,384.96,550.15,7.49,10.91" target="#b9">9,</ref><ref type="bibr" coords="3,395.18,550.15,14.06,10.91" target="#b10">10]</ref> evaluation campaign.</p><p>In 2021, the number of data has decreased significantly compared to previous years. This was due to only using radiology images described by medical experts <ref type="bibr" coords="3,375.56,577.25,17.61,10.91" target="#b10">[10]</ref>.</p><p>Table <ref type="table" coords="3,126.84,590.80,5.04,10.91">2</ref> shows some training data examples with their corresponding concepts and captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Image Concepts</head><p>In this task, for each image, a number of concepts are defined by the Unified Medical Language System¬Æ (UMLS) <ref type="bibr" coords="3,161.57,654.07,15.35,10.91" target="#b5">[5]</ref> Concept Unique Identifiers (CUIs) are specified. The number of concepts is different for each image. In training set, 3718 images have only one concept, while the maximum number of concepts for an image was 50. On average, five concepts are specified for each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Image Captions</head><p>A caption is provided for each image in the training and validation sets in this task. The organizers mentioned that the captions are pre-processed in the following four steps:</p><p>‚Ä¢ Numbers and words containing numbers have been removed</p><p>‚Ä¢ All punctuation was removed.</p><p>‚Ä¢ Lemmatization was applied using spaCy.</p><p>‚Ä¢ Captions were converted to lower-case.</p><p>The length of captions in the training set varies. According to surveys, in training set, 194 images have one-word captions, while the maximum caption length is 391. The most common caption length is ten words, of which 3771 images have captions of this length. The average length of a caption is 19 words. Figure <ref type="figure" coords="4,256.42,598.18,4.97,10.91" target="#fig_2">2</ref> shows the most repetitive words in training set captions and their frequency with stop words, and without them. We also calculated the TTR 7 for this caption dataset. TTR is obtained by dividing the number of unique words by the size of the text and is a simple measure of lexical diversity <ref type="bibr" coords="4,278.74,638.83,16.92,10.91" target="#b11">[11]</ref>. Considering the stop words, the value of TTR in this dataset is 0.022, and without considering the stop words, it is 0.031.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Sample images from the training set along with their concepts and captions <ref type="bibr" coords="5,402.28,102.49,15.05,8.87" target="#b12">[12,</ref><ref type="bibr" coords="5,419.82,102.49,11.46,8.87" target="#b13">13,</ref><ref type="bibr" coords="5,433.77,102.49,11.46,8.87" target="#b14">14,</ref><ref type="bibr" coords="5,447.72,102.49,11.29,8.87" target="#b15">15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Concepts Caption</p><p>‚Ä¢ C1306645 (Plain x-ray)</p><p>‚Ä¢ C0817096 (Chest)</p><p>‚Ä¢ C0225759 (Lung field)</p><p>‚Ä¢ chest radiograph show multiple tiny nodule white arrow in both lung field.</p><p>‚Ä¢ C0024485 (Magnetic Resonance Imaging) ‚Ä¢ C0006104 (Brain) ‚Ä¢ C0740279 (Atrophy of cerebellum)</p><p>‚Ä¢ mri brain show cerebellar atrophy.</p><p>‚Ä¢ C1306645 (Plain x-ray)</p><p>‚Ä¢ C1140618 (Upper Extremity)</p><p>‚Ä¢ C0018563 (Hand)</p><p>‚Ä¢ C0205082 (Severe (severity modifier)) ‚Ä¢ C5194734 (Tubular bones) ‚Ä¢ C0041600 (Bone structure of ulna)</p><formula xml:id="formula_0" coords="5,215.90,485.84,124.01,36.58">‚Ä¢ C1441672 (Observed) ‚Ä¢ C0025526 (Metacarpal bone) ‚Ä¢ C0699952 (Fused)</formula><p>‚Ä¢ hand of a patient with acrodysostosbe and multihormonal resbetance severe and generalized brachydactyly through very short and broad tubular bone include ulna can be observe metacarpals iiv be proximally pointed and coneshape proximal phalangeal epiphysbe be prematurely fuse the general appearance of the hand be bulky and stocky courtesy of prof dr jess argente.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methods</head><p>We first present image preprocessing techniques used for both subtasks, Next, we introduce models developed for the concept detection followed by caption prediction models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image Pre-processing</head><p>We used various techniques to improve the quality of medical images. Two of the most important are as follows.</p><p>‚Ä¢ Histogram Equalization: Histogram Equalization is an image processing method that uses a contrast enhancement technique <ref type="bibr" coords="6,313.70,572.54,18.67,10.91" target="#b16">[16]</ref>. In this method, the image histogram is flattened as much as possible and the probability distribution is mapped to a uniform probability distribution. However, this is not the best way to improve image quality, and in some cases may not have a good output because the average brightness of the output image is significantly different from the input image. ‚Ä¢ Contrast Limited Adaptive Histogram Equalization (CLAHE): CLAHE <ref type="bibr" coords="6,483.10,641.64,22.89,10.91" target="#b17">[17]</ref> is also a type of Histogram Equalization, in which contrast amplification is limited. In a typical Histogram Equalization, we see an increase in noise in near-constant regions. To As it is clear, the image quality has improved in some areas <ref type="bibr" coords="7,312.85,257.94,14.92,8.87" target="#b18">[18]</ref>.</p><p>solve this problem and improve feature extraction, we use CLAHE. CLAHE equalizes the brightness and contrast of the images. This technique divides the image into sections and applies histogram equalization to each section. Then the contrast amplification limit, also known as clip limit, is applied. We use a clip limit of 2 in our models.</p><p>Cropping and flipping were also used for data augmentation. For models that do not use data augmentation techniques, the CLAHE method is used to improve image quality. Figure <ref type="figure" coords="7,470.99,375.19,4.97,10.91" target="#fig_3">3</ref> shows the normal images of the dataset and images of histogram equalization and CLAHE operations on them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Concept Detection</head><p>Concept detection is a classification problem. We examine the two main approaches we adopted to solve this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Information Retrieval Approach</head><p>We studied and implemented the methods presented by Jacutprakart et al. <ref type="bibr" coords="7,426.16,514.89,16.39,10.91" target="#b19">[19]</ref>. Jacutprakart et al. received the second-best F1 score in the concept detection challenge in 2021. Their best approach was to extract image features from a CNN<ref type="foot" coords="7,325.32,540.24,3.71,7.97" target="#foot_3">8</ref> and use ùëò-NN<ref type="foot" coords="7,395.17,540.24,3.71,7.97" target="#foot_4">9</ref> to extract the concepts. That is, for a test image, the closest ùëò training images are found. The concepts of the closest images are then used to assign concepts to the test image. They got the best results by using cosine similarity to calculate the distance between two images, DenseNet121 <ref type="bibr" coords="7,438.34,582.64,20.79,10.91" target="#b20">[20]</ref> to extract features, and setting ùëò to 1. We attempted to implement this approach. However, because this year's dataset size is much larger than the previous year, we had trouble getting results from ùëò-NN. We tried to train a similar model on a much smaller subset of the dataset, but the results were underwhelming. Using the output to extract labels, similar to a multilabel classification task, produced significantly better results. Thus we stopped following this approach and moved on to trying a 1-NN ensemble.</p><p>In the 1-NN ensemble, we used a retrieval approach based on the AUEB NLP group model in 2021 <ref type="bibr" coords="8,120.64,127.61,19.38,10.91" target="#b21">[21]</ref> that achieved first place in the concept detection subtask. At first, we employed three kinds of different CNN encoders including a ResNet-50 <ref type="bibr" coords="8,365.92,141.16,19.07,10.91" target="#b22">[22]</ref>, a DenseNet-201 <ref type="bibr" coords="8,461.02,141.16,19.81,10.91" target="#b20">[20]</ref>, and an EfficientNet-B0 <ref type="bibr" coords="8,172.85,154.71,18.43,10.91" target="#b23">[23]</ref> that made up our primary model. All encoders were pre-trained on ImageNet <ref type="bibr" coords="8,128.13,168.26,19.42,10.91" target="#b24">[24]</ref>. These encoders were fine-tuned on the training set for five epochs and the best weights were saved according to loss values on the training set. In the next step, we trained five models for each encoder as part of the validation set and we got 15 encoders. Each of these encoders were trained for three epochs. After working on encoders, we used trained encoders to get image embedding of training examples. Image embeddings were retrieved from the last average pooling layer of each encoder. To detect concepts in test images, we also find image embeddings of test images. After computing similarity between train embeddings and test embeddings, we find the most similar training image to the test image. In the end, 15 training images will be chosen and each image corresponds to an encoder. To assign concepts to test images, we used a majority-voting mechanism. In this mechanism, among the 15 chosen images, concepts that appear more than ùëÅ times will be assigned to the test image. After trying different values for ùëÅ and evaluating results with accuracy and F1 score, results showed the best value for ùëÅ to be 8. Adding data augmentation to this model also improved its performance. Finally, after computing similarity between image embeddings with different methods, cosine similarity showed better results than other methods. In Table <ref type="table" coords="8,358.32,357.95,4.97,10.91">3</ref> we show a number of validation set images with the ground truth and predicted concepts along with their F1 score calculated by this approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Multi-Label Classification Method</head><p>In this method, we used image concepts as labels and built a multi-label classification model. We used CNNs with ImageNet <ref type="bibr" coords="8,223.52,447.93,20.53,10.91" target="#b24">[24]</ref> pre-trained weights, removed their last layer, and added a classification layer. The output layer has 8374 units and uses sigmoid as the activation function. The final model was then fine-tuned on the target dataset. We experimented with different pre-trained models and different configurations. Models were compiled with Adam <ref type="bibr" coords="8,454.84,488.57,22.24,10.91" target="#b25">[25]</ref> as the optimizer. Results on validation and test set were generated after every five epochs. We tried different thresholds on the output layer's activation function to classify the image concepts and used their F1 score to evaluate and find the best one. Table <ref type="table" coords="8,387.60,529.22,5.17,10.91" target="#tab_1">4</ref> shows the details of the implemented MLC 10 methods. Figure <ref type="figure" coords="8,257.53,542.77,5.06,10.91" target="#fig_4">4</ref> also shows the architecture of the MLC-based concept detection model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>Example of concept detection with ground truth and predicted concepts with F1 scores of them <ref type="bibr" coords="9,478.18,102.49,14.99,8.87" target="#b26">[26,</ref><ref type="bibr" coords="9,495.65,102.49,11.43,8.87" target="#b27">27,</ref><ref type="bibr" coords="9,89.29,114.45,11.46,8.87" target="#b28">28,</ref><ref type="bibr" coords="9,103.24,114.45,11.29,8.87" target="#b29">29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Ground Truth Prediction F1 score  </p><formula xml:id="formula_1" coords="9,219.71,177.94,299.44,145.95">‚Ä¢ C0024485 (Magnetic Res- onance Imaging) ‚Ä¢ C0346308 (Pituitary macroadenoma) ‚Ä¢ C0205129 (Sagittal) ‚Ä¢ C0024485 (Magnetic Res- onance Imaging) ‚Ä¢ C0346308 (Pituitary macroadenoma) ‚Ä¢ C0205129 (Sagittal) ‚Ä¢ 1.0 ‚Ä¢ C0041618 (Ultrasonogra- phy) ‚Ä¢ C0016823 (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Caption Prediction</head><p>For the caption prediction subtask, we studied the method implemented in <ref type="bibr" coords="10,415.51,478.27,16.09,10.91" target="#b30">[30]</ref>, which achieved first place in last year's caption prediction challenge. This team's best approach was to use a multi-label classification model. In this approach, each word is considered to be a label. A classification model is trained to predict the words that will later create a caption for the given image.</p><p>We used a CNN pre-trained on ImageNet <ref type="bibr" coords="10,283.73,546.01,20.65,10.91" target="#b24">[24]</ref> and fine-tuned it on the subtask training set to extract image features, similar to the multi-label classification method used in the concept detection subtask. For fine-tuning, the last layer of the CNN was removed. A dropout layer, an activation layer, and a dense layer were added. We tried different CNN models and different configurations.</p><p>To generate a caption for an image, the model will predict its corresponding words. Probability of each word is calculated in the output layer using sigmoid activation function. Then, the top ùëÅ words with the highest probability are chosen. ùëÅ is a hyper-parameter that will define the length of captions. Different values of ùëÅ in the range of 15 to 27 were tested on the validation set, and the best ùëÅ for each model was chosen using the BLEU score <ref type="bibr" coords="11,397.57,86.97,16.25,10.91" target="#b31">[31]</ref>. Two methods were used to turn the generated words into full captions:</p><p>1. Words are ordered from highest to lowest probability. 2. Words are ordered based on their statistics in the training set. Each word is assigned to its most common position in the caption.</p><p>Overall, we focused on predicting the correct words rather than finding their correct order. These two methods were not able to find the right order of words. That is why the final prediction may not be grammatically correct. But this approach was able to predict words well, which led to a high BLEU score.</p><p>Table <ref type="table" coords="11,126.44,229.79,4.97,10.91" target="#tab_2">5</ref> shows the details of the implemented classification models for the caption generation subtask. Figure <ref type="figure" coords="11,159.64,243.34,5.07,10.91" target="#fig_5">5</ref> also shows the architecture of the MLC-based caption prediction models.</p><p>Table <ref type="table" coords="11,126.87,256.89,5.04,10.91" target="#tab_3">6</ref> shows some examples of validation images with ground truth caption and predicted caption by our best submission. Their BLEU and ROUGE scores are also mentioned in the table. Because we used stemming while creating our vocabulary, some words, like "image", are stemmed in the final caption.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In this part, we review the results of the models implemented in the two subtasks of concept detection and caption prediction. In the concept detection subtask, F1 score was used to evaluate the models and the ranking was based on this metric. Also reported is the Secondary F1 score, which is calculated using only a subset of manual validated concepts. In the caption prediction subtask, BLEU <ref type="bibr" coords="13,148.42,165.48,20.33,10.91" target="#b31">[31]</ref>, ROUGE <ref type="bibr" coords="13,204.11,165.48,22.04,10.91" target="#b36">[36]</ref>, METEOR <ref type="bibr" coords="13,268.28,165.48,22.60,10.91" target="#b37">[37]</ref>, CIDR <ref type="bibr" coords="13,319.09,165.48,19.83,10.91" target="#b38">[38]</ref>, SPICE <ref type="bibr" coords="13,371.86,165.48,20.19,10.91" target="#b39">[39]</ref> and BERTScore <ref type="bibr" coords="13,460.70,165.48,20.79,10.91" target="#b40">[40]</ref> were used to evaluate the models. The ranking was based on BLEU score. ROUGE scores were also reported during the competition. Other metrics were reported after the challenge. Before presenting the results of our implemented models, we review the results obtained in the last six years in this task. Table <ref type="table" coords="13,240.98,219.67,4.97,10.91">7</ref> shows information about the size of the dataset, the number of concepts, and the results of the first three groups in each subtask <ref type="bibr" coords="13,392.87,233.22,13.74,10.91" target="#b2">[2,</ref><ref type="bibr" coords="13,409.35,233.22,12.59,10.91" target="#b10">10,</ref><ref type="bibr" coords="13,424.67,233.22,7.52,10.91" target="#b9">9,</ref><ref type="bibr" coords="13,434.91,233.22,7.52,10.91" target="#b8">8,</ref><ref type="bibr" coords="13,445.16,233.22,12.59,10.91" target="#b41">41,</ref><ref type="bibr" coords="13,460.48,233.22,12.59,10.91" target="#b42">42,</ref><ref type="bibr" coords="13,475.80,233.22,12.59,10.91" target="#b43">43,</ref><ref type="bibr" coords="13,491.12,233.22,12.41,10.91" target="#b44">44]</ref>. Note that the purpose of presenting this table is to express statistical information on this task in the last few years. Due to the differences in the data sets of different years, it is not correct to compare their results with each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 7</head><p>This table shows information about the datasets and the results obtained in ImageCLEFmedical Caption in the last six years. In the data set section, the number of training data, validation and test is specified. The number of concepts in the dataset per year is also shown. In the section on concept detection and caption prediction, the results of the top three groups are mentioned. As mentioned in the text, the purpose of presenting this table is to show the statistical information of the imageCLEFmedical caption task in recent years. The datasets of the years denoted by * are different, so comparing the results of the years with each other does not provide accurate information <ref type="bibr" coords="13,350.92,386.61,12.68,8.87" target="#b2">[2,</ref><ref type="bibr" coords="13,366.09,386.61,11.46,8.87" target="#b10">10,</ref><ref type="bibr" coords="13,380.04,386.61,6.82,8.87" target="#b9">9,</ref><ref type="bibr" coords="13,389.35,386.61,6.82,8.87" target="#b8">8,</ref><ref type="bibr" coords="13,398.67,386.61,11.46,8.87" target="#b41">41,</ref><ref type="bibr" coords="13,412.62,386.61,11.46,8.87" target="#b42">42,</ref><ref type="bibr" coords="13,426.56,386.61,11.46,8.87" target="#b43">43,</ref><ref type="bibr" coords="13,440.51,386.61,11.29,8.87" target="#b44">44]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Concept Detection</head><p>We chose our best models based on F1 score on the validation set and results with the highest score were submitted. One of our submissions used information retrieval method (submission v1) and other nine submissions followed multi-label classification approach. Table <ref type="table" coords="13,429.67,587.28,4.97,10.91" target="#tab_1">4</ref> has information about different MLC models. Table <ref type="table" coords="13,243.50,600.83,4.97,10.91" target="#tab_5">8</ref> shows our scores on the test set and the number of epochs and threshold for each model submission. The best result had 0.398 as F1 score. It also achieved 0.673 for secondary F1. Secondary F1 score was calculated using a subset of manually validated concepts (anatomy and image modality) only. This submission ranked 8 among all submitted group results. This system used ResNet50 as its base model with dropout <ref type="bibr" coords="13,428.01,655.03,19.72,10.91" target="#b45">[45]</ref> and no data augmentation. Information retrieval model had close scores to best MLC methods but at last MLCs ranked higher. It's interesting that information retrieval earned higher secondary F1 than MLC models.</p><p>Although our information retrieval model had good results among our different submissions, but we submitted just one model from this type. It happened because the information retrieval model was more complex than our MLC models, so it needed more time and resources to train. For example, MLC models approximately required three hours to train on server 3, but the information retrieval model took seven days to train on server 2. We tried to improve this model with different methods, but we were unable to get the final results due to time and resource constraints. Thus we decided to train simpler models with fewer parameters in both subtasks. This enabled us to have a faster turn-around time and iterate on more model improvement ideas.</p><p>After trying different models and configurations, we focused on different settings of v2.4 before the deadline, which produced the best result. The last submissions of v2 were from this particular version.</p><p>The submission limit allows us to submit only 10 runs but we still had some results that were not evaluated. After the submission deadline we asked ImageCLEF organizers to evaluate few more models for us which they generously accepted. These extra models showed improvement in concept detection results both in F1 score and secondary F1. Submission ES7, which used InceptionV3 <ref type="bibr" coords="14,141.38,622.23,18.94,10.91" target="#b46">[46]</ref> as its base model and dropout for regularization, achieved 0.419 F1 score, which was the best score for us. This system also used data augmentation techniques including CLAHE and random crop. The best secondary F1 belongs to submission ES2. This system is like our best model in this competition but it trained for 96 epochs and set its threshold to 0.25. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Caption Prediction</head><p>Our MLC approach could predict captions well, as all our ten submissions had BLEU scores higher than 0.430 and ranked from 1 to 10 on the released leaderboard. Using different settings, like using a different activation function, could improve the BLEU score. Overall, choosing the parameter ùëÅ , which determined the length of predicted captions, was a trade-off between the BLEU score and the ROUGE score. A higher ùëÅ resulted in a higher BLEU score and lower ROUGE score, and vice versa. As the primary score in this competition was BLEU, we focused on using a setting that would give us the highest BLEU score.</p><p>As we mentioned in the Methods sections, we used two different methods to sort the predicted words. In the first method, words were sorted from highest to lowest probability. For the second method, we studied the training set and noted the positions each word appears in. Then, we tried sorting the generated words using this data, but it did not improve the scores. The first method had better results. So, we focused on using this method in most submissions.</p><p>The best result was achieved by setting ùëÅ to 26 and using ReLU as the activation function. In this submission, words were ordered from highest to lowest probability. Table <ref type="table" coords="15,452.96,479.42,5.11,10.91" target="#tab_2">5</ref> shows the details of the submitted runs and Table <ref type="table" coords="15,265.28,492.97,10.15,10.91" target="#tab_7">10</ref> shows our submission scores on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">AIOps</head><p>Running a large number of experiments in a short amount of time with limited resources requires meticulous planning and operations. Given our limitations in time and resources, we believe our operations' strategy played a significant role in our success. This section elaborates on what worked and did not work for training models faster with fewer resources, consisting of (GPU, CPU, RAM, and Disk Space).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Memory Optimization</head><p>The first challenge our team faced was running out of memory. One of the bottlenecks in Artificial Intelligence projects is large datasets, which do not fit into memory. Data Generators can help; They will generate values lazy (on demand). It is not efficient or sometimes feasible to load all the data into memory at once. Another benefit is that our Model does not have to wait until all the data is processed before using them. Generators save the internal state without holding the entire data in memory. When the new data is requested, they continue from their previous saved state by providing the next batch of data, which are tiny portions of a larger dataset, to the requestor.</p><p>There are multiple ways to achieve this. We have used Sequence <ref type="bibr" coords="16,394.13,364.00,20.25,10.91" target="#b47">[47]</ref> from Keras API <ref type="bibr" coords="16,485.91,364.00,20.07,10.91" target="#b48">[48]</ref> because it is safer in multiprocessor environments. According to Keras documentation <ref type="bibr" coords="16,482.48,377.55,19.83,10.91" target="#b47">[47]</ref>: "This structure guarantees that the network will only train once on each sample per epoch, which is not the case with generators. "</p><p>There are some pitfalls when using a generator. For example, using a mutable global variable in a data generator, which is called multiple times, can result in unexpected behavior, and some anti-patterns can invert the purpose of generators and cause incremental memory growth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Faster Execution</head><p>To accomplish faster code execution, first, we need to determine why our code is time-consuming and which sections have the most significant impact on it. We used the TensorBoard Profiling tool <ref type="bibr" coords="16,106.95,522.12,17.66,10.91" target="#b49">[49]</ref> to analyze our model.</p><p>Profiling is the study of hardware resource consumption based on information gathered during the program's execution to identify which part of our program needs optimization and how we can speed up the overall program while minimizing resources.</p><p>After running the Profiler, TensorBoard <ref type="bibr" coords="16,268.05,576.32,19.58,10.91" target="#b49">[49]</ref> will offer us a visual representation of the gathered information, which consists of: To achieve higher throughput and better GPU utilization, we can raise the batch size sufficiently without exhausting the resources and running Out of Memory (OOM). To prevent the Model's accuracy from decreasing, we should scale the Model by tuning hyperparameters.</p><p>Parallel execution and multi-threading can also be used, depending on whether the tasks are CPU-Bound or I/O-Bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1.">I/O-Bound</head><p>1. Pre-fetching and caching the data at the cost of higher memory usage will enhance the Model's throughput. The input pipeline prepares the data for the next phase before the data is requested. 2. For I/O operations, multi-threading is highly recommended. It involves adding a new thread to an existing process, and memory is shared among them. Because of the shared memory, we need to use locks to control access to the shared data and prevent race conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2.">CPU-Bound</head><p>Multiprocessing is used for intensive CPU-limited tasks to achieve full CPU utilization. Each process has its own address space, and it is only applicable when we have multiple CPU cores.</p><p>Inter-process communication (IPC) with Pipes and Queues is also possible. Since multiprocessing comes with full CPU utilization, it is ideal for the jobs with the least amount of data but the most operations. "The more processes or threads we have, the faster it is." Another vital observation to note is that this sentence is not entirely accurate. This is because OS has to manage all these processes, and when there are too many, it may face scheduling overhead and reduce its overall speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Disk Usage Best Practices</head><p>Models will be trained regularly, and for having reproducible projects, we should version our Data and Models so that they can be easily shared, compared, and repeatedly reconstructed in our experiments. These tasks will be more manageable using the Version Control System.</p><p>‚Ä¢ When choosing VC 11 , it should support both on-premises and remote Cloud Storage Services (Azure, S3, GC) ‚Ä¢ When facing a lack of storage, try to use Symlinks Instead of duplicate files, we can compress files that currently are not needed to free up some space and keep the files simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Hardware</head><p>Most of our development was done on a system with GTX 1080 Ti GPU and another system with a RTX 2060 GPU provided by the computer engineering department. We also had limited access to an A100 GPU for final runs provided by the Simorgh Cloud. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>This paper describes the participation of IUST_NLPLAB at Iran University of Science and Technology at ImageCLEF caption 2022 task. In the Concept Detection subtask, we ranked 8 among 11 participating teams. We used MLC and information retrieval approaches in this subtask. Our MLC methods with adding dropout had better overall score. In the Caption Prediction subtask, all of our 10 submissions ranked higher than other groups' submissions and we achieved first rank in this subtask. We performed multi-label classification in this subtask as well. We used a classification model to predict the constructing words of a caption. Then, we used two different methods to create a caption. We hope to be able to participate in this competition in the future and achieve better results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,345.89,417.79,8.93;4,89.29,357.90,285.34,8.87;4,89.29,84.19,416.69,255.11"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of ImageCLEFmedical Caption data in the last four years. As shown in the chart, the number of data in 2022 compared to 2021 has grown significantly.</figDesc><graphic coords="4,89.29,84.19,416.69,255.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,215.90,597.03,125.07,9.96;5,224.85,609.92,63.46,8.87;5,215.90,622.30,124.01,9.96;5,215.90,635.61,116.03,9.96;5,215.90,648.92,117.33,9.96;5,215.90,662.23,108.43,9.96;5,369.58,597.03,125.07,9.96;5,378.53,609.92,114.46,8.87;5,378.53,621.88,18.50,8.87;6,106.09,272.06,187.81,9.96;6,119.25,284.01,174.65,9.96;6,118.88,295.97,175.02,9.96;6,119.25,307.92,174.65,9.96;6,119.25,319.88,174.65,9.96;6,118.88,331.83,176.11,9.96;6,118.88,343.79,175.01,9.96;6,119.25,355.74,118.43,9.96;6,301.08,272.06,187.81,9.96;6,314.47,284.01,174.42,9.96;6,314.83,295.97,174.06,9.96;6,314.83,307.92,174.24,9.96;6,314.83,319.88,92.41,9.96"><head>‚Ä¢</head><label></label><figDesc>C0024485 (Magnetic Resonance Imaging) ‚Ä¢ C0037949 (Vertebral column) ‚Ä¢ C0522510 (With intensity) ‚Ä¢ C3853028 (Thoracic Cord ) ‚Ä¢ C0054967 (CD6 antigen) ‚Ä¢ mri spine show hyperintensity in the thoracic cord till level (a) Ten most frequent words in the training set with considered stop words. Given the widespread use of stop words in texts, it is natural for stop words to have a main place in the chart. However, a few non-stop words like "show" have a significant number, which seems natural considering the use of this word to describe images. (b) Ten most frequent words in the training set without considered stop words. By looking at words, it is clear that most of the words are widely used in describing images or corrections in medical.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,89.29,379.36,220.29,8.93"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Ten most frequent words in the training set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,89.29,233.98,417.05,8.93;7,89.29,245.98,416.70,8.87;7,89.29,257.94,242.21,8.87;7,245.10,87.02,125.01,121.60"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The first column shows the normal image of the dataset. The second and third columns show the images after the Histogram Equalization and CLAHE technique applied on the normal image. As it is clear, the image quality has improved in some areas [18].</figDesc><graphic coords="7,245.10,87.02,125.01,121.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,89.29,210.14,273.16,8.93"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Architecture of the MLC-based concept detection model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="11,89.29,432.03,275.11,8.93;11,89.29,320.25,458.35,99.22"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Architecture of the MLC-based caption prediction model</figDesc><graphic coords="11,89.29,320.25,458.35,99.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="10,89.29,84.19,458.36,113.39"><head></head><label></label><figDesc></figDesc><graphic coords="10,89.29,84.19,458.36,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,90.49,324.79,225.29"><head>Table 1</head><label>1</label><figDesc>Most frequent concepts in the training data</figDesc><table coords="3,179.00,122.10,234.79,193.67"><row><cell>UMLS CUI</cell><cell>UMLS Meaning</cell><cell>frequency</cell></row><row><cell cols="2">C0040405 X-Ray Computed Tomography</cell><cell>25989</cell></row><row><cell>C1306645</cell><cell>Plain x-ray</cell><cell>24389</cell></row><row><cell>C0024485</cell><cell>Magnetic Resonance Imaging</cell><cell>14622</cell></row><row><cell>C0041618</cell><cell>Ultrasonography</cell><cell>11147</cell></row><row><cell>C0817096</cell><cell>Chest</cell><cell>7720</cell></row><row><cell>C0002978</cell><cell>angiogram</cell><cell>6027</cell></row><row><cell>C0000726</cell><cell>Abdomen</cell><cell>5772</cell></row><row><cell>C0037303</cell><cell>Bone structure of cranium</cell><cell>5144</cell></row><row><cell>C0221198</cell><cell>Lesion</cell><cell>3845</cell></row><row><cell>C0205131</cell><cell>Axial</cell><cell>3187</cell></row><row><cell>C0030797</cell><cell>Pelvis</cell><cell>3176</cell></row><row><cell>C0023216</cell><cell>Lower Extremity</cell><cell>2739</cell></row><row><cell>C0238767</cell><cell>Bilateral</cell><cell>2722</cell></row><row><cell>C0577559</cell><cell>Mass of body structure</cell><cell>2341</cell></row><row><cell>C0205129</cell><cell>Sagittal</cell><cell>2012</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,88.99,255.25,402.32,165.51"><head>Table 4</head><label>4</label><figDesc>Description of our concept detection models</figDesc><table coords="10,103.96,286.86,387.35,133.90"><row><cell>Name</cell><cell>Base model</cell><cell cols="2">Regularization Learning rate</cell><cell>Data augmentation</cell></row><row><cell>v2.1</cell><cell>Resnet50</cell><cell>None</cell><cell>0.001</cell><cell>None</cell></row><row><cell>v2.2</cell><cell>Resnet50</cell><cell>None</cell><cell>0.001</cell><cell>CLAHE, equalizeHist, hflip, original</cell></row><row><cell>v2.3</cell><cell>Resnet101</cell><cell>None</cell><cell>0.001</cell><cell>None</cell></row><row><cell>v2.4</cell><cell>Resnet50</cell><cell>Dropout(0.5)</cell><cell>0.001</cell><cell>None</cell></row><row><cell>v2.5</cell><cell>DenseNet121</cell><cell>None</cell><cell>0.001</cell><cell>None</cell></row><row><cell>v3.1</cell><cell>InceptionV3</cell><cell>None</cell><cell>0.009</cell><cell>None</cell></row><row><cell>v3.2</cell><cell>InceptionV3</cell><cell>None</cell><cell>0.009</cell><cell>None</cell></row><row><cell>v3.3</cell><cell>InceptionV3</cell><cell>None</cell><cell>0.009</cell><cell>CLAHE, equalizeHist, random crop</cell></row><row><cell>v3.4</cell><cell>InceptionV3</cell><cell>L2</cell><cell>0.009</cell><cell>equalizeHist, random crop</cell></row><row><cell>v3.5</cell><cell>InceptionV3</cell><cell>Dropout (0.5)</cell><cell>0.009</cell><cell>CLAHE, random crop</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,88.99,477.13,417.38,105.74"><head>Table 5</head><label>5</label><figDesc>Description of our caption prediction models</figDesc><table coords="11,95.27,508.75,411.10,74.12"><row><cell>Name</cell><cell>CNN</cell><cell cols="3">Data Augmentation Activation Freeze CNN</cell><cell>Learning rate</cell></row><row><cell>v1.1</cell><cell>ResNet50</cell><cell>None</cell><cell>PReLU</cell><cell>No</cell><cell>5e-4</cell></row><row><cell>v1.2</cell><cell>ResNet50</cell><cell>CLAHE</cell><cell>PReLU</cell><cell>No</cell><cell>5e-4</cell></row><row><cell>v1.3</cell><cell>ResNet50</cell><cell>None</cell><cell>PReLU</cell><cell>Yes</cell><cell>5e-4</cell></row><row><cell>v1.4</cell><cell>ResNet50</cell><cell>None</cell><cell>ReLU</cell><cell>No</cell><cell>5e-4</cell></row><row><cell>v1.5</cell><cell>ResNet50</cell><cell>None</cell><cell>PReLU</cell><cell>Yes</cell><cell>5e-4, decay every 2500 steps</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="12,88.99,92.93,417.00,554.47"><head>Table 6</head><label>6</label><figDesc>Example of caption prediction with different scores. In first row model had a good score but in second row it could not predict well<ref type="bibr" coords="12,207.34,116.89,15.05,8.87" target="#b32">[32,</ref><ref type="bibr" coords="12,224.88,116.89,11.46,8.87" target="#b33">33,</ref><ref type="bibr" coords="12,238.83,116.89,11.46,8.87" target="#b34">34,</ref><ref type="bibr" coords="12,252.78,116.89,11.29,8.87" target="#b35">35]</ref>.</figDesc><table coords="12,142.05,148.46,361.31,498.94"><row><cell>Image</cell><cell>Ground Truth</cell><cell>Prediction</cell><cell cols="2">BLEU ROUGE</cell></row><row><cell></cell><cell>axial ct image of the</cell><cell>arrow show ct axial</cell><cell cols="2">0.806 0.468</cell></row><row><cell></cell><cell>neck with intravenous</cell><cell>imag tomographi en-</cell><cell></cell></row><row><cell></cell><cell>contrast at the level of</cell><cell>hanc scan comput left</cell><cell></cell></row><row><cell></cell><cell>the parotid gland show</cell><cell>right mass muscl lesion</cell><cell></cell></row><row><cell></cell><cell>asymmetric left parotid</cell><cell>enlarg gland red neck</cell><cell></cell></row><row><cell></cell><cell>gland enlargement with</cell><cell>view contrast tissu soft</cell><cell></cell></row><row><cell></cell><cell>replacement by a soft</cell><cell>demonstr white nerv tu-</cell><cell></cell></row><row><cell></cell><cell>tissue mass white arrow</cell><cell>mor</cell><cell></cell></row><row><cell></cell><cell>horizontal section show</cell><cell>axial show imag ct com-</cell><cell cols="2">0.526 0.121</cell></row><row><cell></cell><cell>bony deficit at implant</cell><cell>put scan right tomo-</cell><cell></cell></row><row><cell></cell><cell>site</cell><cell>graphi lesion left patient</cell><cell></cell></row><row><cell></cell><cell></cell><cell>arrow bone view cortic</cell><cell></cell></row><row><cell></cell><cell></cell><cell>measur treatment frac-</cell><cell></cell></row><row><cell></cell><cell></cell><cell>tur month later head</cell><cell></cell></row><row><cell></cell><cell></cell><cell>area cbct plate section</cell><cell></cell></row><row><cell></cell><cell></cell><cell>margin</cell><cell></cell></row><row><cell></cell><cell>chest xray show bilat-</cell><cell>chest xray show left</cell><cell cols="2">0.140 0.193</cell></row><row><cell></cell><cell>eral pneumonia</cell><cell>patient leav right ar-</cell><cell></cell></row><row><cell></cell><cell></cell><cell>row lung pleural ef-</cell><cell></cell></row><row><cell></cell><cell></cell><cell>fus hemithorax admiss</cell><cell></cell></row><row><cell></cell><cell></cell><cell>radiograph lobe medi-</cell><cell></cell></row><row><cell></cell><cell></cell><cell>astin mass tip day medi-</cell><cell></cell></row><row><cell></cell><cell></cell><cell>astinum postop elev up-</cell><cell></cell></row><row><cell></cell><cell></cell><cell>per enlarg tube imag</cell><cell></cell></row><row><cell></cell><cell>result of roi extraction</cell><cell>arrow show right imag</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell></cell><cell>pixel</cell><cell>panoram left maxillari</cell><cell></cell></row><row><cell></cell><cell></cell><cell>radiograph lesion coron</cell><cell></cell></row><row><cell></cell><cell></cell><cell>bone patient bilater im-</cell><cell></cell></row><row><cell></cell><cell></cell><cell>pact mandibular molar</cell><cell></cell></row><row><cell></cell><cell></cell><cell>later view side white cor-</cell><cell></cell></row><row><cell></cell><cell></cell><cell>tic case area fractur si-</cell><cell></cell></row><row><cell></cell><cell></cell><cell>nus first</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="13,95.27,386.61,413.20,117.64"><head></head><label></label><figDesc>.</figDesc><table coords="13,95.27,406.23,413.20,98.03"><row><cell>Year</cell><cell></cell><cell cols="2">Dataset</cell><cell></cell><cell cols="6">Concept Detection (F1) Caption Prediction (BLEU)</cell></row><row><cell></cell><cell>train</cell><cell>valid</cell><cell>test</cell><cell>concepts</cell><cell>1st</cell><cell>2nd</cell><cell>3rd</cell><cell>1st</cell><cell>2nd</cell><cell>3rd</cell></row><row><cell cols="2">2022* 83275</cell><cell>7645</cell><cell>7601</cell><cell>8374</cell><cell cols="2">0.451 0.450</cell><cell>0.447</cell><cell cols="2">0.482 0.322</cell><cell>0.311</cell></row><row><cell>2021</cell><cell>2756</cell><cell>500</cell><cell>444</cell><cell>1586</cell><cell cols="2">0.505 0.468</cell><cell>0.419</cell><cell cols="2">0.509 0.461</cell><cell>0.431</cell></row><row><cell cols="4">2020* 65753 15970 3534</cell><cell>3047</cell><cell cols="2">0.394 0.392</cell><cell>0.380</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">2019* 56629 14157 10000</cell><cell>5528</cell><cell cols="2">0.282 0.265</cell><cell>0.223</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">2018 222305</cell><cell>-</cell><cell>10000</cell><cell>111156</cell><cell cols="2">0.110 0.009</cell><cell>0.050</cell><cell cols="2">0.250 0.179</cell><cell>0.172</cell></row><row><cell cols="4">2017 164614 10000 10000</cell><cell>20464</cell><cell cols="2">0.171 0.164</cell><cell>0.143</cell><cell cols="2">0.563 0.321</cell><cell>0.260</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="14,88.99,90.49,418.53,261.15"><head>Table 8</head><label>8</label><figDesc>IUST_NLPLAB concept detection submissions details and test results. ES stands for "Extra submission". These submissions were sent after the competition's deadline.</figDesc><table coords="14,155.70,134.06,283.87,217.58"><row><cell cols="6">Run ID Name Epochs Threshold F1 score Secondary score</cell></row><row><cell>181667</cell><cell>v1</cell><cell>-</cell><cell>-</cell><cell>0.394</cell><cell>0.750</cell></row><row><cell>181948</cell><cell>v2.1</cell><cell>16</cell><cell>0.1</cell><cell>0.281</cell><cell>0.355</cell></row><row><cell>182279</cell><cell>v2.2</cell><cell>16</cell><cell>0.1</cell><cell>0.252</cell><cell>0.352</cell></row><row><cell>182280</cell><cell>v2.3</cell><cell>12</cell><cell>0.1</cell><cell>0.255</cell><cell>0.352</cell></row><row><cell>182291</cell><cell>v2.4</cell><cell>48</cell><cell>0.1</cell><cell>0.387</cell><cell>0.611</cell></row><row><cell>182292</cell><cell>v2.5</cell><cell>4</cell><cell>0.12</cell><cell>0.242</cell><cell>0.332</cell></row><row><cell>182293</cell><cell>v2.5</cell><cell>8</cell><cell>0.1</cell><cell>0.244</cell><cell>0.318</cell></row><row><cell>182302</cell><cell>v2.3</cell><cell>48</cell><cell>0.1</cell><cell>0.243</cell><cell>0.305</cell></row><row><cell>182304</cell><cell>v2.4</cell><cell>48</cell><cell>0.12</cell><cell>0.394</cell><cell>0.656</cell></row><row><cell>182307</cell><cell>v2.4</cell><cell>48</cell><cell>0.13</cell><cell>0.398</cell><cell>0.673</cell></row><row><cell>ES1</cell><cell>v2.4</cell><cell>60</cell><cell>0.4</cell><cell>0.348</cell><cell>0.730</cell></row><row><cell>ES2</cell><cell>v2.4</cell><cell>96</cell><cell>0.25</cell><cell>0.411</cell><cell>0.785</cell></row><row><cell>ES3</cell><cell>v3.1</cell><cell>20</cell><cell>0.3</cell><cell>0.240</cell><cell>0.356</cell></row><row><cell>ES4</cell><cell>v3.2</cell><cell>20</cell><cell>0.4</cell><cell>0397</cell><cell>0.668</cell></row><row><cell>ES5</cell><cell>v3.3</cell><cell>40</cell><cell>0.4</cell><cell>0.385</cell><cell>0.623</cell></row><row><cell>ES6</cell><cell>v3.4</cell><cell>40</cell><cell>0.4</cell><cell>0.302</cell><cell>0.634</cell></row><row><cell>ES7</cell><cell>v3.5</cell><cell>20</cell><cell>0.4</cell><cell>0.419</cell><cell>0.721</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="15,88.99,90.49,310.78,165.51"><head>Table 9</head><label>9</label><figDesc>Caption prediction submissions' details</figDesc><table coords="15,195.51,122.10,204.26,133.90"><row><cell cols="5">Run ID Name Epochs N Sorting method</cell></row><row><cell>181670</cell><cell>v1.1</cell><cell>20</cell><cell>20</cell><cell>1</cell></row><row><cell>181951</cell><cell>v1.2</cell><cell>10</cell><cell>27</cell><cell>1</cell></row><row><cell>182249</cell><cell>v1.1</cell><cell>10</cell><cell>26</cell><cell>1</cell></row><row><cell>182250</cell><cell>v1.3</cell><cell>10</cell><cell>26</cell><cell>1</cell></row><row><cell>182275</cell><cell>v1.4</cell><cell>5</cell><cell>26</cell><cell>1</cell></row><row><cell>182290</cell><cell>v1.5</cell><cell>10</cell><cell>25</cell><cell>2</cell></row><row><cell>182314</cell><cell>v1.5</cell><cell>10</cell><cell>17</cell><cell>1</cell></row><row><cell>182315</cell><cell>v1.4</cell><cell>5</cell><cell>26</cell><cell>2</cell></row><row><cell>182319</cell><cell>v1.1</cell><cell>15</cell><cell>26</cell><cell>1</cell></row><row><cell>182327</cell><cell>v1.5</cell><cell>15</cell><cell>15</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="16,88.99,90.49,354.59,165.51"><head>Table 10</head><label>10</label><figDesc>Caption prediction submissions' test results</figDesc><table coords="16,151.69,122.10,291.89,133.90"><row><cell cols="6">Run ID BLEU ROUGE METEOR CIDEr SPICE BERTScore</cell></row><row><cell>181670 0.457</cell><cell>0.140</cell><cell>0.082</cell><cell>0.045</cell><cell>0.013</cell><cell>0.570</cell></row><row><cell>181951 0.474</cell><cell>0.138</cell><cell>0.092</cell><cell>0.026</cell><cell>0.006</cell><cell>0.554</cell></row><row><cell>182249 0.480</cell><cell>0.138</cell><cell>0.090</cell><cell>0.027</cell><cell>0.005</cell><cell>0.553</cell></row><row><cell>182250 0.482</cell><cell>0.139</cell><cell>0.089</cell><cell>0.026</cell><cell>0.005</cell><cell>0.557</cell></row><row><cell>182275 0.483</cell><cell>0.142</cell><cell>0.092</cell><cell>0.030</cell><cell>0.007</cell><cell>0.561</cell></row><row><cell>182290 0.481</cell><cell>0.142</cell><cell>0.091</cell><cell>0.031</cell><cell>0.014</cell><cell>0.567</cell></row><row><cell>182314 0.462</cell><cell>0.158</cell><cell>0.085</cell><cell>0.062</cell><cell>0.010</cell><cell>0.578</cell></row><row><cell>182315 0.480</cell><cell>0.142</cell><cell>0.093</cell><cell>0.030</cell><cell>0.013</cell><cell>0.570</cell></row><row><cell>182319 0.469</cell><cell>0.136</cell><cell>0.089</cell><cell>0.029</cell><cell>0.006</cell><cell>0.551</cell></row><row><cell>182327 0.440</cell><cell>0.162</cell><cell>0.083</cell><cell>0.071</cell><cell>0.013</cell><cell>0.574</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="16,103.64,610.19,402.35,66.46"><head>1 .</head><label>1</label><figDesc>Recommendations for next steps in model improvement. These suggestions range from determining whether our Model is input-bound, how much time is spent on Kernel Launch, and what percentage of the operations performed are 16 or 32-bit. 2. to figure out which GPU operations take the longest, TensorFlow Stats are used, and then We should improve the most time-consuming parts to notice substantial changes in execution time.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="18,88.99,174.62,412.78,117.67"><head>Table 11</head><label>11</label><figDesc>Information about the hardware used.</figDesc><table coords="18,93.51,199.79,408.26,92.50"><row><cell>Server ID</cell><cell>GPU</cell><cell></cell><cell>CPU</cell><cell>RAM</cell><cell>Disk</cell><cell>Duration</cell></row><row><cell>Model</cell><cell cols="2">Memory Size Memory Type</cell><cell>VCores</cell><cell>Memory Size</cell><cell>Disk Space</cell><cell>Days</cell></row><row><cell>ID : 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GeForce RTX 2060 Super</cell><cell>8 GB</cell><cell>GDDR6</cell><cell>6</cell><cell>24 GB</cell><cell>200 GB</cell><cell>10</cell></row><row><cell>ID : 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GeForce GTX 1080 Ti</cell><cell>11 GB</cell><cell>GDDR5X</cell><cell>6</cell><cell>32 GB</cell><cell>200 GB</cell><cell>20</cell></row><row><cell>ID : 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A100</cell><cell>40 GB</cell><cell>HBM2e</cell><cell>8</cell><cell>96 GB</cell><cell>200 GB</cell><cell>7</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,108.93,638.15,186.05,8.97"><p>http://www.iust.ac.ir/en (last accessed: 2022-05-27)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="2,108.93,649.11,185.66,8.97"><p>http://ce-inter.iust.ac.ir/ (last accessed: 2022-05-27)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="2,108.93,660.07,180.54,8.97"><p>https://nlplab.iust.ac.ir (last accessed: 2022-05-27)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_3" coords="7,108.93,660.08,113.81,8.97"><p>Convolutional Neural Network</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_4" coords="7,108.93,671.64,73.47,7.86"><p>ùëò-Nearest-Neighbor</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has been supported by the <rs type="funder">Simorgh Supercomputer -Amirkabir University of Technology</rs> under Contract No <rs type="grantNumber">ISI-DCE-DOD-Cloud-900808-1700</rs>. We also thank the support of the <rs type="funder">School of Computer Engineering of Iran University of Science and Technology and Iran's National Elites Foundation</rs> 12 for participating in this competition.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_yMr7TFN">
					<idno type="grant-number">ISI-DCE-DOD-Cloud-900808-1700</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="18,112.66,622.15,395.01,10.91;18,112.66,635.70,395.17,10.91;18,112.39,649.25,394.80,10.91;18,102.87,669.47,5.56,5.98;18,108.93,671.04,162.65,8.97" xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Peteri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kozlovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-D</forename><surname>≈ûtefan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dogariu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<ptr target="https://en.bmn.ir/" />
	</analytic>
	<monogr>
		<title level="j" coord="18,407.69,649.25,94.51,10.91">J. Deshayes-Chossart</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2022-05-27">2022-05-27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,86.97,394.62,10.91;19,112.66,100.52,393.33,10.91;19,112.66,114.06,395.17,10.91;19,112.66,127.61,393.54,10.91;19,112.66,141.16,170.14,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="19,352.50,86.97,154.78,10.91;19,112.66,100.52,300.10,10.91">Overview of the ImageCLEF 2022: Multimedia retrieval in medical, social media and nature applications</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,435.26,100.52,70.73,10.91;19,112.66,114.06,395.17,10.91;19,112.66,127.61,239.58,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<title level="s" coord="19,359.20,127.61,147.00,10.91;19,112.66,141.16,31.10,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,154.71,395.17,10.91;19,111.81,168.26,395.37,10.91;19,112.66,181.81,393.33,10.91;19,112.66,195.36,216.46,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="19,326.22,168.26,180.96,10.91;19,112.66,181.81,177.87,10.91">Overview of ImageCLEFmedical 2022caption prediction and concept detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garc√≠a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Br√ºngel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Idrissi-Yaghir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√§fer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,313.28,181.81,192.71,10.91;19,112.66,195.36,97.38,10.91">CLEF2022 Working Notes, CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,208.91,393.59,10.91;19,112.66,222.46,394.53,10.91;19,112.66,236.01,22.69,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="19,380.21,208.91,126.04,10.91;19,112.66,222.46,77.03,10.91">Overview of the ImageCLEF 2016 medical task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garc√≠a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schaer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bromuri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,212.47,222.46,289.72,10.91">Working Notes of CLEF 2016 (Cross Language Evaluation Forum)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,249.56,394.53,10.91;19,112.28,263.11,395.55,10.91;19,112.66,276.66,394.53,10.91;19,112.66,290.20,395.17,10.91;19,112.66,303.75,393.33,10.91;19,112.66,317.30,394.53,10.91;19,112.66,330.85,393.33,10.91;19,112.66,344.40,394.52,10.91;19,112.66,357.95,118.94,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="19,318.99,303.75,186.99,10.91;19,112.66,317.30,189.06,10.91">ImageCLEF 2019: Multimedia retrieval in medicine, lifelogging, security and nature</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>P√©teri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Liauchuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klimuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tarasau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D.-T</forename><surname>Dang-Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Gurrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kavallieratou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Del Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Rodr√≠guez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Vasillopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Karampidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Campello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,328.82,317.30,178.37,10.91;19,112.66,330.85,393.33,10.91;19,112.66,344.40,127.98,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction, Proceedings of the Tenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="19,272.53,344.40,186.62,10.91">LNCS Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,371.50,393.33,10.91;19,112.66,385.05,258.51,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="19,190.92,371.50,315.07,10.91;19,112.66,385.05,51.86,10.91">The unified medical language system (umls): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,173.16,385.05,98.78,10.91">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="D270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,398.60,393.33,10.91;19,112.33,412.15,393.66,10.91;19,112.66,425.70,394.53,10.91;19,112.66,439.25,123.33,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="19,375.93,398.60,130.06,10.91;19,112.33,412.15,163.48,10.91">Radiology objects in context (ROCO): a multimodal image dataset</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>R√ºckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,298.78,412.15,207.20,10.91;19,112.66,425.70,389.95,10.91">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,452.79,339.14,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="19,171.98,452.79,250.44,10.91">Pubmed central: The genbank of the published literature</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Roberts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,466.34,393.33,10.91;19,112.66,479.89,393.33,10.91;19,112.14,493.44,130.59,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="19,362.68,466.34,143.31,10.91;19,112.66,479.89,126.94,10.91">Overview of the ImageCLEFmed 2019 concept detection task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Seco De</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,269.78,479.89,134.88,10.91">CEUR Workshop Proceedings</title>
		<title level="s" coord="19,478.71,479.89,27.27,10.91;19,112.14,493.44,100.46,10.91">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,506.99,395.16,10.91;19,112.66,520.54,393.32,10.91;19,112.66,534.09,327.42,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="19,400.32,506.99,107.51,10.91;19,112.66,520.54,168.21,10.91">Overview of the Image-CLEFmed 2020 concept prediction task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garc√≠a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,303.26,520.54,202.73,10.91;19,112.66,534.09,193.66,10.91">Proceedings of the CLEF 2020-Conference and labs of the evaluation forum, CONFERENCE</title>
		<meeting>the CLEF 2020-Conference and labs of the evaluation forum, CONFERENCE</meeting>
		<imprint>
			<date type="published" when="2020-09-25">22-25 September 2020, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,547.64,394.53,10.91;19,112.66,561.19,393.57,10.91;19,112.14,574.74,395.42,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="19,112.66,561.19,325.22,10.91">Overview of the ImageCLEFmed 2021 concept &amp; caption prediction task</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,460.92,561.19,45.31,10.91;19,112.14,574.74,64.80,10.91">CLEF2021 Working Notes</title>
		<title level="s" coord="19,184.32,574.74,178.47,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,588.29,394.53,10.91;19,112.48,601.84,237.85,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="19,168.95,588.29,333.67,10.91">Can type-token ratio be used to show morphological complexity of languages?</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kettunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,112.48,601.84,153.92,10.91">Journal of Quantitative Linguistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="223" to="245" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,615.39,393.33,10.91;19,112.66,628.93,393.98,10.91;19,112.66,642.48,28.67,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="19,179.91,615.39,326.07,10.91;19,112.66,628.93,209.24,10.91">Case of pulmonary benign metastasizing leiomyoma from synchronous uterine leiomyoma in a postmenopausal woman</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,331.68,628.93,132.87,10.91">Gynecologic oncology reports</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="33" to="36" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,656.03,395.17,10.91;19,112.66,669.58,239.54,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="19,336.40,656.03,171.42,10.91;19,112.66,669.58,20.20,10.91">Ataxia telangiectasia: Family management</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Seshachalam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cyriac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Gnana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,141.84,669.58,152.70,10.91">Indian Journal of Human Genetics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,86.97,393.33,10.91;20,112.66,100.52,393.32,10.91;20,112.66,114.06,120.24,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="20,163.59,100.52,243.57,10.91">Brachydactyly e: isolated or as a feature of a syndrome</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pereda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Garin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Garcia-Barcina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Gener</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Beristain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Iba√±ez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Perez De Nanclares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,416.33,100.52,89.65,10.91;20,112.66,114.06,56.60,10.91">Orphanet journal of rare diseases</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,127.61,394.53,10.91;20,112.66,141.16,393.33,10.91;20,112.66,154.71,154.54,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="20,164.37,141.16,308.76,10.91">Autoimmune thyroiditis associated with neuromyelitis optica (nmo)</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Sudulagunta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">B</forename><surname>Sodalagunta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Khorram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sepehrar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonivada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Noroozpour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,483.63,141.16,22.35,10.91;20,112.66,154.71,109.74,10.91">GMS German Medical Science</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,168.26,393.33,10.91;20,112.66,181.81,394.52,10.91;20,112.66,195.36,168.72,10.91" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="20,264.15,168.26,241.84,10.91;20,112.66,181.81,389.23,10.91">A comparative study of histogram equalization based image enhancement techniques for brightness preservation and contrast enhancement</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">P</forename><surname>Maravi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.4033</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="20,112.66,208.91,393.98,10.91;20,112.41,222.46,38.81,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="20,180.79,208.91,220.34,10.91">Contrast limited adaptive histogram equalization</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zuiderveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,409.50,208.91,67.10,10.91">Graphics gems</title>
		<imprint>
			<biblScope unit="page" from="474" to="485" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,236.01,393.33,10.91;20,112.66,249.56,295.50,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="20,232.29,236.01,273.70,10.91;20,112.66,249.56,201.39,10.91">Mobile chest x-ray manifestations of 54 deceased patients with coronavirus disease 2019: Retrospective study</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,322.38,249.56,41.00,10.91">Medicine</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,263.11,395.17,10.91;20,112.66,276.66,393.33,10.91;20,112.66,290.20,394.52,10.91;20,112.66,303.75,365.22,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="20,138.15,276.66,367.84,10.91;20,112.66,290.20,389.71,10.91">Nlip-essex-itesm at imageclefcaption 2021 task: deep learning-based information retrieval and multi-label classification towards improving medical image understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jacutprakart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">P</forename><surname>Andrade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Compean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Papanastasiou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>Herrera</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="20,127.29,303.75,112.31,10.91">CLEF2021 Working Notes</title>
		<title level="s" coord="20,247.00,303.75,161.35,10.91">CEUR Workshop Proceedings, CEUR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,317.30,395.17,10.91;20,112.66,330.85,393.33,10.91;20,112.66,344.40,147.08,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="20,377.68,317.30,130.16,10.91;20,112.66,330.85,67.72,10.91">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,203.84,330.85,302.14,10.91;20,112.66,344.40,49.16,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,357.95,393.32,10.91;20,112.66,371.50,393.33,10.91;20,112.66,385.05,246.65,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="20,464.92,357.95,41.06,10.91;20,112.66,371.50,159.63,10.91">Aueb nlp group at imageclefmed caption tasks</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Charalampakos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kougia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,315.13,371.50,109.87,10.91">CLEF2021 Working Notes</title>
		<title level="s" coord="20,432.20,371.50,73.79,10.91;20,112.66,385.05,101.21,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,398.60,395.17,10.91;20,112.66,412.15,395.01,10.91;20,112.41,425.70,38.81,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="20,259.74,398.60,203.38,10.91">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,488.38,398.60,19.45,10.91;20,112.66,412.15,347.24,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,439.25,394.53,10.91;20,112.66,452.79,346.82,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="20,178.42,439.25,323.86,10.91">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,127.29,452.79,202.02,10.91">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,466.34,393.33,10.91;20,112.66,479.89,394.53,10.91;20,112.66,493.44,103.61,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="20,346.64,466.34,159.35,10.91;20,112.66,479.89,67.28,10.91">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="20,228.08,479.89,274.55,10.91">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,506.99,393.33,10.91;20,112.66,520.54,102.10,10.91" xml:id="b25">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m" coord="20,251.96,506.99,172.98,10.91">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="20,112.66,534.09,393.33,10.91;20,112.33,547.64,29.19,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="20,249.44,534.09,201.17,10.91">Atypical presentation of panhypopituitarism</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">K</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Anton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,460.31,534.09,32.37,10.91">Cureus</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,561.19,393.33,10.91;20,112.66,574.74,393.33,10.91;20,112.66,588.29,118.60,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="20,347.89,561.19,158.10,10.91;20,112.66,574.74,208.72,10.91">Effect of embryo transfer depth on ivf/icsi outcomes: A randomized clinical trial</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Davar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Poormoosavi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Mohseni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Janati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,332.82,574.74,173.16,10.91;20,112.66,588.29,55.87,10.91">International Journal of Reproductive BioMedicine</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">723</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,601.84,393.33,10.91;20,112.66,615.39,393.33,10.91;20,112.66,628.93,224.98,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="20,469.82,601.84,36.17,10.91;20,112.66,615.39,393.33,10.91;20,112.66,628.93,25.70,10.91">Unusual case of spontaneous hemopneumothorax in a tunisian pulmonology department: a case report</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Jazia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ayachi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chatbouri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kacem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Faidi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">B</forename><surname>Braiek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Maatallah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,146.58,628.93,146.27,10.91">The Pan African Medical Journal</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="20,112.66,642.48,395.17,10.91;20,112.66,656.03,394.52,10.91;20,112.66,669.58,146.61,10.91" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="20,470.00,642.48,37.83,10.91;20,112.66,656.03,389.46,10.91">Congruency and reactivation aid memory integration through reinstatement of prior knowledge</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">T</forename><surname>Van Kesteren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rignanese</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">G</forename><surname>Gianferrara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Krabbendam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Meeter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="20,112.66,669.58,77.89,10.91">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,86.97,393.32,10.91;21,112.66,100.52,394.52,10.91;21,112.66,114.06,324.77,10.91" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="21,280.31,86.97,225.67,10.91;21,112.66,100.52,252.90,10.91">Puc chile team at caption prediction: Resnet visual encoding and caption classification with parametric relu</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,388.31,100.52,114.10,10.91">CLEF2021 Working Notes</title>
		<title level="s" coord="21,112.66,114.06,179.33,10.91">CEUR Workshop Proceedings, CEUR-WS</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,127.61,393.33,10.91;21,112.66,141.16,393.53,10.91;21,112.66,154.71,203.57,10.91" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="21,310.87,127.61,195.12,10.91;21,112.66,141.16,88.86,10.91">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,224.89,141.16,281.30,10.91;21,112.66,154.71,116.04,10.91">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,168.26,393.32,10.91;21,112.66,181.81,344.65,10.91;21,495.80,181.81,10.19,10.91;21,112.33,195.36,29.19,10.91" xml:id="b32">
	<monogr>
		<title level="m" type="main" coord="21,340.40,168.26,165.58,10.91;21,112.66,181.81,339.45,10.91">A primary parotid mucosa-associated lymphoid tissue non-hodgkin lymphoma in a patient with sjogren syndrome</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Povlow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Streiff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Madireddi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jaramillo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,208.91,393.33,10.91;21,112.66,222.46,393.33,10.91;21,112.66,236.01,193.72,10.91" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="21,402.10,208.91,103.88,10.91;21,112.66,222.46,355.54,10.91">Prolonged nucleic acid conversion and false-negative rt-pcr results in patients with covid-19: A case series</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Trisnawati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">El</forename><surname>Khair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Puspitarani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Fauzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,475.09,222.46,30.90,10.91;21,112.66,236.01,109.79,10.91">Annals of Medicine and Surgery</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="224" to="228" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,249.56,395.17,10.91;21,112.66,263.11,393.33,10.91;21,112.66,276.66,393.33,10.91;21,112.66,290.20,143.50,10.91" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="21,389.81,249.56,118.02,10.91;21,112.66,263.11,393.33,10.91;21,112.66,276.66,280.52,10.91">Implant-prosthetic rehabilitation of patients with severe horizontal bone deficit on mini-implants with two-piece design-retrospective analysis after a mean follow-up of 5 years</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Petrakakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>El-Mahdy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nolte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,401.42,276.66,104.56,10.91;21,112.66,290.20,79.85,10.91">International Journal of Implant Dentistry</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,303.75,393.53,10.91;21,112.66,317.30,335.03,10.91" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="21,272.17,303.75,234.02,10.91;21,112.66,317.30,184.30,10.91">Artificial intelligence in detecting temporomandibular joint osteoarthritis on orthopantomogram</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-K</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,306.15,317.30,77.89,10.91">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,330.85,393.33,10.91;21,112.66,344.40,133.03,10.91" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="21,154.48,330.85,243.31,10.91">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName coords=""><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,419.57,330.85,86.42,10.91;21,112.66,344.40,55.58,10.91">Text summarization branches out</title>
		<imprint>
			<biblScope unit="page" from="74" to="81" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,357.95,393.33,10.91;21,112.66,371.50,393.33,10.91;21,112.66,385.05,134.04,10.91" xml:id="b37">
	<analytic>
		<title level="a" type="main" coord="21,234.11,357.95,271.87,10.91;21,112.66,371.50,104.59,10.91">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,242.66,371.50,263.33,10.91;21,112.66,385.05,46.51,10.91">Proceedings of the ninth workshop on statistical machine translation</title>
		<meeting>the ninth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,398.60,393.33,10.91;21,112.66,412.15,393.33,10.91;21,112.66,425.70,147.08,10.91" xml:id="b38">
	<analytic>
		<title level="a" type="main" coord="21,317.23,398.60,188.76,10.91;21,112.66,412.15,45.51,10.91">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,188.32,412.15,317.66,10.91;21,112.66,425.70,49.16,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,439.25,393.33,10.91;21,112.66,452.79,395.01,10.91;21,112.66,466.34,38.81,10.91" xml:id="b39">
	<analytic>
		<title level="a" type="main" coord="21,339.59,439.25,166.40,10.91;21,112.66,452.79,83.01,10.91">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,224.73,452.79,188.94,10.91">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,479.89,393.33,10.91;21,112.66,493.44,271.84,10.91" xml:id="b40">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09675</idno>
		<title level="m" coord="21,384.35,479.89,121.63,10.91;21,112.66,493.44,90.07,10.91">Bertscore: Evaluating text generation with bert</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="21,112.66,506.99,393.33,10.91;21,112.66,520.54,393.33,10.91;21,112.66,534.09,394.53,10.91;21,112.39,547.64,221.20,10.91" xml:id="b41">
	<analytic>
		<title level="a" type="main" coord="21,430.13,506.99,75.85,10.91;21,112.66,520.54,184.37,10.91">Overview of the ImageCLEF 2018 caption prediction tasks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garc√≠a Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhof</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,319.95,520.54,186.04,10.91;21,112.66,534.09,205.25,10.91">Working Notes of CLEF 2018-Conference and Labs of the Evaluation Forum (CLEF 2018)</title>
		<title level="s" coord="21,173.53,547.64,129.93,10.91">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">September 10-14, 2018. 2018</date>
			<biblScope unit="volume">2125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,561.19,395.16,10.91;21,112.66,574.74,393.58,10.91;21,112.66,588.29,143.91,10.91" xml:id="b42">
	<analytic>
		<title level="a" type="main" coord="21,381.92,561.19,125.91,10.91;21,112.66,574.74,360.37,10.91">Overview of ImageCLEFcaption 2017-image caption prediction and concept detection for biomedical images</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia Seco De</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>M√ºller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,481.80,574.74,24.44,10.91;21,112.66,588.29,88.97,10.91">CLEF 2017 working Notes</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">1866</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,601.84,393.33,10.91;21,112.66,615.39,169.98,10.91" xml:id="b43">
	<monogr>
		<title level="m" type="main" coord="21,213.70,601.84,292.28,10.91;21,112.66,615.39,34.86,10.91">Concept detection on medical images using deep residual learning network</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Dimitris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ergina</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Working Notes CLEF</note>
</biblStruct>

<biblStruct coords="21,112.66,628.93,393.33,10.91;21,112.66,642.48,322.81,10.91" xml:id="b44">
	<analytic>
		<title level="a" type="main" coord="21,256.86,628.93,249.13,10.91;21,112.66,642.48,91.90,10.91">Imagesem at imageclef 2018 caption task: Image retrieval and transfer learning</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,227.50,642.48,99.23,10.91">CLEF CEUR Workshop</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,112.66,656.03,393.33,10.91;21,112.26,669.58,393.72,10.91" xml:id="b45">
	<analytic>
		<title level="a" type="main" coord="21,426.69,656.03,79.30,10.91;21,112.26,669.58,208.14,10.91">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="21,327.56,669.58,178.42,10.91">The journal of machine learning research</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,100.52,393.33,10.91;22,112.66,114.06,393.33,10.91;22,112.66,127.61,147.08,10.91" xml:id="b46">
	<analytic>
		<title level="a" type="main" coord="22,344.36,100.52,161.63,10.91;22,112.66,114.06,83.10,10.91">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,216.60,114.06,289.39,10.91;22,112.66,127.61,49.16,10.91">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,141.16,394.03,10.91;22,112.66,154.71,250.83,10.91" xml:id="b47">
	<monogr>
		<title level="m" type="main" coord="22,166.66,141.16,130.23,10.91">Sequence class from keras api</title>
		<author>
			<persName coords=""><surname>Tensorflow</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence" />
		<imprint>
			<date type="published" when="2022-05-27">2022. 2022-05-27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,168.26,369.15,10.91" xml:id="b48">
	<monogr>
		<title level="m" type="main" coord="22,146.59,168.26,108.60,10.91">Keras api documentation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Api</surname></persName>
		</author>
		<ptr target="https://keras.io/" />
		<imprint>
			<date type="published" when="2022-05-27">2022. 2022-05-27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,181.81,394.53,10.91;22,112.48,195.36,395.34,10.91;22,112.66,208.91,394.53,10.91;22,112.66,222.46,395.17,10.91;22,112.66,236.01,394.53,10.91;22,112.30,249.56,395.36,10.91;22,112.66,263.11,393.33,10.91;22,112.66,276.66,66.68,10.91" xml:id="b49">
	<monogr>
		<title level="m" type="main" coord="22,160.36,249.56,316.02,10.91">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Man√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Vi√©gas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="tensorflow.org" />
		<imprint>
			<date type="published" when="2015">2015. 2022-05-27</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
