<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,91.94,75.32,411.40,17.04;1,211.97,96.08,171.61,17.04">Profiling Irony Speech Spreaders on Social Networks Using Deep Cleaning and BERT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.02,141.46,62.82,10.80"><forename type="first">Leila</forename><surname>Hazrati</surname></persName>
							<email>leilahazrati.75@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Eng. Department</orgName>
								<orgName type="department" key="dep2">Faculty of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Tabriz</orgName>
								<address>
									<settlement>Tabriz</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,144.98,141.46,91.58,10.80"><forename type="first">Alireza</forename><surname>Sokhandan</surname></persName>
							<email>a.sokhandan@tabrizu.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Eng. Department</orgName>
								<orgName type="department" key="dep2">Faculty of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Tabriz</orgName>
								<address>
									<settlement>Tabriz</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,264.05,141.46,78.25,10.80"><forename type="first">Leili</forename><surname>Farzinvash</surname></persName>
							<email>l.farzinvash@tabrizu.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Eng. Department</orgName>
								<orgName type="department" key="dep2">Faculty of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Tabriz</orgName>
								<address>
									<settlement>Tabriz</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">CLEF</orgName>
								<address>
									<postCode>2022</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,91.94,75.32,411.40,17.04;1,211.97,96.08,171.61,17.04">Profiling Irony Speech Spreaders on Social Networks Using Deep Cleaning and BERT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6F1192319D0D4F744625DD753CDB9AD3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Irony Detection</term>
					<term>Author Profiling</term>
					<term>Stereotypes</term>
					<term>BERT</term>
					<term>Text Cleaning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With irony, language is employed figuratively and subtly to mean the opposite of what is stated. In the case of sarcasm, a more aggressive type of irony, the intent is to mock or scorn a victim without excluding the possibility to hurt. Stereotypes are often used, especially in discussions about controversial issues such as immigration, sexism, and misogyny. Regarding PAN's open submission toward tackling this issue, we use BERT (bidirectional encoder representations from transformers) as a way to identify ironic and sarcastic phrases from genuine ones in Twitter posts. Since the goal is to detect irony in texts published on social media, and usually social media users have a different writing style and their texts contain a variety of nonstandard language expressions, the input texts are deep cleaned before feeding into the BERT network. The experimental results show a significant improvement in the accuracy and training loss ratio of the BERT network by applying deep cleaning to input texts. Thus, we achieved up to 98.5 percent accuracy with the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A common definition of verbal irony is saying things opposite to what is meant <ref type="bibr" coords="1,444.85,455.85,11.69,9.94" target="#b0">[1]</ref>. Many studies have diverse opinions regarding sarcasm and irony being different phenomena <ref type="bibr" coords="1,427.06,468.57,12.78,9.94" target="#b1">[2]</ref> or being the same <ref type="bibr" coords="1,72.02,481.17,11.77,9.94" target="#b2">[3]</ref>. Irony as a literary technique is a linguistic device used in social networks such as Twitter to intent an idea while articulating an opposite expression. In this work, we don't differentiate between irony and sarcasm and intent to perform irony speech spreader identification on social media based on their post, to address the PAN-2022 task (IROSTERO 2022) which aims to profile irony and stereotype spreaders on Twitter. <ref type="bibr" coords="1,168.00,531.81,12.68,9.94" target="#b3">[4,</ref><ref type="bibr" coords="1,180.68,531.81,8.45,9.94" target="#b4">5]</ref> In the proposed method, we use a transformer model, called BERT <ref type="bibr" coords="1,399.07,544.41,12.80,9.94" target="#b5">[6]</ref> as the feature extractor. BERT is designed to help computers understand the meaning of ambiguous language in a text by using surrounding text to establish context. The BERT framework was pre-trained using text from Wikipedia and can be fine-tuned for any task by using a related dataset. By adding two dense layers on top of the BERT model, the input text data is binary classified as irony or not irony.</p><p>Approaches based on neural networks (word embedding) treat any input characters including special characters such as ",", ".", "!", "?", "#", and username mentions ("@") as a regular word <ref type="bibr" coords="1,511.20,620.17,12.34,10.04" target="#b6">[7,</ref><ref type="bibr" coords="1,72.02,633.00,7.99,9.94" target="#b7">8]</ref>. These approaches often do not perform data cleaning <ref type="bibr" coords="1,338.23,633.00,11.94,9.94" target="#b6">[7,</ref><ref type="bibr" coords="1,350.17,633.00,7.96,9.94" target="#b8">9]</ref>, considering that the network itself would solve the related problems <ref type="bibr" coords="1,221.90,645.60,17.01,9.94" target="#b9">[10]</ref>. In some works, where word embedding is used, and especially in cases where there are not enough training samples, the use of basic data cleaning significantly improves the feature representation <ref type="bibr" coords="1,236.18,670.92,17.73,9.94" target="#b10">[11,</ref><ref type="bibr" coords="1,253.91,670.92,13.30,9.94" target="#b11">12,</ref><ref type="bibr" coords="1,267.20,670.92,13.30,9.94" target="#b12">13]</ref>. In this work, considering that the input is the text posted on social media and in such texts, grammatical or linguistic principles are usually not fully observed, and from the other side, the database provided by the organizer (described in Section 4.1) has a small number of samples, a deep and targeted data cleaning such as URL filtering, stop words removal, removing punctuations, etc. is performed to increase the classification accuracy. Based on the experimental results (available on the TIRA <ref type="bibr" coords="2,284.91,112.36,18.48,9.94" target="#b13">[14]</ref> platform), the proposed method obtained 98.50% accuracy for detecting irony speaking on the provided dataset.</p><p>The rest of the paper is organized as follows: In Section 2, we present the related work on the irony detection field; in Section 3, we describe the methodology used for classification; in Section 4, the results of our experiments are presented, and finally Section 5, includes conclusion and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>With the merge of the social media era and the contribution of users on online platforms, a vast amount of data for emotional human linguistic behavior analysis can be retrieved as datasets mined from these platforms, to be specific Twitter. Because Twitter users express their feelings and opinions on social networks with frequent irony <ref type="bibr" coords="2,255.67,271.39,17.16,9.94" target="#b14">[15]</ref>, thus Twitter is a platform of choice due to its users posting their thoughts in stereotypical and ironic ways more than usual. Several approaches to irony and sarcasm detection have been developed. Some studies have used feature sets of the text to classify the text as ironic or not <ref type="bibr" coords="2,183.14,309.31,17.01,9.94" target="#b15">[16]</ref>. In <ref type="bibr" coords="2,221.08,309.31,16.99,9.94" target="#b16">[17]</ref>, Transformers architecture is used to contextualize pre-trained word embedding, they contextualize Word2Vec word embedding which is the authors' profile vector. They used BERT in transformer-based contextualization of pre-trained word embedding for irony detection on Twitter. In <ref type="bibr" coords="2,187.20,347.23,17.02,9.94" target="#b17">[18]</ref>, BERT is used for irony detection in the Portuguese language, and BERTs' output is considered as the authors' profile descriptor. In this work, the main objective is feature selection to choose a subset used in the classification process. It mentioned that the text information comprises features that are redundant and irrelevant, and the redundant features have no contribution to separating the classes from each other. Research in <ref type="bibr" coords="2,383.30,397.87,18.48,9.94" target="#b18">[19]</ref> is based on using various neural network models, namely Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and Baseline Convolutional Neural Networks (CNN) in an ensemble model to detect sarcasm on the internet, they used word embedding which is a method of representing words in the numeric vector form. It learns a divided representation of a vocabulary from a collection of text. It possesses the ability to divulge several unknown relationships between the words, the idea of the word embedding is that if there is a user in the training data with a certain personality, and they happen to make sarcastic tweets, then when it gets new data and there is a new user that has a similar style and therefore similar embedding to the previous user, without looking at the new user's tweet, it can predict if this user will be sarcastic or not, just by looking at the similarity of the embedding. This technique enables sharing the representation across words which helps in creating a more stable representation of words. <ref type="bibr" coords="2,185.45,536.97,18.32,9.94" target="#b19">[20]</ref> Presents an Intelligent ML-based sarcasm detection and classification (IMLB-SDC) technique. This technique encompasses different stages such as preprocessing, TF-IDFbased feature engineering, SVM-based classification, and PSO-based parameter tuning, and two Feature selection approaches are utilized, namely chi-square and information gain. The IMLB-SDC model relay on the Support vector machine as a classification model; and <ref type="bibr" coords="2,414.30,587.64,18.36,9.94" target="#b20">[21]</ref> surveys the current state-of-the-art and presents strong baselines for sarcasm detection based on BERT pre-trained language models. These methods only relay on the power of transformer models, and the necessity of data preparation is not well addressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>The proposed method consists of three main steps: clearing the input text, feature extraction, and classification, in these three steps, receiving the tweets of a user, determines whether that user is an ironic speaker or not. The overall procedure of the proposed method is shown in Fig <ref type="figure" coords="2,445.03,708.60,4.14,9.94" target="#fig_0">1</ref>.  Separating words that were stuck together by characters such as "/" and "\" For replacing the emojis with equivalent text we use the demojize function from emoji's python module. <ref type="bibr" coords="3,111.72,346.27,18.33,9.94" target="#b21">[22]</ref> in this module, the entire set of emoji codes as defined by the Unicode consortium is supported. We have to pass the emoji as an argument inside the demojize function, and its CLDR (Common Locale Data Repository) short name will be returned, which is a meaningful description of that emoji in the form of a few words. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Extraction &amp; Classification</head><p>In this work, we benefit from the BERT base model for binary classification. This model contains an encoder with 12 transformer blocks, 12 self-attention heads, and a flat layer with a size of 768. BERT takes an input sequence of no more than 512 tokens and outputs the representation of the sequence. The sequence has one or two segments. The first token of the sequence is always [CLS] which contains the special classification embedding and another special token [SEP] is used for separating segments. For text classification tasks, BERT takes the final hidden state of the first token [CLS] as the representation of the whole sequence. To use BERT or any other model, it must be further pre-trained based on the task at hand. There are three approaches:</p><p>1. Further pre-train the BERT on the training data for the target task, called within-task pretraining 2. Using a further pre-trained model of the other task from the same domain of the target task is called in-domain pre-training 3. Cross-domain pre-training, in which the further pre-trained model is obtained from a task with different domains than the target task <ref type="bibr" coords="3,306.04,715.08,18.59,9.94" target="#b22">[23]</ref> We use the first approach for our work. In this case, the error is back-propagated through the entire architecture and the pre-trained weights of the model are updated based on the new training dataset.</p><p>The output of the BERT model represents the feature vector of the input text and is passed into the Multi-Layer Perceptron (MLP) network to get the conclusion, of which, is the author of the input text is an ironic speaker or not. The MLP network which acts as a classification model consists of one input layer with the size of 768, two hidden layers containing 512 and 128 neurons respectively, with RELU (rectified linear unit) activation function, and a softmax output layer.</p><p>The pre-trained BERT model is followed by the classifier i.e. three dense layers, which together form a deep neural network, trained simultaneously, using the ADAM algorithm. In this process by using the loss function calculation by cross-entropy method, we update the MLP parameters which are the weights between dense layers, and simultaneously fine-tune the BERT's network parameters (the weights of embedding, attentions, and encoder layers which are almost 110M value) corresponding with given PAN's task.</p><p>In the proposed method, all the cleaned tweets of an author are concatenated and given to the BERT model, which gives us a numerical descriptor with 768 dimensions, considered as the identifier for the author's profile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments &amp; Results</head><p>In this section, firstly, we introduce the dataset that is employed. Then, we report and discuss the experimental results of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>The given data set provided by PAN <ref type="bibr" coords="4,259.78,379.63,18.63,9.94" target="#b23">[24]</ref> (shown in Table <ref type="table" coords="4,361.39,379.63,4.00,9.94" target="#tab_2">2</ref>), consists of 420 authors (Twitter users) having 200 tweets each. The grand truth of the dataset is a binary flag that determines whether the author's text is Ironic or not. The dataset is symmetric, containing 210 ironic speakers and 210 non-ironic speakers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments</head><p>We implement and review our proposed model in three ways, and the experimental results of these approaches are reported in Fig. <ref type="figure" coords="4,211.37,645.48,4.14,9.94" target="#fig_1">2</ref>, in terms of loss value (calculated by the cross-entropy method <ref type="bibr" coords="4,498.81,645.48,16.48,9.94" target="#b24">[25]</ref>), and Table <ref type="table" coords="4,118.58,658.08,5.52,9.94" target="#tab_4">3</ref>  As shown in Fig. <ref type="figure" coords="5,166.22,74.44,4.14,9.94" target="#fig_1">2</ref>, with aid of deep data cleaning, in our model, validation and training loss, are significantly reduced, compared to the 1 st and 2 nd approaches; and the amount of training loss tended toward zero. Generally, with a reduction of loss values, we looking for gaining accuracy, according to Table <ref type="table" coords="5,100.94,112.36,4.14,9.94" target="#tab_2">2</ref>, because of using deep preprocessing, in comparison to other modes, our proposed model's accuracy significantly increased reaching 98%.  From the obtained results, it can be seen that suitable cleaning has a significant effect on the results and accuracy of our model in the feature extraction and classification, especially when our data involves social media text that contains a large variety of nonstandard language expressions. By comparing the results of the second (normal cleaning) and third (deep cleaning) approaches, we come to the conclusion that targeted cleaning of data can be useful in diagnosing irony, because emoji is a medium for people to express emotions and their personalities, and conveyed the feelings of the writers more "authentically". Also from this comparison, it can be seen that the numbers are not an important factor in the discussion of ironic speech and their removal can be very effective in increasing the accuracy of irony detection. Results show that proportional BERT final layer size adjustment can increase the classification accuracy. The size of these layers should be commensurate with the number of data available, and since the task's data was limited, we consider fewer neurons for these layers. If more samples were available, we can use more neurons and layers to fine-tune BERT and improve model accuracy to cover a variety of states in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion &amp; Future Work</head><p>In this work, our goal was to identify users and writers who spoke sarcastically. To address this issue, we employed a transformer network as the main base, by performing a deep and targeted cleaning, when appropriate data with proper adjustment is injected into the BERT model, it returns accurate results. The experimental results show that compared to basic cleaning, targeted cleaning </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,72.02,135.62,234.91,11.04;3,72.00,72.00,444.60,61.44"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overall procedure of the proposed method</figDesc><graphic coords="3,72.00,72.00,444.60,61.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,72.02,317.81,347.52,11.04"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of our work versus base and basic preprocessed models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,86.18,232.36,248.28,98.52"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="3,329.87,232.36,4.60,9.94"><row><cell>)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,72.02,409.15,438.70,86.49"><head>Table 1 Replacing</head><label>1</label><figDesc></figDesc><table /><note coords="3,117.74,422.59,121.59,11.04;3,129.74,438.31,86.63,11.04;3,318.67,438.31,160.87,11.04;3,116.06,467.32,101.92,9.96;3,287.45,462.28,223.27,9.96;3,137.90,485.68,57.52,9.96;3,343.87,484.72,110.45,9.96"><p>emojis with equivalent text Tweets with emojis Replaced emoji with equivalent text Good morning everyone! Good morning everyone! Smiling face with open hands Good enough. Good enough. Pouting face</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,72.02,480.55,428.53,93.84"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="4,72.02,493.99,428.53,80.40"><row><cell>Dataset schema</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Name</cell><cell>Samples</cell><cell>Task</cell><cell>Label</cell><cell>Year</cell></row><row><cell>pan22-author-</cell><cell></cell><cell></cell><cell>I (210 Sample)</cell><cell></cell></row><row><cell>profiling-training-</cell><cell>420</cell><cell>Binary</cell><cell></cell><cell>2022</cell></row><row><cell>2022-03-29</cell><cell></cell><cell></cell><cell>NI (210 Sample)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,72.02,353.81,433.13,109.10"><head>Table 3</head><label>3</label><figDesc>Comparison between our work and BERT base model and basic preprocessed model</figDesc><table coords="5,102.14,382.85,403.01,80.06"><row><cell>method</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F1-score</cell></row><row><cell>Without text cleaning</cell><cell>0.6240</cell><cell>0.5816</cell><cell>0.8636</cell><cell>0.6950</cell></row><row><cell>Basic text cleaning</cell><cell>0.8345</cell><cell>0.8571</cell><cell>0.7741</cell><cell>0.8134</cell></row><row><cell>Deep text cleaning</cell><cell>0.9849</cell><cell>0.9850</cell><cell>0.9850</cell><cell>0.9850</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>significantly increased the accuracy, so it is possible to generalize this issue to the other sentimental analysis tasks to achieve higher accuracy.</p><p>In the future, we focus on preprocessing phase and specifically will work on identifying slang words and examine the effect of manipulating them on the accuracy of irony detection</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="6,93.38,170.08,430.05,9.94;6,93.38,182.80,338.96,9.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,166.58,170.08,205.99,9.94">Irony: Negation, Echo, and Metarepresentation</title>
		<author>
			<persName coords=""><forename type="first">Curcó</forename><surname>Carmen</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0024-3841(99)00041-8</idno>
		<ptr target="https://doi.org/10.1016/S0024-3841(99)00041-8" />
	</analytic>
	<monogr>
		<title level="m" coord="6,380.11,170.08,138.31,9.94">Irony in Language and Thought</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="269" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,93.38,195.40,430.23,9.94;6,93.38,208.12,244.49,9.94" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,148.87,195.40,316.49,9.94">Logic and conversation. Speech acts, ed. by peter cole and jerry morgan</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">P</forename><surname>Grice</surname></persName>
		</author>
		<idno type="DOI">10.1163/9789004368811_003</idno>
		<ptr target="https://doi.org/10.1163/9789004368811_003" />
		<imprint>
			<date type="published" when="1975">1975</date>
			<biblScope unit="page" from="41" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,93.38,220.72,430.14,9.94;6,93.38,233.32,43.30,9.94;6,158.05,233.32,45.31,9.94;6,224.61,233.32,15.94,9.94;6,261.91,233.32,50.50,9.94;6,333.64,233.32,32.45,9.94;6,387.48,233.32,26.82,9.94;6,435.58,233.32,41.64,9.94;6,498.57,233.32,24.91,9.94;6,93.38,246.07,193.35,9.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,247.85,220.72,271.28,9.94">A multidimensional approach for detecting irony in Twitter</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Veale</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10579-012-9196-x</idno>
		<ptr target="https://doi.org/10.1007/s10579-012-9196-x" />
	</analytic>
	<monogr>
		<title level="j" coord="6,93.38,233.32,43.30,9.94;6,158.05,233.32,45.31,9.94;6,224.61,233.32,15.94,9.94;6,261.91,233.32,45.91,9.94">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="239" to="268" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,93.38,258.67,429.84,9.94;6,93.38,271.39,430.13,9.94;6,93.38,283.99,203.54,9.94" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,432.34,258.67,90.88,9.94;6,93.38,271.39,295.08,9.94">Profiling Irony and Stereotype Spreaders on Twitter (IROSTEREO) at PAN 2022</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ortega-Bueno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chulvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangeland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fersini</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="6,418.37,271.39,105.14,9.94;6,93.38,283.99,47.72,9.94">CLEF 2022 Labs and Workshops</title>
		<title level="s" coord="6,149.05,283.99,73.61,9.94">Notebook Papers</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,93.38,296.59,430.23,9.94;6,93.38,309.31,429.80,9.94;6,93.38,321.91,429.83,9.94;6,93.38,334.63,301.93,9.94" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="6,209.28,321.91,313.93,9.94;6,93.38,334.63,220.10,9.94">Overview of PAN 2022: Authorship Verification, Profiling Irony and Stereotype Spreaders, and Style Change Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chulvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Heini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kredens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mayerl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ortega-Bueno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pezik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wolska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zangerle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13390</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,93.38,347.23,429.92,9.94;6,93.38,359.83,430.10,9.94;6,93.38,372.55,192.39,9.94" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,325.75,347.23,197.56,9.94;6,93.38,359.83,207.33,9.94">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1810.04805</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1810.04805" />
	</analytic>
	<monogr>
		<title level="j" coord="6,315.31,359.83,133.78,9.94">Computation and Language</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,93.38,385.15,429.98,9.94;6,93.38,397.76,429.82,10.05;6,93.38,410.49,272.45,9.94" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,199.61,385.15,246.27,9.94">Distributed representations of sentences and documents</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1405.4053</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1405.4053" />
	</analytic>
	<monogr>
		<title level="m" coord="6,469.78,385.15,53.59,9.94;6,93.38,397.76,311.28,10.05">Proceedings of the 31st International Conference on Machine Learning (ICML &apos;14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML &apos;14)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,93.38,423.21,430.11,9.94;6,93.38,435.81,430.13,9.94;6,93.38,448.41,186.87,9.94" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,290.69,423.21,232.80,9.94;6,93.38,435.81,154.67,9.94">Adaptive representations for tracking breaking news on Twitter, Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Brigadir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1403.2923</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1403.2923" />
	</analytic>
	<monogr>
		<title level="m" coord="6,291.74,435.81,166.01,9.94">Neural and Evolutionary Computing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>cs.IR</note>
</biblStruct>

<biblStruct coords="6,96.14,461.13,427.20,9.94;6,93.38,473.73,430.05,9.94;6,93.38,486.34,329.44,10.04" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,365.95,461.13,157.40,9.94;6,93.38,473.73,176.76,9.94">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,292.49,473.73,230.94,9.94;6,93.38,486.34,134.52,10.04">Proceedings of the 28th International Conference on Machine Learning (ICML &apos;11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML &apos;11)<address><addrLine>Bellevue, Wash, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,93.38,499.05,429.87,9.94;6,93.38,511.65,381.99,9.94" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="6,276.89,499.05,246.37,9.94;6,93.38,511.65,227.80,9.94">Improving Feature Representation Based on a Neural Network for Author Profiling in Social Media Texts</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">G</forename><surname>Adorno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sidorov</surname></persName>
		</author>
		<idno type="DOI">10.1155/2016/1638936</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,93.38,524.37,430.08,9.94;6,93.38,536.97,429.83,9.94;6,93.38,549.69,430.11,9.94;6,93.38,562.29,141.63,9.94" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,251.21,524.37,272.25,9.94;6,93.38,536.97,90.47,9.94">DRWS: a model for learning distributed representations for words and sentences</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-13560-1_16</idno>
	</analytic>
	<monogr>
		<title level="m" coord="6,207.29,536.97,208.93,9.94">PRICAI 2014: Trends in Artificial Intelligence</title>
		<title level="s" coord="6,204.41,549.69,158.73,9.94">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Pham</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Park</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8862</biblScope>
			<biblScope unit="page" from="196" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,93.38,574.89,430.09,9.94;6,93.38,587.64,429.96,9.94;6,93.38,600.13,429.81,10.04;6,93.38,612.96,166.58,9.94" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,172.10,574.89,351.37,9.94;6,93.38,587.64,31.38,9.94">Unsupervised text normalization using distributed representations of words and phrases</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">K R</forename><surname>Sridhar</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W15-1502</idno>
	</analytic>
	<monogr>
		<title level="m" coord="6,148.34,587.64,375.00,9.94;6,93.38,600.13,111.84,10.04">Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing (NAACL &apos;15)</title>
		<meeting>the 1st Workshop on Vector Space Modeling for Natural Language Processing (NAACL &apos;15)<address><addrLine>Denver, Colo, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="8" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,93.38,625.56,430.11,9.94;6,93.38,638.16,430.11,9.94;6,93.38,650.88,128.43,9.94" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="6,322.39,625.56,201.10,9.94;6,93.38,638.16,181.10,9.94">Microblog sentiment analysis with emoticon space model, Social Media Processing</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11390-015-1587-1</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="76" to="87" />
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,93.38,663.48,429.98,9.94;6,93.38,676.20,429.80,9.94;6,93.38,688.80,139.11,9.94" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-22948-1</idno>
		<title level="m" coord="6,345.90,663.48,177.46,9.94;6,93.38,676.20,245.39,9.94">TIRA Integrated Research Architecture, Information Retrieval Evaluation in a Changing World</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,96.14,701.52,427.24,9.94;6,93.38,714.12,352.93,9.94" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">C M J</forename><surname>Silva</surname></persName>
		</author>
		<idno>arxiv:1607.00976</idno>
		<title level="m" coord="6,389.47,701.52,133.91,9.94;6,93.38,714.12,217.21,9.94">Modelling context with user embeddings for sarcasm detection in social media</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,96.14,726.72,427.40,9.94;6,93.38,739.44,417.32,9.94" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="6,251.69,726.72,271.85,9.94;6,93.38,739.44,153.20,9.94">A multidimensional approach for detecting irony in Twitter, Language resources and evaluation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Veale</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10579-012-9196-x</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="239" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,93.38,74.44,429.97,9.94;7,93.38,87.04,359.55,9.94" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="7,257.33,74.44,266.03,9.94;7,93.38,87.04,182.68,9.94">Transformer based contextualization of pre-trained word embeddings for irony detection in Twitter</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">Á</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">H</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ferran</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2020.102262</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,96.14,99.76,427.47,9.94;7,93.38,112.36,429.68,9.94;7,93.38,125.08,430.01,9.94;7,93.38,137.68,140.23,9.94" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="7,358.63,99.76,164.99,9.94;7,93.38,112.36,100.56,9.94">Irony Detection in the Portuguese Language using BERT</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shengyi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Chuwei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Nankai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,220.25,112.36,302.82,9.94;7,93.38,125.08,430.01,9.94;7,93.38,137.68,38.30,9.94">Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2021) co-located with the Conference of the Spanish Society for Natural Language Processing (SEPLN)</title>
		<meeting>the Iberian Languages Evaluation Forum (IberLEF 2021) co-located with the Conference of the Spanish Society for Natural Language Processing (SEPLN)<address><addrLine>Málaga, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,93.38,150.28,430.03,9.94;7,93.38,163.00,321.03,9.94" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="7,380.47,150.28,142.95,9.94;7,93.38,163.00,136.23,9.94">Sarcasm detection using deep learning and ensemble learning</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nayyar</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11042-022-12930-z</idno>
	</analytic>
	<monogr>
		<title level="m" coord="7,223.51,150.28,145.23,9.94">Sh.Singhal1 and M.Srivastava1</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,93.38,175.60,430.04,9.94;7,93.38,188.32,381.56,9.94" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="7,229.85,175.60,293.57,9.94;7,93.38,188.32,190.25,9.94">An intelligent machine learning-based sarcasm detection and classification model on social networks</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Vinoth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prabhavathy</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11227-022-04312-x</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,93.38,200.92,430.16,9.94;7,93.38,213.52,152.25,9.94" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="7,191.57,200.92,327.40,9.94">Intermediate-Task Transfer Learning with BERT for Sarcasm Detection</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Savini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Caragea</surname></persName>
		</author>
		<idno type="DOI">10.3390/math10050844</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,94.58,226.24,408.26,9.94" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="7,227.69,226.24,105.26,9.94">Emoji for Python v1.7.0</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Wurster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Jalilov</surname></persName>
		</author>
		<ptr target="https://pypi.org/project/emoji/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,96.14,238.84,427.46,9.94;7,93.38,251.59,358.04,9.94" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="7,266.69,238.84,222.70,9.94">How to Fine-Tune BERT for Text Classification?</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Xuanjing</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32381-3_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32381-3_16" />
	</analytic>
	<monogr>
		<title level="j" coord="7,497.37,238.84,26.23,9.94">LNAI</title>
		<imprint>
			<biblScope unit="volume">11856</biblScope>
			<biblScope unit="page" from="194" to="206" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,96.14,264.19,427.08,9.94;7,93.38,276.91,430.01,9.94;7,93.38,289.51,179.79,9.94" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="7,437.62,264.19,85.60,9.94;7,93.38,276.91,226.60,9.94">Profiling Irony and Stereotype Spreaders on Twitter (IROSTEREO)</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Reynier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Berta</forename><forename type="middle">C</forename><surname>Bueno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fransisco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Elisabetta</forename><surname>Fersini</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.6397037</idno>
		<ptr target="https://doi.org/10.5281/zenodo.6397037" />
	</analytic>
	<monogr>
		<title level="m" coord="7,332.70,276.91,122.59,9.94">PAN 22 Author Profiling</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,93.38,302.11,430.26,9.94;7,93.38,314.83,430.11,9.94;7,93.38,327.43,192.75,9.94" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="7,378.79,302.11,144.86,9.94;7,93.38,314.83,31.78,9.94">A Tutorial on the Cross-Entropy Method</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">T</forename><surname>Kroese</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">Y</forename></persName>
		</author>
		<idno type="DOI">10.1007/s10479-005-5724-z</idno>
		<ptr target="https://doi.org/10.1007/s10479-005-5724-z" />
	</analytic>
	<monogr>
		<title level="j" coord="7,143.17,314.83,165.03,9.94">Annals of Operations Research</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="67" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
