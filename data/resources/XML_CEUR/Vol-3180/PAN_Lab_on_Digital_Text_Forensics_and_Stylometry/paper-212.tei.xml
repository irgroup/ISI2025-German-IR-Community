<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,81.24,77.49,432.66,16.71;1,212.40,98.13,170.41,16.71">Profiling Irony and Stereotype Spreaders on Twitter Using TF-IDF and Neural Network</title>
				<funder ref="#_zyqjTDU">
					<orgName type="full">Heilongjiang Province</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.00,142.11,60.28,11.62"><forename type="first">Haolong</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Heilongjiang Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,142.20,142.11,49.52,11.62"><forename type="first">Dingjia</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Heilongjiang Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,219.00,142.11,57.72,11.62"><forename type="first">Yutong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Heilongjiang Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">CLEF</orgName>
								<address>
									<postCode>1ï€ , 2022</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,81.24,77.49,432.66,16.71;1,212.40,98.13,170.41,16.71">Profiling Irony and Stereotype Spreaders on Twitter Using TF-IDF and Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6541AEB7544998FBAD0C8C55051ED1EC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Irony and Stereotype</term>
					<term>Bi-GRU</term>
					<term>Text CNN</term>
					<term>TF-IDF</term>
					<term>Embedding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe our participation in the author profiling task at PAN 2022.This task is mainly to profile the irony and stereotype spreaders on Twitter (IROSTEREO).We regard this task as a binary classification problem.Our proposed methods adopt TF-IDF, Bi-GRU and Text CNN models to extract the word frequency statistical features and deep semantic features of text respectively. Based on this series of features, the fully connected network layer is used to complete the classification prediction.Our final submitted system has an accuracy of 93.33% in the test set.This result verified the idea that word frequency statistical features and deep semantic features obtained by neural network jointly predict irony recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.3" lry="841.9"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.3" lry="841.9"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.3" lry="841.9"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.3" lry="841.9"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.3" lry="841.9"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.3" lry="841.9"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.3" lry="841.9"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the rapid development of the Internet, social media such as Facebook, Twitter, and Weibo have emerged in large numbers.While shrinking the communication distance between people, controversial remarks such as immigration or sexism and misogyny frequently appear.It has greatly affected the human rights and security of special groups, and has had a bad impact on the society <ref type="bibr" coords="1,102.79,472.37,13.19,10.70" target="#b0">[1,</ref><ref type="bibr" coords="1,115.98,472.37,8.80,10.70" target="#b1">2]</ref>.Therefore, identifying possible spreaders of irony and stereotype on Twitter can effectively prevent the large-scale dissemination of these controversial remarks among Twitter online users. Research on how to distinguish authors who have published irony and stereotype remarks in the past from authors who have never made irony and stereotype remarks as far as we know has important implications for regulating the legal compliance of social media information and protecting the purity of online speech dissemination.</p><p>The IROSTEREO task announced by PAN@CLEF in 2022 refers to given a Twitter feed in English, determine whether its author spreads irony and stereotypes <ref type="bibr" coords="1,380.64,560.93,14.03,10.70" target="#b2">[3]</ref>.The data set provided in the task consisted of a set of users who shared some ironic and stereotypical remarks, such as women or the LGTB community.The goal will be to classify authors as ironic or not depending on their number of tweets with ironic content. For the IROSTEREO task, this paper proposed a deep learning method based on the TF-IDF+Bi-GRU+Text CNN ensemble model to extract the n-gram statistical features and deep semantic features in the text,which achieved 93.33% of accuracy in the provided test set.</p><p>In Section 2, we present some related work on profiling irony and stereotype spreaders. In Section 3, we mainly describe the method proposed in this paper, including the extracted feature form and the overall model based on deep learning. In Section 4 ,we introduce the specific work in the experiment and the comparison of experimental results. Finally, in Section 5, we present the conclusions and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Detection and recognition of hateful and irony speech is a hot topic in natural language processing research in recent years.For example, Ibereveal, PAN@CLEF and other academic activities have successively released their related tasks, attracting the participation of many universities and research institutes around the world. In the 'Ibereveal 2018 Automatic Misogyny Recognition' task <ref type="bibr" coords="2,490.08,143.93,11.70,10.70" target="#b3">[4]</ref>, the method ranking first in accuracy rate uses a combination of multiple statistical features such as style, structure and n-gram vocabulary, and is based on SVM for prediction. In 2021 PAN@CLEF,the shared task <ref type="bibr" coords="2,128.52,181.85,12.80,10.70" target="#b4">[5]</ref> is to profiling hate speech Spreaders on Twitter. There are many methods for classification, preprocessing and feature selection.The best performing method is to use an ensemble classifier consisting of five different machine learning models for prediction. Four of them use word n-grams as features, while the fifth one was based on statistical features extracted from the Twitter feeds.The model achieved 75% accuracy in English and 80.5% in Spanish.Through the analysis, it can be found that the current research on the author profiling task usually adopts the traditional discrete statistical model. In terms of feature extraction, the N-gram model based on text vocabulary or word frequency is mainly used to extract features, and then TF-IDF is used to filter the features to capture the feature representation information of the text.However, such machine learning models usually lack context based semantic features.</p><p>Recently, based on the shortcomings of the above machine learning methods, deep learning methods have gradually attracted people's attention. For example, Siino et al. <ref type="bibr" coords="2,414.52,321.05,11.80,10.70" target="#b5">[6]</ref> used a Convolutional Neural Network (CNN) as a classifier to classify authors as HSS or nHSS, and its prediction accuracy on Spanish was 0.85. In addition, some teams use recurrent neural network (RNN), or BERT pretraining language model has also achieved good results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head><p>The model proposed in this paper consists of a total of four components, each of which has its own specific function. Three of these components, TF-IDF, Bi-GRU+Attention, and TextCNN, act as feature extractors in the model, while the fully connected layer is used as a classification prediction.</p><p>In the feature extraction, for each component, we extract a specific set of features separately: (i) use TF-IDF to extract feature vector T1; (ii) use Bi-GRU and attention mechanism to learn feature vector o'; (iii) Extract feature vectors V1 and V2 using two Text CNN models of uni-gram and bigram. In general, the feature vectors extracted from different components are aggregated using the integration method, and the label prediction results are output based on the softmax operation in the fully connected network layer.</p><p>The proposed model is shown in Figure <ref type="figure" coords="2,282.72,531.05,5.08,10.70" target="#fig_0">1</ref>.The following sections describe the details of the different components. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">TF-IDF model</head><p>In the experiment, after preprocessing the text data,we directly use the TF-IDF algorithm of sklearn for training to create a set of statistical features based on word frequency for each author. In particular, the output of TF-IDF is subjected to dimension reduction processing through the PCA algorithm, and a feature vector T1 with an dimension of 800 is obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Bi-GRU+Attention model</head><p>For the Embedding information, the Bi-GRU neural network mode <ref type="bibr" coords="3,395.52,201.65,12.92,10.70" target="#b6">[7]</ref> is used in the experiment to extract the semantic information of words based on the text context. Moreover, the attention calculation matrices Matrix1 and Matrix2 are used as the attention calculation matrices, and the attention weighting is done for the output of each word, which is finally combined into a vector o. The specific steps are as follows:</p><p>ï‚·</p><p>Step 1: Splicing the output of the top layer of Bi-GRU model to get the vector = [â„Ž1; â„Ž' , â„Ž2; â„Ž' -1 , . . . . . . , â„Ž; â„Ž' 1 ]. Its shape is (, â„Ž * 2). ï‚·</p><p>Step 2: Multiply the vector with matrix1, and then multiply with matrix2. Finally obtain the weight vector through the softmax function, the shape of is (1, ) . The calculation method of is shown in Equation <ref type="formula" coords="3,240.00,317.93,4.14,10.70">1</ref>.</p><p>= (( * 1) * 2)</p><p>(1) ï‚·</p><p>Step 3: Along the direction of n, use to make hadmard product of c to weight each word, get = âŠ™ .The shape of vector is (, â„Ž * 2). ï‚·</p><p>Step 4: Along the direction of n, sum the vector o to obtain the vector o' whose size is h*2. The Bi-GRU+Attention Model is shown in Figure <ref type="figure" coords="3,309.60,383.33,4.14,10.70" target="#fig_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Text-CNN model</head><p>In this paper, the TextCNN, a convolutional neural network based on text classification, is also used to extract the Embedding information of words. A TextCNN can be used to obtain ngram-like contextual information. 1D TextCNN, which means that the unigram feature is extracted when the convolution kernel is 1, is the bigram feature when the convolution kernel is 2.Taking 1D Text CNN as an example, the steps to extract features are as follows:</p><p>ï‚·</p><p>Step 1: Passing the word vector through the 1D convolution kernel to get the output = [ 1 , 2 , . . . . . . , ],where m is the number of channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ï‚·</head><p>Step 2: By K-max-pooling, the top K values in each channel are extracted to get ' = --() = [' 1 , ' 2 , . . . . . . , ' ], ' is a vector of size K. ï‚·</p><p>Step 3: Flatten all channels to get 1, 1 = (') = [ 1 , 2 , . . . . . . , * ]. Based on the above method, the eigenvector V2 based on bi-gram can be obtained by changing the size of convolution kernel to 2.Splicing V1 and V 2 to get the output vector of the Text CNN model.The Text CNN model is shown in Figure <ref type="figure" coords="4,285.60,192.29,4.14,10.70" target="#fig_2">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>The following sections mainly describe the experimental setup and the comparative analysis of the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data set</head><p>The data set used by the IROSTEREO task contains more than 400 XML files and a truth txt. A XML file per author (Twitter user) with 200 tweets. The name of the XML file correspond to the unique author id. The truth.txt file with the list of authors and the ground truth,and the first column corresponds to the author id. The second column contains the truth label.</p><p>Additionally, the performance on the IROSTEREO task will be evaluated by accuracy. Accuracy is defined as the ratio of the number of correct predictions to the total number of predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Preprocessing</head><p>The steps for preprocessing are as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ï‚·</head><p>Step 1: First, all tweets of each given author are extracted and stored in the corresponding list auth through regular matching.And extract all the authors' tweets and save the resulting 'auth' to a new list authors = [auth1[twitter1, . . . , twitter200], auth2[twitter1, . . . , teitter]. . . . . . ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ï‚·</head><p>Step 2: Perform lemmatization and word segmentation on the authors list file. Among them, lemmatization is performed through regular expressions, and each sentence is directly segmented through NLTK to separate words from special symbols such as punctuation, tabs, and expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ï‚·</head><p>Step 3: Build a dictionary and word vector information.Download the Glove word vector (300d) from the Stanford official website, import and load about 400,000 tokens and word vectors.And merge all the texts in the authors, select the 95% tokens with the highest frequency, make a difference set with the 400,000 tokens in the above word vector, and get the token difference set named Rest. Create word vectors for each token in Rest set, and then merge them into the 400000 tokens and word vectors formed above.The specific process of building a dictionary is shown in Figure <ref type="figure" coords="5,217.80,229.37,4.14,10.70" target="#fig_3">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental parameters</head><p>The experiment uses train_test_split function to shuffle the authors list file to obtain train_authors and validation_authors, of which the training set accounts for 70% and the validation set accounts for 30%.Table <ref type="table" coords="5,125.04,588.17,4.60,10.70" target="#tab_0">4</ref>.1 lists the parameters of Bi-GRU, Text CNN models, and other parameters choose default values. In the experiments, two baseline methods are used to compare the performance of the model proposed in this paper.The two baseline methods are as follows:</p><p>1. TF-IDF+SVM: This method adopts the idea of machine learning, and firstly uses the TF-IDF model to extract the statistical features based on word frequency in the text data. Secondly, the extracted statistical feature vector is sent to the LSA model to extract the latent semantic representation features between words in the text.Finally, based on the SVM classifier, the classification prediction is performed. 2. Bi-GRU+Fully Connected Network: This method is based on the idea of deep learning, and the text data is first sent to the Embedding layer for processing to obtain the corresponding word vector = [ 1 , 2 , . . . , ] T .After the vector is input to GRU, the output = [, '] T is obtained.Flatten the vector to obtain a one-dimensional vector ' = [ 1 , 2 , . . . , ], where is 2 * â„Ž and â„Ž is the size of the hidden layer.Finally, the full connection layer is approximately regarded as a classifier. The feature vector ' extracted by Bi-GRU is sent to the full connection layer, and the final output result is obtained through softmax function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experimental results</head><p>In the experiment, based on the idea of deep learning, we choose multiple groups of model structures to explore the influence of different feature extraction methods on the experimental results.Table <ref type="table" coords="6,133.08,323.33,4.60,10.70" target="#tab_0">4</ref>.2 reports the results between the proposed method and the baseline method in the training data set. Table <ref type="table" coords="6,175.44,336.05,4.60,10.70" target="#tab_0">4</ref>.3 shows the results of the proposed method on the TIRA platform <ref type="bibr" coords="6,470.84,336.05,13.35,10.70" target="#b8">[9]</ref>. The results of the proposed method in the test set Model Accuracy longma22(Our Team) 0.933</p><p>The model 4 proposed in this paper adopts the shuffle separation method in data processing, which can change long text into short text, so as to solve the problem caused by the insufficient memory ability of GRU. In addition, the model adopts the method of multiple shuffle separation and voting, which makes the value of the judgment probability around 0.5 less and the prediction effect more stable.</p><p>Overall, the accuracy of the irony recognition algorithm based on Model 4 is 1.6%, 1.6%, and 1% higher than the baseline method, respectively.Model 4 combines the word frequency statistical features and the N-Gram features extracted by Text CNN. Compared with models 2 and 3, model 4 can better obtain various information of the text. It also proves that the traditional word frequency statistical features and the deep semantic features play a common role in the prediction of ironic spreaders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we summarize the model submitted through the TIRA system.The model includes three feature extraction components and one classification component.In terms of feature extraction, we use TF-IDF model to extract word frequency features of text, capture semantic features of text based on Bi-GRU, and use Text CNN model to extract uni-gram and bi-gram statistical features,and use these feature vectors to perform classification operations at the fully connected neural network layers. From the results of the training set, the proposed method is significantly better than the baseline method, and the accuracy of the test set is 93.33%.</p><p>For future work, we will consider using more features, such as implicit features and non text features, to further improve the accuracy of prediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,72.00,732.28,292.41,10.83;2,129.72,566.04,296.40,162.12"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Model architecture based on TF-IDF and neural network</figDesc><graphic coords="2,129.72,566.04,296.40,162.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,72.00,664.24,254.08,10.83;3,181.92,405.72,252.84,254.40"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Model architecture based on Bi-GRU+Attention</figDesc><graphic coords="3,181.92,405.72,252.84,254.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,72.00,375.88,216.64,10.83;4,170.52,202.08,267.84,170.16"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Model architecture based on Text-CNN</figDesc><graphic coords="4,170.52,202.08,267.84,170.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,72.00,505.00,162.96,10.83;5,187.80,251.76,233.76,249.00"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Dictionary building process</figDesc><graphic coords="5,187.80,251.76,233.76,249.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,72.00,626.68,406.51,112.42"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table coords="5,72.00,626.68,406.51,112.42"><row><cell>1</cell><cell></cell></row><row><cell>The model parameters</cell><cell></cell></row><row><cell>Model</cell><cell>Parameters</cell></row><row><cell>Bi-GRU</cell><cell>Layer:1,Hidden layer size:300</cell></row><row><cell>Text CNN</cell><cell>convolution kernel:1 or 2ï¼Œchannel:100ï¼Œk=5</cell></row><row><cell></cell><cell>Optimizerï¼šAdamï¼ŒLoss Function: Cross Entropy</cell></row><row><cell>Training</cell><cell>Learning Rateï¼š0.0005ï¼ŒBatch:40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,72.00,361.96,421.39,100.18"><head>Table 4 . 2</head><label>42</label><figDesc>The results of the proposed model and the baseline methods No.</figDesc><table coords="6,84.60,390.28,408.79,71.86"><row><cell></cell><cell>Model</cell><cell>Accuracy</cell></row><row><cell>1</cell><cell>TF-IDF+SVM</cell><cell>0.928</cell></row><row><cell>2</cell><cell>Bi-GRU+Fully Connected Network</cell><cell>0.928</cell></row><row><cell>3</cell><cell>TF-IDF+Bi-GRU+Fully Connected Network</cell><cell>0.934</cell></row><row><cell>4</cell><cell>TF-IDF+Bi-GRU+Text CNN+Fully Connected Network</cell><cell>0.944</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,72.00,480.88,41.59,10.83"><head>Table 4 . 3</head><label>43</label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">Acknowledgments</head><p>This work is supported by the the scientific research project (<rs type="grantNumber">2021QJ08</rs>) of <rs type="funder">Heilongjiang Province</rs>.</p></div>
<div><head n="7.">References</head></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zyqjTDU">
					<idno type="grant-number">2021QJ08</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,93.24,266.33,430.08,10.70;7,93.24,278.93,430.20,10.70;7,93.24,291.53,64.56,10.70" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bosco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nozza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Patti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanguinetti</surname></persName>
		</author>
		<title level="m" coord="7,93.24,278.93,430.20,10.70;7,93.24,291.53,32.28,10.70">SemEval-2019 task 5: Multilingual detection of hate speech against immigrants and women in Twitter</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,93.24,304.25,429.96,10.70;7,93.24,316.85,291.24,10.70" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,272.16,304.25,246.33,10.70">Irony Detection via Sentiment-based Transfer Learning</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,108.24,316.85,171.76,10.70">Information Processing &amp; Management</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1633" to="1644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,93.24,329.57,430.05,10.70;7,93.24,342.17,430.08,10.70;7,93.24,354.77,180.00,10.70" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,386.28,329.57,137.01,10.70;7,93.24,342.17,239.14,10.70">Profiling Irony and Stereotype Spreaders on Twitter (IROSTEREO) at PAN 2022</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ortega-Bueno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chulvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fersini</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="7,361.08,342.17,156.95,10.70">CLEF 2022 Labs and Workshops</title>
		<title level="s" coord="7,93.24,354.77,73.77,10.70">Notebook Papers</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,93.24,367.49,430.02,10.70;7,93.24,380.09,194.40,10.70" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,241.80,367.49,281.46,10.70;7,93.24,380.09,58.03,10.70">Overview of the task on automatic misogyny identification at ibereval 2018</title>
		<author>
			<persName coords=""><forename type="first">Rosso</forename><forename type="middle">P</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Anzovino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,173.88,380.09,75.84,10.70">IberEval@SEPLN</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,93.24,392.81,430.02,10.70;7,93.24,405.41,302.76,10.70" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,264.84,392.81,258.42,10.70">Profiling Hate Speech Spreaders on Twitter Task at PAN</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Fersini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,135.72,405.41,144.83,10.70">CLEF 2021 Labs and Workshops</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note>Notebook Papers</note>
</biblStruct>

<biblStruct coords="7,93.24,418.13,430.08,10.70;7,93.24,430.73,430.08,10.70;7,93.24,443.33,295.80,10.70" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,341.28,418.13,182.04,10.70;7,93.24,430.73,270.26,10.70">Detection of hate speech spreaders using convolutional neural networks-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Siino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Nuovo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Tinnirello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Cascia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,128.64,443.33,144.83,10.70">CLEF 2021 Labs and Workshops</title>
		<editor>
			<persName><forename type="first">Guglielmo</forename><surname>Faggioli</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note>Notebook Papers</note>
</biblStruct>

<biblStruct coords="7,93.24,456.05,430.05,10.70;7,93.24,468.65,75.00,10.70" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,164.76,456.05,358.53,10.70;7,93.24,468.65,40.00,10.70">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</title>
		<author>
			<persName coords=""><forename type="first">Chung</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,93.24,481.37,332.76,10.70" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,129.00,481.37,254.60,10.70">Convolutional neural networks for sentence classification</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,387.84,481.37,4.24,10.70">J</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,93.24,493.97,35.46,10.70;7,144.36,493.97,12.60,10.70;7,172.44,493.97,30.60,10.70;7,218.64,493.97,9.48,10.70;7,243.84,493.97,48.24,10.70;7,307.68,493.97,12.60,10.70;7,335.88,493.97,7.98,10.70;7,359.40,493.97,10.80,10.70;7,385.80,493.97,25.62,10.70;7,427.08,493.97,44.04,10.70;7,486.60,493.97,36.72,10.70;7,93.24,506.57,430.14,10.70;7,93.24,519.17,39.48,10.70" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="7,385.80,493.97,25.62,10.70;7,427.08,493.97,44.04,10.70;7,486.60,493.97,36.72,10.70;7,93.24,506.57,319.34,10.70">TIRA integrated research architecture[M]//Information Retrieval Evaluation in a Changing World</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="123" to="160" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
