<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.78,84.74,404.51,15.42;1,88.71,106.66,286.48,15.42;1,89.29,129.00,157.29,11.96">Text-to-Text Transformer in Authorship Verification Via Stylistic and Semantical Analysis Notebook for PAN at CLEF 2022</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,154.90,73.40,11.96"><forename type="first">Maryam</forename><surname>Najafi</surname></persName>
							<email>maryam.najafi@partdp.ai</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">NLP Department</orgName>
								<orgName type="department" key="dep2">Part AI Research Center</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,180.07,154.90,62.59,11.96"><forename type="first">Ehsan</forename><surname>Tavan</surname></persName>
							<email>ehsan.tavan@partdp.ai</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">NLP Department</orgName>
								<orgName type="department" key="dep2">Part AI Research Center</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.78,84.74,404.51,15.42;1,88.71,106.66,286.48,15.42;1,89.29,129.00,157.29,11.96">Text-to-Text Transformer in Authorship Verification Via Stylistic and Semantical Analysis Notebook for PAN at CLEF 2022</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">5CA5B164A0BD80E699436779E21948A6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>authorship verification</term>
					<term>language models</term>
					<term>T5</term>
					<term>Convolutional Neural Networks (CNN)</term>
					<term>stylistic features</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Authorship verification has gained much attention in recent years, due to the emphasis placed on PAN@CLEF shared tasks.In authorship verification, linguistic patterns are analyzed to reveal information about the author of two or more texts in order to determine if they are written by the same author. We describe in this paper our authorship verification submission system and the deep neural network approach that will allow us to learn the stylistic and semantic features of authors in the contributors to the PAN@CLEF 2022 event [1], [2], [3]. The system uses the T5 language model as a base embedding layer, followed by CNN and an attention mechanism to extract local and contextual features. As a result of studying multiple language models and deep architectures, we obtained an accuracy of 91.79% on our test dataset which was manually created from a PAN-provided dataset. However, on the official PAN test set, our system obtained a 58.7% overall score.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Authorship Verification (AV) is a branch in digital text forensics that deals with comparing the stylistic, and linguistic patterns of two or more texts in order to determine whether they were written by the same author. In other words, the question of whether a documents were written by same author is commonly called AV. The digital library, online journalism, and social networks provide access to an incredible amount of digital texts. Social media plays an integral role in expanding access to AV. In various settings, it is important to verify document authorship automatically.</p><p>Researchers, for instance, are judged and compared according to the impact and quantity of their publications, and public figures are exposed by their posts on social media platforms. Massive amounts of textual data are being uploaded to the Internet. Online crimes are rising along with textual data. In order to reduce the problems raised by the Internet, many researchers have turned to the authorship detection. AV is a type of authorship detection that verifies that a document is written by the author by determining the authorship information of the document. In addition to its many applications it has also been used in other investigations such as phishing emails and plagiarism detection. Personality traits such as the author's text, genre, temperament, sentiment, native language, gender can be determined by stylistic features. In other words, AV is the process of determining whether documents have been written by the same author.</p><p>The accuracy of AV majorly depends on the features that are used for distinguishing the style of writing followed in the documents. In the previous works of AV, the researchers proposed various types of stylistic features to distinguish the author's writing style. The researchers analyzed that the performance of AV was poor when the stylistic features were used alone in the experiment.</p><p>A variety of successful technical approaches have been proposed for this task, many of which are based on traditional linguistic features, which include spelling mistakes, grammatical inconsistencies, and stylistic features to distinguish the author's writing style. These features are well suited to long documents, such as books and novels. AV accuracy can largely be attributed to the features that are used to define the style of writing used in documents. The lack of feature extraction by traditional approaches becomes apparent when dealing with short messages and datasets such as tweets and social media posts. So, a disadvantage of ML is that its reliability is greatly compromised when it comes to short and topically diverse social media texts. on the other hand, ML algorithms traditionally relied on so-called stylometric-features <ref type="bibr" coords="2,445.84,330.85,11.43,10.91" target="#b3">[4]</ref>.</p><p>As opposed to stylometric-feature-based systems, several papers have recently integrated the feature extraction task into a deep learning framework. Generally, author-specific writing styles also depend on the form of the text, e.g. whether it's a blackmail note, an Amazon review, a tweet, or a WhatsApp message.</p><p>Text-To-Text Transfer Transformer (T5) architecture <ref type="bibr" coords="2,329.19,398.60,12.69,10.91" target="#b4">[5]</ref> is shown to exhibit high performance in various Natural Language Processing (NLP) applications. The idea behind this study was to extract the semantic context of embedded text using a language model. In order to do this, we employed the last hidden state of T5-large. In addition to identifying the semantic context of the author, the purpose of this study is to identify stylistic, grammatical, and writing style characteristics of the author. In order to accomplish this, we extracted Part of Speech tagging (POS), emoji, punctuation, author-specific and topic-specific information from the text and provided each as a separate feature to the model. So, we can obtain both semantic and context information as well as stylometric features of the author.</p><p>We present a simple and effective approach to AV for similarity learning that significantly improves the performance on the dataset provided by the PAN@CLEF [1], <ref type="bibr" coords="2,415.89,534.09,12.68,10.91" target="#b1">[2]</ref> organization. We investigate similarities in the writing styles for two different texts with authors and get the reasonable result of 91.79% from our own-created test set from the PAN@CLEF<ref type="foot" coords="2,435.32,559.43,3.71,7.97" target="#foot_0">1</ref> official dataset. Our code is available at GitHub<ref type="foot" coords="2,229.08,572.98,3.71,7.97" target="#foot_1">2</ref> for researchers.</p><p>The remaining of this paper is organized as follows: Section 2 reviews related work. Section 3 describes both tasks and the provided dataset. Section 4 presents the theoretical background of the proposed neural model. Experiments and results are presented in Section 6. Section 7 contains paper conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>The AV using linguistic analysis identifies whether two or more texts were written by the same author based on the similarities between their language patterns. In this section, we present some of the approaches discussed in AV. This paper's approach is based on a hierarchical recurrent Siamese neural network (HRSN). A recurrent neural network (RNN) topology is said to automate the extraction of sensible and context-independent features. Using a similarity analysis, it was possible to draw reasonable conclusions about unknown authors' writing styles <ref type="bibr" coords="3,330.75,192.57,11.58,10.91" target="#b5">[6]</ref>. Writing well involves finding the right words to convey your message. Language analysis of the internal attention weights of the network in <ref type="bibr" coords="3,160.38,219.67,13.00,10.91" target="#b3">[4]</ref> shows that the proposed method can indeed latch onto some traditional linguistic categories. In GLAD there was a framework for AV in four languages of Dutch, English, Spanish, and Greek <ref type="bibr" coords="3,221.57,246.77,11.58,10.91" target="#b6">[7]</ref>. Both known and unknown documents are used to train a binary linear classifier. These features included character ngrams, lexical overlaps, visual text properties, and compression measures.</p><p>In this field, Language Models (LMs) and Graph Neural Networks (GNNs) are commonly used.</p><p>LG4AV claim that incorporating a LMs and GNN eliminates the need to manually extract features, and allows for the validation of relationships between authors <ref type="bibr" coords="3,409.16,314.52,11.43,10.91" target="#b7">[8]</ref>.</p><p>In PAN@CLEF 2014, Using a graph representation that captures a syntactic sequence of texts and a graph similarity measure, <ref type="bibr" coords="3,235.31,341.62,12.99,10.91" target="#b8">[9]</ref> evaluates the similarity between an unknown document and the known documents. An unknown document can be classified as written by the same author if the majority of comparisons exceed a predetermined threshold. In <ref type="bibr" coords="3,433.47,368.71,18.06,10.91" target="#b9">[10]</ref> there was a use of content-based features in experiments. By using the term weight measure, the researcher computed the importance of each term, and these weights were used to calculate the document weight. To verify the test document, document weights of training and test documents were compared. <ref type="bibr" coords="3,139.99,422.91,13.00,10.91" target="#b3">[4]</ref> propose Siamese network on the large-scale corpus of short Amazon reviews and an analysis of the internal attention weights of the network shows that the proposed model outperforms state-of-the-art approaches that were built upon stylometric features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data Description</head><p>The AV task dataset consists of 12264 samples in English. Each sample contains two texts belonging to different Discourse Types (DT). Each sample also has a tag that indicates whether the two texts are written by the same author. The samples are from the Aston 100 Idiolects Corpus in English covering the following DTs: essays, emails, text messages, and business memos. In order to train the deep learning model, we considered 70% of this dataset as train data and 15% as evaluation data. We also used the remaining 15% of the data as test data to evaluate the use of various features on the model performance. In this model, the T5 language model is used as feature representation vector. We use T5 with sharing weights between all features. The input text is the input tokens of text pairs. The input PoS is the PoS tags of input texts. The input information is the authorspecific and topic-specific information. The input punctuation is the sequence of input punctuations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">System Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Embedding Layer</head><p>The embedding module is used to convert input tokens to representation vectors. The embedding of words is fundamental to building a model based on deep learning architecture. As a result, in this module, we have used T5 as the embedding layer to obtain a suitable representation vector. Several recent studies have demonstrated that neural language models trained on unstructured text can implicitly store and retrieve contextualized semantic information. Due to the prominent role of the author's writing structure, the syntactic embedding of the author has also been implemented in this study in addition to semantic embedding.</p><p>Along with token representation an innovative PoS structure based on a T5 language model embedding is presented in the paper to effectively encode syntactic writing style. Each word was tagged with its corresponding PoS tag. Since the raw texts were tagged with their corresponding PoS tag, each PoS tag was indexed independently and fed into a T5 language model. The output of T5 contains contextualized and dense embeddings of PoS tags. Embeddings can be used to properly capture syntactic writing style.</p><p>The architecture incorporates two other stylistic features, punctuation and the author-specific information, as well as PoS in order to capture an author's stylometric information. The T5 receives all three of these features as well as the original text, as four inputs.</p><p>The direct feeding of available texts into a pre-trained transformer architecture eliminates the need for any hand-crafted stylometric features, which are of no use in scenarios where the writing style is, at least partially, standardized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Semantic Embedding</head><p>An essential step in creating NLP models is choosing an appropriate embedding vector. In this research, the T5 Encoder module was implemented as an embedding layer. The main part of this model is the use of the T5 module for all extracted features from the dataset. we use pairs of first and second text as an input of the T5 language model and get embedding from the last hidden state of T5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Syntactic Embedding</head><p>there are three features that were incorporated in this paper. the details will discuss in following sections.</p><p>PoS: Some researchers employed NLP tools to extract more complex syntactic and semantic features. The most popular of these features is PoS. The PoS has been extensively researched for its effectiveness in AV <ref type="bibr" coords="5,205.63,217.59,16.43,10.91" target="#b10">[11,</ref><ref type="bibr" coords="5,224.78,217.59,12.55,10.91" target="#b11">12,</ref><ref type="bibr" coords="5,240.05,217.59,12.55,10.91" target="#b12">13,</ref><ref type="bibr" coords="5,255.33,217.59,12.32,10.91" target="#b13">14]</ref>.</p><p>As a PoS tagger, we use the NLTK library <ref type="bibr" coords="5,290.63,231.14,18.07,10.91" target="#b14">[15]</ref> and utilize a set of 40 PoS tags. We convert the corresponding word in a sentence into PoS tag, then using a lookup table 𝑇 𝑝𝑜𝑠 ∈ 𝑅 |𝑍| we convert each PoS tag into index to use as T5 encoder input.</p><p>Both the PoS tags of the input texts were then fed into the T5 language model to obtain dense vector relationships. A benefit of this type of embedding vector is its fixed size of the syntactic embedding lookup table, which makes it less susceptible to out-of-vocabulary problems.</p><p>Punctuation and Emoji: A step toward interpretable AV is based on punctuation marks as a syntactic feature that consider grammatical structures and is, therefore, independent of content and topic. We also use emojis used in the text in combination with punctuations. Using punctuation and emoji ngrams can provide richer features to the model.</p><p>After extracting the sequence of punctuations and emojis from the input text, one-gram, two-gram, and three-gram of punctuations and emojis can be extracted using Convolutional Neural Networks (CNN) architecture. Our method of identifying punctuation and emoji ngrams allows us to identify the user's punctuation and emoji habits which reflect their writing style. Using these features, which can be called author punctuation writing style, can be very helpful to the model to achieve higher accuracy in AV task</p><p>Author-specific and topic-specific information: There are several special tokens used in this dataset. Special tokens include the &lt;new&gt; tag for indicating message boundaries, the &lt;nl&gt; tag for indicating new lines within a text, and Author-specific and topic-specific information, such as Named Entities Recognition (NER), were also used. Location, subject, job_title, day, and others are all included in this author-specific information. As the use of new lines, the number of times they are used, and how to express some information, such as named entities, can differ between people, using these features can aid the model in the AV task. By extracting different features from topic-specific and author-specific information, the deep learning model can perform better in AV tasks.</p><p>This section introduces features that can be used to extract author information. As can be seen from Figure <ref type="figure" coords="5,164.85,583.42,3.66,10.91" target="#fig_0">1</ref>, each of these features obtained from the data is then given to the T5 encoder, separately so that they can be incorporated into the next layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">CNN Module</head><p>A CNN architecture was designed to extract the punctuation and emoji ngrams from their sequence. Using input text, CNN can extract local context and ngram features. From a sequence of punctuations and emojis, one-gram, two-gram, and three-gram features have been extracted using one-, two-, and three-dimensional convolution filters. Moreover, convolution filters are followed by a max-pool layer to extract rich features. The final ngram features are derived from combining the features extracted from the max-pool layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Attention Module</head><p>Scaled dot-product attention <ref type="bibr" coords="6,214.46,176.94,17.73,10.91" target="#b15">[16]</ref> uses different weights to extract rich stylomeric features from the text. Because attention is focused on the tokens that best convey the style of a writer, using the attention mechanism is a viable method. The writing style of the author is determined by focusing on the most crucial words by considering the occurrences of these (POS and author-specific information) tokens by feeding them to attention.</p><p>Many tasks, including question answering, machine translation, speech recognition, and image captioning, have been successfully completed by attention mechanism. By assigning different weights to each token, we are able to extract rich features using a scaled dot-product attention system.</p><p>Tokens are assigned weights based on their significance and importance in defining text stylometry, regardless of their distance from each other. Therefore, the importance and relevance of tokens can be determined in identifying the writing style of an author. Attention comprises the following elements:</p><formula xml:id="formula_0" coords="6,215.61,364.06,290.37,32.74">𝑊 𝑄 𝑖 , 𝑊 𝐾 𝑖 , 𝑊 𝑉 𝑖 ∈ 𝑅 𝑑 𝑚𝑜𝑑𝑒𝑙 ×𝑑 𝑘 𝑄 = 𝑋𝑊 𝑄 , 𝐾 = 𝑋𝑊 𝐾 , 𝑉 = 𝑋𝑊 𝑉<label>(1)</label></formula><p>𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝑄, 𝐾, 𝑉 ) = 𝑠𝑜𝑓 𝑡𝑚𝑎𝑥( 𝑄𝐾 𝑇 √ 𝑑 𝑘 )𝑉</p><p>There are three trainable parameters: 𝑊 𝑄 , 𝑊 𝐾 , and 𝑊 𝑉 . In order to create three matrices (𝑄, 𝐾, and 𝑉 ), input 𝑋 is multiplied with matrices 𝑄, 𝐾, and 𝑉 . Dot-products between 𝑄 and 𝐾 are divided by √ 𝑑 𝑘 to prevent them from becoming too large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Prediction Module</head><p>The first step was to establish a fully connected layer, for predicting syntactic and semantic similarities in two texts. The general representation is then constructed by using a max-pooling layer based on the same dimensions of multiple tokens. Equation 2 formulates the max-pooling modules. 𝑍 = 𝑀 𝑎𝑥([ℎ 1 , ..., ℎ 𝑙 ])</p><p>The softmax classification method is used to determine the probabilities of the labels. Based on input, the module creates probabilities of distributions in terms of softmax classifiers. A softmax classifier is used to predict a label 𝑦 ˆfor an input sequence from a set of discrete classes (to be same or not to be).The softmax classifier takes R as input:</p><formula xml:id="formula_2" coords="6,216.15,647.61,289.83,10.91">𝑃 (𝑦 | 𝑍) = 𝑠𝑜𝑓 𝑡𝑚𝑎𝑥(𝑊 𝑅 + 𝑏)<label>(3)</label></formula><formula xml:id="formula_3" coords="6,245.79,669.58,260.20,14.58">︀ 𝑦 = 𝑎𝑟𝑔𝑚𝑎𝑥𝑃 (𝑦 | 𝑍)<label>(4)</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,243.01,416.70,8.93;4,89.29,255.02,416.69,8.87;4,89.29,266.97,418.36,8.87;4,89.29,278.93,413.00,8.87"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1:The Proposed model architecture. In this model, the T5 language model is used as feature representation vector. We use T5 with sharing weights between all features. The input text is the input tokens of text pairs. The input PoS is the PoS tags of input texts. The input information is the authorspecific and topic-specific information. The input punctuation is the sequence of input punctuations.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,660.06,239.86,8.97"><p>https://pan.webis.de/clef22/pan22-web/author-identification.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,108.93,671.02,210.03,8.97"><p>https://github.com/MarSanTeam/Authorship_Verification</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Setup</head><p>Model implementation was done in PyTorch on Nvidia V100 GPUs. Training was done through 100 epochs. To train the network, the AdamW optimizer with a learning rate of 2e-5 is used. Early stopping in max mode with 7 epochs of patience ensures a high validation accuracy. The training batch size is set to 8. The T5 tokenizer is limited to a maximum length of 350 tokens. The CNN filter sizes are set to 1, 2, 3 to extract ngram features. Other parameters are initialized at random.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>According to the previous section, our first challenge was to find the suitable embedded contexts of the tokens through language models. Consequently, we first analyzed the quality of the various language models on the data, and after reviewing BERT, RoBERTa, and T5, we concluded that the T5-large model is the most accurate of these language models.</p><p>Table <ref type="table" coords="7,127.31,291.85,5.13,10.91">1</ref> shows the results of the various language models. The results of our tests led us to use the T5-Large language model in subsequent experiments with different features. Following the determination of the appropriate embedding layer, we had to examine properties that would allow us to determine the semantic and grammatical differences and similarities between the two texts. This information can prove to be extremely valuable to a model when determining the authorship of two texts. As mentioned, extracting context-based and semantic features can be very helpful in AV. To capture context and semantic features, we've used a T5-based embedding layer. Based on the structure and writing of an author, we used punctuation based on ngram, author-specific, and topic-specific information, and PoS to extract syntactic features. The ngram-based punctuation feature was calculated using CNN to ensure superior accuracy. The results of different experiments comparing the validation data and test data are shown in Table <ref type="table" coords="7,160.05,566.26,3.74,10.91">2</ref>.</p><p>As can be seen, the introduction of new features led to improvements over the original text. From the Table <ref type="table" coords="7,156.86,593.35,3.66,10.91">2</ref>, it is evident that incorporating punctuation, author-specific, and topic-specific information has increased the F1-score of the model from 82.46% to 87.52%. Following the addition of the PoS tags to extract the syntactic features used by the author, the F1-score of the model increased from 87.52% to 89.93%.</p><p>Finally, using the attention mechanism, we sought to extract the meaning of tokens and particular relationships between them, so that we could further use the meaning of the specific tokens as well as particular relationships within each author's text. Due to the high ability of the attention mechanism to find and prioritize important tokens, as well as to discern the relation between tokens, the F1-score of the model has increased from 89.93% to 91.72% on the test data. In order to evaluate the performance of our proposed model, we used the evaluation platform provided by PAN, which includes the following metrics:</p><p>• AUC: the conventional area under the curve score.</p><p>• c@1: rewards systems that leave complicated problems unanswered <ref type="bibr" coords="8,421.83,339.42,16.25,10.91" target="#b16">[17]</ref>. • F_0.5u: focus on deciding same-author cases correctly <ref type="bibr" coords="8,359.52,354.32,16.25,10.91" target="#b17">[18]</ref>. • F1-score: harmonic way of combining the precision, and recall of the model <ref type="bibr" coords="8,455.92,369.23,16.25,10.91" target="#b18">[19]</ref>.</p><p>• Brier: Brier Score evaluates the accuracy of probabilistic predictions <ref type="bibr" coords="8,421.93,384.13,16.25,10.91" target="#b19">[20]</ref>.</p><p>Based on the hidden test set, table <ref type="table" coords="8,255.93,406.64,5.14,10.91">3</ref> demonstrates the performance of the proposed model. This model was evaluated on the TIRA environment for PAN@CLEF 2022. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Our research proposes a model using semantic, grammatical, and stylometric (i.e. punctuation ngrams and topic-specific information) to predict sameness of two text excerpt author. The T5 language model is used to convert these features into representation vectors.</p><p>Our CNN neural network extracts ngram features from punctuation sequences. The attention module is also employed to extract the most important features and to determine the relationships between the tokens. We conducted many experiments to evaluate the performance of the proposed model with these newly introduced features. As demonstrated by the experimental results, combining semantic-based, grammatically-based, and writing style features with the proposed architecture provides a reasonable range of results for AV.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,111.28,393.33,10.91;9,112.28,124.83,393.70,10.91;9,112.28,138.38,394.00,10.91;9,112.66,151.93,195.00,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,435.83,124.83,70.15,10.91;9,112.28,138.38,161.25,10.91">Overview of the Authorship Verification Task at PAN</title>
		<author>
			<persName coords=""><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krzysztof</forename><surname>Kredens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Pezik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Annina</forename><surname>Heini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Janek</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,317.14,138.38,138.08,10.91">CLEF 2022 Labs and Workshops</title>
		<title level="s" coord="9,463.15,138.38,43.12,10.91;9,112.66,151.93,164.87,10.91">Notebook Papers, CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,165.48,393.33,10.91;9,112.33,179.03,393.65,10.91;9,112.66,192.57,81.94,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,321.82,165.48,184.17,10.91;9,112.33,179.03,53.79,10.91">Overview of the Style Change Detection Task at PAN</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zangerle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mayerl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,208.86,179.03,297.12,10.91;9,112.66,192.57,51.81,10.91">CLEF 2022 Labs and Workshops, Notebook Papers, CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,206.12,394.53,10.91;9,112.66,219.67,393.33,10.91;9,112.66,233.22,394.51,10.91;9,112.66,249.21,123.08,7.90" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,327.46,206.12,175.13,10.91">TIRA Integrated Research Architecture</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-22948-1_5</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,240.99,219.67,264.99,10.91;9,112.66,233.22,123.97,10.91">Information Retrieval Evaluation in a Changing World, The Information Retrieval Series</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,260.32,393.33,10.91;9,112.66,273.87,393.33,10.91;9,112.66,287.42,202.76,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,342.32,260.32,163.67,10.91;9,112.66,273.87,218.45,10.91">Explainable authorship verification in social media via attention-based similarity learning</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Boenninghoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hessler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kolossa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Nickel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,374.02,273.87,131.97,10.91;9,112.66,287.42,98.55,10.91">IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="36" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,300.97,394.53,10.91;9,112.66,314.52,393.59,10.91;9,112.66,328.07,146.44,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<title level="m" coord="9,112.66,314.52,358.56,10.91">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,341.62,393.33,10.91;9,112.39,355.17,393.59,10.91;9,112.28,368.71,345.52,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,351.85,341.62,154.13,10.91;9,112.39,355.17,122.31,10.91">Similarity learning for authorship verification in social media</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Boenninghoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kolossa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,263.25,355.17,242.73,10.91;9,112.28,368.71,220.67,10.91">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2457" to="2461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,382.26,393.33,10.91;9,112.66,395.81,244.37,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,406.52,382.26,99.47,10.91;9,112.66,395.81,88.82,10.91">Groningen lightweight authorship detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hürlimann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Weck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Van Den Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Suster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Glad</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,228.42,395.81,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,409.36,393.33,10.91;9,112.66,422.91,394.53,10.91;9,112.66,436.46,123.33,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,254.23,409.36,251.75,10.91;9,112.66,422.91,137.29,10.91">Lg4av: Combining language models and graph neural networks for author verification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Stubbemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Stumme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,271.13,422.91,231.61,10.91">International Symposium on Intelligent Data Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="315" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,450.01,393.33,10.91;9,112.66,463.56,332.85,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,322.60,450.01,183.39,10.91;9,112.66,463.56,62.58,10.91">Author verification using a graph-based representation</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Vilari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,183.63,463.56,212.01,10.91">International Journal of Computer Applications</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,477.11,393.71,10.91;9,112.66,490.66,393.33,10.91;9,112.33,504.21,179.99,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,498.63,477.11,7.73,10.91;9,112.66,490.66,191.97,10.91">A novel approach for authorship verification</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">Buddha</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Murali Mohan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Vamsi Krishna Raja</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Raghunadha</forename><surname>Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,329.59,490.66,176.40,10.91;9,112.33,504.21,49.03,10.91">Data Engineering and Communication Technology</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="441" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,517.76,393.53,10.91;9,112.66,531.30,393.32,10.91;9,112.66,544.85,347.13,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,341.51,517.76,164.67,10.91;9,112.66,531.30,98.38,10.91">Gated pos-level language model for authorship verification</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,233.43,531.30,272.55,10.91;9,112.66,544.85,249.50,10.91">Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4025" to="4031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,558.40,393.33,10.91;9,112.66,571.95,393.33,10.91;9,112.66,585.50,393.33,10.91;9,112.66,599.05,123.97,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,291.36,558.40,214.63,10.91;9,112.66,571.95,108.94,10.91">Authorship attribution with convolutional neural networks and pos-eliding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Rehbein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,243.87,571.95,262.12,10.91;9,112.66,585.50,22.05,10.91">Proceedings of the Workshop on Stylistic Variation (EMNLP 2017)</title>
		<meeting>the Workshop on Stylistic Variation (EMNLP 2017)<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09-08">September 8, 2017. 2018</date>
			<biblScope unit="page" from="53" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,612.60,393.33,10.91;9,112.28,626.15,173.20,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,166.57,612.60,281.71,10.91">Fake news detection using pos tagging and machine learning</title>
		<author>
			<persName coords=""><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,460.04,612.60,45.94,10.91;9,112.28,626.15,117.36,10.91">Journal of Applied Security Research</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,639.70,393.33,10.91;9,112.66,653.25,354.93,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,351.17,639.70,154.81,10.91;9,112.66,653.25,154.21,10.91">Lexical-syntactic and graph-based features for authorship verification</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Vilariño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>León</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Castillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,289.52,653.25,89.50,10.91">Proceedings of CLEF</title>
		<meeting>CLEF</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="282" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,666.80,393.33,10.91;10,112.66,86.97,253.14,10.91" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="9,228.64,666.80,277.34,10.91;10,112.66,86.97,122.59,10.91">Natural language processing with Python: analyzing text with the natural language toolkit</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>O&apos;Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,100.52,395.17,10.91;10,112.66,114.06,394.53,10.91;10,112.66,127.61,90.72,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,148.82,114.06,106.21,10.91">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,278.07,114.06,224.48,10.91">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,141.16,314.14,10.91" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
		<title level="m" coord="10,210.82,141.16,184.06,10.91">A simple measure to assess non-response</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,154.71,394.62,10.91;10,112.66,168.26,393.53,10.91;10,112.66,181.81,393.33,10.91;10,112.66,195.36,118.77,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,317.53,154.71,170.67,10.91">Generalizing unmasking for short texts</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,112.66,168.26,393.53,10.91;10,112.66,181.81,267.14,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="654" to="659" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="10,112.66,208.91,394.53,10.91;10,112.66,222.46,393.33,10.91;10,112.48,236.01,261.79,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,307.23,222.46,176.03,10.91">Scikit-learn: Machine learning in python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,492.05,222.46,13.94,10.91;10,112.48,236.01,167.70,10.91">the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,249.56,393.60,10.91;10,112.26,263.11,132.64,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,198.45,249.56,259.11,10.91">Verification of forecasts expressed in terms of probability</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">W</forename><surname>Brier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,467.12,249.56,39.15,10.91;10,112.26,263.11,69.00,10.91">Monthly weather review</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
