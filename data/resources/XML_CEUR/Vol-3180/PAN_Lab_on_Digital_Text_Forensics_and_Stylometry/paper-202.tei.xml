<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,360.04,15.42;1,89.29,106.66,300.98,15.42">Profiling Irony and Stereotype Spreaders with Language Models and Bayes&apos; Theorem</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,88.90,134.97,73.07,11.96"><forename type="first">Xinting</forename><surname>Huang</surname></persName>
							<email>wbn969@alumni.ku.dk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,360.04,15.42;1,89.29,106.66,300.98,15.42">Profiling Irony and Stereotype Spreaders with Language Models and Bayes&apos; Theorem</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">9D020FEEC7EEC4E262D58DEEA3402DAB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>author profiling</term>
					<term>irony detection</term>
					<term>language models</term>
					<term>Bayes&apos; theorem</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of profiling irony and stereotype spreaders is to classify authors as irony spreaders or not, based on a certain number of their tweets. In this paper, we present our novel system, which is different from typical approaches to classification tasks. Instead of extracting features and training machine learning classifiers, we exploit the Bayes' theorem and the property and function of language models to derive a classification system. We explain in detail why our system is able to make the right predictions, and also explore the characteristics of our new system by conducting further experiments. Finally, experimental results show that our approach can effectively classify ironic and non-ironic users and achieve a good performance on the shared task IROSTEREO.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Social media plays a more and more important role in our daily life. It has a lot of advantages for us, but it is also a hotbed of offensive and aggressive speech. It is therefore of great significance to profile users on social media. In the task Profiling Irony and Stereotype Spreaders on Twitter (IROSTEREO) 2022 <ref type="bibr" coords="1,175.20,420.23,11.27,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,189.19,420.23,7.51,10.91" target="#b1">2]</ref>, we focus on profiling ironic authors on Twitter, especially those who use irony to spread stereotypes. The goal is to classify users on Twitter as ironic or not, given a certain number of their tweets. This task is to deal with the subtlety and complexity of human language. When using irony, language can be used to mean something opposite to its literal meaning. Moreover, the stereotype can be well hidden, which requires sufficient knowledge of human society and a good understanding of the context to be detected. Thus this task is somewhat challenging.</p><p>In this paper, we present our solution for the IROSTEREO task. We propose a novel approach to classify the users with their tweets. Instead of extracting features and training a classifier to explicitly do classification on the data, we make use of the function of language models and apply Bayes' Theorem to make predictions. In this way, we avoid selecting and extracting features manually, which is quite necessary when doing classification with very long text inputs. Experimental results have shown that our approach is effective on the IROSTEREO task. It can classify users as ironic or not with high probability. Our approach achieves 92.78% accuracy on</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There is a lot of research in the field of author profiling. Many similar tasks are presented <ref type="bibr" coords="2,89.29,159.14,11.48,10.91" target="#b3">[4,</ref><ref type="bibr" coords="2,104.69,159.14,7.52,10.91" target="#b4">5,</ref><ref type="bibr" coords="2,116.13,159.14,7.52,10.91" target="#b5">6,</ref><ref type="bibr" coords="2,127.57,159.14,9.03,10.91" target="#b6">7]</ref> these years, with different focuses such as author gender profiling, bot author detection, fake news spreaders detection, and hate speech spreaders detection. While they focus on different aspects of authors, the general goals are to classify authors on social media, depending on the text content they write. In these tasks, both traditional machine learning models and deep learning models are exploited commonly. Besides, external resources like word embeddings <ref type="bibr" coords="2,145.93,226.89,12.89,10.91" target="#b7">[8]</ref> have been widely used to extract features from raw texts. Typical framework for these tasks can be summarized as 1) extract features from the text, which includes word ngrams, TF-IDF features, word embeddings, sentence embeddings, representation from large-scale pretrained language models, and sometimes linguistic features like POS tagging <ref type="bibr" coords="2,451.46,267.54,11.44,10.91" target="#b7">[8,</ref><ref type="bibr" coords="2,465.62,267.54,7.51,10.91" target="#b8">9,</ref><ref type="bibr" coords="2,475.85,267.54,12.58,10.91" target="#b9">10,</ref><ref type="bibr" coords="2,491.15,267.54,12.39,10.91" target="#b10">11]</ref>.</p><p>2) these features are then fed to classifiers like SVM, BiLSTM, or pretrained language models <ref type="bibr" coords="2,89.29,294.63,16.56,10.91" target="#b11">[12,</ref><ref type="bibr" coords="2,108.86,294.63,12.59,10.91" target="#b12">13,</ref><ref type="bibr" coords="2,124.46,294.63,14.11,10.91" target="#b13">14]</ref> in order to classify the authors. In these works, there are usually many kinds of features and classifiers selected, and many different combinations of features and classifiers are tested to select the best system. While this framework usually requires arduous work, the resulting system can usually achieve very good performance on the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework</head><p>First of all, let 𝑋 be the tweets of a user, and let 𝑦 be the label of the user. We intend to predict the label of a user, given his/her tweet data. In other words, we try to estimate the probability distribution 𝑃 (𝑦|𝑋). Instead of directly building up a model to do this task, we apply Bayes' theorem to the target distribution.</p><formula xml:id="formula_0" coords="2,243.33,480.03,262.66,10.91">𝑃 (𝑦|𝑋) ∝ 𝑃 (𝑋|𝑦)𝑃 (𝑦)<label>(1)</label></formula><p>where the target distribution 𝑃 (𝑦|𝑋) (posterior) is proportional to likelihood 𝑃 (𝑋|𝑦) times prior 𝑃 (𝑦). In this way, we convert the desired probability into two components. Once we know 𝑃 (𝑋|𝑦) and 𝑃 (𝑦), we will know 𝑃 (𝑦|𝑋) and thus which label should be assigned to the given tweets.</p><p>The prior 𝑃 (𝑦) is the probability distribution of labels in the dataset, which can be easily obtained by counting the number of a specific label 𝑛 𝑖 and counting the number of labels in total 𝑛, then 𝑃 (𝑦 𝑖 ) = 𝑛 𝑖 /𝑛.</p><p>The likelihood 𝑃 (𝑋|𝑦) means that given the label (e.g., the tweets are produced by an irony and stereotype spreader), how likely are these tweets to appear. We can use language models to estimate this probability. The language models are able to predict the probability of any given sequence of words.</p><p>During the training stage, we separate the training data according to the labels, more specifically, one collection of data consists of all the ironic (we will use this word as a shorthand for tweets produced by irony and stereotype spreaders) tweets, another collection consists of all the non-ironic tweets. Namely, 𝑆 𝐼 = {𝑋 𝑖 |𝑦 𝑖 = "I"}, and 𝑆 𝑁 𝐼 = {𝑋 𝑖 |𝑦 𝑖 = "NI"}, where "I" denotes ironic, and "NI" denotes non-ironic. Then we train two separate language models on the two collections of data respectively, which brings about ironic domain language model 𝐿𝑀 𝐼 and non-ironic domain language model 𝐿𝑀 𝑁 𝐼 . Since the language model 𝐿𝑀 𝐼 is trained on all the ironic tweets, it can predict the probability of a given sequence of words appearing in the ironic domain. In other words, 𝐿𝑀 𝐼 can estimate the probability 𝑃 (𝑋|𝑦 = "I"), that is, given that the writer is an ironic user, how likely is the sequence to appear. Similarly, 𝐿𝑀 𝑁 𝐼 can estimate the probability 𝑃 (𝑋|𝑦 = "NI"). To make it clear, in the training stage, we do not explicitly predict the labels. We only use the labels to divide all the tweets into two collections.</p><p>When using our models to predict the label of a given set of tweets, we follow the framework of Bayes' theorem specified above. Firstly, the tweets 𝑋 are fed into 𝐿𝑀 𝐼 and 𝐿𝑀 𝑁 𝐼 , giving rise to 𝑃 (𝑋|𝑦 = "I") and 𝑃 (𝑋|𝑦 = "NI") respectively. Then the likelihood is multiplied by 𝑃 (𝑦) which can be computed as the frequency of each label in training data, producing 𝑃 (𝑋|𝑦 = "I")𝑃 (𝑦 = "I") and 𝑃 (𝑋|𝑦 = "NI")𝑃 (𝑦 = "NI"). By comparing these two, we choose the label that produces the greater posterior as our prediction. Note that in the training data, the labels are evenly distributed. The number of ironic users is equal to the number of non-ironic users, namely 𝑃 (𝑦 = "I") = 𝑃 (𝑦 = "NI"). Thus we can further remove the term 𝑃 (𝑦) and compare only the 𝑃 (𝑋|𝑦).</p><p>For example, given a sentence "I love COVID!", which is pretty ironic, the probability of that occurring in the ironic domain is relatively larger than it occurring in the non-ironic domain. It is not likely for a normal user to say so, while it is likely that an ironic spreader would say some sentences like this. Thus for 𝑋 = "I love COVID!", 𝑃 (𝑋|𝑦 = "I") &gt; 𝑃 (𝑋|𝑦 = "NI") if the two language models work effectively. We can then predict that the corresponding label is "I", ironic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Language Models</head><p>As for the language model, one of our choices is transformer-based neural language models. Specifically, we use mainly use the causal language models, GPT2 <ref type="bibr" coords="3,414.55,461.87,18.07,10.91" target="#b14">[15]</ref> and DistilGPT2 <ref type="bibr" coords="3,89.29,475.42,16.41,10.91" target="#b15">[16]</ref>. Given a sequence of tokens, the models are able to predict the next token based on the previous tokens. In other words, the models can estimate the probability of each token 𝑃 (𝑤 𝑖 |𝑤 1 , 𝑤 2 , ..., 𝑤 𝑖-1 ), where (𝑤 1 , 𝑤 2 , ..., 𝑤 𝑖-1 , 𝑤 𝑖 , 𝑤 𝑖+1 , ..., 𝑤 𝑛 ) is a sequence of tokens. The probability of a whole sequence can be obtained as</p><formula xml:id="formula_1" coords="3,191.92,537.86,314.06,33.71">𝑃 (𝑤 1 , 𝑤 2 , ..., 𝑤 𝑛 ) = 𝑛 ∏︁ 𝑖=1 𝑃 (𝑤 𝑖 |𝑤 1 , 𝑤 2 , ..., 𝑤 𝑖-1 )<label>(2)</label></formula><p>In the task IROSTEREO, we need to predict the label for a given set of tweets generated by one user (200 tweets for each user). To estimate the probability of all the tweets 𝑃 (tweets), we do it in two alternative ways: 1) 𝑃 (tweets) = ∏︀ 𝑁 𝑘=1 𝑃 (tweet 𝑘 ), where 𝑃 (tweet 𝑘 ) is the probability of a single tweet, which is computed using equation 2. In this way, the tweets are considered independent. 2) Concatenate all the tweets as a single sequence, then divide it into pieces whose length is equal to the maximum input length of the language model. Since the maximum input length is quite long (e.g., 1024 tokens for DistilGPT2), one piece typically contains multiple tweets. The probability is computed as 𝑃 (tweets) = ∏︀ 𝑁 𝑗=1 𝑃 (piece 𝑗 ). Thus in this way, the language model is able to attend much longer context on average when estimating the probability for each token. In the following part, we refer to our approach as BayLMs, which contains two language models. We refer the approach using 1) to train and predict as BayLMs 𝑠𝑒𝑝 , the one using 2) as BayLMs 𝑐𝑎𝑡 .</p><p>Another kind of important language model we use is the n-gram language model. The main idea of the n-gram language model is that instead of computing the probability of the word (now we use "word" instead of "token" because words are the basic units in this case) based on its whole history (i.e. 𝑃 (𝑤 𝑖 |𝑤 1 , 𝑤 2 , ..., 𝑤 𝑖-1 )), it approximates the history with a few words before (i.e., 𝑃 (𝑤 𝑖 |𝑤 𝑖-2 , 𝑤 𝑖-1 ) in the case of trigram). In other words, the n-gram language model assumes the probability of a word only depends on its previous n-1 words. Therefore, an n-gram language model estimates the probability of a sequence as</p><formula xml:id="formula_2" coords="4,188.80,255.16,317.18,33.71">𝑃 (𝑤 1 , 𝑤 2 , ..., 𝑤 𝑛 ) = 𝑛 ∏︁ 𝑖=1 𝑃 (𝑤 𝑖 |𝑤 𝑖-𝑁 +1 , ..., 𝑤 𝑖-1 )<label>(3)</label></formula><p>where N denotes N-gram.</p><p>To compute the probability of all the tweets written by a single user, we simply concatenate all the tweets as a single sequence, then apply the n-gram language model as 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Further Analysis</head><p>In sum, we do not train the models to explicitly do classification. We train two separate language models with two different corpora. With their ability to estimate the probability of any given sequence, the predictions can be derived following Bayes' theorem.</p><p>By this approach, we avoid manually selecting and extracting features. Otherwise, due to the long length of input (200 tweets) of each instance, we have to carefully extract features to reduce the length of the input, which is tricky and is possible to lose information. But this framework is only suitable when there are just a few classes ("I" and "NI" in our case), since for each class we have to train a language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data</head><p>Since we only have the ground truth included in the training data, we evaluate our approach using training data. Because of the small number of instances in the dataset, we need to evaluate multiple times to get a reliable result and to reduce randomness. Therefore, we use 5-fold cross validation with the training data. More specifically, there are 420 instances in training data with the equal number of ironic and non-ironic instances, thus each fold includes 84 instances. Note that we also keep the ironic and non-ironic instances equal in each fold, thus in each time of training and validation, the number of ironic and non-ironic instances are always equal.</p><p>To better compare the performance of the different schemes, we keep the 5 folds always the same. That is, we first randomly divide training data into 5 folds, then we always use these 5 folds to test different schemes and never re-divide again. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation</head><p>Our neural language models are implemented using HuggingFace Transformers <ref type="bibr" coords="5,458.03,317.50,16.41,10.91" target="#b16">[17]</ref>. They are pretrained on large-scale text corpus beforehand and do causal language modeling during our training process (same task as they did in pretraining stage). They are optimized using stochastic gradient descent with AdamW <ref type="bibr" coords="5,273.80,358.14,17.85,10.91" target="#b17">[18]</ref> as the optimizer, with weight decay of 0.01. The language models are trained for just one epoch with a constant learning rate. The batch size and learning rate are adjusted to suit each scheme respectively. The batch size and learning rate for BayLMs 𝑠𝑒𝑝 are 8 and 1e-4. The batch size and learning rate for BayLMs 𝑐𝑎𝑡 are 4 and 2e-5.</p><p>As for n-gram language models, we apply add-k smoothing (Laplace smoothing when k=1). We also set the vocabulary of 𝐿𝑀 𝐼 and 𝐿𝑀 𝑁 𝐼 as the union of the vocabulary of ironic tweets and the vocabulary of non-ironic tweets in order to make the comparison between 𝐿𝑀 𝐼 and 𝐿𝑀 𝑁 𝐼 fair when making the prediction. We conduct experiments with unigram, bigram, trigram, and 4-gram language models. For each language model, we select the best k value ranging from 0.01 to 10.0 for add-k smoothing according to the accuracy of cross validation. The selected k for each models are: 0.01 (unigram), 3.0 (bigram), 0.1 (trigram), 0.01 (4-gram).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>The results are presented in table 1. In each cell, the number is the mean accuracy in the cross-validation procedure. The number in the bracket is 2 times the standard deviation of the accuracy values, which covers a range that most of the accuracy values would fall in theoretically, assuming the values are normally distributed. We can see that the accuracy varies wildly in cross validation. This is because of the small number of instances in training data, which results in only 84 instances each time of validation. In the table, GPT2 represents using pretrained GPT2 as the language models, DistilGPT2 means using pretrained DistilGPT2 as language models, and n-gram LM means using n-gram language models.</p><p>First of all, we can see that even though the models are not trained under classification tasks, they are able to distinguish between ironic spreaders and normal users effectively. With high probability, our approach can make accurate predictions.</p><p>The results also show that our framework is able to work with entirely different language models, since both BayLMs with GPT2/DistilGPT2 and BayLMs with n-gram language models achieve good performance.</p><p>It is interesting that n-gram language models achieve comparable results with large-scale pretrained language models, even though they are quite simple, do not contain any trainable parameters, and do not require any optimization. They achieve good results simply by counting frequencies in the training data, while neural network-based language models have millions of parameters and require costly optimization.</p><p>As for neural network language models, we can see that using DistilGPT2 is slightly better than using GPT2 in general, even though DistilGPT2 has fewer parameters. BayLMs 𝑠𝑒𝑝 is slightly better than BayLMs 𝑐𝑎𝑡 when they both use GPT2, while BayLMs 𝑠𝑒𝑝 and BayLMs 𝑐𝑎𝑡 has similar performance when using DistilGPT2. In general, even though neural network language models perform slightly better than n-gram language models, the performance under different setting are similar, which means different setting does not matter significantly. Our final result on the test set of the IROSTEREO task is 92.78%, which is obtained by training BayLMs 𝑐𝑎𝑡 with DistilGPT2 on the whole training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Other Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Data</head><p>There are also some interesting results in our experiments that are noteworthy. In this section, our experiments are conducted using another way to divide training data. We randomly divide the training instances into two sets, the training set which consists of 320 users, and the valid set which consists of 100 users. Again, the ironic and non-ironic instances are equal in both sets. As we mentioned before, we also use the same training and valid sets throughout this section and do not re-divide again. But we do not do cross validation anymore, so the experiments are much less time-consuming. In this setting, the valid set contains only 100 instances and we will only validate once, so the accuracy is not as reliable as the one in table 1. But it is still enough to reflect a change on real accuracy when there is a drastic change occurred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">The Effect of Pretraining</head><p>In our approach, we compare the probability of the given tweets produced by two language models. The prediction of our approach depends on the relative magnitude of probability. 𝑃 (𝑋|𝑦 = "I") and 𝑃 (𝑋|𝑦 = "NI") can both increase or decrease, as long as relative relationship (i.e., 𝑃 (𝑋|𝑦 = "I") &gt; 𝑃 (𝑋|𝑦 = "NI")) does not change, the prediction will not change.</p><p>In this experiment, we use pretrained and un-pretrained DistilGPT2 as our language models. Un-pretrained DistilGPT2 means that the parameters of the language model are randomly initialized, and the language models are then trained from scratch using our training data. While the pretrained DistilGPT2 language models have the parameters that are sufficiently optimized on a large-scale corpus. We use BayLMs 𝑠𝑒𝑝 . As for hyper-parameters, we use batch size of 32, a constant learning rate of 1e-4 and weight decay of 0.01, and train language models Accuracy % Loss of 𝐿𝑀 𝐼 Loss of 𝐿𝑀 𝑁 𝐼 BayLMs 𝑠𝑒𝑝 with un-pretrained DistilGPT2 91 5.17 5.66 BayLMs 𝑠𝑒𝑝 with pretrained DistilGPT2 91 3.73 4.00</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Compare the results of our approach using pretrained and un-pretrained language model. The accuracy is the performance on the valid set. The loss of 𝐿𝑀 𝐼 is the final training loss of the 𝐿𝑀 𝐼 on ironic tweets collection. While loss of 𝐿𝑀 𝑁 𝐼 is the final training loss of the 𝐿𝑀 𝑁 𝐼 on non-ironic tweets collection.</p><p>for one epoch. The results are shown in table <ref type="table" coords="7,290.47,201.28,3.68,10.91">2</ref>. Note that the accuracy is obtained on the valid set. The loss values are the training loss of the two language models on the two training corpus respectively.</p><p>From table <ref type="table" coords="7,153.24,241.93,5.17,10.91">2</ref> we can see that using un-pretrained language model can even achieve the comparable accuracy as using pretrained language model. While pretraining does affect the model performance on language modeling tasks, as the training loss is much larger when the model is not pretrained, pretraining does not affect the performance of our BayLMs to a significant extent. This unexpected result is because of the intuition we mentioned above. Without pretraining, the language models are worse at estimating the probability of a given sequence, in other words, fail to assign a high probability to the occurring sequence. Without pretraining, both 𝑃 (𝑋|𝑦 = "I") and 𝑃 (𝑋|𝑦 = "NI") are likely to decrease, while the relative relationship is not likely to change.</p><p>Note that in other parts of this paper, we use pretrained language models in our BayLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Masked Language Models</head><p>It is also possible to use masked language models in BayLMs, (e.g. BERT <ref type="bibr" coords="7,420.27,413.60,15.89,10.91" target="#b13">[14]</ref>). Even though we cannot estimate strictly the probability of the whole tweets, we can do it in an analogous way to causal language models. During training, the masked language models are trained to predict the masked token in the sequence. When we use them in BayLMs to make predictions, we randomly mask tokens in the input tweets, then the identical masked inputs are fed into the two masked language models, giving rise to probabilities on the masked tokens, which are then multiplied together to produce 𝑃 (𝑋|𝑦 = "I") and 𝑃 (𝑋|𝑦 = "NI"). Specifically, we use pretrained DistilRoBERTa-base <ref type="bibr" coords="7,328.12,508.45,17.86,10.91" target="#b15">[16]</ref> and BERTweet-base <ref type="bibr" coords="7,439.37,508.45,16.20,10.91" target="#b18">[19]</ref>. The latter is pretrained on a large-scale tweet corpus, and has special tokens for emojis, user mentions, and url links. Thus it has a strong ability to understand tweets. We also use BayLMs 𝑠𝑒𝑝 as framework. As for hyper-parameters, both DistilRoBERTa and BERTweet are trained with batch size=32, constant learning rate=2e-5, weight decay=0.01 for one epoch. The masking rate for random masking on inputs is 0.15 during training and evaluation. The results are shown in table <ref type="table" coords="7,113.59,589.74,3.74,10.91">3</ref>.</p><p>We can see that the DistilRoBERTa and BERTweet give comparable accuracy. But the performance of both models is noticeably worse than BayLMs with causal language models. It may be because they fail to estimate the probability of all the tokens and finally the probability of the whole input tweets.</p><p>Accuracy BayLMs 𝑠𝑒𝑝 with DistilRoBERTa-base 76 BayLMs 𝑠𝑒𝑝 with BERTweet-base 79</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>The results when using masked language models in our approach. The accuracy is the performance on valid set.</p><p>Accuracy % Loss of 𝐿𝑀 𝐼 Loss of 𝐿𝑀 𝑁 𝐼 BayLMs 𝑠𝑒𝑝 trained for 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Effect of Training Epochs</head><p>In most cases, training only for one epoch is considered to be insufficient. But our approach is different from the typical method, the situation can be a little bit different. We use the BayLMs 𝑠𝑒𝑝 with pretrained DistilGPT2, and train language models with exactly the same hyper-parameters (batch size=8, constant learning rate=1e-4, weight decay=0.01) but for the different number of epochs, ranging from 1 to 6. The results are shown in table <ref type="table" coords="8,354.16,393.40,3.74,10.91" target="#tab_1">4</ref>.</p><p>As we can see, when the number of epochs exceeds 1, the accuracy starts to drop. BayLMs 𝑠𝑒𝑝 trained for 6 epochs performs significantly worse than BayLMs 𝑠𝑒𝑝 trained for 1 epoch. Typically, machine learning models are trained for much more epochs to achieve the best results, and models' performance starts to decline only after training for many epochs, and it usually declines mildly. While the decline in other machine learning models can be attributed to over-fitting, the reason for the decline in our case can not be explained as over-fitting, since the models do not do the same task in the evaluation as in training. In our approach, it is necessary to train the model, since the 𝑃 (𝑋|𝑦 = "I") and 𝑃 (𝑋|𝑦 = "NI") will always be the same if the language models are not trained. But the accuracy will not necessarily increase with more training. Like we mentioned in section 5.2, the prediction accuracy depends on the relative relationship of 𝑃 (𝑋|𝑦 = "I") and 𝑃 (𝑋|𝑦 = "NI"), instead of the absolute magnitude of probabilities. It is also noteworthy that our system has a strong bias towards the class "NI" when trained for too many epochs. In the case of 6 epochs, there are 38 out of 39 wrong predictions are "NI" (corresponding ground truths are "I"). While in the case of 1 epoch, there are 5 wrong predictions are "NI" and 3 are "I".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>This paper has shown a novel approach when dealing with classification tasks. We present a simple scheme, which only makes use of language models and trains with the language modeling task. The proposed system is simple to implement while it can achieve good performance on the task.</p><p>In theory, our approach is compatible with any kind of language model, as long as the language model is able to give the probability of a given sequence. This point is well demonstrated by our experimental results. Thus, we can further test with other language models in future work, including different types of neural network language models and other traditional ones. In addition, regarding n-gram language models there are some sophisticated smoothing methods that are usually believed to achieve better results than simple add-k smoothing. So we can also do some experiments with them.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,86.42,418.09,183.81"><head>Table 1</head><label>1</label><figDesc>The performance on 5-fold cross validation using training data. The number in each cell of the table is the mean accuracy. The numbers in the brackets are 2 times the standard deviation of the accuracy values in cross validation, which covers a range that most of accuracy values would fall in in theory, assuming the values are normally distributed. Note that due to the small number of instances in training data, there are only 84 instances in each time of validation, thus the accuracy varies wildly.</figDesc><table coords="5,208.48,86.42,178.33,106.21"><row><cell>Scheme</cell><cell>Accuracy (%)</cell></row><row><cell>BayLMs 𝑠𝑒𝑝 with GPT2</cell><cell>91.0 (±3.6)</cell></row><row><cell>BayLMs 𝑐𝑎𝑡 with GPT2</cell><cell>88.6 (±5.8)</cell></row><row><cell>BayLMs 𝑠𝑒𝑝 with DistilGPT2</cell><cell>91.7 (±4.5)</cell></row><row><cell>BayLMs 𝑐𝑎𝑡 with DistilGPT2</cell><cell>91.2 (±3.6)</cell></row><row><cell>BayLMs with unigram LM</cell><cell>88.3 (±3.5)</cell></row><row><cell>BayLMs with bigram LM</cell><cell>90.0 (±6.8)</cell></row><row><cell>BayLMs with trigram LM</cell><cell>91.2 (±3.6)</cell></row><row><cell>BayLMs with 4-gram LM</cell><cell>88.1 (±4.5)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,88.99,190.95,416.99,101.51"><head>Table 4</head><label>4</label><figDesc>Compare the results of our approach with different training epochs. The accuracy is the performance on the valid set. The loss of 𝐿𝑀 𝐼 is the final training loss of the 𝐿𝑀 𝐼 on ironic tweets collection. While loss of 𝐿𝑀 𝑁 𝐼 is the final training loss of the 𝐿𝑀 𝑁 𝐼 on non-ironic tweets collection.</figDesc><table coords="8,137.51,190.95,302.13,48.52"><row><cell>5 epoch</cell><cell>87</cell><cell>3.80</cell><cell>4.11</cell></row><row><cell>BayLMs 𝑠𝑒𝑝 trained for 1 epoch</cell><cell>92</cell><cell>3.79</cell><cell>4.03</cell></row><row><cell>BayLMs 𝑠𝑒𝑝 trained for 3 epoch</cell><cell>86</cell><cell>3.24</cell><cell>3.45</cell></row><row><cell>BayLMs 𝑠𝑒𝑝 trained for 6 epoch</cell><cell>61</cell><cell>2.63</cell><cell>2.82</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,253.99,393.33,10.91;9,112.66,267.54,394.53,10.91;9,112.66,281.08,172.05,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,373.25,253.99,132.74,10.91;9,112.66,267.54,219.45,10.91">Profiling Irony and Stereotype Spreaders on Twitter (IROSTEREO) at PAN 2022</title>
		<author>
			<persName coords=""><forename type="first">O.-B</forename><surname>Reynier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Berta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Elisabetta</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="9,358.04,267.54,143.79,10.91">CLEF 2022 Labs and Workshops</title>
		<title level="s" coord="9,112.66,281.08,103.05,10.91">Notebook Papers, CEUR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,294.63,394.53,10.91;9,112.66,308.18,395.17,10.91;9,112.66,321.73,393.32,10.91;9,112.66,335.28,395.01,10.91;9,112.66,348.83,393.33,10.91;9,112.66,362.38,393.32,10.91;9,112.66,375.93,393.33,10.91;9,112.66,389.48,184.76,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,251.61,321.73,254.37,10.91;9,112.66,335.28,259.82,10.91">Overview of PAN 2022: Authorship Verification, Profiling Irony and Stereotype Spreaders, and Style Change Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chulvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Heini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kredens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mayerl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ortega-Bueno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pezik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wolska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zangerle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,446.63,348.83,59.35,10.91;9,112.66,362.38,393.32,10.91;9,112.66,375.93,253.92,10.91">Proceedings of the Thirteenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="9,474.16,376.94,31.82,9.72;9,112.66,390.49,112.43,9.72">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">D E F S C M G P A H M P G F N F</forename><surname>Alberto Barron-Cedeno</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</editor>
		<editor>
			<persName><surname>Martino</surname></persName>
		</editor>
		<meeting>the Thirteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="volume">13390</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="9,112.66,403.03,394.53,10.91;9,112.66,416.58,393.33,10.91;9,112.66,430.13,394.51,10.91;9,112.66,446.12,123.08,7.90" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,327.46,403.03,175.13,10.91">TIRA Integrated Research Architecture</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-22948-1_5</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,240.99,416.58,264.99,10.91;9,112.66,430.13,123.97,10.91">Information Retrieval Evaluation in a Changing World, The Information Retrieval Series</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,457.22,393.53,10.91;9,112.66,470.77,393.33,10.91;9,112.66,484.32,141.09,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,387.01,457.22,119.18,10.91;9,112.66,470.77,315.56,10.91">Overview of the 6th author profiling task at pan 2018: multimodal gender identification in twitter</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,437.06,470.77,68.92,10.91;9,112.66,484.32,85.24,10.91">Working Notes Papers of the CLEF</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,497.87,393.53,10.91;9,112.66,511.42,393.33,10.91;9,112.66,524.97,137.73,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,199.19,497.87,307.00,10.91;9,112.66,511.42,81.11,10.91">Overview of the 7th author profiling task at pan 2019: bots and gender profiling in twitter</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,216.39,511.42,252.87,10.91;9,147.28,524.97,72.33,10.91">Working Notes Papers of the CLEF 2019 Evaluation Labs</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
	<note>CEUR Workshop</note>
</biblStruct>

<biblStruct coords="9,112.66,538.52,393.33,10.91;9,112.66,552.07,394.53,10.91;9,112.39,565.62,243.38,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,345.21,538.52,160.78,10.91;9,112.66,552.07,243.15,10.91">Overview of the 8th author profiling task at pan 2020: Profiling fake news spreaders on twitter</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">H H</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,376.55,552.07,126.02,10.91">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Sun SITE Central Europe</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2696</biblScope>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,579.17,393.33,10.91;9,112.66,592.72,384.17,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,412.24,579.17,93.74,10.91;9,112.66,592.72,160.61,10.91">Profiling hate speech spreaders on twitter task at pan 2021</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L</forename><surname>De La Peña Sarracén</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chulvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,300.19,592.72,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1772" to="1789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,606.27,393.33,10.91;9,112.66,619.81,397.48,10.91;9,112.66,635.81,26.14,7.90" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="9,304.78,606.27,201.20,10.91;9,112.66,619.81,64.26,10.91">Efficient estimation of word representations in vector space</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1301.3781</idno>
		<ptr target="https://arxiv.org/abs/1301.3781.doi:10.48550/ARXIV.1301.3781" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,646.91,394.61,10.91;10,112.66,86.97,393.33,10.91;10,112.33,100.52,136.87,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,290.62,646.91,197.56,10.91">Glove: Global vectors for word representation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,112.66,86.97,393.33,10.91;10,112.33,100.52,37.99,10.91">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,114.06,395.17,10.91;10,112.66,127.61,162.40,10.91;10,290.38,127.61,132.24,10.91;10,437.93,127.61,68.05,10.91;10,112.66,141.16,107.17,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>-Y. Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11175</idno>
		<title level="m" coord="10,290.38,127.61,127.49,10.91">Universal sentence encoder</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,154.71,393.33,10.91;10,112.66,168.26,397.48,10.91;10,112.36,184.25,132.96,7.90" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1802.05365</idno>
		<ptr target="https://arxiv.org/abs/1802.05365.doi:10.48550/ARXIV.1802.05365" />
		<title level="m" coord="10,482.41,154.71,23.58,10.91;10,112.66,168.26,159.70,10.91">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,195.36,375.65,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,208.31,195.36,109.06,10.91">Support-vector networks</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,326.19,195.36,78.19,10.91">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,208.91,393.98,10.91;10,112.41,222.46,48.96,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,253.72,208.91,112.17,10.91">Long short-term memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,376.65,208.91,91.46,10.91">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,236.01,393.33,10.91;10,112.66,249.56,363.59,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="10,353.43,236.01,152.55,10.91;10,112.66,249.56,181.08,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,263.11,393.33,10.91;10,112.66,276.66,253.81,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,412.10,263.11,93.89,10.91;10,112.66,276.66,141.16,10.91">Language models are unsupervised multitask learners</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,262.00,276.66,56.95,10.91">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,290.20,394.53,10.91;10,112.66,303.75,394.51,10.91;10,112.36,319.74,97.35,7.90" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1910.01108</idno>
		<ptr target="https://arxiv.org/abs/1910.01108.doi:10.48550/ARXIV.1910.01108" />
		<title level="m" coord="10,302.07,290.20,205.12,10.91;10,112.66,303.75,116.34,10.91">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,330.85,394.53,10.91;10,112.66,344.40,394.53,10.91;10,112.66,357.95,393.33,10.91;10,112.66,371.50,394.51,10.91;10,112.36,387.49,97.35,7.90" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="10,309.90,357.95,196.09,10.91;10,112.66,371.50,122.25,10.91">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1910.03771</idno>
		<ptr target="https://arxiv.org/abs/1910.03771.doi:10.48550/ARXIV.1910.03771" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,398.60,395.00,10.91;10,112.66,412.15,257.17,10.91" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="10,219.02,398.60,173.81,10.91">Decoupled weight decay regularization</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1711.05101</idno>
		<ptr target="https://arxiv.org/abs/1711.05101.doi:10.48550/ARXIV.1711.05101" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,425.70,393.32,10.91;10,112.33,439.25,393.65,10.91;10,112.66,452.79,230.27,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,270.60,425.70,235.39,10.91;10,112.33,439.25,28.53,10.91">BERTweet: A pre-trained language model for English Tweets</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,162.88,439.25,343.10,10.91;10,112.66,452.79,157.27,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
