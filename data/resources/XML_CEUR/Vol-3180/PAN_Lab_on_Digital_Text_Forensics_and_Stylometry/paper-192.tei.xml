<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.02,75.44,425.40,17.04;1,72.02,96.20,364.25,17.04;1,72.02,115.96,148.90,9.94">A Multi-Model Voting Ensemble Classifier based on BERT for Profiling Irony and Stereotype Spreaders on Twitter Notebook for PAN at CLEF 2022</title>
				<funder ref="#_uPY8E3j">
					<orgName type="full">Natural Science Foundation of Guangdong Province, China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.02,141.58,52.97,10.80"><forename type="first">Haojie</forename><surname>Cao</surname></persName>
							<email>caohaojie0322@163.com</email>
						</author>
						<author>
							<persName coords="1,135.02,141.58,76.42,10.80"><forename type="first">Zhongyuan</forename><surname>Han</surname></persName>
							<email>hanzhongyuan@gmail.com</email>
						</author>
						<author>
							<persName coords="1,227.45,141.58,59.31,10.80"><forename type="first">Zhenwei</forename><surname>Mo</surname></persName>
							<email>mozhenwei45@163.com</email>
						</author>
						<author>
							<persName coords="1,297.91,141.58,55.34,10.80"><forename type="first">Zengyao</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName coords="1,362.23,141.58,53.33,10.80"><forename type="first">Ziwei</forename><surname>Xiao</surname></persName>
						</author>
						<author>
							<persName coords="1,425.38,141.58,43.42,10.80"><forename type="first">Zijian</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName coords="1,495.10,141.58,27.80,10.80;1,72.02,155.38,26.66,10.80"><forename type="first">Leilei</forename><surname>Kong</surname></persName>
							<email>kongleilei@fosu.edu.cn@mail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Foshan University</orgName>
								<address>
									<settlement>Foshan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">CLEF</orgName>
								<address>
									<postCode>2022</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.02,75.44,425.40,17.04;1,72.02,96.20,364.25,17.04;1,72.02,115.96,148.90,9.94">A Multi-Model Voting Ensemble Classifier based on BERT for Profiling Irony and Stereotype Spreaders on Twitter Notebook for PAN at CLEF 2022</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B41C3001C76A173750EC2D74AA56BA87</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Irony and Stereotype Spreader</term>
					<term>BERT</term>
					<term>Multi-Model Voting</term>
					<term>Classifier 2.1. Model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For the Profiling Irony and Stereotype Spreaders on Twitter (IROSTEREO) task, the goal is to determine whether its author spreads irony and stereotypes through a Twitter feed in English. Profiling irony and stereotype spreaders on Twitter is regarded as a classification task by us. This paper presents our classifier based on BERT and Multi-Model Voting. On top of training our model with BERT, we use multi-model voting and dynamically adjust two hyperparameters to improve the accuracy of our model. Finally, the accuracy of our classifier on test data is 0.9389.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Irony is a way in which language expresses the opposite of its literal meaning metaphorically and subtly. Someone sometimes uses it to mock or scorn a victim, which puts him or her at risk of being hurt. Stereotypes are often used when discussing controversial issues. At PAN'22 <ref type="bibr" coords="1,427.24,422.98,11.69,10.04" target="#b0">[1]</ref>, The task Profiling Irony and Stereotype Spreaders on Twitter (IROSTEREO) <ref type="bibr" coords="1,328.85,435.69,12.78,9.94" target="#b1">[2]</ref> was set. The goal is to determine whether its author spreads irony and stereotypes through a Twitter feed in English.</p><p>We propose a multi-model voting ensemble classifier based on BERT to solve this task. We trained three models with BERT and used three models to predict and vote whether an author is an irony and stereotype spreader. Suppose the number of votes obtained by an author is greater than or equal to 2. In that case, we consider an author an irony and stereotype spreader, and vice versa, we consider an author not to be an irony and stereotype spreader.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>We abbreviate irony and stereotype spreader to Irony. This part describes our classifier for the Profiling Irony and Stereotype Spreaders on Twitter (IROSTEREO) task. In section 2.1. the entire framework is described. Section 2.2. introduces the Data Processing. And in section 2.3. the input and training are described. Section 2.4. Classifier and Output describes how we tune our classifier to improve the accuracy of our Classifier for Stereotype Spreaders.</p><p>Our team uses BERT and multi-model voting to approach the task, and in Figure <ref type="figure" coords="2,458.28,74.44,4.14,9.94" target="#fig_0">1</ref>, we give the architecture of the whole model. We divide the data into two parts, one for PartA and the other for PartB, and then feed the data from PartA and PartB into BERT for training to obtain BERT modelA and BERT modelB. In addition, we feed all the data into BERT together to obtain BERT modelC. Finally, we use the three models to vote on the data to determine whether an author is an Irony and Stereotype Spreader. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Data Processing</head><p>The dataset <ref type="bibr" coords="2,143.49,390.32,12.78,10.04" target="#b2">[3]</ref> was provided by PAN'22, containing 420 authors. In the dataset, each author corresponds to an XML file that stores the 200 tweets of the author. Of the 420 authors, half were irony and stereotype spreaders, while the other half were not.</p><p>In the dataset provided by PAN'22, each author's tweets are stored in an XML file. We remove tags like #USER from the tweets to ensure accuracy.</p><p>If an author is labeled as Irony, we assume that every tweet this author posts is Irony. If an author is labeled as not Irony, we assume that every tweet this author posts is not Irony. We label each tweet in the training dataset based on such rules and feed these tweets and their corresponding labels into the BERT model for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Input and Model Training</head><p>After processing the training dataset according to the method mentioned in 2.2. Data Processing, we obtain the processed training dataset. Among the 420 authors in the dataset, the first 210 authors are all Irony, and the last 210 are not Irony. We take the 106th to 315th authors in the dataset as a subset, and the remaining 1st to 105th authors and 316th to 420th authors as another subset. This way, we can get two subsets with the same number of Irony authors and not Irony authors. We denote these two subsets as PartA and PartB. We feed each tweet from PartA and its corresponding label into BERT for training and obtain BERT modelA. Similarly, we feed each tweet in PartB and its corresponding label into BERT for training and obtain BERT modelB. Later we will use the hard voting strategy to determine whether an author is Irony. In order to prevent a tie vote, in addition to BERT modelA and BERT modelB, we feed each tweet and its corresponding label from the whole training dataset into BERT for training, and finally, we get BERT modelC. The architecture of input and model training is shown in Figure <ref type="figure" coords="2,103.55,688.44,4.14,9.94" target="#fig_1">2</ref>.</p><p>The trained BERT model is capable of scoring each tweet. The score of a tweet is represented by (Irony, not_Irony). For example, a tweet with a score of (0.8, 0.2) means an 80% probability that the statement is an Irony tweet and a 20% probability that it is not. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Classifier and Output</head><p>In our classifier, we set two hyperparameters, one of which is passProbability, representing the threshold for the tweet to be Irony. We set the tweet to be Irony only if the value of the Irony part of the tweet's score is greater than the passProbability and greater than the value of the not Irony part.</p><p>Each author has posted 200 tweets, and by feeding each tweet and its label into the BERT model, we get 200 scores for each author. To determine whether the author is an Irony author, we set a hyperparameter passNumber, representing the threshold for the author to be Irony. When the number of Irony tweets is greater than the passNumber, we consider the author to be an Irony.</p><p>To improve our accuracy, we also use multi-model voting. Input and Model Training mentions that BERT is trained with the dataset of PartA, and BERT modelB is trained with the dataset of PartB. We use BERT modelA to evaluate the PartB dataset on whether an author is Irony or not, and compare the evaluation results with the PartB dataset to obtain the accuracy of BERT modelA. BERT modelB does the same thing. In the evaluation process, we keep adjusting the passProbability and passNumber of the classifiers corresponding to the two BERT models so that the accuracy of the two BERT models is maximized. The optimal values of the two hyperparameters are found by analyzing the hyperparameters of BERT modelA and BERT modelB. The analysis process for the optimal values of the two hyperparameters is described in 3. Results. The whole dataset is fed into BERT for training to get a BERT modelC, and we set the passProbability of the classifier corresponding to the BERT modelC to 0.6 and the passNumber to 100. Now, we get three models in total. We use a hard voting strategy to determine whether an author is Irony. We use each model to vote for an author, and if the author receives more than or equal to 2 votes, then we consider the author to be Irony, and we consider the author to be non-Irony otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>In the end, as shown in Table1, the accuracy of the top three teams was 0.9944, 0.9778, and 0.9722, respectively. Our model achieves an accuracy of 0.9389 and is ranked 27th. <ref type="bibr" coords="3,428.76,598.92,12.91,9.94" target="#b1">[2]</ref> We attribute this accuracy to two things that we have done. First, we set two hyperparameters and dynamically adjusted both of them, allowing the classifier corresponding to each model to achieve high accuracy. The second is that we used multi-model voting, which allows us to more accurately predict whether an author is an irony and stereotype spreader. We analyzed the two hyperparameters of BERT modelA and BERT modelB so that the optimal values of the two hyperparameters are found.</p><p>As shown in Figure <ref type="figure" coords="4,174.47,99.76,4.14,9.94">3</ref>, we plot the variation of passProbability in BERT modelA and BERT modelB with the increase of passNumber for different values of accuracy, where the values of passProbability are 0.01,0.02,0.03.... .0.99,1.00, a total of 100 values. By observation, we can see that more curves are reaching the peak when the passNumber is between 100-125, so we know that the best value of passNumber is between 100-125.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3: The curve of accuracy with passNumber for different values of passProbability</head><p>As shown in Figure <ref type="figure" coords="4,182.46,320.47,4.14,9.94" target="#fig_2">4</ref>, we plotted the scatter plots of BERT modelA and BERT modelB with passNumber as the x-coordinate, passProbability as the y-axis, and accuracy as the z-axis, respectively. We observed that passProbability peaked at more points at 0.5-0.6. Based on the above observation, we know that the passProbability should be set to 0.6, and the passNumber should be set to 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>This paper describes our team's experiments to solve the task Profiling Irony and Stereotype Spreaders on Twitter (IROSTEREO). We consider this task to be a classification task. In our experiments, we first trained three different models via BERT. To improve the accuracy of our models, we analyzed two hyperparameters, and finally, we obtained the optimal values of the two hyperparameters for each BERT model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,72.02,331.61,187.53,11.04;2,124.23,147.89,346.52,181.05"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of the whole model</figDesc><graphic coords="2,124.23,147.89,346.52,181.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,72.02,216.62,222.49,11.04;3,152.75,72.00,289.40,141.65"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of input and model training</figDesc><graphic coords="3,152.75,72.00,289.40,141.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,72.02,526.51,446.24,11.04;4,86.18,540.21,436.61,9.94;4,72.02,552.93,148.52,9.94;4,85.25,355.92,424.16,168.40"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Scatter plot with passNumber as x-axis, passProbability as y-axis, and accuracy as the z-axis Based on the above observation, we know that the passProbability should be set to 0.6, and the passNumber should be set to 100.</figDesc><graphic coords="4,85.25,355.92,424.16,168.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,72.02,661.90,400.26,95.30"><head>Table 1 The</head><label>1</label><figDesc></figDesc><table coords="3,91.21,675.34,381.07,81.86"><row><cell cols="2">results of the top three teams and our team</cell><cell></cell></row><row><cell>POS</cell><cell>TEAM</cell><cell>ACCURACY</cell></row><row><cell>1</cell><cell>wentaoyu</cell><cell>0.9944</cell></row><row><cell>2</cell><cell>harshv</cell><cell>0.9778</cell></row><row><cell>3</cell><cell>edapal</cell><cell>0.9722</cell></row><row><cell>27</cell><cell>caohaojie</cell><cell>0.9389</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5.">Acknowledgements</head><p>This work is supported by the <rs type="funder">Natural Science Foundation of Guangdong Province, China</rs> (No. <rs type="grantNumber">2022A1515011544</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_uPY8E3j">
					<idno type="grant-number">2022A1515011544</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,90.02,105.52,432.78,9.94;5,90.02,118.12,432.86,9.94;5,90.02,130.84,433.21,9.94;5,90.02,143.44,405.87,9.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,355.88,105.52,166.93,9.94;5,90.02,118.12,409.16,9.94">Overview of PAN 2022: Authorship Verification, Profiling Irony and Stereotype Spreaders, and Style Change Detection</title>
		<author>
			<persName coords=""><forename type="first">Janek</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Berta</forename><surname>Chulvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Elisabetta</forename><surname>Fersini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,90.02,130.84,433.21,9.94;5,90.02,143.44,330.36,9.94">Proceedings of the Thirteenth International Conference of the CLEF Association (CLEF 2022)</title>
		<meeting>the Thirteenth International Conference of the CLEF Association (CLEF 2022)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="5,90.02,156.16,432.93,9.94;5,90.02,168.76,433.08,9.94;5,90.02,181.48,276.52,9.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,90.02,168.76,358.95,9.94">Profiling Irony and Stereotype Spreaders on Twitter (IROSTEREO) at PAN 2022</title>
		<author>
			<persName coords=""><forename type="first">Chulvi</forename><surname>Ortega-Bueno Reynier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rangel</forename><surname>Berta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rosso</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fersini</forename><surname>Paolo</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Elisabetta</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="5,471.41,168.76,51.70,9.94;5,90.02,181.48,90.44,9.94">CLEF 2022 Labs and Workshops</title>
		<title level="s" coord="5,188.51,181.48,104.12,9.94">Notebook Papers, CEUR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,90.02,194.08,432.77,9.94;5,90.02,206.68,433.13,9.94;5,90.02,219.40,258.15,9.94" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Chulvi</forename><surname>Ortega-Bueno Reynier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rangel</forename><surname>Berta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rosso</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fersini</forename><surname>Paolo</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Elisabetta</surname></persName>
		</author>
		<ptr target="https://zenodo.org/record/6514916#.Yos_yXZBxD9" />
		<title level="m" coord="5,500.78,194.08,22.01,9.94;5,90.02,206.68,398.91,9.94">PAN 22 Author Profiling: Profiling Irony and Stereotype Spreaders on Twitter (IROSTEREO)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
