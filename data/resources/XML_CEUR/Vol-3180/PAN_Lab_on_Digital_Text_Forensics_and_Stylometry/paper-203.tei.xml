<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,355.58,15.42;1,89.29,106.66,251.74,15.42;1,88.93,142.55,164.42,11.96">UniNE at PAN-CLEF 2022: Profiling Irony and Stereotype Spreaders on Twitter (Notebook for PAN at CLEF 2022)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,89.29,182.00,71.72,11.96"><forename type="first">Catherine</forename><surname>Ikae</surname></persName>
							<email>catherine.ikae@unine.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Neuch√¢tel</orgName>
								<address>
									<addrLine>Avenue du 1er-Mars 26</addrLine>
									<postCode>2000</postCode>
									<settlement>Neuch√¢tel</settlement>
									<country>Switzerland, Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,355.58,15.42;1,89.29,106.66,251.74,15.42;1,88.93,142.55,164.42,11.96">UniNE at PAN-CLEF 2022: Profiling Irony and Stereotype Spreaders on Twitter (Notebook for PAN at CLEF 2022)</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">28D3BC00E1753E416570DE94616E0683</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Irony detection</term>
					<term>machine learning</term>
					<term>natural language processing</term>
					<term>Random forest (RF)</term>
					<term>Twitter</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work proposes to solve the problem of profiling irony and stereotype spreaders on twitter using a random forest model with features obtained using chi square feature scoring. The task is to determine whether the author of a given Twitter feed in English spreads irony and stereotypes. The training sample contains timelines of authors sharing irony and stereotypes towards, for instance, women or the LGTB community. Transforming this question into binary classification problem which requires us to classify authors as ironic or not. Evaluation with 300 chi2 features shows an overall performance of ACC = 0.9722.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Irony uses metaphorical and sophisticated language to indicate the polar opposite of what is literally said. The purpose of sarcasm, a more aggressive form of irony, is to mock or humiliate a victim while not ruling out the possibility of harm. Stereotypes are frequently employed, particularly when discussing contentious issues such as immigration or sexism and misogyny.</p><p>Irony and stereotype has the unique virtue of making it difficult to bridge the gap between its literal and intended meaning. Detecting irony and stereotype behavior in online social networks like Facebook, Twitter, Instagram, polls, and other places has become a crucial task since it affects social and personal connections. Irony detection is a critical processing challenge in natural language processing (NLP), which is required for better understanding and to serve as an interface for machine-human communication.</p><p>Ironic and stereotyped tweets made in a friendly tone go unnoticed. People can use politeness to be sarcastic, using very formal language that do not fit the casual discourse. Irony is often shown by complimenting someone in a formal manner. Ironic and stereotyped tweets, by their very nature, lead to incorrect replies from review summarization systems, sentiment classifiers, review ranking systems, and any other application that deals with the semantics and pragmatics of text. Nonetheless, most social media and microblogging sites, such as Twitter, set text length limits that arguably do little to prohibit the use of innovative language such as sarcasm and irony, which allow strong views to be expressed effectively <ref type="bibr" coords="2,354.40,249.56,11.43,10.91" target="#b0">[1]</ref>.</p><p>Irony detection was first explored by Reyes <ref type="bibr" coords="2,281.56,276.66,12.70,10.91" target="#b1">[2]</ref> [3], he approaches irony as a language phenomena, undistinguished from sarcasm, that incorporates something unexpected or contradictory, using decision trees to classify features indicating unexpectedness such as emotive words, contradictory phrases, and punctuation.</p><p>[4] is an example of automatic humor recognition in which he detected sarcasm in areas including politics, education, and humor. The hashtags #sarcasm, #politics, #education, and #humour were used to collect data from Twitter. He uses the American National Corpus Frequency Data and the morphology of tweets to create a measure of unexpectedness and probable ambiguities based on words that are primarily used in spoken language. Random forests and decision trees are used to classify the data.</p><p>To detect irony, <ref type="bibr" coords="2,162.38,439.25,12.99,10.91" target="#b4">[5]</ref> employed Logistic Regression and focused on the unexpectedness factor, which is defined as an emotional imbalance between words in a text. While <ref type="bibr" coords="2,437.12,452.79,12.99,10.91" target="#b5">[6]</ref> suggested a pattern-based technique for detecting irony that uses n-grams and combinations of adverbs and acronyms that imply comedy as characteristics. <ref type="bibr" coords="2,325.86,479.89,12.95,10.91" target="#b6">[7]</ref> introduced a new method that treats sarcasm as a binary classification job, pitting positive-negative statements against each other. WordNet-Affect and LWIC are used to extract lexical features, as well as so-called "pragmatic factors, " which indicate user sentiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Corpus</head><p>The training corpus was available in the English to be used to determine determine whether its author spreads irony and stereotypes. The dataset had 210 documents of label NI (contains set of tweets)) and 210 documents of label I (tweets containing some form of irony). In our point of view, the problem is therefore to identify a set of tweets containing irony, leading to consider that the user generates and/or spreads stereotype <ref type="bibr" coords="3,352.07,86.97,24.05,10.91">[8][9]</ref>. This task will be performed only English language. As one can see in Tables 2, tweets in Category #I describe questions to a facts " in the fact that it actually physically exists" followed by ironic statements "Assuming it's not photoshopped".</p><p>In tweets appearing under the second label, #NI the statements used in the tweets are facts followed by an explanation to the point noted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Feature Selection</head><p>The process of picking a subset of the terms in the training set and using only this subset as features in text classification is known as feature selection. The objective of feature selection is twofold. First, it reduces the quantity of the effective vocabulary, making training and using a classifier more efficient. This is especially important for classifiers that are more expensive to train. Second, by removing noisy features, feature selection frequently improves classification accuracy. A noise feature is one that raises the classification error on new(unseen) data as it is added to the document representation.</p><p>Feature selection can be thought of as a way of substituting a complex classifier (one that uses all features) with a simpler one (using a subset of the features). Tokens are considered according to their document frequency (df) and word frequency difference in the two-stage feature selection technique. This exercise was completed with a three-fold threshold (df &gt; 3) and a one-fold threshold (tf &gt; 1). We design a feature set capable of differentiating each category using these two limitations. A term frequency difference is computed from the reduced number of tokens obtained by applying df &gt; 3, but only tokens with a term frequency greater than 1 (tf &gt; 1) are taken into account, leaving out tokens that appear just once in the text.</p><p>Chi-square (ùúí 2 ) is a function used to test the independence of two variables in the correct case, the independence of a feature and a category is provided by a ùúí 2 value. The higher value of the ùúí 2 , the closer relationship the variables have Pointwise Mutual Information (PMI) measures how much information a term contains about a class. It measures how much information the presence/absence of a term contributes to making the correct classification decision. The magnitude of PMI will indicate if an association between a feature and a category exist or not <ref type="bibr" coords="4,289.39,528.54,17.75,10.91" target="#b10">[11]</ref>  <ref type="bibr" coords="4,309.85,528.54,16.09,10.91" target="#b9">[10]</ref>. PMI can therefore be used to decide if a feature is informative or not, and a feature selection is done on that basis. Having less features often improves the performance of your classification algorithm. To calculate ùëÉ ùëÄ ùêº, as a ratio the joint probability (ùëù(ùë° ùëñ , ùëê ùëó )) and the probability of occurrence of term ùë° ùëñ multiplied by the probability of selecting a text belonging to the category ùëê ùëó .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ùëÉ ùëÄ ùêº(ùë°</head><formula xml:id="formula_0" coords="4,194.91,605.13,311.08,77.21">ùëñ , ùëê ùëó ) = ùëôùëúùëî 2 ( ùëù(ùë° ùëñ , ùëê ùëó ) ùëù(ùë° ùëñ ) * ùëù(ùëê ùëó ) ) = ùëôùëúùëî 2 ‚éõ ‚éú ‚éù ùëé ùëõ ùëé + ùëè ùëõ * ùëé + ùëê ùëõ ‚éû ‚éü ‚é† = ùëôùëúùëî 2 (Ô∏Ç ùëé * ùëõ (ùëé + ùëè) * (ùëé + ùëê) )Ô∏Ç (2)</formula><p>We discovered that such a figure is still too huge after evaluating the terms found in feature sets with thousands of terms. Only the top m terms (e.g., m = 300) depicting the highest discriminating powers selected with the chi2 (ùúí 2 ) and PMI feature selection techniques will be used in the model. The model with the best score was the chosen to build the final classifier.</p><p>Splitting the training data into train (300) and development (120), the two step feature selection reduces the features as shown in the Table <ref type="table" coords="5,280.93,168.26,3.74,10.91" target="#tab_2">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Random Forest Classifier</head><p>Random Forest consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model prediction <ref type="bibr" coords="5,322.56,371.50,16.41,10.91" target="#b11">[12]</ref>. This model can perform well given large feature sets because it combines the predictions of various decision trees to build a more robust classifier. While constructing new decision trees, this method uses a random subset of features which gets rid of spurious features and improving the robustness of our estimate.</p><p>The random forest method operates by following the steps below: 1): From the dataset provided, the algorithm selects random samples. 2): For each sample chosen, the algorithm will generate a decision tree. Then, for each decision tree constructed, it will acquire a prediction result. 3): For each expected outcome, voting will be conducted. It will employ mode to solve a classification problem and mean to solve a regression problem. 4): Finally, the algorithm will choose the prediction with the most votes as the final prediction.</p><p>In random forests, hyperparameters are used to either improve the model's performance and predictive capacity or to make it faster. The hyperparameters are listed as follows. 1). n estimators -the number of trees built by the algorithm before averaging the predictions. 2). max features -the number of features that a random forest evaluates while splitting a node. 3.) mini sample leaf -specifies how many leaves are necessary to separate an internal node. 4). n jobs-this parameter tells the engine how many processors it can use. It can only use one processor if the value is 1, but there is no limit if the value is -1. 5). random state-regulates the sample's unpredictability. If the model has a definite value of random state and is given the same hyperparameters and training data, it will always generate the same results. 6). oob score -OOB is an abbreviation for "out of the bag. " It's a cross-validation method based on random forests. In this case, one-third of the sample is used to evaluate the data rather than train it. These samples are drawn from a bag of samples.</p><p>Random forest has been used in a variety of applications, for example <ref type="bibr" coords="6,390.69,363.46,17.75,10.91" target="#b12">[13]</ref> combined data enrichment with the introduction of semantics in random forest to improve short text classification. The authors in <ref type="bibr" coords="6,158.89,390.56,18.07,10.91" target="#b13">[14]</ref> described a new method on random forest and feature selection (FS) for text classification and achieved macro-F1 score 73%. <ref type="bibr" coords="6,324.91,404.11,17.96,10.91" target="#b14">[15]</ref> performs sentiment classification of You Tube comments using the random forest, and Word2Vec Skip-gram for features extraction.</p><p>[16] explores random forest with several term weighting method for sentiment analysis in Indonesian language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation</head><p>To train our model, features are extracted from the training documents by taking into account the steps explained in section 3. Features to be considered must have a tf&gt;1 and df&gt;3 from which the ranking is done according to chi2 and PMI with k feature set of 300.</p><p>The accuracy of several classifiers are computed as shown in the Table <ref type="table" coords="6,400.04,598.23,3.66,10.91" target="#tab_3">4</ref>. It was easy to analyse the performance of the classifiers where we can see that accuracy of XGB = 0.917 is the highest when all selected features are used. When chi2 was used to rank the features and using the top 300, random forest scored highest accuracy of RF = 0.942. Ranking the features again with PMI and using the top 300, again random forest had the best performance, RF = 0.925.</p><p>The use of RF in the final model with chi2 ranking was based on its overall highest score of 0.942 accuracy. The model was used to evaluated the test set which resulted into accuracy of 0.9722 as seen in Table <ref type="table" coords="7,192.89,114.06,3.74,10.91" target="#tab_4">5</ref>. The final evaluation result is obtained on the TIRA platform <ref type="bibr" coords="7,359.02,404.22,17.91,10.91" target="#b16">[17]</ref> is exposed in Table <ref type="table" coords="7,466.86,404.22,3.74,10.91" target="#tab_4">5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>The research proposes a machine learning approach for detecting irony and stereotype spreaders. A random forest classifier was proposed. For the test set on TIRA, the final performance provided us an accuracy of roughly 0.9722. Because the features used in the classification are drawn in similar amounts from both classes, our approach is quite competent of identifying irony/nonirony tweets. The discriminating strength of each feature in the class is measured using a concept called probability difference. These characteristics highlight the distinction between the two classes, which are subsequently ranked based on their chi2 values to enable further reduction in the features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,319.62,382.26,17.91,10.91;4,164.20,414.06,6.83,9.57;4,171.02,411.56,4.23,6.99;4,175.76,414.06,41.27,10.63;4,221.26,404.61,97.51,12.65;4,313.67,406.62,48.82,10.63;4,357.40,404.61,67.75,12.65;4,425.15,404.67,4.23,6.99;4,265.74,419.53,119.65,12.65;4,257.30,445.19,8.49,9.57;4,307.52,437.81,80.12,9.57;4,387.64,435.86,4.23,6.99;4,270.01,452.67,159.88,9.57;4,494.41,428.46,11.57,10.91"><head>[ 10 ]</head><label>10</label><figDesc>ùúí 2 (ùë° ùëñ , ùëê ùëó ) = ùëõ * ((ùëù(ùë° ùëñ , ùëê ùëó ) * ùëù(ùë° ¬Øùëñ, ùëê ¬Øùëó))(ùëù(ùë° ùëñ , ùëê ¬Øùëó) * ùëù(ùë° ¬Øùëñ, ùëê ùëó ))) 2 ùëù(ùë° ùëñ ) * ùëù(ùë° ¬Øùëñ) * ùëù(ùëê ùëó ) * ùëù(ùëê ¬Øùëó) = ùëõ * (ùëé * ùëë -ùëê * ùëè) 2 (ùëé + ùëê) * (ùëè + ùëë) * (ùëé + ùëè) * (ùëê + ùëë) (1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,89.29,90.62,145.34,8.93;6,143.64,106.52,305.50,166.50"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Random Forest Classifier</figDesc><graphic coords="6,143.64,106.52,305.50,166.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,129.52,319.97,86.37"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="3,89.29,141.53,319.68,74.37"><row><cell>Overall statistics about the training data</cell><cell></cell><cell></cell></row><row><cell></cell><cell>irony(I)</cell><cell>not irony (NI)</cell></row><row><cell>Nb. doc.</cell><cell>210</cell><cell>210</cell></row><row><cell>Nb tweets</cell><cell>42000</cell><cell>42000</cell></row><row><cell>Mean length</cell><cell>5979</cell><cell>5296</cell></row><row><cell>|Voc|</cell><cell>57247</cell><cell>46642</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,88.99,234.21,419.11,336.63"><head>Table 2</head><label>2</label><figDesc>Sample of three tweets in English for each class English tweets Class I #user# $80 billion tshirt. Gotta be some #HASHTAG# in the fact that it actually physically exists.. Assuming it's not photoshopped..</figDesc><table /><note coords="3,150.54,310.51,357.56,8.87;3,150.54,322.47,357.17,8.87;3,150.54,334.42,357.17,8.87;3,150.54,346.38,103.53,8.87;3,150.54,370.29,357.17,8.87;3,150.54,382.25,357.35,8.87;3,150.54,394.20,357.17,8.87;3,150.54,406.16,169.39,8.87;3,95.67,418.51,34.51,8.87;3,150.54,418.51,357.16,8.87;3,150.54,430.46,357.17,8.87;3,150.54,442.42,357.17,8.87;3,150.54,454.37,62.18,8.87;3,150.54,478.29,357.17,8.87;3,150.31,490.24,357.40,8.87;3,150.54,502.20,278.28,8.87;3,150.54,526.11,357.17,8.87;3,150.54,538.06,357.41,8.87;3,150.54,550.02,357.17,8.87;3,150.54,561.97,62.18,8.87"><p><p>#user# #user# #user# Kinda like rewards points from the merchant? That's not bad.. Fees? Converting to crypto is still extra effort and expense; electronic payment apps are just so easy and secure.. Outside the U.S., electronic payment has been ubiquitous for almost 3 decades. That's a big hill.</p>#user# #user# #user# #user# You haven't explained shit. Scarcity has nothing to do with anything. You don't understand money. If you think "banks" print up billions of 1 dollar bills and just hand them out, you are an infant. Money is for using, not holding. There is no economy without spending. Get a job. Class NI Busy with plea deal in corruption case, Netanyahu is absent from the Knesset: Opposition leader Benjamin Netanyahu tells attorneys to push on in negotiations with Israel's attorney general. Sources close to the former PM claim he missed Knesset. . . #URL# Haaretz #URL# Prof. Mevorach warns: 'We don't know post-corona effects of Omicron': 'Children are a vulnerable population, we don't know post-corona effects of Omicron, ' head of Hadassah Medical Center's COVID-19 ward warns. #URL# ArutzSheva #URL# The Iraqi cleric's gamble to sideline in Iran-backed factions in new government: The movement led by Shi'ite cleric Moqtada al-Sadr already re-elected a parliamentary speaker opposed by the Iran-aligned camp, and could leave them in Iraq's. . . #URL# Haaretz #URL#</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,88.99,194.05,387.77,74.02"><head>Table 3</head><label>3</label><figDesc></figDesc><table coords="5,89.02,206.05,387.74,62.01"><row><cell>Two-step feature selection</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>All</cell><cell>tf &gt; 1</cell><cell>tf diff and df &gt; 3</cell></row><row><cell>vocubulary I</cell><cell>46437</cell><cell>19316</cell><cell>5455</cell></row><row><cell>vocubulary NI</cell><cell>38379</cell><cell>16160</cell><cell>4322</cell></row><row><cell>Total number of features</cell><cell>84816</cell><cell>35476</cell><cell>9777</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,88.99,140.62,338.64,244.75"><head>Table 4</head><label>4</label><figDesc></figDesc><table coords="7,89.29,152.63,338.34,232.74"><row><cell>Evaluation based on different feature sizes</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>features</cell><cell></cell></row><row><cell>Classifiers</cell><cell>fs</cell><cell cols="2">chi2, k= 300 pmi, k= 300</cell></row><row><cell>KN</cell><cell>0.692</cell><cell>0.725</cell><cell>0.775</cell></row><row><cell>SVC</cell><cell>0.742</cell><cell>0.742</cell><cell>0.742</cell></row><row><cell>Extra Trees</cell><cell>0.858</cell><cell>0.892</cell><cell>0.867</cell></row><row><cell>Decision Tree</cell><cell>0.800</cell><cell>0.825</cell><cell>0.783</cell></row><row><cell>GaussianNB</cell><cell>0.858</cell><cell>0.875</cell><cell>0.808</cell></row><row><cell>BernoulliNB</cell><cell>0.858</cell><cell>0.833</cell><cell>0.850</cell></row><row><cell>MultinomialNB</cell><cell>0.558</cell><cell>0.558</cell><cell>0.542</cell></row><row><cell>MLP</cell><cell>0.867</cell><cell>0.850</cell><cell>0.758</cell></row><row><cell>SGD</cell><cell>0.625</cell><cell>0.633</cell><cell>0.625</cell></row><row><cell>LDA</cell><cell>0.833</cell><cell>0.625</cell><cell>0.500</cell></row><row><cell>Random Forest</cell><cell>0.858</cell><cell>0.942</cell><cell>0.925</cell></row><row><cell>AdaBoost</cell><cell>0.892</cell><cell>0.892</cell><cell>0.875</cell></row><row><cell>Bagging</cell><cell>0.833</cell><cell>0.875</cell><cell>0.883</cell></row><row><cell>Gradient Boosting</cell><cell>0.892</cell><cell>0.917</cell><cell>0.900</cell></row><row><cell>XGB</cell><cell>0.917</cell><cell>0.925</cell><cell>0.917</cell></row><row><cell>Logistic Regression</cell><cell>0.617</cell><cell>0.617</cell><cell>0.625</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,88.99,433.23,291.28,61.34"><head>Table 5</head><label>5</label><figDesc>Official Evaluation with (m = 300)</figDesc><table coords="7,210.67,461.00,169.60,33.57"><row><cell>Classifiers</cell><cell>Acc</cell></row><row><cell>Random Forest (Early Bird)</cell><cell>0.9722</cell></row><row><cell>Random Forest (Final )</cell><cell>0.9722</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,114.06,393.33,10.91;8,112.66,127.61,393.33,10.91;8,112.66,141.16,393.53,10.91;8,112.66,154.71,395.01,10.91;8,112.66,168.26,201.71,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,442.62,114.06,63.37,10.91;8,112.66,127.61,279.04,10.91">SemEval-2015 task 11: Sentiment analysis of figurative language in Twitter</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Veale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Barnden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Reyes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S15-2080</idno>
		<ptr target="https://aclanthology.org/S15-2080.doi:10.18653/v1/S15-2080" />
	</analytic>
	<monogr>
		<title level="m" coord="8,420.06,127.61,85.92,10.91;8,112.66,141.16,393.53,10.91;8,112.66,154.71,113.32,10.91">Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), Association for Computational Linguistics</title>
		<meeting>the 9th International Workshop on Semantic Evaluation (SemEval 2015), Association for Computational Linguistics<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="470" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,181.81,393.33,10.91;8,112.66,195.36,397.48,10.91;8,112.66,211.35,103.29,7.90" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,250.83,181.81,255.16,10.91;8,112.66,195.36,107.31,10.91">From humor recognition to irony detection: The figurative language of social media</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Buscaldi</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.datak.2012.02.005</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,228.79,195.36,133.98,10.91">Data Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,222.46,394.53,10.91;8,112.66,236.01,244.31,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,240.01,222.46,263.13,10.91">A multidimensional approach for detecting irony in twitter</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Veale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,112.66,236.01,160.37,10.91">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="239" to="268" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,249.56,394.51,10.91;8,112.66,265.55,50.37,7.90" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Saggion</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/E14-3007</idno>
		<title level="m" coord="8,222.79,249.56,117.43,10.91">Modelling irony in twitter</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="56" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,276.66,393.32,10.91;8,112.66,290.20,393.33,10.91;8,112.66,303.75,394.53,10.91;8,112.28,317.30,395.00,10.91;8,112.66,330.85,292.62,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,288.18,276.66,217.80,10.91;8,112.66,290.20,214.11,10.91">An impact analysis of features in a classification approach to irony detection in product reviews</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Buschmeier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Klinger</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W14-2608</idno>
		<ptr target="https://aclanthology.org/W14-2608.doi:10.3115/v1/W14-2608" />
	</analytic>
	<monogr>
		<title level="m" coord="8,352.95,290.20,153.03,10.91;8,112.66,303.75,394.53,10.91;8,112.28,317.30,190.63,10.91">Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, Association for Computational Linguistics</title>
		<meeting>the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,344.40,395.17,10.91;8,112.66,357.95,393.33,10.91;8,112.66,371.50,393.53,10.91;8,112.66,385.05,394.03,10.91;8,112.41,398.60,233.99,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,354.56,344.40,153.28,10.91;8,112.66,357.95,181.21,10.91">Clues for detecting irony in usergenerated contents: Oh...!! it&apos;s &quot;so easy</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sarmento</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Oliveira</surname></persName>
		</author>
		<idno type="DOI">10.1145/1651461.1651471</idno>
		<ptr target="https://doi.org/10.1145/1651461.1651471.doi:10.1145/1651461.1651471" />
	</analytic>
	<monogr>
		<title level="m" coord="8,339.41,357.95,166.58,10.91;8,112.66,371.50,320.53,10.91">Proceedings of the 1st International CIKM Workshop on Topic-Sentiment Analysis for Mass Opinion, TSA &apos;09</title>
		<meeting>the 1st International CIKM Workshop on Topic-Sentiment Analysis for Mass Opinion, TSA &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="53" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,412.15,393.53,10.91;8,112.66,425.70,393.32,10.91;8,112.66,439.25,394.52,10.91;8,112.66,452.79,368.24,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,326.33,412.15,179.86,10.91;8,112.66,425.70,17.76,10.91">Identifying sarcasm in Twitter: A closer look</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gonz√°lez-Ib√°√±ez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Wacholder</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/P11-2102" />
	</analytic>
	<monogr>
		<title level="m" coord="8,153.60,425.70,352.39,10.91;8,112.66,439.25,390.24,10.91">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="581" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,466.34,394.53,10.91;8,112.66,479.89,395.17,10.91;8,112.66,493.44,393.32,10.91;8,112.66,506.99,395.01,10.91;8,112.66,520.54,393.33,10.91;8,112.66,534.09,393.32,10.91;8,112.66,547.64,393.33,10.91;8,112.66,561.19,184.76,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,251.61,493.44,254.37,10.91;8,112.66,506.99,259.82,10.91">Overview of PAN 2022: Authorship Verification, Profiling Irony and Stereotype Spreaders, and Style Change Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chulvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Heini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kredens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mayerl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ortega-Bueno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pezik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wolska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zangerle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,446.63,520.54,59.35,10.91;8,112.66,534.09,393.32,10.91;8,112.66,547.64,253.92,10.91">Proceedings of the Thirteenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="8,474.16,548.65,31.82,9.72;8,112.66,562.20,112.43,9.72">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">D E F S C M G P A H M P G F N F</forename><surname>Alberto Barron-Cedeno</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</editor>
		<editor>
			<persName><surname>Martino</surname></persName>
		</editor>
		<meeting>the Thirteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="volume">13390</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="8,112.66,574.74,393.33,10.91;8,112.66,588.29,394.53,10.91;8,112.66,601.84,172.05,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,373.25,574.74,132.74,10.91;8,112.66,588.29,219.45,10.91">Profiling Irony and Stereotype Spreaders on Twitter (IROSTEREO) at PAN 2022</title>
		<author>
			<persName coords=""><forename type="first">O.-B</forename><surname>Reynier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Berta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Elisabetta</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="8,358.04,588.29,143.79,10.91">CLEF 2022 Labs and Workshops</title>
		<title level="s" coord="8,112.66,601.84,103.05,10.91">Notebook Papers, CEUR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,615.39,394.53,10.91;8,112.66,628.93,394.03,10.91;8,112.66,642.48,158.43,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,154.13,615.39,348.81,10.91">Comparative evaluation of term selection functions for authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
		<idno type="DOI">10.1093/llc/fqt047</idno>
		<ptr target="https://doi.org/10.1093/llc/fqt047.doi:10.1093/llc/fqt047" />
	</analytic>
	<monogr>
		<title level="j" coord="8,112.66,628.93,166.84,10.91">Digital Scholarship in the Humanities</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="246" to="261" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,656.03,394.53,10.91;8,112.66,669.58,308.97,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,214.24,656.03,288.27,10.91">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hanks</surname></persName>
		</author>
		<idno type="DOI">10.3115/981623.981633</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,112.66,669.58,117.87,10.91">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,86.97,235.10,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<title level="m" coord="9,167.61,86.97,148.23,10.91">Random forests-random features</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,100.52,393.33,10.91;9,112.66,114.06,394.51,10.91;9,112.66,130.06,123.08,7.90" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dartigues-Pallez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Da Costa Pereira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Precioso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lloret</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10160-6_26</idno>
		<title level="m" coord="9,481.34,100.52,24.65,10.91;9,112.66,114.06,224.05,10.91">Short text classification using semantic random forest</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="288" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,141.16,393.33,10.91;9,112.66,154.71,395.01,10.91;9,112.66,168.26,168.81,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,250.47,141.16,255.52,10.91;9,112.66,154.71,135.89,10.91">Improving text classification performance with random forests-based feature selection</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Maruf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Babri</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13369-015-1945-x</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,257.59,154.71,204.06,10.91">Arabian Journal for Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,181.81,394.53,10.91;9,112.33,195.36,306.41,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,169.82,181.81,333.47,10.91">Sentiment analysis on youtube comments using word2vec and random forest</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Khomsah</surname></persName>
		</author>
		<idno type="DOI">10.31315/telematika.v18i1.4493</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,112.33,195.36,49.04,10.91">Telematika</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">61</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,208.91,394.53,10.91;9,112.66,222.46,395.01,10.91;9,112.66,236.01,197.48,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,167.04,208.91,335.32,10.91">Random forest approach for sentiment analysis in indonesian language</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fauzi</surname></persName>
		</author>
		<idno type="DOI">10.11591/ijeecs.v12.i1.pp46-50</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,112.66,222.46,315.76,10.91">Indonesian Journal of Electrical Engineering and Computer Science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="46" to="50" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,249.56,394.53,10.91;9,112.66,263.11,270.41,10.91" xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-22948-1_5</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,327.23,249.56,175.37,10.91">TIRA Integrated Research Architecture</title>
		<imprint>
			<biblScope unit="page" from="123" to="160" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
