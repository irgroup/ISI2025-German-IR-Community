<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,366.64,15.42;1,88.71,106.66,91.78,15.43">Different Encoding Approaches for Authorship Verification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,110.38,11.96"><forename type="first">Stefanos</forename><surname>Konstantinou</surname></persName>
							<email>stefanos.konstantinou@uzh.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<addrLine>Rämistrasse 71</addrLine>
									<postCode>8006</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,212.32,134.97,46.85,11.96"><forename type="first">Jinqiao</forename><surname>Li</surname></persName>
							<email>jinqiao.li@uzh.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<addrLine>Rämistrasse 71</addrLine>
									<postCode>8006</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,290.17,134.97,83.08,11.96"><forename type="first">Angelos</forename><surname>Zinonos</surname></persName>
							<email>angelos.zinonos@uzh.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<addrLine>Rämistrasse 71</addrLine>
									<postCode>8006</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,366.64,15.42;1,88.71,106.66,91.78,15.43">Different Encoding Approaches for Authorship Verification</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">97AF51C723311060BA49E55F5F00E7F4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>NLP</term>
					<term>Author Verification</term>
					<term>PAN22</term>
					<term>Pre-trained model</term>
					<term>Text information</term>
					<term>Classification 2. Related</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>PAN is a series of scientific events and shared tasks which focuses on digital text forensics and stylometry. In previous editions of PAN, the effectiveness of authorship verification technology in several languages and text genres was tackled, and this year the content shifted to cross discourse type pairs of text. The purpose of this paper is to test various Transformer based encoder models, using Cross encoder and Bi-Encoder approaches. The results illustrate a decent performance, reaching an F1 score of 80% on the best model. Further experimentation was performed on the training dataset, which resulted in no positive outcome.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper presents our approach for the Authorship Verification Shared Task <ref type="bibr" coords="1,436.19,367.70,12.78,10.91" target="#b0">[1]</ref> at PAN 2022 <ref type="bibr" coords="1,89.29,381.25,11.47,10.91" target="#b1">[2]</ref>. The goal of the task is to decide whether two texts have been written by the same author based on comparing their writing styles. Compared to the tasks in previous editions, this year's aim is to focus on more challenging scenarios, as this will allow studying the ability of stylometric approaches to capture authorial characteristics even when different discourse types are imposed. Discriminating between documents by stylometric means, could indicate significant boosts in the area of Cyber Security and Criminology. Moreover, people producing or writing hate speech on social platforms anonymously could be identified from even their business emails if the task is successfully solved thus, they can be held accountable.</p><p>The dataset contains essays, emails, text messages and business memos in English. The purpose of this task is to develop a method that will compare a pair of texts consisting of different discourse types and to predict whether they are written by the same author or not.</p><p>After analyzing related work, we have decided to follow a transformer-based approach since it is widely and successfully used in Natural Language Processing. Our goal is to experiment with a wider variety of transformer models than what has been tested before. verification task. Successful methods and their maximum performance on an identical task and evaluation metric as this year's task are presented, with differences in context in the data set. <ref type="bibr" coords="2,108.94,114.06,13.00,10.91" target="#b3">[4]</ref> concludes that best performance is achieved by combining heterogeneous methods, for instance, applying machine learning techniques such as decision trees and artificial neural networks.</p><p>[5] aims to analyze the problem of correlating the author's characteristics with the attributes of documents written by that same author. They highlight that the first step should be identifying the essential feature in the text to conduct a better analytic phase. Once the relevant features are extracted from a document, different methods can be experimented with to identify the author. This was useful for their implementation of SVMs and Random Forests.</p><p>Transformer-based architectures have been experimented with on earlier versions of this task, but were limited to using BERT. In <ref type="bibr" coords="2,274.53,236.01,11.58,10.91" target="#b5">[6]</ref>, a pre-trained model of Bert was presented as a solution for encoding text information of text pairs. Data-record splitting was introduced to create short texts that can be encoded by BERT, achieving the highest c@1 and F1-score on PAN Authorship Verification datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Material and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">An overview on the dataset</head><p>The provided dataset contains 12,264 pairs, from which 10,424 (85%) are used for training and the rest for validation. Furthermore, the dataset comes with many peculiarities. A correlation between authors is challenging because the texts come from different writing scenarios. For example, most people follow a formal way of writing business memos. As a result, it is more difficult to find stylistic similarities between e-mails and business memos and text messages or essays, even if a pair is written by the same author.</p><p>When using transformers, the common practice is to encode the dataset in its original form without applying much preprocessing. After comparing emails and texts, we decided to remove HTML character artifacts as they do not contribute to authorship verification from a stylometric point of view. Since the Roberta model uses byte pair encoding, we suspected that the HTML characters would artificially increase a pair's dissimilarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Distribution of Text Length</head><p>First, an exploratory data analysis of the given dataset was conducted. The overall text length distribution at word level and the distribution of different types of text lengths is shown in Table <ref type="table" coords="2,116.65,568.17,5.17,10.91" target="#tab_0">1</ref> and Figure <ref type="figure" coords="2,176.75,568.17,3.81,10.91" target="#fig_0">1</ref>. This illustrates the necessity of assigning a high maximum length for tokenization, including padding and truncation.</p><p>The box plot shows that the length of the different discourse types varies greatly. The 'essay' type is much longer than the others, and the 'text message' type is the shortest.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Statistics for Pairs</head><p>To better understand the dataset, an analysis was performed at the discourse type (DT) level and the similarity was calculated for each of them -see table <ref type="table" coords="3,366.62,522.76,3.77,10.91" target="#tab_1">2</ref>. Here, texts are encoded with the S-BERT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Encoder Experiments</head><p>The aim of our submission is to test pre-trained models of different architectures. These models are to be fine-tuned to this task using the dataset provided. For text comparison tasks such as semantic similarity or author verification, two approaches are popular: the Bi-Encoder and the Cross Encoder approach. These architectures are considered to be particularly powerful for similarity tasks.</p><p>MPNet <ref type="bibr" coords="3,135.31,653.78,12.99,10.91" target="#b6">[7]</ref> is trained using permuted language modeling and claims to gain a better understanding of bidirectional contexts, which may prove crucial for a text similarity task. The Roberta models <ref type="bibr" coords="4,160.55,244.62,12.77,10.91" target="#b7">[8]</ref> that we used were already fine-tuned on similarity tasks, thus enabling the transfer of text similarity knowledge to our task. Therefore, we expect that the training time will be shorter and the performance will be better. The pretrained model configurations are shown in Table <ref type="table" coords="4,127.99,285.27,3.77,10.91">3</ref>. The maximum length in Bi-Encoder models is 256, which is half of Cross-Encoder because Bi-Encoders encode two sentences separately: limiting the context to two 256 subtoken sequences, which are then concatenated. Note that for the experiments, only the pre-trained base models with a maximum text length of 512 subtokens were used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>Details about the tested pretrained models. 'Max.Subtokens' is the value of hyperparameter 'max_length' in tokenizer. 'Batchsize' is the size of the batch used in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Cross-Encoder Approach</head><p>In the Cross-Encoder architecture, both texts are simultaneously fed through a transformer network. In this architecture, a single encoding for both texts is used for classification (see Figure <ref type="figure" coords="4,120.68,554.25,5.13,10.91" target="#fig_1">2</ref> on the right). Cross-Encoders are normally used when you have a pre-defined set of text pairs that you want to score. Cross-encoders usually outperform Bi-Encoders, but do not scale well with large datasets. That's why Cross-Encoders seem suitable for our task.</p><p>For the Cross-Encoder approach, we used several Roberta models: Roberta-Muppet, Roberta fine-tuned on the STSB task <ref type="bibr" coords="4,223.07,608.44,11.58,10.91" target="#b8">[9]</ref>, and plain BERT that was trained with the Next Sentence Prediction Task <ref type="bibr" coords="4,161.33,621.99,16.25,10.91" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Bi-Encoder Approach</head><p>Bi-Encoder architecture creates a twin network that processes two sentences simultaneously in the same way <ref type="bibr" coords="5,149.84,121.08,16.09,10.91" target="#b10">[11]</ref>. All parameters are shared. The pooling layer creates fixed-size representation for input sentences of varying lengths, while also extracting the features that are considered the most important ones (see Figure <ref type="figure" coords="5,252.41,148.18,5.07,10.91" target="#fig_1">2</ref> on the left).</p><p>For the Bi-Encoder approach, we experiment with Roberta-Muppet and MPNet adding a pooling and dense layer after the standard encoding step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Cross-Encoder vs Bi-Encoder Approaches</head><p>The main difference between these two architectures is that two sentences are concatenated in a Cross-Encoder using the SEP special token as a separator. Thus, the encoder can have attention to information from both sentences, while in the Bi-Encoder architecture, the word embeddings of the two sentences are encoded separately and then concatenated. In addition, the Bi-Encoder does not compute attention information between the subtokens of the two texts, as the embedding process is done separately.  <ref type="bibr" coords="5,442.68,461.07,14.85,8.87" target="#b10">[11]</ref>. Illustrated using a BERT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dataset Manipulation Experiments</head><p>We also experimented with modifying and manipulating the dataset, in particular, splitting the text segments, negative sampling, and combining these two techniques. The purpose is to explore whether these techniques improve prediction results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Splitting Text Segments</head><p>Splitting text segments in half results in twice as much data items as before. Since the models see smaller contexts during training, we hope that shorter but more texts force the model to better learn authorial characteristics that remain stable across various discourse types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Negative Sampling</head><p>The negative sampling method was initially used to accelerate the training of Skip-Gram models <ref type="bibr" coords="6,89.29,121.08,18.07,10.91" target="#b11">[12]</ref> and has since been widely used in the natural language processing. Negative sampling serves two purposes: efficiency and effectiveness.</p><p>• Efficiency: Negative sampling can reduce the training load by optimizing only the vectors involved in the cost-finding process. • Effectiveness: Negative sampling provides high-quality negative examples in a targeted manner, both to speed up convergence and allow the model to be optimized in the desired direction.</p><p>In this dataset experiment, all authors were mapped with their corresponding texts, and new pairs of negative data are created by combining texts from different authors.</p><p>Our aim was to create a dataset with 20% (6,132) positive samples and 80% (approx. 24,000) negative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>To evaluate the proposed models, we use the TIRA evaluation tool <ref type="bibr" coords="6,399.17,340.00,18.06,10.91" target="#b12">[13]</ref> with the following metrics:</p><p>AUC: The area-under-the-curve (ROC) score F1-score: F1 score is the harmonic mean between precision and recall. c@1: A variant of the F1-score, which rewards systems that leave difficult problems unanswered, like scores of exactly 0.5. F 0.5𝑢 : A measure that puts more emphasis on deciding same-author cases correctly. Brier: The complement of the Brier score for evaluating the goodness of (binary) probabilistic classifiers.</p><p>The results of the models trained on this task will then be compared with a set of baseline results. The baseline results are obtained using a simple method that calculates the cosine similarities between TFIDF-normalized, bag-of-character-tetragrams representations of the text pairs. Then the resulting scores are shifted using a simple grid search, to arrive at an optimal performance on the validation set.</p><p>It has to be noted that the validation set across all models are always to be kept the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Discussion</head><p>In Table <ref type="table" coords="6,127.68,601.84,3.70,10.91" target="#tab_3">4</ref>, the results of the trained models are shown. Overall, the models scored decently on the validation dataset with an overall score mostly higher than 70%, higher than the baseline. MPNet using the Bi-Encoder approach scored the highest. Its permutation language modeling turned out to be better than the other models that used masked language modeling in pretraining.</p><p>The two Roberta-Muppet models performed similarly, falling right behind MPNet. At the same time, the two Roberta-Muppet models managed to perform considerably better than the remaining two models. A possible explanation is that Roberta-Muppet's multitask pre-training translated into better stylometric understanding due to its more generalized embeddings.</p><p>In Figure <ref type="figure" coords="7,141.96,332.29,3.66,10.91" target="#fig_2">3</ref>, an analysis of the predictions of MPNet is shown to check the accuracy results on all combinations of discourse types on the validation set. The accuracy results range around 75% across all combinations. What is interesting is that all discourse type combinations achieved a similar score with marginal differences. Together with the F 0.5𝑢 results, this indicates that up to a certain level, the model managed to learn authorial characteristics across discourse types. Further experimentation is reported in Table <ref type="table" coords="7,295.01,649.34,3.66,10.91" target="#tab_5">5</ref>. The models chosen for this analysis were the 3 best performing ones from  did not contribute positively to the stylometric learning of the models. We observe a decrease in the performance of MPNet. Maybe splitting the data caused its permutation language model to be less effective given that less information for each shortened text, is encoded. Negative sampling reduced the performance for all models. This discrepancy could be attributed to the positive to negative ratio of the dataset, therefore a smaller sample of negative data could have been better.</p><p>Finally, what we observe is that the Cross-Encoder models with Roberta-Muppet are clearly better than the other two models in the dataset manipulation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In our submission, we tested various pre-trained encoder models using Cross-Encoder and Bi-Encoder architectures to solve the authorship verification problem of PAN@CLEF 2022. We conclude that the inherent difficulty of the dataset is the major obstacle because the different types of discourse require different linguistic expressions, for instance, the considerable dissimilarity between business memos and text messages. The results show that a simple approach of selecting a pre-trained model and fine-tuning it is able to grasp some stylometric information useful for author verification, but overall the performance is not strong enough to reliably solve this difficult task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,439.08,416.69,8.93;3,89.29,450.82,250.85,9.14"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Box plot of the text length distribution of different text types. Granularity of counting is at the word level. Logarithmic scaling was applied to the 𝑥-axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,89.29,461.02,416.69,8.93;5,89.29,473.02,84.93,8.87;5,172.63,315.07,250.02,139.36"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Schematic diagram of the structure of our two different encoder approaches [11]. Illustrated using a BERT model.</figDesc><graphic coords="5,172.63,315.07,250.02,139.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,89.29,619.69,357.37,8.93"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Accuracy results on all combinations of discourse types on the validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,89.95,330.12,118.25"><head>Table 1</head><label>1</label><figDesc>Length of different types of text.</figDesc><table coords="3,176.16,89.95,242.96,86.08"><row><cell></cell><cell cols="5">Overall Essay Text_Message Email Memo</cell></row><row><cell>mean</cell><cell>410</cell><cell>1718</cell><cell>96</cell><cell>1718</cell><cell>220</cell></row><row><cell>min</cell><cell>31</cell><cell>240</cell><cell>63</cell><cell>240</cell><cell>31</cell></row><row><cell>25%</cell><cell>96</cell><cell>1217</cell><cell>87</cell><cell>1217</cell><cell>169</cell></row><row><cell>50%</cell><cell>289</cell><cell>1603</cell><cell>93</cell><cell>1603</cell><cell>212</cell></row><row><cell>75%</cell><cell>363</cell><cell>2254</cell><cell>100</cell><cell>2254</cell><cell>292</cell></row><row><cell>max</cell><cell>3270</cell><cell>3270</cell><cell>474</cell><cell>3270</cell><cell>416</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,88.98,89.95,417.00,130.20"><head>Table 2</head><label>2</label><figDesc>Analysis on each DT combination. 'Mean Similarity' is the mean of cosine similarity of all pairs in each DT combination.</figDesc><table coords="4,140.78,89.95,313.72,86.08"><row><cell cols="4">Discourse Type Pairs (DT) # Pairs # Identical Authors Mean Similarity</cell></row><row><cell>essay, email</cell><cell>1618</cell><cell>809</cell><cell>0.4850</cell></row><row><cell>email, text_message</cell><cell>7484</cell><cell>3742</cell><cell>0.6127</cell></row><row><cell>essay, text_message</cell><cell>1182</cell><cell>591</cell><cell>0.4803</cell></row><row><cell>memo, email</cell><cell>1014</cell><cell>507</cell><cell>0.4952</cell></row><row><cell>memo, text_message</cell><cell>780</cell><cell>390</cell><cell>0.5459</cell></row><row><cell>essay, memo</cell><cell>186</cell><cell>93</cell><cell>0.4161</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,88.99,89.95,418.53,190.77"><head>Table 4</head><label>4</label><figDesc>Results of each model on the validation set and the best model's performance on the test set. "Bi-" prefix represents the Bi-Encoder Architecture &amp; "CE-"prefix represents Cross-Encoder Architecture approach. The voting ensemble with 3 models includes: Bi-MPNet, Bi-Roberta-Muppet, CE-Roberta-Muppet. Bolded scores mark the best performance on each metric. Bi-MPNet (on TIRA) was evaluated on the test set of TIRA.</figDesc><table coords="7,116.95,89.95,361.38,110.78"><row><cell>Model</cell><cell>auc</cell><cell>c@1</cell><cell cols="2">F 0.5𝑢 F1</cell><cell>brier</cell><cell>Overall</cell></row><row><cell>Voting Ensemble 3 Models</cell><cell cols="6">0.765 0.759 0.718 0.800 0.759 0.760</cell></row><row><cell>Bi-MPNet</cell><cell cols="6">0.777 0.771 0.729 0.807 0.771 0.771</cell></row><row><cell>Bi-Roberta-Muppet</cell><cell cols="6">0.748 0.743 0.708 0.781 0.743 0.745</cell></row><row><cell>CE-Roberta-Muppet</cell><cell cols="6">0.749 0.744 0.709 0.782 0.744 0.746</cell></row><row><cell>CE-Roberta-stsb</cell><cell>0.68</cell><cell cols="2">0.672 0.65</cell><cell cols="3">0.745 0.672 0.684</cell></row><row><cell cols="7">CE-BertForNextSentencePrediction 0.705 0.701 0.679 0.724 0.701 0.702</cell></row><row><cell>Baseline</cell><cell>0.55</cell><cell>0.5</cell><cell cols="4">0.546 0.671 0.749 0.603</cell></row><row><cell>Bi-MPNet (on TIRA)</cell><cell cols="6">0.577 0.557 0.563 0.581 0.589 0.573</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,207.98,662.89,298.00,10.91"><head>Table 4 .</head><label>4</label><figDesc>The results show that the dataset manipulation experiments NEG S.-N. SPLIT NEG S.-N. SPLIT NEG S.-N. SPLIT NEG S.-N. SPLIT NEG S.-N. SPLIT NEG S.-N.</figDesc><table coords="8,93.65,93.93,407.72,80.90"><row><cell>Model</cell><cell>auc</cell><cell>c@1</cell><cell>F 0.5𝑢</cell><cell>F1</cell><cell>brier</cell><cell>overall</cell></row><row><cell>Bi-MPNet</cell><cell cols="6">0.689 0.499 0.501 0.689 0.499 0.501 0.674 0.555 0.556 0.739 0.666 0.667 0.689 0.499 0.501 0.696 0.544 0.545</cell></row><row><cell cols="7">Bi-Roberta-Muppet 0.640 0.501 0.672 0.640 0.501 0.672 0.637 0.556 0.658 0.691 0.667 0.742 0.640 0.501 0.672 0.650 0.545 0.684</cell></row><row><cell cols="7">CE-Roberta-Muppet 0.729 0.696 0.590 0.729 0.696 0.590 0.705 0.676 0.603 0.770 0.756 0.699 0.729 0.696 0.590 0.732 0.704 0.615</cell></row></table><note coords="8,151.65,112.68,15.08,5.48"><p>SPLIT</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,88.99,187.48,418.53,68.69"><head>Table 5</head><label>5</label><figDesc>Results of each model on the validation set on 3 experimental variants: "SPLIT" means splitting text segments in half (doubling the data item number). "NEG" is negative sampling. "S.-N." uses both SPLIT and NEG. "B-" prefix represents the Bi-Encoder approach and "CE-" prefix the Cross-Encoder. Underlined scores mark the experiment's (column) highest score and bold-ed scores mark the highest score of the metric.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Special thanks to <rs type="person">Dr. Simon Clematide</rs> and <rs type="person">Andrianos Michail</rs> for all the help and guidance given to our team for the completion of this work. Moreover, special thanks to the PAN members for the support given to us in situations of technical difficulties.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,111.28,393.33,10.91;9,112.28,124.83,393.70,10.91;9,112.28,138.38,394.00,10.91;9,112.66,151.93,195.00,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,435.83,124.83,70.15,10.91;9,112.28,138.38,161.25,10.91">Overview of the Authorship Verification Task at PAN</title>
		<author>
			<persName coords=""><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krzysztof</forename><surname>Kredens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Pezik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Annina</forename><surname>Heini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Janek</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,317.14,138.38,138.08,10.91">CLEF 2022 Labs and Workshops</title>
		<title level="s" coord="9,463.15,138.38,43.12,10.91;9,112.66,151.93,164.87,10.91">Notebook Papers, CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,165.48,394.53,10.91;9,112.66,179.03,395.17,10.91;9,112.66,192.57,393.32,10.91;9,112.66,206.12,394.52,10.91;9,112.66,219.67,394.53,10.91;9,112.66,233.22,394.53,10.91;9,112.66,246.77,393.33,10.91;9,112.33,260.32,393.66,10.91;9,112.66,273.87,230.66,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,251.61,192.57,254.37,10.91;9,112.66,206.12,265.21,10.91">Overview of PAN 2022: Authorship Verification, Profiling Irony and Stereotype Spreaders, and Style Change Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chulvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Heini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kredens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mayerl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ortega-Bueno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pezik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wolska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zangerle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,112.66,246.77,393.33,10.91;9,112.33,260.32,301.51,10.91">Proceedings of the Thirteenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="9,124.27,274.88,146.73,9.72">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Alberto</forename><surname>Barron-Cedeno</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mirko</forename><surname>Martino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fabrizio</forename><surname>Degli Esposti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Craig</forename><surname>Sebastiani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gabriella</forename><surname>Macdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Allan</forename><surname>Pasi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Martin</forename><surname>Hanbury</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Guglielmo</forename><surname>Potthast</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicola</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><surname>Ferro</surname></persName>
		</editor>
		<meeting>the Thirteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="volume">13390</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="9,112.66,287.42,394.53,10.91;9,112.66,300.97,393.32,10.91;9,112.66,314.52,394.04,10.91;9,112.66,328.07,65.09,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,210.70,300.97,295.28,10.91">Overview of the cross-domain authorship verification task at pan</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Manjavacas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-147.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,154.03,314.52,99.01,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="1743" to="1759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,341.62,395.17,10.91;9,112.66,355.17,265.95,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,177.94,341.62,229.08,10.91">Authorship verification: A review of recent advances</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<idno type="DOI">10.13053/rcs-123-1-1</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,415.63,341.62,92.20,10.91;9,112.66,355.17,50.30,10.91">Research in Computing Science</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="9" to="25" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,368.71,393.57,10.91;9,112.33,382.26,196.83,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,154.85,368.71,101.74,10.91">Authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Juola</surname></persName>
		</author>
		<idno type="DOI">10.1561/1500000005</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,266.18,368.71,231.79,10.91">Foundations and Trends® in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="233" to="334" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,395.81,393.33,10.91;9,112.66,409.36,220.47,10.91" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="9,316.76,395.81,189.23,10.91;9,112.66,409.36,146.09,10.91">Encoding text information by pre-trained model for authorship verification</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,422.91,393.54,10.91;9,112.66,436.46,287.79,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.09297</idno>
		<title level="m" coord="9,294.27,422.91,211.93,10.91;9,112.66,436.46,105.29,10.91">Mpnet: Masked and permuted pre-training for language understanding</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,450.01,394.53,10.91;9,112.30,463.56,393.68,10.91;9,112.66,477.11,107.17,10.91" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="9,173.53,463.56,256.77,10.91">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,490.66,395.01,10.91;9,112.66,504.21,135.81,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,148.14,490.66,236.54,10.91">Machine translated multilingual sts benchmark dataset</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>May</surname></persName>
		</author>
		<ptr target="https://github.com/PhilipMay/stsb-multi-mt" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,517.76,393.33,10.91;9,112.66,531.30,395.01,10.91;9,112.66,544.85,187.21,10.91" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="9,319.43,517.76,186.56,10.91;9,112.66,531.30,180.57,10.91">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,558.40,394.53,10.91;9,112.66,571.95,173.79,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m" coord="9,219.74,558.40,282.85,10.91">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,585.50,393.33,10.91;9,112.26,599.05,393.72,10.91;9,112.66,612.60,80.12,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,369.78,585.50,136.21,10.91;9,112.26,599.05,197.11,10.91">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,317.63,599.05,188.35,10.91;9,112.66,612.60,35.32,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,626.15,394.53,10.91;9,112.66,639.70,393.33,10.91;9,112.66,653.25,394.51,10.91;9,112.66,669.24,123.08,7.90" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="9,327.46,626.15,175.13,10.91">TIRA Integrated Research Architecture</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-22948-1_5</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,240.99,639.70,264.99,10.91;9,112.66,653.25,123.97,10.91">Information Retrieval Evaluation in a Changing World, The Information Retrieval Series</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
