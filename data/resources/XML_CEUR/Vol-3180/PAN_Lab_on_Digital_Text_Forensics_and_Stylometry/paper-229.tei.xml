<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,282.11,15.42;1,89.29,107.08,157.29,11.96">BERT-based ironic authors profiling Notebook for PAN at CLEF 2022</title>
				<funder ref="#_GzSfage">
					<orgName type="full">Deutsche Forschungsgemeinschaft (DFG -German Research Foundation</orgName>
				</funder>
				<funder ref="#_6M9nhaY">
					<orgName type="full">German Federal Ministry of Education and Research</orgName>
				</funder>
				<funder ref="#_2N4NdQQ">
					<orgName type="full">PhD School &quot;SecHuman</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.72,132.98,53.25,11.96"><forename type="first">Wentao</forename><surname>Yu</surname></persName>
							<email>wentao.yu@rub.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Communication Acoustics</orgName>
								<orgName type="institution">Ruhr University Bochum</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,154.61,132.98,113.01,11.96"><forename type="first">Benedikt</forename><surname>Boenninghoff</surname></persName>
							<email>benedikt.boenninghoff@rub.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Communication Acoustics</orgName>
								<orgName type="institution">Ruhr University Bochum</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,298.62,132.98,86.16,11.96"><forename type="first">Dorothea</forename><surname>Kolossa</surname></persName>
							<email>dorothea.kolossa@rub.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Communication Acoustics</orgName>
								<orgName type="institution">Ruhr University Bochum</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Ironyidentification.git</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">CLEF</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,282.11,15.42;1,89.29,107.08,157.29,11.96">BERT-based ironic authors profiling Notebook for PAN at CLEF 2022</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">83977AC404BDDCF9D1844E42C0FE79AD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>BERT</term>
					<term>Long text</term>
					<term>Irony detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The present paper addresses the PAN at CLEF 2022 challenge "Profiling Irony and Stereotype Spreaders on Twitter" (IROSTEREO). The challenge strives to identify whether an author spreads sarcasm through their tweets. In general, author profiling tasks, whether mechanical or manual, are based on extensive author text. Many machine learning-based author profiling studies indicate that author-by-author classification could benefit the performance compared with a text-by-text way.</p><p>We address the challenge through fine-tuning BERT model. BERT has shown satisfactory results on many natural language processing tasks. However, BERT model cannot exert its advantages in some specific tasks, like handling long documents, due to its limitation on the maximum input token length. Our author profiling task is one of these specific tasks. The present work addresses this dilemma through a re-segmentation approach: We first concatenate all tweets of an author into one document representation. We then split the document in such a way that the split text lengths do not exceed the maximum input token length of the BERT model and that we still retain the advantages of continuous text through an appropriate choice of overlap. Ultimately, the BERT model uses the hard voting method made the final decision.</p><p>Our work first compares the performance of two pre-trained BERT models, i.e., the RoBERTa and BERTweet models, trained with external datasets. Then, we fine-tune BERT models with three different loss functions. In addition, we also demonstrate and evaluate a BERT feature-based CNN model. The winning models of the PAN author profiling task in recent years are re-implemented as baselines. Finally, the BERTweet model trained with the cross-entropy weighted focal loss function achieves an accuracy of 98.89% on the official test set. Adding a further soft voting ensemble method, which integrates BERTweet models with different loss functions as well as the BERT feature-based CNN model, we placed first in the challenge and improved our model performance to 99.44%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Despite great effort being exerted by researchers and developers to detect and filter toxic language, the amount and impact of hate speech on social media are still posing serious threats to the mental health and well-being of users as well as to the possibility of democratic discourse</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>The PAN organizing committee launched a series of author profiling tasks during the past decade. Until 2021, all challenges were multilingual tasks. Table <ref type="table" coords="2,371.37,552.07,4.99,10.91" target="#tab_0">1</ref> gives an overview of all PAN author profiling challenges.</p><p>The objective of the Author Profiling Tasks from PAN 2013 to 2016 was to identify authors' gender and age group through their documents <ref type="bibr" coords="2,305.72,592.72,11.44,10.91" target="#b4">[5,</ref><ref type="bibr" coords="2,319.89,592.72,7.51,10.91" target="#b5">6,</ref><ref type="bibr" coords="2,330.12,592.72,7.51,10.91" target="#b6">7,</ref><ref type="bibr" coords="2,340.36,592.72,7.63,10.91" target="#b7">8]</ref>. In 2015, in addition to identifying gender and age, participants also needed to score five personality traits <ref type="bibr" coords="2,420.07,606.27,11.58,10.91" target="#b6">[7]</ref>. The 2017 PAN challenge focused not only on authors' gender detection but also on the language variety identification <ref type="bibr" coords="2,148.31,633.36,12.65,10.91" target="#b8">[9]</ref>. The task in 2018 was still to detect gender, but in a multi-modal way. The organizer provided text and image data for the model training <ref type="bibr" coords="2,361.62,646.91,16.09,10.91" target="#b9">[10]</ref>. With the rapid rise in social network users, people have also become aware of the problems that arise in such virtual spaces. Hence, the PAN challenge author profiling task has a new focus on Twitter from 2019. The goal in 2019 was bot detection, meanwhile identifying the gender of the human authors <ref type="bibr" coords="3,487.21,286.16,16.37,10.91" target="#b10">[11]</ref>. The tasks for 2020 and 2021 were Twitter fake news and hate speech spreader identification, respectively <ref type="bibr" coords="3,145.59,313.26,16.43,10.91" target="#b11">[12,</ref><ref type="bibr" coords="3,164.75,313.26,12.32,10.91" target="#b12">13]</ref>.</p><p>The best result papers in the past show the development of author profiling over the last few years. Most contestants achieved the best results using conventional classifiers like support vector machines (SVMs), decision trees, Expectation Maximization Clustering (EMC), and LibLINEAR <ref type="bibr" coords="3,143.51,367.46,16.52,10.91" target="#b13">[14,</ref><ref type="bibr" coords="3,162.74,367.46,12.58,10.91" target="#b14">15,</ref><ref type="bibr" coords="3,178.04,367.46,12.58,10.91" target="#b15">16,</ref><ref type="bibr" coords="3,193.34,367.46,12.58,10.91" target="#b16">17,</ref><ref type="bibr" coords="3,208.63,367.46,12.58,10.91" target="#b17">18,</ref><ref type="bibr" coords="3,223.93,367.46,12.58,10.91" target="#b18">19,</ref><ref type="bibr" coords="3,239.23,367.46,12.58,10.91" target="#b19">20,</ref><ref type="bibr" coords="3,254.53,367.46,12.39,10.91" target="#b20">21]</ref>. Among these, SVM combined with n-gram features is the most frequently used. Before 2018, researchers mainly discussed text preprocessing and feature extraction with the conventional classifiers. Since 2018, some new deep learning algorithms have gained advantages-the winning group in 2018 adopted representation fusion: text and image features are extracted by RNN and CNN, respectively <ref type="bibr" coords="3,390.70,421.66,16.09,10.91" target="#b21">[22]</ref>. Features are fused by using direct-product, column-and row-wise pooling. The fused representation of the texts and images is fed to the fully connected layers for classification. In 2021, deep learning methods still outperformed conventional classifiers. The optimal model utilizes CNN for classification; a self-trained embedding layer extracts the features <ref type="bibr" coords="3,312.98,475.85,12.84,10.91" target="#b1">[2]</ref> (detailed in Section 4.1).</p><p>Since the transformer model was proposed in 2017 <ref type="bibr" coords="3,323.38,489.40,16.08,10.91" target="#b22">[23]</ref>, attention mechanisms have attracted much attention and discussion. In 2018, the BERT model, proposed by the Google team, has achieved remarkable results in many natural language processing (NLP) tasks. Due to the attractiveness of the BERT model, more and more teams choose the BERT model to handle author profiling tasks. Only one team used the BERT model in 2019 <ref type="bibr" coords="3,396.95,543.60,16.38,10.91" target="#b23">[24]</ref>, in 2020, there were three teams <ref type="bibr" coords="3,144.12,557.15,16.44,10.91" target="#b24">[25,</ref><ref type="bibr" coords="3,163.28,557.15,12.55,10.91" target="#b25">26,</ref><ref type="bibr" coords="3,178.55,557.15,12.33,10.91" target="#b26">27]</ref>, and most recently, the BERT model was widely used in the 2021 PAN author profiling task.</p><p>The author profiling tasks are usually based on many documents of that author. It is a sensible and effective strategy to profile by combining all of the author's manuscripts, as can be confirmed in some previous winning models <ref type="bibr" coords="3,308.88,611.34,11.58,10.91" target="#b1">[2]</ref>. Although the BERT model is attractive, it has a bound on the maximum input token sequence length, thus limiting the ability of the BERT model to handle long texts. To overcome this drawback, some researchers have proposed a document re-segmentation strategy, dividing long documents into sub-documents that match the maximum sequence length of BERT <ref type="bibr" coords="3,264.53,665.54,16.31,10.91" target="#b27">[28,</ref><ref type="bibr" coords="3,283.47,665.54,12.23,10.91" target="#b28">29]</ref>. Last year, one team achieved the best accuracy for the English author profiling task by fine-tuning the BERT model with a similar strategy <ref type="bibr" coords="4,487.08,86.97,16.08,10.91" target="#b29">[30]</ref>, concatenating 20 tweets of each author into one sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset</head><p>This year's PAN challenge <ref type="bibr" coords="4,213.61,159.14,18.07,10.91" target="#b30">[31]</ref> author profiling subtask is a monolingual task that aims to identify English-language sarcasm spreaders on Twitter <ref type="bibr" coords="4,333.61,172.69,16.09,10.91" target="#b31">[32]</ref>. The balanced training set contains 420 author samples, each of which has 200 tweets by this author. The official test set has 180 author samples, again with 200 tweets each. Tagged users, hashtags, and URLs are already normalized as "#USER#", "#HASHTAG#" and "#URL#", respectively. To train the model and test its effectiveness, a 4-fold cross-validation is adopted during the training stage. Thus, the official training set is split into an inner training set and a test set in each fold, with 315 and 105 author samples, respectively.</p><p>This work experiments with two BERT models. The first is the twitter-roberta-base -irony [33]<ref type="foot" coords="4,147.86,279.33,3.71,7.97" target="#foot_1">2</ref> , and the other is bertweet-large <ref type="bibr" coords="4,324.27,281.08,18.07,10.91" target="#b33">[34]</ref> <ref type="foot" coords="4,346.27,279.33,3.71,7.97" target="#foot_2">3</ref> . Two distinct text preprocessing schemes are adopted for these two BERT models. In addition, there is one scheme for the CNN model and the TF-IDF (term frequency-inverse document frequency) features. The TF-IDF features are used to train conventional classification models like SVMs.</p><p>â€¢ Scheme 1: for the twitter-roberta-base-irony model remove "#USER#", "#HASHTAG#" and "#URL#" -replace multiple spaces by one single space convert emojis to text with the Python emoji replace "#USER#" with "@USER" -replace "#HASHTAG#" with "#HASHTA" -replace "#URL#" with "HTTPURL" -text normalization with the embedded normalization processes</p><p>â€¢ Scheme 3: for the CNN model and the conventional classification approaches like SVMs, linear regression (LR), and random forest (RF) classifiers.</p><p>remove "#USER#", "#HASHTAG#" and "#URL#" -replace multiple spaces by a single space convert emojis to text with the Python emoji package normalize all text into lowercase -remove punctuation and numbers remove stop words</p><p>The TF-IDF features are obtained by word-based 1-to 3-gram and character-based 3-to 5-gram models. Then the truncated singular value decomposition (SVD) <ref type="bibr" coords="5,424.68,134.31,18.07,10.91" target="#b34">[35]</ref> <ref type="foot" coords="5,446.03,132.55,3.71,7.97" target="#foot_4">5</ref> reduces the feature dimension to 1000 to reduce the computational complexity. Specifically, the TF-IDF features are extracted by the TfidfVectorizer function from the scikit-learn library <ref type="foot" coords="5,478.85,159.65,3.71,7.97" target="#foot_5">6</ref> . The minimum document frequency is 2, and the maximum is set to 100%, i.e., the terms occurring in all documents or in less than two documents are ignored. The word-based and character-based models are obtained separately, each producing a 1000-dimensional vector of every input tweet. Finally, the two vectors are concatenated as a representation of the tweet.</p><p>As introduced in Section 2, the BERT model is limited in handling long documents. However, profiling authors based on long manuscripts can benefit the accuracy. To address this conflict, a re-segmentation strategy is adopted to make the sequence length of the sub-document fit the maximum input token length of the BERT model. The continuity of the segmented sentences is guaranteed by overlapping segmentation. The specific steps are as follows:</p><p>â€¢ concatenate all 200 tweets of each author â€¢ the new sub-document is segmented with the same text length ğ‘ and with an overlap of ğ‘‚. To simplify the program, only the number of words in the text is considered here instead of the number of tokens. Therefore, ğ‘ should be smaller than the maximum token length of the BERT model to guarantee that there are no text segments that are too long for BERT. â€¢ the author's label is assigned to every sub-document of that author Two external irony detection datasets were utilized to pre-train both BERT models for better performance. One is the Ironic Corpus <ref type="bibr" coords="5,271.51,418.13,16.41,10.91" target="#b35">[36]</ref>, the other is the SemEval-2018 irony detection dataset <ref type="bibr" coords="5,123.42,431.68,16.22,10.91" target="#b36">[37]</ref>. The datasets were labeled on each document. Only 0.168% document exceeds the BERT maximum token length limitation. Therefore, we have not applied the sub-segmentation strategy for these two corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Models</head><p>Besides two BERT-based models, we also consider a CNN model that builds on the BERT embeddings. In addition, three traditional classifiers are also trained as baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Baselines</head><p>The models below are used as baselines. Among those, you can also find the winning models of the PAN author profiling task in previous years, in our respective re-implementations.</p><p>â€¢ SVM, LR, and RF models: All tweets of an author are concatenated and preprocessed according to Scheme 3. The TF-IDF features are extracted from the processed document. Finally, the scikit-learn library is utilized to train the models.</p><p>â€¢ CNN : We re-implemented the winning model of the PAN 2021 author profiling task <ref type="bibr" coords="6,492.39,86.97,11.46,10.91" target="#b1">[2]</ref>. Again, all the author's tweets are concatenated, and Scheme 3 is adopted for text normalization. The model structure is the same as in <ref type="bibr" coords="6,348.73,114.06,11.58,10.91" target="#b1">[2]</ref>. The embedding layer projects each input token into a 100-dimensional vector. A 1D-convolution layer with 64 filters of size 36 was applied to the embedding tensors. Then an average pooling with a size of 8 reduces the features' complexity, and the global average pooling decreases the dimension of the features. Finally, a fully connected layer outputs the results in the desired size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Fine-tuning BERT models</head><formula xml:id="formula_0" coords="6,227.50,353.87,183.24,9.88">D 0 i D 1 i D M i</formula><p>Text-pre-processing</p><formula xml:id="formula_1" coords="6,89.29,230.86,256.46,272.00">â€¢ â€¢ â€¢ p(c m i |D m i ) E m i BERT D i 1 Figure 1: Fine-tuning a BERT model</formula><p>We initially chose the RoBERTa model (twitter-roberta-base-irony) <ref type="bibr" coords="6,447.21,522.30,18.07,10.91" target="#b32">[33]</ref> and pretrained it on the SemEval2018 irony detection database <ref type="bibr" coords="6,348.39,535.85,16.42,10.91" target="#b36">[37]</ref>. However, the leaderboard of TweetEval on GitHub <ref type="foot" coords="6,190.65,547.64,3.71,7.97" target="#foot_6">7</ref> indicates that the BERTweet model (bertweet-large) <ref type="bibr" coords="6,453.09,549.40,18.07,10.91" target="#b33">[34]</ref> outperforms other candidate models for the sarcasm identification task. Therefore, the BERTweet model is also considered. Both base models are first trained with external datasets, and these pre-trained models are marked as RoBERTa-ext and BERTweet-ext. Subsequently, these two pre-trained models are fine-tuned with the PAN challenge dataset to obtain the final models RoBERTa and BERTweet.  As stated in <ref type="bibr" coords="7,154.33,400.69,11.28,10.91" target="#b2">[3]</ref>, there are many benefits to training a model using BERT embeddings as fixed features. On the one hand, the BERT model structure cannot suit all tasks, rather, sometimes it needs to add some task-specific design to increase flexibility. On the other hand, featurebased methods can speed up the computation because the text representation only needs to be computed once. This paper adopts the CNN layer for classification (BERTweet-CONV ). The word embeddings of all sub-documents of an author are concatenated to train the CNN model, where the embeddings are extracted from the pre-trained BERTweet model. The CNN model structure here is principally the same as in the CNN described in Section 4.1, the only difference is the input dimension. The Bi-LSTM layer is not considered because the word-embeddings of all sub-documents are concatenated. The concatenated word-embeddings could be regarded as a word-embedding of a long document, and its token length could be as long as 4000. For such a lengthy document, BLSTM layers could face the problem of vanishing gradients and exploding gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Feature-based approach</head><formula xml:id="formula_2" coords="7,151.59,176.96,299.70,204.55">E 0 i E 1 i E M i â€¢ â€¢ â€¢ CNN FC p CN N (c i |D i ) if epoch &lt; 2 p(c 0 i |D 0 i ) p(c 1 i |D 1 i ) p(c M i |D M i ) â€¢ â€¢ â€¢ Mean Attention Weighting if epoch &gt; 2 p Att (c i |D i ) p CN N (c i |D i ) p BERT (c i |D i ) 1</formula><p>Figure <ref type="figure" coords="7,131.95,576.83,5.17,10.91" target="#fig_2">2</ref> illustrates the details of the BERTweet-CONV model. On the left, the CNN model based on the BERTweet embedding is shown. The concatenated embedding is the input of the CNN model. On the right side, the predicted probabilities of the BERTweet model for each sub-document of that author are utilized. The average of the probabilities of each sub-document ğ‘ ğµğ¸ğ‘…ğ‘‡ (ğ‘ ğ‘– |ğ· ğ‘– ) and the probabilities predicted by the CNN model ğ‘ ğ¶ğ‘ ğ‘ (ğ‘ ğ‘– |ğ· ğ‘– ) are input to an attention weighting block. The probability ğ‘ ğ´ğ‘¡ğ‘¡ (ğ‘ ğ‘– |ğ· ğ‘– ) on the right is obtained as follows:</p><formula xml:id="formula_3" coords="7,178.21,669.08,327.77,12.32">ğ‘ ğ´ğ‘¡ğ‘¡ (ğ‘ ğ‘– |ğ· ğ‘– ) = w 1 â€¢ ğ‘ ğµğ¸ğ‘…ğ‘‡ (ğ‘ ğ‘– |ğ· ğ‘– ) + w 2 â€¢ ğ‘ ğ¶ğ‘ ğ‘ (ğ‘ ğ‘– |ğ· ğ‘– ),<label>(1)</label></formula><p>where</p><formula xml:id="formula_4" coords="8,190.79,100.52,315.20,12.32">w = softmax(FC(ğ‘ ğµğ¸ğ‘…ğ‘‡ (ğ‘ ğ‘– |ğ· ğ‘– ); ğ‘ ğ¶ğ‘ ğ‘ (ğ‘ ğ‘– |ğ· ğ‘– ))).<label>(2)</label></formula><p>During the first two epochs, only the CNN model parameters are trained because in comparison with the CNN, the initial BERTweet model predictions are too accurate early in the training, which could dominate the entire model. After two epochs, the final probability is predicted using attention-weighting for both model type outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Setup</head><p>Participants can access only the training set at the beginning. For each DNN model, our work first uses the Python RAY<ref type="foot" coords="8,206.25,231.61,3.71,7.97" target="#foot_7">8</ref> package to find the best hyperparameters. The training set has a total of 420 author samples. To find the best set of hyperparameters, 100 authors were randomly selected as the internal test set and the remaining 320 authors were used for the internal training set. When the best set of hyperparameters is found, 4-fold cross-validation is applied to test the robustness of the model. For this purpose, 105 authors are used as the internal test set in each fold. We used the scikit-learn library GridSearchCV package to find the optimal hyperparameters for the SVM and LR models, while the RandomizedSearchCV package is applied for the RF model. Table <ref type="table" coords="8,231.52,328.21,5.07,10.91" target="#tab_1">2</ref> lists the optimal hyperparameters of different DNN models. As previously described, we use the re-segmentation strategy to improve the performance of the BERT model. However, the re-segmentation leads to a problem: the originally balanced dataset is now imbalanced. The cross-entropy (CE) loss function for binary classification</p><formula xml:id="formula_5" coords="8,263.10,536.30,242.89,11.50">CE = -log(ğ‘ ğ‘˜ )<label>(3)</label></formula><p>generally does not perform well on imbalanced data <ref type="bibr" coords="8,323.42,560.81,16.24,10.91" target="#b37">[38]</ref>. In contrast to the cross-entropy, the focal loss function <ref type="bibr" coords="8,173.42,574.36,17.91,10.91" target="#b38">[39]</ref> for binary classification</p><formula xml:id="formula_6" coords="8,245.41,597.10,260.58,13.27">F = -(1 -ğ‘ ğ‘˜ ) ğ›¾ log(ğ‘ ğ‘˜ )<label>(4)</label></formula><p>assigns a larger weight to poorly estimated training samples.</p><p>This work adopts the Cross-Entropy Weighted Focal (CEWF) loss function <ref type="bibr" coords="9,432.56,86.97,17.86,10.91" target="#b37">[38]</ref> to deal with this problem during the model training. It is defined as</p><formula xml:id="formula_7" coords="9,198.11,122.77,307.88,26.84">CEWF = - ğ‘’ (1-ğ‘ ğ‘˜ )ğ‘¡ + ğ‘’ ğ‘ ğ‘˜ ğ‘¡ (1 -ğ‘ ğ‘˜ ) ğ›¾ ğ‘’ ğ‘ ğ‘˜ ğ‘¡ + ğ‘’ (1-ğ‘ ğ‘˜ )ğ‘¡ log(ğ‘ ğ‘˜ ),<label>(5)</label></formula><p>where ğ‘ ğ‘˜ is the estimated target probability. The CEWF is a compromise between the CE and focal loss functions. When the classifier is very confident about the classification probability, the CEWF is close to CE; otherwise, the CEWF is close to the focal loss function. In our work, we set ğ‘¡ = 4 and ğ›¾ = 5 in Equation <ref type="formula" coords="9,249.13,199.49,3.74,10.91" target="#formula_7">5</ref>.</p><p>The RoBERTa and BERTweet models make predictions ğ‘ ğ‘š ğ‘– for each segment ğ· ğ‘š ğ‘– because of the re-segmentation strategy. The prediction of one author's class ğ‘ ğ‘– is obtained by applying hard voting (HV) over all of the sub-segments of this author:</p><formula xml:id="formula_8" coords="9,241.10,265.47,264.88,14.19">ğ‘ ğ‘– = mode(ğ‘ 0 ğ‘– , ğ‘ 1 ğ‘– . . . ğ‘ ğ‘€ ğ‘– ).<label>(6)</label></formula><p>To implement the ensemble approach, we also use the soft voting method (SV) to obtain the class probability of each author:</p><formula xml:id="formula_9" coords="9,177.51,322.06,328.47,14.19">ğ‘(ğ‘ ğ‘– |ğ· ğ‘– ) = mean(ğ‘(ğ‘ 0 ğ‘– |ğ· 0 ğ‘– ), ğ‘(ğ‘ 1 ğ‘– |ğ· 1 ğ‘– ) . . . ğ‘(ğ‘ ğ‘€ ğ‘– |ğ· ğ‘€ ğ‘– )).<label>(7)</label></formula><p>All our models are trained by the PyTorch library <ref type="foot" coords="9,332.71,345.59,3.71,7.97" target="#foot_8">9</ref> . Early stopping prevents overfitting. Specifically, the training is terminated if the evaluation accuracy does not improve within four epochs. The AdamW optimizer optimizes the parameters of the BERT model; the get_linear _schedule_with_warmup from the transformer model is used to schedule the learning rate. The learning rate has a warm-up process in the first four epochs, its upper limit is given in Table <ref type="table" coords="9,500.34,401.54,3.66,10.91" target="#tab_1">2</ref>. Other models are optimized by Adam; the learning rate is scheduled by ReduceLROnPlateau, i.e., the learning rate is reduced by 50% if the evaluation accuracy does not improve. For the BERT fine-tuning models, the segment length ğ‘ is 500 with an overlap of ğ‘‚ = 128. Since the BERT-CONV model is based on the trained BERTweet model, the order of the author's tweets is shuffled to avoid overfitting during the BERT-CONV model training phase; the segment length ğ‘ is still 500, while the overlap length ğ‘‚ is 64. The output dimension in all models is 2 with a softmax output function. The training process is carried out on NVIDIA's Volta-based DGX-1 multi-GPU system, using 2 TeslaV100 GPUs with 32 GB memory each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>Our experimental results are presented in this section.  Having established this, we focus on the BERTweet model with different loss functions. However, the results of the three loss functions are not significantly different. The best average accuracy is the BERTweet model with the CEWF loss function. Due to the outstanding performance of the BERTweet + CEWF model, the feature-based BERTweet-CONV model utilizes the BERTweet + CEWF to extract the embeddings as the fixed features. However, the feature-based strategy did not improve model accuracy on average. This conclusion is also in line with what was claimed in <ref type="bibr" coords="10,182.59,364.60,11.38,10.91" target="#b2">[3]</ref>, although, interestingly, the BERTweet-CONV model achieves the best accuracy in one of the folds. Ultimately, the soft voting ensemble learning method is implemented to boost the final performance. The author class probability ğ‘(ğ‘ ğ‘– |ğ· ğ‘– ) of the BERTweet model is obtained through Equation <ref type="formula" coords="10,131.07,586.44,3.66,10.91" target="#formula_9">7</ref>. We integrate the BERTweet models with different loss functions, as well as the BERT feature-based CNN model BERTweet-CONV. The ensemble model achieves the best accuracy on average (ensemble1,2,3,4 in Table <ref type="table" coords="10,225.59,613.54,3.57,10.91" target="#tab_4">4</ref>).</p><p>For comparison and completeness, we also give the results of the 'classical' baseline modelsthe SVM, LR, and RF models-in Table <ref type="table" coords="10,261.69,640.64,3.76,10.91" target="#tab_6">5</ref>. These three models are trained on the same internal training and test sets, i.e., 320 author samples are used for training; 100 author samples form the test set. Comparing Table <ref type="table" coords="10,222.34,667.74,5.05,10.91" target="#tab_4">4</ref> and Table <ref type="table" coords="10,275.90,667.74,3.72,10.91" target="#tab_6">5</ref>, all results of the BERTweet model are much better than those of the SVM, LR, and RF models. Among these three models, the LR, and RF models are inferior to the SVM model.</p><p>The evaluation on the official test set was performed on the TIRA platform <ref type="bibr" coords="11,441.27,191.25,16.40,10.91" target="#b39">[40]</ref>. The hard voting of the 4-fold BERTweet + CEWF model achieves an accuracy of 98.89%. The soft voting of the 4-fold ensemble1,2,3,4 model improves the performance to 99.44% on the official PAN test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>This work describes our models for the PAN 2022 challenge, which poses the task of identifying whether an author is spreading sarcasm in their tweets. By using a re-segmentation strategy for lengthy documents, we can overcome the text length limitation of the BERT model. We compare two BERT models, specifically looking at the Roberta model in comparison to the BERTweet model. Our experiments show that the BERTweet model is clearly more suitable for the sarcasm discrimination task in twitter data. Based on the BERTweet model, a feature-based model is also designed. However, the feature-based model can not improve the accuracy compared to fine-tuning the BERT model on average. Nevertheless, the advantage of the feature-based model cannot be neglected: Compared to fine-tuning a BERT model, training a feature-based model is faster, while the results are also comparable to the fine-tuned BERT model. In this work, we consider three different loss functions, seeing that the cross-entropy weighted focal loss function as a compromise between the cross-entropy and the focal loss function yields slightly better results. Three conventional classifiers are also evaluated, namely, SVM, LR, and RF models, but the BERTweet model far outperforms these traditional classifiers. Finally, our experiments demonstrate that an ensemble approach based on soft voting of BERTweet models with different loss functions and the feature-based CNN model can further boost performance on the PAN challenge task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,347.87,390.33,3.71,7.97;4,354.80,392.09,36.22,10.91;4,129.28,406.99,156.20,10.91;4,129.28,421.90,162.05,10.91;4,107.28,439.93,398.71,10.91;4,116.56,453.48,390.62,10.91;4,116.56,467.03,284.92,10.91"><head></head><label></label><figDesc>4 package normalize all text into lowercase remove punctuation and numbers â€¢ Scheme 2: for the bertweet-large model. The BERTweet model has an embedded text normalization. Therefore, we only change the text to fit the BERTweet text style, then process the input text with BERTweet's text normalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,218.86,617.14,287.13,10.91;6,89.29,630.69,71.39,10.91;6,160.98,629.47,7.49,6.99;6,160.68,636.49,2.88,6.99;6,168.97,630.69,218.03,10.91;6,387.00,629.47,7.49,6.99;6,387.00,636.49,2.88,6.99;6,397.73,630.69,108.26,10.91;6,89.29,644.24,45.57,10.91;6,134.86,643.02,7.49,6.99;6,134.86,650.04,2.88,6.99;6,146.52,644.24,119.45,11.36;6,269.48,644.24,236.50,10.91;7,89.29,86.97,416.69,10.91;7,89.29,100.52,362.33,10.91;7,451.62,99.29,7.49,6.99;7,451.62,106.31,2.88,6.99;7,459.61,101.25,12.06,9.57;7,471.97,99.29,7.49,6.99;7,471.67,106.31,2.88,6.99;7,479.96,100.52,26.02,10.91;7,89.29,114.06,226.96,10.91;7,316.25,112.71,7.49,6.99;7,316.25,119.72,2.88,6.99;7,324.24,114.06,2.40,10.91"><head>Figure 1</head><label>1</label><figDesc>depicts an overview of the fine-tuning process for the BERT model. ğ· ğ‘š ğ‘– , ğ‘š âˆˆ ğ‘€ is the ğ‘šth sub-document of author ğ‘–; ğ‘ ğ‘š ğ‘– is the predicted class, in our case ğ‘ ğ‘š ğ‘– âˆˆ [0, 1]. The input text ğ· ğ‘– is processed using the above text preprocessing and re-segmentation strategy. The BERT model takes one sub-document at a time and gives two outputs. One is the probability predicted based on the current sub-document ğ‘(ğ‘ ğ‘š ğ‘– |ğ· ğ‘š ğ‘– ), and the other is the corresponding word-embeddings E ğ‘š ğ‘– .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,89.29,371.27,314.51,8.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: BERTweet-CONV model, with FC denoting a fully connected layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,90.49,377.29,144.62"><head>Table 1</head><label>1</label><figDesc>PAN author profiling task timeline. 2013 â€¢ multilingual, predicting authors' age and gender 2014 â€¢ multilingual, predicting authors' age and gender 2015 â€¢ multilingual, predicting authors' age and gender, scoring five personalities 2016 â€¢ multilingual, predicting authors' age and gender 2017 â€¢ multilingual, predicting authors' gender and language variety 2018 â€¢ multilingual, multi-modal, identification authors' gender 2019 â€¢ multilingual, bot detection, human authors' gender identification 2020 â€¢ multilingual, fake news spreader identification 2021 â€¢ multilingual, hate speech spreader identification 2022 â€¢ monolingual, irony spreader identification</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,88.99,357.22,406.02,111.12"><head>Table 2</head><label>2</label><figDesc>Optimal hyperparameters of different mdoels</figDesc><table coords="8,100.26,385.30,394.76,83.03"><row><cell></cell><cell>lr</cell><cell cols="3">Batch size Epochs Optimizer</cell><cell>Scheduler</cell></row><row><cell>CNN</cell><cell>0.001</cell><cell>4</cell><cell>25</cell><cell>Adam</cell><cell>ReduceLROnPlateau</cell></row><row><cell>RoBERTa-ext</cell><cell>5e-5</cell><cell>16</cell><cell>1</cell><cell cols="2">AdamW get_linear_schedule_with_warmup</cell></row><row><cell>BERTweet-ext</cell><cell>1e-5</cell><cell>16</cell><cell>3</cell><cell cols="2">AdamW get_linear_schedule_with_warmup</cell></row><row><cell>RoBERTa</cell><cell>1e-5</cell><cell>16</cell><cell>3</cell><cell cols="2">AdamW get_linear_schedule_with_warmup</cell></row><row><cell>BERTweet</cell><cell>1e-5</cell><cell>4</cell><cell>3</cell><cell cols="2">AdamW get_linear_schedule_with_warmup</cell></row><row><cell cols="2">BERTweet-CONV 2e-2</cell><cell>4</cell><cell>4</cell><cell>Adam</cell><cell>ReduceLROnPlateau</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,88.96,568.41,418.70,92.20"><head>Table 3</head><label>3</label><figDesc>below lists the results of the two selected BERT models, which are trained on the external training set. Our training results are similar to those listed on TweetEval's leaderboard. The BERTweet-ext model clearly outperforms the RoBERTa-ext model on sarcasm detection.Table4lists the experimental results of the previously described models on the PAN database. The final prediction of an author is obtained by hard voting. All models are trained under the same 4-fold cross-validation, i.e., each model's internal training and test sets are identical in each</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,88.96,90.49,417.02,190.18"><head>Table 3</head><label>3</label><figDesc>The accuracy of the BERTweet-ext and RoBERTa-ext models, trained on external datasets with Cross Entropy (CE) as the loss function. The CNN model achieves better results than the RoBERTa model. The advantage of the CNN model is that its training time is much faster than fine-tuning a BERT model. Comparing the two BERT models, the BERTweet model is again much more effective. Under the same loss function, the BERTweet + CE model performs a relative error rate reduction by 7.091% compared to the RoBERTa + CE model on average. One possible reason is that the BERTweet model was pre-trained with a vast amount of Twitter data, and the PAN database was also collected from Twitter.</figDesc><table coords="10,89.29,130.53,282.61,68.84"><row><cell>Loss</cell><cell>Acc.</cell></row><row><cell cols="2">RoBERTa-ext + CE 0.599 0.670</cell></row><row><cell cols="2">BERTweet-ext + CE 0.409 0.872</cell></row><row><cell>fold.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,88.98,407.10,360.83,135.83"><head>Table 4</head><label>4</label><figDesc>Accuracy comparison between different models and setups.</figDesc><table coords="10,144.36,435.19,305.45,107.74"><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>meanÂ±std</cell></row><row><cell>CNN</cell><cell cols="5">0.933 0.895 0.924 0.895 0.912Â±0.020</cell></row><row><cell>RoBERTa + CE</cell><cell cols="5">0.905 0.933 0.895 0.829 0.891Â±0.044</cell></row><row><cell>BERTweet + CE (1)</cell><cell cols="5">0.952 0.981 0.971 0.933 0.959Â±0.021</cell></row><row><cell>BERTweet + F (2)</cell><cell cols="5">0.971 0.981 0.962 0.924 0.960Â±0.025</cell></row><row><cell>BERTweet + CEWF (3)</cell><cell cols="5">0.962 0.971 0.990 0.933 0.964Â±0.024</cell></row><row><cell cols="6">BERTweet-CONV + CE (4) 0.952 0.962 1.000 0.923 0.959Â±0.031</cell></row><row><cell>ensemble1,2,3</cell><cell cols="5">0.971 0.981 0.981 0.933 0.966Â±0.020</cell></row><row><cell>ensemble1,2,3,4</cell><cell>0</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,262.95,533.97,187.97,8.93"><head>.971 0.981 1.000 0.923 0.969Â±0.028</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="11,88.98,90.49,266.71,49.31"><head>Table 5</head><label>5</label><figDesc>Accuracy of SVM, LR, and RF, evaluated on 100 author samples.</figDesc><table coords="11,239.58,118.58,116.11,21.22"><row><cell>SVM</cell><cell>LR</cell><cell>RF</cell></row><row><cell cols="3">Acc. 0.900 0.890 0.860</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,671.03,254.42,8.97"><p>https://help.twitter.com/en/rules-and-policies/hateful-conduct-policy</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,108.93,649.11,143.83,8.97"><p>https://github.com/cardiffnlp/tweeteval</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,108.93,660.07,166.14,8.97"><p>https://github.com/VinAIResearch/BERTweet</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,108.93,671.03,133.82,8.97"><p>https://github.com/carpedm20/emoji</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,108.93,660.08,339.91,8.97"><p>https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="5,108.93,671.04,370.84,8.97"><p>https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="6,108.93,671.03,143.83,8.97"><p>https://github.com/cardiffnlp/tweeteval</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="8,108.93,670.98,126.08,8.97"><p>https://github.com/ray-project/ray</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="9,108.93,671.04,130.30,8.97"><p>https://github.com/pytorch/pytorch</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The work was supported by the <rs type="funder">PhD School "SecHuman</rs> <rs type="projectName">-Security for Humans in Cyberspace</rs>" by the federal state of NRW, and partially funded by the <rs type="funder">Deutsche Forschungsgemeinschaft (DFG -German Research Foundation</rs>) [Project<rs type="grantNumber">-ID 429873205</rs>] and by the <rs type="funder">German Federal Ministry of Education and Research</rs> [Grant No: <rs type="grantNumber">16KIS1518K</rs>]. The authors are responsible for the content of this publication.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_2N4NdQQ">
					<orgName type="project" subtype="full">-Security for Humans in Cyberspace</orgName>
				</org>
				<org type="funding" xml:id="_GzSfage">
					<idno type="grant-number">-ID 429873205</idno>
				</org>
				<org type="funding" xml:id="_6M9nhaY">
					<idno type="grant-number">16KIS1518K</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,112.66,111.28,61.72,10.91;12,192.90,111.28,213.21,10.91;12,424.62,111.28,83.21,10.91;12,112.66,124.83,394.04,10.91;12,112.66,138.38,185.33,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,192.90,111.28,208.11,10.91">New Tweets per second record, and how!</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Krikorian</surname></persName>
		</author>
		<ptr target="https://blog.twitter.com/engineering/en_us/a/2013/new-tweets-per-second-record-and-how" />
	</analytic>
	<monogr>
		<title level="m" coord="12,446.98,111.28,60.85,10.91;12,112.66,124.83,39.49,10.91">Twitter Official Blog</title>
		<imprint>
			<date type="published" when="2013-08-16">August 16, 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,151.93,393.33,10.91;12,112.66,165.48,209.61,10.91" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="12,328.20,151.93,177.79,10.91;12,112.66,165.48,134.42,10.91">Detection of hate speech spreaders using convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Siino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Di Nuovo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Tinnirello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">La</forename><surname>Cascia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,179.03,393.33,10.91;12,112.66,192.57,363.59,10.91" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="12,323.15,179.03,182.83,10.91;12,112.66,192.57,181.08,10.91">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,112.66,206.12,393.33,10.91;12,112.66,219.67,395.01,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="12,208.09,206.12,77.17,10.91;12,309.07,206.12,196.92,10.91;12,112.66,219.67,263.15,10.91">Handbook of research on machine learning applications and trends: algorithms, methods, and techniques</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Torrey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shavlik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>IGI global</publisher>
			<biblScope unit="page" from="242" to="264" />
		</imprint>
	</monogr>
	<note>Transfer learning</note>
</biblStruct>

<biblStruct coords="12,112.66,233.22,393.33,10.91;12,112.66,246.77,393.33,10.91;12,112.28,260.32,203.44,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,361.73,233.22,144.26,10.91;12,112.66,246.77,54.92,10.91">Overview of the author profiling task at PAN</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Inches</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,216.88,246.77,289.11,10.91;12,112.28,260.32,78.68,10.91">CLEF Conference on Multilingual and Multimodal Information Access Evaluation</title>
		<imprint>
			<publisher>CELCT</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="352" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,273.87,395.17,10.91;12,112.66,287.42,393.33,10.91;12,112.66,300.97,349.52,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,142.86,287.42,220.84,10.91">Overview of the 2nd author profiling task at PAN</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Chugur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Trenkmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Verhoeven</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,407.81,287.42,98.17,10.91;12,112.66,300.97,188.64,10.91">CLEF 2014 Evaluation Labs and Workshop Working Notes Papers</title>
		<meeting><address><addrLine>Sheffield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014. 2014</date>
			<biblScope unit="page" from="1" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,314.52,393.33,10.91;12,112.66,328.07,393.33,10.91;12,112.14,341.62,165.35,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,449.85,314.52,56.14,10.91;12,112.66,328.07,184.01,10.91">Overview of the 3rd Author Profiling Task at PAN 2015</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Rangel Pardo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Celli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,319.53,328.07,186.46,10.91;12,112.14,341.62,97.74,10.91">CLEF 2015 Evaluation Labs and Workshop Working Notes Papers</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,355.17,393.32,10.91;12,112.41,368.71,393.58,10.91;12,112.66,382.26,394.10,10.91;12,112.66,395.81,107.02,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,433.13,355.17,72.85,10.91;12,112.41,368.71,272.00,10.91">Overview of the 4th author profiling task at PAN 2016: cross-genre evaluations</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Verhoeven</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,407.20,368.71,98.78,10.91;12,112.66,382.26,147.72,10.91">Working Notes Papers of the CLEF 2016 Evaluation Labs</title>
		<title level="s" coord="12,267.67,382.26,135.34,10.91">CEUR Workshop Proceedings/</title>
		<editor>et al.</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="750" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,409.36,393.33,10.91;12,112.66,422.91,393.33,10.91;12,112.66,436.46,105.38,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,292.68,409.36,213.31,10.91;12,112.66,422.91,258.53,10.91">Overview of the 5th author profiling task at PAN 2017: gender and language variety identification in twitter</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,379.23,422.91,126.76,10.91;12,112.66,436.46,24.17,10.91">Working notes papers of the CLEF</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1613" to="0073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,450.01,393.53,10.91;12,112.66,463.56,393.33,10.91;12,112.66,477.11,141.09,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,387.01,450.01,119.18,10.91;12,112.66,463.56,316.81,10.91">Overview of the 6th author profiling task at PAN 2018: multimodal gender identification in twitter</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y GÃ³mez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,437.54,463.56,68.44,10.91;12,112.66,477.11,85.24,10.91">Working Notes Papers of the CLEF</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,490.66,393.53,10.91;12,112.66,504.21,393.33,10.91;12,112.66,517.76,137.73,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,198.13,490.66,308.06,10.91;12,112.66,504.21,81.53,10.91">Overview of the 7th author profiling task at PAN 2019: bots and gender profiling in twitter</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,216.88,504.21,252.44,10.91;12,147.28,517.76,72.33,10.91">Working Notes Papers of the CLEF 2019 Evaluation Labs</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2380</biblScope>
		</imprint>
	</monogr>
	<note>CEUR Workshop</note>
</biblStruct>

<biblStruct coords="12,112.66,531.30,395.17,10.91;12,112.66,544.85,393.33,10.91;12,112.66,558.40,302.63,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,359.44,531.30,148.39,10.91;12,112.66,544.85,291.75,10.91">Overview of the 8th author profiling task at PAN 2020: Profiling fake news spreaders on twitter</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">H H</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,428.85,544.85,77.14,10.91;12,112.66,558.40,51.81,10.91">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>Sun SITE Central Europe</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2696</biblScope>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,571.95,393.32,10.91;12,112.66,585.50,181.44,10.91" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="12,352.21,571.95,153.77,10.91;12,112.66,585.50,85.73,10.91">Profiling hate speech spreaders on twitter task at PAN</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>SarracÃ©n</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chulvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,599.05,393.33,10.91;12,112.66,612.60,279.82,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">C M M K</forename><surname>BrodziÅ¥ska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Celmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Patera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pezacki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wilk</surname></persName>
		</author>
		<title level="m" coord="12,432.90,599.05,73.08,10.91;12,112.66,612.60,247.90,10.91">Ensemble-based classification for author profiling using various features</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,626.15,393.54,10.91;12,112.66,639.70,229.44,10.91" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="12,312.13,626.15,194.07,10.91;12,112.66,639.70,174.50,10.91">Author profiling: Predicting age and gender from blogs, Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Santosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Varma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,653.25,393.32,10.91;12,112.66,666.80,362.44,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,424.78,653.25,81.20,10.91;12,112.66,666.80,139.38,10.91">Using intra-profile information for author profiling</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>LÃ³pez-Monroy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y GÃ³mez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">V</forename><surname>Pineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,278.46,666.80,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1116" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,86.97,394.53,10.91;13,112.66,100.52,393.32,10.91;13,112.66,114.06,135.11,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,191.83,100.52,240.03,10.91">INAOE&apos;s participation at PAN&apos;15: Author profiling task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Alvarez-Carmona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>LÃ³pez-Monroy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes-Y GÃ³mez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Villasenor-Pineda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jair-Escalante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,439.84,100.52,66.15,10.91;13,112.66,114.06,85.24,10.91">Working Notes Papers of the CLEF</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">103</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,127.61,394.53,10.91;13,112.66,141.16,360.95,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">B</forename><surname>Vollenbroek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Carlotto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kreutz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Medvedeva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bjerva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Haagsma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Nissim</surname></persName>
		</author>
		<title level="m" coord="13,163.93,141.16,277.76,10.91">Gronup: Groningen user profiling, Notebook for PAN at CLEF</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,154.71,393.73,10.91;13,112.66,168.26,330.77,10.91" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Dwyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Medvedeva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rawee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Haagsma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Nissim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03764</idno>
		<title level="m" coord="13,444.84,154.71,61.55,10.91;13,112.66,168.26,148.08,10.91">N-gram: new groningen author-profiling model</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,112.66,181.81,373.65,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="13,159.51,181.81,171.94,10.91">Using n-grams to detect bots on twitter</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pizarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,357.69,181.81,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,195.36,393.61,10.91;13,112.66,208.91,288.44,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="13,204.14,195.36,302.13,10.91;13,112.66,208.91,133.59,10.91">An ensemble model using n-grams and statistical features to identify fake news spreaders on twitter</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Bolonyai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,272.48,208.91,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,222.46,393.33,10.91;13,112.66,236.01,393.32,10.91;13,112.66,249.56,73.01,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="13,437.86,222.46,68.12,10.91;13,112.66,236.01,273.94,10.91">Text and image synergy with feature cross technique for gender identification</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tahara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nagatani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Taniguchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ohkuma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,394.64,236.01,111.34,10.91;13,112.66,249.56,41.09,10.91">Working Notes Papers of the CLEF</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,263.11,395.17,10.91;13,112.66,276.66,394.53,10.91;13,112.66,290.20,90.72,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="13,148.82,276.66,106.21,10.91">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Å</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,278.07,276.66,224.48,10.91">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,303.75,393.32,10.91;13,112.66,317.30,393.59,10.91;13,112.33,330.85,29.19,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="13,390.98,303.75,115.00,10.91;13,112.66,317.30,263.31,10.91">Author profiling on social media: An ensemble learning model using various features</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,384.18,317.30,122.07,10.91">Notebook for PAN at CLEF</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,344.40,393.33,10.91;13,112.66,357.95,204.48,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="13,315.09,344.40,190.90,10.91;13,112.66,357.95,47.24,10.91">Automatic detection of fake news spreaders using BERT</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Baruah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">A</forename><surname>Barbhuiya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,188.52,357.95,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,371.50,393.33,10.91;13,112.66,385.05,220.28,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="13,322.22,371.50,183.77,10.91;13,112.66,385.05,64.51,10.91">Ensemble of ELECTRA for profiling fake news spreaders</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Baruah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">A</forename><surname>Barbhuiya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,204.32,385.05,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,398.60,394.62,10.91;13,112.66,412.15,128.62,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="13,213.53,398.60,269.37,10.91">A BERT based two-stage fake news spreader profiling system</title>
		<author>
			<persName coords=""><forename type="first">S.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-L</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,112.66,412.15,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,425.70,395.17,10.91;13,112.66,439.25,70.36,10.91" xml:id="b27">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<title level="m" coord="13,265.76,425.70,242.07,10.91;13,112.66,439.25,38.44,10.91">BERT-AL: BERT for arbitrarily long document understanding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,452.79,393.32,10.91;13,112.66,466.34,395.01,10.91" xml:id="b28">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08167</idno>
		<title level="m" coord="13,314.64,452.79,191.34,10.91;13,112.66,466.34,216.25,10.91">Multi-passage BERT: A globally normalized BERT model for open-domain question answering</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,112.66,479.89,335.42,10.91" xml:id="b29">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dukic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>KrÅ¾ic</surname></persName>
		</author>
		<title level="m" coord="13,209.61,479.89,206.55,10.91">Detection of hate speech spreaders with BERT</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,493.44,394.53,10.91;13,112.66,506.99,395.17,10.91;13,112.66,520.54,393.32,10.91;13,112.66,534.09,393.32,10.91;13,112.66,547.64,395.17,10.91;13,112.66,561.19,393.33,10.91;13,112.66,574.74,146.92,10.91" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="13,251.61,520.54,254.37,10.91;13,112.66,534.09,268.21,10.91">Overview of PAN 2022: Authorship Verification, Profiling Irony and Stereotype Spreaders, and Style Change Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chulvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Heini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kredens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mayerl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ortega-Bueno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pezik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wolska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zangerle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,404.06,534.09,101.93,10.91;13,112.66,547.64,395.17,10.91;13,112.66,561.19,218.18,10.91">Proceedings of the Thirteenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="13,437.02,562.20,68.96,9.72;13,112.66,575.75,74.59,9.72">Lecture Notes in Computer Science</title>
		<meeting>the Thirteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="volume">13390</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="13,112.66,588.29,393.33,10.91;13,112.66,601.84,394.53,10.91;13,112.66,615.39,172.05,10.91" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="13,373.25,588.29,132.74,10.91;13,112.66,601.84,219.45,10.91">Profiling Irony and Stereotype Spreaders on Twitter (IROSTEREO) at PAN 2022</title>
		<author>
			<persName coords=""><forename type="first">O.-B</forename><surname>Reynier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Berta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Elisabetta</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="13,358.04,601.84,143.79,10.91">CLEF 2022 Labs and Workshops</title>
		<title level="s" coord="13,112.66,615.39,103.05,10.91">Notebook Papers, CEUR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,628.93,395.17,10.91;13,112.66,642.48,393.58,10.91;13,112.33,656.03,29.19,10.91" xml:id="b32">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Espinosa-Anke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12421</idno>
		<title level="m" coord="13,389.99,628.93,117.84,10.91;13,112.66,642.48,246.67,10.91">Tweeteval: Unified benchmark and comparative evaluation for tweet classification</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,112.66,669.58,393.32,10.91;14,112.66,86.97,209.16,10.91" xml:id="b33">
	<monogr>
		<title level="m" type="main" coord="13,271.07,669.58,234.91,10.91;14,112.66,86.97,26.95,10.91">BERTweet: A pre-trained language model for english tweets</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10200</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,100.52,395.17,10.91;14,112.66,114.06,298.89,10.91" xml:id="b34">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Halko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Martinsson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tropp</surname></persName>
		</author>
		<title level="m" coord="14,269.65,100.52,238.18,10.91;14,112.66,114.06,266.97,10.91">Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,127.61,393.33,10.91;14,112.33,141.16,393.65,10.91;14,112.28,154.71,387.33,10.91" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="14,302.83,127.61,203.16,10.91;14,112.33,141.16,145.56,10.91">Humans require context to infer ironic intent (so computers probably do, too)</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kertz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,285.64,141.16,220.34,10.91;14,112.28,154.71,188.23,10.91">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="512" to="516" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct coords="14,112.66,168.26,394.53,10.91;14,112.66,181.81,394.61,10.91;14,112.66,195.36,152.61,10.91" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="14,260.14,168.26,242.55,10.91">SemEval-2018 task 3: Irony detection in English tweets</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Van Hee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Lefever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Hoste</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/S18-1005" />
	</analytic>
	<monogr>
		<title level="m" coord="14,127.34,181.81,325.02,10.91">Proceedings of The 12th International Workshop on Semantic Evaluation</title>
		<meeting>The 12th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,208.91,393.61,10.91;14,112.66,222.46,331.54,10.91" xml:id="b37">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04197</idno>
		<title level="m" coord="14,274.82,208.91,231.45,10.91;14,112.66,222.46,149.38,10.91">BERT-based chinese text classification for emergency domain with a novel loss function</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,236.01,394.61,10.91;14,112.66,249.56,395.01,10.91" xml:id="b38">
	<analytic>
		<title level="a" type="main" coord="14,327.53,236.01,159.84,10.91">Focal loss for dense object detection</title>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,112.66,249.56,299.48,10.91">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,263.11,394.53,10.91;14,112.66,276.66,393.33,10.91;14,112.66,290.20,394.51,10.91;14,112.66,306.20,123.08,7.90" xml:id="b39">
	<analytic>
		<title level="a" type="main" coord="14,327.46,263.11,175.13,10.91">TIRA Integrated Research Architecture</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-22948-1_5</idno>
	</analytic>
	<monogr>
		<title level="m" coord="14,240.99,276.66,264.99,10.91;14,112.66,290.20,123.97,10.91">Information Retrieval Evaluation in a Changing World, The Information Retrieval Series</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
