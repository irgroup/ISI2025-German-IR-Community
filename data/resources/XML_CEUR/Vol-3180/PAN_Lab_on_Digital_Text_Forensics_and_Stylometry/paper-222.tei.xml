<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,371.44,15.42;1,89.29,106.66,365.16,15.42;1,89.29,129.00,157.29,11.96">Identifying Author Profiles Containing Irony or Spreading Stereotypes with SBERT and Emojis Notebook for PAN at CLEF 2022</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,154.90,65.05,11.96"><forename type="first">Narjes</forename><surname>Tahaei</surname></persName>
							<email>narjesossadat.tahaei@mail.concordia.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Concordia University</orgName>
								<address>
									<settlement>Montreal</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,165.41,154.90,61.60,11.96"><forename type="first">Harsh</forename><surname>Verma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Concordia University</orgName>
								<address>
									<settlement>Montreal</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,238.77,154.90,90.16,11.96"><forename type="first">Parsa</forename><surname>Bagherzadeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Concordia University</orgName>
								<address>
									<settlement>Montreal</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,340.42,154.90,86.78,11.96"><forename type="first">Farhood</forename><surname>Farahnak</surname></persName>
							<email>farhood.farahnak@mail.concordia.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Concordia University</orgName>
								<address>
									<settlement>Montreal</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,438.60,154.90,64.93,11.96"><forename type="first">Nadia</forename><surname>Sheikh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Concordia University</orgName>
								<address>
									<settlement>Montreal</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,113.26,168.85,70.95,11.96"><forename type="first">Sabine</forename><surname>Bergler</surname></persName>
							<email>bergler@cse.concordia.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Concordia University</orgName>
								<address>
									<settlement>Montreal</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,371.44,15.42;1,89.29,106.66,365.16,15.42;1,89.29,129.00,157.29,11.96">Identifying Author Profiles Containing Irony or Spreading Stereotypes with SBERT and Emojis Notebook for PAN at CLEF 2022</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">874C7608B3BDA02220BDF7FEB16F7AC3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>SBERT</term>
					<term>Tokenization</term>
					<term>Emojis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Profiling Irony and Stereotype Spreaders on Twitter Shared Task at CLEF 2022 asks to analyze a set of tweets from an author in order to determine whether the author spreads irony and stereotypes. Our approach is to feed all tweets from an author to SBERT a a single input batch and feed all output vectors from the model to an additive attention layer to produce a vector representation per author. This vector representation is the input to a linear binary classifier. We placed second among 40 participants with a test accuracy of 97.8. We selected the best model using 5 fold cross-validation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Irony is the expression of one's meaning by using language that normally signifies the opposite <ref type="bibr" coords="1,89.29,404.97,11.58,10.91" target="#b0">[1]</ref>. Stereotype is a fixed, over-generalized belief about a particular group that can propagate false biases regarding that group <ref type="bibr" coords="1,239.83,418.52,11.58,10.91" target="#b1">[2]</ref>. Twitter is a widely used communication platform with a high percentage of tweets using irony and stereotypes. The Profiling Irony and Stereotype Spreaders on Twitter Shared Task <ref type="bibr" coords="1,245.34,445.62,13.00,10.91" target="#b2">[3]</ref> is to classify an author as someone who spreads irony and stereotypes from so-called Profiles of 200 tweets for each of 420 authors.</p><p>Current best practice for NLP tasks is to feed sentence input to a BERT-like pre-trained language model <ref type="bibr" coords="1,165.72,486.26,13.00,10.91" target="#b3">[4]</ref> and use the CLS output token as input to a classifier. If this task was to classify individual tweets as containing ironic or stereotypical content, this would be a satisfactory approach and would be expected to perform well. But this task calls for the classification of Profiles with 200 tweets each, with each tweet providing important context. We thus batch the data with batch size 200 to model the Profiles.</p><p>We fine-tune SBERT <ref type="bibr" coords="1,194.65,554.01,12.90,10.91" target="#b4">[5]</ref> to construct individual vector representations for each tweet from an author followed by an additive attention layer to calculate a vector representation for each Profile. Profile vectors feed into a linear classifier.</p><p>We submitted two runs which differ only in the number of epochs, for which the model ran. narcis ran for six epochs and obtained rank 10 with an accuracy of 0.93; harshv ran for seven epochs and obtained rank 2 with an accuracy of 0.98 <ref type="bibr" coords="2,332.61,114.06,11.59,10.91" target="#b5">[6]</ref>. The results were submitted to the TIRA platform <ref type="bibr" coords="2,156.69,127.61,11.43,10.91" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The authors of the top ranked system on SemEval15 Task 11 on Sentiment Analysis of Figurative Language in Twitter (Task 11) <ref type="bibr" coords="2,218.83,199.79,11.23,10.91" target="#b7">[8,</ref><ref type="bibr" coords="2,231.98,199.79,8.88,10.91" target="#b8">9]</ref> did not make any adaptations to a sentiment analysis pipeline aside from training on the training data. This suggests that sentiment (which is often inversed in ironic language) co-occurs with other cues that are strong enough for a normal sentiment analysis system to succeed.</p><p>In <ref type="bibr" coords="2,112.76,253.99,16.41,10.91" target="#b9">[10]</ref>, the authors applied supervised machine learning to a set of Twitter corpora that have been used earlier for the irony and sarcasm detection to identify ironic tweets. Different groups of features have been used. Structural features used to detect common patterns of the ironic tweets including length, type of punctuation, and emoticons. Other features used to capture information about affect, such as semantic lexicons and dictionaries of affective terms. Affective features outperformed in distinguishing among ironic and non-ironic tweets in all Twitter corpora.</p><p>The authors in <ref type="bibr" coords="2,166.55,348.83,17.76,10.91" target="#b10">[11]</ref> propose a model to detect hate spreaders on Twitter. BERTweet is used to embed each tweet into a vector representation. Two methods are used to construct an author's profile. First, a graph neural network is used to relate information from different posts to their authors. Second, a sequence of encoded tweets is given to an additive attention-based, fully connected neural network. The single vector coming from adding all weighted vectors is used by a classifier. To classify whether an author is irony and stereotype spreader or not a version of impostor method is introduced. In <ref type="bibr" coords="2,292.94,430.13,17.97,10.91" target="#b11">[12]</ref> the Impostors method is used to determine whether two documents are from the same author. The method checks whether X (from a hate spreader set), is closer to Y (from non-hate spreader set) than to each one of impostors, which is a similarity function here. The system proved useful for the hate speech spreaders identification shared task at PAN 2021. The second method, which is a sequence based Profile modeling used similar additive attention approach. They first applied the additive attention to obtain a vector representation for each tweet and then added them together to have a Profile representation. In contrast, we have all tweets of an author available for fine tuning and it is the additive attention layer which gives us a representation for a Profile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Model</head><p>For comparison, we feed the data to two classifiers, a SVM classifier using authors' TF-IDF vectors and a fine-tuned SBERT model.</p><p>For the SVM baseline, we construct TF-IDF vectors for all Profiles. Tokens that occur fewer than 30 times in the whole corpus were removed from the vocabulary. The final size of the vocabulary is 4128. The SVM baseline yields 86% accuracy on our evaluation set.</p><p>Our main model is based on SBERT <ref type="bibr" coords="3,255.70,179.03,11.20,10.91" target="#b4">[5]</ref>, a modification of BERT that is fine-tuned for sentence representations on NLI data to produce semantically meaningful sentence embeddings that can be compared using cosine-similarity. SBERT adds a pooling layer on top of BERT's 12 transformer layers, which produces a fixed size sentence embedding. For fine-tuning there is a siamese and triplet networks to update weights. During training process of finding the most similar sentence pairs, each sentence in a pair are inputs to the networks, which outputs two sentence embeddings. Then, the similarity of these embeddings is computed using cosine similarity between vectors. The resulting sentence embeddings can be used for different tasks.</p><p>SBERT used mean pooling for the semantic similarity task <ref type="bibr" coords="3,377.96,287.42,11.58,10.91" target="#b4">[5]</ref>. We experimented with combining the CLS token output with mean pooling, max pooling, or additive attention. For mean pooling and max pooling, we compute the mean and maximum of all 200 CLS outputs for a Profile. The additive attention on the 200 CLS outputs outperformed the other two methods.</p><p>Each training sample corresponds to a Profile and comprises 200 tweets. We use SBERT and input tweets with a batch size of 200:</p><formula xml:id="formula_0" coords="3,160.10,382.26,345.89,12.14">ğ» = [â„ ğ¶ğ¿ğ‘† 1 ; . . . ; â„ ğ¶ğ¿ğ‘† 200 ] = SBERT(ğ‘‡ ğ‘¤ğ‘’ğ‘’ğ‘¡ 1 , . . . , ğ‘‡ ğ‘¤ğ‘’ğ‘’ğ‘¡ 200 )<label>(1)</label></formula><p>where ğ» = [â„ ğ¶ğ¿ğ‘† 1 ; . . . ; â„ ğ¶ğ¿ğ‘† 200 ] is the row-level concatenation of the [CLS] representations of the 200 Profile tweets. To obtain a single representation for the authors' profile we then use additive attention:</p><formula xml:id="formula_1" coords="3,237.80,454.72,268.19,13.13">â„ = ğ‘ ğ‘œğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘Š ğ‘ğ‘¡ğ‘¡ ğ» ğ‘‡ )ğ»<label>(2)</label></formula><p>where ğ‘Š ğ‘ğ‘¡ğ‘¡ âˆˆ R 1Ã—384 is a learnable parameter. The additive attention assigns importance weights to tweets and provides a weighted sum of the [CLS] representation. The profile representation â„ is then used for the final classification:</p><formula xml:id="formula_2" coords="3,247.36,530.71,258.63,10.91">ğ‘ = ğ‘ ğ‘œğ‘¡ğ‘šğ‘ğ‘¥(â„ğ‘Š + ğ‘)<label>(3)</label></formula><p>where ğ‘Š âˆˆ R 384Ã—2 is a linear transformation that characterizes the classifier and ğ‘ âˆˆ R 2 represents the class probabilities. <ref type="foot" coords="3,234.46,562.53,3.71,7.97" target="#foot_1">1</ref> We calculate loss using Cross-Entropy loss and optimize the network using the Adam <ref type="bibr" coords="3,202.78,577.83,17.91,10.91" target="#b12">[13]</ref> optimizer with ğ‘™ğ‘Ÿ = 5ğ‘’ -6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Input representations</head><p>There are two ways to input the data into the model. One way is to assign the Profile's label to all 200 of its tweets and classify individual tweets. When we have assigned predictions to all tweets of a profile, they have to be aggregated, for instance with a majority vote or an additive attention layer to obtain Profile labels. We chose instead to represent Profiles in batches of size 200, putting all tweets from one Profile into a single batch. The Profile vector is obtained using an attention layer over the CLS tokens of all tweets in a batch. This high-level author encoding vector is used as input to the final layer for the classification.</p><p>We use the batch method for our system. During training, cost function and gradient descent are calculated for Profiles only. This is intended to avoid the noise created by over-assigning the Profile label to tweets that do not contribute to its assignment<ref type="foot" coords="4,374.03,193.61,3.71,7.97" target="#foot_2">2</ref> and produces better gradient descent and finally a richer final representation for a Profile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Model Architecture</head><p>The BERT tokenizer <ref type="foot" coords="4,181.87,265.78,3.71,7.97" target="#foot_3">3</ref> prepares the input for the model. It splits input into sub-word token strings, converts tokens to their ids, adds new tokens to the vocabulary, manages special tokens, pads, and truncates vectors. For each input batch, we first called the tokenizer along with the model. It turns out that the computational overhead of the subword tokenizer restricts the model to run properly on the large batch size. This was addressed by transferring the tokenizer outside the model.</p><p>We used PyTorch<ref type="foot" coords="4,174.95,347.08,3.71,7.97" target="#foot_4">4</ref> and the SBERT architecture implemented by Huggingface<ref type="foot" coords="4,430.88,347.08,3.71,7.97" target="#foot_5">5</ref> , and ran models on one GPU. The longest tweet has over 600 tokens, which made it impossible to give all Profile tweets as one batch to the SBERT model. Since only 25 tweets have between 200 to about 600 tokens, we truncated tweets to a maximal length of 200 tokens and set padding to True. Thus the model has the dimensions of 200 tweets * 200 tokens * 384 hidden layers (SBERT's default).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Feature Exploration</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Emojis</head><p>In many texts, specifically in the context of short texts like tweets, emojis serve as a proxy for the emotional contents of the text <ref type="bibr" coords="4,242.53,486.87,16.24,10.91" target="#b13">[14]</ref>. When using irony, authors may add emojis to the text to ensure that the double entendre does not go unnoticed. Emojis both, text descriptions and UNICODE values <ref type="foot" coords="4,166.74,512.22,3.71,7.97" target="#foot_6">6</ref> . We choose UNICODE values, to keep emojis distinct from text.</p><p>In order to use emojis in SBERT, we add their UNICODEs to the SBERT vocabulary with random weight vectors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We used 5-fold cross validation during training for fine-tuning and to select the best performing model. The splits were fixed by setting random state values to integer values. Adding emojis to the SBERT vocabulary improved performance during development and was retained for the submitted system. Table <ref type="table" coords="5,126.37,370.66,4.97,10.91" target="#tab_1">2</ref> shows the results of feeding all tweets from a user as a batch to BERT and SBERT. For this table, we truncated the maximum length of tokenized tweet to 100, due to the much higher dimension of hidden units in BERT which is 768. We limited the maximum length of tokenized tweet in SBERT to 100 as well. The result confirms that SBERT outperforms the BERT model. Table <ref type="table" coords="5,116.16,424.86,5.15,10.91" target="#tab_1">2</ref> shows that adding emojis slightly increases accuracy and most interestingly, reduces standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Results</head><p>Table <ref type="table" coords="5,116.87,497.03,5.17,10.91" target="#tab_2">3</ref> shows our two competition runs, of the same model. narcis ran for six epochs and obtained rank 10 with an accuracy of 0.93; harshv ran for seven epochs and obtained rank 2 with an accuracy of 0.98. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Analysis</head><p>To explore the features of the most important tweets for detecting authors who use irony, we selected the top 15 tweets according to attention scores. We counted the number of hashtags in the 15 tweets with highest attention scores across ironic and non ironic Profiles. Although all hashtags were mapped to HASHTAG, they are still good indicators for ironic profiles. There are authors in both groups who have multiple hashtags in their top ranked tweets, but ironic authors used them more frequently compared to others.</p><p>We also find that authors of ironic Profiles write more short tweets than non-ironic authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We focused on trying to preserve the context of an entire Profile and chose to encode Profiles as single batches of size 200 with additive attention. In experiments on the training data, this performed better than the alternative classification of individual tweets. We also saw an improvement in performance when adding all emojis to the language model vocabulary. And finally, SBERT, trained for sentence similarity, yielded better performance than BERT on the training data. The high performance of our system (run for 7 epochs: rank 2) indicates that the training data foreshadows the test data well. The performance of our system run for 6 epochs and rank 10 shows that subtle parameters can be more influential than system design.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,90.49,416.99,76.35"><head>Table 1</head><label>1</label><figDesc>F1 scores for positive class and the accuracy of predicting irony and stereotype profiles on validation data using BERT and SBERT models. Maximum length of tokens is 100.</figDesc><table coords="5,248.15,130.13,98.98,36.70"><row><cell cols="2">Model accuracy f1</cell></row><row><cell>BERT</cell><cell>87 87</cell></row><row><cell>SBERT</cell><cell>94 95</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.99,189.09,416.99,76.35"><head>Table 2</head><label>2</label><figDesc>Effect of adding emojis to the vocabulary. The f1 score and accuracy of predicting irony and stereotype Profiles on validation data based on SBERT models from 5 fold cross-validation on epoch 6.</figDesc><table coords="5,205.70,228.74,183.87,36.70"><row><cell>Model</cell><cell>accuracy</cell><cell>f1</cell><cell>std</cell></row><row><cell>SBERT with emojis</cell><cell cols="3">92.3 93.2 2.99</cell></row><row><cell>SBERT</cell><cell cols="3">91.6 91.6 3.49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,88.99,553.08,274.35,64.39"><head>Table 3</head><label>3</label><figDesc></figDesc><table coords="5,89.29,565.08,274.05,52.39"><row><cell>Competition results</cell><cell></cell><cell></cell></row><row><cell cols="3">submission epoch accuracy</cell></row><row><cell>narcis</cell><cell>6</cell><cell>0.93</cell></row><row><cell>harshv</cell><cell>7</cell><cell>0.98</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="2,88.96,597.15,417.02,10.91;2,89.29,610.69,416.90,10.91;2,89.29,624.24,416.69,10.91;2,89.29,637.79,378.78,10.91;2,100.20,651.34,405.79,10.91;2,89.29,664.89,228.09,10.91"><p>The training dataset consists of 420 XML files, corresponding to authors, and each XML file contains 200 tweets from its author. The dataset contains anonymized URLs, hashtags, and user mentions. The URLs, hashtags, and user mentions are replaced with tags like #URL#, which are frequently repeated at the beginning or end of tweets. These repetitions are retained.Tweets also frequently contain emojis, in fact many tweets contain many different and even repetitions of the same emojis. Emojis are retained.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1" coords="3,108.93,671.02,190.37,8.97"><p>Note that the task is a binary classification problem.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="4,108.93,627.07,322.56,8.97"><p>An irony spreader may have a majority of tweets that are neutral, factual, and objective.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3" coords="4,108.93,638.02,271.25,8.97"><p>https://huggingface.co/transformers/v3.3.1/mainğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘’ğ‘ /ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘–ğ‘§ğ‘’ğ‘Ÿ.â„ğ‘¡ğ‘šğ‘™</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4" coords="4,108.93,648.98,72.24,8.97"><p>https://pytorch.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5" coords="4,108.93,659.94,197.86,8.97"><p>https://huggingface.co/transformers/v3.3.1/index.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6" coords="4,108.93,670.90,190.40,8.97"><p>https://unicode.org/emoji/charts/full-emoji-list.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,112.66,414.48,393.32,10.91;6,112.66,428.03,393.33,10.91;6,112.66,441.57,394.53,10.91;6,112.28,455.12,313.11,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,288.18,414.48,217.80,10.91;6,112.66,428.03,214.11,10.91">An impact analysis of features in a classification approach to irony detection in product reviews</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Buschmeier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Klinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,352.95,428.03,153.03,10.91;6,112.66,441.57,389.90,10.91">Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</title>
		<meeting>the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,112.66,468.67,393.33,10.91;6,112.66,482.22,395.17,10.91;6,112.66,495.77,393.33,10.91;6,112.66,509.32,233.56,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,314.29,468.67,191.70,10.91;6,112.66,482.22,210.44,10.91">Reinforcement guided multi-task learning framework for low-resource stereotype detection</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pujari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Oveson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nouri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,344.43,482.22,163.40,10.91;6,112.66,495.77,228.32,10.91">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="6,112.66,522.87,394.53,10.91;6,112.66,536.42,395.17,10.91;6,112.66,549.97,393.32,10.91;6,112.66,563.52,395.01,10.91;6,112.66,577.07,393.33,10.91;6,112.66,590.62,393.32,10.91;6,112.66,604.17,393.33,10.91;6,112.66,617.71,184.76,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,251.61,549.97,254.37,10.91;6,112.66,563.52,259.82,10.91">Overview of PAN 2022: Authorship Verification, Profiling Irony and Stereotype Spreaders, and Style Change Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chulvi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Heini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kestemont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kredens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mayerl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ortega-Bueno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pezik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wolska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Zangerle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,446.63,577.07,59.35,10.91;6,112.66,590.62,393.32,10.91;6,112.66,604.17,253.92,10.91">Proceedings of the Thirteenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="6,474.16,605.18,31.82,9.72;6,112.66,618.73,112.43,9.72">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">D E F S C M G P A H M P G F N F</forename><surname>Alberto Barron-Cedeno</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</editor>
		<editor>
			<persName><surname>Martino</surname></persName>
		</editor>
		<meeting>the Thirteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="volume">13390</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="6,112.66,631.26,393.33,10.91;6,112.66,644.81,393.33,10.91;6,112.66,658.36,393.32,10.91;7,112.66,86.97,393.33,10.91;7,112.66,100.52,188.61,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,323.15,631.26,182.83,10.91;6,112.66,644.81,186.91,10.91">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,327.87,644.81,178.11,10.91;6,112.66,658.36,393.32,10.91;7,112.66,86.97,99.97,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="7,112.66,114.06,395.17,10.91;7,112.66,127.61,393.33,10.91;7,112.66,141.16,393.33,10.91;7,112.66,154.71,394.53,10.91;7,112.66,168.26,54.59,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,231.60,114.06,276.23,10.91;7,112.66,127.61,39.98,10.91">Sentence-BERT: Sentence embeddings using Siamese BERTnetworks</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,182.44,127.61,323.55,10.91;7,112.66,141.16,393.33,10.91;7,112.66,154.71,128.81,10.91">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,181.81,393.33,10.91;7,112.66,195.36,394.53,10.91;7,112.66,208.91,172.05,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,373.25,181.81,132.74,10.91;7,112.66,195.36,219.45,10.91">Profiling Irony and Stereotype Spreaders on Twitter (IROSTEREO) at PAN 2022</title>
		<author>
			<persName coords=""><forename type="first">O.-B</forename><surname>Reynier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Berta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Elisabetta</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="7,358.04,195.36,143.79,10.91">CLEF 2022 Labs and Workshops</title>
		<title level="s" coord="7,112.66,208.91,103.05,10.91">Notebook Papers, CEUR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,222.46,394.53,10.91;7,112.66,236.01,393.33,10.91;7,112.66,249.56,326.25,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,327.46,222.46,175.13,10.91">TIRA Integrated Research Architecture</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,240.99,236.01,264.99,10.91;7,112.66,249.56,123.40,10.91">Information Retrieval Evaluation in a Changing World, The Information Retrieval Series</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,263.11,393.33,10.91;7,112.66,276.66,393.33,10.91;7,112.66,290.20,223.61,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,218.04,263.11,287.95,10.91;7,112.66,276.66,75.48,10.91">A comparative study of different sentiment lexica for sentiment analysis of tweets</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ã–zdemir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bergler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,210.64,276.66,295.35,10.91;7,112.66,290.20,193.87,10.91">Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2015)</title>
		<meeting>the International Conference on Recent Advances in Natural Language Processing (RANLP 2015)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,303.75,394.62,10.91;7,112.66,317.30,229.62,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,219.45,303.75,267.28,10.91">CLaC-SentiPipe: SemEval2015 Subtasks 10 b,e, and Task 11</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ã–zdemir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bergler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,112.66,317.30,198.14,10.91">Proceedings of SemEval 2015 at NAACL/HLT</title>
		<meeting>SemEval 2015 at NAACL/HLT</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,330.85,394.52,10.91;7,112.28,344.40,178.07,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,259.07,330.85,243.57,10.91">Irony detection in twitter: The role of affective content</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">I H</forename><surname>FarÃ­as</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Patti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,112.28,344.40,133.28,10.91">ACM Trans. Internet Technol</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,357.95,395.17,10.91;7,112.66,371.50,393.61,10.91;7,112.66,385.05,393.59,10.91;7,112.66,398.60,292.62,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,363.05,357.95,144.78,10.91;7,112.66,371.50,393.61,10.91;7,112.66,385.05,75.02,10.91">Deep Modeling of Latent Representations for Twitter Profiles on Hate Speech Spreaders Identification Task-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">Labadie</forename><surname>Tamayo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Castro</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">Ortega</forename><surname>Bueno</surname></persName>
		</author>
		<ptr target="-WS.org" />
	</analytic>
	<monogr>
		<title level="m" coord="7,481.76,385.05,24.49,10.91;7,112.66,398.60,112.59,10.91">CLEF 2021 Labs and Workshops</title>
		<title level="s" coord="7,233.24,398.60,103.05,10.91">Notebook Papers, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,412.15,393.33,10.91;7,112.66,425.70,75.01,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,166.47,412.15,224.48,10.91">Authorship verification using the impostors method</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Seidman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,414.25,412.15,91.74,10.91;7,112.66,425.70,24.17,10.91">Notebook for PAN at CLEF</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,439.25,393.33,10.91;7,112.33,452.79,29.19,10.91" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="7,238.21,439.25,167.55,10.91">A method for stochastic optimization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,466.34,395.17,10.91;7,112.66,479.89,394.53,10.91;7,112.66,493.44,395.17,10.91;7,112.66,506.99,361.66,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="7,371.16,466.34,136.67,10.91;7,112.66,479.89,389.85,10.91">Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Felbo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mislove</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>SÃ¸gaard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Rahwan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,127.42,493.44,380.41,10.91;7,112.66,506.99,30.43,10.91">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
