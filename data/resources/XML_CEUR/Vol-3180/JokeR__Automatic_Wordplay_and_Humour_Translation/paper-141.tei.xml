<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,403.65,15.42;1,89.29,106.66,249.86,15.42">LJGG @ CLEF JOKER Task 3: An improved solution joining with dataset from task 1</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,73.17,11.96"><forename type="first">Leopoldo</forename><surname>Jesús</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cadiz</orgName>
								<address>
									<addrLine>28 Paseo de Carlos III</addrLine>
									<postCode>11003</postCode>
									<settlement>Cádiz</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,165.45,134.97,89.22,11.96"><forename type="first">Gutiérrez</forename><surname>Galeano</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cadiz</orgName>
								<address>
									<addrLine>28 Paseo de Carlos III</addrLine>
									<postCode>11003</postCode>
									<settlement>Cádiz</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,403.65,15.42;1,89.29,106.66,249.86,15.42">LJGG @ CLEF JOKER Task 3: An improved solution joining with dataset from task 1</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">C5F17B14ABC7B80828F62CA7A10C1F67</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Wordplay</term>
					<term>Pun</term>
					<term>Computational Humour</term>
					<term>Machine Translation</term>
					<term>Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe the results of our participation to the CLEF JOKER 2022 Task 3, "Translate entire phrases containing wordplay". The purpose of this task is the translation of English phrases, which contain wordplay, to French phrases. Our contribution starts explaining the implementation of a basic solution, training just one model, using the given data for task 3. Since we wanted to improve results, we developed a 3-step architecture, which basically is the training of three models: two of them calculate additional information to concatenate to the English phrase, as input for the third neural network. After the generation of results, we decided to translate the whole dataset using DeepL translator, in order to finally compare results between this system and our improved implementation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This article contains the strategy for the development of an automatic pun and humour translation system, proposed in the CLEF Workshop JOKER 2022 <ref type="bibr" coords="1,350.77,392.48,11.45,10.91" target="#b0">[1]</ref>. They propose three pilot tasks, using the datasets they have prepared for each one, and an unshared task, which accepts any type of submission that uses the provided data. The first pilot task is "classify and explain instances of wordplay", the second one is "translate single words containing wordplay" and the third task is "translate entire phrases containing wordplay".</p><p>We have chosen task 3, which basically requires the translation of English phrases, that contain wordplay, to French phrases. Due to a classic neural network trained to generate translations does not usually take care of these kind of senses, we expect better translations using several models instead of just one.</p><p>In order to perform this task, we carried out different approaches. Initially, we implemented a basic solution, just using the provided data prepared for task 3. We selected the model mT5-base and we fine-tuned a pre-trained model using the wrapper library SimpleT5. The problem we have seen was training a simple model, in a simple way, without marking any peculiarities, that is, passing just an English phrase as input.</p><p>Therefore, after that, we decided to perform a more complex strategy, building an architecture based on three models, through which we could point out characteristic parts for the given phrases. So, for the development of this experiment, we decided to enlarge the dataset with the data prepared for the task 1. In this way, at first steps, it is possible to obtain special words from a given English phrase, so that, building an input based on the mentioned words and the initial phrase, we could generate a French translation, given the constructed input.</p><p>Finally, we decided to use the online translator DeepL, in order to compare results and then check how good were both systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Dataset</head><p>The provided train data for task 3 contains a set of translated wordplay instances, which have the following fields:</p><p>• id: The wordplay instance unique identifier.</p><p>• en: The wordplay text in English.</p><p>• fr: The wordplay text in French.</p><p>There are five wordplay instances in the Figure <ref type="figure" coords="2,313.15,484.70,3.74,10.91" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Models</head><p>We have selected the T5 model for our experiments, which is known as Text-to-Text Transfer Transformer (T5). It is prepared for a diverse set of tasks and we have to keep in mind that all the tasks are in a "text-to-text" format, so we have to pass text as input and output is text too. This text-to-text model is prepared to perform different tasks, including translation, question answering, and classification <ref type="bibr" coords="2,220.34,588.29,11.43,10.91" target="#b1">[2]</ref>.</p><p>We will use the library SimpleT5 for all the experiments since it is a wrapper which makes easier the use of T5 and mT5 models. This library is built on top of PyTorch-lightning, with transformers. It is just needed to use Pandas to deal with inputs and outputs, and it is possible to do any task, such as, summarisation, translation, question-answering, or any other sequenceto-sequence tasks. This library takes care of import, instantiation, downloading pre-trained models and training <ref type="bibr" coords="2,181.30,669.58,11.43,10.91" target="#b2">[3]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Hardware and Software Resources</head><p>The resources employed for all the experiments are the library SimpleT5, Pandas and sacreBLEU. For the development, we have used Jupyter Notebook. Since t5-large and mt5-base are heavy models, it was needed the use of a high specs machine for the training phase. We used a machine with a GPU NVIDIA Quadro P6000 and 24GB of GPU memory, 30GB of RAM and 8 vCPUs. Anyway, we tried t5-base, t5-small and mt5-small but we decided to select the biggest models for the best machine we could use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Previous Experiment</head><p>The main objective for the previous experiments is the implementation of a basic solution, based on the training of just one model, which returns French translations, given English phrases. After that, the next goal to reach is the use of this experiment as a reference model, in order to get to know if the model implemented in the main experiment is better.</p><p>The main tasks performed to see this experiment through are the following:</p><p>• Data cleaning: it is possible to find empty values which leads the model to predict wrong values after training. Therefore, the first task to do is the elimination of all missing values. • Preparation of dataset: the selected model expects a dataframe with two columns:</p><p>"source_text" and "target_text". We pass the English phrase as "source_text" because this is the input value, and the French phrase as "target_text", since this is what we want to predict.</p><p>• Training: for this step we tried the model mT5-base, which initially could be promising since it could return good results. We trained this model with source_max_token_len = 512, target_max_token_len = 128, batch_size = 8 and max_epochs = 5. After training, the best epoch is the fourth and the model accuracy is 0%. This result means that there was not any literal coincidence between translated phrases and expected phrases. The Figure <ref type="figure" coords="3,116.56,650.56,5.07,10.91" target="#fig_1">2</ref> displays the epoch trend. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Main Experiment</head><p>The main objective of this experiment is the implementation of an architecture, composed by a group of interconnected models, which improve the results of a basic model, like the implemented in the previous experiment. Another goal is testing and comparing different models for relevant parts of the implemented architecture, and finally selecting the best to include in the architecture.</p><p>The previous experiment was a fine-tuning of a model with the English phrase as input and the French translation as output. That was a simple task and results could be better if we try to improve the system. In this experiment, we have selected an approach based on an architecture of models, which improve the results. In order to deal with this challenge, we found more data which could be added to the initial dataset provided for task 3. Since task 3 and task 1 datasets have a common field, it was possible to enlarge our dataset with two more fields: one of them is first meaning, which is the first meaning of the pun or wordplay, and the second one is both meanings, which basically is the disambiguation.</p><p>The T5 model accepts more than a simple phrase, so we decided to add more information to the model input, in order to get better results. For our improved solution, we use an approach which two or more data are joined used a separator, &lt;sep&gt; <ref type="bibr" coords="4,365.30,561.97,11.58,10.91" target="#b3">[4]</ref>. Moreover, there is another approach which uses the symbol | as separator <ref type="bibr" coords="4,303.66,575.51,11.58,10.91" target="#b4">[5]</ref>. Therefore, if we pass the English phrase, the first meaning and both meanings, joined together with a separator, we realised that the results were better. In the Figure <ref type="figure" coords="4,237.69,602.61,3.76,10.91" target="#fig_2">3</ref>, you can see the architecture with the step 1 model, which predicts first meaning given the English phrase, the step 2 model, which predicts both meanings given the English phrase, and the step 3 model, which returns the French phrase, given the concatenation of the English phrase, a separator, first meaning, another separator and both meanings. After studying the provided data for all the tasks, we found some kind of relation between the datasets prepared for task 3 and task 1. The main purpose of improving the experiment could be fulfilled if we join both datasets, just if we attach additional information to the model input, in order to obtain better predictions.</p><p>The main tasks performed for this experiment are the following:</p><p>• Data cleaning: we start with the same data cleaning task performed for the previous experiments. Besides that, since we are going to join data provided for task 1, we need to clean the same kind of missing values for that dataset. • Joining task 3 and task 1 data: the dataset prepared for task 1 contains a bunch of fields, which three of them are useful for our experiment. The "WORDPLAY" field, in task 1 data, contains the English phrase, that is, the "en" field in task 3 data. This fact makes possible joining more fields to the initial dataset. We will add the fields "TARGET_WORD" and "DISSAMBIGUATION", which will be renamed to "first_meaning" and "both_meanings", respectively. A sample of data, after enlarging the initial dataset with two fields more from task 1 data can be seen in Figure <ref type="figure" coords="5,288.00,445.23,3.74,10.91" target="#fig_3">4</ref>. • Looking for separators: since the main goal of this experiment is improving results, and we will reach this purpose by passing the new fields, "first_meaning" and "both_meanings", to the model input, we need a way to pass a different input to the model. Due to the model expects a string as input, we need to build a new string with the English phrase and both fields. In order to solve this issue, we have to find a proper separator to avoid ambiguities. For instance, in the Figure <ref type="figure" coords="5,298.88,527.88,3.81,10.91" target="#fig_3">4</ref>, you can see that the "both_meanings" field contains the slash character, therefore we cannot use it as separator. We studied all the fields we are using and the hash character is used, as well as, the dollar, the "at" symbol or the ampersand. Finally, we found that the vertical bar was not used in the dataset, so we have selected that character. In the Figure <ref type="figure" coords="5,320.80,582.07,3.74,10.91" target="#fig_4">5</ref>, you can find an outline of how the new concatenated input is formed. • Preparing dataset for the three step strategy: our improved 3-step architecture is composed by three models, which need to be trained. In order to do that, and now that we have cleaned the data, we have to prepare the three datasets needed for this task.</p><p>Since the first step model will predict the first meaning, given an English phrase, we will create a dataset with two columns: "en" renamed as "source_text" and "first_meaning"  renamed as "target_text". Similarly, the second step model will predict both meanings, given an English phrase, so we will create a dataset with two columns: "en" renamed as "source_text" and "both_meanings" renamed as "target_text". Finally, since the third step model will predict the French phrase, given a concatenation of the English phrase, first meaning and both meanings, we will create a dataset with two columns: the concatenated field named as "source_text" and "fr" renamed as "target_text".  is 0%. This result means that there was not any literal coincidence between translated phrases and expected phrases. The Figure <ref type="figure" coords="7,313.11,480.74,5.17,10.91">8</ref> contains a chart, which shows train and validation loss with respect to each epoch. • Training the step 3 model with t5-large: in this case, we used the same model and parameters used for steps 1 and 2 models. In this case, after training, the best epoch is the second and the model accuracy is 0,17%. We have to keep in mind the same difficulties to get a realistic accuracy than we had for the previous models, due to the comparison of literal strings. The problem is that we are expecting a concrete translation and the model returns a translation, which could be valid, but we count it as invalid. The Figure <ref type="figure" coords="7,501.01,576.94,4.97,10.91" target="#fig_8">9</ref> contains a chart, which shows train and validation loss with respect to each epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Analysis of results</head><p>We analysed results using the BLEU score. For the previous experiment, we obtained 4.88 using the mT5 model. This means that translations generated are almost useless. For the improved solution, in step 3, we obtained 3.14 for the mT5 model, which means that translations are almost useless too. However, for the T5 model we got 19.99, which means that it's hard to get the gist. We have to say that the obtained score is almost 20, so the feedback could be between that one and the next one, for scores between 20 and 29, which says that the gist is clear but has significant errors <ref type="bibr" coords="8,169.43,317.91,11.56,10.91" target="#b5">[6]</ref>. Moreover, phrases are in French. Anyway, the third step is better with t5-large, since we got better BLEU score and better accuracy. Therefore, we have selected the improved architecture, with t5-large for step 3, as the best implementation.</p><p>The results obtained were send to the organization and were ranked as the third best submission, out of seven. All the teams sent a total 2378 translations. We have seen that this is the third best submission with more valid translations, 2264, in contrast to 206 not translated phrases and 349 with non sense. This submission generated more untranslated and non sense phrases than the others. However, 46 phrases have syntax problems and 78 have lexical problems, and just one submission was worse for both indicators. 1595 translations preserve the lexical field of the source wordplay, 1327 preserve the sens of the source wordplay and 827 uses comprehensible terms, which are the fourth best indicators. 261 corresponds to translations that are wordplay, 240 are wordplays that are understandable for general audience, 4 have style shift, e.g. in case whether a vulgarism is present either in a source wordplay or in a translation but not in both and 838 hilariousness shift, or translations that were judged much or much less funny than the source wordplay, which are in the third position. And we sent 9 over-translations, or translations that have useless multiple wordplay instances when the source text has just one, which is the worst value <ref type="bibr" coords="8,200.72,534.70,11.43,10.91" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Manual translations with DeepL</head><p>DeepL is an online translator, which could be used for free with limitations or with a subscription if we want to use more features. Their translations are direct modifications of the Transformer and the neural networks of DeepL also contain different parts of this architecture, such as attention mechanisms <ref type="bibr" coords="8,190.32,633.97,11.43,10.91" target="#b7">[8]</ref>.</p><p>We submitted DeepL as manual translations since we had to copy and paste English phrases and French translations in batches of 70 or 80 phrases, due to the limitation of characters for the free version.</p><p>The main objective was getting good results and comparing them with the automatic translations returned by the model implemented in the main experiments.</p><p>This results were sent to the organization and were ranked as the best submission. 2378 translations were sent and 2324 were valid, so this submission is the second in valid translations. 39 phrases were not translated, 59 have non sense, 17 have syntax problems, 25 have with lexical problems and 3 over-translations, and all these indicators obtained the third best values. The following indicators were the best: 2184 translations preserve lexical field of the source wordplay, 1938 preserve the sense of the source wordplay, 1188 use comprehensible terms, 373 are translations that are wordplay and 342 have identifiable wordplays. Finally, these submission indicators were ranked as the second best: 9 have style shift and 930 hilariousness shift <ref type="bibr" coords="9,479.82,222.46,11.43,10.91" target="#b6">[7]</ref>.</p><p>To sum up, DeepL results were better than the improved solution submission for all indicators. Moreover, it was better than the second classified for all indicators, except for two: overtranslations and style shift <ref type="bibr" coords="9,210.20,263.11,11.43,10.91" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions and Perspectives for future work</head><p>Results obtained using DeepL thrown the best results. They have a translation service which has been improved over the years. In spite of that, the results generated by the developed experiments are showing that the best results were obtained using an improved architecture of models. Although the automatic results were ranked in third position, this solution could be refined in the future.</p><p>The main perspective for future work is trying to improve the results, changing the implemented architecture in the main experiment. For step 3, the input was English phrase, separator, first meaning, separator and both meanings. Another idea could be checking results if we swap the information in the concatenation step. For instance, it could be possible to get better translations if we pass first meaning, separator, both meanings, separator and English phrase. Another additional way could be by adding more data from task 1 dataset, converting to lowercase all characters or maybe separating step 3 in two models, one for puns and the other one for wordplays.</p><p>Additionally, another model could improve results. For instance, mt5-large, or maybe any other. Trying different parameters for the model or using T5 directly with PyTorch, instead of SimpleT5, could be another way to get better translations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,213.11,254.34,8.93;2,89.29,84.19,416.71,104.40"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The first five rows of the provided dataset for task 3</figDesc><graphic coords="2,89.29,84.19,416.71,104.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,89.29,243.46,170.66,8.93;3,197.18,84.19,198.43,146.71"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Loss chart for the easy solution</figDesc><graphic coords="3,197.18,84.19,198.43,146.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,89.29,300.56,134.75,8.93;4,89.29,84.19,416.70,191.85"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Improved Architecture</figDesc><graphic coords="4,89.29,84.19,416.70,191.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,116.56,204.63,334.95,8.93;5,89.29,84.19,416.69,95.92"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The first five rows of task 3 data with two fields added from task 1 data</figDesc><graphic coords="5,89.29,84.19,416.69,95.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,116.56,170.74,282.18,8.93;6,126.31,84.19,340.16,73.99"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The new concatenated input for the improved architecture</figDesc><graphic coords="6,126.31,84.19,340.16,73.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,116.56,356.87,224.53,8.93;6,197.18,197.60,198.43,146.71"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Loss chart for the improved solution -Step 1</figDesc><graphic coords="6,197.18,197.60,198.43,146.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="6,107.28,477.12,398.71,10.91;6,116.56,490.67,389.42,10.91;6,116.56,504.22,389.42,10.91;6,116.31,517.77,389.67,10.91;6,116.56,531.32,63.93,10.91;6,107.28,546.22,398.71,10.91;6,116.31,559.77,389.67,10.91;6,116.56,573.32,389.42,10.91;6,116.56,586.87,389.71,10.91;6,116.56,600.42,389.42,10.91;6,116.16,613.97,121.71,10.91;6,107.28,628.87,399.91,10.91;6,116.16,642.42,389.82,10.91;6,116.56,655.97,389.42,10.91;6,116.56,669.52,389.71,10.91"><head>•</head><label></label><figDesc>Training the step 1 model: for this step, we used the model t5-large, with the following parameters: source_max_token_len = 50, target_max_token_len = 50, batch_size = 16 and max_epochs = 10. After training, the best epoch is the seventh and the model accuracy is 79,36%. The Figure 6 contains a chart, which shows train and validation loss with respect to each epoch. • Training the step 2 model: we used the same model and parameters used for the step 1 model. In this case, the best epoch is the third and the model accuracy is 19,04%. We have to keep in mind that the accuracy was calculated by comparing if strings are the same, that is, we did not study the real accuracy by studying if each result was actually equivalent or valid. The Figure 7 contains a chart, which shows train and validation loss with respect to each epoch. • Training the step 3 model with mt5-base: we tried a different model, mT5-base, which initially could be promising since it could return better results. We trained this model with source_max_token_len = 512, target_max_token_len = 128, batch_size = 8 and max_epochs = 5. After training, the best epoch is the fourth and the model accuracy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="7,116.56,243.46,224.53,8.93;7,197.18,84.19,198.43,146.71"><head>Figure 7 : 2 Figure 8 :</head><label>728</label><figDesc>Figure 7: Loss chart for the improved solution -Step 2</figDesc><graphic coords="7,197.18,84.19,198.43,146.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="8,116.56,239.66,258.17,8.93;8,197.18,84.19,198.42,142.91"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Loss chart for the improved solution -Step 3 with T5</figDesc><graphic coords="8,197.18,84.19,198.42,142.91" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,107.59,583.60,399.60,10.91;9,107.59,594.67,399.60,13.38;9,107.59,610.69,399.60,10.91;9,107.59,624.24,398.40,10.91;9,107.59,637.79,400.08,10.91;9,107.59,651.34,187.11,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,107.59,610.69,325.72,10.91">CLEF Workshop JOKER: Automatic Wordplay and Humour Translation</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Puchalski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Regattin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">É</forename><surname>Mathurin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Araújo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A.-G</forename><surname>Bosser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bokiniec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L</forename><surname>Corre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Jeanjean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hannachi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ġ</forename><surname>Mallia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Saki</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-99739-7_45</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,449.68,624.24,56.30,10.91;9,107.59,637.79,93.70,10.91">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Hagen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Verberne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Seifert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Balog</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Nørvåg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Setty</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13186</biblScope>
			<biblScope unit="page" from="355" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,107.59,86.97,399.60,10.91;10,107.59,100.52,399.69,10.91;10,107.59,114.06,314.10,10.91" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,107.59,100.52,345.50,10.91">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1910.10683</idno>
		<ptr target="https://arxiv.org/abs/1910.10683.doi:10.48550/ARXIV.1910.10683" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,107.59,127.61,8.98,10.91;10,137.23,127.61,50.24,10.91;10,212.63,127.61,47.95,10.91;10,281.24,127.61,3.76,10.91;10,305.67,127.61,21.79,10.91;10,348.12,127.61,11.82,10.91;10,380.62,127.61,32.35,10.91;10,433.64,127.61,9.05,10.91;10,463.35,127.61,16.79,10.91;10,500.81,127.61,5.17,10.91;10,107.59,141.16,21.30,10.91;10,157.10,141.16,9.06,10.91;10,194.37,141.16,23.50,10.91;10,252.45,141.16,23.14,10.91;10,303.82,141.16,22.39,10.91;10,354.42,141.16,152.27,10.91;10,107.59,154.71,392.85,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Shivanand</surname></persName>
		</author>
		<ptr target="https://medium.com/geekculture/simplet5-train-t5-models-in-just-3-lines-of-code-by-shivanand-roy-2021-354df5ae46ba" />
		<title level="m" coord="10,212.63,127.61,47.95,10.91;10,281.24,127.61,3.76,10.91;10,305.67,127.61,21.79,10.91;10,348.12,127.61,11.82,10.91;10,380.62,127.61,32.35,10.91;10,433.64,127.61,9.05,10.91;10,463.35,127.61,16.79,10.91;10,500.81,127.61,5.17,10.91;10,107.59,141.16,21.30,10.91;10,157.10,141.16,9.06,10.91;10,194.37,141.16,18.80,10.91">SIMPLET5 -train T5 models in just 3 lines of code</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,107.59,168.26,398.40,10.91;10,107.59,181.81,400.08,10.91;10,107.59,195.36,167.31,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,260.33,168.26,245.66,10.91;10,107.59,181.81,192.72,10.91">Discriminative models can still outperform generative models in aspect based sentiment analysis</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mullick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fyshe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2206.02892</idno>
		<ptr target="https://arxiv.org/abs/2206.02892.doi:10.48550/ARXIV.2206.02892" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,107.59,208.91,398.40,10.91;10,107.59,222.46,398.40,10.91;10,107.59,236.01,400.24,10.91;10,107.59,249.56,399.60,10.91;10,107.59,263.11,402.55,10.91;10,107.59,279.10,74.11,7.90" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,307.04,208.91,198.94,10.91;10,107.59,222.46,33.39,10.91">Towards generative aspect-based sentiment analysis</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-short.64</idno>
		<ptr target="https://aclanthology.org/2021.acl-short.64.doi:10.18653/v1/2021.acl-short.64" />
	</analytic>
	<monogr>
		<title level="m" coord="10,163.46,222.46,342.52,10.91;10,107.59,236.01,400.24,10.91;10,107.59,249.56,14.61,10.91;10,247.27,249.56,192.09,10.91">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="504" to="510" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct coords="10,107.59,290.20,399.11,10.91;10,107.59,303.75,39.59,10.91" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,145.76,290.20,82.02,10.91">Evaluating models</title>
		<author>
			<persName coords=""><surname>Google</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/translate/automl/docs/evaluate" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,107.59,317.30,399.59,10.91;10,107.20,330.85,398.79,10.91;10,107.06,344.40,400.12,10.91;10,107.59,357.95,399.60,10.91;10,107.59,371.50,400.08,10.91;10,107.59,385.05,398.66,10.91;10,107.59,398.60,77.31,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,309.14,330.85,196.85,10.91;10,107.06,344.40,203.35,10.91">Overview of JOKER@CLEF 2022: Automatic Wordplay and Humour Translation workshop</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Regattin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A.-G</forename><surname>Bosser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">É</forename><surname>Mathurin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L</forename><surname>Corre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Araújo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Boccou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Digue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Damoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Campen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Jeanjean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,180.46,371.50,322.84,10.91;10,107.59,385.05,398.66,10.91">Proceedings of the Thirteenth International Conference of the CLEF Association (CLEF</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Degli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Esposti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sebastiani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Macdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Pasi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hanbury</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Potthast</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><surname>Ferro</surname></persName>
		</editor>
		<meeting>the Thirteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="10,107.59,412.15,399.11,10.91;10,107.59,425.70,101.72,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="10,149.59,412.15,128.09,10.91">How does DeepL work?</title>
		<author>
			<persName coords=""><surname>Deepl</surname></persName>
		</author>
		<ptr target="https://www.deepl.com/sv/blog/how-does-deepl-work" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
