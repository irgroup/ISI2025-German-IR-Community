<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,381.25,15.42;1,88.69,106.66,361.08,15.42">FAST-MT Participation for the JOKER CLEF-2022 Automatic Pun and Humour Translation Tasks</title>
				<funder>
					<orgName type="full">National University of Computer and Emerging Sciences</orgName>
					<orgName type="abbreviated">NU</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,79.01,11.96"><forename type="first">Farhan</forename><surname>Dhanani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Computer and Emerging Sciences (NUCES-FAST)</orgName>
								<address>
									<settlement>Karachi</settlement>
									<country key="PK">Pakistan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,180.95,134.97,80.84,11.96"><forename type="first">Muhammad</forename><surname>Rafi</surname></persName>
							<email>muhammad.rafi@nu.edu.pk</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Computer and Emerging Sciences (NUCES-FAST)</orgName>
								<address>
									<settlement>Karachi</settlement>
									<country key="PK">Pakistan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.78,134.97,109.82,11.96"><forename type="first">Muhammad</forename><forename type="middle">Atif</forename><surname>Tahir</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Computer and Emerging Sciences (NUCES-FAST)</orgName>
								<address>
									<settlement>Karachi</settlement>
									<country key="PK">Pakistan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,381.25,15.42;1,88.69,106.66,361.08,15.42">FAST-MT Participation for the JOKER CLEF-2022 Automatic Pun and Humour Translation Tasks</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">110E1AA7D2C924B151F623BC82CE2E28</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Text Classification</term>
					<term>Token Classification</term>
					<term>Question Answering</term>
					<term>Machine Translation</term>
					<term>Transformers</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the solution proposed by team FAST Machine Translation to the shared tasks of JOKER CLEF 2022 Automatic pun and humour translation. State-of-the-art Transformer-based models are used to solve the three tasks introduced in the JOKER CLEF workshop. The Transformer model is a kind of neural network that tries to learn the contextual information from the sequential data by implicitly comprehending the existing relationships. In task 1, given a piece of text, we need to classify/explain any instance of wordplay is present in it or not. The proposed solution to task 1 combines the pipeline of token classification, text classification, and text generation. In task 2, we need to translate single words (nouns) containing a wordplay. This task is mapped to the problem of question answering (Q/A) on programmatically extracted texts from the OPUS parallel corpus. In task 3, contestants are required to translate the entire phrase containing the wordplay. Sequence-to sequence translation models are used to solve this task. The team has adopted different strategies for each task as they suited to the requirements therein. The paper reports proposed solutions, implementation details, experimental studies, and results obtained in JOKER CLEF 2022 automatic pun and humour translation tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In our daily communications, humour is one of the most ubiquitous elements that we, as, a human comprehend comfortably with the help of previous cultural experiences and understandings. But, for the computers, this still remains one of the most daunting jobs as it is extremely difficult even for the expensive deep-learning-based solutions to decipher the double-meaning words which is a prominent feature of humour in different languages, and generate its appropriate parallel translation in the target language. This year JOKER CLEF-2022 team has come up with a unique set of challenges under the natural language processing domain. The workshop has brought professional translators and computer scientists together by presenting three different tasks to evaluate the understanding of translators and computer-based models about the humour. This paper will present our strategy to solve the given problems by fine-tuning transformer-based pre-trained deep learning models and then discuss the obtained results. The first task, named "Explain and classify instances of wordplay," presents tabular data with the from different regions. So, the authors try extremely hard to preserve original emotions by appropriately translating the name of their story characters while keeping the cultural background of the target language in context. The Pokemon series is a suitable example to explain this concept. For example, consider the following Pokemon shown in the figure <ref type="figure" coords="3,445.68,127.61,31.61,10.91">1 below</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1:</head><p>The image displays a Pokemon <ref type="bibr" coords="3,221.08,292.82,11.83,9.96" target="#b0">[1]</ref> character. It is named "EKANS" in the English version of the series, but French version, it's called "ABO. "</p><note type="other">Figure 2:</note><p>The image displays a character from the Astrix comic series <ref type="bibr" coords="3,375.84,304.78,10.51,9.96" target="#b1">[2]</ref>. Its name is "Dogmatrix" in the English version, but in the French version, its name is "Idéfix. "</p><p>The name of the character shown in the above figure <ref type="figure" coords="3,336.88,351.02,5.03,10.91">1</ref> is "EKANS" in the English version of the Pokemon <ref type="bibr" coords="3,151.71,364.57,12.97,10.91" target="#b0">[1]</ref> series, but in the French version, it is named "ABO." You can notice that the character visually looks like a small snake, and the authors wanted to preserve this similarity linguistically to educate their audience about the nature of the character. Therefore, they named the character "EKANS" in the English version to make it an anadrome of "SNAKE. " In the French language, the boa means a masculine snake. Thus, in the French version, the writers have translated the name of this character to "ABO," which is an anadrome of "BOA." Because the french audience doesn't understand the word "SNAKE, " the authors have renamed the character to keep them engaged with the vocabulary of their own native language. The challenge in Task 2 is to learn this mapping and predict an appropriate French translation for a given English noun. Figure <ref type="figure" coords="3,120.84,486.51,5.16,10.91">2</ref> illustrates another example to understand this task. It presents a character from the Asterix series <ref type="bibr" coords="3,151.40,500.06,12.75,10.91" target="#b1">[2]</ref> named "Idéfix" in French. The French pun in this name is on the phrase "idée fixe", which means a fixed idea that illustrates that this character has a single-minded obsession. Alternatively, in the English version, the name of this character is "Dogmatix, " which correctly maps the pun on the English word dogmatic to express the same idea of single-mindedness. Lastly, the third task presents English phrases containing a wordplay, and the challenge is to predict the corresponding French translation. It's important to note that multiple valid French translations may exist for a given English sentence containing a wordplay, and the task is to predict any one of it correctly. For example, consider the following scenario where both French translations are correct for the given English sentence.</p><p>• English: "Be still my hart" she murmured, thinking how magnificent and stag -like he was.</p><p>• French-1: "Mon coeur se cerf", murmura-t-elle en voyant ce beau et majestueux mâle.</p><p>• French-2: Elle murmura "Calme-toi mon destrier" en pansant combien il était magnifique. This paper is further divided into three sections. The next section presents our approach to solving the mentioned tasks. The subsequent section lists the names of transformer models we have applied to solve these tasks. It provides the details of our experiments and the observed results. Finally, the last section discusses the conclusion and our learning in this workshop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Materials and Methods</head><p>This section presents an overview of our strategies and summarizes the rationale for selecting these approaches to solve the three tasks of the JOKER CLEF 2022 <ref type="bibr" coords="4,384.99,199.72,12.82,10.91" target="#b2">[3]</ref> workshop. Let's discuss our solution for task 1 first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">TASK-1: Classify and explain instances of wordplay</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Proposed approach for task-1</head><p>We have trained seven distinct models independently to predict the value for each of the seven target columns of task 1 listed in the previous section 1, given a pair of English wordplay text with its id. Firstly, we have utilized the token-classification-based method for preparing a model to extract the words forming the wordplay in a given English text, as depicted in figure <ref type="figure" coords="4,480.86,324.15,3.75,10.91" target="#fig_0">3</ref>. We have treated the English wordplay text as a series of space-separated tokens to locate the words forming the wordplay by classifying each token into the following three categories.</p><p>• word_play_token_B: To identify the word which begins the wordplay.</p><p>• word_play_token_I: To identify the other remaining words in the wordplay.</p><p>• other_token: To identify all the words which don't belong to the wordplay. The figure portrays our approach to mapping the task of finding the words forming the wordplay in the given English text onto the problem of token classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4:</head><p>The figure portrays our approach to mapping the task of predicting the labels for the five different categorical columns of the given tabular dataset onto the problem of text classification.</p><p>Employing this approach, we can apply several implementations of the token classification pipeline from the hugging face repository using auto encoding BERT-like transformer models such as "bert-base-cased" <ref type="bibr" coords="4,204.30,642.48,13.00,10.91" target="#b3">[4]</ref> to locate the words forming the wordplay in the given English text, as depicted in figure <ref type="figure" coords="4,206.01,656.03,3.80,10.91" target="#fig_0">3</ref>. Next, we have used the auto-regressive technique to construct a model to generate the interpretation for the extracted wordplay and used the text classification scheme to build five separate models for inferring the values for the other five target columns, as illustrated in figure <ref type="figure" coords="5,189.69,100.52,3.74,10.91">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Data-set and processing</head><p>The JOKER CLEF 2022 <ref type="bibr" coords="5,192.19,149.84,12.84,10.91" target="#b2">[3]</ref> team has shared two versions of 10-column-based training and test sets for task 1 in both CSV and JSON formats. The tables below list the column names and the structure of both provided data sets. The JOKER CLEF 2022 <ref type="bibr" coords="6,203.70,86.97,12.86,10.91" target="#b2">[3]</ref> team has first provided a smaller version of the training and test data set for task 1. Later as the competition timeline grew, they released an updated pair of more diverse training and test sets by adding more number of records in it. Both versions of the provided training data set for task 1 contain the correct values of all the remaining target columns against a given wordplay, along with its id. On the other hand, both the given test sets were for evaluation purposes to rank the approaches submitted by different teams that participated in the workshop. Thus, the test set only contains values under the wordplay and id column. All the remaining columns are empty, and for each listed wordplay in the English language, the challenge was to predict the values of the empty columns. It's important to note that both the versions of training and test sets have the same 10-column-based tabular structure apart from subtle naming differences. The "target_word" and "disambiguation" columns in the first pair of the given training and test sets map equally to the "location" and "interpretation" columns in the second pair of provided training and test set. We have used the provided training data sets to prepare our models and then used them to predict the values for each target column given a pair of English wordplay text and its id from the test data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">TASK-2: Translate single words containing wordplay 2.2.1. Proposed approach for task-2</head><p>We have mapped the task of learning the relation between English nouns and their corresponding French translations into the extractive Question/Answer (Q/A) problem by transforming the given three-column-based tabular data sets (Id, English Noun, French Noun) into extractive Question/Answer problem-styled data sets. To accomplish this transformation, we have utilized OPUS open-source parallel corpus <ref type="bibr" coords="6,244.34,401.15,12.81,10.91" target="#b4">[5]</ref> to artificially develop the context for all English/French noun pairs provided in the task 2 data set. We have iteratively selected each English/French noun pair listed in the provided data set. Then extracted those English/French parallel sentence pairs from the OPUS open-source parallel corpus <ref type="bibr" coords="6,313.01,441.80,12.94,10.91" target="#b4">[5]</ref> that contains the selected English noun in its English version and the translated French noun in its French version. In such a way, we have collected contexts for all English/French noun pairs. And transformed the task 2 data set where each record is composed of an English noun and its French translation, along with a list of extracted English/French parallel sentence pairs in the form of contexts. Now the task for the Question/Answering models is to use the English noun as a query and predict the location of its French translation in the French version of the extracted English/French parallel sentence pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Data-set and processing</head><p>The JOKER CLEF 2022 <ref type="bibr" coords="6,198.01,572.42,12.99,10.91" target="#b2">[3]</ref> team has shared two 3-column-based tabular data sets for task 2 in both CSV and JSON formats. One is for training the models to predict an equivalent French version of the given English noun, along with a unique Id as input. The second is for testing/ranking the models submitted by different teams by evaluating their predictions for the given pairs of English nouns with a distinct id. It's essential to note that the test set only holds the list of English nouns and their corresponding ids. The associated French translations for each English noun are absent from the test set, and the challenge in this task is to predict these unknown French translations based on which the submissions from various teams will get ranked. The training set contains 1164 records, while the test set holds 284 data points. Table <ref type="table" coords="7,500.98,86.97,5.01,10.91">5</ref> and 6 below tabulates the structure of the provided train and test data sets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No. Column Names</head><formula xml:id="formula_0" coords="7,98.06,141.30,40.31,34.67">1 ID 2 EN 3 FR</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5</head><p>The table shows the structure of the training data set provided by the JOKER CLEF team for task 2 with the 1164 records.</p><p>No. Column Names 1 ID (Given) 2 EN (Given) 3 FR (Empty)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 6</head><p>The table shows the structure of the test data set provided by the JOKER CLEF team for task 2 with the 284 number of records.</p><p>We have transformed the provided 3-column-based tabular training and test data set for task 2 into the extractive question-answer styled data set by utilizing OPUS parallel corpus <ref type="bibr" coords="7,453.65,258.99,11.28,10.91" target="#b4">[5]</ref>. In order to transform the training set, we have iteratively selected each of the listed English/French noun pairs. And then pulled out those English/French parallel sentence pairs from the OPUS parallel corpus <ref type="bibr" coords="7,121.78,299.64,12.79,10.91" target="#b4">[5]</ref> that possess the selected English noun in its English version and the corresponding French translation in its French version. We have programmatically ensured that the English version sentence pulled from the OPUS parallel corpus <ref type="bibr" coords="7,332.55,326.74,12.73,10.91" target="#b4">[5]</ref> must hold at least one English noun from task-2's training data set. And its corresponding parallel French version must also contain a French translation of that English noun. So to visualize the transformation, suppose we have only one record in the task 2 training data set, as shown in the table 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Id En Fr 4 Obelix Obélix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 7</head><p>The table describes the structure of a single record in the training data set of task 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Id</head><p>Context Question Answers 4 astérix et obélix ne devraient plus quitter le village.</p><p>Obelix {"text": [Obélix], "answer_start": <ref type="bibr" coords="7,464.43,512.55,15.37,9.96" target="#b10">[11]</ref>}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 8</head><p>The table shows the conversion of the table 7 records into the extractive Q/A styled data. The transformation shows that the English noun has now become the question and the context column holds a French sentence extracted from the OPUS corpus. The answers column holds a JSON that contains the actual French translation and the position of its appearance in the French sentence of the context.</p><p>Given such a scenario, we can pull the following sentence (English/French) pair listed below from the OPUS parallel corpus <ref type="bibr" coords="7,227.37,619.97,12.77,10.91" target="#b4">[5]</ref> to generate the extractive Question/Answer problem-styled data set shown in the table 8.</p><p>• English Version: asterix and obelix should stay in the village and not go in the forest! • French Version: astérix et obélix ne devraient plus quitter le village.</p><p>Observe that the English and French version of the extracted pair contains the English and French nouns mentioned in table 7, respectively. After the transformation, we can utilize popular pre-trained extractive question answering models from the hugging face <ref type="bibr" coords="8,439.78,114.06,12.69,10.91" target="#b5">[6]</ref> repositories to predict the French translation for a given English noun. The models will use the English nouns from the JOKER CLEF <ref type="bibr" coords="8,224.50,141.16,13.00,10.91" target="#b2">[3]</ref> task 2 training's data set as the input question, along with the corresponding French sentence pulled from the OPUS parallel corpus <ref type="bibr" coords="8,420.06,154.71,12.90,10.91" target="#b4">[5]</ref> as their context. And now, the task for the deep learning models is to learn to locate the exact position of the French translation in the French text for the queried English noun. After the process of training completes, we have again transformed the test data set for task 2 into the extractive Question/Answer styled test data set by applying a similar strategy. The test set of task 2 holds test records for which we don't know the correct French translation of the given English noun. Because of this, we have only ensured the existence of the given English noun in the English version of the extracted (English/French) sentence pairs and assumed that the corresponding French translation must also hold its equivalent French version. It's a weak assumption, but we have made this architectural choice to design the solution. Again to mentally simulate the process suppose we only have one record in the test data set given in the table 9. Then we can pull the following (English/French) sentence pair from the OPUS parallel corpus <ref type="bibr" coords="8,441.55,303.75,12.68,10.91" target="#b4">[5]</ref> to generate the extractive Question/Answer problem-styled test-data set shown in the table 10 below.</p><p>• English Version: I went to loompaland looking for exotic new flavors for candy.</p><p>• French Version: j'étais venu à lumpaland pour chercher de nouvelles saveurs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Id En 18 Loompaland</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 9</head><p>The table shows the structure of a single record in the test data set of task 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Id</head><p>Context Question 17 j'étais venu à lumpaland pour chercher de nouvelles saveurs. loompaland</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 10</head><p>The table describes the conversion of the table 9 records into the extractive question/answer styled test data. Note, unlike table 8, there is no answer column here because we are transforming test records, we don't know the correct translations for the queried English noun. During the test time, the trained models need to generate the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">TASK-3: Translate entire phrases containing wordplay 2.3.1. Proposed approach for task-3</head><p>The problem description of task 3 is a classical example of sequence to sequence prediction, where the model needs to predict an equivalent French translation for the given English text.</p><p>But, in this case, the challenge is there can be multiple valid possible translations for a given input English sentence containing a wordplay. We have utilized sequence-to-sequence models from the hugging face repository to learn any matching French translations for the given English sentence from the task 3 data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Data-set and processing</head><p>The JOKER CLEF 2022 <ref type="bibr" coords="9,192.52,176.86,12.86,10.91" target="#b2">[3]</ref> team has shared two 3-column-based tabular data sets for task 3 in both CSV and JSON formats. One is provided for training the model to make it learn to generate an equivalent French translation for the given English text which may possess a wordplay. The other is for testing/ranking the models submitted by different teams by evaluating their predictions for the listed English texts with a distinct id. Again the id is just for uniquely identifying each record in the provided data sets. Plus, the test set only holds the list of English sentences along with the associated ids. The French translations for each English sentence are absent from the test set, and the challenge in this task is to generate the French translations for each of the given English sentences in the test set based on which the submissions from various teams will get ranked. The training set contains 5115 records, while the test set holds 2378 data points. Table <ref type="table" coords="9,150.54,312.35,10.34,10.91" target="#tab_0">11</ref> and 12 below tabulates the structure of the provided train and test data sets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No. Column Names</head><formula xml:id="formula_1" coords="9,98.06,366.86,40.31,34.67">1 ID 2 EN 3 FR</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 11</head><p>The table shows the structure of the training data set provided by the JOKER CLEF team for task 3 with the 5115 records.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No. Column Names</head><formula xml:id="formula_2" coords="9,310.57,366.86,74.13,34.67">1 ID (Given) 2 EN (Given) 3 FR (Empty)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 12</head><p>The table shows the structure of the test data set provided by the JOKER CLEF team for task 3 with the 2378 number of records.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Id</head><p>En Fr 18 Tom said piously. déclara Tom pi-eusement. 19 Tom said piously.</p><p>dit Tom pieusement. 20 Tom said piously.</p><p>Tom dit pieusement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 13</head><p>The table shows the configuration of the task 3 data set when there exist multiple valid French translations for an English text.</p><p>The provided data sets for task 3 are in tabular form, due to which there are duplicate entries when multiple valid French translations are possible for a given English sentence. For example, consider the following English text. The multiple possible French translations for this English text can be arranged in the tabular structure as shown in table 13.</p><p>• English Text: "Tom said piously. "</p><p>We have transformed the provided tabular training set into a JSON dictionary. The key in this dictionary is the English text, and its associated value contains the list of all possible French translations for the keyed English text from the training set. We have used the prepared JSON object to train sequence-to-sequence transformer models for learning the mapping between the English text and any of its corresponding valid French translations. Figure <ref type="figure" coords="10,419.29,141.16,5.02,10.91" target="#fig_1">5</ref> visually expresses this transformation by converting the records listed in table 13 into a JSON object. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments and Results</head><p>This section will list the transformer architectures we have utilized to implement the approaches discussed in the previous section for solving tasks of the JOKER CLEF 2022 <ref type="bibr" coords="10,426.64,318.56,12.86,10.91" target="#b2">[3]</ref> workshop and their results. It's important to note that we have also shared our codebase on the public GitHub repository <ref type="bibr" coords="10,139.01,345.65,11.59,10.91" target="#b6">[7]</ref>. So, all the experiments can be easily re-executed to reproduce the mentioned results. Let's first discuss the implementation of the solution for task 1 and the obtained results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">IMPLEMENTATION OF TASK-1: Classify/explain instances of wordplay</head><p>We have used the listed pre-trained transformer models from the hugging face repository and fine-tuned them on the given training data sets for task 1 to make them learn to locate the words forming the wordplay in the given English text through token classification. The KEY-BERT <ref type="bibr" coords="10,493.30,435.79,12.68,10.91" target="#b7">[8]</ref> model can be utilized with different embedders. In this experiment, we have only used it with the fine-tuned "bert-base" <ref type="bibr" coords="10,206.00,462.89,12.84,10.91" target="#b3">[4]</ref> model and the pre-trained "all-MiniLM-L6-v2" <ref type="bibr" coords="10,429.67,462.89,12.84,10.91" target="#b8">[9]</ref> model.</p><p>• Pre-trained BERT-BASE <ref type="bibr" coords="10,226.10,483.88,12.84,10.91" target="#b3">[4]</ref> • KEY-BERT <ref type="bibr" coords="10,167.15,498.36,12.84,10.91" target="#b7">[8]</ref> with fine-tuned BERT-BASE <ref type="bibr" coords="10,311.35,498.36,12.84,10.91" target="#b3">[4]</ref> as its embedder.</p><p>• KEY-BERT <ref type="bibr" coords="10,166.71,512.84,12.77,10.91" target="#b7">[8]</ref> with pre-trained all-MiniLM-L6-v2 <ref type="bibr" coords="10,338.47,512.84,12.77,10.91" target="#b8">[9]</ref> sentence transformer as embedder.</p><p>The JOKER CLEF 2022 team has provided two pairs of training and test sets for task 1. They have first released a smaller version of the data set, and then later in time, they have disclosed an updated bigger version of both the training and test set. We have processed them independently and employed the listed BERT-based transformer models on each of them separately. We have applied the hold-out approach and pulled aside 9% of the records from both the training data sets provided for task 1, where the length of the given English text containing the wordplay was more than two. We have kept them hidden from the model throughout its training and used them later to evaluate and rank the predictions of the fine-tuned models in locating the words forming the wordplay. Moreover, we have also further extracted the 4% of the data from both the training set for validation purposes and then fine-tuned two separate instances of the BERT base model for less than five epochs on the remaining records of the training sets. Lastly, after fine-tuning, we have evaluated the predictions generated from the fine-tuned models for the 9% of the records, which were initially extracted from the training sets to estimate their performance on unknown data points, as shown in the table below. Overall our approach has generated comparatively good results for the first data set provided by the JOKER CLEF 2022 <ref type="bibr" coords="11,89.29,141.16,12.84,10.91" target="#b2">[3]</ref> team for task 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Name</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy on 9% of the records extracted from training set-1</head><p>Accuracy on 9% of the records extracted from training set-2 BERT-BASE <ref type="bibr" coords="11,150.61,204.61,11.83,9.96" target="#b3">[4]</ref> (FINE-TUNED) 71% 31% KEY-BERT <ref type="bibr" coords="11,145.33,222.94,11.83,9.96" target="#b7">[8]</ref> with fine-tuned BERT-BASE <ref type="bibr" coords="11,150.61,234.89,11.83,9.96" target="#b3">[4]</ref> as its embedder 33% 16% KEY-BERT <ref type="bibr" coords="11,145.33,259.20,11.83,9.96" target="#b7">[8]</ref> with pre-trained all-MiniLM-L6-v2E <ref type="bibr" coords="11,179.68,271.16,11.83,9.96" target="#b8">[9]</ref> as its embedder 15% 3%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 14</head><p>The table shows the performance of selected BERT based transformers models for precisely identifying the words forming the wordplay with exact matches on 9% of the records extracted from the provided training data-sets for task 1 via hold out approach. If the model fails to locate even a single character of the wordplay in the given English text, then we have counted as an overall failure.</p><p>Table <ref type="table" coords="11,126.78,385.05,10.04,10.91" target="#tab_1">14</ref> showcases that the fine-tuned BERT-BASE <ref type="bibr" coords="11,329.97,385.05,12.76,10.91" target="#b3">[4]</ref> model delivers the best performance compared to other variants of KEY-BERT models on both versions of the training data sets. So, we have used this model to locate the words forming the wordplay among the listed English sentences in the provided test data sets for task 1. We have used the hold-out approach instead of the K cross-validation to evaluate the performance of BERT based transformer models for locating the wordplay in the given English sentences because the provided training data sets contain numerous instances where the length of the English sentences was one. And in all such instances, it was apparent that the given English sentence was itself the wordplay. So, obviously, the deep learning models will achieve a perfect score in such scenarios. Thus, using such instances for evaluating BERT based transformer models will result in an unfair boost in their performance. To mitigate this effect, we have used the hold-out approach and selectively extracted those records from the training set in which the length of the English sentence was more than two. Additionally, we have downloaded the pre-trained DistilBERT <ref type="bibr" coords="11,434.47,547.64,17.81,10.91" target="#b9">[10]</ref> model from the hugging face repository to perform the text classification on the provided English sentence to predict the categorical label for the remaining target columns of the task 1 listed in table <ref type="table" coords="11,89.04,588.29,8.53,10.91" target="#tab_3">15</ref>. We have made five separate copies of the pre-trained DistilBERT <ref type="bibr" coords="11,402.23,588.29,18.07,10.91" target="#b9">[10]</ref> model and trained them individually to infer the label of each of the five categorical target columns of task 1. We have removed the "nan" values from both versions of the provided training data sets for task 1 and used them to evaluate the performances of all five copies of the DistilBERT <ref type="bibr" coords="11,487.91,628.93,18.07,10.91" target="#b9">[10]</ref> model independently via 10-fold cross-validation. Table <ref type="table" coords="11,346.15,642.48,10.35,10.91" target="#tab_3">15</ref> shows the mean accuracy of the DistilBERT <ref type="bibr" coords="11,142.91,656.03,18.06,10.91" target="#b9">[10]</ref> model on both versions of the training data set for correctly predicting the categorical labels of all the five mentioned target columns. Comparatively, we can observe that the DistilBERT <ref type="bibr" coords="12,158.70,86.97,17.93,10.91" target="#b9">[10]</ref> model has provided more promising results for the first data set provided by the JOKER CLEF 2022 <ref type="bibr" coords="12,203.18,100.52,12.84,10.91" target="#b2">[3]</ref> team for task 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Column Name Mean Accuracy of 10 fold cross validation on training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 16</head><p>The table states the obtained scores on the predictions generated by our fine-tuned models for the test records of test set-2 of task 1. The scores were allotted by evaluators from the JOKER CLEF 2022 <ref type="bibr" coords="12,494.02,430.61,11.97,9.96" target="#b2">[3]</ref> team, and the scoring criterion was straightforward. They have given a point for correctly predicting the value of a target column for each of the provided English texts in the second test set of task 1.</p><p>Lastly, we have selected the fine-tuned BERT-BASE <ref type="bibr" coords="12,326.88,493.44,12.69,10.91" target="#b3">[4]</ref> model for locating the words forming the wordplay in the English sentences listed in both versions of the test sets. Along with five separate copies of the fine-tuned DistilBERT <ref type="bibr" coords="12,289.27,520.54,17.90,10.91" target="#b9">[10]</ref> model for predicting labels of the remaining five categorical target columns of task 1 and submitted our predictions to the evaluators of the JOKER CLEF 2022 <ref type="bibr" coords="12,191.25,547.64,12.99,10.91" target="#b2">[3]</ref> workshop. We were the only team that successfully submitted the predictions for the test set-1. Thus for the first test set, we have implicitly got the first rank, so the evaluators haven't released any other statistical details or scores for the test set-1 of task 1. Furthermore, for test set-2, we have managed to get ourselves among the top three positions. The table 16 reveals the scores for the generated predictions from our fine-tuned models to correctly predict the values of all the target columns for each English text listed in the test set-2 of task 1. It's important to note that the evaluators have awarded a score of one point for predicting a correct value for each target column of task 1 against an English text containing a wordplay from test set-2. Plus, the evaluators have not evaluated the predictions for the "cultural reference", "conventional form", and "offensive" columns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">IMPLEMENTATION OF TASK-2: Translate words containing wordplay</head><p>We have downloaded pre-trained CamemBERT <ref type="bibr" coords="13,311.31,107.54,18.07,10.91" target="#b10">[11]</ref> and DistilBERT <ref type="bibr" coords="13,408.17,107.54,18.07,10.91" target="#b11">[12]</ref> models and finetuned them to perform the task of extractive question answering for task 2. The CamemBERT <ref type="bibr" coords="13,89.29,134.63,18.02,10.91" target="#b10">[11]</ref> model is pre-trained in the French language to retrieve answers for the provided French queries in the French context. Contrastingly, the DistilBERT <ref type="bibr" coords="13,364.55,148.18,18.06,10.91" target="#b11">[12]</ref> model is pre-trained in the English language to extract answers in the English context for the given English questions. The reason for choosing these two distinct pre-trained models designed for different languages is because the contents of the transformed data set for task 2 consist of both French and English language. In the previous section, we have observed that our approach has transformed the problem of translating single-word English nouns to their equivalent French versions into the extractive question answering domain. As a result of this transformation, the extractive question-answering models have to process the input question in English and the associated context in the French language. So now, the task of the extractive question-answering models is to extract the French translation for the given English query from the French context. Because of this heterogeneity of different languages in the transformed data set, we have utilized two different English and French pre-trained extractive question-answering models and compared their performance. We have used 10-fold cross-validation to evaluate the performance of both the DistilBERT <ref type="bibr" coords="13,156.37,324.32,17.76,10.91" target="#b11">[12]</ref> and CamemBERT <ref type="bibr" coords="13,255.48,324.32,17.76,10.91" target="#b10">[11]</ref> models on the transformed training data set. Table <ref type="table" coords="13,496.29,324.32,9.94,10.91" target="#tab_0">17</ref> shows the mean performance of both models by stating the average percentages of predictions that exactly matched the expected translations in French for the given English nouns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Name</head><p>Mean Accuracy of 10 fold cross validation on the transformed training set CamemBERT <ref type="bibr" coords="13,216.08,404.52,16.46,9.96" target="#b10">[11]</ref> 59% DistilBERT <ref type="bibr" coords="13,211.06,416.87,16.46,9.96" target="#b11">[12]</ref> 94%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 17</head><p>The table shows the performance of the DistilBERT <ref type="bibr" coords="13,300.21,449.08,17.71,9.96" target="#b11">[12]</ref> and CamemBERT <ref type="bibr" coords="13,388.44,449.08,22.19,9.96" target="#b10">[11]</ref> models for performing extractive question answering across the ten folds of the transformed training data set of task 2 after getting fine-tuned for less than five epochs.</p><p>Table <ref type="table" coords="13,127.01,513.49,10.15,10.91" target="#tab_0">17</ref> also reveals that the fine-tuned DistilBERT <ref type="bibr" coords="13,332.60,513.49,17.91,10.91" target="#b11">[12]</ref> model generates more satisfactory predictions compared to the CamemBERT <ref type="bibr" coords="13,275.33,527.04,17.76,10.91" target="#b10">[11]</ref> model. We have used the DistilBERT <ref type="bibr" coords="13,458.67,527.04,17.76,10.91" target="#b11">[12]</ref> model to generate the French translations for the English nouns of the test set, and our submission ranked first in the competition. The test set consists of a total of 284 English nouns that include named entities from official movies, and novels for which official French translations were available. However, the evaluators have also considered unofficial translations for a few English nouns, which were popular among the native French audience. The evaluators have used simple case insensitive string matching to evaluate official French translations for English-named entities with their expected French versions. But string matching can not be used to assess unofficial translations because they are not part of authentic literature. So, the evaluators have manually assessed the unofficial translations based on lexical field preservation, sense preservation, comprehensibility, and the formed wordplay. The table <ref type="table" coords="13,402.27,662.53,10.31,10.91" target="#tab_0">18</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 18</head><p>The table shows the obtained scores on the translations produced by our fine-tuned models for the test records of task 2, along with a brief description of what the mentioned numbers against each testing parameter represent. The official paper released by JOKER CLEF <ref type="bibr" coords="14,367.22,352.70,11.97,9.96" target="#b2">[3]</ref> 2022 organizers discusses the mentioned results in more detail. And also showcase a thorough comparison of the performance of all participating teams on the provided test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">IMPLEMENTATION OF TASK-3: Translate entire phrases containing wordplay.</head><p>The training data set provided for task 3 only comprised 1,185 unique English sentences for which there exist multiple valid French translations. Because the data set was not huge, we have decided not to fine-tune pre-train models on the training data set. Our goal was to select the pre-trained model that generates predictions that, on average, provide the highest BLEU <ref type="bibr" coords="14,89.29,506.99,17.76,10.91" target="#b12">[13]</ref> scores and least TER <ref type="bibr" coords="14,200.99,506.99,17.76,10.91" target="#b13">[14]</ref> scores for the given English phrases from the training set without fine-tuning. We have assumed that the high BLEU <ref type="bibr" coords="14,311.70,520.54,17.77,10.91" target="#b12">[13]</ref> scores and low TER <ref type="bibr" coords="14,421.05,520.54,17.77,10.91" target="#b13">[14]</ref> scores indicate that the generated French translations by the model for the given English phrases are more similar to expected French translations. It's a weak assumption, but still, we have made this architectural choice to design the solution for task 3. We have downloaded four popular pretrained sequences to sequence transformer models from the hugging face repositories <ref type="bibr" coords="14,467.00,574.74,12.70,10.91" target="#b5">[6]</ref> listed in the table 19 and evaluated their performance on the provided training set of task 3. Table <ref type="table" coords="14,89.04,601.84,10.35,10.91" target="#tab_0">19</ref> mentions the names of the selected models and states their performance with the help of BLEU <ref type="bibr" coords="14,116.93,615.39,17.76,10.91" target="#b12">[13]</ref> and TER <ref type="bibr" coords="14,177.45,615.39,17.76,10.91" target="#b13">[14]</ref> scores. We have iteratively extracted English texts from the data-set of task 3 and provided it as an input to these four models to evaluate their predictions. We have recorded BLEU <ref type="bibr" coords="14,158.80,642.48,17.86,10.91" target="#b12">[13]</ref> and TER <ref type="bibr" coords="14,220.01,642.48,17.86,10.91" target="#b13">[14]</ref> scores for each generated prediction by the four models and then calculated the average to rank the overall performance of the models on the entire training set.</p><p>Model Name AVERAGE BLEU SCORE AVERAGE TER SCORE Helsinki-NLP/opus-mt-en-fr <ref type="bibr" coords="15,230.29,102.62,16.46,9.96" target="#b14">[15]</ref> 18.43 0.80 GOOGLE T5 BASE <ref type="bibr" coords="15,211.18,114.97,16.46,9.96" target="#b15">[16]</ref> 12.64 0.84 GOOGLE T5 SMALL <ref type="bibr" coords="15,214.98,127.33,16.46,9.96" target="#b16">[17]</ref> 11.07 0.85 GOOGLE T5 LARGE <ref type="bibr" coords="15,214.72,139.68,16.46,9.96" target="#b17">[18]</ref> 11.90 0.84</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 19</head><p>The table presents the average BLEU <ref type="bibr" coords="15,247.38,171.89,16.61,9.96" target="#b12">[13]</ref> and TER <ref type="bibr" coords="15,305.36,171.89,16.60,9.96" target="#b13">[14]</ref> scores achieved by the selected pre-trained sequence-to-sequence transformer models for translating the English texts provided in the training data set of task 3 to their equivalent French versions along with the composed wordplays. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 20</head><p>The table shows the obtained scores on the translations produced by our fine-tuned Helsinki-NLP/opusmt-en-fr <ref type="bibr" coords="15,126.02,599.84,16.38,9.96" target="#b14">[15]</ref> model for the test records of task 3, along with a brief description of what the mentioned numbers against each testing parameter represent. The official paper released by JOKER CLEF <ref type="bibr" coords="15,494.02,611.80,11.97,9.96" target="#b2">[3]</ref> 2022 discusses the mentioned results in more detail. And also showcase a thorough comparison of the performance of all participating teams on the provided test set.</p><p>The table 19 entails that the Helsinki/NLP/opus-mt-en-fr <ref type="bibr" coords="16,367.76,86.97,18.07,10.91" target="#b14">[15]</ref> model has given the best performance and produced more desired translation as compared to other models. We have used this model to generate predictions for English phrases listed in the test set of task 3 and received second position in the contest. There were a total of 2378 English phrases provided in the test set, and we have generated a French translation for each of them. So, our submission file contains a list of 2378 pairs of English and French sentences. The evaluators have manually scored each of the submitted French translations using thirteen different parameters to rate the quality of the generated predictions. Table <ref type="table" coords="16,276.32,181.81,9.94,10.91">20</ref> shows a list of these parameters and explains how the evaluators have used each of them. The table also lists the obtained scores of our submitted predictions against each of the listed thirteen parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>The BERT-Base model has given the best performance for locating the words forming the wordplay in the English texts of the task 1 data set. And the DistilBERT model has achieved the best performance in predicting classification labels for the remaining target columns of task 1. The DisitilBERT model successfully delivered more than 89% accuracy in predicting all of the categorical target columns of task 1, except the manipulation type of wordplay. We think in the future, the results of the DistilBERT model can be compared with other popular text classification BERT alternatives to rank its performance. Additionally, we haven't employed the large BERT variants to locate the wordplay in the given English text of the task 1 data set, but we strongly think that they will boost the performance of our approach. The main highlight of our work is our designed technique for solving task 2 in Question/Answering style. And our submission of task 2 is also ranked top in the competition. An extension of this work can be to test the approach with different language pairs because, in this paper, we have only evaluated it on English/French noun pairs as per task 2 requirements. The DistilBERT model again delivers the best performance for solving task 2. And accurately predicts 96% of the French translations for the given English nouns in the extractive question/answer style. Lastly, we have concluded that Helsinki-NLP/opus-mt-en-fr model has provided the best performance on the task 3 data set by achieving 18.43 and 0.80 averaged BLEU and TER scores. The data set of task 3 doesn't contain a large number of records in it. We think in the future, increasing the size of the English/French parallel corpus containing wordplay and humour will benefit in excelling the research and will immensely help in training and evaluating the sequence-to-sequence models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,537.44,39.41,8.93;4,98.26,439.94,183.77,85.16"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3:</figDesc><graphic coords="4,98.26,439.94,183.77,85.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,89.29,230.14,380.24,9.96"><head>{"Figure 5 :</head><label>5</label><figDesc>Figure 5: The figure shows the conversion of table 13 data records into a single JSON object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,204.23,417.23,461.06"><head>Table 1</head><label>1</label><figDesc>The table shows the structure of the first training data set provided by the JOKER CLEF team for task 1 with the 531 records. The data set can be used for training the models to predict the value for each of the columns given a wordplay along with its Id.</figDesc><table coords="5,88.99,204.23,417.23,461.06"><row><cell cols="2">No. Column Names</cell><cell cols="2">No. Column Names</cell></row><row><cell>1</cell><cell>ID</cell><cell>1</cell><cell>ID (Given)</cell></row><row><cell>2</cell><cell>WORDPLAY</cell><cell>2</cell><cell>WORDPLAY (Given)</cell></row><row><cell>3</cell><cell>TARGET_WORD</cell><cell>3</cell><cell>TARGET_WORD (Empty)</cell></row><row><cell>4</cell><cell>DISAMBIGUATION</cell><cell>4</cell><cell>DISAMBIGUATION (Empty)</cell></row><row><cell>5</cell><cell>HORIZONTAL/VERTICAL</cell><cell>5</cell><cell>HORIZONTAL/VERTICAL (Empty)</cell></row><row><cell>6</cell><cell>MANIPULATION_TYPE</cell><cell>6</cell><cell>MANIPULATION_TYPE (Empty)</cell></row><row><cell>7</cell><cell>MANIPULATION_LEVEL</cell><cell>7</cell><cell>MANIPULATION_LEVEL (Empty)</cell></row><row><cell>8</cell><cell>CULTURAL_REFERENCE</cell><cell>8</cell><cell>CULTURAL_REFERENCE (Empty)</cell></row><row><cell>9</cell><cell>CONVENTIONAL_FORM</cell><cell>9</cell><cell>CONVENTIONAL_FORM (Empty)</cell></row><row><cell>10</cell><cell>OFFENSIVE</cell><cell>10</cell><cell>OFFENSIVE (Empty)</cell></row><row><cell></cell><cell></cell><cell>Table 2</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">The table shows the structure of the first test data</cell></row><row><cell></cell><cell></cell><cell cols="2">set provided by the JOKER CLEF team for task 1</cell></row><row><cell></cell><cell></cell><cell cols="2">with the 4517 number of records. For each given</cell></row><row><cell></cell><cell></cell><cell cols="2">pair of Wordplay and Id, the task is to predict the</cell></row><row><cell></cell><cell></cell><cell cols="2">values of each empty column.</cell></row><row><cell cols="2">No. Column Names</cell><cell cols="2">No. Column Names</cell></row><row><cell>1</cell><cell>ID</cell><cell>1</cell><cell>ID (Given)</cell></row><row><cell>2</cell><cell>WORDPLAY</cell><cell>2</cell><cell>WORDPLAY (Given)</cell></row><row><cell>3</cell><cell>LOCATION</cell><cell>3</cell><cell>LOCATION (Empty)</cell></row><row><cell>4</cell><cell>INTERPRETATION</cell><cell>4</cell><cell>INTERPRETATION (Empty)</cell></row><row><cell>5</cell><cell>HORIZONTAL/VERTICAL</cell><cell>5</cell><cell>HORIZONTAL/VERTICAL (Empty)</cell></row><row><cell>6</cell><cell>MANIPULATION_TYPE</cell><cell>6</cell><cell>MANIPULATION_TYPE (Empty)</cell></row><row><cell>7</cell><cell>MANIPULATION_LEVEL</cell><cell>7</cell><cell>MANIPULATION_LEVEL (Empty)</cell></row><row><cell>8</cell><cell>CULTURAL_REFERENCE</cell><cell>8</cell><cell>CULTURAL_REFERENCE (Empty)</cell></row><row><cell>9</cell><cell>CONVENTIONAL_FORM</cell><cell>9</cell><cell>CONVENTIONAL_FORM (Empty)</cell></row><row><cell>10</cell><cell>OFFENSIVE</cell><cell>10</cell><cell>OFFENSIVE (Empty)</cell></row><row><cell>Table 3</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">The table shows the structure of the second training</cell><cell></cell><cell></cell></row><row><cell cols="2">data set provided by the JOKER CLEF team for task</cell><cell></cell><cell></cell></row><row><cell cols="2">1 with the 2077 records. The data set can be used</cell><cell></cell><cell></cell></row><row><cell cols="2">for training the models to predict the value for each</cell><cell></cell><cell></cell></row><row><cell cols="2">of the columns given a wordplay along with its Id.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,301.50,595.10,204.73,34.32"><head>Table 4</head><label>4</label><figDesc>The table shows the structure of the second test data set provided by the JOKER CLEF team for task</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="12,98.06,127.10,411.86,85.19"><head>set-1 Mean Accuracy of 10 fold cross validation on training set-2</head><label></label><figDesc></figDesc><table coords="12,98.06,152.92,350.15,59.38"><row><cell>MANIPULATION_TYPE</cell><cell>65%</cell><cell>50%</cell></row><row><cell>MANIPULATION_LEVEL</cell><cell>99%</cell><cell>99%</cell></row><row><cell>HORIZONTAL/VERTICAL</cell><cell>93%</cell><cell>99%</cell></row><row><cell>CULTURAL_REFERENCE</cell><cell>96%</cell><cell>95%</cell></row><row><cell>CONVENTIONAL_FORM</cell><cell>92%</cell><cell>89%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="12,88.99,224.47,418.51,171.94"><head>Table 15</head><label>15</label><figDesc>The table shows the mean accuracies of the DistilBERT<ref type="bibr" coords="12,310.91,235.54,16.31,9.96" target="#b9">[10]</ref> model across ten folds of the training sets of task 1 for predicting labels of the categorical columns after getting fine-tuned for less than five epochs.</figDesc><table coords="12,116.56,298.46,344.45,97.94"><row><cell>Column Name</cell><cell>Obtained Scores on test set-2 having 3256 records</cell></row><row><cell>LOCATION</cell><cell>1455</cell></row><row><cell>MANIPULATION_TYPE</cell><cell>1667</cell></row><row><cell>MANIPULATION_LEVEL</cell><cell>2437</cell></row><row><cell>HORIZONTAL/VERTICAL</cell><cell>68</cell></row><row><cell>CULTURAL_REFERENCE</cell><cell>Not evaluated</cell></row><row><cell>CONVENTIONAL_FORM</cell><cell>Not evaluated</cell></row><row><cell>OFFENSIVE</cell><cell>Not evaluated</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="13,415.31,662.53,90.68,10.91"><head></head><label></label><figDesc>below demonstrates the obtained score of test submissions against each of the mentioned parameters.</figDesc><table coords="14,101.95,112.56,388.62,193.98"><row><cell>Metric</cell><cell>Score</cell><cell>Explanation</cell></row><row><cell>Total</cell><cell>284</cell><cell>Total number of records in the test set.</cell></row><row><cell>Not translated</cell><cell>0</cell><cell>Total number of records that are either missing or not translated in the submission file.</cell></row><row><cell>Official</cell><cell>250</cell><cell>Number of the official named entities that are correctly translated in the submission file.</cell></row><row><cell>Not Official</cell><cell>34</cell><cell>Number of translations in the submission file that are unofficial.</cell></row><row><cell>Lexical Field Preservation</cell><cell>16</cell><cell>Number of translations that preserve the lexical field of the source wordplay in the submission file.</cell></row><row><cell>Sense Preservation</cell><cell>13</cell><cell>Number of translations that preserve the sense of the source wordplay in the submission file.</cell></row><row><cell>Comprehensible Terms</cell><cell>26</cell><cell>Number of translations that do not exploit any specialized terms in the submission file.</cell></row><row><cell>Wordplay form</cell><cell>3</cell><cell>Number of translations that are itself wordplay in the submission file.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,308.69,631.41,197.29,9.96;5,301.80,643.37,204.18,9.96;5,301.80,655.33,97.08,9.96"><p>with the 3256 number of records. For each given pair of Wordplay and Id, the task is to predict values for each empty column.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5.">Acknowledgments</head><p>This research is supported by the school of computing <rs type="funder">National University of Computer and Emerging Sciences (FAST-NU)</rs>. I would like to thank my supervisor, <rs type="person">Dr. Muhammad Rafi</rs>, and <rs type="person">Dr. Atif Tahir</rs>, director of the <rs type="affiliation">university</rs>, for providing technical insights and expertise that greatly assisted the research. We also like to recognize the efforts of the JOKER 2022 team to organize this workshop and develop practical tasks along with datasets. I believe this workshop has provided a platform where students can apply their NLP knowledge to solve challenging problems and evaluate their understanding. We look forward to participating again in this event next year and wish this kind of event should happen more. Furthermore, we are also immensely grateful to <rs type="person">Liana Ermakova</rs> for keeping us posted about the updates of the event and solving our queries timely to help us throughout the event.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="17,112.66,172.69,395.00,10.91;17,112.66,186.24,257.21,10.91" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><surname>Pokémon</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/Pok%C3%A9mon" />
		<title level="m" coord="17,159.62,172.69,199.06,10.91">Pokémon -Wikipedia, the free encyclopedia</title>
		<imprint>
			<date type="published" when="2001-05-24">2001. May 24, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,199.79,394.04,10.91;17,112.26,213.34,195.38,10.91" xml:id="b1">
	<monogr>
		<ptr target="https://en.wikipedia.org/wiki/Asterix" />
		<title level="m" coord="17,112.66,199.79,226.78,10.91">Asterix, Asterix -Wikipedia, the free encyclopedia</title>
		<imprint>
			<date type="published" when="2001-05-24">2001. May 24, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,226.89,394.53,10.91;17,112.28,240.44,393.71,10.91;17,112.14,253.99,395.05,10.91;17,112.66,267.54,394.53,10.91;17,112.66,281.08,395.01,10.91;17,112.66,294.63,393.59,10.91;17,112.66,308.18,77.31,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="17,311.74,240.44,194.25,10.91;17,112.14,253.99,200.68,10.91">Overview of JOKER@CLEF 2022: Automatic Wordplay and Humour Translation workshop</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Regattin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A.-G</forename><surname>Bosser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L</forename><surname>Mathurin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Corre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Araújo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Boccou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Digue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Damoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Campen</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jeanjean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,183.84,281.08,319.46,10.91;17,112.66,294.63,393.59,10.91">Proceedings of the Thirteenth International Conference of the CLEF Association (CLEF</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Degli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Esposti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sebastiani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Macdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Pasi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hanbury</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Potthast</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><surname>Ferro</surname></persName>
		</editor>
		<meeting>the Thirteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="17,112.66,321.73,393.33,10.91;17,112.66,335.28,395.01,10.91;17,112.66,348.83,187.21,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="17,319.43,321.73,186.56,10.91;17,112.66,335.28,180.57,10.91">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,362.38,394.53,10.91;17,112.33,375.93,393.66,10.91;17,112.66,389.48,394.53,10.91;17,112.66,403.03,330.99,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="17,178.57,362.38,182.36,10.91">Parallel data, tools and interfaces in opus</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,452.90,375.93,53.09,10.91;17,112.66,389.48,394.53,10.91;17,112.66,403.03,223.19,10.91">Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC&apos;12), European Language Resources Association (ELRA)</title>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">C C</forename><surname>Chair</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">)</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Choukri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Declerck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">U</forename><surname>Dogan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Maegaard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Mariani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Odijk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Piperidis</surname></persName>
		</editor>
		<meeting>the Eight International Conference on Language Resources and Evaluation (LREC&apos;12), European Language Resources Association (ELRA)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,416.58,394.53,10.91;17,112.66,430.13,394.53,10.91;17,112.66,443.67,395.17,10.91;17,112.66,457.22,393.33,10.91;17,112.66,470.77,394.53,10.91;17,112.66,484.32,385.60,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="17,315.63,443.67,192.20,10.91;17,112.66,457.22,72.82,10.91">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m" coord="17,207.25,457.22,298.74,10.91;17,112.66,470.77,390.37,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,497.87,394.04,10.91;17,112.66,511.42,180.03,10.91" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="17,160.98,497.87,198.32,10.91">Fast-mt team submission for joker clef 2022</title>
		<author>
			<persName coords=""><surname>Farhan</surname></persName>
		</author>
		<ptr target="https://github.com/FarhanDhanani/joker-clef-22-FAST-MT" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,524.97,394.62,10.91;17,112.31,538.52,302.76,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="17,195.82,524.97,220.28,10.91">Keybert: Minimal keyword extraction with bert</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Grootendorst</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4461265</idno>
		<ptr target="https://doi.org/10.5281/zenodo.4461265.doi:10.5281/zenodo.4461265" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,552.07,394.53,10.91;17,112.66,565.62,395.17,10.91;17,112.66,579.17,395.01,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="17,219.74,552.07,282.85,10.91">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1908.10084" />
	</analytic>
	<monogr>
		<title level="m" coord="17,126.77,565.62,381.06,10.91;17,112.66,579.17,201.93,10.91">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,592.72,394.53,10.91;17,112.66,606.27,394.04,10.91;17,112.66,619.81,203.26,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno>ArXiv abs/1910.01108</idno>
		<ptr target="https://huggingface.co/distilbert-base-uncased" />
		<title level="m" coord="17,303.00,592.72,204.19,10.91;17,112.66,606.27,117.90,10.91">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<imprint>
			<date type="published" when="2019">2019. 2022-05-24</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,633.36,394.52,10.91;17,112.66,646.91,393.33,10.91;17,112.66,660.46,284.05,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="17,154.95,646.91,187.97,10.91">Camembert: a tasty french language model</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J O</forename><surname>Suárez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Romary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">É</forename><forename type="middle">V</forename><surname>De La Clergerie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sagot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,366.37,646.91,139.62,10.91;17,112.66,660.46,254.40,10.91">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,86.97,394.53,10.91;18,112.66,100.52,395.01,10.91;18,112.66,114.06,394.98,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="18,303.00,86.97,204.19,10.91;18,112.66,100.52,230.34,10.91">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter (for question answering)</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<ptr target="https://huggingface.co/distilbert-base-cased-distilled-squad" />
	</analytic>
	<monogr>
		<title level="m" coord="18,366.80,100.52,109.49,10.91">NeurIPS EMC Workshop</title>
		<imprint>
			<date type="published" when="2019-05-24">2019. May 24, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,127.61,393.33,10.91;18,112.66,141.16,393.33,10.91;18,112.66,154.71,394.53,10.91;18,112.66,168.26,397.48,10.91;18,112.66,184.25,121.09,7.90" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="18,318.69,127.61,187.29,10.91;18,112.66,141.16,100.78,10.91">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
		<ptr target="https://aclanthology.org/P02-1040.doi:10.3115/1073083.1073135" />
	</analytic>
	<monogr>
		<title level="m" coord="18,237.08,141.16,268.90,10.91;18,112.66,154.71,133.28,10.91">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,195.36,393.33,10.91;18,112.26,208.91,393.73,10.91;18,112.66,222.46,393.33,10.91;18,112.33,236.01,394.94,10.91;18,112.66,249.56,203.73,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="18,368.80,195.36,137.19,10.91;18,112.26,208.91,139.74,10.91">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Makhoul</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2006.amta-papers.25" />
	</analytic>
	<monogr>
		<title level="m" coord="18,274.58,208.91,231.41,10.91;18,112.66,222.46,393.33,10.91;18,112.33,236.01,125.40,10.91">Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers, Association for Machine Translation in the Americas</title>
		<meeting>the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers, Association for Machine Translation in the Americas<address><addrLine>Cambridge, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="223" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,263.11,394.53,10.91;18,112.66,276.66,393.32,10.91;18,112.33,290.20,394.36,10.91;18,112.66,303.75,173.57,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="18,236.90,263.11,265.27,10.91">OPUS-MT -Building open translation services for the World</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Thottingal</surname></persName>
		</author>
		<ptr target="https://huggingface.co/Helsinki-NLP/opus-mt-en-fr" />
	</analytic>
	<monogr>
		<title level="m" coord="18,127.20,276.66,378.79,10.91;18,112.33,290.20,87.52,10.91">Proceedings of the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT)</title>
		<meeting>the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-05-24">2020. May 24, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,317.30,394.53,10.91;18,112.66,330.85,394.53,10.91;18,112.48,344.40,394.70,10.91;18,112.66,357.95,104.39,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="18,112.66,330.85,390.33,10.91">Exploring the limits of transfer learning with a unified text-to-text transformer (t5 base)</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://huggingface.co/t5-base" />
	</analytic>
	<monogr>
		<title level="j" coord="18,112.48,344.40,166.50,10.91">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020-05-24">2020. May 24, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,371.50,394.53,10.91;18,112.66,385.05,394.52,10.91;18,112.48,398.60,394.70,10.91;18,112.66,412.15,104.39,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="18,112.66,385.05,390.38,10.91">Exploring the limits of transfer learning with a unified text-to-text transformer (t5 small)</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://huggingface.co/t5-small" />
	</analytic>
	<monogr>
		<title level="j" coord="18,112.48,398.60,164.70,10.91">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020-05-24">2020. May 24, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,425.70,394.53,10.91;18,112.66,439.25,394.52,10.91;18,112.48,452.79,394.70,10.91;18,112.66,466.34,104.39,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="18,112.66,439.25,390.56,10.91">Exploring the limits of transfer learning with a unified text-to-text transformer (t5 large)</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://huggingface.co/t5-large" />
	</analytic>
	<monogr>
		<title level="j" coord="18,112.48,452.79,165.30,10.91">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020-05-24">2020. May 24, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
