<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,367.03,15.42;1,88.69,106.66,303.75,15.42">Use of SimpleT5 for the CLEF workshop JokeR: Automatic Pun and Humor Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,89.29,134.97,70.18,11.96"><forename type="first">Loïc</forename><surname>Glemarec</surname></persName>
							<email>loic.glemarec3010@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Bretagne Occidentale</orgName>
								<orgName type="institution" key="instit2">UBO</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,367.03,15.42;1,88.69,106.66,303.75,15.42">Use of SimpleT5 for the CLEF workshop JokeR: Automatic Pun and Humor Translation</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">8A7FEB6C53ACE7DBE3CDA8500F30A0D9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Computational Humor</term>
					<term>CLEF</term>
					<term>JokeR</term>
					<term>T5 models</term>
					<term>SimpleT5</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present our work in the JokeR workshop. JokeR is a project aiming to study the strategies of localization of humor and puns and to create a multilingual parallel corpus and evaluation metrics in order to put a step forward to Automatic Pun and Humor Translation. 3 Tasks were proposed and to predict the requested fields for each of them, we trained the T5 model specialist in natural language processing tasks. Then we applied the models on test datasets, generating the runs as requested. By doing this we propose a simple method to study the translatability of puns. Also, we propose another way to implement our method to obtain more varied results. All this is to study the multilingual character of wordplay.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the context of the CLEF workshop: JokeR <ref type="bibr" coords="1,291.93,379.58,11.28,10.91" target="#b0">[1]</ref>, 3 tasks were proposed: 1) Wordplay instances classification, 2) Single-words-wordplay translation, 3) Wordplay-containing-phrases translation. We propose to use the T5 model <ref type="bibr" coords="1,234.64,406.68,11.42,10.91" target="#b1">[2]</ref>. T5 models (T5, mT5 or byT5) are mainly used for natural language processing tasks and makes use of Transformers <ref type="bibr" coords="1,347.77,420.23,11.35,10.91" target="#b2">[3]</ref>. To train it, we use the SimpleT5 library <ref type="bibr" coords="1,123.49,433.78,11.58,10.91">[4]</ref>. SimpleT5 is design from PyTorch-lightning <ref type="bibr" coords="1,346.01,433.78,11.58,10.91" target="#b3">[5]</ref>, a high-level python library for PyTorch <ref type="bibr" coords="1,127.86,447.33,11.27,10.91" target="#b4">[6]</ref>, a machine learning framework. SimpleT5 provides an easier way to use pre-trained models and makes it easy to train our T5 models with a few lines of code. First, we'll explain our method, how to train the model, and how to use it. In a second step, we will present our results, and explain why they are not so good, then we propose a way to obtain better results while maintaining our process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>The objective is for tasks 2 and 3 to translate from English to French or for task 1 to predict classification fields for a given sentence. To do that, the method is the same for each task. First, we train the t5 model to predict what we want, and then we generate the runs with a script that apply the model to the input dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Training scripts</head><p>The training scripts are divided into 2 parts, the first is the dataset reading and its formatting. Then comes the training of the model.</p><p>For tasks 2 and 3, the input dataset is three-fields-composed, the identifiers, the words/sentences in English then the same words or sentences in French.</p><p>Before doing anything, we start by cleaning the data, that is to say, we go through all the existing fields by checking the correspondence of the types as well as the content. Indeed, a column that must contain character strings must not contain anything other than strings, likewise, no null values must appear in the entire dataset.</p><p>So we start by separating the data in the form of data-frames into two parts. A train part and a test part. These two data-frames are then mixed. We end up removing the identifiers field and renaming the "en" and "fr" columns to "source_text" and "target_text" respectively. (cf. Table <ref type="table" coords="2,497.57,383.79,4.09,9.72">1</ref>)</p><formula xml:id="formula_0" coords="2,121.38,407.67,51.49,8.96">"source_text"</formula><p>"target_text" My boyfriend and I started to date after he backed his car into mine. We met by accident.</p><p>Mon petit ami et moi avons commencé à sortir ensemble après qu'il ait reculé sa voiture contre la mienne. Nous nous sommes rencontrés par accident.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1 Table detailing the precision of each field of Task 1</head><p>Now the model training can begin by using the SimpleT5 librarie. We first load T5 model, before running the training method. Now that the T5 model is trained, it is ready to predict the "target_text" field according to the "source_text" given to it.</p><p>Note that we have given the example of Tasks 3 because it is simpler and more condensed to present, but concerning Task 1, it is the same process. The only difference is in the number of fields to predict, so we have 1 training per field to predict. The shape of the data-frames is the same too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Generalizing script</head><p>Now that we have a trained model, we can launch our runs. Of course, just like during the training, we must clean the input dataset, to make sure to not have erroneous values.</p><p>For this, we just need to have a data frame with a column containing values similar to the preceding "source_text" fields. Then we have to load the trained model. Then using the apply method of the Pandas library, we can predict one by one all the desired corresponding values ("target_text").</p><p>We finish everything by formatting all the results in the JOKER-requested way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Method summarization</head><p>We have seen that to predict each desired field in tasks 1, 2, and 3, it was necessary each time to organize and format the input data to train the T5 model. We also saw that once a model was trained, it was then possible to apply it to a given input dataset. Without forgetting to be rigorous about the formatting of the output data which must correspond to the given format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Task 1</head><p>Task 1 is quite special due to its number of fields to predict. Therefore we have chosen to train a model for each of these fields. It was quite possible and even better to couple certain fields during the prediction.</p><p>So here are the different elements that we have been able to observe : (cf. A)</p><p>• Location : location field predictions are globally accurate (80 -Short wordplays : they contain only 2 or 3 words, and the pun is on these few words, the model, therefore, predicts that the location is on these words. -Sentences with wordplays : In these cases, the location of the puns is often at the end of the sentences and the model easily locates them.</p><p>But in these two cases, it is impossible to say if the model is really precise or if it is based only on the standard of the input data. In fact, as for short puns or sentences, the location is very much the same, the model could very well base itself on this to predict the locations. Therefore it would not predict the location of the pun but the normal location it is supposed to have. • Interpretation : regarding this field, we can see different behaviors but none is really correct :</p><p>-Sometimes the interpretation prediction is just bad and doesn't make sense. The model takes a word and displays it two times.</p><p>-Other times the prediction just took the two worded wordplay. It doesn't give us any explanation about the wordplay. -Otherwise, the prediction took only one word from the original text. So that doesn't give us an explanation either. -Then, there are some other behaviors, but there are never precisely explaining the wordplay.</p><p>• Manipulation level : all rows are predicted as "Sound", which is not always true nor false. And even if the "Sound" level is found and true, the interpretation is not able to find the initial word. (probably because both columns have to be trained together) • Horizontal and vertical : this field seems to be exclusively predicted as "vertical". </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Task 2</head><p>For this task, we want to translate term-based wordplay, e.g. the Pokémon names, from English to French (cf. B). The model has translation difficulties (especially because we had to use the T5 model and not the mT5 model which is more efficient with translations). The results are mostly the same word as the source. Sometimes there are changes with the letters, accents appear and some double letters become single.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Task 3</head><p>For this last task, it was a question of translating sentences containing puns (cf. C). All in all, the translation is not bad if a few small errors are disregarded. The subject of the sentences is often kept, the humorous effect is not always present, or is more complicated to understand after translation. Nevertheless, some sentences work very well in French and have been translated well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English "Some burglars are always looking for windows of opportunity. " French "Certains cambrioliers cherchent toujours des fenêtres d'opportunité. "</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3 Example of sentence with wordplay that as been translated from English to French retaining meaning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>Table <ref type="table" coords="5,114.05,224.69,49.47,8.96">representing</ref> the analyzes made on different runs submitted to JokeR, the color gradient makes it possible to identify which are the best in opposition to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>Based on our results, it's clear that our model needs much better training. Which in our case was complicated to do because of time and material constraints. Nevertheless, our results allow us to notice several things from the point of view of the training data. especially for task 1, certain classification elements must be more varied, to avoid the models being based on standards that would be detected.</p><p>• For example, the classification fields "manipulation_level" and "horizontal/vertical" are not varied enough.</p><p>Still concerning Task 1, it would surely be interesting to dissociate short puns and sentences containing puns. always with the aim of improving the models. Speaking of models, we used T5, but mT5 is the multi-lingual version of T5. Unfortunately we haven't been able to use it (again for hardware performance reasons). This multi-lingual version would surely have been better for translations. Now let's discuss the translation part, we were able to have our results analyzed and we can therefore compare them to other methods and models. Table <ref type="table" coords="5,362.86,504.09,5.10,10.91">4</ref> shows this, the lines represent the runs that were submitted to JokeR, ours being "CECILIA", the columns give us quantitative numerical indications of what was observed. The color code simply shows us who has the best results (green) and who has the worst (red) for each category. From this table (cf. Table <ref type="table" coords="6,212.54,204.81,3.57,9.72">2</ref>), it is much easier to give an opinion on our method and the use of T5. Indeed we have opted for a simple and quick implementation approach that may not be the case for the other proposals. We can then figure out the ratio between efficiency and translation accuracy, not only from the translation point of view as-it-is but also from the puns hilariousness transition aspect. Now, if we take the time to look at and compare our results to those of the other proposals, we observe several things. First, our method can translate the majority of sentences. Indeed, we present a small number of untranslated fields. However, regarding the presence of syntactic and lexical errors, we are at the opposite, less good. Regardless, this remains at a low ratio, given the total number of translations. Concerning the lexical and meaning preservation fields, we place ourselves this time at the forefront, especially with the second place for both, while remaining close to the first results. The number of understandable terms is not to our advantage, we place ourselves 4th out of 5, but we are still on the same number scale as the first. The number of pun forms is not the highest, but it remains the same magnitude as the best results. The same goes for identifiable puns. Note that we are the ones who show the highest number of over-translations, it is only about ten cases.</p><p>Concerning the translation of puns and considering this evaluation, it allows us to show our strong and weak points. Also, we can situate ourselves from others' submissions. What is clear is that our translation is globally good. Indeed our results always follow the norm with no figures standing out, whether for good or less. However, there is one that scores, the more than 1400 hilariousness shifts. We are certainly the only ones having correct translations while having a significantly higher number of translations that have lost their funny character. Our method, therefore, seems to be a good translator, which does not translate the second degree but which translates puns in the first degree (this can be likened to a form of humor, can't it ?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In the context of the JOKER workshop, we decided to use existing models and to train them to predict asked fields.</p><p>In summary, we don't have any significantly good results. But our method works, and there are only tracks of improvement, mainly around the models and their training. Only for task 1, our version: 1 model per field to predict works but is in our opinion not the only one possible, nor the most efficient. It is nevertheless simple to implement because it follows the same procedure as all the other models that we have been able to use. We indeed think that the coupling of several fields for the training of the models was a concrete track of improvement for the predictions or at least a good way to know what is the best way to do this. The next step would be an improvement in hardware capabilities to better train our models. Also, the choice of model is not fixed and other models can be used for our objectives such as Jurassic <ref type="bibr" coords="7,126.33,197.02,12.84,10.91" target="#b5">[7]</ref> for example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Online Resources</head><p>The sources for the three JokeR tasks are available via • GitHub,</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,107.28,566.58,135.58,10.91;5,107.28,581.29,175.27,10.91;5,107.28,596.01,296.26,10.91;5,107.28,610.72,214.01,10.91;5,107.28,625.44,290.61,10.91;5,107.28,640.15,289.03,10.91;5,107.28,654.87,353.25,10.91;5,107.28,669.58,400.38,10.91;6,107.28,86.97,127.21,10.91;6,107.28,101.87,294.88,10.91;6,107.28,116.77,354.17,10.91;6,107.28,131.68,94.80,10.91;6,107.28,146.58,349.75,10.91;6,107.28,161.49,398.71,10.91;6,116.29,175.04,23.08,10.91"><head>•</head><label></label><figDesc>Total : Total row submitted. • Valid : Number of valid submissions. • not-translated : Number of rows that have not been translated. • nonsens : Number of rows that have no sens. • syntax_problem : Number of rows that present syntax errors. • lexical_problem : Number of rows that present lexical errors. • lexical_field_preservation : Number of row that have retained lexical_field • sens_preservation : Number of translation that preserve the sens from english to french. • comprehensible_terms : • wordplay_form : Number of rows that present wordplay form. • identifiable_wordplay : Number of rows that present identifiable wordplay. • over-translation : • style_shift : Number of rows that present shift from the style point of view. • hilariousness_shift : Number of rows that present shift from the hilariousness point of view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="9,89.29,145.90,416.69,345.09"><head></head><label></label><figDesc></figDesc><graphic coords="9,89.29,145.90,416.69,345.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="10,89.29,110.26,145.85,507.17"><head></head><label></label><figDesc></figDesc><graphic coords="10,89.29,110.26,145.85,507.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,89.29,232.36,416.70,178.53"><head>•</head><label></label><figDesc>Manipulation type : This seems to be the most accurately predicted field, but again it's hard to tell if the model is really accurate or if it's not just based on the rate of training values. Which statistically, would allow him to be precise, by chance in a way. Speaking of statistics (cf. Table 2), despite high accuracy rates in certain fields, it remains difficult to know if our model is accurate, or if it is not based on a certain standard. This standard repeating itself in our test data, most likely allows our model to be accurate.</figDesc><table coords="4,138.89,340.26,317.50,70.63"><row><cell></cell><cell cols="5">Accuratcy rate Matches Mismatches Shapes Errors</cell></row><row><cell>location</cell><cell>0.804819</cell><cell>334</cell><cell>81</cell><cell>415</cell><cell>0</cell></row><row><cell cols="2">interpretation 0.0578313</cell><cell>24</cell><cell>391</cell><cell>415</cell><cell>0</cell></row><row><cell>hor/ver</cell><cell>0.985542</cell><cell>409</cell><cell>6</cell><cell>415</cell><cell>0</cell></row><row><cell>Manip_type</cell><cell>0.584541</cell><cell>242</cell><cell>172</cell><cell>414</cell><cell>0</cell></row><row><cell>Manip_level</cell><cell>0.99759</cell><cell>414</cell><cell>1</cell><cell>415</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,88.99,419.84,200.49,20.91"><head>Table 2 Table detailing</head><label>2detailing</label><figDesc></figDesc><table /><note coords="4,150.06,431.80,139.42,8.96"><p>the precision of each field of Task 1</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>I would like to thank <rs type="person">BOSSER Anne-Gwenn</rs> for the opportunity she gave me by offering me an internship, and for her help through her feedback and advice on my work. Also, thanks to <rs type="person">ERMAKOVA Liana</rs>, who provided me with lots of technical help and examples in a context where time was tight.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,107.59,451.27,399.59,10.91;7,107.59,464.82,398.66,10.91;7,107.06,478.37,400.12,10.91;7,107.59,491.92,398.40,10.91;7,107.59,505.47,399.60,10.91;7,107.59,519.02,270.41,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,481.59,464.82,24.66,10.91;7,107.06,478.37,280.27,10.91">CLEF Workshop JOKER: Automatic Wordplay and Humour Translation</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Puchalski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Regattin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mathurin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A.-G</forename><surname>Araújo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bosser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">L</forename><surname>Bokiniec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Corre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jeanjean</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hannachi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mallia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Saki</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-99739-7_45</idno>
	</analytic>
	<monogr>
		<title level="m" coord="7,391.75,491.92,114.23,10.91;7,107.59,505.47,38.77,10.91">Advances in Information Retrieval</title>
		<title level="s" coord="7,153.61,505.47,158.75,10.91">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Hagen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Verberne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Seifert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Balog</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Nørvåg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Setty</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="355" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,107.59,532.57,399.60,10.91;7,107.59,546.12,400.08,10.91;7,107.59,559.67,399.97,10.91" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,107.59,546.12,369.56,10.91">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683arXiv:1910.10683</idno>
		<ptr target="http://arxiv.org/abs/1910.10683" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct coords="7,107.59,573.21,399.60,10.91;7,107.41,586.76,400.42,10.91;7,107.59,600.31,399.10,10.91;7,107.59,613.86,318.20,10.91" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="7,415.89,586.76,91.94,10.91;7,107.59,600.31,179.15,10.91">Transformers: Stateof-the-Art Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Le</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno>T13:56:00Z</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-demos.6" />
		<imprint>
			<date type="published" when="2018-10-29">2020. 2018-10-29</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,107.59,640.96,399.10,10.91;7,107.59,654.51,318.50,10.91" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Falcon</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.3828935</idno>
		<ptr target="https://github.com/PyTorchLightning/pytorch-lightning.doi:10.5281/zenodo.3828935" />
		<title level="m" coord="7,155.66,640.96,210.46,10.91">The PyTorch Lightning team, PyTorch Lightning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.59,86.97,37.80,10.91;8,193.96,86.97,23.14,10.91;8,256.49,86.97,22.39,10.91;8,318.28,86.97,188.41,10.91;8,107.59,100.52,257.55,10.91;8,382.19,100.52,60.99,10.91;8,469.26,100.52,38.57,10.91;8,107.34,114.06,61.23,10.91" xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://github.com/pytorch/pytorch/blob/88fca3be5924dd089235c72e651f3709e18f76b8/CITATION" />
		<imprint>
			<date type="published" when="2016-08-26">2022. 2016-08-13T05:26:41</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,107.59,127.61,399.05,10.91;8,107.59,141.16,7.47,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Lieber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Lentz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
		<title level="m" coord="8,286.78,127.61,191.05,10.91">Jurassic-1: Technical Details and Evaluation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
