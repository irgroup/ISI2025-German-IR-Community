<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,350.97,15.42;1,89.29,106.66,357.67,15.42;1,88.69,128.58,84.79,15.43;1,89.29,150.91,398.16,11.96">Query-focused Extractive Summarisation for Biomedical and COVID-19 Complex Question Answering Macquarie University&apos;s Participation at BioASQ10 Synergy and BioASQ10b Phase</title>
				<funder>
					<orgName type="full">National Computational Infrastructure</orgName>
					<orgName type="abbreviated">NCI</orgName>
				</funder>
				<funder>
					<orgName type="full">Australian Government</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,89.29,176.82,59.79,11.96"><forename type="first">Diego</forename><surname>Moll√°</surname></persName>
							<email>diego.molla-aliod@mq.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Macquarie University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,350.97,15.42;1,89.29,106.66,357.67,15.42;1,88.69,128.58,84.79,15.43;1,89.29,150.91,398.16,11.96">Query-focused Extractive Summarisation for Biomedical and COVID-19 Complex Question Answering Macquarie University&apos;s Participation at BioASQ10 Synergy and BioASQ10b Phase</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">9C15DA42B3239097F333746982738106</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>BioASQ</term>
					<term>Synergy</term>
					<term>query-focused summarisation</term>
					<term>Biomedical</term>
					<term>COVID-19</term>
					<term>DistilBERT</term>
					<term>sBERT</term>
					<term>data-centric</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents Macquarie University's participation to the two most recent BioASQ Synergy Tasks (as per June 2022), and to the BioASQ10 Task B (BioASQ10b), Phase B. In these tasks, participating systems are expected to generate complex answers to biomedical questions, where the answers may contain more than one sentence. We apply query-focused extractive summarisation techniques. In particular, we follow a sentence classification-based approach that scores each candidate sentence associated to a question, and the ùëõ highest-scoring sentences are returned as the answer. The Synergy Task corresponds to an end-to-end system that requires document selection, snippet selection, and finding the final answer, but it has very limited training data. For the Synergy task, we selected the candidate sentences following two phases: document retrieval and snippet retrieval, and the final answer was found by using a DistilBERT/ALBERT classifier that had been trained on the training data of BioASQ9b. Document retrieval was achieved as a standard search over the CORD-19 data using the search API provided by the BioASQ organisers, and snippet retrieval was achieved by re-ranking the sentences of the top retrieved documents, using the cosine similarity of the question and candidate sentence. We observed that vectors represented via sBERT have an edge over tf.idf. BioASQ10b Phase B focuses on finding the specific answers to biomedical questions. For this task, we followed a data-centric approach. We hypothesised that the training data of the first BioASQ years might be biased and we experimented with different subsets of the training data. We observed an improvement of results when the system was trained on the second half of the BioASQ10b training data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The BioASQ challenge 1 organises shared tasks on biomedical semantic indexing and question answering. In this paper, we present Macquarie University's participation in several of these tasks. 2  Question Orteronel was developed for treatment of which cancer?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type factoid</head><p>Snippet Pooled-analysis was also performed, to assess the effectiveness of agents targeting the androgen axis via identical mechanisms of action (abiraterone acetate, orteronel).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exact answer castration-resistant prostate cancer</head><p>Ideal answer Orteronel was developed for treatment of castration-resistant prostate cancer. The Synergy tasks aim to evaluate technologies useful for the development of an end-to-end question answering (QA) system for questions about COVID-19 asked by biomedical experts. In particular, the Synergy tasks evaluate the quality of document retrieval over a snapshot of CORD-19 <ref type="bibr" coords="2,133.57,283.85,11.28,10.91" target="#b0">[1]</ref>, snippet retrieval, and the generation of "ideal answers" that may contain multiple sentences. We present our participation in the second BioASQ9 Synergy task that ran between May and June 2021, and the BioASQ10 Synergy task that ran between December 2021 and February 2022.</p><p>Task B of BioASQ focuses on biomedical semantic QA. Similar to the Synergy tasks, several technologies corresponding to components of an end-to-end QA system are evaluated. In contrast with the Synergy tasks, Task B of BioASQ has two distinct phases. Phase A evaluates the quality of document and snippet retrieval on a snapshot of PubMed<ref type="foot" coords="2,417.53,376.94,3.71,7.97" target="#foot_0">3</ref> , whereas Phase B, given a question, its question type ("summary", "factoid", "yesno", "list") , and a list of candidate snippets, evaluates the system's ability to find short answers ("exact answers") and long, possibly multi-sentence answers ("ideal answers"). Figure <ref type="figure" coords="2,316.68,419.34,5.17,10.91" target="#fig_0">1</ref> shows an example of a question and its question type, a correct snippet for the question, a correct exact answer, and a correct ideal answer. We present our participation in Task B, Phase B of BioASQ10, that ran between March and May 2022 (henceforth BioASQ10b, Phase B).</p><p>All of our contributions to the above tasks are based on a common question-answering architecture that we will describe in Section 2. Section 3 presents our participation in the Synergy tasks. Section 4 presents our participation in BioASQ10b, Phase B. Finally, Section 5 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Question Answering Architecture</head><p>The question-answering system that is the focus of our participation in all of the tasks presented in this paper is based on query-focused extractive summarisation. The architecture of the system is illustrated in Figure <ref type="figure" coords="2,223.59,599.91,3.74,10.91" target="#fig_1">2</ref>, and follows the classification set up proposed by <ref type="bibr" coords="2,452.39,599.91,11.43,10.91" target="#b1">[2]</ref>.</p><p>The query-focused summarisation system takes the question, a candidate sentence, and the sentence position <ref type="foot" coords="2,165.46,625.25,3.71,7.97" target="#foot_1">4</ref> , and calculates a sentence score. The system computes the word embeddings of the question and candidate sentence using a BERT architecture <ref type="bibr" coords="3,393.58,299.88,11.58,10.91" target="#b2">[3]</ref>. In particular, for the BioASQ9 Synergy task 2 we used ALBERT <ref type="bibr" coords="3,280.55,313.43,11.43,10.91" target="#b3">[4]</ref>, which was the best-performing system in <ref type="bibr" coords="3,485.98,313.43,12.00,10.91" target="#b4">[5]</ref>'s participation in BioASB8b <ref type="foot" coords="3,207.46,325.23,3.71,7.97" target="#foot_2">5</ref> . For the BioASQ10 Synergy task, we used DistilBERT <ref type="bibr" coords="3,459.58,326.98,11.58,10.91" target="#b5">[6]</ref>, which performed very well in <ref type="bibr" coords="3,189.87,340.53,11.83,10.91" target="#b1">[2]</ref>'s participation in BioASQ9b, and even outperformed BioBERT <ref type="bibr" coords="3,473.65,340.53,11.28,10.91" target="#b6">[7]</ref>. For BioASQ10, Phase B, we also used DistilBERT. Average pooling is then used to merge the word embeddings of the candidate sentence into the sentence embeddings. The sentence position is then concatenated to the sentence embeddings, and an additional intermediate dense layer is added. A final classification layer predicts the sentence score.</p><p>The question and the sentence were fed to BERT in the same way as defined by the creators of BERT <ref type="bibr" coords="3,129.38,421.82,11.51,10.91" target="#b2">[3]</ref>. That is, the input consisted of an initial "[CLS]" token, followed by the question text, then a "[SEP]" token that indicates a new sentence, and finally the candidate sentence text. This information was passed to BERT, indicating the question and the candidate sentence as two separate text segments.</p><p>The classification labels used for training the system were automatically generated from the training data, based on the ROUGE score of the candidate sentence with respect to the annotated ideal answer. In particular, given a particular question, the top 5 sentences according to their ROUGE score were labelled as 1, and the rest were labelled as 0. For the Synergy tasks we used the BioASQ9b training data, whereas for BioASQ10b, Phase B, we used the BioASQ10b training data.</p><p>We used the pre-trained ALBERT and DistilBERT models available by Huggingface<ref type="foot" coords="3,469.84,555.56,3.71,7.97" target="#foot_3">6</ref> . These models were frozen during training, so that only the weights of the additional layers shown in Figure <ref type="figure" coords="3,120.36,584.41,5.07,10.91" target="#fig_1">2</ref> were updated.</p><p>improvement of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Synergy Tasks</head><p>This section describes the systems that participated in the Synergy task 2 of BioASQ9, and the Synergy task of BioASQ10 (in this paper, we will use the collective expression "the Synergy tasks" to refer to these). The Synergy task 2 of BioASQ9 ran in 2021 but the results were not made available at the time of the paper submission deadline for BioASQ9. For this reason, we are describing the system in this paper.</p><p>Our participation in the Synergy tasks share the same question answering system architecture described in Section 2. The only difference between the two Synergy tasks is, as mentioned in Section 2, that the BioASQ9 Synergy 2 system used ALBERT, whereas the BioASQ10 Synergy system used DistilBERT. In both cases, the system was trained with the training data of BioASQ9b.</p><p>To generate the candidate sentences required by the question answering system, we followed this procedure:</p><p>1. Retrieve the most relevant documents as described in Section 3.1; 2. Split the retrieved documents into sentences and select the candidate sentences as described in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Document Retrieval</head><p>The relevant documents were retrieved using the search API provided by the organisers of the BioASQ Synergy task. This API is based on a Web service that accepts a query and returns a JSON data structure. We simply used the unmodified question as the search query. In subsequent work we are exploring pre-processing and fine-tuning steps to improve the quality of the Document Retrieval stage. The final runs submitted consist of the top 10 documents, after removing those that were in previous feedback, to conform with the submission requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Snippet Retrieval</head><p>Every sentence from every retrieved document was a candidate snippet. This includes sentences from documents that were retrieved but were not submitted in the Document Retrieval runs. We then experimented with the combination of 2 dimensions to re-rank the candidate snippets, for a total of 4 different approaches.</p><p>The first dimension was based on the calculation of the similarity between the question and candidate snippet. We experimented with the following two options: TfidfCosine. We represented the question and candidate sentences using tf.idf. Each candidate sentence was then scored based on the cosine similarity between the question vector and the sentence vector. sBERTCosine. We used sBERT <ref type="bibr" coords="5,244.89,168.43,12.99,10.91" target="#b7">[8]</ref> to represent the question and the candidate sentences, and to determine the similarities between the question and the sentences. We used the default set up for sBERT, which computes the cosine similarity between the question vector and the sentence vector.</p><p>The second dimension was based on the criteria used for the final ranking of the candidate sentences. We experimented with local sorting and global sorting.</p><p>LocalSorting. For every relevant document, we extracted the top 3 sentences according to the cosine similarity approaches described above. The final list of sentences was composed of the top 3 sentences from the top document, followed by the top 3 sentences of the second document, and so on.</p><p>GlobalSorting. In contrast to the local sorting approach, all sentences of all documents were now sorted according to their cosine similarity with the question, regardless of what document the snippets were obtained from.</p><p>The final runs submitted consist of the first 10 snippets, after removing those that were in previous feedback, to conform with the submission requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Answer Generation</head><p>As mentioned above, the question, candidate sentences, and sentence position were fed to the system illustrated in Figure <ref type="figure" coords="5,213.49,461.18,3.73,10.91" target="#fig_1">2</ref>. The sentence position was simply the unnormalised position of the sentence within the list of snippets, after the snippets have been ranked as described in Section 3.2. Given a question, the top-scoring ùëõ sentences according to the scores produced by the QA system were combined to form the final answer. These sentences were presented in the order of appearance in the list of snippets. The value of ùëõ was based on the question type and is shown in Table <ref type="table" coords="5,169.99,528.92,3.74,10.91" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Results of the Synergy Tasks</head><p>This section describes the results of the runs submitted to the Synergy tasks.</p><p>Table <ref type="table" coords="5,128.10,592.20,5.17,10.91" target="#tab_1">2</ref> shows the F1 score of the documents returned by our systems. As mentioned in Section 3.1, these documents were found by submitting the unmodified question as the query to the search API provided by the developers of the Synergy task. As expected, the results were poor relative to other submissions.</p><p>Table <ref type="table" coords="5,127.37,646.40,5.14,10.91" target="#tab_2">3</ref> shows the F1 score of the snippets returned by our runs. For each run, we indicate the run name, the type of similarity used, and the type of sorting performed. We observe that, considering the poor quality of the documents retrieved, the snippets were of quality comparable to that of other runs of the BioASQ9 Synergy 2 task (but not the runs of the BioASQ10 Synergy task), but there is room for improvement. Among our runs, the most successful configuration was using sBERT cosine similarity and global sort. Table <ref type="table" coords="6,127.05,605.96,5.08,10.91">4</ref> shows the human evaluation results of the ideal answers returned by our runs. Our runs are very competitive, especially given the relatively poor quality of the input snippets. Given the poor quality of the input snippets in all of our runs, it is dangerous to make generalisations about how the quality of the snippets affect the quality of the answers. Having said that, we can observe that, in the BioASQ9 Synergy 2 task, the runs that generated the best snippets Table <ref type="table" coords="7,116.06,90.49,5.12,8.93">4</ref> Ideal answer results of the submissions to the BioASQ9 Synergy 2 (top) and BioASQ10 Synergy (bottom) tasks. Metric: Average of human evaluation scores. The best of our systems in each round is highlighted in bold. The results of rows labelled "Best", "Median", and "Worst" refer to the results of other systems, other than our own, submitted to the challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Similarity (MQ-4) did not lead to generating the best ideal answers. The impact of and interplay between the document and snippet retrieval stages, and the question-answering stage, deserves further exploring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">BioASQ10b, Phase B</head><p>For BioASQ10b, Phase B, we used the question answering system described in Section 2, using DistilBERT as the BERT variant chosen to compute the word embeddings. Following a datacentric approach, the main difference between the Synergy tasks and BioASQ10, Phase B, is the choice of training data. We hypothesised that the training data that corresponds to the early years of BioASQ, that is, the first samples of the BioASQ10b training data, might be biased. We therefore tested the use of different portions of the training data as shown in Table <ref type="table" coords="7,499.87,518.66,3.66,10.91" target="#tab_4">5</ref>, by incrementally removing the first samples of the training data. We can observe that best evaluation results are obtained with only 50% of the training data.</p><p>To double-check that indeed the first samples of the training data are biased, we conducted another round of experiments, but this time removing the last samples of the training data. Table <ref type="table" coords="7,116.83,586.41,5.17,10.91" target="#tab_5">6</ref> shows that results worsen as the amount of training data diminishes, as one might expect in systems that are based on supervised approaches to machine learning.</p><p>Hyperparameter search showed that the same hyperparameters give optimal results when training using the entire training data, or using only 50% of the training data: dropout=0.6, number of epochs=1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Submission Results to BioASQ10b, Phase B</head><p>Table <ref type="table" coords="8,116.69,568.62,5.17,10.91" target="#tab_6">7</ref> shows the results of our submissions to BioASQ10b, Phase B 7 . Note that the results reported in the BioASQ website 8 may change in the future after the test data is potentially enriched with further annotations. Our runs are comparable to the median of those of other participating systems. Surprisingly, there is little difference between using all training data or only the latter 50%. When we visually inspected the outputs of the runs, we noticed that the output of all runs in each batch were virtually identical, with only a few differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Summary and Conclusions</head><p>We have presented Macquarie University's contribution to the BioASQ9 Synergy task 2, the BioASQ10 Synergy task, and BioASQ10b, Phase B (Ideal Answers). In all of our runs, the base question answering architecture was virtually the same, the only differences being the choice of DistilBERT vs. ALBERT, and the training data used.</p><p>For the synergy tasks, we used a system that has been trained using BioASQ9b training data. We experimented with approaches for snippet retrieval based on two dimensions: vectors used for similarity comparison, and final ranking approach. Cosine similarity using sBERT gave the best results, and we observed that not always the best snippets for the snippet retrieval task led to best answers in the question answering task.</p><p>Overall, the results of the question answering parts were competitive, especially given the relatively poor quality of the documents and snippets retrieved. We will investigate approaches to increase the quality of the retrieval stages, and explore the relation between quality of retrieval vs. quality of final answers.</p><p>For the BioASQ10b, Phase B task, we followed a data-centric approach and experimented with training regimes that incrementally removed samples from the training data. During our preliminary cross-validation experiments we observed an improvement of results using only the latter 50% of the training data, but this difference of results vanished in the submitted runs.</p><p>With a data-centric approach in mind, we plan to conduct further experiments that test the impact of changes and transformations of the training data. For example, besides further examining the impact of using portions of the training data, we will investigate the use of data augmentation techniques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,193.64,416.70,8.93;2,89.29,205.65,246.93,8.87"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example question with its question type, a relevant snippet, an exact answer, and a correct ideal answer, extracted from the training data of BioASQ10b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,89.29,262.28,353.31,8.93"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of the question answering system used for BioASQ9b, Phase B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,90.49,295.92,54.00"><head>Table 1</head><label>1</label><figDesc>Number of sentences selected, for each question type</figDesc><table coords="5,210.36,118.13,174.55,26.36"><row><cell></cell><cell cols="4">Summary Factoid Yesno List</cell></row><row><cell>n</cell><cell>6</cell><cell>2</cell><cell>2</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.99,90.49,417.24,167.06"><head>Table 2</head><label>2</label><figDesc>Document retrieval results of the submissions to the BioASQ9 Synergy 2 (top) and BioASQ10 Synergy (bottom) tasks. Metric: F1. The results of rows labelled "Best", "Median", and "Worst" refer to the results of other systems, other than our own, submitted to the challenge.</figDesc><table coords="6,175.17,142.09,244.94,115.46"><row><cell>Run</cell><cell cols="4">Round 1 Round 2 Round 3 Round 4</cell></row><row><cell>Best</cell><cell>0.3693</cell><cell>0.2039</cell><cell>0.1327</cell><cell>0.1896</cell></row><row><cell>Median</cell><cell>0.2388</cell><cell>0.1423</cell><cell>0.0710</cell><cell>0.0800</cell></row><row><cell>Worst</cell><cell>0.0157</cell><cell>0.0067</cell><cell>0.0053</cell><cell>0.0175</cell></row><row><cell>MQ-BioASQ9</cell><cell>0.1978</cell><cell>0.1087</cell><cell>0.0483</cell><cell>0.0800</cell></row><row><cell>Best</cell><cell>0.3220</cell><cell>0.2221</cell><cell>0.1970</cell><cell>0.1564</cell></row><row><cell>Median</cell><cell>0.3100</cell><cell>0.1646</cell><cell>0.1327</cell><cell>0.1067</cell></row><row><cell>Worst</cell><cell>0.2729</cell><cell>0.1003</cell><cell>0.0655</cell><cell>0.0478</cell></row><row><cell>MQ-BioASQ10</cell><cell></cell><cell>0.1003</cell><cell>0.0754</cell><cell>0.0808</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,88.99,277.06,418.09,250.76"><head>Table 3</head><label>3</label><figDesc>Snippet retrieval results of the submissions to the BioASQ9 Synergy 2 (top) and BioASQ10 Synergy (bottom) tasks. Metric: F1. The best of our systems in each round is highlighted in bold. The results of rows labelled "Best", "Median", and "Worst" refer to the results of other systems, other than our own, submitted to the challenge.</figDesc><table coords="6,124.24,340.61,346.80,187.20"><row><cell>Run</cell><cell cols="6">Similarity Sorting Round 1 Round 2 Round 3 Round 4</cell></row><row><cell>Best</cell><cell></cell><cell></cell><cell>0.3290</cell><cell>0.1726</cell><cell>0.1262</cell><cell>0.1355</cell></row><row><cell>Median</cell><cell></cell><cell></cell><cell>0.2288</cell><cell>0.1365</cell><cell>0.0732</cell><cell>0.0764</cell></row><row><cell>Worst</cell><cell></cell><cell></cell><cell>0.0311</cell><cell>0.0101</cell><cell>0.0231</cell><cell>0.0132</cell></row><row><cell>MQ-1-BioASQ9</cell><cell>tfidf</cell><cell>local</cell><cell>0.1031</cell><cell>0.1035</cell><cell>0.0707</cell><cell>0.0764</cell></row><row><cell>MQ-2-BioASQ9</cell><cell>tfidf</cell><cell>global</cell><cell>0.1100</cell><cell>0.0540</cell><cell>0.0324</cell><cell>0.0619</cell></row><row><cell>MQ-3-BioASQ9</cell><cell>sBERT</cell><cell>local</cell><cell>0.1071</cell><cell>0.0999</cell><cell>0.0692</cell><cell>0.0749</cell></row><row><cell>MQ-4-BioASQ9</cell><cell>sBERT</cell><cell>global</cell><cell>0.1923</cell><cell>0.1075</cell><cell>0.1044</cell><cell>0.0762</cell></row><row><cell>Best</cell><cell></cell><cell></cell><cell>0.2910</cell><cell>0.1525</cell><cell>0.1574</cell><cell>0.1217</cell></row><row><cell>Median</cell><cell></cell><cell></cell><cell>0.2757</cell><cell>0.1410</cell><cell>0.1087</cell><cell>0.0948</cell></row><row><cell>Worst</cell><cell></cell><cell></cell><cell>0.2296</cell><cell>0.0540</cell><cell>0.0273</cell><cell>0.0416</cell></row><row><cell cols="2">MQ-1-BioASQ10 tfidf</cell><cell>local</cell><cell></cell><cell>0.0660</cell><cell>0.0465</cell><cell>0.0771</cell></row><row><cell cols="2">MQ-2-BioASQ10 tfidf</cell><cell>global</cell><cell></cell><cell>0.0540</cell><cell>0.0273</cell><cell>0.0416</cell></row><row><cell cols="2">MQ-3-BioASQ10 sBERT</cell><cell>local</cell><cell></cell><cell>0.0683</cell><cell>0.0457</cell><cell>0.0770</cell></row><row><cell cols="2">MQ-4-BioASQ10 sBERT</cell><cell>global</cell><cell></cell><cell>0.0928</cell><cell>0.0725</cell><cell>0.0827</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,88.99,90.49,418.52,125.72"><head>Table 5</head><label>5</label><figDesc>Results of 10-fold cross-validation after removing the first samples of the BioASQ10b training data. Metric: Average ROUGE-SU4 F1. Best result shown in bold.</figDesc><table coords="8,217.49,130.13,160.30,86.08"><row><cell cols="2">Percentage removed ROUGE-SU4 F1</cell></row><row><cell>10%</cell><cell>0.281</cell></row><row><cell>20%</cell><cell>0.288</cell></row><row><cell>30%</cell><cell>0.298</cell></row><row><cell>40%</cell><cell>0.309</cell></row><row><cell>50%</cell><cell>0.311</cell></row><row><cell>60%</cell><cell>0.308</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,88.99,235.71,418.52,125.72"><head>Table 6</head><label>6</label><figDesc>Results of 10-fold cross-validation after removing the last samples of the BioASQ10b training data.</figDesc><table coords="8,89.29,259.67,288.50,101.76"><row><cell>Metric: Average ROUGE-SU4 F1.</cell><cell></cell></row><row><cell cols="2">Percentage removed ROUGE-SU4 F1</cell></row><row><cell>10%</cell><cell>0.275</cell></row><row><cell>20%</cell><cell>0.268</cell></row><row><cell>30%</cell><cell>0.270</cell></row><row><cell>40%</cell><cell>0.255</cell></row><row><cell>50%</cell><cell>0.241</cell></row><row><cell>60%</cell><cell>0.229</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,88.99,380.94,417.00,143.15"><head>Table 7</head><label>7</label><figDesc>Preliminary results of the submissions to BioASQ10b, Phase B. The best of our systems in each batch is highlighted in bold. The results of rows labelled "Best", "Median", and "Worst" refer to the results of all systems, including our own, submitted to the challenge.</figDesc><table coords="8,318.93,432.54,64.79,8.87"><row><cell>ROUGE-SU4 F1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,108.93,649.12,122.23,8.97"><p>https://pubmed.ncbi.nlm.nih.gov/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="2,108.93,660.08,397.06,8.97;2,89.29,671.04,416.69,8.97"><p>The sentence position was incorporated as an absolute number: 1, 2, . . . ùëõ, where ùëõ is the total number of input sentences. We chose to include the sentence position as earlier experiments in past BioASQ years showed an</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="3,108.93,638.12,397.06,8.97;3,89.29,649.08,45.45,8.97"><p>At the time of training the system for the BioASQ9 Synergy task 2, the final results of BioASQ9 had not been released yet.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="3,108.93,660.04,398.57,8.97;3,89.29,670.99,33.60,8.97"><p>https://huggingface.co/ -For ALBERT, we used 'albert-xxlarge-v2'. For DistilBERT, we used 'distilbert-baseuncased'.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4" coords="8,108.93,660.08,279.67,8.97"><p>At the time of writing, only the automated evaluation results were available.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5" coords="8,108.93,671.03,60.84,8.97"><p>http://bioasq.org</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was undertaken with the assistance of resources and services from the <rs type="funder">National Computational Infrastructure (NCI)</rs>, which is supported by the <rs type="funder">Australian Government</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,107.59,547.38,399.59,10.91;9,107.59,560.93,399.59,10.91;9,107.59,574.48,399.59,10.91;9,107.59,588.02,398.40,10.91;9,107.59,601.57,398.40,10.91;9,107.59,615.12,395.89,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,218.37,588.02,211.21,10.91">CORD-19: The COVID-19 open research dataset</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Reas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Burdick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Funk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Katsis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Murdick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Rishi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sheehan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stilson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">D</forename><surname>Wade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">X R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kohlmeier</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.nlpcovid19-acl.1" />
	</analytic>
	<monogr>
		<title level="m" coord="9,452.24,588.02,53.74,10.91;9,107.59,601.57,251.15,10.91">Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020</title>
		<meeting>the 1st Workshop on NLP for COVID-19 at ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,107.59,628.67,400.25,10.91;9,107.59,642.22,399.60,10.91;9,107.59,655.77,398.40,10.91;10,107.59,86.97,400.07,10.91;10,107.59,100.52,17.97,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,351.77,628.67,156.07,10.91;9,107.59,642.22,323.13,10.91">Query-focused extractive summarisation for finding ideal answers to biomedical and COVID-19 questions</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moll√°</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Galat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rybinski</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936//paper-20.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,297.56,655.77,208.42,10.91;10,107.59,86.97,125.53,10.91">Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<meeting><address><addrLine>Bucharest</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,107.59,114.06,398.40,10.91;10,107.59,127.61,398.40,10.91;10,107.59,141.16,398.40,10.91;10,107.59,154.71,398.40,10.91;10,107.59,168.26,397.50,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,321.86,114.06,184.12,10.91;10,107.59,127.61,188.00,10.91">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,326.05,127.61,179.94,10.91;10,107.59,141.16,398.40,10.91;10,107.59,154.71,100.80,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="10,107.59,181.81,400.25,10.91;10,107.59,195.36,398.40,10.91;10,107.59,208.91,399.10,10.91;10,107.59,222.46,116.74,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,379.51,181.81,128.32,10.91;10,107.59,195.36,213.55,10.91">ALBERT: A lite BERT for selfsupervised learning of language representations</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<ptr target="https://iclr.cc/virtual_2020/poster_H1eA7AEtvS.html" />
	</analytic>
	<monogr>
		<title level="m" coord="10,344.24,195.36,161.75,10.91;10,107.59,208.91,182.18,10.91">Proceedings of the 8th International Conference on Learning Representations</title>
		<meeting>the 8th International Conference on Learning Representations<address><addrLine>Virtual</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,107.59,236.01,398.40,10.91;10,107.59,249.56,399.61,10.91;10,107.59,263.11,399.10,10.91;10,107.23,276.66,108.83,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,240.21,236.01,265.78,10.91;10,107.59,249.56,20.14,10.91">Query-focused multi-document summarisation of biomedical texts</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moll√°</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Nguyen</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_119.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,361.02,249.56,146.18,10.91;10,107.59,263.11,198.05,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferraro</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,107.59,290.20,399.60,10.91;10,107.59,303.75,398.40,10.91;10,107.26,317.30,194.20,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,292.76,290.20,214.43,10.91;10,107.59,303.75,112.98,10.91">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
	</analytic>
	<monogr>
		<title level="m" coord="10,242.91,303.75,263.08,10.91;10,107.26,317.30,64.38,10.91">33rd Conference on Neural Information Processing Systems (NeurIPS 2019)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,107.59,330.85,398.40,10.91;10,107.59,344.40,399.60,10.91;10,107.34,357.95,216.47,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,360.51,330.85,145.48,10.91;10,107.59,344.40,254.79,10.91">BioBERT: pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz682</idno>
	</analytic>
	<monogr>
		<title level="j" coord="10,371.12,344.40,65.15,10.91">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,107.59,371.50,400.24,10.91;10,107.59,385.05,398.40,10.91;10,107.59,398.60,400.08,10.91;10,107.59,412.15,283.65,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,230.13,371.50,277.69,10.91;10,107.59,385.05,39.98,10.91">Sentence-BERT: Sentence embeddings using siamese BERTnetworks</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D19-1410/" />
	</analytic>
	<monogr>
		<title level="m" coord="10,179.38,385.05,326.61,10.91;10,107.59,398.60,293.48,10.91">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
