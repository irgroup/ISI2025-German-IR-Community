<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,309.31,15.42;1,89.29,106.66,396.68,15.42;1,89.29,128.58,324.70,15.43">Deep Learning solutions based on fixed contextualized embeddings from PubMedBERT on BioASQ 10b and traditional IR in Synergy</title>
				<funder ref="#_HyEaeYu">
					<orgName type="full">Foundation for Science and Technology (FCT)</orgName>
				</funder>
				<funder ref="#_rVSK7V6">
					<orgName type="full">EU/EFPIA Innovative Medicines Initiative 2 Joint Undertaking</orgName>
				</funder>
				<funder ref="#_kyKN5ve">
					<orgName type="full">FCT</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.93,156.89,71.84,11.96"><forename type="first">Tiago</forename><surname>Almeida</surname></persName>
							<email>tiagomeloalmeida@ua.pt</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Aveiro</orgName>
								<orgName type="institution" key="instit2">IEETA</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,173.42,156.89,62.26,11.96"><forename type="first">Andr√©</forename><surname>Pinho</surname></persName>
							<email>andre.s.pinho@ua.pt</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Aveiro</orgName>
								<orgName type="institution" key="instit2">IEETA</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,248.32,156.89,76.51,11.96"><forename type="first">Rodrigo</forename><surname>Pereira</surname></persName>
							<email>rodrigo.pereira@ua.pt</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Aveiro</orgName>
								<orgName type="institution" key="instit2">IEETA</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,355.83,156.89,63.78,11.96"><forename type="first">S√©rgio</forename><surname>Matos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Aveiro</orgName>
								<orgName type="institution" key="instit2">IEETA</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,440.11,638.11,23.94,8.97"><surname>Pereira</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Aveiro</orgName>
								<orgName type="institution" key="instit2">IEETA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,309.31,15.42;1,89.29,106.66,396.68,15.42;1,89.29,128.58,324.70,15.43">Deep Learning solutions based on fixed contextualized embeddings from PubMedBERT on BioASQ 10b and traditional IR in Synergy</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">BBB7369F4A858A2AEA98CB034A1D0CD4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Neural ranking</term>
					<term>Document Retrieval</term>
					<term>Binary classification</term>
					<term>BioASQ 10B</term>
					<term>Relevance feedback</term>
					<term>Synergy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the participation of the University of Aveiro Biomedical Informatics and Technologies group (BIT) in the tenth edition of the BioASQ challenge for document retrieval (task B phase A and Synergy) and 'yes or no' answering (task B phase B). For the Synergy task, we adopted a relevance feedback approach that leveraged the traditional BM25 retrieval model combined with query expansion based on the positive documents. Regarding task B phase A, we adopted a two-stage retrieval pipeline that consists of the traditional BM25 retrieval with pseudo-relevance feedback followed by a neural retrieval model. For the neural models, we experimented with the publicly available Transformer-UPWM already trained on last year's BioASQ data and with a local implementation of the PARADE-CNN model. Lastly, for the 'yes or no' questions, we trained a binary classifier over the pretrained PubMedBERT model, and also studied the impact of data augmentation and dataset balancing techniques. In terms of results, our Synergy and task B phase B systems underperformed, scoring below the average. Our best results came from the task B phase A systems that achieved above-average results, being in the top three in terms of teams. Furthermore, after the challenge, we also conducted additional experiments to evaluate the impact of the Transformer-UPWM when trained on 10b data. This trial produced an improvement of 1 to 5 MAP percentage points in all official evaluation batches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The rapid growth of biomedical literature, through open or peer reviewed publications, creates a situation of information overload that affects researchers, doctors or any other health care practitioner. According to Klerings et al. <ref type="bibr" coords="1,273.38,516.27,11.52,10.91" target="#b0">[1]</ref>, the main problem is not the abundance of information, but rather the lack of more capable information retrieval systems that can effectively search this continually growing literature.</p><p>The BioASQ challenge <ref type="bibr" coords="1,203.01,556.92,12.84,10.91" target="#b1">[2]</ref> is an annual competition that promotes the development of such intelligent retrieval systems that can aid with the current problem of information overload.</p><p>Currently, the challenge is in its tenth edition and is divided into tasks A and B plus the recent Synergy task. More concretely, task A is concerned with biomedical semantic indexing, while task B and Synergy focus on information retrieval and question answering. Furthermore, task B is additionally subdivided into phase A and phase B, where the first is concerned with finding the relevant documents/snippets that answer a given biomedical question (IR) and the later is concerned with answer extraction and generation (QA). The aim of tasks A and B is the development of state-of-the-art systems that can find evidence or answer an open biomedical question, while the primary objective of the Synergy task is in using IR and QA systems for collectively finding answers to open questions about Covid- <ref type="bibr" coords="2,354.37,195.36,14.20,10.91">19.</ref> This paper describes the participation of the Biomedical Informatics and Technologies (BIT) group of the Aveiro University in BioASQ task B and BioASQ Synergy. For task B, our approaches rely on the fixed contextualized embeddings produced by the PubMedBERT model. More precisely, for phase A we adopted the PARADE-CNN <ref type="bibr" coords="2,328.00,249.56,12.84,10.91" target="#b2">[3]</ref> and Transformer-UPWM <ref type="bibr" coords="2,457.99,249.56,12.84,10.91" target="#b3">[4]</ref> models. The first of these two models was trained on this year's training data, while for the latter we re-used the model trained on last year's data. This produced an interesting duality of new training data against one year old training data that we later explored in a post-challenge experience (Section 4). For phase B we trained a binary classifier to answer 'yes or no' type questions, while experimenting with data augmentation through a paraphrasing technique and dataset balancing. Regarding the Synergy task, we followed a simple, yet successful, relevance feedback approach <ref type="bibr" coords="2,175.87,344.40,11.46,10.91" target="#b4">[5]</ref>, where we used positively annotated documents to expand the current search query that was fed to the traditional BM25 retrieval model.</p><p>The remainder of the paper is organized as follows. In Section 2 we present the datasets and corpora used, followed by a detailed description of each method. The corresponding submissions and results are presented and discussed in Section 3. Then, in Section 4 we analyze the impact of training the Transformer-UPWM in BioASQ 9b data against 10b data. We finish the paper with a conclusion in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Materials and Methods</head><p>This section starts by presenting corpora used in each task (B and Synergy). Then it presents, for each task that we participated, the dataset preparation, the method and the official submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Corpora</head><p>As previously mentioned, the Synergy task aims to develop systems capable of answering Covid-19 related questions, while task B focuses on the development of more broad systems that can answer any biomedical type of question. Therefore, the Synergy questions were envisioned to be answered using the CORD-19 <ref type="bibr" coords="2,234.01,588.25,12.68,10.91" target="#b5">[6]</ref> dataset, while task B uses articles from PubMed/MEDLINE dataset as collection to answer the given biomedical questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">CORD-19</head><p>CORD-19 dataset is a joint effort by several research groups in response to the Covid-19 pandemic <ref type="bibr" coords="2,137.24,664.67,11.58,10.91" target="#b5">[6]</ref>. It aggregates several academic papers related to the topic of Covid-19 and general coronavirus research from peer reviewed sources such as PubMed, the World Health Organization, and open source archives such as bioRxiv, medRxiv and arXiv, in a structured manner that facilitate the creation of text mining and information retrieval systems, which can help the scientific community better understand this virus. Nowadays, the CORD-19 dataset is updated on a weekly basis, which means that each round of the Synergy task used a different snapshot of the CORD-19 dataset.</p><p>Each entry of this dataset represents a publication, described by multiple fields. For our task, we only focus on the title, abstract, PubMed ID, PubMedCentral ID and arXiv ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">MEDLINE/PubMed</head><p>MEDLINE <ref type="bibr" coords="3,137.78,231.14,12.87,10.91" target="#b6">[7]</ref> is a database that aggregates abstracts and metadata from biomedical literature from several sources, while PubMed [8] is a free searching engine over the MEDLINE data. The PubMed index is updated on daily basis and a baseline snapshot is produced at the end of each year. In this year's challenge task B used the 2022 PubMed baseline as the source of documents.</p><p>Similar to CORD-19, each datapoint describes a publication composed of multiple fields. The subset of these fields useful to us are the article title, the abstract and the PubMed ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Synergy</head><p>The Synergy task had four discrete moments of evaluation, also called rounds, where the organizers provided a set of unanswered questions related to Covid-19, and teams had three days to submit their systems' answers. Table <ref type="table" coords="3,292.90,375.71,5.13,10.91" target="#tab_0">1</ref> shows a summary of the number of questions made available in each round and the total number of feedback documents from the previous round. These are documents that either were consider to be positive or negative. Finally, in this task we only submitted results regarding the document retrieval task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Dataset Preparation</head><p>Upon inspecting the CORD-19 dataset, we concluded that it contains some datapoints that need further treatment. Some of the issues present are unstructured abstracts, abstracts in foreign languages and even papers without an abstract. Considering these problems, we created a module to preprocess and structure the input dataset. The first step of this process is to remove all articles that don't contain a title, abstract or at least one of the three identifiers (PubMed ID, PubMed Central ID, arXiv ID). Afterwards, considering that very short abstracts contained irrelevant information, we filter out all the articles whose abstracts contain less than six words. We also observed that multiple articles were written in other languages (such as Chinese). Unfortunately our models are not designed to deal with multi-languages, even less with a different character set. To filter these, we leveraged FastText <ref type="bibr" coords="4,129.41,141.16,12.84,10.91" target="#b7">[9]</ref> using the model lid.176.bin.</p><p>Having done basic filtering, the module advances to a more fine filtering. Each of the remaining abstracts are split into a list of sentences. To perform this task we used the NLP module of the Spacy Python library <ref type="bibr" coords="4,250.48,181.81,16.24,10.91" target="#b8">[10]</ref>. In the case that some non-English characters passed through the initial removal of foreign languages (for example abstracts where the main language was in fact English, but that did contain characters from another language), they are deleted.</p><p>We also noticed that multiple articles contained URLs scattered in the abstract. Once again, our model is not designed to deal with this, so we filter them out using a regular expression. At the end, all of these steps contributed to a total reduction of roughly 58% of the total number of documents in the collection. Note that this number slightly changed from round to round since the CORD-19 was updated continually.</p><p>As an additional step, besides indexing the documents as a whole, we also indexed directly the individual sentences of the documents to have a more fine grain control of the search. However, when looking at the training data, it is observable that most of the relevant snippets are composed of a sequence of sentences. Therefore, we implemented a sentence merging mechanism in order to concatenate some of the sentences. More specifically, adjacent sentences are merged until the sum of the sizes does not exceed 80 characters. This also drastically increases the efficiency of the neural rerankers.</p><p>In terms of the training data, these correspond to last year's questions to the BioASQ 9b Synergy v1 and v2 challenge and their respective feedback data. Regarding data preparation, some duplicate questions were identified and merged, and their answers were combined. Afterwards, we also identified that for some questions there were documents with inconsistent labeling. In other words, for a question, the same document could be labeled as both relevant and irrelevant. In those cases we considered these documents to be irrelevant (as opposed to unknown or relevant).</p><p>Furthermore, considering that articles can be revoked from CORD-19 on each update, it was necessary to guarantee that each of the relevant documents for a specific question was still present in the recent version of CORD-19. If this was not the case, those document references were removed from the feedback data. Finally, questions that did not have any associated relevant documents were also removed. The final clean training set contained 175 questions, each with 28.8 positive documents and 149.5 negative documents on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Method</head><p>Our main method adopted for the Synergy task relies on a relevance feedback approach, since in <ref type="bibr" coords="4,100.74,610.52,12.71,10.91" target="#b4">[5]</ref> it was shown to be a simple but quite effective technique. Therefore, our method directly follows the ideas presented in <ref type="bibr" coords="4,224.42,624.07,11.42,10.91" target="#b4">[5]</ref>, however, with a different implementation. More concretely, we adopted the Pyserini <ref type="bibr" coords="4,200.27,637.62,18.00,10.91" target="#b9">[11]</ref> (Python wrapper of Anserini <ref type="bibr" coords="4,353.80,637.62,17.04,10.91" target="#b10">[12]</ref>) information retrieval toolkit, instead of the Elastic Search (ES) engine, due to its increasing popularity within the information retrieval community and, from our experience, better out-of-the box standard retrieval models.</p><p>Similarly to <ref type="bibr" coords="5,154.08,86.97,11.31,10.91" target="#b4">[5]</ref>, we used the BM25 probabilistic ranking model with query expansion to find documents that answer biomedical questions. Here, the query expansion acts as the relevance feedback mechanism, where the intuition is to introduce the most important terms from the set of positive documents into the original query. Internally, the Anserini uses the rm3 <ref type="bibr" coords="5,487.91,127.61,18.07,10.91" target="#b11">[13]</ref> relevance model to perform the query expansion. First, this model tries to estimate, from a set of positive feedback data, ‚Ñõ, the importance of each document term. Then all of the terms are normalized and sorted, where the top ùëÄ are selected to compose the new query. Additionally, in order to maintain the original query information, the new query weights are linearly interpolated with the original query weights, which ensures that the original query terms are present in the new query. After this step, a new BM25 search is performed, where the previously computed query weights are kept. For a more complete description framed within the probabilistic framework, refer to Section 3.1.4 in <ref type="bibr" coords="5,316.83,236.01,16.09,10.91" target="#b11">[13]</ref>. Finally, it is important to mention that the rm3 relevance model can be used for relevance or for pseudo-relevance feedback, where in the first case the set ‚Ñõ is provided by the data, as explicit feedback, while in the later case ‚Ñõ corresponds to the top-N retrieved by the BM25 ranking model over the original query. Recalling Table <ref type="table" coords="5,161.52,290.20,3.81,10.91" target="#tab_0">1</ref>, some of the test queries did not have feedback data and in those cases we performed pseudo-relevance feedback.</p><p>Regarding the positive feedback data, ‚Ñõ, the obvious choice is to use the documents that are classified as relevant for each query. But as suggested in <ref type="bibr" coords="5,336.70,330.85,11.28,10.91" target="#b4">[5]</ref>, we also used the snippet sentences as the positive feedback data, since, when comparing to a whole document, these are more focused on the answer. In order to find the best set of parameters for ‚Ñõ, ùëÄ , ùëò1 and ùëè we used Optuna <ref type="bibr" coords="5,125.48,371.50,17.91,10.91" target="#b12">[14]</ref> to search a good combination over the training data.</p><p>Besides this method we also tried to train the PARADE-CNN <ref type="bibr" coords="5,382.77,385.05,13.00,10.91" target="#b2">[3]</ref> ranking model (refer to Section 2.3.2 for more details) to rerank the top-100 documents retrieved by the BM25. However, we did not achieved satisfactory results and hence we did not submit any run with this system. Nevertheless, for sake of completeness we present on Table <ref type="table" coords="5,359.06,425.70,5.13,10.91">2</ref> our preliminary benchmark, in terms of MAP@10, between the BM25 and the PARADE-CNN calculated on the same validation subset. We believe that the scarcity of the data and the difficulty of the questions are the main reason for the under-performance of the PARADE-CNN reranking model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Comparison of the PARADE-CNN model trained on the Synergy training dataset against the finetuned BM25 (pyserini)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Map@10 BM25 (pyserini) 0.1924 PARADE-CNN <ref type="bibr" coords="5,296.43,559.71,11.83,8.87" target="#b2">[3]</ref> 0.1494</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Task 10b Phase A</head><p>Task 10B Phase A had five moments of evaluation, also called batches, with an additional non-official batch at the end of the challenge. In each batch, the organizers provided a set of biomedical questions for which the teams had 24 hours to find the top-10 most relevant documents (document retrieval task) and/or to also retrieve the top-10 most relevant snippets extracted from relevant documents (snippet retrieval task). Each batch contained 90 new questions, except the sixth batch which only contained 37.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Dataset Preparation</head><p>The training dataset for Task 10B consists of all of the previous year's questions and their respective relevant information that was annotated by experts. However, since in each year BioASQ uses different PubMed baselines as document collections this raises an issue of time inconsistency. For example, questions that belonged to Task 3B had the PubMed 2015 as a baseline, which means that only documents that where indexed in the 2015 baseline were consider for retrieval. So, using those questions in the 2022 baseline may produce several inconsistencies. For example, the article identifier may have changed, the article may have been removed, the article content may have changed and new articles that answer that question could have been added. We believe that all of these inconsistencies must be addressed in order to produce a clean set of training data. To solve this issue, our solution was to ensure that each question would only use their corresponding baseline. To accomplish that, we firstly downloaded all the baselines since 2013 and, after that, we iterated through these baselines and generated a data structure which contained the most recent version of the abstract and title of each article and kept record of all the baselines in which that article appeared on. Then, when searching, we filter out the documents that did not appear in that respective question baseline. This approach ensures that each training question only sees the documents that were available at the time they were written. Figure <ref type="figure" coords="6,253.90,366.63,4.98,10.91" target="#fig_0">1</ref> shows, for each year's question set, a distribution of the removed articles from the top-150 BM25 retrieval. As observable, the older questions (1B, 2B) have a larger amount of documents that needed to be removed, since these are documents that were not available at the time that the gold-standard were produced, hence its relevance value is unknown.</p><p>Regarding the preprocessing of the documents, it uses the sentence splitting (using the same approach as in Synergy) and merging strategy that was also applied to the Cord-19 dataset, however for this particular case, the foreign charset and URL cleaning was not necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Method</head><p>For this task we decided to use a two-stage retrieval system, where we adopted the traditional BM25 with rm3 (in pseudo-relevance feedback mode) as the first stage ranker and then the PARADE-CNN <ref type="bibr" coords="6,157.60,537.90,12.74,10.91" target="#b2">[3]</ref> model as a top-100 reranking model. Additionally, we also tried the BioASQ 9B Transformer-UPWM <ref type="bibr" coords="6,195.29,551.45,12.68,10.91" target="#b3">[4]</ref> model to rerank the same top-100 documents as a way of comparison.</p><p>Transformer-based architectures have emerged to the top, as the most adopted models for almost every NLP task, inclusively information retrieval. However, when considering the subtask of document retrieval it still remains a challenge to use these type of models, due to the limited number of tokens that they can process, which are bound by the quadratic complexity order of the self-attention mechanism. To alleviate this problem, it is practical to divide the input document into a set of sentences and then use a transformer model to compute a query-sentence relevance score, that are later aggregated to compute the final query-document relevance score.</p><p>Similarly to this paradigm,  More precisely, for each year's edition testset we retrieve the top-150 most relevant articles using the BM25 from the 2022 PubMed baseline. Then from those 150, we removed the documents that did not exist at that time.</p><p>model to encode query-sentence relevance in a highly dimensional space (reflected in the CLS token). Then the authors tested several aggregation strategies, like a MultiLayer Perceptron (MLP), 2-Layer Transformer, and CNN to produce the final document score from the set of querysentences CLSs. We decided to adopt the PARADE-CNN model, presented in Figure <ref type="figure" coords="7,470.12,431.20,3.80,10.91" target="#fig_1">2</ref>, based on the reported results and on the fact that the CNN kernel (ùëë √ó 2 size, where d corresponds to the CLS dimension, corresponding to a 1D CNN) can easily encode the relative positions of sentences. The downside of this was that the PARADE-CNN was the only model that did not have an open-source implementation at that time. Therefore, we made a local implementation of the model in Tensorflow. Furthermore, in contrast to the original model, we used PubMedBERT as the transformer query-sentence encoder and during train we did not update its weights. We believed that this could harm the overall performance, but we were also interested in seeing if the out-of-the-box projections were good enough for this task. Additionally, we did not possess the hardware to do such training.</p><p>Taking into consideration that we made a local implementation of the PARADE-CNN, we also decided to adopt the Transformer-UPWM <ref type="bibr" coords="7,292.42,580.24,12.68,10.91" target="#b3">[4]</ref> model that was already trained in the BioSAQ 9b data. This way we could benchmark against the PARADE-CNN and have an idea if the PARADE-CNN implementation was working as expected, since we believe that the results would be similar.</p><p>The Transformer-UPWM (Figure <ref type="figure" coords="7,247.41,634.44,4.09,10.91" target="#fig_2">3</ref>) is, as the name suggests, a transformer-based model that follows the same ideology of dividing a document into sentences, processing each individual query-sentence pair using a transformer and then aggregating them. However, differently to the PARADE-CNN, this model does not encode the query-sentences pair to a highly dimensional space, but instead tries to predict a query-relevance score, bounded between 0 and 1. This model is inspired by the way humans search for information <ref type="bibr" coords="8,326.94,282.39,16.31,10.91" target="#b13">[15,</ref><ref type="bibr" coords="8,345.96,282.39,12.23,10.91" target="#b14">16]</ref>. More precisely, a person usually scans a document for regions of interest, usually denoted by relevant keywords, then reads it and finally produces a mental score of relevance. Therefore, to compute the query-sentence score the model considers the query-sentence relevance score produced by a classifier layer over the PubMedBERT model and the query-sentence relevance score produced by the "a priori" layer. In other words, the "a priori" layer tries to find the regions of interest by query-sentence embedding matching and relative query term importance. For more details about the architecture, consider our previous work <ref type="bibr" coords="8,174.38,377.23,11.43,10.91" target="#b3">[4]</ref>. To train the PARADE-CNN model we used the standard AdamW optimizer with the pairwise cross-entropy loss. Regarding the negative sampling, we consider as pool of negatives every document that does not belong to the set of positive documents and as hard negative every negative document that belongs to the top-100 BM25 ranking order. We tried some negative sampling strategies, but found that training only with hard negatives seems to be the most stable approach. Note that the Transformer-UPWM was already pre-trained on BioASQ 9b data so it was not trained at this phase. The reasons for not training the Transformer-UPWM model were two fold. The first was that we wanted to focus our efforts on making sure that the PARADE-CNN model was working correctly, while the other reason was that we also wanted to see how a one year old model, trained on a one year old dataset handled the new questions provided by the organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Task 10b Phase B</head><p>Similarly to Phase A, Phase B also had six moments of evaluation, where the sixth was a nonofficial batch. Here, the teams also had 24 hours to answer questions of four types, namely 'yes or no', 'list', 'factoid' and 'summary'. Due to time constrains we only participated in the 'yes or no' subtask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1.">Dataset Preparation</head><p>Regarding the training data, we used a combination of the testsets from the previous editions of BioASQ task B. The data consists of a list of questions and the corresponding collections of snippets, obtained from multiple documents, capable of answering each question. From that collection of questions we first extracted the ones of type "yesno". Associated with each 'yes or no' question, a boolean label is also given to represent the correct answer (Yes/No) of that question. To process this dataset, we pair every question (and classification label) with each snippet. These are then processed, expanded and filtered in several steps as explained next.</p><p>In an attempt to increase the size of the dataset, we explored the use of paraphrasing tools. Initially, we performed tests with T5 Sentence Paraphrasing Model <ref type="bibr" coords="9,402.89,362.16,16.41,10.91" target="#b15">[17]</ref>. However, after a preliminary analysis we opted for the use of QuillBot <ref type="bibr" coords="9,332.27,375.71,16.38,10.91" target="#b16">[18]</ref>, which offered better results. Also based on a deep learning architecture, this tool was capable of analyzing text segments and generating legitimate alternative wordings while preserving the original internal meaning. This method was applied to increase the number of questions, and corresponding answering snippets. In order to fairly distribute the generated snippets and questions, their pairings are shuffled. This means that for a question (may it be a paraphrased or original one), the list of snippets corresponds to the mixture of original and paraphrased versions.</p><p>Previous works in this task describe the use of down sampling strategies based on the unbalance between "yes" and "no" questions, with the original dataset being heavily skewed towards "yes" questions <ref type="bibr" coords="9,195.09,497.65,16.08,10.91" target="#b17">[19]</ref>. This down sampling however, presents a problem in itself, namely the fact that not only is the dataset skewed towards "yes" question, but also that "yes" questions have a higher average of "corroborative snippets", meaning that the number of input pairs Question + Snippet will be unbalanced. As a preventive measure we decided to be more aggressive in the down sampling of "yes" questions making it so that the unbalanced ratio between "yes" and "no" questions offsets the difference in number of average "corroborative snippets".</p><p>The balanced data obtained was then split into training and validation sets. Different ratios where experimented, from very small validations sets, that mimicked the size of the test sets provided by BioASQ, and at the same time provided a larger training set to the model, to more conventional split ratios (80%/20%) that gave us a better understanding of whether the model was generalizing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2.">Method</head><p>Our approach to this particular task was to perform a more traditional classification method, centered on the PubMedBERT deep-learning model (Figure <ref type="figure" coords="10,348.92,121.08,3.50,10.91" target="#fig_3">4</ref>). The datapoint input composition of our model corresponds to the pairing between one question and one corresponding snippet. This datapoint is fed to the PubMedBERT model (using the two phrase input option), and its CLS output is retrieved. This CLS is then processed by a configurable amount of dense layers. The last dense layer of the model has only one output that serves as the pair score to be used for classification. If this value is less than 0.5 then we consider the answer to be "no", and "yes" otherwise. Earlier experiments indicated that enabling the training of the internal BERT layers closest to the output could be beneficial. As such, we also considered this option as an hyperparameter. For the training setup of the model we used a binary cross entropy (BCE) loss function paired with the AdamW optimization strategy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3.">Model Prediction and Optimization</head><p>Given our initial approach of using Question + Snippet pairings as the model input, it was necessary for us to create mechanisms that allowed to obtain the classification of a single question regarding all their snippet pairings. To this end two different approaches were implemented: Majority voting and Max confidence. The majority voting strategy reaches a consensus based on the most frequent label (yes or no) for that question. On the other hand the Max Confidence strategy follows a less democratic approach, where only the (question, snippet) pair with highest confidence in its classification (value most close to 1 for yes or 0 for no) is used as the final classification for the correspondent question.</p><p>The performance of the model is dependent on a number of variables, from the splitting approach, to the use of paraphrasing, to the model architecture and its hyperparameters. As such we tested various combinations for these variables, while tracking their validation performance.</p><p>To that end we made use of one of many machine learning tracker tools (Weights and Biases <ref type="bibr" coords="10,89.29,620.37,16.93,10.91" target="#b18">[20]</ref>) in order to log and more easily compare them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results and Discussion</head><p>In this section, for each BioASQ task, we present a description of the systems that we submitted, their corresponding results followed by a discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Synergy</head><p>Regarding the submissions, due to time constrains we only enrolled in the third and fourth rounds of the Synergy challenge. Furthermore, after the third round submission we noticed that our generated outputs erroneously contained the known positive documents, which were fed as feedback data to our method. This strongly affected our results, as these documents do not count for the ranking metrics. Table <ref type="table" coords="11,251.04,228.75,5.01,10.91">3</ref> shows a summary with the systems description for each of the rounds, where the main difference was the source of the positive feedback data, ‚Ñõ, used. System 0 directly used the whole documents as the feedback data, while System 2 uses snippets. System 1 and 3 are ensemble runs produced by the rank reciprocal fusion (RRF) <ref type="bibr" coords="11,449.70,269.40,18.00,10.91" target="#b19">[21]</ref> method, from slightly variations of hyperparameters on individual runs from the System 0 and System 2, respectively. Finally, System 4 is an RRF run that combines System 1 and System 3, which are runs produced with different sources of the positive feedback data (documents and snippets). The official results for each system are presented in Table <ref type="table" coords="11,368.13,452.19,3.81,10.91">4</ref>, with the scores achieved by the first place team presented at the bottom. As previously mentioned, the results of round 3 are under representatives. Nevertheless, it is interesting to see that the systems that used the snippets as source of feedback data achieved better results when compared to using the documents as source of feedback data. This result is indicative that when using the snippets as feedback data, the retrieval system did not score the corresponding positive documents as higher, suggesting that the query expansion is less biased towards the positive documents. Regarding the fourth round results, our best submission scored close to the best participation teams, being only 0.0251 MAP percentage points away of the top score. Another interesting result was the fact that here the systems that used the documents as source of feedback data achieved slightly higher results than those using the snippets. This results contradicts the results achieved by <ref type="bibr" coords="11,175.33,601.23,12.69,10.91" target="#b4">[5]</ref> who achieved the best results when using snippets as feedback data. We believe the main reason behind this is due to the difference in the relevance feedback method used. More precisely, we used the rm3 relevance model to produce a new weighted query, while <ref type="bibr" coords="11,89.29,641.88,12.69,10.91" target="#b4">[5]</ref> uses ES that implements a more simplistic query expansion mechanism. Additionally, it was also clear that using RRF resulted in minimal improvements when similar runs are combined and a much larger improvement when we combined runs that used different source of positive feedback data (System 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>Batch performance (Rank, Mean Average Precision) of submitted models for the Synergy challenge. Note, that we had an issue with the third round submission, where the known positive feedback documents were not removed from the final ranking order </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Task 10B Phase A</head><p>For the phase A challenge we enrolled in all batches, although in the first batch we could only submit runs using the Transformer-UPWM model trained on last year's data and not with the PARADE-CNN model. Table <ref type="table" coords="12,240.61,353.75,5.17,10.91">5</ref> shows a summary containing the characteristics of each submitted system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5</head><p>Summary containing the description of the submitted systems to the BioASQ 10 B phase A challenge As observable in Table <ref type="table" coords="12,202.20,506.41,3.68,10.91">5</ref>, we only submitted one individual run that used the PARADE-CNN (System 4 on batch 2). That decision was based on pre-challenge experiments that compared the PARADE-CNN performance trained on exactly the same data under comparable conditions against the Transformer-UPWM. The results, presented on Table <ref type="table" coords="12,370.32,547.06,3.66,10.91">6</ref>, show that the PARADE-CNN was slightly under-performing when compared to the Transformer-UPWM. Therefore, we made one single submission using the PARADE-CNN model to confirm if these results hold in the final testset. We believe that the main reason for this difference in results is related to the fact that we did not train the encoder transformer layers and hence the pre-trained PubMedBERT model may be less sensitive to finding matching signals between the query and document. This becomes more evident when we consider that the Transformer-UPWM utilizes the same frozen transformer encoder, but has an additional layer (a priori layer) that focuses on finding matching evidence, hence reinforcing our belief.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 6</head><p>Comparison of the PARADE-CNN model against the Transfomer-UPWM model. Both models were trained in the exact same data and under similar conditions Method Map@10 Transformer-UPWM 0.4623 PARADE-CNN <ref type="bibr" coords="13,296.43,152.62,11.83,8.87" target="#b2">[3]</ref> 0.3787</p><p>Regarding the results, Table <ref type="table" coords="13,227.90,185.44,5.12,10.91" target="#tab_4">7</ref> details the achieved performance for each system submitted, compared to the top competitor at the bottom of the table. Overall, we consider our results to be above average, with some runs in the top 3 in terms of teams. Our best submissions were those that used the Transformer-UPWM model, which was trained with last year's data, which is a surprising result since we expected this would achieve a lower performance due to a slightly larger amount of training data. Given this unexpected results, we conducted an additional experiment following the challenge, where we trained the Transformer-UPWM model using this year's training data (see Section 4). Our weakest submission was based on the PARADE-CNN model, confirming our initial suspicions. However, despite the lower individual performance, it achieved the best results in batch 2 and 3 when combined with the Transformer-UPWM, which also suggests that the two models are focusing on different types of matching signals. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Task 10B Phase B Yes/No</head><p>During the challenge, we submitted five different models, four of those following the architecture described in Section 2.4.2, each with slightly different hyperparameters (see Table <ref type="table" coords="13,469.63,560.62,5.17,10.91">8</ref> for an overview on the differences). The last model is an ensemble model, that aggregates the results of several variations of our model using a majority voting strategy (the output label is given based on the most common predicted label across the ensembled models).</p><p>Although we submitted systems for all six batches, in this manuscript we only focus on the last four. This is due to the first two batches our model not including paraphrasing, which we determined to be beneficial according to the experiment shown in Table <ref type="table" coords="13,412.07,641.92,3.74,10.91">9</ref>.</p><p>The paraphrasing strategy adopted was effective, resulting in an increase of 3 percentage points in F1 score when comparing the best performing model without paraphrasing data augmentation against the performing model that used paraphrasing, with a majority voting approach. Considering this, we opted to use paraphrasing in the subsequent batches. The performance reached in these is shown on Table <ref type="table" coords="14,307.15,331.60,8.36,10.91" target="#tab_5">10</ref>. Overall, the results obtained from our submitted systems are placed within or slightly bellow the average on all batches. The exception to this pattern is the sixth batch, where two of our systems placed within the first three spots. These last results were surprisingly high, since we did not perform any modifications to the system between the batches. Being this last round a non official batch, we believe that this unexpected results can be attributed to a different data distribution when compared to the official batches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Post-Challenge results</head><p>As previously mentioned, we present on Table <ref type="table" coords="14,304.21,624.02,10.35,10.91" target="#tab_6">11</ref> a comparison of the Transformer-UPWM model that was trained on the BioASQ 9B data, against a Transformer-UPWM that was trained on this year's data. For both models we trained 6 instances that were used to report the best, average and ensemble scores in terms of MAP@10 measured on the BioASQ 10b batch 5 testset. Furthermore, both models used the same training hyperparameters, which were finetuned for the BioASQ 9b dataset and as such may present some bias towards that data. As previously suspected, training the model with the BioASQ 10b gave a slight boost in terms of the results. Even more interestingly, this experience enable us to conclude that the addition of roughly more 500 biomedical questions (difference between BioASQ 9b and BioASQ 10b) only gave us a boost of 1 percentage point in terms of MAP@10. This makes us question if the BioASQ phase A has entered the phase of marginal gains, where exponential more annotated data would be needed to consistently improving the state-of-the-art.</p><p>After this experiment, for the sake of completeness, we also present, in Table <ref type="table" coords="15,460.81,323.04,8.53,10.91" target="#tab_0">12</ref>, the results of the Transformer-UPWM model trained on the BioASQ 10b data, which we defined as system 0, against the same model trained on the BioASQ 9b data, defined as system 1, for all of the evaluation batches. As observable, system 0 consistently outperforms system 1,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 12</head><p>Performance results (Rank, MAP@10) on the five official batches of RRF runs produced from 6 T-UPWM models trained on BioASQ 10b and 9b data. Note that the run "RRF-&gt; T-UPWM (BioASQ 9b)" corresponds to the System-2 from Table <ref type="table" coords="15,202.74,428.35,4.63,8.87" target="#tab_4">7</ref> System reinforcing the benefit of training with the additional 500 questions. Overall, we can consider the improvements to be marginal except on batch 3, where system 0 outscored system 1 by 5 MAP points. Furthermore, we also reported the corresponding BioASQ system ranking position if we had made submissions with system 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper we detailed our participation on tasks B phase A, B phase B and Synergy of the tenth edition of the BioASQ challenge. For each task, we described a different method that was the core of our submissions. Regarding Synergy we tested a relevance feedback based approach, that despite its simplicity achieved results that were close to the overall average. In task B phase A, we explored the PARADE-CNN and Transformer-UPWM models, where the later had been previously trained with last year's data. Here, we achieved quite interesting results, namely, the fact that an one year old model remained competitive, raising the question how it would performed if it was trained with this year's training data. After additional post-challenge experiments, we found that the additional training data contributed for an 1 point increase in terms of MAP@10. Finally on task B phase B, we made use of the PubMedBERT model for the classification task. Given the relatively small size of the dataset when compared to the complexity of the model used, we explored and made use of data augmentation techniques based on paraphrasing. While the inclusion of this data augmentation did indeed show an increase of 3 points when compared with the basic setup, it was not enough to bring the model past the average performance of the submissions by other teams.</p><p>In terms of future work, we aim to jointly address phase A (document retrieval and snippet retrieval) and phase B (yes or no), leveraging a multi-objective training paradigm. Furthermore, for the task b phase B "yes no" question answering, we would like to also explore the integration of sentiment-word scores into our classification score function, given the results reported on <ref type="bibr" coords="16,89.29,263.11,16.25,10.91" target="#b20">[22]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,89.29,319.31,418.23,8.93;7,89.29,331.31,416.70,8.87;7,89.29,343.27,416.69,8.87;7,89.29,355.22,72.21,8.87"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Distribution of the removed articles from the top-150 BM25 retrieval for each year's testset. More precisely, for each year's edition testset we retrieve the top-150 most relevant articles using the BM25 from the 2022 PubMed baseline. Then from those 150, we removed the documents that did not exist at that time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,89.29,219.33,263.60,8.93;8,91.37,84.19,412.51,122.57"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A general overview of the PARADE-CNN architecture.</figDesc><graphic coords="8,91.37,84.19,412.51,122.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,89.29,545.91,288.19,8.93;8,91.37,399.12,412.48,134.22"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A general overview of the Transformer-UPWM architecture.</figDesc><graphic coords="8,91.37,399.12,412.48,134.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="10,89.29,392.35,305.16,8.93;10,91.37,265.74,412.53,114.05"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A general overview of the Task 10b phase B Yes/No architecture.</figDesc><graphic coords="10,91.37,265.74,412.53,114.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="11,88.99,339.06,32.19,8.93;11,89.29,351.06,406.43,8.87;11,173.10,366.70,32.42,8.93;11,234.89,366.70,37.85,8.93;11,343.22,366.70,37.85,8.93;11,170.76,378.83,115.89,9.14;11,329.31,378.83,65.68,9.14;11,170.76,391.06,116.66,8.87;11,328.54,391.06,67.22,8.87;11,170.76,403.01,37.09,8.87;11,226.81,402.74,54.01,9.14;11,335.14,402.74,54.01,9.14;11,170.76,414.97,116.66,8.87;11,328.54,414.97,67.22,8.87;11,170.76,426.92,37.09,8.87;11,252.16,426.92,3.32,8.87;11,299.78,426.92,124.73,8.87"><head>Table 3</head><label>3</label><figDesc>Summary containing the description of the submitted systems to the BioASQ 10 Synergy challenge</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="12,159.40,423.82,32.33,8.55;12,271.37,423.82,32.33,8.55;12,394.50,423.82,77.59,8.55;12,95.02,435.70,113.72,8.50;12,254.37,435.70,66.34,8.50;12,400.13,435.70,66.35,8.50;12,95.02,447.16,113.72,8.50;12,254.37,447.16,66.34,8.50;12,400.13,447.16,66.35,8.50;12,95.02,458.61,405.24,8.50;12,95.02,470.07,405.24,8.50;12,95.02,481.52,35.54,8.50;12,173.98,481.52,3.18,8.50;12,258.65,481.52,57.77,8.50;12,400.13,481.52,66.35,8.50"><head>6</head><label>6</label><figDesc>System 0 RRF -&gt; T-UPWM RRF -&gt; T-UPWM RRF -&gt; T-UPWM System 1 RRF -&gt; T-UPWM RRF -&gt; T-UPWM RRF -&gt; T-UPWM System 2 RRF -&gt; T-UPWM RRF -&gt; T-UPWM + PARADE-CNN RRF -&gt; T-UPWM + PARADE-CNN System 3 RRF -&gt; T-UPWM RRF -&gt; T-UPWM + PARADE-CNN RRF -&gt; T-UPWM + PARADE-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="15,241.39,443.91,32.18,8.51;15,295.99,443.91,32.18,8.51;15,350.60,443.91,32.18,8.51;15,405.21,443.91,32.18,8.51;15,461.79,443.91,32.18,8.51;15,237.05,455.31,57.51,8.51;15,310.30,455.31,93.48,8.51;15,419.52,455.31,40.85,8.51;15,476.10,455.31,21.09,8.51;15,94.99,467.14,405.29,8.46;15,94.99,478.54,125.86,8.46;15,237.29,478.54,263.00,8.46"><head></head><label></label><figDesc>-&gt; T-UPWM (BioASQ 10b) 5 0.4381 12 0.3291 5 0.4795 12 0.3605 6 0.3834 1) RRF -&gt; T-UPWM (BioASQ 9b) 8 0.4130 12 0.3223 8 0.4290 14 0.3502 10 0.3706</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.93,445.37,417.05,56.74"><head>Table 1</head><label>1</label><figDesc>Statistics of the testset provided by the organizers of the Synergy task. It presents the number of questions that were made available in each round and in parenthesis the number of new queries relatively to the previous set of questions. It also shows the total number of feedback documents that were available to each question</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,141.31,508.87,312.65,33.62"><head>Round 1 Round 2 Round 3 Round 4</head><label></label><figDesc></figDesc><table coords="3,141.31,521.28,306.38,21.22"><row><cell>Nr of Questions</cell><cell>72 (+72)</cell><cell>70 (-2)</cell><cell>70 (+0)</cell><cell>64 (-6)</cell></row><row><cell>Nr of Feedback Documents</cell><cell>-</cell><cell>17820</cell><cell>20601</cell><cell>24268</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,221.28,659.85,284.70,10.91"><head></head><label></label><figDesc>Li et al. propose a set of PARADE models that use a BERT based</figDesc><table coords="7,177.18,282.90,249.78,13.25"><row><cell>1B</cell><cell>2B</cell><cell>3B</cell><cell>4B</cell><cell>5B BioASQ testset edition</cell><cell>6B</cell><cell>7B</cell><cell>8B</cell><cell>9B</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="13,88.99,349.94,417.00,144.95"><head>Table 7</head><label>7</label><figDesc>Batch performance (Rank, Mean Average Precision) of submitted models for the Phase A Document retrieval challenge</figDesc><table coords="13,95.93,389.54,403.41,105.35"><row><cell>Model</cell><cell>#</cell><cell>Batch 1 MAP</cell><cell>#</cell><cell>Batch 2 MAP</cell><cell>#</cell><cell>Batch 3 MAP</cell><cell>#</cell><cell>Batch 4 MAP</cell><cell>#</cell><cell>Batch 5 MAP</cell><cell>#</cell><cell>Batch 6 MAP</cell></row><row><cell>System 0</cell><cell cols="12">10 0.4098 14 0.3186 6 0.4352 15 0.3404 8 0.3734 17 0.0862</cell></row><row><cell>System 1</cell><cell cols="12">8 0.4130 12 0.3223 8 0.4290 14 0.3502 10 0.3706 18 0.0854</cell></row><row><cell>System 2</cell><cell cols="12">11 0.4059 11 0.3339 9 0.4290 12 0.3552 11 0.3704 19 0.0854</cell></row><row><cell>System 3</cell><cell cols="12">12 0.4042 13 0.3199 13 0.4174 10 0.3613 9 0.3718 21 0.0806</cell></row><row><cell>System 4</cell><cell>-</cell><cell>-</cell><cell cols="10">19 0.2813 10 0.4279 13 0.3519 6 0.3760 14 0.0928</cell></row><row><cell>Top Competitor</cell><cell cols="12">1 0.4805 1 0.3977 1 0.5063 1 0.4058 1 0.4154 1 0.1704</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="14,88.99,360.61,359.74,121.04"><head>Table 10</head><label>10</label><figDesc>Batch performance (Rank, Macro F1) of submitted models for the Yes/No challenge</figDesc><table coords="14,146.55,388.25,302.19,93.40"><row><cell>Model</cell><cell>#</cell><cell>Batch 3 M.F1</cell><cell>#</cell><cell>Batch 4 M.F1</cell><cell>#</cell><cell cols="2">Batch 5 M.F1</cell><cell>#</cell><cell>Batch 6 M.F1</cell></row><row><cell>System 0</cell><cell cols="9">26 0.6528 21 0.7723 23 0.7846 21 0.4857</cell></row><row><cell>System 1</cell><cell cols="9">25 0.7024 29 0.6581 28 0.7497 22 0.4857</cell></row><row><cell>System 2</cell><cell cols="9">24 0.7029 20 0.8634 21 0.8212 2 1.0000</cell></row><row><cell>System 3</cell><cell cols="9">28 0.6362 23 0.7474 22 0.8212 28 0.3333</cell></row><row><cell>System 4</cell><cell cols="9">27 0.6528 22 0.7723 24 0.7846 3 1.0000</cell></row><row><cell cols="7">Top Competitor 1 1.0000 1 1.0000 1</cell><cell>0.93</cell><cell cols="2">1 1.0000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="15,88.93,129.27,418.15,96.73"><head>Table 11</head><label>11</label><figDesc>Direct comparison of last year's Transformer-UPWM model (BioASQ 9b), with the same model trained with this year's training data (BioASQ 10b). We computed the MAP@10 scores, over the BioASQ 10B Batch 5 testset, of 6 models trained with the 9b and 10b datasets, and report the best individual score, the average and standard deviation and an ensemble performance with RRF</figDesc><table coords="15,156.96,192.77,281.35,33.22"><row><cell>Data</cell><cell>Best individual</cell><cell>Average</cell><cell>Ensemble (RRF)</cell></row><row><cell>BioASQ 10b</cell><cell>0.3823</cell><cell>0.3792 (0.004)</cell><cell>0.3834</cell></row><row><cell>BioASQ 9b</cell><cell>0.3776</cell><cell>0.3670 (0.008)</cell><cell>0.3706</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was partially supported by national funds through the <rs type="funder">Foundation for Science and Technology (FCT)</rs> in the context of the project <rs type="grantNumber">UIDB/00127/2020</rs> and from the <rs type="funder">EU/EFPIA Innovative Medicines Initiative 2 Joint Undertaking</rs> under grant agreement No <rs type="grantNumber">806968</rs>. <rs type="person">Tiago Almeida</rs> is funded by <rs type="funder">FCT</rs> under the grant <rs type="grantNumber">2020.05784</rs>.BD.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HyEaeYu">
					<idno type="grant-number">UIDB/00127/2020</idno>
				</org>
				<org type="funding" xml:id="_rVSK7V6">
					<idno type="grant-number">806968</idno>
				</org>
				<org type="funding" xml:id="_kyKN5ve">
					<idno type="grant-number">2020.05784</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="16,112.66,421.01,393.33,10.91;16,112.66,434.55,318.51,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="16,293.58,421.01,212.41,10.91;16,112.66,434.55,58.54,10.91">Information overload in healthcare: too much of a good thing?</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Klerings</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Weinhandl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">J</forename><surname>Thaler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,179.46,434.55,162.71,10.91">Z. Evid. Fortbild. Qual. Gesundhwes</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="285" to="290" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,448.10,395.17,10.91;16,112.66,461.65,395.17,10.91;16,112.66,475.20,395.17,10.91;16,111.87,488.75,394.11,10.91;16,112.66,502.30,395.01,10.91;16,112.66,515.85,168.81,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="16,218.64,488.75,287.34,10.91;16,112.66,502.30,214.03,10.91">An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsatsaronis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Balikas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Malakasiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zschunke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Alvers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wei√üenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Polychronopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Almirantis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Baskiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Artieres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A.-C</forename><surname>Ngonga Ngomo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Heino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Barrio-Alvers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12859-015-0564-6</idno>
	</analytic>
	<monogr>
		<title level="j" coord="16,343.15,502.30,94.70,10.91">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">138</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,529.40,393.32,10.91;16,112.66,542.95,286.64,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09093</idno>
		<title level="m" coord="16,311.35,529.40,194.63,10.91;16,112.66,542.95,104.30,10.91">Parade: Passage representation aggregation for document reranking</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="16,112.66,556.50,394.61,10.91;16,112.66,570.05,393.33,10.91;16,112.66,583.60,393.53,10.91;16,112.66,597.15,395.01,10.91;16,112.41,610.69,253.82,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,211.72,556.50,275.69,10.91">Universal passage weighting mecanism (UPWM) in bioasq 9b</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Matos</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-13.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="16,348.18,570.05,157.81,10.91;16,112.66,583.60,248.52,10.91">Proceedings of the Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="16,275.43,597.15,148.42,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<meeting>the Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum<address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">September 21st -to -24th, 2021. 2936. 2021</date>
			<biblScope unit="page" from="196" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,624.24,393.32,10.91;16,112.66,637.79,393.33,10.91;16,112.14,651.34,395.05,10.91;17,112.66,86.97,394.52,10.91;17,112.66,100.52,363.68,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,211.39,624.24,294.60,10.91;16,112.66,637.79,37.55,10.91">Bioasq synergy: A strong and simple baseline rooted in relevance feedback</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Matos</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-12.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="16,422.59,637.79,83.40,10.91;16,112.14,651.34,340.26,10.91">Proceedings of the Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="17,377.97,87.98,129.22,9.72;17,112.66,100.52,21.79,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<meeting>the Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum<address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">September 21st -to -24th, 2021. 2936. 2021</date>
			<biblScope unit="page" from="188" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,114.06,394.53,10.91;17,112.66,127.61,394.53,10.91;17,112.66,141.16,394.53,10.91;17,112.66,154.71,394.53,10.91;17,112.66,168.26,393.53,10.91;17,112.66,181.81,395.00,10.91;17,112.66,195.36,75.26,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="17,283.18,154.71,219.68,10.91">CORD-19: The COVID-19 open research dataset</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Reas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Burdick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Funk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Katsis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Murdick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Rishi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sheehan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stilson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">D</forename><surname>Wade</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">X R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kohlmeier</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.nlpcovid19-acl.1" />
	</analytic>
	<monogr>
		<title level="m" coord="17,127.31,168.26,303.80,10.91">Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020</title>
		<meeting>the 1st Workshop on NLP for COVID-19 at ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,208.91,383.11,10.91" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="17,112.66,208.91,91.02,10.91">MEDLINE -database</title>
		<ptr target="https://lhncbc.nlm.nih.gov/ii/information/MBR.html" />
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,249.56,394.53,10.91;17,112.66,263.11,393.33,10.91;17,112.66,276.66,393.33,10.91;17,112.66,290.20,387.70,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="17,317.88,249.56,185.39,10.91">Bag of tricks for efficient text classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/E17-2068" />
	</analytic>
	<monogr>
		<title level="m" coord="17,130.20,263.11,375.79,10.91;17,112.66,276.66,134.59,10.91">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct coords="17,112.66,303.75,393.32,10.91;17,112.66,317.30,324.50,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Montani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Van Landeghem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Boyd</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.1212303</idno>
		<title level="m" coord="17,353.07,303.75,152.91,10.91;17,112.66,317.30,140.35,10.91">spaCy: Industrial-strength Natural Language Processing in Python</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,330.85,393.33,10.91;17,112.66,344.40,394.61,10.91;17,112.66,357.95,393.33,10.91;17,112.66,371.50,323.68,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="17,385.89,330.85,120.10,10.91;17,112.66,344.40,374.97,10.91">Pyserini: A Python toolkit for reproducible information retrieval research with sparse and dense representations</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,112.66,357.95,393.33,10.91;17,112.66,371.50,225.91,10.91">Proceedings of the 44th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021)</title>
		<meeting>the 44th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2356" to="2362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,385.05,393.33,10.91;17,112.66,398.60,393.33,10.91;17,112.66,412.15,394.52,10.91;17,112.66,425.70,395.01,10.91;17,112.66,439.25,155.44,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="17,224.70,385.05,281.29,10.91;17,112.66,398.60,34.73,10.91">Anserini: Enabling the use of lucene for information retrieval research</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3077136.3080721</idno>
		<ptr target="https://doi.org/10.1145/3077136.3080721.doi:10.1145/3077136.3080721" />
	</analytic>
	<monogr>
		<title level="m" coord="17,168.74,398.60,337.24,10.91;17,112.66,412.15,214.72,10.91">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1253" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,452.79,394.53,10.91;17,112.66,466.34,335.42,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="17,112.66,466.34,164.30,10.91">Umass at trec 2004: Novelty and hard</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Abdul-Jaleel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Larkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,311.94,466.34,105.44,10.91">Proceedings of TREC-13</title>
		<meeting>TREC-13</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,479.89,395.17,10.91;17,112.66,493.44,393.33,10.91;17,112.66,506.99,271.56,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="17,340.16,479.89,167.68,10.91;17,112.66,493.44,140.77,10.91">Optuna: A next-generation hyperparameter optimization framework</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yanase</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,276.87,493.44,229.11,10.91;17,112.66,506.99,241.18,10.91">Proceedings of the 25rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 25rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,520.54,395.17,10.91;17,112.66,534.09,395.01,10.91;17,112.66,547.64,396.29,10.91;17,112.07,561.19,294.12,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="17,299.60,520.54,208.24,10.91;17,112.66,534.09,124.99,10.91">A retrospective study of a hybrid documentcontext based retrieval model</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">W</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kwok</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2006.10.009</idno>
		<ptr target="https://doi.org/10.1016/j.ipm.2006.10.009" />
	</analytic>
	<monogr>
		<title level="j" coord="17,245.76,534.09,170.82,10.91">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1308" to="1331" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>patent Processing</note>
</biblStruct>

<biblStruct coords="17,112.66,574.74,393.33,10.91;17,112.66,588.29,393.33,10.91;17,112.66,601.84,393.53,10.91;17,112.66,615.39,394.03,10.91;17,112.66,628.93,233.99,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="17,338.79,574.74,167.20,10.91;17,112.66,588.29,210.66,10.91">Deeprank: A new deep architecture for relevance ranking in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1145/3132847.3132914</idno>
		<ptr target="https://doi.org/10.1145/3132847.3132914.doi:10.1145/3132847.3132914" />
	</analytic>
	<monogr>
		<title level="m" coord="17,352.24,588.29,153.75,10.91;17,112.66,601.84,315.06,10.91">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM &apos;17</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management, CIKM &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,642.48,394.04,10.91;17,112.66,656.03,316.29,10.91" xml:id="b15">
	<monogr>
		<ptr target="https://huggingface.co/ramsrigouthamg/t5_sentence_paraphraser" />
		<title level="m" coord="17,112.66,642.48,271.80,10.91">Huggingface -ramsrigouthamg/t5_sentence_paraphraser</title>
		<imprint>
			<date type="published" when="2022-06-20">2022. 2022-06-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,669.58,260.48,10.91" xml:id="b16">
	<monogr>
		<ptr target="https://quillbot.com/" />
		<title level="m" coord="17,112.66,669.58,34.67,10.91">Quillbot</title>
		<imprint>
			<date type="published" when="2022-06-20">2022. 2022-06-20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,86.97,394.62,10.91;18,112.66,100.52,394.53,10.91;18,112.66,114.06,22.69,10.91" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="18,412.55,86.97,94.73,10.91;18,112.66,100.52,344.97,10.91">Ku-dmis at bioasq 9: Data-centric and model-centric approaches for biomedical question answering</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>CLEF</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,127.61,395.01,10.91;18,112.66,141.16,188.31,10.91" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Biewald</surname></persName>
		</author>
		<ptr target="https://www.wandb.com/,softwareavailablefromwandb.com" />
		<title level="m" coord="18,163.90,127.61,197.33,10.91">Experiment tracking with weights and biases</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,154.71,393.33,10.91;18,112.66,168.26,393.33,10.91;18,112.66,181.81,394.53,10.91;18,112.28,195.36,395.00,10.91;18,112.31,208.91,312.30,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="18,305.76,154.71,200.23,10.91;18,112.66,168.26,169.31,10.91">Reciprocal rank fusion outperforms condorcet and individual rank learning methods</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Buettcher</surname></persName>
		</author>
		<idno type="DOI">10.1145/1571941.1572114</idno>
		<ptr target="https://doi.org/10.1145/1571941.1572114.doi:10.1145/1571941.1572114" />
	</analytic>
	<monogr>
		<title level="m" coord="18,307.29,168.26,198.70,10.91;18,112.66,181.81,390.58,10.91">Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;09</title>
		<meeting>the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="758" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,222.46,393.33,10.91;18,112.66,236.01,393.33,10.91;18,112.66,249.56,355.68,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="18,263.89,222.46,242.10,10.91;18,112.66,236.01,174.33,10.91">A yes/no answer generator based on sentiment-word scores in biomedical question answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">El</forename><surname>Ouatik</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Alaoui</surname></persName>
		</author>
		<idno type="DOI">10.4018/IJHISI.2017070104</idno>
	</analytic>
	<monogr>
		<title level="j" coord="18,295.59,236.01,210.40,10.91;18,112.66,249.56,111.85,10.91">International Journal Of Healthcare Information Systems And Informatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="62" to="74" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
