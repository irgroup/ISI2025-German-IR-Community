<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,374.64,15.42;1,89.29,106.66,361.64,15.42">Biomedical Spanish Language Models for entity recognition and linking at BioASQ DisTEMIST</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.90,134.97,89.64,11.96"><forename type="first">Vincenzo</forename><surname>Moscato</surname></persName>
							<email>vincenzo.moscato@unina.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Information Technology (DIETI)</orgName>
								<orgName type="institution">University of Naples Federico II</orgName>
								<address>
									<addrLine>Via Claudio</addrLine>
									<postCode>21 -80125</postCode>
									<settlement>Naples</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">ITEM National Lab</orgName>
								<orgName type="laboratory">Consorzio Interuniversitario</orgName>
								<orgName type="institution" key="instit1">Nazionale per l&apos;Informatica (CINI</orgName>
								<orgName type="institution" key="instit2">Complesso Universitario Monte S.Angelo</orgName>
								<address>
									<settlement>Naples</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,196.47,134.97,88.37,11.96"><forename type="first">Marco</forename><surname>Postiglione</surname></persName>
							<email>marco.postiglione@unina.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Information Technology (DIETI)</orgName>
								<orgName type="institution">University of Naples Federico II</orgName>
								<address>
									<addrLine>Via Claudio</addrLine>
									<postCode>21 -80125</postCode>
									<settlement>Naples</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,315.83,134.97,78.84,11.96"><forename type="first">Giancarlo</forename><surname>SperlÃ­</surname></persName>
							<email>giancarlo.sperli@unina.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Information Technology (DIETI)</orgName>
								<orgName type="institution">University of Naples Federico II</orgName>
								<address>
									<addrLine>Via Claudio</addrLine>
									<postCode>21 -80125</postCode>
									<settlement>Naples</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">ITEM National Lab</orgName>
								<orgName type="laboratory">Consorzio Interuniversitario</orgName>
								<orgName type="institution" key="instit1">Nazionale per l&apos;Informatica (CINI</orgName>
								<orgName type="institution" key="instit2">Complesso Universitario Monte S.Angelo</orgName>
								<address>
									<settlement>Naples</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,374.64,15.42;1,89.29,106.66,361.64,15.42">Biomedical Spanish Language Models for entity recognition and linking at BioASQ DisTEMIST</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">A178D24B183A4B3ED83AFEAE97A177AC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Biomedical Named Entity Recognition</term>
					<term>Entity Linking</term>
					<term>Transformers</term>
					<term>EHRs</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Named Entity Recognition and Entity Linking systems usually require a rich and annotated dataset to be trained and produce high-quality results, but the annotation process is time consuming and expensive, especially when it needs the effort of domain experts, such as in the medical field. However, recent developments in Natural Language Processing (NLP) allow us to easily use transformer language models which have been pre-trained on a huge quantity of data (often coming from specialized domains), and thus obtain high performance without excessive efforts. In this work, we outline our approach to NER and EL tasks on Spanish clinical notes for the DisTEMIST track at the BioASQ 2022 challenge. Our results demonstrate that the proposed methodology based on biomedical pre-trained language models turned out the best for the NER task with a âˆ¼ 3% higher ğ¹ 1 w.r.t. the second-best solution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Biomedical Named Entity Recognition (NER) and Entity Linking (EL) are often the first and essential steps in many text understanding applications <ref type="bibr" coords="1,329.31,438.96,11.27,10.91" target="#b0">[1]</ref>, such as the construction and analysis of structured knowledge bases (e.g. knowledge graphs) or conversational agents including medical chatbots and research assistants.</p><p>NER aims at recognizing mentions of pre-defined entity types within unstructured text data, while EL links them to concepts in a (usually) external knowledge base (e.g. UMLS <ref type="bibr" coords="1,445.85,493.16,11.28,10.91" target="#b1">[2]</ref>, SNOMED CT 1 ). These two tasks are the subject of the DisTEMIST (DISease TExt Mining Shared Task) track at the BioASQ 2022 challenge <ref type="bibr" coords="1,220.06,520.25,11.37,10.91" target="#b2">[3]</ref>, which invites researchers, biomedical industry professionals, NLP, and ontology experts to develop systems capable of indexing the content about diseases within Spanish clinical notes. In this work, we describe the approach of our team (PICUSLab) which allowed us to win the NER track with a âˆ¼ 3% higher ğ¹ 1 measure w.r.t. the second-best solution.</p><p>The development of a biomedical text understanding system with high precision and recall is a challenging task due to the fact that biomedical datasets are characterized by a large number of synonyms, alternate spellings of entities, which are often referred to with non-standard abbreviations, and polysemous words, i.e. words that can have different meanings based on their context. For example, VHL may refer to the Von Hippel-Lindau disease or to the gene name which causes the disease, based on the context in which it appears.</p><p>Initially, NER and EL systems were mainly dictionary-and rule-based <ref type="bibr" coords="2,420.92,168.26,11.58,10.91" target="#b3">[4]</ref>, but they failed dealing with unseen and polysemous words <ref type="bibr" coords="2,285.26,181.81,11.35,10.91" target="#b4">[5]</ref>. The availability of annotated datasets allowed systems to evolve by means of deep learning techniques, such as Bidirectional Long-Short Term Memory (BiLSTM) networks <ref type="bibr" coords="2,220.97,208.91,11.42,10.91" target="#b5">[6,</ref><ref type="bibr" coords="2,235.12,208.91,9.00,10.91" target="#b6">7]</ref> and Transformer architectures <ref type="bibr" coords="2,386.61,208.91,11.42,10.91" target="#b7">[8,</ref><ref type="bibr" coords="2,400.76,208.91,7.61,10.91" target="#b8">9]</ref>. However, due to the above-mentioned problems related to biomedical corpora, directly applying state-of-the-art NLP methodologies to biomedical text mining has limitations, since language models are trained and tested mainly on datasets containing general domain texts (e.g. Wikipedia). Hence, recent models proposed in biomedical text mining rely on adapted versions of pre-trained language models <ref type="bibr" coords="2,123.74,276.66,16.43,10.91" target="#b9">[10,</ref><ref type="bibr" coords="2,142.90,276.66,12.32,10.91" target="#b10">11]</ref>, even in low-and mid-resource languages <ref type="bibr" coords="2,348.05,276.66,16.43,10.91" target="#b11">[12,</ref><ref type="bibr" coords="2,367.21,276.66,12.32,10.91" target="#b12">13]</ref>.</p><p>In this work, we describe our approach to the DisTEMIST track, which is based on embeddings computed with a Spanish biomedical RoBERTa backbone network <ref type="bibr" coords="2,401.50,303.75,16.41,10.91" target="#b12">[13]</ref>. We use a simple classification head (a linear layer with a softmax activation function) to produce an high-quality NER system, and then apply a similarity-based EL process to entities retrieved in the NER step. Our experimental results show the appropriateness of the methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Tasks formulation</head><p>We start with a corpus of annotated sentences ğ’Ÿ = {(x ğ‘– , y ğ‘– ) âˆˆ ğ’³ Ã— ğ’´}, where:</p><p>â€¢ ğ‘– âˆˆ {1, ..., ğ‘ }, ğ‘ is the length of the dataset â€¢ ğ’³ is the set of sentences â€¢ x ğ‘– is a sentence, which is defined as a sequence of tokens ğ‘¥ ğ‘— âˆˆ x ğ‘– , ğ‘— âˆˆ {1, . . . , ğ» ğ‘– }, where ğ» ğ‘– is the sequence length â€¢ ğ’´ is the set of labels. In our work, we will refer to the IOB2 annotation scheme <ref type="bibr" coords="2,464.19,480.13,16.09,10.91" target="#b13">[14]</ref>, thus ğ’´ = {ğµ, ğ¼, ğ‘‚}, where ğµ indicates the beginning, ğ¼ the inside and ğ‘‚ the outside of an entity mention â€¢ y ğ‘– maps each token ğ‘¥ ğ‘— âˆˆ x ğ‘– to its corresponding label ğ‘¦ ğ‘— .</p><p>Based on this corpus, the objective of a NER model is to assign the correct label in ğ’´ to each token in an input sentence.</p><p>Given the set of entity mentions ğ‘€ resulting from the NER step, and a knowledge base containing a set of entities ğ¸, EL aims to map each entity mention ğ‘š âˆˆ ğ‘€ to the most appropriate concept ğ‘’ âˆˆ ğ¸.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Materials</head><p>The DisTEMIST corpus was manually annotated by clinical experts following guidelines containing rules for annotating diseases in Spanish clinical cases. Guidelines were created de  <ref type="formula" coords="3,267.62,337.98,3.47,8.87" target="#formula_1">2</ref>) concept embeddings for each concept within the gazetteer which will be used by the Linker to associate the nearest concept to an entity mention based on similarity measures.</p><p>novo by clinical experts defined after several cycles of quality control and annotation consistency analysis before annotating the entire dataset.</p><p>The training set for NER and EL consists of 750 and 584 annotated clinical cases, respectively. However, every entity mention in the original DisTEMIST corpus has been linked to a Snomed-CT, also when the exact concept is not present within the ontology (e.g. "Chorioretinal lacunae" is normalized to "Chorioretinal disorder").</p><p>The DisTEMIST gazetteer contains main terms and synonyms from the relevant branches of Snomed-CT for the grounding of disease mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>Figure <ref type="figure" coords="3,119.54,550.69,4.97,10.91" target="#fig_0">1</ref> shows an overview of the methodological flow of our solution for the DisTEMIST track. A Transformer backbone network pre-trained with Spanish biomedical corpora has been used in both NER and EL tasks. In the former case, it has been used to compute token embeddings for a classification head with a linear layer and a softmax activation function; in the latter, it computes concept embeddings for each concept within the gazetteer, which will be then used to link an entity mention to the nearest concept based on a measure of similarity. In this section, each module of our methodology will be extensively described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Biomedical Transformer Backbone network</head><p>The Biomedical Transformer Backbone network used in this work has been pre-trained and made publicly available by Carrino et al. <ref type="bibr" coords="4,280.56,121.08,16.29,10.91" target="#b12">[13]</ref>. It uses a RoBERTa <ref type="bibr" coords="4,394.71,121.08,18.07,10.91" target="#b14">[15]</ref> base model with 12 self-attention layers with masked language modeling as the pre-training objective. The dataset used to pre-train the network consists in two corpora with different sizes and domains:</p><p>â€¢ Clinical corpus: it contains 91M tokens from more than 278K clinical documents (e.g. discharge reports, clinical course notes). â€¢ Biomedical corpus: it contains data from a variety of sources, such as medical crawlers, PubMed<ref type="foot" coords="4,153.12,213.94,3.71,7.97" target="#foot_0">2</ref> and Scielo<ref type="foot" coords="4,205.21,213.94,3.71,7.97" target="#foot_1">3</ref> publications and patents. The entire corpus counts a total of 968M words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Named Entity Recognizer</head><p>The Transformer-based backbone network is used to extract an embedded representation of each token ğ‘¥ ğ‘— in an input sample x, z = ğ‘“ ğœƒ ğ¿ğ‘€ (ğ‘¥ ğ‘— ), ğœƒ ğ¿ğ‘€ being the set of language model parameters.</p><p>Thereafter, a linear layer (a.k.a. classification head) with parameters ğœƒ ğ¿ = {W, b} project the Transformer-based representation z into the label space, ğ‘“ ğœƒ ğ¿ (z) = ğ‘†ğ‘œğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥(Wz + b). The model parameters are then optimized by minimizing cross-entropy:</p><formula xml:id="formula_0" coords="4,213.62,356.98,292.37,34.42">â„’ ğ¶ğ¸ = âˆ‘ï¸ (x,y)âˆˆğ’Ÿ ğ» âˆ‘ï¸ ğ‘–=1 ğ¾ğ¿ (ï¸ ğ‘¦ ğ‘– âƒ’ âƒ’ âƒ’ğ‘(ğ‘¦ ğ‘– |ğ‘¥ ğ‘– ) )ï¸ ,<label>(1)</label></formula><p>where ğ¾ğ¿(ğ‘|ğ‘) is the Kullback-Leibler divergence between the two distributions ğ‘ and ğ‘, and ğ‘ is the prediction probability vector for each token:</p><formula xml:id="formula_1" coords="4,207.12,441.06,298.86,12.46">ğ‘(ğ‘¦|ğ‘¥) = ğ‘†ğ‘œğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥(W â€¢ ğ‘“ ğœƒ ğ‘ƒ ğ¿ğ‘€ (ğ‘¥) + b)<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Entity Linker</head><p>Inspired by Kraljevic et al. <ref type="bibr" coords="4,202.65,490.79,16.21,10.91" target="#b15">[16]</ref>, our EL approach relies on a Concept Database (ğ¶ğ·ğµ) component, i.e. a table representing a concept dictionary. To this end, e used the gazetteer provided by DisTEMIST track organizers. Even though not every concept within the gazetteer appears in our training set, we decided to keep all the concepts due to the unpredictability of concepts in the test set. Our linking approach is based on context similarity: we learn an embedded representation for each concept and for new documents, when an entity mention is detected by the NER model, its context is compared to the embedded representations of all the concepts in the ğ¶ğ·ğµ to choose the most appropriate one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concept Embeddings</head><p>We learn concept embeddings in a supervised fashion. For each concept ğ‘ âˆˆ ğ¶ğ·ğµ, we perform the steps described as follows to compute its concept embedding ğ‘‰ ğ‘ ğ‘ğ‘œğ‘›ğ‘ğ‘’ğ‘ğ‘¡ : 1. Initialization: given the concept name ğ‘ ğ‘›ğ‘ğ‘šğ‘’ and its description ğ‘ ğ‘‘ğ‘’ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘› provided with the gazetteer, we initialize ğ‘‰ ğ‘ ğ‘ğ‘œğ‘›ğ‘ğ‘’ğ‘ğ‘¡ with the embedding of the concatenation of the two strings [ğ‘ ğ‘›ğ‘ğ‘šğ‘’ , ğ‘ ğ‘‘ğ‘’ğ‘ ğ‘ğ‘Ÿğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ] computed with the Biomedical Transformer backbone network. 2. Context embeddings: for each entity in the training set annotated with the concept ğ‘, we compute its context embedding ğ‘‰ ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ with the Biomedical Transformer backbone network. 3. Update: for each entity in the training set annotated with the concept ğ‘, the concept embedding ğ‘‰ ğ‘ ğ‘ğ‘œğ‘›ğ‘ğ‘’ğ‘ğ‘¡ is updated with the context embedding ğ‘‰ ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ . Specifically, the update criterion is described by the following equation:</p><formula xml:id="formula_2" coords="5,208.77,287.25,297.22,14.19">ğ‘‰ ğ‘ ğ‘ğ‘œğ‘›ğ‘ğ‘’ğ‘ğ‘¡ = ğ‘‰ ğ‘ ğ‘ğ‘œğ‘›ğ‘ğ‘’ğ‘ğ‘¡ + ğ‘™ğ‘Ÿ â€¢ (1 -ğ‘ ğ‘–ğ‘š) â€¢ ğ‘‰ ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ ,<label>(3)</label></formula><p>where:</p><p>â€¢ ğ‘™ğ‘Ÿ is the learning rate, computed as ğ‘™ğ‘Ÿ = 1 ğ’©ğ‘ , ğ’© ğ‘ being the number of times the concept appears during training.</p><p>â€¢ ğ‘ ğ‘–ğ‘š is the cosine similarity between ğ‘‰ ğ‘ ğ‘ğ‘œğ‘›ğ‘ğ‘’ğ‘ğ‘¡ and ğ‘‰ ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ ,</p><formula xml:id="formula_3" coords="5,232.46,368.28,273.53,29.82">ğ‘ ğ‘–ğ‘š = ğ‘šğ‘ğ‘¥ (ï¸ 0, ğ‘‰ ğ‘ ğ‘ğ‘œğ‘›ğ‘ğ‘’ğ‘ğ‘¡ â€–ğ‘‰ ğ‘ ğ‘ğ‘œğ‘›ğ‘ğ‘’ğ‘ğ‘¡ â€– â€¢ ğ‘‰ ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ â€–ğ‘‰ ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ â€– )ï¸<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linking</head><p>Given the entity mention recognized by the NER model, we compute its context embedding ğ‘‰ ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ by means of the Biomedical Transformer backbone network. Then, we compute its cosine similarity ğ‘ ğ‘–ğ‘š with all the concept embeddings ğ‘‰ ğ‘ğ‘œğ‘›ğ‘ğ‘’ğ‘ğ‘¡ . We eventually link the entity with the most similar concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>The performance of our proposed approaches for NER and EL has been evaluated by participating to the DISease TExt Mining Shared Task (DisTEMIST) track within the BioASQ 2022 challenge.</p><p>In this section we show the performance results of our methodology on the final test set and some preliminary experiments on the training corpus provided by the challenge organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental setup Evaluation Metrics</head><p>Evaluation is done by comparing the automatically generated results to the results generated by manual annotation of experts. The primary evaluation metric for both the NER and EL sub-tracks will consist of micro-averaged precision (MiP), recall (MiR) and F1-scores (MiF1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Configuration</head><p>Both the NER and EL models were implemented using the HuggingFace Transformers library (v4.4.0) <ref type="bibr" coords="6,241.80,121.08,16.41,10.91" target="#b16">[17]</ref>.</p><p>The biomedical Spanish Transformer backbone network has been downloaded from the HuggingFace model repository (PlanTL-GOB-ES/roberta-base-biomedical-clinical-es). To deal with the limited length of input samples, we consider each sentence in a clinical case as a separate input samples for our models. In a preliminary phase to our submission, we studied the effects of various hyperparameters and the generalization error of our models by splitting the original corpus of clinical cases in three parts: (1) a training set (60% of the original corpus) used to train the model, (2) a validation set (20% of the original corpus) to evaluate the effects of hyperparameters and (3) a test set (20% of the original corpus) to evaluate the ability of our models to generalize to unseen data. We fine-tune our models with a Google Colab environment, which provided us a Tesla T4 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NER hyperparameters and evaluation</head><p>We studied the effects of different hyperparameters on our validation set:</p><p>â€¢ learning rate: the initial learning rate for AdamW optimizer. Initialized to 5e-5.</p><p>â€¢ weight decay: the weight decay to apply to all layers except all bias and LayerNorm weights in AdamW optimizer. Initialized to 0 (no weight decay applied) â€¢ batch size: the batch size per device (e.g. CPU, GPU) for training. Initialized to 16.</p><p>For each experiment, we train the NER model for two epochs -at the end of the selection process we will analyze the effects of an increased number of epochs. Our search for hyperparameters divides into two stages: in the first stage, we make hyperparameters vary in large ranges with the aim to detect a smaller range where we will perform a grid search. All the different configurations and associated performance results are listed in Table <ref type="table" coords="6,438.00,475.44,3.74,10.91" target="#tab_0">1</ref>.</p><p>In the second stage, we perform a grid search based on a uniform distribution within the following hyperparameters ranges (which have been chosen based on the results of the first stage):</p><p>â€¢ learning rate: [7e-5, 8e-5] â€¢ weight decay: [0.1, 0.2] â€¢ batch size: 8</p><p>Given the best results from the grid search, we increased the number of training epochs with an early stopping criterion, by stopping training when the performance on the validation set does not increase for 5 consecutive epochs. The final preliminary results and the generalization error are shown in Table <ref type="table" coords="6,201.80,637.55,3.74,10.91">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EL evaluation</head><p>We evaluated results of our linking module with and without the gazetteer: challenge organizers declared that it contains all the possibile links to all the entity mentions in the test set. However, its size (113609 concepts) is much higher w.r.t. the number of concepts in our training set (2430 concepts). When a concept does not appear in the training set, its embedding is determined by its name and description, which could result in many "noisy" concepts leading to wrong linking results.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Leaderboard</head><p>Official results of the DisTEMIST track are reported in Table <ref type="table" coords="8,347.96,464.92,4.97,10.91" target="#tab_3">4</ref> (NER) and Table <ref type="table" coords="8,426.53,464.92,4.97,10.91" target="#tab_4">5</ref> (EL). Specifically, we show (1) our results, (2) results from the best participant team (second-best in case of NER, since our team is the first ranked), and (3) median results (computed by considering the best submissions of each participant team). While the domain-specific pre-training of the backbone network has been the key for a successful NER system, the EL solution seems to suffer from a design flaw. We can indeed observe a big discrepancy between results on our internal test set and the leaderboard, which may be caused by two main factors: (1) pipelined errors of NER and EL predictions and (2) the inappropriateness of the size of the training set: the gazetteer size (113609 concepts) suggests us that the leaderboard test set contains many concepts which are not present in our training set (2430 concepts). However, our context-based EL methodology computes embedded representations of concepts based on their occurrences in the training set, and all the other concepts are represented with their description provided with the gazetteer, which may be useless or even detrimental for similarity computation. Further investigations to handle the above-described problems are thus needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper we have presented a simple but strong baseline based in biomedical Spanish language models. Specifically, we used a pre-trained biomedical Spanish transformer backbone network to fine-tune a NER model and to perform EL with an embedding similarity-based approach. Results on the official leaderboard of the DisTEMIST track at BioASQ 2022 challenge show that our NER approach largely surpasses the other participant baselines, while the EL approach has to be further investigated to improve its generalization ability over new clinical cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,314.02,418.35,8.93;3,89.29,325.97,416.70,8.96;3,89.29,337.93,416.69,8.96;3,88.93,349.88,417.30,8.96;3,89.29,361.89,40.75,8.87"><head>(Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of our NER + EL solution for the DisTEMIST track. A biomedical Spanish pretrained Transformer backbone network is used to compute: (1) token embeddings to be classified by a classification head (linear layer + softmax); (2) concept embeddings for each concept within the gazetteer which will be used by the Linker to associate the nearest concept to an entity mention based on similarity measures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,88.99,90.49,413.44,367.70"><head>Table 1</head><label>1</label><figDesc>NER hyperparameter selection (first stage)</figDesc><table coords="7,88.99,124.32,413.44,333.87"><row><cell cols="2">batch size</cell><cell cols="2">learning rate</cell><cell cols="2">weight decay</cell><cell>MiP</cell><cell></cell><cell>MiR</cell><cell>MiF1</cell></row><row><cell>16</cell><cell></cell><cell>5e-5</cell><cell></cell><cell>0.0</cell><cell></cell><cell>0.7199</cell><cell></cell><cell>0.7759</cell><cell>0.7521</cell></row><row><cell>16</cell><cell></cell><cell>4e-5</cell><cell></cell><cell>0.0</cell><cell></cell><cell>0.7136</cell><cell></cell><cell>0.7677</cell><cell>0.7448</cell></row><row><cell>16</cell><cell></cell><cell>3e-5</cell><cell></cell><cell>0.0</cell><cell></cell><cell>0.7095</cell><cell></cell><cell>0.7749</cell><cell>0.7460</cell></row><row><cell>16</cell><cell></cell><cell>2e-5</cell><cell></cell><cell>0.0</cell><cell></cell><cell>0.6805</cell><cell></cell><cell>0.7672</cell><cell>0.7263</cell></row><row><cell>16</cell><cell></cell><cell>1e-5</cell><cell></cell><cell>0.0</cell><cell></cell><cell>0.6209</cell><cell></cell><cell>0.7175</cell><cell>0.6704</cell></row><row><cell>16</cell><cell></cell><cell>6e-5</cell><cell></cell><cell>0.0</cell><cell></cell><cell>0.7274</cell><cell></cell><cell>0.7836</cell><cell>0.7598</cell></row><row><cell>16</cell><cell></cell><cell>7e-5</cell><cell></cell><cell>0.0</cell><cell></cell><cell>0.7370</cell><cell></cell><cell>0.7822</cell><cell>0.7624</cell></row><row><cell>16</cell><cell></cell><cell>8e-5</cell><cell></cell><cell>0.0</cell><cell></cell><cell cols="2">0.7400</cell><cell>0.7836</cell><cell>0.7642</cell></row><row><cell>16</cell><cell></cell><cell>9e-5</cell><cell></cell><cell>0.0</cell><cell></cell><cell>0.7331</cell><cell></cell><cell>0.7827</cell><cell>0.7571</cell></row><row><cell>16</cell><cell></cell><cell>1e-4</cell><cell></cell><cell>0.0</cell><cell></cell><cell>0.7373</cell><cell></cell><cell>0.7754</cell><cell>0.7612</cell></row><row><cell>16</cell><cell></cell><cell>8e-5</cell><cell></cell><cell>0.1</cell><cell></cell><cell>0.7375</cell><cell></cell><cell>0.7885</cell><cell>0.7675</cell></row><row><cell>16</cell><cell></cell><cell>8e-5</cell><cell></cell><cell>0.2</cell><cell></cell><cell cols="2">0.7428</cell><cell>0.7865</cell><cell>0.7694</cell></row><row><cell>16</cell><cell></cell><cell>8e-5</cell><cell></cell><cell>0.3</cell><cell></cell><cell>0.7396</cell><cell></cell><cell>0.7846</cell><cell>0.7668</cell></row><row><cell>8</cell><cell></cell><cell>8e-5</cell><cell></cell><cell>0.2</cell><cell></cell><cell cols="2">0.7479</cell><cell>0.7865</cell><cell>0.7722</cell></row><row><cell>Table 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Final preliminary NER results</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>epochs</cell><cell>batch</cell><cell>learning</cell><cell cols="2">weight</cell><cell>MiP</cell><cell>MiR</cell><cell>MiF1</cell></row><row><cell></cell><cell>size</cell><cell>rate</cell><cell cols="2">decay</cell><cell></cell><cell></cell><cell></cell></row><row><cell>18</cell><cell>8</cell><cell>8.516e-5</cell><cell cols="2">0.1844</cell><cell>0.7814</cell><cell>0.8031</cell><cell>0.7921</cell><cell>best hyper-parameters</cell></row><row><cell>18</cell><cell>8</cell><cell>8.516e-5</cell><cell cols="2">0.1844</cell><cell>0.7738</cell><cell>0.7931</cell><cell>0.7833</cell><cell>internal test set error</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,89.29,575.13,416.69,51.56"><head></head><label></label><figDesc>Table 3 reports results on our "internal" test set (a 20% subset of the training files provided for entity linking) obtained with and without the gazetteer, i.e. we considered only concepts appearing at least one time in the training set. Since results are equivalent, we decided to keep the gazetteer for our submission.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,88.99,90.49,352.45,75.82"><head>Table 3</head><label>3</label><figDesc>Entity linking "internal" test set results with and without using the gazetteer.</figDesc><table coords="8,91.07,124.32,350.37,42.00"><row><cell>System</cell><cell>MiP</cell><cell>MiR</cell><cell>MiF1</cell></row><row><cell>With gazetteer</cell><cell>0.7374</cell><cell>0.7374</cell><cell>0.7374</cell></row><row><cell>Without gazetteer</cell><cell>0.7374</cell><cell>0.7374</cell><cell>0.7374</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,88.99,189.49,416.99,102.03"><head>Table 4</head><label>4</label><figDesc>Official results of BioASQ DisTEMIST NER task. We show our result, the second-best result and median result (computed by considering just the best MiF1 score for each participant team).</figDesc><table coords="8,91.07,235.27,352.79,56.24"><row><cell>System</cell><cell>MiP</cell><cell>MiR</cell><cell>MiF1</cell></row><row><cell>Ours</cell><cell>0.7915</cell><cell>0.7629</cell><cell>0.7770</cell></row><row><cell>Second-best participant</cell><cell>0.7434</cell><cell>0.7483</cell><cell>0.7458</cell></row><row><cell>Median</cell><cell>0.7146</cell><cell>0.6736</cell><cell>0.6935</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,88.99,314.69,416.99,102.03"><head>Table 5</head><label>5</label><figDesc>Official results of BioASQ DisTEMIST linking task. We show our result, the best result and median result (computed by considering just the best MiF1 score for each participant team).</figDesc><table coords="8,91.07,360.48,352.79,56.24"><row><cell>System</cell><cell>MiP</cell><cell>MiR</cell><cell>MiF1</cell></row><row><cell>Ours</cell><cell>0.2814</cell><cell>0.2748</cell><cell>0.2780</cell></row><row><cell>Best participant</cell><cell>0.6207</cell><cell>0.5196</cell><cell>0.5657</cell></row><row><cell>Median</cell><cell>0.4795</cell><cell>0.2292</cell><cell>0.3102</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="4,108.93,659.97,119.33,8.97"><p>https://pubmed.ncbi.nlm.nih.gov</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="4,108.93,670.93,61.43,8.97"><p>https://scielo.org</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,251.20,394.62,10.91;9,112.66,264.75,395.01,10.91;9,112.66,278.30,107.97,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,278.03,251.20,229.24,10.91;9,112.66,264.75,68.23,10.91">Named entity recognition and relation extraction: State-of-the-art</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Nasar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">W</forename><surname>Jaffry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Malik</surname></persName>
		</author>
		<idno type="DOI">10.1145/3445965</idno>
		<ptr target="https://doi.org/10.1145/3445965.doi:10.1145/3445965" />
	</analytic>
	<monogr>
		<title level="j" coord="9,192.75,264.75,93.40,10.91">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,291.85,393.33,10.91;9,112.66,305.40,313.94,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,190.92,291.85,315.07,10.91;9,112.66,305.40,51.86,10.91">The unified medical language system (umls): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,173.16,305.40,98.78,10.91;9,287.54,305.40,65.42,10.91">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>Database issue</note>
</biblStruct>

<biblStruct coords="9,112.66,318.95,394.53,10.91;9,112.28,332.50,395.00,10.91;9,112.28,346.05,394.91,10.91;9,112.66,359.59,393.33,10.91;9,112.33,373.14,227.74,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,362.24,332.50,145.03,10.91;9,112.28,346.05,394.91,10.91;9,112.66,359.59,119.53,10.91">Overview of distemist at bioasq: Automatic detection and normalization of diseases from clinical texts: results, methods, evaluation and multilingual</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miranda-Escalada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">L</forename><surname>-L. Luis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>GascÃ³</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>FarrÃ©-Maduell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Krallinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,254.96,359.59,251.03,10.91;9,112.33,373.14,197.61,10.91">Working Notes of Conference and Labs of the Evaluation (CLEF) Forum. CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,386.69,393.32,10.91;9,112.26,400.24,391.03,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,212.91,386.69,293.07,10.91;9,112.26,400.24,142.56,10.91">Unsupervised biomedical named entity recognition: Experiments with clinical and biological texts</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,262.69,400.24,148.85,10.91">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1088" to="1098" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,413.79,393.32,10.91;9,112.26,427.34,395.42,10.91;9,112.66,440.89,393.33,10.91;9,112.66,454.44,65.26,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,212.91,413.79,293.07,10.91;9,112.26,427.34,142.82,10.91">Unsupervised biomedical named entity recognition: Experiments with clinical and biological texts</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Elhadad</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbi.2013.08.004</idno>
		<ptr target="https://doi.org/10.1016/j.jbi.2013.08.004" />
	</analytic>
	<monogr>
		<title level="j" coord="9,262.96,427.34,150.45,10.91">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1088" to="1098" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>special Section: Social Media Environments</note>
</biblStruct>

<biblStruct coords="9,112.66,467.99,393.53,10.91;9,112.66,481.54,393.33,10.91;9,112.66,495.09,394.53,10.91;9,112.28,508.64,395.00,10.91;9,112.66,522.18,295.80,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,402.15,467.99,104.04,10.91;9,112.66,481.54,109.69,10.91">Neural architectures for named entity recognition</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1030</idno>
		<ptr target="https://aclanthology.org/N16-1030.doi:10.18653/v1/N16-1030" />
	</analytic>
	<monogr>
		<title level="m" coord="9,245.17,481.54,260.82,10.91;9,112.66,495.09,394.53,10.91;9,112.28,508.64,184.59,10.91">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,535.73,393.33,10.91;9,112.66,549.28,393.32,10.91;9,112.66,562.83,393.32,10.91;9,112.66,576.38,395.01,10.91;9,112.66,589.93,138.14,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,198.84,535.73,307.15,10.91;9,112.66,549.28,111.52,10.91">Recurrent neural network models for disease name recognition using domain invariant features</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1209</idno>
		<ptr target="https://aclanthology.org/P16-1209.doi:10.18653/v1/P16-1209" />
	</analytic>
	<monogr>
		<title level="m" coord="9,246.72,549.28,259.27,10.91;9,112.66,562.83,135.56,10.91">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2216" to="2225" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="9,112.66,603.48,393.33,10.91;9,112.66,617.03,393.33,10.91;9,112.66,630.58,393.32,10.91;9,112.66,644.13,393.33,10.91;10,112.66,86.97,394.03,10.91;10,112.66,100.52,185.51,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,323.15,603.48,182.83,10.91;9,112.66,617.03,186.91,10.91">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://aclanthology.org/N19-1423.doi:10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m" coord="9,327.87,617.03,178.11,10.91;9,112.66,630.58,393.32,10.91;9,112.66,644.13,99.97,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="10,112.66,114.06,394.53,10.91;10,112.66,127.61,394.53,10.91;10,112.66,141.16,394.53,10.91;10,112.66,154.71,394.53,10.91;10,112.66,168.26,397.48,10.91;10,112.36,184.25,132.96,7.90" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,112.66,168.26,166.45,10.91">Language models are few-shot learners</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2005.14165</idno>
		<ptr target="https://arxiv.org/abs/2005.14165.doi:10.48550/ARXIV.2005.14165" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,195.36,393.33,10.91;10,112.66,208.91,393.58,10.91;10,111.46,222.46,31.40,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,361.64,195.36,144.35,10.91;10,112.66,208.91,254.68,10.91">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,376.17,208.91,65.08,10.91">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,236.01,394.53,10.91;10,112.66,249.56,393.33,10.91;10,112.66,263.11,394.52,10.91;10,112.66,276.66,394.51,10.91;10,112.36,292.65,68.18,7.90" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,112.66,249.56,194.37,10.91">Publicly available clinical BERT embeddings</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-H</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jindi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mcdermott</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-1909</idno>
		<ptr target="https://aclanthology.org/W19-1909.doi:10.18653/v1/W19-1909" />
	</analytic>
	<monogr>
		<title level="m" coord="10,330.71,249.56,175.27,10.91;10,112.66,263.11,330.65,10.91">Proceedings of the 2nd Clinical Natural Language Processing Workshop, Association for Computational Linguistics</title>
		<meeting>the 2nd Clinical Natural Language Processing Workshop, Association for Computational Linguistics<address><addrLine>Minneapolis, Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="72" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,303.75,394.52,10.91;10,112.66,317.30,393.33,10.91;10,112.66,330.85,393.33,10.91;10,112.66,344.40,393.33,10.91;10,112.66,357.95,395.01,10.91;10,112.66,371.50,209.35,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,391.98,317.30,114.01,10.91;10,112.66,330.85,277.74,10.91">BioBERTpt -a Portuguese neural language model for clinical named entity recognition</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">T R</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">V A</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Knafou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">E S E</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Copara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">B</forename><surname>Gumiel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">F A D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">C</forename><surname>Paraiso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Teodoro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M C M</forename><surname>Barra</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.clinicalnlp-1.7</idno>
		<ptr target="https://aclanthology.org/2020.clinicalnlp-1.7.doi:10.18653/v1/2020.clinicalnlp-1.7" />
	</analytic>
	<monogr>
		<title level="m" coord="10,419.75,330.85,86.24,10.91;10,112.66,344.40,393.33,10.91;10,112.66,357.95,47.51,10.91">Proceedings of the 3rd Clinical Natural Language Processing Workshop, Association for Computational Linguistics</title>
		<meeting>the 3rd Clinical Natural Language Processing Workshop, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,385.05,394.53,10.91;10,112.28,398.60,395.00,10.91;10,112.66,412.15,394.62,10.91;10,112.66,425.70,314.10,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">P</forename><surname>Carrino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Armengol-EstapÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>GutiÃ©rrez-FandiÃ±o</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Llop-Palao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>PÃ mies</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2109.03570</idno>
		<ptr target="https://arxiv.org/abs/2109.03570.doi:10.48550/ARXIV.2109.03570" />
		<title level="m" coord="10,263.37,398.60,243.90,10.91;10,112.66,412.15,337.42,10.91">Biomedical and clinical language models for spanish: On the benefits of domain-specific pretraining in a mid-resource scenario</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,439.25,393.33,10.91;10,112.14,452.79,364.13,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,226.37,439.25,231.19,10.91">Text chunking using transformation-based learning</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Marcus</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/W95-0107" />
	</analytic>
	<monogr>
		<title level="m" coord="10,480.76,439.25,25.23,10.91;10,112.14,452.79,148.71,10.91">Third Workshop on Very Large Corpora</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,466.34,394.53,10.91;10,112.30,479.89,394.97,10.91;10,112.31,493.44,288.85,10.91" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="10,170.33,479.89,252.97,10.91">Roberta: A robustly optimized bert pretraining approach</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1907.11692</idno>
		<ptr target="https://arxiv.org/abs/1907.11692.doi:10.48550/ARXIV.1907.11692" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,506.99,395.01,10.91;10,112.66,520.54,394.52,10.91;10,112.66,534.09,393.32,10.91;10,112.66,547.64,395.01,10.91;10,112.66,561.19,197.80,10.91" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="10,262.65,534.09,243.33,10.91;10,112.66,547.64,216.50,10.91">Multi-domain clinical natural language processing with medcat: the medical concept annotation toolkit</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Kraljevic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Searle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Roguski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Noor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mascio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Folarin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bendayan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">D</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">T</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Dobson</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2010.01165</idno>
		<ptr target="https://arxiv.org/abs/2010.01165.doi:10.48550/ARXIV.2010.01165" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,574.74,394.53,10.91;10,112.66,588.29,394.53,10.91;10,112.66,601.84,395.17,10.91;10,112.66,615.39,393.33,10.91;10,112.66,628.93,393.33,10.91;10,112.66,642.48,395.00,10.91;10,112.66,656.03,197.48,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,311.38,601.84,196.45,10.91;10,112.66,615.39,77.28,10.91">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Le</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-demos.6.doi:10.18653/v1/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m" coord="10,219.91,615.39,286.07,10.91;10,112.66,628.93,393.33,10.91;10,112.66,642.48,47.14,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
