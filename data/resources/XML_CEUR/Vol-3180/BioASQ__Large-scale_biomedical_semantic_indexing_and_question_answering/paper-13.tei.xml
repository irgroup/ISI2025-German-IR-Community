<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,375.11,15.42;1,89.29,106.66,354.55,15.42;1,89.29,128.58,194.76,15.43">Exploring Biomedical Question Answering with BioM-Transformers At BioASQ10B challenge: Findings and Techniques</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,89.29,156.89,74.34,11.96"><forename type="first">Sultan</forename><surname>Alrowili</surname></persName>
							<email>alrowili@udel.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>Delaware</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,176.27,156.89,83.27,11.96"><forename type="first">K</forename><surname>Vijay-Shanker</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<settlement>Newark</settlement>
									<region>Delaware</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,375.11,15.42;1,89.29,106.66,354.55,15.42;1,89.29,128.58,194.76,15.43">Exploring Biomedical Question Answering with BioM-Transformers At BioASQ10B challenge: Findings and Techniques</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">5897ABC282B6BA9B9A979FAD256B4294</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>BERT</term>
					<term>ELECTRA</term>
					<term>ALBERT</term>
					<term>BioASQ</term>
					<term>BioM-Transformers SQuAD1.1 Format BioASQ List</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper details the methods and techniques we used at the BioASQ10B challenge with our BioM-Transformers models. As of last year, we continue to use our BioM-Transformers: an adaptation of both ELECTRA and ALBERT models to the biomedical domain. However, this year, we extend the investigation of our biomedical Questing Answering models with BioM-Transformers models by extending our grid search for hyper-parameters and addressing the limited size of the BioASQ10B-Factoid training set by merging it with the List training set. Additionally, in Transformers-based models (e.g., ELECTRA, ALBERT), task-specific (e.g., question answering) layers are randomly initialized for every new run causing the performance to fluctuate on downstream tasks ( e,g SQuAD, BioASQ). We study the range of this randomness at the BioASQ10B challenge by running two identical models with the same hyperparameters. Our results show that tuning our hyper-parameters led to significant performance gain (e.g., 20% lead in list questions and 100% accuracy on several Yes/No batches). Moreover, our approach to merge both BioASQ10B ( Factoid / List ) training set show better performance than our model, which was fine-tuned only on the Factoid training set. Finally, our results also show that the randomness caused by task-specific wights initializations causes a significant performance variance, especially in small datasets such as BioASQ.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The introduction of the BioBERT model <ref type="bibr" coords="1,260.74,477.99,11.28,10.91" target="#b0">[1]</ref>, introduces the idea of domain adaptation of the BERT model <ref type="bibr" coords="1,119.72,491.54,12.91,10.91" target="#b1">[2]</ref> to the biomedical domain. This adaptation of BERT shows significant performance gains on downstream tasks such as Question Answering, Text Classification, and Named Entity Recognition (NER). BioBERT model has been widely used by the majority of the teams on both BioASQ7B <ref type="bibr" coords="1,163.33,532.18,11.58,10.91" target="#b2">[3]</ref>, and BioASQ8B <ref type="bibr" coords="1,251.90,532.18,13.00,10.91" target="#b3">[4]</ref> challenges. However at BioASQ9B challenge <ref type="bibr" coords="1,474.31,532.18,11.58,10.91" target="#b4">[5]</ref>, we have witnessed using variety of State-of-The-Art models including RoBERTa <ref type="bibr" coords="1,426.98,545.73,11.30,10.91" target="#b5">[6]</ref>, ELECTRA <ref type="bibr" coords="1,492.13,545.73,11.29,10.91" target="#b6">[7]</ref>, XLNET <ref type="bibr" coords="1,125.61,559.28,11.59,10.91" target="#b7">[8]</ref>, and ALBERT <ref type="bibr" coords="1,205.88,559.28,11.58,10.91" target="#b8">[9]</ref>. We also had, at the BioASQ9B challenge, introduced our large biomedical question answering models <ref type="bibr" coords="1,268.66,572.83,18.07,10.91" target="#b9">[10]</ref> , which we built based on both BioM-ELECTRA and BioM-ALBERT models <ref type="bibr" coords="1,215.99,586.38,16.41,10.91" target="#b10">[11]</ref>. However, our focus last year was more on evaluating the reproducibility of our BioM-Transformers model by using the exact hyperparameters settings that we chose in our early work <ref type="bibr" coords="1,234.08,613.48,16.25,10.91" target="#b10">[11]</ref>, that we published prior to BioASQ9B challenge. This year at the BioASQ10B challenge, however, we extended our investigation scope by increasing our hyperparameters grid search to explore the potential of our BioM-Transformers model. In addition, we address the issue of the limited size of the BioASQ10B-Factoid dataset by combining both Factoid and List training datasets. Finally, several studies <ref type="bibr" coords="2,429.95,127.61,17.86,10.91" target="#b11">[12]</ref> indicate that the random initialization of task-specific layers' weights inside Transformer models causes a fluctuation in performance between each run. In this paper, we study the range of this randomness with our Models on BioASQ10 challenges.</p><p>We can summarize the main findings of our investigations in the following points:</p><p>• We show that improving the hyperparameters choices of our BioM-Transformers models led to a significant performance improvement, especially on both list and yes/no questions. • We introduce a new approach that merges both Factoid and List training datasets, which resulted in us taking the lead on two batches of the BioASQ10B Factoid task. • Adapting Text Classification task for BioASQ Yes/No question led us to take the lead in batches 2, 3, and 4 and score second in both the first and last batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Relate Work</head><p>The adaption of BERT to the biomedical domain with BioBERT <ref type="bibr" coords="2,365.32,336.40,12.69,10.91" target="#b0">[1]</ref> has demonstrated significant success in addressing the performance of BERT on biomedical Question Answering tasks. Consequently, new state-of-the-art Transformers model have followed similar approach to BioBERT model including PubMedBERT base <ref type="bibr" coords="2,293.37,377.04,16.41,10.91" target="#b11">[12]</ref>, PubMedBERT large <ref type="bibr" coords="2,401.19,377.04,16.41,10.91" target="#b12">[13]</ref>, BioRoBERTa <ref type="bibr" coords="2,486.66,377.04,16.42,10.91" target="#b13">[14]</ref>, BioMegaTron <ref type="bibr" coords="2,153.03,390.59,16.25,10.91" target="#b14">[15]</ref>, and recently BioLinkBERT <ref type="bibr" coords="2,297.49,390.59,16.25,10.91" target="#b15">[16]</ref>.</p><p>PubMedBERT is a new model introduced by Microsoft which pretrained BERT on both PubMed abstracts and PMC full articles. However, it differs from BioBERT that it uses a large batch size <ref type="bibr" coords="2,135.49,431.24,11.76,10.91" target="#b7">(8,</ref><ref type="bibr" coords="2,147.26,431.24,15.69,10.91">192)</ref>. Using a large batch size has shown effectiveness in improving the Language Model's perplexity, and performance on downstream tasks <ref type="bibr" coords="2,344.76,444.79,12.68,10.91" target="#b5">[6]</ref>  <ref type="bibr" coords="2,359.70,444.79,16.09,10.91" target="#b16">[17]</ref>. Moreover, the PubMedBERT team recently introduced a large-scale variant of PubMedBERT <ref type="bibr" coords="2,368.70,458.34,16.11,10.91" target="#b12">[13]</ref>, which improves the result on downstream biomedical tasks, including BioASQ7B Yes/No task. They have also introduced in the same paper both PubMedELECTRA base and PubMedELECTRA large , which follow similar design factors of PubMedBERT but replacing the Masked Language Model MLM (BERT) objective with ELECTRA objective.</p><p>Additionally, more large-scale biomedical language models have been introduced in the last two years, including BioMegaTron, BioRoBERTA, and recently BioLinkBERT. Both BioMegaTron and BioRoBERTA follow a similar approach by studying the impact of design factors (e.g., corpora and vocabulary domain, batch size, training steps) on improving the performance on downstream biomedical tasks, including the BioASQ challenge. On the other hand, BioLinkBERT is a new biomedical language model that adds an additional pre-training objective to the Masked Language Model (MLM) objective of BERT. This new training objective is called Document Relation Prediction (DPR), and it aims to capture dependencies from citation links inside PubMed corpora, which they added to the pre-train corpora. By adopting this approach, BioLinkBERT shows better results on downstream biomedical tasks than the PubMedBERT model, including BioASQ7B Yes/No task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">BioM-Transformers</head><p>Similar to what we did last year at the BioASQ9B challenge, we continue to use BioM-Transformers <ref type="bibr" coords="3,149.88,145.80,17.75,10.91" target="#b10">[11]</ref> models at the BioASQ10B challenge. BioM-Transformers are large biomedical language models, which we introduced last year, pre-trained on biomedical corpora (PubMed Abstracts) and use specific domain vocabulary (PubMed Abstracts). We use both BioM-ELECTRA and BioM-ALBERT.</p><p>BioM-ELECTRA is a large biomedical language model that uses ELECTRA loss function <ref type="bibr" coords="3,493.16,199.99,12.83,10.91" target="#b6">[7]</ref> instead of Masking Language Model MLM. We pre-train BioM-ELECTRA for 434K steps with a batch size of 4096 on TPU3-512 units. BioM-ELECTRA model uses the same vocabulary as the PubMedBERT <ref type="bibr" coords="3,172.80,240.64,16.41,10.91" target="#b11">[12]</ref>. In contrast to the BERT loss function, which uses Masked Language Model MLM, ELECTRA uses generative and discriminative loss functions. Figure <ref type="figure" coords="3,454.10,254.19,5.10,10.91" target="#fig_0">1</ref> illustrates the idea of the ELECTRA function. The generator inside ELECTRA is a small Masked Language Model MLM aiming to generate fake tokens that could fit the context around [MASK] token. On the other hand, the discriminator is a model that aims to judge whether the generated tokens are original (real) or replaced. Both the generator and the discriminator are pre-trained jointly inside ELECTRA, and both are improving simultaneously in a way described in the Game Theory field as a "Cat and Mouse" game.</p><p>On the other hand, our BioM-ALBERT model is a large model based on ALBERTxxlarge architecture <ref type="bibr" coords="3,144.89,362.58,11.32,10.91" target="#b8">[9]</ref>, which has a hidden size of 4096 compared to BioM-ELECTRA, which has only 1024 hidden size. Although ALBERT still uses the traditional Masked Language Model MLM, it incorporates several techniques that decrease the pre-training cost and support the model's scalability. Those techniques include parameter-sharing, large batch size optimizer technique LAMB <ref type="bibr" coords="3,120.93,416.78,16.25,10.91" target="#b16">[17]</ref>, and factorization of vocabulary embedding matrix.</p><p>The parameter-sharing technique allows the ALBERT model to address the parameters redundancy issue inside the Transformers. LAMB optimizer allows ALBERT to be pre-trained with large batch size, 8192, compared to 256 in the case of the BERT model <ref type="bibr" coords="3,413.30,457.43,11.28,10.91" target="#b1">[2]</ref>. The factorization of the vocabulary embedding matrix technique allows ALBERT-xxlarge to control the size of the parameters (235M), despite having a larger hidden size (4096). We pre-train our BioM-ALBERT model for 264K steps and a batch size of 8192 on TPUv3-512 units. Like BioM-ELECTRA, BioM-ALBERT pre-trained on collecting 27GB of PubMed Abstracts and uses a specific domain vocabulary that has a size of 30K tokens. We build our BioM-ALBERT's vocabulary by training the SentencePeice model on PubMed Abstracts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Yes/No Question as a Text Classification Problem</head><p>We continue to adopt a binary classification approach to address Yes/No questions, which is the same approach we did last year at the BioASQ9B challenge. Thus, we use a snippet (context) as "sentence 1", questions as "sentence 2" and the answer (yes/no) as our "label. " We use a preprocessing script by the PubMedBERT team <ref type="bibr" coords="3,286.45,629.10,17.87,10.91" target="#b11">[12]</ref> to generate the BioASQ classification dataset. We optimized our best hyperparameters for the Yes/No task using the training and testing set of BioASQ9B with Huggingface Transformers <ref type="bibr" coords="3,295.83,656.20,17.91,10.91" target="#b17">[18]</ref> implementation of text classification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Combining Factoid and List Question</head><p>Most of the participants' teams at the previous BioASQ challenges BioASQ9B <ref type="bibr" coords="4,447.08,328.88,13.00,10.91" target="#b4">[5]</ref> adapt the SQuAD1.1 <ref type="bibr" coords="4,138.81,342.43,17.96,10.91" target="#b18">[19]</ref> format with BioASQ-Phase-B list questions. Tasks in the format of SQuAD1.1 treat the question answering task as a reading comprehension task where the model will scan through the context (snippet), looking for the start and end indexes of the answer span. Thus, to treat BioASQ list questions as reading comprehension tasks, we need to convert the training set of BioASQ-B list questions into Factoid-SQuAD-style questions. The BioBERT team initially proposed this idea at the BioASQ7B challenge <ref type="bibr" coords="4,295.82,410.17,16.25,10.91" target="#b19">[20]</ref>, and Figure <ref type="figure" coords="4,369.08,410.17,5.07,10.91">2</ref> illustrates this process.</p><p>Since those list-type questions are in a similar format to BioASQ10B-Factoid questions, we hypothesize that combining both the list and factoid training sets could improve the performance of our models on Factoid questions. To test this hypothesis at the BioASQ10B challenge, we combine the training set of the BioASQ10B Factoid/list tasks as one training set. This resulted in a training set that has 18,587 triplets of a question, context, and answer compared to 4,691 Factoid-only questions. We report our findings and analysis of this approach in the results section for all five batches except the second batch. Due to technical and time constraint issues, we did not use this approach in the second batch of the BioASQ10B-Factoid task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Hyperparamters Fine-Tuning</head><p>Reproducibility is one of the major concerns in the research community, especially with Transformers-based models where hyperparameter settings (e.g., learning rate, training steps) play a significant role in improving the results on downstream tasks. Last year at the BioASQ9B challenge, we decided to use the same hyperparameters setting that we reported in our early work <ref type="bibr" coords="4,114.83,622.49,16.26,10.91" target="#b10">[11]</ref>, which we published prior to the BioASQ9B challenge. Last year, we decided to test how our hyperparameters settings would perform on the new BioASQ test set (BioASQ9B). However, this year at BioASQ10B, we increased our grid search for hyperparameter settings and used BioASQ9B training and test dataset to find the best hyperparameters settings instead of using BioASQ7B as we did last year. Our grid search for hyperparameters this year includes: batch size of <ref type="bibr" coords="5,146.38,382.30,16.44,10.91" target="#b15">[16,</ref><ref type="bibr" coords="5,162.82,382.30,12.33,10.91">24,</ref><ref type="bibr" coords="5,175.15,382.30,12.33,10.91">32,</ref><ref type="bibr" coords="5,187.48,382.30,12.33,10.91">40,</ref><ref type="bibr" coords="5,199.81,382.30,12.33,10.91">48,</ref><ref type="bibr" coords="5,212.14,382.30,12.33,10.91">64,</ref><ref type="bibr" coords="5,224.47,382.30,16.44,10.91">128]</ref>, learning rate of [1e-5,2e-5,3e-5,5e-5] and epochs number of <ref type="bibr" coords="5,89.29,395.85,11.50,10.91" target="#b1">[2,</ref><ref type="bibr" coords="5,100.79,395.85,7.67,10.91" target="#b2">3,</ref><ref type="bibr" coords="5,108.46,395.85,7.67,10.91" target="#b3">4,</ref><ref type="bibr" coords="5,116.13,395.85,7.67,10.91" target="#b4">5]</ref>. To help reproduce our results, we share our best hyper-parameters setting for both BioM-ELECTRA and BioM-ALBERT in the appendix section A. Additionally, we investigate studying the configuration of the post-processing script (trans-form_nbset2bioasqform.py) that we used in list-type questions. As we previously illustrated in Figure <ref type="figure" coords="5,131.98,450.05,3.74,10.91">2</ref>, we convert the list question into SQuAD format, and then we fine-tune our BioM-Transformers models using this SQuAD-style list question. However, following the fine-tuning process, we need to convert the list of SQuAD-like prediction to a list of answers and its common practice at this stage to use a post-processing script developed by BioBERT team<ref type="foot" coords="5,443.43,488.94,3.71,7.97" target="#foot_0">1</ref> to obtain the final predictions for list-type questions. This post-processing script has a threshold parameter that sets the limit at which specific probability an answer can be part of a correct list of answers. We study the impact of this "threshold" value on the BioASQ9B list question's performance by using different threshold values ranging from [0.1-0.4]. We use the training and test dataset of BioASQ9B-list questions to optimize this value, and we use the best threshold we find to predict our answers on the BioASQ10B challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Randomness of Transformers-based Models</head><p>In Transformers-based models (e.g., BERT, ELECTRA, ALBERT), the initial weights of taskspecific layers (e.g., question answering, text classification layers) are randomly assigned before the fine-tuning. This randomness will cause the performance of Transformers-based models to fluctuate on downstream tasks, especially in smaller datasets such as BioASQ <ref type="bibr" coords="6,429.14,100.52,16.09,10.91" target="#b11">[12]</ref>. We examine the range of this randomness at the BioASQ10B challenge by using two identical BioM-ALBERT models (UDEL-LAB1, UDEL-LAB2), where both use the same hyper-parameters and fine-tuning dataset. In the result section, we report our observation and the effect of weights randomness on both Factoid and List questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Task-To-Task Transferability</head><p>The transferability between SQuAD and The Multi-Genre Natural Language Inference MNLI <ref type="bibr" coords="6,89.29,217.99,18.07,10.91" target="#b20">[21]</ref> dataset at the BioASQ challenge was first observed by the BioBERT team in their early work <ref type="bibr" coords="6,114.59,231.54,16.17,10.91" target="#b21">[22]</ref>. MNLI is a text classification task that is part of the GLUE benchmark. This year, we followed a similar approach by testing the transferability effect on the performance with our BioM-ELECTRA models. At the BioASQ10B challenge, we use BioM-ELECTRA-SQuAD (UDEL-LAB3) and BioM-ELECTRA-SQuAD-MNLI (UDEL-LAB4) models to test this transferability effect. For the BioM-ELECTRA-SQuAD model, we first fine-tune our BioM-ELECTRA model on the SQuAD2.0 dataset and then on the training set of the BioASQ10B. For our BioM-ELECTRA-SQuAD-MNLI model, we fine-tune our BioM-ELECTRA model on the MNLI dataset first, then on the SQuAD2.0 set, and finally on the training set of the BioASQ10B dataset. We report our findings of this approach for both BioASQ10B-Factoid and List questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Discussion</head><p>In this section, we reported the results of our models at the BioASQ10B challenge. We split our results and discussion based on Yes/No, Factoid, and List questions. Results are taken from the official leaderboard of the BioASQ10B challenge<ref type="foot" coords="6,321.90,423.90,3.71,7.97" target="#foot_1">2</ref> . For both Factoid and List questions, we analyze the effect of Transferability (MNLI-to-SQuAD) with both our models BioM-ELECTRA-SQuAD (UDEL-LAB3) and BioM-ELECTRA-SQuAD-MNLI (UDEL-LAB4). For both Factoid and List tasks, we reported our findings of the randomness of the results by running two BioM-ALBERT models (UDEL-LAB1, UDEL-LAB2) where both use the same hyper-parameters and fine-tuning setting. Results are sorted using Accuracy, mean reciprocal rank (MRR), and mean F-Measure as main ranks for Yes/No, Factoid and List questions, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Yes/No Questions</head><p>Table <ref type="table" coords="6,115.96,556.68,5.11,10.91" target="#tab_0">1</ref> shows the result of our models against other participants' teams on Yes/No questions. Results show that adapting the text-classification approach with both BioM-ELECTRA and BioM-ALBERT shows effectiveness in addressing the performance on Yes/No questions. This approach leads us to take the lead in Batch 2,3, and 4. In batch 3, the result shows that we took the lead against other systems by scoring 100% accuracy with both BioM-ELECTRA and BioM-ALBERT. Also, we can observe the consistency in results between BioM-ELECTRA and BioM-ALBERT in all batches except batch 4. This consistency also highlights that randomness is less likely to occur with the binary dataset (Yes/No). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Factoid Questions</head><p>Table <ref type="table" coords="7,115.79,529.97,5.07,10.91" target="#tab_1">2</ref> shows the results of our models on BioASQ10B-Factoid questions ranked by the mean reciprocal rank (MRR). Results illustrate that combining both Factoid and List questions led us to take the lead in both batch 1 and batch 3. Taking the lead in two batches, not only one, with this method indicates that randomness was not the reason for this performance but our technique to combine both Factoid and List questions. Moreover, we can observe significant randomness (3%) in MRR performance with both BioM-ALBERT-SQuAD-Run2 and BioM-ALBERT-SQuAD-Run1 in the first and third batches. Similar to last year's BioASQ9B result, Task-to-Task transferability did not always lead to better performance than the BioM-ELECTRA-SQuAD model. Furthermore, We can observe that our BioM-ELECTRA models outperform our BioM-ALBERT on all four batches, suggesting that it is better to use BioM-ELECTRA models for BioASQ factoid questions. Indeed BioM-ELECTRA models also have less hidden layer size (1024) than BioM-ALBERT models (4096), and this leads to better inference and fine-tuning time (0.33x), as we show in our early work <ref type="bibr" coords="8,438.00,548.91,16.25,10.91" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">List Questions</head><p>The threshold value, part of the "nbset2bioasqform" script, plays a significant role in choosing which answers can be considered candidate answers for list-type questions. We optimized the threshold value using the training and test dataset of the BioASQ9B challenge. Figure <ref type="figure" coords="8,470.39,625.73,5.05,10.91" target="#fig_2">3</ref> shows how our BioM-ALBERT performs in terms of F-Measure score on last year's BioASQ9B-List task with different threshold values. Results in figure <ref type="figure" coords="8,298.93,652.83,4.97,10.91" target="#fig_2">3</ref> show that we gain %4 (0.68-0.64) improvement in the F-measure score at the 0.18 threshold compared to the default value of the threshold set by the BioBERT team (0.42). Based on these results, we chose our threshold value for this year's challenge to be 0.18. We believe that this choice alone explains the significant margin we have in terms of performance against other models at the BioASQ10B-list task this year. Table <ref type="table" coords="9,397.34,419.67,4.97,10.91" target="#tab_2">3</ref> shows the performance of our models on the BioASQ10B-list task. Our models score among the top best-performing models in all five batches of the BioASQ10B-list task. In contrast to the Factoid task, BioM-ALBERT performs better than BioM-ELECTRA on list questions. In addition, the performance of two runs of BioM-ALBERT models shows a larger range of randomness (1-7%) in performance than the Factoid task (0-%3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper illustrates and explains the methods and models we use at the BioASQ10B challenge. We show that by adapting the Text Classification approach for Yes/No questions and tuning our hyper-parameters, we reach 100% accuracy in batches 2,3, and 4 and score second with 95.6% accuracy on the First and last Batch. In addition, our new technique to combine both Factoid and List questions contributes to the lead we achieve in two batches of BioASQ10Bfactoid questions. Finally, we show how tuning threshold probability from 0.42 to 0.18 led to a significant performance gain on the BioASQ10B-list task, leading us to rank first in all five batches of the BioASQ10B-list task. For our future work, we will focus on addressing the limited size of the BioASQ dataset through data augmentation and investigate building an ensemble model with both our models BioM-ELECTRA and BioM-ALBERT. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,258.75,345.26,8.93"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of ELECTRA Function. Figure adapted from ELECTRA paper<ref type="bibr" coords="4,422.73,258.80,11.83,8.87" target="#b6">[7]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,89.29,307.24,416.70,8.93;5,89.29,319.24,186.35,8.87"><head>Question: 1 : 2 : 3 :Figure 2 :</head><label>1232</label><figDesc>Figure 2: Overview of the idea of converting BioASQBList questions to Factoid-style question as proposed by BioBERT Team at BioASQ7B [3].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,89.29,328.65,416.69,8.93;9,89.06,340.66,416.92,8.87;9,89.29,352.61,416.70,8.87;9,89.29,364.57,191.84,8.87;9,144.64,123.23,306.00,195.24"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance of our BioM-ALBERT on BioASQ9B-List questions with different threshold values. The default threshold value assigned by BioBERT team is 0.42. We take the average F-Measure score of five different batches of BioASQ9B-List task. We use last year BioASQ9B-List since we have access to both the training and golden dataset.</figDesc><graphic coords="9,144.64,123.23,306.00,195.24" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,88.99,129.46,325.59,345.52"><head>Table 1</head><label>1</label><figDesc>Results of our Models on BioASQ10B-YesNo task.</figDesc><table coords="7,180.69,160.11,233.89,314.87"><row><cell>Batch</cell><cell>Model</cell><cell cols="2">Accuracy Macro F1</cell></row><row><cell></cell><cell>bio-answerfinder</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell></cell><cell>bio-answerfinder-2</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell cols="2">Batch1 LaRSA</cell><cell>0.9565</cell><cell>0.9464</cell></row><row><cell></cell><cell>BioM-ELECTRA</cell><cell>0.9565</cell><cell>0.9403</cell></row><row><cell></cell><cell>BioM-ALBERT</cell><cell>0.9565</cell><cell>0.9403</cell></row><row><cell></cell><cell>orpheus_kg</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell></cell><cell>BioM-ALBERT</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell cols="2">Batch2 BioM-ELECTRA</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell></cell><cell>Ir_sys1</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell></cell><cell>bio-answerfinder</cell><cell>0.9444</cell><cell>0.9345</cell></row><row><cell></cell><cell>BioM-ALBERT</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell></cell><cell>BioM-ELECTRA</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell cols="2">Batch3 KU-AAA637-system2</cell><cell>0.9600</cell><cell>0.9524</cell></row><row><cell></cell><cell>KU-AAA637-system3</cell><cell>0.9600</cell><cell>0.9524</cell></row><row><cell></cell><cell>Ir_sys1</cell><cell>0.9600</cell><cell>0.9524</cell></row><row><cell></cell><cell>BioM-ELECTRA</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell></cell><cell>Ir_sys1</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell cols="2">Batch4 Ir_sys2</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell></cell><cell>lalala</cell><cell>1.0000</cell><cell>1.0000</cell></row><row><cell></cell><cell>bio-answerfinder</cell><cell>0.9583</cell><cell>0.9473</cell></row><row><cell></cell><cell>KU-AAA637-system2</cell><cell>0.9286</cell><cell>0.9271</cell></row><row><cell></cell><cell>Ir_sys1</cell><cell>0.9286</cell><cell>0.9282</cell></row><row><cell cols="2">Batch5 Ir_sys2</cell><cell>0.9286</cell><cell>0.9282</cell></row><row><cell></cell><cell>lalala</cell><cell>0.9286</cell><cell>0.9282</cell></row><row><cell></cell><cell>BioM-ALBERT</cell><cell>0.8929</cell><cell>0.8893</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,88.99,90.49,417.00,418.16"><head>Table 2</head><label>2</label><figDesc>Results of our Models at BioASQ10B-Factoid task. Results show the top-5 best performing models from the BioASQ10B leaderboard. We extend the results in both Batch1 and Batch3 to show the effect of randomness by showing the performance of different runs of BioM-ALBERT.</figDesc><table coords="8,107.16,145.96,380.95,362.69"><row><cell>Batch</cell><cell>Model</cell><cell cols="3">Strict Acc. Lenient Acc. MRR</cell></row><row><cell></cell><cell>BioM-ELECTRA-Factoid+List [UDEL-LAB5]</cell><cell>0.3824</cell><cell>0.5588</cell><cell>0.4608</cell></row><row><cell></cell><cell>Ir_sys1</cell><cell>0.4118</cell><cell>0.5000</cell><cell>0.4559</cell></row><row><cell cols="2">Batch1 BioM-ALBERT-SQuAD-Run2 [UDEL-LAB2]</cell><cell>0.3824</cell><cell>0.5588</cell><cell>0.4534</cell></row><row><cell></cell><cell>Ir_sys3</cell><cell>0.4118</cell><cell>0.5000</cell><cell>0.4485</cell></row><row><cell></cell><cell>lalala</cell><cell>0.3824</cell><cell>0.5000</cell><cell>0.4363</cell></row><row><cell></cell><cell>BioM-ALBERT-SQuAD-Run1 [UDEL-LAB1]</cell><cell>0.3529</cell><cell>0.5588</cell><cell>0.4299</cell></row><row><cell></cell><cell>BioM-ELECTRA-SQuAD [UDEL-LAB3]</cell><cell>0.5588</cell><cell>0.6765</cell><cell>0.6000</cell></row><row><cell></cell><cell>BioM-ELECTRA-SQuAD-MNLI [UDEL-LAB4]</cell><cell>0.5588</cell><cell>0.6765</cell><cell>0.5912</cell></row><row><cell cols="2">Batch2 BioM-ALBERT-SQuAD-Run2 [UDEL-LAB2]</cell><cell>0.5588</cell><cell>0.6176</cell><cell>0.5882</cell></row><row><cell></cell><cell>BioM-ALBERT-SQuAD-Run1 [UDEL-LAB1]</cell><cell>0.5588</cell><cell>0.6176</cell><cell>0.5809</cell></row><row><cell></cell><cell>Ir_sys3</cell><cell>0.5588</cell><cell>0.6176</cell><cell>0.5809</cell></row><row><cell></cell><cell>BioM-ELECTRA-Factoid+List [UDEL-LAB5]</cell><cell>0.5313</cell><cell>0.6563</cell><cell>0.5792</cell></row><row><cell></cell><cell>KU-AAA637-system4</cell><cell>0.5000</cell><cell>0.6875</cell><cell>0.5755</cell></row><row><cell cols="2">Batch3 BioM-ALBERT-SQuAD-Run1 [UDEL-LAB1]</cell><cell>0.5313</cell><cell>0.6250</cell><cell>0.5729</cell></row><row><cell></cell><cell>LaRSA</cell><cell>0.5000</cell><cell>0.6563</cell><cell>0.5677</cell></row><row><cell></cell><cell>KU-AAA637-system2</cell><cell>0.5000</cell><cell>0.6875</cell><cell>0.5661</cell></row><row><cell></cell><cell>KU-AAA637-system3</cell><cell>0.5000</cell><cell>0.6875</cell><cell>0.5651</cell></row><row><cell></cell><cell>KU-AAA637-system1</cell><cell>0.4688</cell><cell>0.6875</cell><cell>0.5505</cell></row><row><cell></cell><cell>BioM-ALBERT-SQuAD-Run2 [UDEL-LAB2]</cell><cell>0.4688</cell><cell>0.6250</cell><cell>0.5417</cell></row><row><cell></cell><cell>lalala</cell><cell>0.5806</cell><cell>0.6452</cell><cell>0.5995</cell></row><row><cell></cell><cell>Ir_sys3</cell><cell>0.5161</cell><cell>0.6774</cell><cell>0.5806</cell></row><row><cell cols="2">Batch4 KU-AAA637-system4</cell><cell>0.5161</cell><cell>0.6452</cell><cell>0.5656</cell></row><row><cell></cell><cell>BioASQ-2022_UNCC</cell><cell>0.5161</cell><cell>0.6129</cell><cell>0.5645</cell></row><row><cell></cell><cell>BioM-ELECTRA-SQuAD-MNLI [UDEL-LAB4]</cell><cell>0.5484</cell><cell>0.6129</cell><cell>0.5613</cell></row><row><cell></cell><cell>Ir_sys3</cell><cell>0.4828</cell><cell>0.5862</cell><cell>0.5098</cell></row><row><cell></cell><cell>NCU-IISR-AS-GIS-4</cell><cell>0.4483</cell><cell>0.5862</cell><cell>0.4983</cell></row><row><cell cols="2">Batch5 NCU-IISR-AS-GIS-5</cell><cell>0.4483</cell><cell>0.5862</cell><cell>0.4983</cell></row><row><cell></cell><cell>BioM-ELECTRA-SQuAD [UDEL-LAB3]</cell><cell>0.4138</cell><cell>0.5862</cell><cell>0.4828</cell></row><row><cell></cell><cell>BioASQ-2022_UNCC1</cell><cell>0.4138</cell><cell>0.6207</cell><cell>0.4764</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,88.99,90.49,418.65,356.17"><head>Table 3</head><label>3</label><figDesc>Results of our Models at BioASQ10B-List task. We only show top-5 best performing systems on F-Measure score.</figDesc><table coords="10,136.14,131.79,322.99,314.87"><row><cell>Batch</cell><cell>Model</cell><cell cols="3">Mean Prec. Recall F-Measure</cell></row><row><cell></cell><cell>BioM-ALBERT-Run2</cell><cell>0.7201</cell><cell>0.8405</cell><cell>0.7469</cell></row><row><cell></cell><cell>BioM-ALBERT-Run1</cell><cell>0.6974</cell><cell>0.8226</cell><cell>0.7346</cell></row><row><cell cols="2">Batch1 BioM-ELECTRA-SQuAD-MNLI</cell><cell>0.6762</cell><cell>0.8464</cell><cell>0.7229</cell></row><row><cell></cell><cell>BioM-ELECTRA-SQuAD</cell><cell>0.6893</cell><cell>0.7429</cell><cell>0.6731</cell></row><row><cell></cell><cell>lalala</cell><cell>0.6046</cell><cell>0.7286</cell><cell>0.6459</cell></row><row><cell></cell><cell>BioM-ELECTRA-SQuAD</cell><cell>0.7042</cell><cell>0.7400</cell><cell>0.7051</cell></row><row><cell></cell><cell>BioM-ELECTRA-SQuAD-MNLI</cell><cell>0.6859</cell><cell>0.7530</cell><cell>0.7011</cell></row><row><cell cols="2">Batch2 BioM-ALBERT-Run2</cell><cell>0.6914</cell><cell>0.7193</cell><cell>0.6787</cell></row><row><cell></cell><cell>BioM-ALBERT-Run1</cell><cell>0.6707</cell><cell>0.6530</cell><cell>0.6393</cell></row><row><cell></cell><cell>lalala</cell><cell>0.4955</cell><cell>0.6067</cell><cell>0.5177</cell></row><row><cell></cell><cell>BioM-ALBERT-Run2</cell><cell>0.5442</cell><cell>0.6742</cell><cell>0.5655</cell></row><row><cell></cell><cell>BioM-ALBERT-Run1</cell><cell>0.5174</cell><cell>0.6591</cell><cell>0.5558</cell></row><row><cell cols="2">Batch3 BioM-ELECTRA-SQuAD-MNLI</cell><cell>0.5293</cell><cell>0.6439</cell><cell>0.5255</cell></row><row><cell></cell><cell>BioM-ALBERT-Run1</cell><cell>0.5263</cell><cell>0.5985</cell><cell>0.5188</cell></row><row><cell></cell><cell>bio-answerfinder</cell><cell>0.6273</cell><cell>0.4472</cell><cell>0.4843</cell></row><row><cell></cell><cell>BioM-ALBERT-Run2</cell><cell>0.5834</cell><cell>0.5844</cell><cell>0.5386</cell></row><row><cell></cell><cell>BioM-ALBERT-Run1</cell><cell>0.5799</cell><cell>0.5017</cell><cell>0.4950</cell></row><row><cell cols="2">Batch4 BioM-ELECTRA-SQuAD-MNLI</cell><cell>0.6162</cell><cell>0.4753</cell><cell>0.4752</cell></row><row><cell></cell><cell>BioM-ELECTRA-SQuAD</cell><cell>0.5584</cell><cell>0.4438</cell><cell>0.4501</cell></row><row><cell></cell><cell>lalala</cell><cell>0.4089</cell><cell>0.4507</cell><cell>0.3835</cell></row><row><cell></cell><cell>BioM-ELECTRA-SQuAD-MNLI</cell><cell>0.6009</cell><cell>0.6313</cell><cell>0.6016</cell></row><row><cell></cell><cell>BioM-ALBERT-Run2</cell><cell>0.5587</cell><cell>0.6297</cell><cell>0.5793</cell></row><row><cell cols="2">Batch5 BioM-ALBERT-Run1</cell><cell>0.6003</cell><cell>0.5795</cell><cell>0.5707</cell></row><row><cell></cell><cell>BioM-ELECTRA-SQuAD</cell><cell>0.5651</cell><cell>0.5929</cell><cell>0.5669</cell></row><row><cell></cell><cell>NCU-IISR-AS-GIS-4</cell><cell>0.6222</cell><cell>0.4975</cell><cell>0.5332</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,108.93,660.07,397.64,8.97;5,89.29,671.03,116.86,8.97"><p>This post-processing script can be obtained at https://github.com/dmis-lab/biobert/blob/master/biocodes/ transform_nbset2bioasqform.py</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,108.93,649.11,397.05,8.97;6,89.29,660.07,416.69,8.97;6,89.29,671.03,341.57,8.97"><p>Our results on this paper are from preliminary results reported on the BioASQ10B leaderboard, which can be accessed at http://participants-area.bioasq.org/results/10b/phaseB/. We only reported the name of other team systems without a description of each since the BioASQ team has not released further details.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors would like to Acknowledge the ultimate support from <rs type="institution">Google Research Cloud TRC</rs> for providing access to Tensor Processing Unit TPU which we use to pre-train our BioM-Transformers and fine-tune our model for Both BioASQ9B and BioASQ10 challenges. The authors also would like to thank <rs type="institution">BioBERT</rs> team for their continuous effort to make their codes ( e.g transform_nbset2bioasqform.py ) available to the public community and share their codes and resources on their GitHub repository.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>Table <ref type="table" coords="13,116.20,111.28,5.16,10.91">4</ref> shows the details of our hyper-parameters that we use for both BioM-ELECTRA and BioM-ALBERT for Yes/No, Factoid, and List questions. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,112.66,624.45,393.33,10.91;10,112.66,638.00,394.62,10.91;10,112.66,651.54,394.96,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,355.95,624.45,150.04,10.91;10,112.66,638.00,261.24,10.91">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz682</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btz682.doi:10.1093/bioinformatics/btz682" />
	</analytic>
	<monogr>
		<title level="j" coord="10,382.77,638.00,66.92,10.91">Bioinformatics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,665.09,393.33,10.91;11,112.66,86.97,393.33,10.91;11,112.66,100.52,393.32,10.91;11,112.66,114.06,393.33,10.91;11,112.66,127.61,394.03,10.91;11,112.66,141.16,234.20,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,323.15,665.09,182.83,10.91;11,112.66,86.97,186.91,10.91">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1423.doi:10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m" coord="11,327.87,86.97,178.11,10.91;11,112.66,100.52,393.32,10.91;11,112.66,114.06,99.97,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="11,112.66,154.71,393.33,10.91;11,112.66,168.26,393.33,10.91;11,112.66,181.81,395.01,10.91;11,112.66,195.36,17.97,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,349.23,154.71,156.76,10.91;11,112.66,168.26,72.96,10.91">Results of the seventh edition of the bioasq challenge</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bougiatiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/2006.09174.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="11,209.43,168.26,296.55,10.91;11,112.66,181.81,103.39,10.91">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,208.91,395.17,10.91;11,112.66,222.46,393.33,10.91;11,112.66,236.01,393.33,10.91;11,112.66,249.56,395.01,10.91;11,112.66,263.11,312.00,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,194.79,222.46,311.20,10.91;11,112.66,236.01,243.74,10.91">Overview of bioasq 2020: The eighth bioasq challenge on large-scale biomedical semantic indexing and question answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bougiatiotis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rodriguez-Penagos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58219-7_16</idno>
		<ptr target="https://link.springer.com/chapter/10.1007/978-3-030-58219-7_16" />
	</analytic>
	<monogr>
		<title level="m" coord="11,380.16,236.01,125.83,10.91;11,112.66,249.56,279.47,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,276.66,394.53,10.91;11,112.66,290.20,393.33,10.91;11,112.66,303.75,395.17,10.91;11,112.66,317.30,395.01,10.91;11,112.66,330.85,187.11,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,112.66,290.20,393.33,10.91;11,112.66,303.75,141.34,10.91">Overview of BioASQ 2021: The ninth BioASQ challenge on large-scale biomedical semantic indexing and question answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Vandorou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gasco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-85251-1_18</idno>
		<ptr target="https://doi.org/10.1007%2F978-3-030-85251-1_18.doi:10.1007/978-3-030-85251-1_18" />
	</analytic>
	<monogr>
		<title level="s" coord="11,275.99,303.75,150.07,10.91">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="239" to="263" />
			<date type="published" when="2021">2021</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,344.40,395.17,10.91;11,112.66,357.95,395.01,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="11,137.85,357.95,241.29,10.91">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,371.50,393.33,10.91;11,112.66,385.05,295.16,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m" coord="11,334.34,371.50,171.65,10.91;11,112.66,385.05,165.13,10.91">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,398.60,393.33,10.91;11,112.66,412.15,394.53,10.91;11,112.28,425.70,393.71,10.91;11,112.66,439.25,395.01,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,420.76,398.60,85.22,10.91;11,112.66,412.15,247.89,10.91">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://proceedings" />
	</analytic>
	<monogr>
		<title level="s" coord="11,363.07,425.70,142.92,10.91;11,112.66,439.25,85.70,10.91">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,466.34,393.53,10.91;11,112.66,479.89,361.04,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<title level="m" coord="11,407.44,466.34,98.74,10.91;11,112.66,479.89,231.08,10.91">Albert: A lite bert for self-supervised learning of language representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,493.44,393.33,10.91;11,112.66,506.99,321.09,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,231.75,493.44,274.23,10.91;11,112.66,506.99,28.24,10.91">Large biomedical question answering models with albert and electra</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Alrowili</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">K</forename><surname>Shanker</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936//paper-14.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="11,163.48,506.99,21.05,10.91">CLEF</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,520.54,393.32,10.91;11,112.26,534.09,393.73,10.91;11,112.66,547.64,394.52,10.91;11,112.66,561.19,397.48,10.91;11,112.66,577.18,68.18,7.90" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,219.86,520.54,286.12,10.91;11,112.26,534.09,153.47,10.91">BioM-transformers: Building large biomedical language models with BERT, ALBERT and ELECTRA</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Alrowili</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Shanker</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.bionlp-1.24</idno>
		<ptr target="https://www.aclweb.org/anthology/2021.bionlp-1.24.doi:10.18653/v1/2021.bionlp-1.24" />
	</analytic>
	<monogr>
		<title level="m" coord="11,289.99,534.09,216.00,10.91;11,112.66,547.64,288.32,10.91">Proceedings of the 20th Workshop on Biomedical Language Processing, Association for Computational Linguistics</title>
		<meeting>the 20th Workshop on Biomedical Language Processing, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="221" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,588.29,394.53,10.91;11,112.66,601.84,394.53,10.91;11,112.66,615.39,122.77,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="11,112.66,601.84,390.01,10.91">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.15779</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,628.93,393.32,10.91;11,112.66,642.48,394.62,10.91;11,112.66,656.03,314.10,10.91" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="11,453.42,628.93,52.57,10.91;11,112.66,642.48,336.99,10.91">Fine-tuning large neural language models for biomedical natural language processing</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2112.07869</idno>
		<ptr target="https://arxiv.org/abs/2112.07869.doi:10.48550/ARXIV.2112.07869" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,669.58,393.33,10.91;12,112.66,86.97,393.33,10.91;12,112.66,100.52,393.33,10.91;12,112.66,114.06,395.01,10.91;12,112.66,127.61,288.58,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,287.48,669.58,218.51,10.91;12,112.66,86.97,286.01,10.91">Pretrained language models for biomedical and clinical tasks: Understanding and extending the state-of-the-art</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.clinicalnlp-1.17</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.clinicalnlp-1.17.doi:10.18653/v1/2020.clinicalnlp-1.17" />
	</analytic>
	<monogr>
		<title level="m" coord="12,421.76,86.97,84.22,10.91;12,112.66,100.52,393.33,10.91;12,112.66,114.06,47.51,10.91">Proceedings of the 3rd Clinical Natural Language Processing Workshop, Association for Computational Linguistics</title>
		<meeting>the 3rd Clinical Natural Language Processing Workshop, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,141.16,394.61,10.91;12,112.66,154.71,393.33,10.91;12,112.66,168.26,395.17,10.91;12,112.66,181.81,394.03,10.91;12,112.66,195.36,303.46,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,447.74,141.16,59.53,10.91;12,112.66,154.71,191.28,10.91">BioMegatron: Larger biomedical domain language model</title>
		<author>
			<persName coords=""><forename type="first">H.-C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Bakhturina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mani</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.379</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-main.379.doi:10.18653/v1/2020.emnlp-main.379" />
	</analytic>
	<monogr>
		<title level="m" coord="12,328.58,154.71,177.40,10.91;12,112.66,168.26,271.20,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4700" to="4706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,208.91,395.17,10.91;12,112.66,222.46,395.17,10.91;12,112.66,236.01,394.53,10.91;12,112.66,249.56,395.01,10.91;12,112.66,263.11,191.55,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,273.58,208.91,234.25,10.91;12,112.66,222.46,46.55,10.91">LinkBERT: Pretraining language models with document links</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.551</idno>
		<ptr target="https://aclanthology.org/2022.acl-long.551.doi:10.18653/v1/2022.acl-long.551" />
	</analytic>
	<monogr>
		<title level="m" coord="12,182.78,222.46,325.05,10.91;12,112.66,236.01,85.65,10.91">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8003" to="8016" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="12,112.66,276.66,394.52,10.91;12,112.66,290.20,395.01,10.91;12,112.66,303.75,338.78,10.91" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="12,164.93,290.20,312.85,10.91">Large batch optimization for deep learning: Training bert in 76 minutes</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1904.00962</idno>
		<ptr target="https://arxiv.org/abs/1904.00962.doi:10.48550/ARXIV.1904.00962" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,317.30,394.53,10.91;12,112.66,330.85,394.53,10.91;12,112.66,344.40,395.17,10.91;12,112.66,357.95,393.33,10.91;12,112.66,371.50,393.33,10.91;12,112.66,385.05,395.00,10.91;12,112.66,398.60,197.48,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,311.38,344.40,196.45,10.91;12,112.66,357.95,77.28,10.91">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Le</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-demos.6.doi:10.18653/v1/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m" coord="12,219.91,357.95,286.07,10.91;12,112.66,371.50,393.33,10.91;12,112.66,385.05,47.14,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,412.15,393.53,10.91;12,112.66,425.70,393.33,10.91;12,112.66,439.25,395.17,10.91;12,112.66,452.79,395.00,10.91;12,112.66,466.34,138.14,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,245.77,412.15,260.41,10.91;12,112.66,425.70,29.70,10.91">Know what you don&apos;t know: Unanswerable questions for SQuAD</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2124</idno>
		<ptr target="https://www.aclweb.org/anthology/P18-2124.doi:10.18653/v1/P18-2124" />
	</analytic>
	<monogr>
		<title level="m" coord="12,166.12,425.70,339.86,10.91;12,112.66,439.25,49.38,10.91;12,286.92,439.25,192.00,10.91">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct coords="12,112.66,479.89,393.33,10.91;12,112.66,493.44,397.48,10.91;12,112.36,509.43,61.75,7.90" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="12,310.24,479.89,195.75,10.91;12,112.66,493.44,85.62,10.91">Pre-trained language model for biomedical question answering</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1909.08229</idno>
		<ptr target="https://arxiv.org/abs/1909.08229.doi:10.48550/ARXIV.1909.08229" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,520.54,393.61,10.91;12,112.66,534.09,393.33,10.91;12,112.66,547.64,394.52,10.91;12,112.28,561.19,395.00,10.91;12,112.66,574.74,347.35,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="12,368.95,520.54,137.31,10.91;12,112.66,534.09,260.52,10.91">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5446</idno>
		<ptr target="https://www.aclweb.org/anthology/W18-5446.doi:10.18653/v1/W18-5446" />
	</analytic>
	<monogr>
		<title level="m" coord="12,397.85,534.09,108.13,10.91;12,112.66,547.64,394.52,10.91;12,112.28,561.19,191.51,10.91">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Association for Computational Linguistics</title>
		<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Association for Computational Linguistics<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,588.29,393.33,10.91;12,112.66,601.84,365.37,10.91" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="12,390.74,588.29,115.25,10.91;12,112.66,601.84,234.98,10.91">Transferability of natural language inference to biomedical question answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00217</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
