<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,380.17,15.42;1,89.29,106.66,376.09,15.42;1,89.29,128.58,314.23,15.43">NCU-IISR/AS-GIS: Using BERTScore and Snippet Score to Improve the Performance of Pretrained Language Model in BioASQ 10b Phase B</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,156.89,81.76,11.96"><forename type="first">Hao-Hsuan</forename><surname>Ting</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Interdisciplinary Program of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">National Central University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,186.69,156.89,47.32,11.96"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Central University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,249.64,156.89,72.54,11.96"><forename type="first">Jen-Chieh</forename><surname>Han</surname></persName>
							<email>joyhan@cc.ncu.edu.tw</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Central University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,356.17,156.89,118.73,11.96"><forename type="first">Richard Tzong-Han</forename><surname>Tsai</surname></persName>
							<email>thtsai@csie.ncu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Central University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">IoX Center</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Research Center for Humanities and Social Sciences</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,380.17,15.42;1,89.29,106.66,376.09,15.42;1,89.29,128.58,314.23,15.43">NCU-IISR/AS-GIS: Using BERTScore and Snippet Score to Improve the Performance of Pretrained Language Model in BioASQ 10b Phase B</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">19926C8C9C9293E1AC5370EDC2E67BC2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Biomedical Question Answer</term>
					<term>Pre-trained Language Model</term>
					<term>Text Similarity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents our system for the BioASQ10b Phase B task. For ideal answers, we used the fine-tuned BioBERT model on the MNLI dataset to construct sentence embeddings and combined it with BERTScore to select sentences from the provided Snippets as answers. For the exact answers, we also used the BioBERT model and used the snippet scores generated from the ideal answer selection model to predict the exact answers for factoid and list questions. The exact answers of our fifth test batch ranked second place. In addition, the ideal answers we submitted achieved first place in the ROUGE score in all test batches from batch second to fifth.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Since 2013, BioASQ has held annual biomedical semantic indexing and question answering challenges. This year, BioASQ Task 10b Phase B (QA task) provides biomedical questions and some relevant snippets, and the participants have to generate either the exact answer or the ideal answer by using the snippets. BioASQ Task 10b PhaseB task provided 4,234 training questions, including the previous year's test set with gold annotations, plus 500 new test questions for evaluation, divided into five batches of 100 questions each. All questions and answers are constructed by a biomedical expert team from across Europe. The questions are categorized into four types: Yes/no, factoid, list, and summary. Three types of questions required exact answers: yes/no, factoid, and list. Participants need to submit the ideal answer for every question. In Task 10b, each participant was allowed to submit up to five results per batch.</p><p>Four examples of QA types for BioASQ Task 10b Phase B (QA task) are illustrated in figure <ref type="figure" coords="1,500.33,577.84,3.67,10.91" target="#fig_0">1</ref>. Each BioASQ QA instance includes a question and several relevant PubMed abstract snippets. As a result, we formulated the task as query-based multi-document extraction (for the exact answer) and summarization (for the ideal answer) tasks. Last year, we used the BioBERT model combined with linear regression to achieve the best result in generating ideal answers <ref type="bibr" coords="2,474.71,370.67,11.43,10.91" target="#b0">[1]</ref>.</p><p>This year, we improved our previous BioASQ 9B system in two ways. First, we selected the most relevant segments for each question by replacing ROUGE-SU4 with BERTScore, and used different linear regression layers to improve the ideal answers. In addition, we used sequential learning BioBERT to generate the exact answer. Meanwhile, we combined KU-DMIS's method <ref type="bibr" coords="2,89.29,438.42,12.96,10.91" target="#b1">[2]</ref> with the snippet scores given by the ideal selection model to select the final answer. This method is applied to factoid and list problems.</p><p>The sections are organized as follows. Section 2 briefly reviews recent works on biomedical QA. The details of our methods are described in Section 3. Section 4 is our detailed experiment procedure. Section 5 describes our configurations submitted to the BioASQ 10b Phase B challenge and the results. Section 6 discusses and summarizes our system's performance in the BioASQ QA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Transfer Learning with Pre-trained Language Model: Since the introduction of BERT <ref type="bibr" coords="2,89.29,591.89,11.58,10.91" target="#b2">[3]</ref>, the pre-trained language model has achieved excellent results in various tasks in NLP. However, the pre-trained language model has reached a bottleneck, and it is hard to improve performance in specific domains. To solve this problem, domain-adapted models are gradually being developed. TANDA <ref type="bibr" coords="2,203.94,632.54,12.68,10.91" target="#b3">[4]</ref> is an effective approach for the question answering task -Answer Sentence Selection (AS2). They used transfer learning on a pre-trained language model. In the paper, they also built a new dataset -ASNQ to fine-tune the pre-trained language model. They outperform the previous state-of-the-art to prove that transfer learning can produce great results.</p><p>For Biomedical QA task, there are also many domain-adapted models are proposed, such as BioBERT <ref type="bibr" coords="3,130.91,127.61,11.28,10.91" target="#b4">[5]</ref>, PubMedBERT <ref type="bibr" coords="3,211.23,127.61,11.28,10.91" target="#b5">[6]</ref>, and SciBERT <ref type="bibr" coords="3,286.47,127.61,11.28,10.91" target="#b6">[7]</ref>, and so on. In this paper, we choose BioBERT to be our basic model to do transfer learning. Sequential transfer learning <ref type="bibr" coords="3,403.33,141.16,12.70,10.91" target="#b1">[2]</ref> is another approach developed by KU-DMIS. They applied this approach to biomedical QA and demonstrated that transferring the knowledge of MNLI and SQuAD to the BioASQ task can improve the performance of BioBERT. They also mentioned that the order of datasets in sequential transfer learning is important. After BioASQ 8b, they released fine-tuned models such as BioBERT-MNLI and BioBERT-MNLI-SQuAD.</p><p>In addition, QA tasks require a large amount of annotated corpus to train the model. This is a prerequisite for deep learning. In addition to BioASQ, many QA datasets annotated by biomedical experts have been published <ref type="bibr" coords="3,269.78,249.56,11.44,10.91" target="#b7">[8]</ref>. The WikiQA dataset is collected from Wikipedia, the question is selected from the question-like queries that more than five distinct users click, and the candidate answer is the sentence from the summary section of the associated Wikipedia page.</p><p>Extractive Summarization: Summarization tasks can be divided into extractive and abstractive summarization. Extractive summaries are derived by selecting sentences or concatenating the most important sentences in a document. This approach is more robust and more suitable for tasks with less training data. There are many extractive abstraction methods based on neural networks have been proposed. Some of these studies were based on the RNN family of models <ref type="bibr" coords="3,89.29,371.50,11.23,10.91" target="#b8">[9,</ref><ref type="bibr" coords="3,102.88,371.50,12.23,10.91" target="#b9">10]</ref>. However, in the last years, with the success of BERT, many new studies have switched to using pre-trained language models. Yang Liu first proposed the BERTSUM architecture, which is based on BERT with the addition of inter-sentence Transformer layers to obtain document-level features, and surpasses other neural network models <ref type="bibr" coords="3,326.02,412.15,16.22,10.91" target="#b10">[11]</ref>. Later, some scholars have used text matching to improve the effectiveness of the BERT model in extractive summarization tasks <ref type="bibr" coords="3,89.29,439.25,16.18,10.91" target="#b11">[12]</ref>. Based on the above studies, We tried to accomplish the ideal answer task as an extractive summarization task. In contrast to other summarization datasets, for the BioASQ dataset, we can focus on extracting the relationships between questions and candidate sentences without worrying about the relationships between the candidate sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data</head><p>To generate the exact and ideal answer, we first use BioASQ 10b and BioASQ 9b datasets to split specific training and testing data. We use BioASQ 9b as our training set and the portion of BioASQ 10b that is not included in BioASQ 9b as our testing set. Furthermore, we employed a distinct way to make the data for each type of problem into exact answers.</p><p>For Yes/no problem, we concatenate the question and each snippet into a sentence pair. Moreover, each sentence pair has one [ANSWER] label that is the answer to this question. In other words, if there are N snippets in one question, we will get the N sentence pairs with an answer label finally.</p><p>We make the question and each snippet a QA pair for the factoid and list problem. However, answer text is required in QA. We use Longest Common Subsequence (LCS) to find the answer text and index where are the answer started in each snippet. However, we found many invalid or too short answers found by the LCS algorithm, which might decrease the model performance and prediction. To solve this problem, we use Levenshtein distance to find the closer text to be the answer text so that all the answer in the training and testing set is meaningful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ideal Answer</head><p>For the ideal answer, we improved on our system of BioASQ 9B <ref type="bibr" coords="4,392.50,204.44,13.00,10.91" target="#b0">[1]</ref> and got better results. The improvements we made this year consist of two main parts: (1) We replaced ROUGE-SU4 with BERTScore and obtained better results. (2) Different linear regression layers were trained for different problem types, which allowed the problem type information to be integrated into the model.</p><p>Our model is shown in figure <ref type="figure" coords="4,231.59,272.19,4.99,10.91" target="#fig_1">2</ref> . The goal is to select the sentence that best fits as an answer from multiple snippets for each question in the BioASQ question examples. The method consists of two parts. We split the input text into candidate sentences in the first step and scored each sentence according to its similarity to the golden answer. We fed the question and each candidate sentence into the pre-trained language model for fine-tuning in the second step. We apply an additional linear regression layer to the [CLS] token output of the BERT model to calculate the output score. The loss function is the mean square error between the output scores and the similar scores from the previous step. The fine-tuned model can predict scores based on the questions and candidate sentences. We selected the candidate sentences with the highest scores under each question as the system's ideal answer prediction result in the prediction stage.</p><p>The pre-trained language model we use is BioBERT-MNLI, which is a fine-tuned BioBERT checkpoint on the MultiNLI dataset <ref type="bibr" coords="5,250.20,114.06,16.25,10.91" target="#b12">[13]</ref>, and it comes from Korea University <ref type="bibr" coords="5,434.50,114.06,11.43,10.91" target="#b1">[2]</ref>.</p><p>Text Similarity: Many different methods can be used for the calculation of text similarity scores. The classical methods are ROUGE <ref type="bibr" coords="5,273.88,141.16,16.12,10.91" target="#b13">[14]</ref>, BLEU <ref type="bibr" coords="5,324.53,141.16,16.12,10.91" target="#b14">[15]</ref>, and other evaluation methods based on the difference between n-gram tokens. We used ROUGE-SU4 because it is one of the official methods for evaluating ideal answers in BioASQ. In addition, some studies have also developed evaluation methods based on pre-trained language models in recent years, the most famous and widely used of which is BERTScore <ref type="bibr" coords="5,245.83,195.36,16.09,10.91" target="#b15">[16]</ref>. BERTScore uses pre-trained contextual embeddings of BERT to match words in candidate and reference sentences by cosine similarity. It has been shown to correlate with human judgments on sentence-level evaluations. Therefore, we also used BERTScore based on the BioBERT-MNLI model this year.</p><p>Question Type Specific Layer: It is worth noting that there are four types of questions in the BioASQ 10b dataset, namely factoid, yes/no, list, and summary. We believe that there are differences in the ideal answers for different question types, e.g., the answer to a list question may contain multiple entities, and the answer to a factoid question is more likely to be something like a definition. Therefore, the model should have different parameter weights depending on the type of question. We can fine-tune the four BioBERTs with entirely different parameters, but this will significantly reduce the amount of training data. Therefore, we let the four different problem types share the vast majority of the model parameters and only have different parameters in the last layer of the linear regression part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Exact Answer</head><p>To predict the exact answer, we used the transfer learning pre-trained BioBERT model and KU-DMIS's method <ref type="bibr" coords="5,177.69,421.23,12.69,10.91" target="#b1">[2]</ref> to get the possible answer list. The final answer list was combined with the snippet score given by the ideal answer selection model. We will introduce the system below respectively.</p><p>For Yes/no problem, we used the BioBERT-MNLI model, which is BioBERT fine-tuned on the MNLI dataset. MNLI (Multi-Type Natural Language Inference), published by New York University, is a text entailment task requiring determining whether a hypothesis holds given a premise (Premise) or whether the hypothesis is contradictory and neutral to the premise. The snippet score was applied to the answer list generated from the model. For each snippet in every question, if the probability of the snippet's answer is higher than 0.5, this snippet's weight is a positive weight "1"; otherwise, it is a negative weight "-1". The snippet score is then multiplied by its weight, yielding a new score for this snippet. We obtain the final answer to this question by adding all of the snippets' new scores together. The final answer is yes or no, depending on whether the summary is positive or negative.</p><p>For factoid problem, we used the BioBERT-SQuAD model, which is BioBERT fine-tuned on the SQuAD dataset. SQuAD (Stanford Question Answering Dataset) is a reading comprehension dataset consisting of questions posed by crowdworkers on a set of Wikipedia articles. The answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. The snippet score was also applied to the generated answer list. In the answer list, we have the probability of each possible answer of each snippet. Each answer's probability was multiplied by the snippet score and the resulting probability was used to re-order all snippets' answers. We got all possible answers ranked by new probability and selected the top five answers to be the final answers for each factoid question.</p><p>For list problem, we also used the BioBERT-SQuAD model. The snippet score was also applied to the generated answer list. As same as the method for the factoid problem, the answer's probability was multiplied by the snippet score, and the resulting probability was used to re-order all snippets' answers. We got all possible answers ranked by new probability for each list question. However, it is not a good strategy to submit all possible answers. We set a threshold to select answers that new probability exceeds the threshold. Different thresholds are set for different models and snippet score versions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ideal Answer</head><p>Table <ref type="table" coords="6,126.82,412.51,5.03,10.91" target="#tab_0">1</ref> shows the performance of our system on the BioASQ 9b dataset. The average scores were calculated from the system's predictions and the golden answers. All the experiment data are the best results of the model under this method. We set the epoch number to 3, batch size to 20, and learning rate to 2e-5.</p><p>To our surprise, the system with BERTScore outperformed the system with ROUGE on all evaluation criteria, even on the evaluations related to ROUGE. We believe that such experimental results imply that BERTScore is an overall better method for evaluating text similarity. In addition, the experimental results also show that training specific linear regression layers for different problem types can help improve system performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Exact Answer</head><p>For the following experimental procedure, we use the same testing set split by Training 10b and Training 9b to do the test to observe the experimental results.</p><p>For Yes/no problem, we experimented with different transfer learning models, including BioBERT, BioBERT-MNLI, BioBERT-SQuAD, and BioBERT-MNLI-SQuAD, and compared the differences between the models. As shown in Table <ref type="table" coords="6,325.09,624.83,3.81,10.91" target="#tab_2">2</ref>, the performance of BioBERT-MNLI is substantially higher than the other models by more than 0.04 in both metrics. We consider that it is because yes/no problem is more similar to MNLI (text entailment task) than the others. Additionally, we evaluated the performances of the BioBERT-MNLI model with/without the usage of a snippet score. To our surprise, Table <ref type="table" coords="7,332.48,411.90,5.17,10.91" target="#tab_2">2</ref> shows that the performances of the BioBERT-MNLI on applied snippet score are decreased by roughly 0.01.</p><p>For factoid problem, we evaluated different transfer learning models, including BioBERT, BioBERT-SQuAD, and BioBERT-MNLI-SQuAD, as shown in Table <ref type="table" coords="7,380.68,452.55,3.68,10.91">3</ref>. KU-DMIS <ref type="bibr" coords="7,436.75,452.55,12.73,10.91" target="#b1">[2]</ref> showed that BioBERT-MNLI-SQuAD had a higher performance than BioBERT-SQuAD. However, our experiment found that BioBERT-SQuAD outperformed BioBERT-MNLI-SQuAD in all measurements by more than 0.02. We also evaluated the performance of BioBERT-SQuAD with and without using the snippet score. There is an increase of 0.02 on SAcc, which shows that it predicts more accurate answers. Meanwhile, MRR has a small improvement of 0.005.</p><p>In addition, we conducted experiments on different snippet score calculations. We combined the snippet score and the answer's probability by two different operations, "Multiply" and "Plus". In Table <ref type="table" coords="7,127.20,560.94,3.66,10.91">3</ref>, using the "Multiply" approach outperformed using the "Plus" approach in all metrics.</p><p>For list problem, BioBERT, BioBERT-SQuAD, and BioBERT-MNLI-SQuAD are evaluated. In Table <ref type="table" coords="7,115.59,588.04,3.71,10.91" target="#tab_3">4</ref>, we found that BioBERT-SQuAD with the "Multiply" approach outperformed BioBERT-MNLI-SQuAD on F-Measure, this result is also different from KU-DMIS <ref type="bibr" coords="7,423.38,601.59,11.58,10.91" target="#b1">[2]</ref>. We evaluated the performances of BioBERT-SQuAD with and without using the snippet score. There is an increase of 0.03 for the "Multiply" approach on the F-Measure, and Precision also has an increase of 0.06.</p><p>In addition, we also conducted experiments using different snippet score calculations. we observed that, unlike factoid, using "Plus" for the list problem might have a considerable negative impact on performance. Therefore, we tried calculating the result of the "Multiply" approach in various powers. We found that the Precision increased after adding the power. However, F-Measure and Recall are decreased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Submission</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ideal Answer</head><p>We participated in all but the first batch and obtained the highest score in the ROUGE evaluation (based on ROUGE-2 F1 and ROUGE-SU4 F1) from the second to fifth batches. In each batch, we submitted multiple configurations, most of which differed only in their hyperparameter settings. We simplified the description and reported the best results of each method in Table <ref type="table" coords="8,500.08,439.61,3.71,10.91" target="#tab_4">5</ref>.</p><p>The results demonstrate the overall effectiveness of our system. The results of batch 4 show that BERTScore is a better measure of text similarity and positively affects our model training. However, Batch 5 is different from our experimental results on BioASQ 9b dataset. The model's performance became worse after adding the question type for Linear Layer selection. Because the gold answers for BioASQ 10b have not yet been published, we have not been able to analyze its possible causes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Exact Answer</head><p>In the official submission, we did not submit other models' results. We only submitted the system that performed the best during the experiment.</p><p>In the fifth test batch, we used the newest snippet score version, which applied to factoid and list problems to generate an answer list, detailed as shown in Table <ref type="table" coords="8,415.45,611.28,3.81,10.91">6</ref>. We also show the first place of each problem together in the table. Our factoid answer is ranked second in this batch, and the difference between it and first place (Ir_sys3) is quite minimal, indicating that our solution is very effective in the factoid problem. In addition, we came in sixth place in the list problem, with UDEL-LAB providing the top five solutions. However, there is still a discrepancy between our results and those of UDEL-LAB, indicating that our approach to the list should be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and Conclusion</head><p>In the BioASQ 10b phaseB task, we used the fine-tuned BioBERT model on the MNLI dataset to construct sentence embeddings and combined it with BERTScore to select sentences from the provided snippets as ideal answers. As for the exact answer, we combined the KU-DMIS approach with our ideal answer selection model to predict exact answers for factoid and list problems.</p><p>Our approach proved that replacing ROUGE-SU4 with BERTScore is a practical improvement for the ideal answer. This further validates BERTScore as a better method for text evaluation. At the same time, our proposed regression method for linear layer selection based on question types is worth further investigation. Meanwhile, the experiment results of our proposed regression method for linear layer selection based on problem type were inconsistent, but it deserves further study.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,90.62,289.75,8.93;2,97.97,106.51,396.85,227.91"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The QA examples of the BioASQ Task 10b Phase B (QA task)</figDesc><graphic coords="2,97.97,106.51,396.85,227.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,417.69,196.64,8.93;4,183.01,433.59,226.76,242.06"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Model for predicting the ideal answer</figDesc><graphic coords="4,183.01,433.59,226.76,242.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,88.99,90.49,398.13,93.79"><head>Table 1</head><label>1</label><figDesc>Results of Ablation Study of Ideal Answer Prediction on BioASQ9b Dataset</figDesc><table coords="6,105.94,122.10,381.19,62.17"><row><cell>Method</cell><cell>P</cell><cell>ROUGE-SU4 R F1</cell><cell>P</cell><cell>BERTScore R</cell><cell>F1</cell></row><row><cell>ROUGE + BioBERT-MNLI</cell><cell cols="5">39.08 36.86 37.94 74.09 74.19 74.14</cell></row><row><cell>BERTScore + BioBERT-MNLI</cell><cell cols="5">40.12 36.57 38.26 75.45 74.59 75.02</cell></row><row><cell cols="2">BERTScore + BioBERT-MNLI + Type Layer 41</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,301.13,175.35,188.21,8.93"><head>.04 36.99 38.91 76.05 74.85 75.45</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,88.99,90.49,369.72,293.63"><head>Table 2</head><label>2</label><figDesc>Result of Ablation Study for Yes/no problem</figDesc><table coords="7,88.99,124.61,369.72,259.50"><row><cell>Model</cell><cell></cell><cell cols="3">Snippet Score Acc</cell><cell>Ma F1</cell></row><row><cell>BioBERT</cell><cell></cell><cell cols="2">Not Applied</cell><cell>0.810 0.771</cell></row><row><cell>BioBERT-MNLI</cell><cell></cell><cell cols="3">Not Applied 0.888 0.872 Applied 0.879 0.861</cell></row><row><cell cols="2">BioBERT-SQuAD</cell><cell cols="2">Not Applied</cell><cell>0.845 0.830</cell></row><row><cell cols="4">BioBERT-MNLI-SQuAD Not Applied</cell><cell>0.810 0.767</cell></row><row><cell>Table 3</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Result of Ablation Study for factoid problem</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="4">Snippet Score Approach SAcc</cell><cell>LAcc MRR</cell></row><row><cell>BioBERT</cell><cell cols="2">Applied</cell><cell>Multiply</cell><cell>0.342 0.563 0.420</cell></row><row><cell>BioBERT-MNLI-SQuAD</cell><cell cols="2">Not Applied Applied</cell><cell>Multiply</cell><cell>0.506 0.747 0.598 0.519 0.747 0.602</cell></row><row><cell>BioBERT-SQuAD</cell><cell cols="2">Not Applied</cell><cell></cell><cell>0.519 0.785 0.620</cell></row><row><cell>BioBERT-SQuAD</cell><cell cols="2">Applied</cell><cell cols="2">Multiply 0.532 0.778 0.625 Plus 0.532 0.747 0.615</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,88.99,90.49,464.28,174.01"><head>Table 4</head><label>4</label><figDesc>Result of Ablation Study for list problem</figDesc><table coords="8,95.27,124.61,458.01,139.88"><row><cell>Model</cell><cell>Snippet Score</cell><cell>Approach</cell><cell>Prec.</cell><cell cols="2">Rec. F-Measure</cell></row><row><cell>BioBERT</cell><cell>Applied</cell><cell>Multiply</cell><cell cols="2">0.522 0.639</cell><cell>0.514</cell></row><row><cell>BioBERT-MNLI-SQuAD</cell><cell>Not Applied Applied</cell><cell>Multiply</cell><cell cols="2">0.434 0.754 0.533 0.648</cell><cell>0.505 0.532</cell></row><row><cell>BioBERT-SQuAD</cell><cell>Not Applied</cell><cell></cell><cell cols="2">0.485 0.747</cell><cell>0.542</cell></row><row><cell></cell><cell></cell><cell>Plus</cell><cell>0.593</cell><cell>0.35</cell><cell>0.381</cell></row><row><cell>BioBERT-SQuAD</cell><cell>Applied</cell><cell cols="3">Multiply Result of "Multiply" with Power of 1.5 0.630 0.498 0.547 0.711</cell><cell>0.572 0.493</cell></row><row><cell></cell><cell></cell><cell>Result of "Multiply" with Power of 2</cell><cell cols="2">0.705 0.415</cell><cell>0.475</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,88.99,90.49,399.52,241.24"><head>Table 5</head><label>5</label><figDesc>Test batch results for the ideal answer</figDesc><table coords="9,88.99,119.88,399.52,211.85"><row><cell>Method</cell><cell cols="2">Batch Rank</cell><cell cols="2">ROUGE-2 Recall F1</cell><cell cols="2">ROUGE-SU4 Recall F1</cell></row><row><cell>ROUGE + BioBERT-MNLI</cell><cell>2</cell><cell>#1</cell><cell>47.28</cell><cell cols="2">42.62 48.23</cell><cell>41.68</cell></row><row><cell>ROUGE + BioBERT-MNLI</cell><cell>3</cell><cell>#1</cell><cell>41.93</cell><cell cols="2">37.61 42.36</cell><cell>36.89</cell></row><row><cell>ROUGE + BioBERT-MNLI</cell><cell>4</cell><cell>#4</cell><cell>40.40</cell><cell cols="2">40.26 40.64</cell><cell>39.41</cell></row><row><cell>BERTScore + BioBERT-MNLI</cell><cell>4</cell><cell>#1</cell><cell>41.79</cell><cell cols="2">42.29 41.57</cell><cell>41.65</cell></row><row><cell>BERTScore + BioBERT-MNLI</cell><cell>5</cell><cell>#1</cell><cell>42.70</cell><cell cols="2">40.20 42.08</cell><cell>39.16</cell></row><row><cell cols="2">BERTScore + BioBERT-MNLI + Type Layer 5</cell><cell>#2</cell><cell>39.62</cell><cell cols="2">37.07 39.28</cell><cell>36.26</cell></row><row><cell>Table 6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Test Batch fifth Result of Exact Answer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>System name</cell><cell cols="5">Yes/no: Macro F1 factoid: MRR list: mean F-Measure</cell></row><row><cell>NCU-IISR-AS-GIS-4</cell><cell>0.8893</cell><cell>0.4983 (#2)</cell><cell></cell><cell cols="2">0.5332 (#6)</cell></row><row><cell>Ir_sys1</cell><cell>0.9282 (#1)</cell><cell>0.4195</cell><cell></cell><cell cols="2">0.5224</cell></row><row><cell>Ir_sys3</cell><cell>0.8916</cell><cell>0.5098 (#1)</cell><cell></cell><cell cols="2">0.4620</cell></row><row><cell>UDEL-LAB4</cell><cell>0.8893</cell><cell>0.4017</cell><cell></cell><cell cols="2">0.6016 (#1)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>We tried to apply the snippet score to three types of problems for the exact answer. For the yes/no problem, our approach could not further improve the performance, even if it negatively impacts model performance. We found an effective approach to improve the model's performance for the factoid problem, placing us in second place in the fifth test batch. Our approach has also resulted in a significant improvement in our performance on the test set for the list problem. We are, however, still a long way from first place in the fifth test batch. Overall, we have improved the performance of the sequential learning model for factoid and list questions.</p><p>However, our method is not optimal, and there is still potential for improvement in yes/no and list problems. We hope to try to come up with better ways to increase performance in the future.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="10,112.66,159.14,393.33,10.91;10,112.66,172.69,394.62,10.91;10,112.66,186.24,160.06,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,293.86,159.14,212.13,10.91;10,112.66,172.69,374.14,10.91">Ncu-iisr/as-gis: Results of various pre-trained biomedical language models and linear regression model in bioasq task 9b phase b</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-C</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">T</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">-H</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,112.66,186.24,129.93,10.91">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2021">123. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,199.79,393.33,10.91;10,112.66,213.34,393.57,10.91;10,112.33,226.89,29.19,10.91" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,390.94,199.79,115.05,10.91;10,112.66,213.34,239.70,10.91">Transferability of natural language inference to biomedical question answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00217</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,240.44,393.33,10.91;10,112.66,253.99,363.59,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="10,353.43,240.44,152.55,10.91;10,112.66,253.99,181.08,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,267.54,393.33,10.91;10,112.66,281.08,393.33,10.91;10,112.66,294.63,198.72,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,241.82,267.54,264.16,10.91;10,112.66,281.08,134.68,10.91">Tanda: Transfer and adapt pre-trained transformer models for answer sentence selection</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,275.28,281.08,230.70,10.91;10,112.66,294.63,50.10,10.91">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7780" to="7788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,308.18,393.33,10.91;10,112.66,321.73,393.98,10.91;10,112.41,335.28,48.96,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,361.64,308.18,144.35,10.91;10,112.66,321.73,268.25,10.91">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,394.29,321.73,66.92,10.91">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,348.83,394.53,10.91;10,112.66,362.38,394.53,10.91;10,112.28,375.93,332.27,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,112.66,362.38,390.01,10.91">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,112.28,375.93,268.63,10.91">ACM Transactions on Computing for Healthcare (HEALTH)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,389.48,393.60,10.91;10,112.66,403.03,146.44,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10676</idno>
		<title level="m" coord="10,234.39,389.48,239.83,10.91">Scibert: A pretrained language model for scientific text</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,416.58,393.33,10.91;10,112.66,430.13,393.33,10.91;10,112.66,443.67,186.50,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,250.33,416.58,255.66,10.91;10,112.66,430.13,44.87,10.91">Wikiqa: A challenge dataset for open-domain question answering</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>-T. Yih</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,187.73,430.13,318.25,10.91;10,112.66,443.67,88.51,10.91">Proceedings of the 2015 conference on empirical methods in natural language processing</title>
		<meeting>the 2015 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2013" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,457.22,393.33,10.91;10,112.66,470.77,393.32,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02305</idno>
		<title level="m" coord="10,357.65,457.22,148.34,10.91;10,112.66,470.77,211.05,10.91">Neural document summarization by jointly learning to score and select sentences</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,484.32,394.52,10.91;10,112.66,497.87,173.79,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07187</idno>
		<title level="m" coord="10,284.31,484.32,217.96,10.91">Neural latent extractive document summarization</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,511.42,393.33,10.91;10,112.33,524.97,29.19,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10318</idno>
		<title level="m" coord="10,148.67,511.42,199.25,10.91">Fine-tune bert for extractive summarization</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,538.52,393.32,10.91;10,112.66,552.07,222.03,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08795</idno>
		<title level="m" coord="10,358.92,538.52,147.06,10.91;10,112.66,552.07,39.39,10.91">Extractive summarization as text matching</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,565.62,393.33,10.91;10,112.66,579.17,393.33,10.91;10,112.28,592.72,393.71,10.91;10,112.33,606.27,394.86,10.91;10,112.66,619.81,265.66,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,284.97,565.62,221.02,10.91;10,112.66,579.17,148.23,10.91">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/N18-1101" />
	</analytic>
	<monogr>
		<title level="m" coord="10,284.46,579.17,221.52,10.91;10,112.28,592.72,393.71,10.91;10,112.33,606.27,56.93,10.91">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="10,112.66,633.36,393.33,10.91;10,112.66,646.91,202.65,10.91" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="10,168.83,633.36,337.16,10.91;10,112.66,646.91,20.77,10.91">Rouge 2.0: Updated and improved measures for evaluation of summarization tasks</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ganesan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01937</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,660.46,393.33,10.91;11,112.66,86.97,393.53,10.91;11,112.66,100.52,203.57,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,310.87,660.46,195.12,10.91;11,112.66,86.97,88.86,10.91">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,224.89,86.97,281.30,10.91;11,112.66,100.52,116.04,10.91">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,114.06,393.33,10.91;11,112.66,127.61,271.84,10.91" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09675</idno>
		<title level="m" coord="11,384.35,114.06,121.63,10.91;11,112.66,127.61,90.07,10.91">Bertscore: Evaluating text generation with bert</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
