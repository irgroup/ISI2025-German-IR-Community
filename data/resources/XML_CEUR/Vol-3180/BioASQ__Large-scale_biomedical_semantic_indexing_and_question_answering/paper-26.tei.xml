<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,92.18,75.44,410.80,17.04;1,192.89,96.20,209.68,17.04">mBERT and Simple Post-Processing: A Baseline for Disease Mention Detection in Spanish</title>
				<funder ref="#_HQSZa8X">
					<orgName type="full">Mexican Government</orgName>
				</funder>
				<funder ref="#_QXFaGNt">
					<orgName type="full">Secretaría de Investigación y Posgrado of the Instituto Politécnico Nacional, Mexico</orgName>
				</funder>
				<funder ref="#_bX3wFNc #_EzJBUXb">
					<orgName type="full">CONACYT, Mexico</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.02,129.10,81.86,10.80"><forename type="first">Antonio</forename><surname>Tamayo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centro de Investigación en Computación (CIC)</orgName>
								<orgName type="institution">Instituto Politécnico Nacional (IPN)</orgName>
								<address>
									<addrLine>Av. Juan de Dios Batiz, s/n</addrLine>
									<postCode>07320</postCode>
									<settlement>Mexico City</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,163.94,129.10,81.64,10.80"><forename type="first">Diego</forename><forename type="middle">A</forename><surname>Burgos</surname></persName>
							<email>burgosda@wfu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Wake Forest University</orgName>
								<address>
									<addrLine>1834 Wake Forest Road, Winston-Salem</addrLine>
									<postCode>27109</postCode>
									<settlement>Winston Salem</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.17,129.10,94.20,10.80"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
							<email>gelbukh@gelbukh.com</email>
							<affiliation key="aff0">
								<orgName type="department">Centro de Investigación en Computación (CIC)</orgName>
								<orgName type="institution">Instituto Politécnico Nacional (IPN)</orgName>
								<address>
									<addrLine>Av. Juan de Dios Batiz, s/n</addrLine>
									<postCode>07320</postCode>
									<settlement>Mexico City</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,92.18,75.44,410.80,17.04;1,192.89,96.20,209.68,17.04">mBERT and Simple Post-Processing: A Baseline for Disease Mention Detection in Spanish</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">54021561D0F382065327D44911263AA9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Disease mention detection</term>
					<term>multilingual BERT</term>
					<term>named entity recognition (NER)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic disease mention extraction is a relevant task due to its various applications in the medical field. During the last decade, many related works have been published, which have accelerated the progress of this research area, but most of them have been carried out in English. In this work, we propose a deep-learning baseline for this task in Spanish. We report an approach based on transfer learning using multilingual BERT and a straightforward postprocessing to tackle the problem. Our system does not use any external resources and rely only on efficient fine tuning, which makes it a fair baseline (Micro F1 = 0.5456) for disease mention identification in Spanish using transformer-based models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Named entity recognition (NER) has become a key component to more complex systems due to its contribution to document indexing and categorization, knowledge acquisition, etc. Disease mentions in clinical cases are considered a type of domain-specific named entity <ref type="bibr" coords="1,397.90,419.23,12.91,9.94" target="#b0">[1]</ref> and its recognition and extraction are of great help for higher-level tasks in the field of medicine. The way NER and other text mining tasks are tackled has rapidly changed over time, though. While text preprocessing (e.g., tokenization, pos-tagging, stopwords), feature engineering, and (limited) data were an essential input for classical probabilistic and machine learning models during years, modern deep-learning models are trained on huge data, but with much simpler feature representations <ref type="bibr" coords="1,371.56,482.61,11.77,9.94" target="#b1">[2]</ref>.</p><p>In this paper, we propose a baseline for disease mention detection in Spanish as a contribution to the entities sub-track of DisTEMIST (Disease Text Mining Shared Task, 2022). This sub-track required the participants to automatically identify disease mentions in a dataset of clinical cases in Spanish <ref type="bibr" coords="1,487.87,520.53,11.69,9.94" target="#b2">[3]</ref>. The task is particularly challenging because the evaluation metric measures the system's ability to determine the exact starting and ending location of the disease mention in the document.</p><p>For this task, we followed a transfer learning approach, that is, we used and fine-tuned the multilingual BERT (mBERT) language model that was originally trained on a different, more heterogeneous data, i.e., not only medical documents, and that was also trained for a different task. Since we addressed the entities task as a sequence labeling problem, the training data provided by the organizers required substantial preprocessing in order to take it to the format required by the model. Likewise, we carried out simple post-processing on the model's output in order to concatenate subwords, clean up the output, and take the predictions to the DisTEMIST's format.</p><p>DisTEMIST is using a dictionary lookup baseline on Levenshtein distance, which looks for train and development annotations in the test set. The same approach for a similar task was used in <ref type="bibr" coords="1,478.78,659.64,11.70,9.94" target="#b3">[4]</ref>, which yielded a Micro F1 baseline of 0.291. In the present work, we propose the Micro F1 that we reached for this task (0.5456) as a deep-learning baseline for disease mention detection in Spanish since we did not use any external resources such as terminological databases, POS tagging, and so on. As described in detail below, these results were achieved only by fine-tuning mBERT with the best hyperparameter combination found after several experiments and by using a BIO scheme <ref type="bibr" coords="2,418.63,112.60,12.78,9.94" target="#b4">[5]</ref> to annotate disease mentions in the training dataset. Moreover, while the literature for this task in English reports F1 scores as high as 89.7 <ref type="bibr" coords="2,139.92,137.92,11.68,9.94" target="#b1">[2]</ref>, works in disease mention detection in Spanish before DisTEMIST 2022 are scarce, which makes a deep-learning baseline for this task in Spanish helpful for future work reference.</p><p>The paper is structured as follows. Section two presents relevant works related to NER and transfer learning in specific domains. Section three describes the dataset, preprocessing steps, the language model and fine-tuning used, as well as other methodological aspects. Section four reports and discusses the results, and Section five concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>As many natural language processing problems, NER in the medical field has been addressed with some of the following three approaches: a) rule-based, b) machine learning, or c) deep learning techniques. As for the rule-based approach, <ref type="bibr" coords="2,272.45,284.23,12.92,9.94" target="#b5">[6]</ref> reports promising results in English using rules and regular expressions. On the other hand, the first works utilizing machine learning techniques to tackle NER in the medical field reached their best results using feature engineering, support vector machines (SVMs) and tree-based kernels <ref type="bibr" coords="2,211.31,322.15,11.74,9.94" target="#b6">[7]</ref>.</p><p>With regards to deep learning, following a similar approach to the one we report in this paper, <ref type="bibr" coords="2,510.29,334.87,13.20,9.94" target="#b7">[8]</ref> used BERT <ref type="bibr" coords="2,124.82,347.47,12.92,9.94" target="#b8">[9]</ref> and ELMO <ref type="bibr" coords="2,191.41,347.47,18.47,9.94" target="#b9">[10]</ref> for NER in two corpora in English, namely, PubMed titles and abstracts and clinical notes. They used a BIO scheme and reached the highest F1 score (86.6) on disease mention detection with a BERT-base model on PubMed only using the configurations reported by <ref type="bibr" coords="2,485.62,372.79,11.70,9.94" target="#b5">[6]</ref>. <ref type="bibr" coords="2,505.30,372.79,18.20,9.94" target="#b10">[11]</ref> carried out transfer learning for biomedical NER in English also. They pre-trained a bidirectional language model (BiLM) on unlabeled data and used it to initialize weights instead of initializing them randomly. This strategy outperformed other experiments without pre-training or with unidirectional pre-training. Their F1 scores reached 87.34 on the NCBI-Disease corpus and 89.28 on the BC5CDR corpus. It is worth noting that besides disease mentions, BC5CDR includes chemical entities, which seem to respond better to bidirectional models. (cf. <ref type="bibr" coords="2,305.34,448.65,11.73,9.94" target="#b7">[8]</ref>). In <ref type="bibr" coords="2,341.18,448.65,17.45,9.94" target="#b11">[12,</ref><ref type="bibr" coords="2,362.22,448.65,13.06,9.94" target="#b12">13]</ref>, the authors trained BERT from scratch on a huge biomedical corpus in English and then fine-tuned it for different tasks including NER; <ref type="bibr" coords="2,72.02,473.97,18.44,9.94" target="#b11">[12]</ref> accomplished an impressive improvement (0.51%) in biomedical NER. Likewise, <ref type="bibr" coords="2,470.00,473.97,18.69,9.94" target="#b13">[14]</ref> trained BERT but reports better results fine-tuning the model presented in <ref type="bibr" coords="2,366.45,486.69,18.41,9.94" target="#b11">[12]</ref> than with their own.</p><p>Regarding experiments with Spanish, <ref type="bibr" coords="2,253.13,499.29,18.32,9.94" target="#b14">[15]</ref> published a corpus with nested entities and addressed the NER problem using a biLSTM-CRF architecture and word embeddings trained over both clinical embeddings and Spanish Wikipedia. In <ref type="bibr" coords="2,248.09,524.61,16.99,9.94" target="#b15">[16]</ref>, the authors proposed a novel join deep learning model to tackle NER and normalization in a corpus of cancer in Spanish achieving an F1 score of 0.87; an equivalent result was reported by <ref type="bibr" coords="2,222.28,549.93,18.45,9.94" target="#b16">[17]</ref> using ensemble pre-trained BERT models and post-processing. This latter work also followed a similar method to the one we present in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>The subsections below provide an outline of the dataset used as well as a description of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>The dataset for this task consists of 1,000 clinical cases in Spanish in plain, unstructured text on one hand, and 750 structured text files on the other. Disease mentions in the original clinical cases were manually annotated by experts following thorough guidelines and these annotations were used to generate the structured files, which have the following fields (see also Out of the 1,000 clinical cases, the organization randomly chose 250 documents for the test dataset and the remaining 750 documents made up the training set. The test set was shuffled with 2,750 more clinical cases, for a total of 3,000 documents to be mined for disease mentions, but the organizers only evaluated NER on the 250 documents originally chosen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">System Description</head><p>Our approach to extract disease mentions from Spanish clinical cases is based on the transfer learning technique using BERT pretrained on a multilingual corpus plus simple post-processing. To implement the fine-tuning process with the mBERT model, the BIO (Begin, Inside, Outside) scheme was used. Since the dataset provided by DisTEMIST is formatted in a different way, a pre-processing was needed to take it to the BIO scheme. Below we describe our system in four subsections, namely, pre-processing, pretrained model description, transfer learning, and post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Pre-processing</head><p>Before converting the plain text dataset described in section 3.1 into the BIO scheme, we first tokenized it using SpaCy <ref type="bibr" coords="3,183.80,467.25,16.96,9.94" target="#b17">[18]</ref>. While tokenization is a straightforward task, some flaws in the plain text files tremendously hindered this step. A substantial normalization process was necessary in order to take full advantage of the tokenization stage. After this, each lexical unit in the documents was annotated with its corresponding label (B, I, or O) and the document's tokenization was reversed. For the annotation, we used the disease mentions in the structured dataset as a reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Pretrained model</head><p>We used the mBERT model which is a BERT version trained in 104 languages on the largest Wikipedias, including Spanish. It is a transformer-based model which was trained to predict masked words and the next sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Transfer learning</head><p>To fine-tune the model, we modified its prediction head to tackle the extraction of disease mentions in clinical cases as a token classification problem using the transfer learning approach. In this work, we did not use sentence tokenization because the model's performance decreased. Instead, we fine-tuned the model by taking the whole clinical case text as a sample.</p><p>During the fine-tuning process, we conducted a hyperparameter tuning search to identify the best model's configuration using a grid search for the epochs <ref type="bibr" coords="3,327.10,721.92,11.99,9.94" target="#b2">(3,</ref><ref type="bibr" coords="3,342.20,721.92,8.28,9.94" target="#b4">5,</ref><ref type="bibr" coords="3,353.70,721.92,9.20,9.94" target="#b6">7)</ref> and the learning rate (5e-03, 5e-05, 5e-07). After that, the best results were found with the configuration shown in Table <ref type="table" coords="3,445.95,734.64,4.14,9.94" target="#tab_1">2</ref>. For all the experiments, we used 562 texts for training and 188 for validation. The results that we show below correspond to the best performance achieved on the validation partition.</p><p>The fine-tuning process was carried out using the Transformer library with the mBERT cased model available at Hugging Face (https://huggingface.co/bert-base-multilingual-cased) and, as infrastructure, we used Google Colab Pro with a GPU Tesla P100 with 27.3 gigabytes of available RAM. The clean version of the data we used for our training process together with the source code to replicate this work are available at a GitHub repository (https://github.com/ajtamayoh/NLP-CIC-WFU-Contribution-to-DisTEMIST-shared-task-2022).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Post-processing</head><p>Once we obtained the predictions from our model, simple post-processing was carried out. As mBERT works with subword tokenization, the first step was to concatenate all the subwords belonging to the same disease mention found by the model.</p><p>Secondly, we concatenate all the predictions of disease mentions which were found by the model one after the other. This means that if the model detects a disease mention whose final character position plus one concurs with the first position of the next disease mention detected, these two mentions are considered part of the same entity by our system. This was necessary because the model tagged some words as "B" but they were actually a continuation of a previous entity and their label should be fixed to "I".</p><p>We also applied simple post-processing based on some orthographic and grammatical rules, which were applied in the order that they appear in Table <ref type="table" coords="4,293.33,500.37,4.14,9.94">3</ref>. Moreover, because our model was fine-tuned with a BIO scheme, its predictions come out in the same format. Therefore, the last step consisted in decoding the predictions from the BIO format to the structured format required by DisTEMIST, which was described in section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3 Post-processing rules</head><p>If the predicted disease mention… … then apply this rule Concurs with non-content words or punctuation marks Leave out of the entities detected Ends with non-content words or punctuation marks Contains a mark of new line</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Delete the match and fix the off1</head><p>Replace the match with a space Contains a space before and/or after a hyphen or a parenthesis Delete the space(s) and fix the off1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and discussion</head><p>The best configuration was found by intuitively tuning the hyperparameters instead of using brute force with a grid search. We started from a default configuration and carefully decreased the learning rate while increasing the number of epochs. The results achieved by our system for both training and test can be seen in Table <ref type="table" coords="5,186.88,112.60,4.08,9.94" target="#tab_2">4</ref>. Micro-Precision (MiP), Micro-Recall (MiR), and Micro-F1 (MiF1) were used to measure the model performance. The evaluation on our local training set aimed at determining the best hyperparameter configuration. For this preliminary evaluation we used BIO annotations as gold standard rather than the starting and ending locations of disease mentions in DisTEMIST's data. This is the reason why a comparison with and without post-processing is not presented here as our postprocessing rules for this shared task were designed to work on DisTEMIST's format rather than on the BIO format. While mBERT is inherently a complex model, our approach can be considered simple enough for a baseline, but in turn, compared to a dictionary lookup baseline (cf. <ref type="bibr" coords="5,377.32,339.19,11.61,9.94" target="#b3">[4]</ref>), our results (0.5456) are not negligible. A caveat must be added here with regards to the model's ability to identify disease mentions vs the way this task is evaluated. Metrics for disease mention extraction typically assess an exact match of the extracted mention with the starting and ending position of the mention in the gold standard. However, a qualitative scan of the mentions we extracted show our model's capacity to identify disease mentions effectively. The reason why the values for MiR and MiP in Table <ref type="table" coords="5,404.30,402.43,5.52,9.94" target="#tab_2">4</ref> are low is not necessarily that the model missed to detect the mentions in the gold standard or that it detected irrelevant text spans, but also that it may have truncated or extended a text span that contains a correct mention and that our post-processing was not robust enough. See Table <ref type="table" coords="5,295.10,440.37,5.52,9.94" target="#tab_3">5</ref> for some examples. The examples in Table <ref type="table" coords="5,190.16,605.13,5.52,9.94" target="#tab_3">5</ref> suggest that, for a baseline, our model can be considered a decent disease mention extractor. The ability of the model to correctly predict some simple and complex disease mentions according to the gold standard is also remarkable, e.g., hemorragia frontal de angioma cavernoso frontal derecho, angioma protuberancial izquierdo. We acknowledge, though, that a reliable system must meet the text span match requirement to the greatest extent possible.</p><p>We intend this baseline to emulate an expert's identification of specialized knowledge in a text without any external resources. The task of accurately delimiting a term in context, or a disease mention in this case, is hard, not only for machines but also for human experts. <ref type="bibr" coords="5,389.71,693.72,16.89,9.94" target="#b18">[19]</ref>, for example, proved that human experts are good at identifying approximate windows of specialized mentions in context, but they do not perform as well to determine the starting and ending position of the term. We would expect a language model properly fine-tuned in combination with external resources such as dictionaries, ontologies, grammars, taggers, etc. to dramatically outperform our baseline, just like an expert with similar resources would do.</p><p>The scarce literature on disease mention detection in Spanish also suggests that there is still much to do in this line. We chose mBERT as we think it fairly represents the Spanish language, but some languages, e.g., English, may benefit more from its subword tokenizer. For example, many disease mentions in English of the pattern noun + noun are translated into Spanish as noun + de + noun. It seems the model learns that de is a prototypical part of complex terms in Spanish and it may explain the extended text span in the last example in Table <ref type="table" coords="6,297.52,137.92,4.14,9.94" target="#tab_3">5</ref>.</p><p>Lastly, in these results we also need to factor in the tokenization issues due to our need to handle the BIO scheme and to the way some acronyms and punctuation interacted with disease mentions. Of course, the combination of all the involved variables may have also impacted the model's performance, that is, the model itself and its configuration during training, which may have not built the best word representation for extracting disease mentions, the data, and the pre-and post-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, we propose a baseline for disease mention identification in a corpus of clinical cases in Spanish. It is our contribution to the sub-track 1 of DisTEMIST shared task. Our system is based on the transfer learning technique using the multilingual version of the well-known language model called BERT. Besides the simple post-processing that we carried out, the fine-tuned model reported here can be considered a strong baseline (Micro F1=5456) since it does not use any external resources.</p><p>Our qualitative analysis suggests that the model performs well to identify disease mentions, but that accuracy in text span delimitation needs improvement. An effective use of external resources, such as the ones made available by DisTEMIST organizers, should help fixing text span truncation and extension and improving recall and precision. However, the system, as described here, can be used as a starting point for an information extraction system in the medical field.</p><p>The optimization of subword tokenization for Spanish is an open line for future work. We think such an improvement would boost performance and would decrease the need for post-processing. Likewise, we plan to use the baseline proposed here and the state of the art from DisTEMIST and other sources to set up a competitive disease mention identification system. We plan to integrate domain-specific resources as well as to replicate and strengthen our experiments with other language models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,90.02,716.04,332.27,47.85"><head>Table 1</head><label>1</label><figDesc>off0: starting position of the mention in the document • off1: ending position of the mention in the document • span: text span</figDesc><table coords="3,72.02,137.42,442.03,81.96"><row><cell>Table 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset sample</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>filename</cell><cell>mark</cell><cell>label</cell><cell>off0</cell><cell>off1</cell><cell>span</cell></row><row><cell>file_1</cell><cell>T1</cell><cell>ENFERMEDAD</cell><cell>89</cell><cell>111</cell><cell>síndrome postflebítico</cell></row><row><cell>file_1</cell><cell>T2</cell><cell>ENFERMEDAD</cell><cell>234</cell><cell>246</cell><cell>sepsis grave</cell></row><row><cell>file_1</cell><cell>T3</cell><cell>ENFERMEDAD</cell><cell>285</cell><cell>288</cell><cell>HTA</cell></row></table><note coords="2,414.15,716.04,8.14,9.94;2,90.02,741.24,131.72,9.94;2,90.02,753.96,136.88,9.94;3,90.02,74.66,188.80,9.94;3,90.02,87.92,4.58,9.05"><p><p><p>):</p>• filename: document name • mark: identifier mention id</p>• label: mention type (ENFERMEDAD) •</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,72.02,74.16,425.91,135.62"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="4,72.02,87.62,425.91,122.16"><row><cell>Chosen hyperparameter configuration</cell><cell></cell></row><row><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell>learning_rate</cell><cell>5e-5</cell></row><row><cell>train_batch_size</cell><cell>8</cell></row><row><cell>seed</cell><cell>42</cell></row><row><cell>optimizer</cell><cell>Adam betas=(0.9,0.999); epsilon=1e-08</cell></row><row><cell>lr_schedler_type</cell><cell>linear</cell></row><row><cell>num_epochs</cell><cell>7</cell></row><row><cell>eval_batch_size</cell><cell>8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,72.02,213.26,434.65,81.99"><head>Table 4</head><label>4</label><figDesc>Results for training and test</figDesc><table coords="5,88.46,242.42,418.21,52.83"><row><cell></cell><cell></cell><cell>Training</cell><cell></cell><cell></cell><cell>Testing</cell><cell></cell></row><row><cell>Model</cell><cell>MiP</cell><cell>MiR</cell><cell>MiF1</cell><cell>MiP</cell><cell>MiR</cell><cell>MiF1</cell></row><row><cell>mBERT</cell><cell>0.5637</cell><cell>0.5801</cell><cell>0.5718</cell><cell>0.6095</cell><cell>0.4619</cell><cell>0.5456</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,72.02,465.19,439.13,122.16"><head>Table 5 Error</head><label>5</label><figDesc></figDesc><table coords="5,83.66,478.51,427.49,108.84"><row><cell cols="2">analysis -Truncated (t), extended (e), and missed (-) disease mention examples</cell></row><row><cell>Gold Standard</cell><cell>Model's prediction</cell></row><row><cell>Amaurosis</cell><cell>amaurosis de uno o ambos ojos (e)</cell></row><row><cell>hemihipoestesia derecha</cell><cell>hemihipoestesia (t)</cell></row><row><cell>nido vascular epicraneal frontal izquierdo</cell><cell>-</cell></row><row><cell>complicaciones neurológicas</cell><cell>-</cell></row><row><cell cols="2">hematoma protuberancial posterior derecho hematoma protuberancial posterior derecho de</cell></row><row><cell></cell><cell>(e)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">Acknowledgements</head><p>This work was done with partial support from the <rs type="funder">Mexican Government</rs> through the grant <rs type="grantNumber">A1-S-47854</rs> of <rs type="funder">CONACYT, Mexico</rs>, grants <rs type="grantNumber">20220852</rs> and <rs type="grantNumber">20220859</rs> of the <rs type="funder">Secretaría de Investigación y Posgrado of the Instituto Politécnico Nacional, Mexico</rs>. The authors thank the <rs type="institution">CONACYT</rs> for the computing resources brought to them through the <rs type="institution">Plataforma de Aprendizaje Profundo para Tecnologías del Lenguaje of the Laboratorio de Supercómputo of the INAOE, Mexico</rs> and acknowledge the support of Microsoft through the <rs type="grantName">Microsoft Latin America PhD Award</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HQSZa8X">
					<idno type="grant-number">A1-S-47854</idno>
				</org>
				<org type="funding" xml:id="_bX3wFNc">
					<idno type="grant-number">20220852</idno>
				</org>
				<org type="funding" xml:id="_EzJBUXb">
					<idno type="grant-number">20220859</idno>
				</org>
				<org type="funding" xml:id="_QXFaGNt">
					<orgName type="grant-name">Microsoft Latin America PhD Award</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,108.02,614.76,415.13,9.94;6,108.02,627.48,289.97,9.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,274.44,614.76,212.81,9.94">Nested Named Entity Recognition: A Survey</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,498.10,614.76,25.05,9.94;6,108.02,627.48,219.06,9.94">ACM Transactions on Knowledge Discovery from Data</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>TKDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,108.02,640.08,414.96,9.94;6,108.02,652.80,206.43,9.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,202.63,640.08,257.56,9.94">Medical information extraction in the age of deep learning</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Oleynik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,467.38,640.08,55.60,9.94;6,108.02,652.80,86.03,9.94">Yearbook of medical informatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="208" to="220" />
			<date type="published" when="2020-08">2020 Aug</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,108.02,665.40,415.52,9.94;6,108.02,678.00,415.45,9.94;6,108.02,690.72,415.13,9.94;6,108.02,703.32,415.46,9.94;6,108.02,716.04,286.83,9.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,351.54,678.00,171.93,9.94;6,108.02,690.72,415.13,9.94;6,108.02,703.32,174.72,9.94">Overview of DISTEMIST at BioASQ: Automatic detection and normalization of diseases from clinical texts: results, methods, evaluation and multilingual resources</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miranda-Escalada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gascó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lima-López</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Farré-Maduell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,293.51,703.32,229.97,9.94;6,108.02,716.04,114.23,9.94">Working Notes of Conference and Labs of the Evaluation (CLEF) Forum</title>
		<title level="s" coord="6,230.12,716.04,132.52,9.94">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,108.02,728.64,415.15,9.94;6,108.02,741.24,415.26,9.94;6,108.02,753.96,407.48,9.94" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,343.92,728.64,179.26,9.94;6,108.02,741.24,415.26,9.94;6,108.02,753.96,232.75,9.94">Named Entity Recognition, Concept Normalization and Clinical Coding: Overview of the Cantemist Track for Cancer Text Mining in Spanish, Corpus, Guidelines, Methods and Results</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Miranda-Escalada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Farré</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,347.93,753.96,80.35,9.94">IberLEF@ SEPLN</title>
		<imprint>
			<biblScope unit="page" from="303" to="323" />
			<date type="published" when="2020-09">2020 Sep</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,103.58,74.66,419.79,9.94;7,103.58,87.28,383.74,9.94" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="7,237.41,74.66,285.97,9.94;7,103.58,87.28,199.43,9.94">Text chunking using transformation-based learning. InNatural language processing using very large corpora</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="157" to="176" />
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,103.58,99.89,419.80,10.05;7,103.58,112.60,419.77,9.94;7,103.58,125.32,87.22,9.94" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,297.72,99.89,225.66,10.05;7,103.58,112.60,310.23,9.94">A rule-based named-entity recognition method for knowledge extraction of evidence-based dietary recommendations</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Eftimov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Koroušić</forename><surname>Seljak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Korošec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,425.84,112.60,41.65,9.94">PloS one</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">179488</biblScope>
			<date type="published" when="2017-06-23">2017 Jun 23</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,103.58,137.92,419.87,9.94;7,103.58,150.52,189.16,9.94" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,194.01,137.92,303.22,9.94">A kernel-based approach for biomedical named entity recognition</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,506.43,137.92,17.02,9.94;7,103.58,150.52,106.55,9.94">The Scientific World Journal</title>
		<imprint>
			<date type="published" when="2013-01-01">2013 Jan 1. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,103.58,163.24,419.80,9.94;7,103.58,175.84,419.70,9.94;7,103.58,188.56,13.80,9.94" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="7,198.52,163.24,324.85,9.94;7,103.58,175.84,221.69,9.94">Transfer learning in biomedical natural language processing: an evaluation of BERT and ELMo on ten benchmarking datasets</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05474</idno>
		<imprint>
			<date type="published" when="2019-06-13">2019 Jun 13</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,103.58,201.16,419.80,9.94;7,103.58,213.76,334.18,9.94" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="7,296.00,201.16,227.37,9.94;7,103.58,213.76,118.41,9.94">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805.2018</idno>
		<imprint>
			<date>Oct 11</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,108.02,226.48,415.24,9.94;7,103.58,239.08,409.77,9.94" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="7,500.11,226.48,23.15,9.94;7,103.58,239.08,157.34,9.94">Deep contextualized word representations</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365.1802;12</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.02,251.80,415.07,9.94;7,103.58,264.43,419.93,9.94;7,103.58,277.15,210.01,9.94" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,289.69,251.80,233.40,9.94;7,103.58,264.43,256.15,9.94">Effective use of bidirectional language modeling for transfer learning in biomedical named entity recognition</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">S</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,368.45,264.43,155.06,9.94;7,103.58,277.15,48.31,9.94">InMachine learning for healthcare conference</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-11-29">2018 Nov 29</date>
			<biblScope unit="page" from="383" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.02,289.75,415.41,9.94;7,103.58,302.35,419.61,9.94;7,103.58,315.07,80.52,9.94" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="7,363.88,289.75,159.55,9.94;7,103.58,302.35,284.15,9.94">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,400.38,302.35,64.11,9.94">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020-02-15">2020 Feb 15</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.02,327.67,415.18,9.94;7,103.58,340.39,419.60,9.94;7,103.58,352.99,70.69,9.94" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="7,181.18,327.67,342.01,9.94;7,103.58,340.39,45.82,9.94">Named entity recognition in Spanish biomedical literature: Short review and bert model</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Akhtyamova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,157.91,340.39,304.95,9.94">2020 26th Conference of Open Innovations Association (FRUCT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-04-20">2020 Apr 20</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.02,365.59,415.52,9.94;7,103.58,378.31,371.14,9.94" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="7,486.58,365.59,36.95,9.94;7,103.58,378.31,159.35,9.94">Publicly available clinical BERT embeddings</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">H</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jin</forename><forename type="middle">D</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03323.2019</idno>
		<imprint>
			<date>Apr 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,108.02,390.91,415.18,9.94;7,103.58,403.63,419.89,9.94;7,103.58,416.23,281.44,9.94" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="7,338.84,390.91,184.36,9.94;7,103.58,403.63,259.93,9.94">The Chilean Waiting List Corpus: a new resource for clinical named entity recognition in Spanish</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Báez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Villena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Durán</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dunstan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,372.75,403.63,150.73,9.94;7,103.58,416.23,169.00,9.94">InProceedings of the 3rd clinical natural language processing workshop</title>
		<imprint>
			<date type="published" when="2020-11">2020 Nov</date>
			<biblScope unit="page" from="291" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.02,428.83,415.31,9.94;7,103.58,441.57,382.93,9.94" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="7,349.30,428.83,174.03,9.94;7,103.58,441.57,130.75,9.94">A joint model for medical named entity recognition and normalization</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Nic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="http://ceur-ws.orgISSN" />
		<imprint>
			<date type="published" when="2020">2020. 0073</date>
			<biblScope unit="volume">1613</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.02,454.17,415.22,9.94;7,103.58,466.89,403.66,9.94" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="7,290.10,454.17,106.44,9.94;7,429.47,454.17,93.77,9.94;7,103.58,466.89,164.07,9.94">InProceedings of the Iberian Languages Evaluation Forum</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García-Pablos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cuadros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,345.17,466.89,134.47,9.94">CEUR Workshop Proceedings</title>
		<meeting><address><addrLine>IberLEF</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020. 2020</date>
		</imprint>
	</monogr>
	<note>Vicomtech at cantemist</note>
</biblStruct>

<biblStruct coords="7,108.02,479.49,415.18,9.94;7,103.58,492.09,343.85,9.94" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="7,220.39,479.49,302.82,9.94;7,103.58,492.09,240.58,9.94">spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Montani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-07">2017 Jul</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="411" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,108.02,504.81,415.18,9.94;7,103.58,517.41,419.76,9.94;7,103.58,530.13,419.74,9.94;7,103.58,542.73,64.56,9.94" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="7,161.05,530.13,358.11,9.94">La identificación de unidades terminológicas en contexto: de la teoría a la práctica</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Estopà</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Burgos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Monserrat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Montané</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Quispe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rivadeneira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sabater</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Samara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Santis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Seghezzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Souto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1000" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
