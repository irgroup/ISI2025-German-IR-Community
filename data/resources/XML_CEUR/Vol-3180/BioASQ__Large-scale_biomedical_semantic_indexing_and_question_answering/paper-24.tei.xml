<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,358.55,15.42;1,89.29,106.66,322.03,15.42">ELECTROLBERT: Combining Replaced Token Detection and Sentence Order Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,89.29,134.97,71.20,11.96"><forename type="first">Martin</forename><surname>Reczko</surname></persName>
							<email>reczko@fleming.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Fundamental Biomedical Science</orgName>
								<orgName type="institution">Biomedical Sciences Research Center &quot;Alexander Fleming&quot;</orgName>
								<address>
									<addrLine>34 Fleming Street</addrLine>
									<postCode>16672</postCode>
									<settlement>Vari</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,358.55,15.42;1,89.29,106.66,322.03,15.42">ELECTROLBERT: Combining Replaced Token Detection and Sentence Order Prediction</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">10D05B3AB3A7016A99AB074770141A95</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Biomedical Question Answering</term>
					<term>ELECTRA</term>
					<term>ALBERT</term>
					<term>BioASQ</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ELECTROLBERT is a novel transformer algorithm that combines the replaced token prediction of the ELECTRA system <ref type="bibr" coords="1,198.64,223.09,10.55,8.97" target="#b0">[1]</ref> with the sentence order prediction used in the ALBERT system <ref type="bibr" coords="1,446.12,223.09,9.39,8.97" target="#b1">[2]</ref>. As reported, the default next sentence prediction component used in most BERT-based transformers has the drawback that a random next sentence can be easily predicted based on its different scope. The sentence order prediction facilitates the detection of semantic flow and is well suited for finetuning question answering systems, as the pairing of a question with text related to the correct answer resembles the correct order of two sentences in a scientific text. The implementation of ELECROLBERT is based on the BioELECTRA [3] code. ELECTROLBERT is pretrained on the 2022 baseline set of all PubMed abstracts provided by the National Library of Medicine and two predictors are finetuned using pairs of relevant and non-relevant question-abstract pairs for document prediction and examples for the "yes/no" type questions, both generated using the BioASQ10 training dataset <ref type="bibr" coords="1,388.52,321.72,9.39,8.97" target="#b3">[4]</ref>. For each novel question in the document prediction task of BioASQ10, 6750 Pubmed abstracts are filtered for processing by ELEC-TROLBERT from all Pubmed abstracts using a combination of the GENSIM topic modelling system [5] and a simple infrequent word detection method. The system was continuously improved during the BioASQ10 competition and in the last batch, ELECTROLBERT ranked as the 3 ùëüùëë team for document prediction and 1 ùë†ùë° place and team for the "yes/no" type questions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Autoencoders with attention mechanisms led to impressive improvements in many natural language processing tasks <ref type="bibr" coords="1,191.53,491.76,11.28,10.91" target="#b5">[6]</ref>. A computationally efficient approach is called ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately) <ref type="bibr" coords="1,405.11,505.31,13.00,10.91" target="#b0">[1]</ref> which replaces the traditional reconstruction of masked tokens from their context as introduced in BERT <ref type="bibr" coords="1,470.27,518.86,12.76,10.91" target="#b6">[7]</ref> with a simpler identification of replaced tokens that are generated by an adaptive generator. Another successful algorithm is the ALBERT (A Lite BERT) transformer that incorporates several parameter reduction techniques and replaces the next sentence prediction (NSP) component of BERT with a sentence order prediction (SOP) component that improves the detection of inter-sentence coherence. In the transformer called ELECTROLBERT (ELECTR-A-LBERT) introduced here, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>34894155</head><p>The question to ask is, is this prescribed load regimen congruent with Wolff's law, and does it provide an adequate mechanical stimulus to maintain the functional health of periodontal complex? This question was answered by studying the effects of mice chewing on soft food (SF) and hard food (HF) while undergoing experimental tooth movement (ETM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>34893939</head><p>Will Artificial Intelligence (AI) re-humanize or de-humanize medicine? As AI becomes pervasive in clinical medicine, we argue that the ethical framework that sustains a responsible implementation of such technologies should be reconsidered.</p><p>the SOP component of ALBERT is combined with the ELECTRA approach that is missing any component to predict inter-sentence relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Motivation</head><p>NSP is defined as a prediction if two sentences are consecutive in the training texts. The negative case uses sentences from two different documents. During the design of ALBERT, it was shown that a BERT-style NSP essentially fails on the SOP task, while SOP as used in ALBERT also has reasonable performance on the NSP problem. The conclusion was that NSP solves the easier task of predicting the topic-shift between the two segments and fails to model the discourse-level coherence between consecutive sentences. As shown in Table <ref type="table" coords="2,381.08,480.01,3.81,10.91" target="#tab_0">1</ref>, it is not uncommon that scientific texts contain consecutive segments with a question and an answer. An efficient SOP would directly identify these cases as correct answers and finetuning a question-document relevance prediction could effectively reuse the parts of the embedding that the SOP developed during training. The embedding obtained with NSP based on detecting topic-shift trends to accept all answers containing a topic similar to the question, but do not necessarily answer the question. An overview of the system architecture is shown in Figure <ref type="figure" coords="2,396.83,561.31,3.74,10.91" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data generation and training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Initial document retrieval</head><p>As the complete corpus cannot be processed by the transformer for each question in reasonable time, a computationally efficient filter has to filter the corpus according to each question. To this end, the TF/IDF based free topic modelling system GENSIM <ref type="bibr" coords="2,347.31,668.00,12.68,10.91" target="#b4">[5]</ref> (with logarithmic term frequency weighting, IDF document frequency weighting and no document normalization) is used to return a list of 6750 documents relevant for each question to be processed by ELECTROLBERT.</p><p>A question in the fifth batch was "What is Waylivra?" (id:626aea6ae764a5320400003d) with the answer in the document with PMID 3130103: "Volanesorsen (Waylivra¬Æ), an antisense oligonucleotide inhibitor of . . . ". As the special 'registered trademark' character "¬Æ" was included in the dictionary entry for this word, but not in the question, the document was not detected in my submission. A new dictionary and index was constructed omitting the special characters ¬Æ,¬© and ‚Ñ¢.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Infrequent words treatment</head><p>Processing the corpus of 22542347 pubmed abstracts and titles, a dictionary with 5440825 words is obtained. To ensure the detection of infrequent words that would not be detected using the TF/IDF score, a list of documents is constructed for each word occurring less than 4000 times in the complete corpus. These words cover 99.55% of the dictionary. In case a question contains one of the these infrequent words, the union of the lists of documents for each infrequent word occurring in the question is appended to the list of documents detected by TF/IDF for evaluation by ELECTROLBERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pretraining data</head><p>Pretraining data was generated using a modified version of the ALBERT code. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Pretraining</head><p>For subsequent relevance prediction, a model with the 'base' configuration of BioELECTRA is pretrained due to computational limitations. This corresponds to an embedding size of 768, a generator hidden size of 256 and 12 hidden layers. The model was trained for 1.5 million steps using the ADAM algorithm (ùõΩ 1 = 0.9, ùõΩ 2 = 0.999, ùúñ = 10 -5 , learning_rate = 10 -4 , weight_decay_rate = 0.005, max_seq_length = 128, train_batch_size = 14). The final replaced token discriminator has an area under the the curve (AUC) of 0.92 ¬± 0.0087. For finetuning the "Yes/No" task, a 'large' configuration is pretrained for 5 million steps with an embedding size of 1024, a generator hidden size of 256 and 24 hidden layers (ùõΩ 1 = 0.9, ùõΩ 2 = 0.999, ùúñ = 10 -5 , learning_rate = 10 -4 , weight_decay_rate = 0.0025, max_seq_length = 128, train_batch_size = 12). It should be noted that the training of the 'large' model did not yet converge after 5 million steps, with an discriminator AUC of 0.85 ¬± 0.001. GPU memory restrictions required the smaller train_batch_size for the 'large' configuration. As known, smaller batch_sizes lead to less accurate gradient calculations. This, in combination with the larger number of parameters to optimize for the 'large' configuration ('large': 335 million vs 'base': 110 million) could explain the slow convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Document relevance prediction finetuning</head><p>As initial question answering is focused on the "yes/no" type questions, 1148 questions of this type are extracted from the BioASQ10 training set (873 "yes", 275 "no"). All questions with shared answer documents are grouped into equivalence sets. Initially, all documents relevant for a questions are used as positive relevance examples and 2% of all documents not contained in the equivalence set for a question are used as negative relevance examples, leading to an approximate negative to positive example ratio of 20 to 1. As negative examples are generated from completely unrelated questions, the class boundaries are distant. To better discriminate the relevant documents obtained with GENSIM topic modelling, construction of the negative example was modified after the third batch of BioASQ10. All questions of the relevance training set were processed with GENSIM to produce 1000 documents for each question. The documents were ranked according to their TF/IDF score and all documents between rank 750 and 875 were used as negative examples, excluding potential positive examples in these ranks. The values of the start and end rank positions for the negative set was optimized by retraining and maximizing the mean average precision measured on the second batch.</p><p>The model was trained for 18768 steps using the ADAM algorithm (ùõΩ 1 = 0.9, ùõΩ 2 = 0.999, ùúñ = 10 -5 , learning_rate = 2 ‚Ä¢ 10 -6 , weight_decay_rate = 0.003, max_seq_length = 200, train_batch_size = 16).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">"Yes/No" prediction finetuning</head><p>A training set of 70% of the "Yes/No" type questions was extracted from the BioASQ10 training set. To prevent trivial tests, the training set, the 10% validation set and the 20% test set all contain questions that are not in the equivalence set of any question in all other sets. The model was trained for 88260 steps using the ADAM algorithm (ùõΩ 1 = 0.9, ùõΩ 2 = 0.999, ùúñ = 10 -5 , learn-ing_rate = 4 ‚Ä¢ 10 -5 , weight_decay_rate = 0.008, max_seq_length = 256, train_batch_size = 4).</p><p>To classify a question, all documents relevant for the question are scored by ELECTROLBERT.</p><p>If the average score over all documents exceeds a fixed threshold, the answer to the question is "Yes". In the opposite case, an additional requirement to assign "No" to a question is a certain degree of unanimity in the scores of all relevant documents. Only if the variance of all scores is below a fixed threshold, the final answer is negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Fast abstract retrieval</head><p>A custom R script accessing chunks of 100000 abstracts based on their PMIDs can retrieve and store &gt; 3000 abstracts with titles per minute using 15 CPUs. This step is one of the major bottlenecks to guarantee the processing of 100 questions within 24 hours without the use of large compute clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">BioASQ10 results</head><p>The performance of ELECTROLBERT for each batch of BioASQ10 is listed in Table <ref type="table" coords="5,463.19,596.90,3.77,10.91" target="#tab_2">2</ref>. For the document retrieval task, the submissions that reflect the different development stages are listed as well as a retrospective analysis for batches 1 to 4 that is obtained with the final system used in batch 5 and 6. The improvements during the development of the system are obvious.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Next Steps</head><p>All document retrieval results of ELECTROLBERT have been obtained with the 'base' sized architecture and are thus promising, fueling the expectation of competitive performance once training of the 'large' architecture has converged. ELECTROLBERT will be finetuned for the snippet, factoid and list tasks. Transfer learning from SOP to relevance prediction and from relevance prediction to the "Yes/No" task will be evaluated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,302.76,418.36,8.93;3,89.29,314.76,324.64,8.87;3,89.29,84.19,416.66,211.98"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The ELECTROLBERT system. The components above the blue line are used during pretraining. During question answering, all components except the Generator are used.</figDesc><graphic coords="3,89.29,84.19,416.66,211.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,88.99,90.49,418.66,122.24"><head>Table 1</head><label>1</label><figDesc>Question-Answer pairs in real abstracts. Coherence detected by sentence order prediction (SOP) facilitates answer identification. The topic-shift detection obtained with next sentence prediction (NSP) is less specific.</figDesc><table coords="2,104.15,146.01,385.14,66.71"><row><cell>PMID</cell><cell>Question Answer</cell></row><row><cell>34884907</cell><cell></cell></row><row><cell></cell><cell>Neurogenic Inflammation in the Context of Endometriosis-What Do We Know? En-</cell></row><row><cell></cell><cell>dometriosis (EM) is an estrogen-dependent disease characterized by the presence of ep-</cell></row><row><cell></cell><cell>ithelial, stromal, and smooth muscle cells outside the uterine cavity.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,88.89,627.91,418.30,51.56"><head></head><label></label><figDesc>To emphasize sentence order prediction and subsequent question answering, all examples have two segments, where sentences longer than half of the maximal sequence length without separators are truncated to this length. In 50% of the examples, the sentence order is swapped. Instead of a random 10% fraction of the examples having only a single segment, no single segment examples are used. The vocabulary available in BioELECTRA<ref type="bibr" coords="4,315.76,100.52,12.68,10.91" target="#b2">[3]</ref> containing 31620 entries 1 is used and all training texts are lowercase. The titles and abstracts are extracted from the pubmed baseline xml files with a custom R script based on the xml2 package.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,88.99,90.49,416.99,52.45"><head>Table 2</head><label>2</label><figDesc>BioASQ10 prediction performance: Document relevance and "yes/no" task. For documents, the mean average precision (MAP) is used for evaluation.</figDesc><table coords="5,227.53,134.01,172.88,8.93"><row><cell>documents</cell><cell>"yes/</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,107.49,134.01,380.30,115.52"><head>no" type questions BioASQ submission final system batch MAP per team rank MAP per team rank accuracy per team rank</head><label></label><figDesc></figDesc><table coords="5,115.92,180.88,344.57,68.65"><row><cell>1</cell><cell>0.1121</cell><cell>7</cell><cell>0.3649</cell><cell>5</cell><cell>-</cell><cell>-</cell></row><row><cell>2</cell><cell>0.1632</cell><cell>9</cell><cell>0.3090</cell><cell>5</cell><cell>-</cell><cell>-</cell></row><row><cell>3</cell><cell>0.3209</cell><cell>8</cell><cell>0.3666</cell><cell>6</cell><cell>0.76</cell><cell>10</cell></row><row><cell>4</cell><cell>0.3101</cell><cell>6</cell><cell>0.3140</cell><cell>6</cell><cell>0.75</cell><cell>11</cell></row><row><cell>5</cell><cell>0.3242</cell><cell>4</cell><cell>0.3242</cell><cell>4</cell><cell>0.6429</cell><cell>10</cell></row><row><cell>6 2</cell><cell>0.0977</cell><cell>3</cell><cell>0.0977</cell><cell>3</cell><cell>1.0</cell><cell>1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,108.93,660.07,390.74,8.97;4,89.29,671.03,10.06,8.97"><p>https://github.com/SciCrunch/bio_electra/blob/master/electra/data/pmc_2017_abstracts_wp_vocab_sorted. txt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,108.93,660.08,397.06,8.97;5,89.29,671.04,378.97,8.97"><p>Batch 6 consisted of questions posed by new biomedical experts interested in material and answers that can be automatically provided by state-of-the-art IR and QA systems. It is not part of the official evaluation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,108.93,671.03,97.69,8.97"><p>https://hypatia.athenarc.gr</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>GPU computations were offered by HYPATIA 3 , the Cloud infrastructure that supports the computational needs of the Greek ELIXIR community.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="6,107.59,296.28,398.40,10.91;6,107.59,309.83,402.55,10.91;6,107.29,325.82,132.96,7.90" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,332.75,296.28,173.24,10.91;6,107.59,309.83,164.67,10.91">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2003.10555</idno>
		<ptr target="https://arxiv.org/abs/2003.10555.doi:10.48550/ARXIV.2003.10555" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,336.93,400.24,10.91;6,107.59,350.47,400.08,10.91;6,107.59,364.02,167.31,10.91" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,391.21,336.93,116.62,10.91;6,107.59,350.47,204.82,10.91">Albert: A lite bert for selfsupervised learning of language representations</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1909.11942</idno>
		<ptr target="https://arxiv.org/abs/1909.11942.doi:10.48550/ARXIV.1909.11942" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,377.57,398.40,10.91;6,107.59,391.12,398.40,10.91;6,107.59,404.67,400.08,10.91;6,107.59,418.22,398.49,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,343.87,377.57,162.12,10.91;6,107.59,391.12,150.22,10.91">BioELECTRA:pretrained biomedical text encoder using discriminators</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">R</forename><surname>Kanakarajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kundumani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sankarasubbu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.bionlp-1.16</idno>
		<ptr target="https://aclanthology.org/2021.bionlp-1.16.doi:10.18653/v1/2021.bionlp-1.16" />
	</analytic>
	<monogr>
		<title level="m" coord="6,281.59,391.12,224.39,10.91;6,107.59,404.67,279.69,10.91">Proceedings of the 20th Workshop on Biomedical Language Processing, Association for Computational Linguistics</title>
		<meeting>the 20th Workshop on Biomedical Language Processing, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="143" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,431.77,399.60,10.91;6,107.59,445.32,398.40,10.91;6,107.59,458.87,400.24,10.91;6,107.59,472.42,400.08,10.91;6,107.59,485.97,187.11,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,107.59,445.32,398.40,10.91;6,107.59,458.87,150.34,10.91">Overview of bioasq 2021: The ninth bioasq challenge on large-scale biomedical semantic indexing and question answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Vandorou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gasco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paliouras</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-85251-1_18</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-030-85251-1_18.doi:10.1007/978-3-030-85251-1_18" />
	</analytic>
	<monogr>
		<title level="j" coord="6,269.32,458.87,238.50,10.91;6,107.59,472.42,85.47,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<imprint>
			<biblScope unit="page" from="239" to="263" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,499.52,399.60,10.91;6,107.59,513.06,330.94,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="6,202.77,499.52,241.52,10.91">Gensim-python framework for vector space modelling</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rehurek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sojka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">3</biblScope>
			<pubPlace>Brno, Czech Republic</pubPlace>
		</imprint>
		<respStmt>
			<orgName>NLP Centre, Faculty of Informatics, Masaryk University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,526.61,399.59,10.91;6,107.59,540.16,364.20,10.91" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="6,236.34,526.61,266.64,10.91">Deberta: Decoding-enhanced bert with disentangled attention</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2006.03654</idno>
		<ptr target="https://arxiv.org/abs/2006.03654.doi:10.48550/ARXIV.2006.03654" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,553.71,398.40,10.91;6,107.59,567.26,400.08,10.91;6,107.59,580.81,167.31,10.91" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="6,351.81,553.71,154.18,10.91;6,107.59,567.26,189.33,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1810.04805</idno>
		<ptr target="https://arxiv.org/abs/1810.04805.doi:10.48550/ARXIV.1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
