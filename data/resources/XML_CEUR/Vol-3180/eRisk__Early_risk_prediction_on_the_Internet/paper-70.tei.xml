<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,80.81,327.42,19.63;1,89.29,102.73,379.32,19.63;1,88.59,124.65,174.98,19.63">CLEF eRisk 2022: Detecting Early Signs of Pathological Gambling using ML and DL models with dataset chunking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,89.29,155.26,127.13,13.63"><forename type="first">Tudor-Andrei</forename><surname>Dumitra»ôcu</surname></persName>
							<email>dumitrascu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,80.81,327.42,19.63;1,89.29,102.73,379.32,19.63;1,88.59,124.65,174.98,19.63">CLEF eRisk 2022: Detecting Early Signs of Pathological Gambling using ML and DL models with dataset chunking</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">6A025860181BB8022864F6598AAB9284</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>machine learning</term>
					<term>deep learning</term>
					<term>dataset processing</term>
					<term>chunking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The first shared task of CLEF eRisk 2022 proposes to investigate how gambling addiction can be detected from texts posted by users on the social platforms. It is an interesting proposal because the connection between language and mental health is studied for many years, and interesting NLP insights could be revealed in this case. Any finding can be used in the early treatment of a mental disease, avoiding the chances of developing a serious condition. Our approach to this task includes both standard Machine Learning and Deep Learning solutions applied to the provided dataset. We then modified the dataset in different ways in order to find better correlations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>If early signs of any pathological behaviour would be transparent to us, most of the suffering caused by diseases can be minimized. Any preventive intervention is beneficial to the overall health of an individual. Pathological gambling affects the quality of life of anyone who excessively engages in casino/sports betting and video games activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The current task is a continuation of the previous year's T1 task. The training data is composed of all 2021's T1 test users. In 2021 no training data was provided to the participants and they needed to search online sources and to create a training data set from scratch.</p><p>Reddit is a social platform that allows researchers to use the public data it hosts for academic purposes. The work of Bucur et al. <ref type="bibr" coords="1,255.86,542.67,11.43,12.44" target="#b0">[1]</ref>, the last year's participating BLUE team, included a training data set constructed from the r/GamblingAddiction and r/problemgambling subreddits as positive examples. The number of negative examples was chosen to match the number of posts in the positive class and it was gathered from various users active on other subreddits.</p><p>The authors fine-tuned a pre-trained BERT classifier with an AdamW optimizer having a learning rate of 0.0002. In predicting the positive class, they used an aggressive threshold of 0.99 and 0.98 on the output probability to decide if the user is at risk of developing a gambling addiction. The purpose of this high threshold is to minimize false positives, ensuring a decision will be made only if a post contains any addiction indicators.</p><p>The results they obtained had a high recall score with a low precision score and low F1 score. They concluded that the problem of predicting the early signs of gambling addiction is far from being solved and further research is strongly needed.</p><p>The work of Maupone et al. <ref type="bibr" coords="2,222.89,193.79,10.93,12.44" target="#b1">[2]</ref>, the last year's participating RELAI team, also included creating a data set from scratch using social media posts for both training and making predictions. They used two authorship attribution approaches that assess whether a test users belongs to two different sets of gambling users.</p><p>The first set of gambling users was constructed from testimonials published on the Gambler's Help site <ref type="foot" coords="2,132.36,259.39,4.06,9.95" target="#foot_0">1</ref> . The second set was constructed based on questionnaires found on websites such as Gamblers Anonymous Montreal<ref type="foot" coords="2,247.38,272.94,4.06,9.95" target="#foot_1">2</ref> .</p><p>The Embedding Topic Model (ETM) presented in the article written by Dieng et al. <ref type="bibr" coords="2,472.94,288.63,12.84,12.44" target="#b2">[3]</ref> was the authors' model selection for extracting the topics present in their training corpus. ETM is a model that combines traditional topic models such as Latent Dirichlet Allocation (LDA) with word embeddings (word2vec). RELAI team's results were similar to the BLUE team's results, obtaining a high recall score, but a low precision score and F1 score, further enforcing the need for additional NLP research in this mental health area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset</head><p>The dataset consists of multiple XML files containing the user posts, timestamps for each post and the title of the post. In order to use the data in a meaningful manner, we extracted the user posts using three different methods:</p><p>1. Combining all the posts into a single text file. The number of files is equal to the number of users. 2. Each user posts is saved in file, seen as an individual sample which inherits the label from the user. This results in a great number of individual files and each file of various length. 3. Chunking the user posts into fixed size lengths and create multiple entries for the minority class. This is detailed in subsection 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Chunking</head><p>In order to balance the disparity of the two classes, one method we have implemented was to split the users into sub users using a chunking algorithm. Due to the fact that collecting all posts of a user in a single file wasn't feasible because the length of the posts would vary greatly, we decided to split the texts into fixed sizes, defined as Max Len. For users with label 1, we also split the whole text into chunks of Max Len but the starting point of the next chunk is defined by a Chunk Delay. Meaning that ùëà ùëÜùê∏ùëÖ ùëå _1 would retain text from ùëà ùëÜùê∏ùëÖ ùëå _0. We did this in order to capture more information about gambling addiction. At the end of the processing we ended up with 22k positive examples and 44k negative examples. A visual representation of the algorithm can be seen in Figure <ref type="figure" coords="3,249.48,153.14,3.74,12.44" target="#fig_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Machine learning approach</head><p>The Machine Learning approach included the following text normalization techniques applied to the subjects' posts: lowercasing, whitespace removal, URL removal, word tokenization, stopwords removal, punctuation removal and word stemming.</p><p>After processing the provided XML files, the resulting training dataset had an unbalanced number of examples for each of the two classes we needed to make predictions (0=NOT AD-DICTED, 1=ADDICTED). It contained 2184 examples with label 0 and 164 examples with label 1. We also used a stratified 5-fold cross-validation to try to reduce the class imbalance in the train/validation splits. For feature extraction we used the Bag-of-Words (BOW) and Term Frequency-Inverse Document Frequency (TF-IDF) models with additional properties for extracting relevant features such as removing the rare words (min_df=0.2) or frequent words (max_df=0.8), constructing 2-grams and 3-grams.</p><p>In the first run of the ML models, we used the unbalanced dataset with no other modification made to the distribution of the examples across the two classes. The best results we obtained for this unbalanced dataset are presented in Table <ref type="table" coords="3,313.34,387.91,3.74,12.44" target="#tab_0">1</ref>.</p><p>We observed that the scores are too high, and we were skeptic about these results. One reason for such high scores could be the unbalanced nature of the dataset. We also observed that training the models with 2-grams and 3-grams brought down the overall performance in all cases. Also, in some cases, dropping the most frequent words or the rare words improved the results.</p><p>Next, we explored other ways of constructing and balancing the training dataset. We extracted from the collection of posts with label 1 each post in individual data points labeled with 1. From the collection of posts with label 0 we formed chunks of 20 posts in individual examples. The total examples were 54840 with label 1 and 52989 with label 0, resulting in a more balanced dataset. After the second run of the ML models, the results we obtained for this version of the training dataset are presented in Table <ref type="table" coords="3,327.00,536.95,3.74,12.44" target="#tab_1">2</ref>.</p><p>We observed that the new results were similar to the results from the first run, they were also too high. Next, we decided to further modify the training dataset to include for the negative examples (labeled with 0) only the first 25 posts, stored individually (not chunked). We discarded the remaining negative posts from the collection. This approach resulted in a training dataset balanced differently, with the same number of 54840 for positive examples and a total of 52723 posts for the negative examples.</p><p>After the third run of the ML models, the results we obtained were lower than the first two runs and are presented in Table <ref type="table" coords="3,232.35,645.34,3.74,12.44" target="#tab_2">3</ref>.</p><p>We concluded that this third run may not be entirely accurate because it discarded a big part </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Deep learning approach</head><p>For the final implementation of the system, a single deep learning model was used. We started by implementing transformer based systems, with a focus on pretrained BERT based models such as BERT-uncased <ref type="bibr" coords="4,193.78,545.51,12.84,12.44" target="#b3">[4]</ref> RoBERTa <ref type="bibr" coords="4,255.12,545.51,12.85,12.44" target="#b4">[5]</ref> and ALBERT <ref type="bibr" coords="4,333.26,545.51,11.43,12.44" target="#b5">[6]</ref>. Unfortunately, these models were too powerful for the available hardware and it proved to be a real challenge to transfer learning (e.g. the training time for a single epoch for a RoBERTA based model was around 1 hour and for a BERT model was around 3 hours). Due to this issue, we have decided to implement the simple transformer that is very similar to the one presented in the Transformer paper <ref type="bibr" coords="4,467.35,599.70,13.52,12.44" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>When using deep learning models, the best outcome was obtained by a model containing a hidden dimension of 128 of the linear projection, an embedding size of 64 tokens, and four attention heads. The model achieved an F1 Score of 88.8% and a loss of 0.218 after training for 56 epochs. One significant observation is that all the models used a single transformer block. One significant finding is that when more than one transformer block was used, the score dropped to 0%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In conclusion, the problem of assessing the risk of a user for developing a gambling addiction is in strong need for further research. The particularities of the dataset need to be addressed individually, like the high occurrences of web links, emoji symbols or misspelled words. Also, the problem could benefit from collecting new positive data points in order to reduce the data imbalance. Further research could be directed towards including other points of interest, and not only relying on pure text, such as frequency of posting and time of day with respect to the timezone of the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Machine Learning Experiments</head><p>We present additional ML experiments with different values for feature extraction.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Deep Learning Grid Search</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="10,89.29,82.32,173.09,16.35;10,213.46,178.68,168.37,11.36;10,151.80,335.28,291.67,11.36;10,165.26,347.23,278.23,11.36;10,165.26,359.19,155.88,11.36;10,213.81,616.22,167.66,11.36"><head></head><label></label><figDesc>All the aggregated posts for each user. (b) Depending on the label, the posts are split to a maximum length. The posts of users with label 1, contain information from the previous post chunk, depending on the overlap. (c) Final result of the chunking algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,89.29,633.74,306.93,11.36;10,151.80,372.67,291.68,238.11"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A visual explanation of the algorithm described in subsection 3.1</figDesc><graphic coords="10,151.80,372.67,291.68,238.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,89.29,88.21,352.84,99.58"><head>Table 1</head><label>1</label><figDesc>Best results for the ML approach in the first run, on the unbalanced dataset.</figDesc><table coords="4,153.15,116.26,288.99,71.53"><row><cell>ML + feature extractor</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>LinearSVC + TF-IDF (min_df=0.2)</cell><cell>0.99</cell><cell>0.83</cell><cell>0.9</cell></row><row><cell>LogisticRegression + BOW (max_df=0.8)</cell><cell>0.87</cell><cell cols="2">0.87 0.87</cell></row><row><cell>RandomForest + BOW (min_df=0.2)</cell><cell>1.00</cell><cell cols="2">0.46 0.63</cell></row><row><cell>KNeighbors + BOW (max_df=0.8)</cell><cell>0.96</cell><cell cols="2">0.73 0.83</cell></row><row><cell>DecisionTree + TF-IDF</cell><cell>0.9</cell><cell cols="2">0.91 0.91</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,89.29,205.30,330.61,99.58"><head>Table 2</head><label>2</label><figDesc>Best results for the ML approach in the second run, on the balanced dataset.</figDesc><table coords="4,175.38,233.35,244.52,71.53"><row><cell>ML + feature extractor</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>LinearSVC + TF-IDF</cell><cell>0.86</cell><cell cols="2">0.89 0.88</cell></row><row><cell>LogisticRegression + BOW</cell><cell>0.87</cell><cell cols="2">0.96 0.91</cell></row><row><cell>RandomForest + TF-IDF</cell><cell>0.91</cell><cell>0.89</cell><cell>0.9</cell></row><row><cell>KNeighbors + BOW</cell><cell>0.63</cell><cell cols="2">0.87 0.73</cell></row><row><cell>DecisionTree + TF-IDF</cell><cell>0.9</cell><cell cols="2">0.91 0.91</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,89.29,322.40,416.68,149.82"><head>Table 3</head><label>3</label><figDesc>Best results for the ML approach in the third run, on the balanced dataset with discarded posts.</figDesc><table coords="4,172.43,350.44,250.41,59.58"><row><cell>ML + feature extractor</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>LinearSVC + TF-IDF</cell><cell>0.78</cell><cell cols="2">0.66 0.71</cell></row><row><cell>LogisticRegression + TF-IDF</cell><cell>0.79</cell><cell cols="2">0.66 0.72</cell></row><row><cell>KNeighbors + BOW</cell><cell>0.67</cell><cell cols="2">0.55 0.60</cell></row><row><cell>DecisionTree + TF-IDF</cell><cell>0.7</cell><cell cols="2">0.65 0.68</cell></row></table><note coords="4,89.29,432.68,416.68,12.44;4,89.29,446.23,416.68,12.44;4,89.29,459.78,323.57,12.44"><p>of the posts from the 0 labeled collection of posts and information was lost. The problem with the unbalanced dataset needs to be addressed in a different manner and to find new ways of augmenting the positive examples or to collect new examples altogether.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,88.98,137.99,357.08,458.23"><head>Table 4</head><label>4</label><figDesc>Additional results for the ML approach in the second run, on the balanced dataset.</figDesc><table coords="7,149.21,166.04,296.86,430.19"><row><cell>ML + feature extractor</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>LinearSVC + BOW</cell><cell>0.83</cell><cell>0.92</cell><cell>0.87</cell></row><row><cell>LinearSVC + BOW (min_df=0.2)</cell><cell>0.75</cell><cell>0.93</cell><cell>0.83</cell></row><row><cell>LinearSVC + BOW (max_df=0.8)</cell><cell>0.83</cell><cell>0.92</cell><cell>0.87</cell></row><row><cell>LinearSVC + BOW (n_gram=2)</cell><cell>0.95</cell><cell>0.95</cell><cell>0.82</cell></row><row><cell>LinearSVC + BOW (n_gram=3)</cell><cell>0.58</cell><cell>1.00</cell><cell>0.74</cell></row><row><cell>LinearSVC + TF-IDF</cell><cell>0.86</cell><cell cols="2">0.89 0.88</cell></row><row><cell>LinearSVC + TF-IDF (min_df=0.2)</cell><cell>0.77</cell><cell>0.78</cell><cell>0.77</cell></row><row><cell>LinearSVC + TF-IDF (max_df=0.8)</cell><cell>86</cell><cell>0.89</cell><cell>0.88</cell></row><row><cell>LinearSVC + TF-IDF (n_gram=2)</cell><cell>0.85</cell><cell>0.87</cell><cell>0.86</cell></row><row><cell>LinearSVC + TF-IDF (n_gram=3)</cell><cell>0.76</cell><cell>0.91</cell><cell>0.83</cell></row><row><cell>LogisticRegression + BOW</cell><cell>0.87</cell><cell cols="2">0.96 0.91</cell></row><row><cell>LogisticRegression + BOW (min_df=0.2)</cell><cell>0.76</cell><cell>0.91</cell><cell>0.83</cell></row><row><cell>LogisticRegression + BOW (max_df=0.8)</cell><cell>0.87</cell><cell>0.96</cell><cell>0.91</cell></row><row><cell>LogisticRegression + BOW (n_gram=2)</cell><cell>0.81</cell><cell>0.98</cell><cell>0.89</cell></row><row><cell>LogisticRegression + BOW (n_gram=3)</cell><cell>0.61</cell><cell>1.0</cell><cell>0.75</cell></row><row><cell>LogisticRegression + TF-IDF</cell><cell>0.87</cell><cell>0.92</cell><cell>0.89</cell></row><row><cell>LogisticRegression + TF-IDF (min_df=0.2)</cell><cell>0.78</cell><cell>0.78</cell><cell>0.78</cell></row><row><cell>LogisticRegression + TF-IDF (max_df=0.8)</cell><cell>0.87</cell><cell>0.92</cell><cell>0.89</cell></row><row><cell>LogisticRegression + TF-IDF (n_gram=2)</cell><cell>0.83</cell><cell>0.9</cell><cell>0.86</cell></row><row><cell>LogisticRegression + TF-IDF (n_gram=3)</cell><cell>0.71</cell><cell>0.95</cell><cell>0.81</cell></row><row><cell>RandomForest + BOW</cell><cell>0.89</cell><cell>0.89</cell><cell>0.89</cell></row><row><cell>RandomForest + BOW (min_df=0.2)</cell><cell>0.77</cell><cell>0.89</cell><cell>0.82</cell></row><row><cell>RandomForest + BOW (max_df=0.8)</cell><cell>0.89</cell><cell>0.89</cell><cell>0.89</cell></row><row><cell>RandomForest + BOW (n_gram=2)</cell><cell>0.99</cell><cell>0.41</cell><cell>0.58</cell></row><row><cell>RandomForest + BOW (n_gram=3)</cell><cell>0.6</cell><cell>0.02</cell><cell>0.05</cell></row><row><cell>RandomForest + TF-IDF</cell><cell>0.91</cell><cell>0.89</cell><cell>0.9</cell></row><row><cell>RandomForest + TF-IDF (min_df=0.2)</cell><cell>0.77</cell><cell>0.88</cell><cell>0.82</cell></row><row><cell>RandomForest + TF-IDF (max_df=0.8)</cell><cell>0.91</cell><cell>0.89</cell><cell>0.9</cell></row><row><cell>KNeighbors + BOW</cell><cell>0.63</cell><cell cols="2">0.87 0.73</cell></row><row><cell>DecisionTree + BOW</cell><cell>0.84</cell><cell>0.85</cell><cell>0.85</cell></row><row><cell>DecisionTree + BOW (min_df=0.2)</cell><cell>0.73</cell><cell>0.87</cell><cell>0.8</cell></row><row><cell>DecisionTree + BOW (max_df=0.8)</cell><cell>0.87</cell><cell>0.87</cell><cell>0.87</cell></row><row><cell>DecisionTree + TF-IDF</cell><cell>0.9</cell><cell cols="2">0.91 0.91</cell></row><row><cell>DecisionTree + TF-IDF (min_df=0.2)</cell><cell>0.73</cell><cell>0.88</cell><cell>0.8</cell></row><row><cell>DecisionTree + TF-IDF (max_df=0.8)</cell><cell>0.9</cell><cell>0.83</cell><cell>0.86</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,88.98,194.95,417.30,374.55"><head>Table 5</head><label>5</label><figDesc>Additional results for the ML approach in the third run, on the balanced dataset with discarded posts.</figDesc><table coords="8,150.46,222.99,294.37,346.50"><row><cell>ML + feature extractor</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>LinearSVC + BOW</cell><cell>0.78</cell><cell cols="2">0.62 0.69</cell></row><row><cell>LinearSVC + BOW (max_df=0.8)</cell><cell>0.78</cell><cell cols="2">0.62 0.69</cell></row><row><cell>LinearSVC + BOW (n_gram=2)</cell><cell>0.77</cell><cell cols="2">0.47 0.58</cell></row><row><cell>LinearSVC + BOW (n_gram=3)</cell><cell>0.86</cell><cell cols="2">0.12 0.21</cell></row><row><cell>LinearSVC + TF-IDF</cell><cell>0.78</cell><cell cols="2">0.66 0.71</cell></row><row><cell>LinearSVC + TF-IDF (max_df=0.8)</cell><cell>0.78</cell><cell cols="2">0.66 0.71</cell></row><row><cell>LinearSVC + TF-IDF (n_gram=2)</cell><cell>0.77</cell><cell cols="2">0.54 0.64</cell></row><row><cell>LinearSVC + TF-IDF (n_gram=3)</cell><cell>0.82</cell><cell cols="2">0.26 0.39</cell></row><row><cell>LogisticRegression + BOW</cell><cell>0.8</cell><cell>0.62</cell><cell>0.7</cell></row><row><cell>LogisticRegression + BOW (max_df=0.8)</cell><cell>0.8</cell><cell>0.62</cell><cell>0.7</cell></row><row><cell>LogisticRegression + BOW (n_gram=2)</cell><cell>0.8</cell><cell>0.49</cell><cell>0.6</cell></row><row><cell>LogisticRegression + BOW (n_gram=3)</cell><cell>0.87</cell><cell cols="2">0.16 0.27</cell></row><row><cell>LogisticRegression + TF-IDF</cell><cell>0.79</cell><cell cols="2">0.66 0.72</cell></row><row><cell>LogisticRegression + TF-IDF (max_df=0.8)</cell><cell>0.79</cell><cell cols="2">0.66 0.72</cell></row><row><cell>LogisticRegression + TF-IDF (n_gram=2)</cell><cell>0.77</cell><cell cols="2">0.54 0.64</cell></row><row><cell>LogisticRegression + TF-IDF (n_gram=3)</cell><cell>0.8</cell><cell cols="2">0.32 0.46</cell></row><row><cell>KNeighbors + BOW</cell><cell>0.67</cell><cell>0.55</cell><cell>0.6</cell></row><row><cell>KNeighbors + BOW (max_df=0.8)</cell><cell>0.67</cell><cell>0.55</cell><cell>0.6</cell></row><row><cell>KNeighbors + BOW (n_gram=2)</cell><cell>0.63</cell><cell cols="2">0.39 0.41</cell></row><row><cell>KNeighbors + BOW (n_gram=3)</cell><cell>0.9</cell><cell cols="2">0.02 0.04</cell></row><row><cell>KNeighbors + TF-IDF</cell><cell>0.58</cell><cell cols="2">0.22 0.32</cell></row><row><cell>KNeighbors + TF-IDF (max_df=0.8)</cell><cell>0.58</cell><cell cols="2">0.22 0.32</cell></row><row><cell>KNeighbors + TF-IDF (n_gram=2)</cell><cell>0.63</cell><cell cols="2">0.16 0.25</cell></row><row><cell>KNeighbors + TF-IDF (n_gram=3)</cell><cell>0.73</cell><cell cols="2">0.32 0.24</cell></row><row><cell>DecisionTree + BOW</cell><cell>0.7</cell><cell cols="2">0.63 0.67</cell></row><row><cell>DecisionTree + BOW (max_df=0.8)</cell><cell>0.7</cell><cell cols="2">0.64 0.67</cell></row><row><cell>DecisionTree + BOW (n_gram=2)</cell><cell>0.75</cell><cell cols="2">0.47 0.58</cell></row><row><cell>DecisionTree + TF-IDF</cell><cell>0.7</cell><cell cols="2">0.65 0.68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,88.98,88.21,361.48,637.56"><head>Table 6</head><label>6</label><figDesc>Additional results for the ML approach in the first run, on the unbalanced dataset.</figDesc><table coords="9,144.81,116.26,305.65,609.51"><row><cell>ML + feature extractor</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>LinearSVC + BOW</cell><cell>0.82</cell><cell cols="2">0.88 0.85</cell></row><row><cell>LinearSVC + BOW (min_df=0.2)</cell><cell>0.79</cell><cell cols="2">0.85 0.82</cell></row><row><cell>LinearSVC + BOW (max_df=0.8)</cell><cell>0.83</cell><cell cols="2">0.9 0.86</cell></row><row><cell>LinearSVC + BOW (n_gram=2)</cell><cell>0.79</cell><cell cols="2">0.68 0.73</cell></row><row><cell>LinearSVC + BOW (n_gram=3)</cell><cell>0.98</cell><cell cols="2">0.15 0.25</cell></row><row><cell>LinearSVC + TF-IDF</cell><cell>1.0</cell><cell cols="2">0.81 0.89</cell></row><row><cell>LinearSVC + TF-IDF (min_df=0.2)</cell><cell>0.99</cell><cell>0.83</cell><cell>0.9</cell></row><row><cell>LinearSVC + TF-IDF (max_df=0.8)</cell><cell>1.0</cell><cell cols="2">0.81 0.89</cell></row><row><cell>LinearSVC + TF-IDF (n_gram=2)</cell><cell>0.99</cell><cell cols="2">0.51 0.67</cell></row><row><cell>LinearSVC + TF-IDF (n_gram=3)</cell><cell>0.8</cell><cell cols="2">0.07 0.13</cell></row><row><cell>LogisticRegression + BOW</cell><cell>0.85</cell><cell cols="2">0.87 0.86</cell></row><row><cell>LogisticRegression + BOW (min_df=0.2)</cell><cell>0.83</cell><cell cols="2">0.85 0.84</cell></row><row><cell>LogisticRegression + BOW (max_df=0.8)</cell><cell>0.87</cell><cell cols="2">0.87 0.87</cell></row><row><cell>LogisticRegression + BOW (n_gram=2)</cell><cell>0.89</cell><cell cols="2">0.61 0.72</cell></row><row><cell>LogisticRegression + BOW (n_gram=3)</cell><cell>1.00</cell><cell cols="2">0.11 0.19</cell></row><row><cell>LogisticRegression + TF-IDF</cell><cell>1.0</cell><cell cols="2">0.67 0.80</cell></row><row><cell>LogisticRegression + TF-IDF (min_df=0.2)</cell><cell>1.00</cell><cell cols="2">0.63 0.77</cell></row><row><cell>LogisticRegression + TF-IDF (max_df=0.8)</cell><cell>1.0</cell><cell>0.67</cell><cell>0.8</cell></row><row><cell>LogisticRegression + TF-IDF (n_gram=2)</cell><cell>0.8</cell><cell>0.05</cell><cell>0.1</cell></row><row><cell>LogisticRegression + TF-IDF (n_gram=3)</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>RandomForest + BOW</cell><cell>1.00</cell><cell cols="2">0.29 0.45</cell></row><row><cell>RandomForest + BOW (min_df=0.2)</cell><cell>1.00</cell><cell cols="2">0.46 0.63</cell></row><row><cell>RandomForest + BOW (max_df=0.8)</cell><cell>1.00</cell><cell cols="2">0.29 0.44</cell></row><row><cell>RandomForest + BOW (n_gram=2)</cell><cell>0.99</cell><cell cols="2">0.41 0.58</cell></row><row><cell>RandomForest + BOW (n_gram=3)</cell><cell>0.6</cell><cell cols="2">0.02 0.05</cell></row><row><cell>RandomForest + TF-IDF</cell><cell>1.0</cell><cell cols="2">0.67 0.57</cell></row><row><cell>RandomForest + TF-IDF (min_df=0.2)</cell><cell>1.00</cell><cell cols="2">0.57 0.72</cell></row><row><cell>RandomForest + TF-IDF (max_df=0.8)</cell><cell>1.0</cell><cell>0.34</cell><cell>0.5</cell></row><row><cell>RandomForest + TF-IDF (n_gram=2)</cell><cell>1.0</cell><cell cols="2">0.42 0.59</cell></row><row><cell>RandomForest + TF-IDF (n_gram=3)</cell><cell>0.6</cell><cell cols="2">0.04 0.08</cell></row><row><cell>KNeighbors + BOW</cell><cell>0.96</cell><cell cols="2">0.63 0.76</cell></row><row><cell>KNeighbors + BOW (min_df=0.2)</cell><cell>0.94</cell><cell cols="2">0.6 0.74</cell></row><row><cell>KNeighbors + BOW (max_df=0.8)</cell><cell>0.96</cell><cell cols="2">0.73 0.83</cell></row><row><cell>KNeighbors + BOW (n_gram=2)</cell><cell>0.91</cell><cell>0.12</cell><cell>0.2</cell></row><row><cell>KNeighbors + BOW (n_gram=3)</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>KNeighbors + TF-IDF</cell><cell>0.94</cell><cell cols="2">0.67 0.78</cell></row><row><cell>KNeighbors + TF-IDF (min_df=0.2)</cell><cell>0.9</cell><cell cols="2">0.76 0.82</cell></row><row><cell>KNeighbors + TF-IDF (max_df=0.8)</cell><cell>0.98</cell><cell cols="2">0.61 0.75</cell></row><row><cell>KNeighbors + TF-IDF (n_gram=2)</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>KNeighbors + TF-IDF (n_gram=3)</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>DecisionTree + BOW</cell><cell>0.84</cell><cell cols="2">0.85 0.85</cell></row><row><cell>DecisionTree + BOW (min_df=0.2)</cell><cell>0.67</cell><cell cols="2">0.69 0.68</cell></row><row><cell>DecisionTree + BOW (max_df=0.8)</cell><cell>0.86</cell><cell cols="2">0.86 0.86</cell></row><row><cell>DecisionTree + BOW (n_gram=2)</cell><cell>0.88</cell><cell cols="2">0.82 0.85</cell></row><row><cell>DecisionTree + BOW (n_gram=3)</cell><cell>0.71</cell><cell cols="2">0.45 0.55</cell></row><row><cell>DecisionTree + TF-IDF</cell><cell>0.9</cell><cell cols="2">0.91 0.91</cell></row><row><cell>DecisionTree + TF-IDF (min_df=0.2)</cell><cell>0.75</cell><cell cols="2">0.73 0.74</cell></row><row><cell>DecisionTree + TF-IDF (max_df=0.8)</cell><cell>0.87</cell><cell cols="2">0.88 0.87</cell></row><row><cell>DecisionTree + TF-IDF (n_gram=2)</cell><cell>0.87</cell><cell cols="2">0.86 0.86</cell></row><row><cell>DecisionTree + TF-IDF (n_gram=3)</cell><cell>0.51</cell><cell cols="2">0.63 0.56</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="11,89.29,125.31,348.09,351.05"><head>Table 7</head><label>7</label><figDesc>Deep Learning Grid Search Results</figDesc><table coords="11,157.89,153.37,279.50,322.99"><row><cell>Hidden Dimension</cell><cell>Embedding Size</cell><cell>Num. Att. heads</cell><cell>Trained Epochs</cell><cell cols="2">F1 score Loss</cell></row><row><cell></cell><cell></cell><cell></cell><cell>14</cell><cell>0.705</cell><cell>0.452</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>19 11</cell><cell>0.479 0.536</cell><cell>0.514 0.531</cell></row><row><cell></cell><cell></cell><cell></cell><cell>21</cell><cell>0.232</cell><cell>0.763</cell></row><row><cell></cell><cell>64</cell><cell></cell><cell>41 36</cell><cell>0.872 0.850</cell><cell>0.228 0.259</cell></row><row><cell>64</cell><cell></cell><cell>4</cell><cell>22 15</cell><cell>0.848 0.843</cell><cell>0.262 0.287</cell></row><row><cell></cell><cell></cell><cell></cell><cell>43</cell><cell>0.851</cell><cell>0.289</cell></row><row><cell></cell><cell></cell><cell></cell><cell>29</cell><cell>0.859</cell><cell>0.336</cell></row><row><cell></cell><cell>128</cell><cell>1</cell><cell>33 10</cell><cell>0.813 0.537</cell><cell>0.403 0.559</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>12</cell><cell>0.831</cell><cell>0.298</cell></row><row><cell></cell><cell></cell><cell></cell><cell>40</cell><cell>0.705</cell><cell>0.436</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>31 10</cell><cell>0.690 0.093</cell><cell>0.465 0.632</cell></row><row><cell></cell><cell>64</cell><cell></cell><cell>16</cell><cell>0.449</cell><cell>0.729</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>6</cell><cell>0.730</cell><cell>0.390</cell></row><row><cell>128</cell><cell></cell><cell>4</cell><cell>56 18</cell><cell>0.888 0.816</cell><cell>0.218 0.360</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>24 16</cell><cell>0.827 0.841</cell><cell>0.275 0.307</cell></row><row><cell></cell><cell>128</cell><cell></cell><cell>40</cell><cell>0.859</cell><cell>0.257</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>27</cell><cell>0.858</cell><cell>0.257</cell></row><row><cell></cell><cell></cell><cell></cell><cell>14</cell><cell>0.743</cell><cell>0.440</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,658.79,106.75,10.22"><p>https://gamblershelp.com.au/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,108.93,669.75,77.47,10.22"><p>http://gamontreal.ca/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,107.59,109.71,400.23,12.44;6,107.59,123.26,399.11,12.44;6,107.59,136.81,181.36,12.44" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A.-M</forename><surname>Bucur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cosma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">P</forename><surname>Dinu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.16175[cs</idno>
		<idno>arXiv: 2106.16175 version: 1</idno>
		<ptr target="http://arxiv.org/abs/2106.16175" />
		<title level="m" coord="6,271.44,109.71,236.38,12.44;6,107.59,123.26,148.61,12.44">Early Risk Detection of Pathological Gambling, Self-Harm and Depression Using BERT</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,150.36,400.23,12.44;6,107.59,163.91,398.40,12.44;6,107.26,177.45,41.66,12.44" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,345.91,150.36,161.91,12.44;6,107.59,163.91,398.40,12.44">Early Detection of Signs of Pathological Gambling, Self-Harm and Depression through Topic Extraction and Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Maupom√©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rancourt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Soulas</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,107.59,191.00,399.68,12.44;6,107.59,204.55,380.15,12.44" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">J R</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04907arXiv:1907.04907</idno>
		<ptr target="http://arxiv.org/abs/1907.04907" />
		<title level="m" coord="6,280.55,191.00,169.41,12.44">Topic Modeling in Embedding Spaces</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct coords="6,107.59,218.10,400.23,12.44;6,107.59,231.65,399.59,12.44;6,107.59,245.20,399.60,12.44;6,107.59,258.75,153.27,12.44" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,336.62,218.10,171.20,12.44;6,107.59,231.65,227.35,12.44">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1810.04805</idno>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<idno>arXiv:1810.04805 [cs</idno>
		<ptr target="http://arxiv.org/abs/1810.04805.doi:10.48550/arXiv.1810.04805" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>type: article</note>
</biblStruct>

<biblStruct coords="6,107.59,272.30,399.60,12.44;6,107.23,285.85,400.59,12.44;6,107.59,299.40,398.87,12.44;6,107.59,312.95,255.21,12.44" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="6,165.13,285.85,273.79,12.44">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1907.11692</idno>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<idno>arXiv:1907.11692 [cs</idno>
		<ptr target="http://arxiv.org/abs/1907.11692.doi:10.48550/arXiv.1907.11692" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>type: article</note>
</biblStruct>

<biblStruct coords="6,107.59,326.50,398.60,12.44;6,107.59,340.04,399.59,12.44;6,107.59,353.59,399.60,12.44;6,107.59,367.14,153.27,12.44" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="6,391.05,326.50,115.13,12.44;6,107.59,340.04,237.97,12.44">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1909.11942</idno>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<idno>arXiv:1909.11942 [cs</idno>
		<ptr target="http://arxiv.org/abs/1909.11942.doi:10.48550/arXiv.1909.11942" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>type: article</note>
</biblStruct>

<biblStruct coords="6,107.59,380.69,400.23,12.44;6,107.59,394.24,399.68,12.44;6,107.59,407.79,398.39,12.44;6,107.59,421.34,75.29,12.44" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="6,144.06,394.24,115.08,12.44">Attention Is All You Need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1706.03762</idno>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<idno>arXiv:1706.03762 [cs</idno>
		<ptr target="http://arxiv.org/abs/1706.03762.doi:10.48550/arXiv.1706.03762" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
