<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,352.48,15.42;1,89.29,106.66,391.34,15.42">LauSAn at eRisk 2022: Simply and Effectively Optimizing Text Classification for Early Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,138.55,83.46,5.42"><forename type="first">Andreas</forename><surname>SÃ¤uberli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational Linguistics</orgName>
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,190.26,138.55,65.01,5.42"><forename type="first">Sooyeon</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational Linguistics</orgName>
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,291.55,138.55,72.21,5.42"><forename type="first">Laura</forename><surname>Stahlhut</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational Linguistics</orgName>
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,352.48,15.42;1,89.29,106.66,391.34,15.42">LauSAn at eRisk 2022: Simply and Effectively Optimizing Text Classification for Early Detection</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">BB629CD240881E878599A4B6544DC0D5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>depression detection</term>
					<term>early risk detection</term>
					<term>threshold scheduling</term>
					<term>natural language processing</term>
					<term>social media</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of early detection tasks at eRisk is to classify social media users as early as possible, based on streams of posts written by those users. We present two simple strategies of adapting standard text classification models in order to optimize them for early detection: concatenating the posts in different ways during training and inference, and continuously moving the decision boundary at inference time. We applied these approaches to two different text classification architectures based on pre-trained language models in eRisk 2022's Task 2 (early detection of depression), and were able to reach top 5 placements in all time-sensitive evaluation metrics. A systematic post-submission ablation study confirmed that both strategies were effective at optimizing for early detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this paper, we describe our team's participation at at the 2022 Conference and Labs of the Evaluation Forum (CLEF) eRisk task for early detection of depression (Task 2) <ref type="bibr" coords="1,438.33,418.29,11.50,4.94" target="#b0">[1]</ref>. Depressive disorders are common in the general population and associated with burdens such as conflict in private life and an increased risk of suicide. Many cases remain undiagnosed, e.g., due to patient somatization and denial or social stigma <ref type="bibr" coords="1,309.36,458.94,11.58,4.94" target="#b1">[2]</ref>. It has been shown that certain patterns in a person's writing can be indicative of depression <ref type="bibr" coords="1,326.45,472.49,11.42,4.94" target="#b2">[3,</ref><ref type="bibr" coords="1,340.59,472.49,7.61,4.94" target="#b3">4]</ref>. Being able to detect depression at an early stage from social media, postings could play a part in enabling more people to get treatment earlier and give social media sites a tool to detect potentially suicidal users. The task of early prediction on social media postings can also be extended to other topics, as can be seen from the other eRisk tasks (e.g. early detection of signs of gambling, signs of self-harm or signs of anorexia).</p><p>The aim of this task is to detect signs of depression in posts from social media as early as possible. In the training stage, labeled data from the depression subreddit is available to develop depression detection models. During the test phase, models receive the users' posts one by one in chronological order and have to make a binary decision after each post whether the user is depressed or not. The decision for a particular user cannot be undone later.</p><p>CLEF 2022: Conference and Labs of the Evaluation Forum, September 5-8, 2022, Bologna, Italy andreas.saeuberli@uzh.ch (A. SÃ¤uberli); sooyeon.cho@uzh.ch (S. Cho); lauracelina.stahlhut@uzh.ch (L. Stahlhut)</p><p>The main factor that differentiates this task from typical classification tasks is time-sensitivity, i.e., the necessity to classify a user as depressed as early as possible and not only after seeing the entire post history. In addition to standard classification measures such as precision, recall, and ğ¹ 1 , eRisk uses several time-sensitive evaluation metrics such as early risk detection error (ğ¸ğ‘…ğ·ğ¸), latency, speed and latency-weighted ğ¹ 1 . See Parapar et al. <ref type="bibr" coords="2,425.49,144.43,12.84,4.94" target="#b4">[5]</ref> for a complete description of these metrics.</p><p>In this paper, we present methods to adapt standard text classification models in order to optimize for early detection, and show that these can be effectively applied to Task 2 of eRisk 2022. Our main contributions are twofold:</p><p>â€¢ We experiment with different strategies for concatenating a sequence of posts in order to optimize training for the early detection setting. â€¢ We present threshold scheduling, a method to change the decision boundary over the course of a post history in order to optimize for one of the time-sensitive evaluation metrics.</p><p>The remainder of this paper is organized in the following way: In Section 2, we mention some related work on earlier installments of eRisk shared tasks, which inspired our approaches. Section 3 describes the task dataset. Section 4 explains our approaches and models. The experimental setup for the submisson of our models is introduced in Section 5. In Section 6, we report and discuss our results on the submitted models and post-submisson ablation study. Finally, Section 7 provides a general conclusion and brief outlook on future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Task 2 (Early Detection of Depression) of CLEF 2022 eRisk is a continuation of eRisk's 2017 Task 1 and eRisk 2018's Task 1 (Losada et al. <ref type="bibr" coords="2,262.34,445.10,11.31,4.94" target="#b5">[6,</ref><ref type="bibr" coords="2,276.38,445.10,7.25,4.94" target="#b6">7]</ref>). Thus, there have been multiple groups that have worked with a subset of the dataset we worked on with the same aim we have. While approaches in the preceding versions of this task were mostly concentrated on feature engineering and the application of various classification models, approaches to related early classification tasks in more recent years were often based on Transformer models and transfer learning.</p><p>Examples include un Nisa and Muhammad <ref type="bibr" coords="2,294.65,512.85,11.44,4.94" target="#b7">[8]</ref>, who applied pre-trained BERT embeddings in combination with logistic regression for early detection of self-harm. For the same task, MartÃ­nez-CastaÃ±o et al. <ref type="bibr" coords="2,192.44,539.95,12.84,4.94" target="#b8">[9]</ref> finetuned various transformer models, and trigger a positive decision when the moving average of the predicted probability reaches a specified threshold within a certain time window in the user's post history. For early detection of signs of pathological gambling, Bucur et al. <ref type="bibr" coords="2,186.55,580.60,17.91,4.94" target="#b9">[10]</ref> finetuned BERT models on single posts, and used aggressive decision boundaries in order to prevent false positives.</p><p>These submissions, which are based on binary text classification, had to make use of very high decision boundaries, or limit the time window where a positive decision can be made, in order to avoid low precision when repeatedly classifying the same users at every time step. In our submission, we experiment with slightly different approaches to overcome these challenges. Number of users and posts in positive and negative groups in the dataset. Each users's writing contains a series of posts in chronological order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset</head><p>The data used in eRisk 2017 and 2018 are provided as training data by the organizers. This dataset, which was initially presented in Losada and Crestani <ref type="bibr" coords="3,365.83,269.53,16.26,4.94" target="#b10">[11]</ref>, was collected from Reddit, and contains a chronological collection of posts (title and content) and comments for each of 1,707 users from wide range of subreddits. Each user is labeled either positive (depressed) or negative (control group), based on whether the user has clearly expressed a depression diagnosis in one of their posts (e.g. 'I was diagnosed with depression').</p><p>Table <ref type="table" coords="3,127.54,337.28,5.17,4.94" target="#tab_0">1</ref> shows the distribution of labels among posts and users. Each user has between 10 and 2,000 posts (median: 366), and each post (title and text combined) contains between 0 and 8,177 whitespace-delimited tokens (median: 13).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methods</head><p>We frame the early detection task as a standard text classification problem, where the decision whether a user is depressed or not is done based on a classification of the user's post history each time a new post is added. In this section, we describe the two approaches we developed to achieve this, as well as the models we chose for the shared task submission. The code used in our experiments is available on GitHub.<ref type="foot" coords="3,266.32,474.57,3.71,3.61" target="#foot_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Concatenation strategies</head><p>We consider three different strategies to prepare the input data. The simplest way would be to use only the most recent post at each given point in time as input to the model. We call this strategy no-concat. For instance, if a user has 6 posts [ğ‘, ğ‘, ğ‘, ğ‘‘, ğ‘’, ğ‘“ ] where ğ‘ is the oldest and ğ‘“ is the most recent post, we will train the model by giving it single posts as input ([ğ‘, ğ‘, ğ‘, ğ‘‘, ğ‘’, ğ‘“ ]).</p><p>However, our hypothesis is that it is difficult to classify whether someone has depression based on a single post, thus we want to include several posts by concatenating a post with a number of directly preceding posts. We propose two such concatenation strategies, which we name concat1 and concat2.</p><p>In with ğ‘› = 4). A potential problem with this strategy (particularly for users with long post histories) is that almost none of the training samples (only the first ğ‘› -1) are shorter than ğ‘› posts, which means that the model will likely perform worse on the first few posts of a user's history. Since it is exactly in this early part where we want to detect most of the depressed users, we propose our final strategy (concat2). This strategy generates both a higher ratio of shorter and additional longer training samples, in order to widen the distribution of the numbers of concatenated posts. For a user's post, we train the model on the individual posts as well as the specified number of concatenations. If we concatenate 1, 2, 3, and 6 posts of a user with 6 posts, the resulting training data is [ğ‘, ğ‘, ğ‘, ğ‘‘, ğ‘’, ğ‘“, ğ‘ğ‘, ğ‘ğ‘‘, ğ‘’ğ‘“, ğ‘ğ‘ğ‘, ğ‘‘ğ‘’ğ‘“, ğ‘ğ‘ğ‘ğ‘‘ğ‘’ğ‘“ ] (see Figure <ref type="figure" coords="4,460.42,423.64,3.57,4.94" target="#fig_0">1</ref>). During inference, in concat1 we use the most recent ğ‘› posts, and in concat2 the entire post history (limited only by the model's maximum input sequence length).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Threshold Scheduling</head><p>As we repeatedly classify users based on their increasingly long post history, taking into account that a positive decision cannot be reversed later, another issue is that a constant decision boundary means that the probability of classifying a user as depressed increases continuously as more posts are processed. Consider the following example: A trained model has a chance ğ‘ = 5% of correctly classifying the user as depressed at any point in time. In this case, the chance of detecting a positive user with a history of 10 posts in total is 1 -âˆï¸€ 10 ğ‘–=1 (1 -ğ‘) = 40%, whereas for a history of 100 posts, it is already &gt; 99%. Similarly, the chance of incorrectly classifying a user as depressed increases very quickly. Therefore, users with a long post history have a much higher chance of being misclassified as depressed, while users with a short post history have a much higher chance of remaining undetected. To fix this imbalance, we propose continuously changing the decision boundary during prediction, in order to increase chances of early detection, and reduce misclassifications later in the post history. We use the following exponential threshold scheduling function to achieve this: thr(ğ‘¡) = thr max + (thr minthr max ) Ã— 10 -ğ‘¡/ğ‘ thr(ğ‘¡) is the decision boundary applied to the model output after ğ‘¡ posts, thr min is the starting threshold, thr max is the upper limit threshold, which is asymptotically approached by thr(ğ‘¡), and ğ‘ determines how many posts it takes for thr(ğ‘¡) to reach 90% of the way towards thr max . We optimize thr min , thr max , and ğ‘ using grid search on the training data, after training the model itself. Figure <ref type="figure" coords="5,176.37,387.13,4.97,4.94">2</ref> shows three threshold scheduling functions optimized on the same model for three different metrics. Note that for metrics which highly favor early detection, the initial threshold is much lower, and the threshold increases very quickly.</p><p>In addition, we realized that the first few posts are still more difficult to classify, even when using the concat1 or concat2 strategies, so we experiment with an additional modification of the function described above, where the first ğ‘› posts are forced to yield negative decisions, and only then the actual threshold scheduling is applied, i.e., the first ğ‘› posts are ignored. This is similar to how MartÃ­nez-CastaÃ±o et al. <ref type="bibr" coords="5,276.99,481.98,12.84,4.94" target="#b8">[9]</ref> enforce a minimum number of posts to be read before a positive decision can be made, but we explicitly include ğ‘› in the grid search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Models</head><p>Due to the way we frame the task, any machine learning architecture capable of binary text classification could be used. After initial experiments with different architectures, two models based on pre-trained language models appeared most promising.</p><p>The first model is a logistic regression model which we feed a vector representation of the input sequence, obtained by averaging embeddings from the final four layers of a pre-trained BERT model (Devlin et al. 12; bert-base-uncased on Hugging Face 2 ) across the entire input sequence. This approach is similar to un Nisa and Muhammad <ref type="bibr" coords="5,366.08,626.22,11.41,4.94" target="#b7">[8]</ref>, although we do not use any additional preprocessing and simply truncate the concatenated raw input posts (title and text) if it is longer than 512 subwords.</p><p>2 https://huggingface.co/bert-base-uncased For our second model architecture, we finetune a DistilBERT model <ref type="bibr" coords="6,400.43,90.23,60.02,4.94">(Sanh et al. 13</ref>; distilbert-base-uncased on Hugging Face) directly on the binary classification task, again with no additional preprocessing and truncating to 512 subwords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Setup</head><p>For the training of our submitted models, we combined the training and test sets from eRisk 2017 and 2018 and split off 20% of the users as validation data for model selection. We undersampled the majority class to reach a positive ratio of 25% in the training data, and trained for three epochs. We used scikit-learn <ref type="bibr" coords="6,240.87,216.34,17.99,4.94" target="#b13">[14]</ref> for the logistic regression model, and the Transformers library by Hugging Face <ref type="bibr" coords="6,197.76,229.89,17.91,4.94" target="#b14">[15]</ref> for the transformer models.</p><p>After experimenting with the different concatenation strategies and threshold scheduling optimizations, we submitted five runs with the following models:</p><p>â€¢ LauSAn#0: Logistic regression with BERT embeddings, no-concat, exponential threshold scheduling optimized for ğ¸ğ‘…ğ·ğ¸ 5 â€¢ LauSAn#1: Logistic regression with BERT embeddings, concat1 (5 posts), exponential threshold scheduling optimized for ğ¸ğ‘…ğ·ğ¸ 50 â€¢ LauSAn#2: Logistic regression with BERT embeddings, concat1 (5 posts), exponential threshold scheduling optimized for ğ¸ğ‘…ğ·ğ¸ 50 (ignoring the first 3 posts) â€¢ LauSAn#3: Logistic regression with BERT embeddings, concat1 (5 posts), exponential threshold scheduling optimized for latency-weighted ğ¹ 1 (ignoring first 3 posts) â€¢ LauSAn#4: Finetuned DistilBERT, concat2, exponential threshold scheduling optimized for ğ¸ğ‘…ğ·ğ¸ 5</p><p>For models with concat1, we chose the maximum number of concatenated posts to be ğ‘› = 5. For concat2, we concatenated 1, 2, 3, 4, 10, 20, 30, 40, and 50 posts without overlap. In all cases, we concatenated the posts in reverse order, such that sequences longer than 512 subwords are truncated on the oldest posts rather than the most recent ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results and discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Submitted models</head><p>Table <ref type="table" coords="6,115.79,549.97,5.07,4.94" target="#tab_1">2</ref> shows the results of our submitted models on the official test set. <ref type="foot" coords="6,412.17,547.33,3.71,3.61" target="#foot_1">3</ref>Out of 62 runs by 13 teams, our runs 4 and 0 ranked 1st and 2nd in terms of ğ¸ğ‘…ğ·ğ¸ 5 , run 2 ranked 4th in terms of ğ¸ğ‘…ğ·ğ¸ 50 , and run 3 ranked 5th in terms of latency-weighted ğ¹ 1 . However, none of our architectures ranked in the top 10 for several of these metrics at once. This suggests that our threshold scheduling approach was very effective at optimizing towards a specific evaluation metric without re-training the model (note that the models from runs 1, 2, and 3 use exactly the same parameters, apart from the decision boundary). ğ¸ğ‘…ğ·ğ¸ is a measure that penalizes late decision. ğ¸ğ‘…ğ·ğ¸ 5 begins punishing strongly from the 5th post, ğ¸ğ‘…ğ·ğ¸ 50 from the 50th post. This means that our models were successful at classifying users early, especially runs 0 and 4. Comparing with standard classification metrics, it can be seen that there is a tradeoff between accurate and early classification. For instance, among our models, run 3 achieved the best ğ¹ 1 as well as the worst ğ¸ğ‘…ğ·ğ¸ 5 and ğ¸ğ‘…ğ·ğ¸ 50 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Post-submission ablation study</head><p>The results described above are not very informative for comparing the different parts of our approach independently. Therefore, we conduct a post-submission ablation study in order to assess the contributions of concatenation strategies, threshold scheduling, and model architectures separately, and investigate their interactions. We trained models to cover the full combination space of 2 model architectures, 3 concatenation strategies, and 2 thresholding strategies, each optimized for ğ¸ğ‘…ğ·ğ¸ 5 , ğ¸ğ‘…ğ·ğ¸ 50 , and latency-weighted ğ¹ 1 . We finetuned the DistilBERT models for 6 epochs in the case of no-concat and concat1, and 3 epochs in the case of concat2, in order to account for the different number of training samples generated by the processing strategies, without undersampling the training set. Otherwise, the experimental setup is the same as in Section 5. Results can be seen in Table <ref type="table" coords="7,363.73,479.13,3.71,4.94">3</ref>. Note that these scores are not directly comparable to the ones in Table <ref type="table" coords="7,269.15,492.68,3.70,4.94" target="#tab_1">2</ref>, as they are only based on our own development set and not the official test set.</p><p>In almost all cases, threshold scheduling with the exponential function defined in Section 4.2 outperforms or equals the constant decision boundary. These results confirm the hypothesis that a constant threshold is suboptimal for repeatedly classifying the same users. The optimized threshold scheduling functions look similar across different concatenation strategies: starting below the midpoint, and quickly increasing towards a value above it. <ref type="foot" coords="7,394.66,571.34,3.71,3.61" target="#foot_2">4</ref> In contrast, the optimal constant threshold fluctuates strongly between optimization metrics, and in one case even leads to an ğ¹ 1 score of 0.</p><p>Regarding the concatenation strategies, the picture is less clear. ğ¸ğ‘…ğ·ğ¸ 50 and ğ¹ 1 both profit from the concatenation strategies that involve history (concat1, concat2). Compared to no-concat, they tend to lose performance when optimized for ğ¸ğ‘…ğ·ğ¸ 5 , although concat2 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>Ablation results for logistic regression with averaged BERT embeddings. The changing factors were concatenation strategy (no-concat, concat1, concat2), threshold scheduling (constant, exponential), and the metric for which we optimized (ğ¸ğ‘…ğ·ğ¸ 5 , ğ¸ğ‘…ğ·ğ¸ 50 , latency-weighted ğ¹ 1 ), which resulted in 18 trained models for each architecture. Best scores for each metric and architecture are shown in bold.</p><p>manages to recover some of it, likely due to the additional short samples seen during training.</p><p>Overall, it appears that concat2 leads to better results, unless the evaluation metric exclusively favors very early detection as in ğ¸ğ‘…ğ·ğ¸ 5 .</p><p>Comparing the two model architectures, the logistic regression model mostly outperforms DistilBERT, especially with no-concat and concat1 strategies. DistilBERT outperforms the logistic regression model in terms of ğ¸ğ‘…ğ·ğ¸ 5 and ğ¸ğ‘…ğ·ğ¸ 50 only in the setting with exponential threshold scheduling and concat2. This suggests that classifying averaged word representations from pre-trained language models can be a viable option for this task. An explanation for this could be that depression mainly manifests itself in the general semantic topic of the posts rather than subtle linguistic details, and can thus largely be captured by averaged word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and outlook</head><p>We presented our approaches to 2022's eRisk task for early detection of depression. Framing this problem as a modified text classification, we experimented with different strategies for input concatenation and threshold scheduling in order to take the post history into account and enable early detection. Two of our models ranked first and second in the ğ¸ğ‘…ğ·ğ¸ 5 metric, which implies that they were particularly successful at detecting depression as early as possible.</p><p>In addition, we conducted an ablation study in order to determine the effect of the concatenation strategies and threshold scheduling. Our results show that adapting the classification threshold at inference time (specifically, starting with a small threshold and increasing over time) is highly effective for supporting early detection when the number of samples is unknown at first. We tested three concatenation strategies and observed that our concat2 strategy, which generates training data with both short and long post histories, results in a good trade-off between early and accurate detection. We also show that for this specific task, classification of pre-trained hidden representations with traditional machine learning models can be a very effective and more robust alternative to finetuning.</p><p>Overall, the performance achieved on early detection of depression still leaves room for improvement, with best achieved ğ¹ 1 scores in eRisk 2022 of 0.712 by NLPGroup-IISERB. In future experiments, our approaches could be extended by further optimizing hyperparameters for the concatenating strategies. Comparing our method of using local classification scores with threshold scheduling with more globally accumulated confidence scores may provide more insights into its effectiveness. And finally, our approaches leave room for integrating domain-specific knowledge such as engineered linguistic features, which we have not explored at all.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,226.57,418.22,8.93;4,89.29,240.34,416.69,4.79;4,89.29,250.26,416.92,8.74;4,89.29,264.26,416.69,4.79;4,89.29,276.21,294.81,4.79"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of training sample generation for one user with 6 posts using different strategies.Note the different distributions of post length and samples with signs of depression. The oldest post is always to the left. In this example, we assume that this user has 6 posts, [ğ‘, ğ‘, ğ‘, ğ‘‘, ğ‘’, ğ‘“ ]. In the no-concat strategy, each post is fed as a single input. Strategy concat1 concatenates the 4 most recent posts. In the concat2 strategy, 1, 2, 3 and 6 concatenated posts are used as input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,89.29,248.73,416.69,9.65;5,89.29,260.69,401.23,9.65"><head>1 Figure 2 :</head><label>12</label><figDesc>Figure 2: Threshold scheduling functions optimized on the same model for ğ¸ğ‘…ğ·ğ¸ 5 , ğ¸ğ‘…ğ·ğ¸ 50 , and latency-weighted ğ¹ 1 . In the function optimized for ğ¸ğ‘…ğ·ğ¸ 5 , thr min = -3, thr max = 3, and ğ‘ = 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,91.72,277.38,86.09"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="3,228.91,91.72,137.46,63.57"><row><cell></cell><cell cols="2">Users Posts</cell></row><row><cell>Negative</cell><cell cols="2">1,493 986,360</cell></row><row><cell>Positive</cell><cell>214</cell><cell>90,222</cell></row><row><cell>Total</cell><cell cols="2">1,707 1,076,582</cell></row><row><cell cols="3">Positive ratio 12.5% 8.4%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,88.93,89.68,435.69,133.65"><head>Table 2</head><label>2</label><figDesc>Run Precisionâ†‘ Recall â†‘ ğ¹ 1 â†‘ ğ¸ğ‘…ğ·ğ¸ 5 â†“ ğ¸ğ‘…ğ·ğ¸ 50 â†“ latency â†“ speed â†‘ lw-ğ¹ 1 â†‘Test set results of our submitted models, compared with a strong model by team UNSL, which performed well across all time-sensitive metrics.</figDesc><table coords="7,95.27,107.33,429.35,74.18"><row><cell>LauSAn#0</cell><cell>0.137</cell><cell>0.827 0.235</cell><cell>0.041</cell><cell>0.038</cell><cell>1</cell><cell>1.000</cell><cell>0.235</cell></row><row><cell>LauSAn#1</cell><cell>0.165</cell><cell>0.888 0.279</cell><cell>0.053</cell><cell>0.040</cell><cell>2</cell><cell>0.996</cell><cell>0.278</cell></row><row><cell>LauSAn#2</cell><cell>0.174</cell><cell>0.867 0.290</cell><cell>0.056</cell><cell>0.031</cell><cell>4</cell><cell>0.988</cell><cell>0.287</cell></row><row><cell>LauSAn#3</cell><cell>0.420</cell><cell>0.643 0.508</cell><cell>0.059</cell><cell>0.041</cell><cell>6</cell><cell>0.981</cell><cell>0.498</cell></row><row><cell>LauSAn#4</cell><cell>0.201</cell><cell>0.724 0.315</cell><cell>0.039</cell><cell>0.033</cell><cell>1</cell><cell>1.000</cell><cell>0.315</cell></row><row><cell>UNSL#2</cell><cell>0.4</cell><cell>0.755 0.523</cell><cell>0.045</cell><cell>0.026</cell><cell>3</cell><cell>0.992</cell><cell>0.519</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,131.84,85.62,331.59,304.77"><head>Logistic regression with averaged BERT embeddings:</head><label></label><figDesc></figDesc><table coords="8,131.84,99.78,331.59,290.60"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Concatenation strategy</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>no-concat</cell><cell></cell><cell></cell><cell>concat1</cell><cell></cell><cell></cell><cell>concat2</cell><cell></cell></row><row><cell></cell><cell>5</cell><cell>50</cell><cell></cell><cell>5</cell><cell>50</cell><cell></cell><cell>5</cell><cell>50</cell><cell></cell></row><row><cell>Threshold scheduling</cell><cell>ğ¸ğ‘…ğ·ğ¸</cell><cell>ğ¸ğ‘…ğ·ğ¸</cell><cell>lw-ğ¹ 1</cell><cell>ğ¸ğ‘…ğ·ğ¸</cell><cell>ğ¸ğ‘…ğ·ğ¸</cell><cell>lw-ğ¹ 1</cell><cell>ğ¸ğ‘…ğ·ğ¸</cell><cell>ğ¸ğ‘…ğ·ğ¸</cell><cell>lw-ğ¹ 1</cell></row><row><cell>constant, optimized for</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>... ğ¸ğ‘…ğ·ğ¸ 5</cell><cell cols="9">0.126 0.126 0.000 0.117 0.101 0.458 0.106 0.075 0.512</cell></row><row><cell>... ğ¸ğ‘…ğ·ğ¸ 50</cell><cell cols="9">0.132 0.089 0.388 0.124 0.075 0.419 0.106 0.075 0.512</cell></row><row><cell cols="10">... latency-weighted ğ¹ 1 0.124 0.093 0.485 0.113 0.090 0.490 0.104 0.077 0.528</cell></row><row><cell>exponential, optimized for</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>... ğ¸ğ‘…ğ·ğ¸ 5</cell><cell cols="9">0.073 0.069 0.347 0.081 0.079 0.326 0.073 0.066 0.361</cell></row><row><cell>... ğ¸ğ‘…ğ·ğ¸ 50</cell><cell cols="9">0.086 0.068 0.470 0.077 0.054 0.413 0.087 0.068 0.471</cell></row><row><cell cols="10">... latency-weighted ğ¹ 1 0.086 0.068 0.470 0.105 0.065 0.521 0.104 0.077 0.528</cell></row><row><cell>Finetuned DistilBERT:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Concatenation strategy</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>no-concat</cell><cell></cell><cell></cell><cell>concat1</cell><cell></cell><cell></cell><cell>concat2</cell><cell></cell></row><row><cell></cell><cell>5</cell><cell>50</cell><cell></cell><cell>5</cell><cell>50</cell><cell></cell><cell>5</cell><cell>50</cell><cell></cell></row><row><cell>Threshold scheduling</cell><cell>ğ¸ğ‘…ğ·ğ¸</cell><cell>ğ¸ğ‘…ğ·ğ¸</cell><cell>lw-ğ¹ 1</cell><cell>ğ¸ğ‘…ğ·ğ¸</cell><cell>ğ¸ğ‘…ğ·ğ¸</cell><cell>lw-ğ¹ 1</cell><cell>ğ¸ğ‘…ğ·ğ¸</cell><cell>ğ¸ğ‘…ğ·ğ¸</cell><cell>lw-ğ¹ 1</cell></row><row><cell>constant, optimized for</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>... ğ¸ğ‘…ğ·ğ¸ 5</cell><cell cols="9">0.129 0.109 0.224 0.174 0.094 0.256 0.150 0.097 0.267</cell></row><row><cell>... ğ¸ğ‘…ğ·ğ¸ 50</cell><cell cols="9">0.167 0.106 0.244 0.171 0.099 0.251 0.151 0.083 0.308</cell></row><row><cell cols="10">... latency-weighted ğ¹ 1 0.174 0.114 0.261 0.173 0.096 0.258 0.158 0.085 0.315</cell></row><row><cell>exponential, optimized for</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>... ğ¸ğ‘…ğ·ğ¸ 5</cell><cell cols="9">0.086 0.083 0.391 0.105 0.102 0.272 0.094 0.088 0.353</cell></row><row><cell>... ğ¸ğ‘…ğ·ğ¸ 50</cell><cell cols="9">0.100 0.096 0.318 0.103 0.086 0.362 0.096 0.052 0.474</cell></row><row><cell cols="10">... latency-weighted ğ¹ 1 0.088 0.083 0.404 0.107 0.102 0.271 0.102 0.057 0.478</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,108.93,673.71,162.64,4.06"><p>https://github.com/saeub/eRisk2022-LauSAn</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="6,108.93,651.80,398.04,4.06;6,89.29,662.76,416.70,4.06;6,89.29,673.72,357.36,4.06"><p>The organizers also report ranking-based evaluation metrics (ranking based on model scores after 1, 100, 500, and 1000 posts). Since our model classifies only based on the local post history and does not accumulate scores across time, these measurements are not informative in our case, and we do not report them here.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="7,108.93,660.68,398.04,7.86;7,88.96,673.72,306.58,4.06"><p>An exception is the model trained with concat2 and threshold scheduling optimized for latency-weighted ğ¹1, where the optimized exponential function turns out to be equal to the constant one.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>We would like to thank <rs type="person">Simon Clematide</rs> and <rs type="person">Andrianos Michail</rs> for their valuable feedback and to the <rs type="institution">Department of Computational Linguistics</rs> for providing the technical infrastructure for our experiments.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,543.43,394.52,4.94;9,112.66,556.97,393.33,4.94;9,112.66,570.52,168.28,4.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,353.70,543.43,153.47,4.94;9,112.66,556.97,193.12,4.94">eRisk 2022: Pathological gambling, depression, and eating disorder challenges</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>MartÃ­n-Rodilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,333.57,556.97,172.42,4.94;9,112.66,570.52,38.01,4.94">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="436" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,584.07,394.52,4.94;9,112.48,597.62,397.66,4.94;9,112.36,610.35,73.62,7.90" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,283.82,584.07,218.88,4.94">Awareness, diagnosis, and treatment of depression</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Champion</surname></persName>
		</author>
		<idno type="DOI">10.1046/j.1525-1497.1999.03478.x</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,112.48,597.62,167.30,4.94">Journal of General Internal Medicine</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="569" to="580" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,624.72,279.35,4.94;9,422.75,624.72,85.08,4.94;9,112.66,638.27,292.38,4.94;9,431.07,638.27,75.32,4.94;9,112.66,651.82,395.01,4.94;9,112.66,665.37,108.74,4.94;9,262.82,665.37,244.84,4.94;10,112.66,89.41,394.61,7.90;10,112.41,103.78,42.98,4.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,422.75,624.72,85.08,4.94;9,112.66,638.27,288.17,4.94">Psychological aspects of natural language use: Our words, our selves</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Mehl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">G</forename><surname>Niederhoffer</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev.psych.54.101601.145041</idno>
		<idno>pMID: 12185209</idno>
		<ptr target="https://doi.org/10.1146/annurev.psych.54.101601.145041" />
	</analytic>
	<monogr>
		<title level="j" coord="9,431.07,638.27,75.32,4.94;9,112.66,651.82,76.11,4.94">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="547" to="577" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,117.33,393.33,4.94;10,112.66,130.88,219.53,4.94" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,237.98,117.33,222.71,4.94">Verbal behavior: Adaptation and psychopathology</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Weintraub</surname></persName>
		</author>
		<idno type="DOI">10.2307/3790837</idno>
	</analytic>
	<monogr>
		<title level="j" coord="10,469.52,117.33,36.46,4.94;10,112.66,130.88,51.17,4.94">Political Psychology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">107</biblScope>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,144.43,393.61,4.94;10,112.66,157.97,393.33,4.94;10,112.66,171.52,269.16,4.94" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,353.57,144.43,152.70,4.94;10,112.66,157.97,110.69,4.94">Overview of eRisk 2021: Early risk prediction on the internet</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>MartÃ­n-Rodilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,245.65,157.97,260.33,4.94;10,112.66,171.52,138.12,4.94">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="324" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,185.07,395.17,4.94;10,112.66,198.62,393.92,4.94;10,112.66,211.35,14.27,7.90" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,257.62,185.07,23.59,4.94;10,308.89,185.07,198.94,4.94;10,112.66,198.62,12.37,4.94">CLEF lab on early risk prediction on the internet</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-65813-1_30</idno>
	</analytic>
	<monogr>
		<title level="j" coord="10,132.34,198.62,111.34,4.94">Experimental foundations</title>
		<imprint>
			<biblScope unit="page" from="346" to="360" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note>eRisk</note>
</biblStruct>

<biblStruct coords="10,112.66,225.72,395.17,4.94;10,112.66,239.27,393.33,4.94;10,112.66,252.82,175.93,4.94" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,272.93,225.72,234.90,4.94;10,112.66,239.27,12.55,4.94">Overview of eRisk: early risk prediction on the internet</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,147.99,239.27,358.00,4.94;10,112.66,252.82,44.89,4.94">International Conference of the Cross-language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="343" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,266.37,393.33,4.94;10,112.66,279.92,394.53,4.94;10,112.66,293.47,336.73,4.94" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,236.78,266.37,269.21,4.94;10,112.66,279.92,139.90,4.94">Towards transfer learning using BERT for early detection of self-harm of social media users</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Nisa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Muhammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,277.59,279.92,229.60,4.94;10,112.66,293.47,201.59,4.94">Proceedings of the Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<meeting>the Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</meeting>
		<imprint>
			<date type="published" when="2021">September 21-24, 2021, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,307.02,393.33,4.94;10,112.66,320.56,393.98,4.94;10,112.41,334.11,12.55,4.94" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,366.61,307.02,139.38,4.94;10,112.66,320.56,248.95,4.94">Early risk detection of self-harm and depression severity using BERT-based transformers</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>MartÃ­nez-CastaÃ±o</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Htait</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Moshfeghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,370.15,320.56,106.75,4.94">Working Notes of CLEF</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,347.66,393.33,4.94;10,112.66,361.21,302.28,4.94" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A.-M</forename><surname>Bucur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cosma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">P</forename><surname>Dinu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.16175</idno>
		<title level="m" coord="10,264.59,347.66,241.40,4.94;10,112.66,361.21,119.00,4.94">Early risk detection of pathological gambling, self-harm and depression using BERT</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,374.76,394.53,4.94;10,112.39,388.31,315.48,4.94" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="10,221.46,374.76,281.34,4.94">A test collection for research on depression and language use</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-44564-9_3</idno>
		<imprint>
			<date type="published" when="2016">9822. 2016</date>
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,401.86,393.33,4.94;10,112.66,415.41,363.59,4.94" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="10,353.43,401.86,152.55,4.94;10,112.66,415.41,181.08,4.94">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,428.96,394.52,4.94;10,112.66,442.51,295.45,4.94" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<title level="m" coord="10,295.22,428.96,211.96,4.94;10,112.66,442.51,113.82,4.94">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,456.06,394.53,4.94;10,112.66,469.61,394.53,4.94;10,112.66,483.16,393.32,4.94;10,112.66,496.70,176.63,4.94" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,228.10,483.16,182.14,4.94">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,419.22,483.16,86.76,4.94;10,112.66,496.70,82.55,4.94">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,510.25,394.53,4.94;10,112.66,523.80,394.53,4.94;10,112.66,537.35,395.17,4.94;10,112.66,550.90,393.33,4.94;10,112.66,564.45,394.53,4.94;10,112.66,578.00,385.60,4.94" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,315.63,537.35,192.20,4.94;10,112.66,550.90,72.82,4.94">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m" coord="10,207.25,550.90,298.74,4.94;10,112.66,564.45,390.37,4.94">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
