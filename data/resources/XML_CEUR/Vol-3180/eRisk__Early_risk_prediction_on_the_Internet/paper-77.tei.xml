<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,402.01,15.42;1,89.29,106.66,331.59,15.42;1,88.78,128.58,417.11,15.43;1,89.29,150.49,354.43,15.43;1,89.29,172.41,226.00,15.43">NLP-IISERB@eRisk2022: Exploring the Potential of Bag of Words, Document Embeddings and Transformer Based Framework for Early Prediction of Eating Disorder, Depression and Pathological Gambling Over Social Media</title>
				<funder ref="#_pebjeVE">
					<orgName type="full">Indian Institute of Science Education and Research Bhopal, India</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,200.73,123.10,11.96"><forename type="first">Harshvardhan</forename><surname>Srivastava</surname></persName>
							<email>srivastavahv@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Oracle India Private Limited</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Data Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Science Education and Research Bhopal</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,225.04,200.73,22.52,11.96"><surname>Lijin</surname></persName>
							<email>lijin19@iiserb.ac.in</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Data Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Science Education and Research Bhopal</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,350.17,200.73,66.05,11.96"><forename type="first">Tanmay</forename><surname>Basu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Data Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Science Education and Research Bhopal</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,402.01,15.42;1,89.29,106.66,331.59,15.42;1,88.78,128.58,417.11,15.43;1,89.29,150.49,354.43,15.43;1,89.29,172.41,226.00,15.43">NLP-IISERB@eRisk2022: Exploring the Potential of Bag of Words, Document Embeddings and Transformer Based Framework for Early Prediction of Eating Disorder, Depression and Pathological Gambling Over Social Media</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">BF9DBE8A37A55CD4625569D0825CB90B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>information extraction</term>
					<term>depression detection</term>
					<term>identification of eating disorder</term>
					<term>text classification</term>
					<term>clinical text mining</term>
					<term>biomedical NLP</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The eRisk lab at CLEF 2022 had released three different tasks based on the posts of different users over Reddit, a popular social media. The first task was early detection of signs of pathological gambling. The second task was the early prediction of depression. The third one was assessing the severity of eating disorders over social media posts. The BioNLP research group at the Indian Institute of Science Education and Research Bhopal (IISERB) participated in all three tasks and submitted five runs using five different text mining frameworks for task 1 and task 2 and four different runs for task 3. The methods involve different feature engineering schemes and text classification techniques. The performance of the classical bag of words model, paragraph embedding technique and transformer-based models were explored to identify significant features from the given corpora. Moreover, we have identified features based on the biomedical concepts for pathological gambling using Unified Medical Language Systems, a repository for biomedical vocabularies. Subsequently, we have explored the performance of different classifiers, e.g., logistic regression, random forest etc. using various such features generated from the given data. The official results on the test data of individual tasks show that the proposed frameworks achieve top scores in terms of some of the evaluation techniques, e.g., precision, F1 score, speed etc. for all three tasks. The paper describes the performance, value and validity of the proposed frameworks for individual tasks and the scopes for further improvement.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Early risk prediction is a new research area potentially applicable to various situations, such as identifying people with a risk of mental disorders, which have become a predominant issue today. Especially for the population of people living in conflicted-affected areas, the chance of exposure to activities which can mentally affect them is very high. As mentioned in the study by Charlson et al. <ref type="bibr" coords="2,172.21,100.52,11.59,10.91" target="#b0">[1]</ref>, the estimated prevalence of mental disorders (i.e., depression, anxiety, post-traumatic stress disorder, bipolar disorder, and schizophrenia) was 22â€¢1% (95% UI 18â€¢8-25 <ref type="bibr" coords="2,493.37,114.06,13.26,10.91">â€¢7)</ref> at any point in time when assessed in the conflict-affected populations. A strong correlation also exists between the income class of an individual and the mental illnesses associated with that person and low levels of household income are associated with several lifetime mental disorders and suicide attempts, and a reduction in household income is associated with increased risk for incident mental disorders <ref type="bibr" coords="2,205.35,181.81,11.43,10.91" target="#b1">[2]</ref>.</p><p>With the advent of internet, online social platforms have become a regular media for almost all the people to share and express their thoughts and feelings freely and publicly with other people <ref type="bibr" coords="2,121.58,222.46,11.41,10.91" target="#b2">[3]</ref>. The information available over social media is a rich source for sentiment analysis or inferring mental health issues <ref type="bibr" coords="2,237.98,236.01,11.49,10.91" target="#b3">[4]</ref>. The CLEF eRisk 2022 shared task focuses on three tasks i.e., (i) early detection of signs of pathological gambling, (ii) early detection of depression and (iii) measuring the severity of the signs of eating disorders. The main goal of the eRisk 2022 challenge is to instigate discussion on the creation of reusable benchmarks for evaluating early risk detection algorithms by exploring issues of evaluation methodology, effectiveness metrics and other processes. Early identification advances can be utilized in various areas, especially those connected with the health and security of users interacting on the web and to identify the potential predators on the internet. The lab had organized three tasks this year and released different corpora for the individual tasks, which were developed using the postings of individual users over Reddit, a popular social media. We, the BioNLP group at IISERB, participated in all three tasks and performed reasonably well.</p><p>The performance of different feature engineering schemes and classification techniques was explored to identify pathological gambling, depression and eating disorder from the posts of the users over social media data released as part of individual shared tasks of the eRisk 2022. The proposed framework for task 1 and task 2 aims to train a machine learning classifier by using different types of features generated from the given training corpus to classify the documents of the test data. Note that the performance of a text classification technique is highly dependent on the important features of a corpus. Therefore the performance of different classifiers has been tested following different feature engineering schemes. The classical bag of words (BOW) model <ref type="bibr" coords="2,119.05,493.44,11.31,10.91" target="#b4">[5]</ref>, paragraph embeddings <ref type="bibr" coords="2,239.98,493.44,12.72,10.91" target="#b5">[6]</ref> and transformer architecture based deep learning models were used to generate features from the given corpora. Two different term weighting schemes were used for the BOW model, viz., term frequency and inverse document frequency-based term weighting scheme <ref type="bibr" coords="2,197.31,534.09,12.89,10.91" target="#b4">[5]</ref> and entropy-based term weighting scheme <ref type="bibr" coords="2,407.05,534.09,11.49,10.91" target="#b6">[7]</ref>. Furthermore, four different attention layer-based deep learning models, namely, BERT (Bidirectional Encoder Representations from Transformers) <ref type="bibr" coords="2,253.31,561.19,11.41,10.91" target="#b7">[8]</ref>, BioBERT <ref type="bibr" coords="2,306.48,561.19,15.10,10.91" target="#b8">[9]</ref>, RoBERTa <ref type="bibr" coords="2,367.22,561.19,21.64,10.91" target="#b9">[10]</ref> and Longformer <ref type="bibr" coords="2,461.30,561.19,20.24,10.91" target="#b10">[11]</ref> were used to generate semantic features from the given training data.</p><p>Subsequently, the performance of ada boost <ref type="bibr" coords="2,295.11,588.29,16.15,10.91" target="#b11">[12]</ref>, logistic regression <ref type="bibr" coords="2,400.58,588.29,16.15,10.91" target="#b12">[13]</ref>, random forest [14] and support vector machine <ref type="bibr" coords="2,219.34,601.84,18.07,10.91" target="#b14">[15]</ref> classifiers have been reported using the BOW features and the paragraph embeddings based features individually on the training corpus following 10 fold cross-validation technique. Therefore, the best five frameworks were chosen based on their performance on the training corpus in terms of F1 score and subsequently, they have been implemented on the test corpus. Similarly, the features generated by a transformer-based architecture were used to train the classifier of the same architecture using the training data following 10 fold cross-validation technique. Based on the decision-based results of task 1, the proposed Longformer model achieved the best score among all the submissions in terms of recall. The random forest classifier following the entropy-based term weighting scheme, achieved the top score in terms of recall, latency ğ‘‡ ğ‘ƒ and speed among all the runs of task 1. The proposed entropy-based term weighting scheme using support vector (SVM) classifier outperforms the other runs in terms of F1 score and latency-weighted F1 score <ref type="bibr" coords="3,370.37,154.71,18.02,10.91" target="#b15">[16]</ref> for decision-based results of task 2. For task 3, the semantic similarity between a given question and the posts of the Reddit users were identified using different similarity measures, e.g., Jaro-Winkler distance <ref type="bibr" coords="3,487.08,181.81,16.08,10.91" target="#b16">[17]</ref>, Cosine similarity <ref type="bibr" coords="3,165.96,195.36,17.75,10.91" target="#b17">[18]</ref> etc. The official results show that the proposed method using a pretrained BERT model and cosine similarity measure performed better than all the runs submitted by different teams in almost all the evaluation techniques used for task 3.</p><p>The paper is organized as follows. Section 2 describes the proposed frameworks for individual tasks. The experimental results are reported and analyzed in section 3. The conclusions and scopes of further works are presented in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed Frameworks for Individual Tasks</head><p>Different text mining frameworks were proposed based on the requirements of individual tasks. The documents of the given corpora for individual tasks were released in XML format. Each XML document contains the postings of a Reddit user over a period of time with the corresponding dates. We extracted these postings from the XML documents and ignored the other entries. Therefore the corpus used for experiments in this article contains only the texts related to different posts on Reddit for individual users. The proposed frameworks for task 1 and task 2 include different feature engineering schemes and classification techniques. For task 3, the proposed framework uses various semantic similarity measures to identify the similarity between a given question and possible answers among the posts of individual Reddit users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Feature Engineering Schemes for Task 1 and Task 2 2.1.1. Bag Of Words Features</head><p>The text documents are generally represented by the bag of words (BOW) model <ref type="bibr" coords="3,474.20,500.38,11.58,10.91" target="#b4">[5]</ref>. In this model, each document in a corpus is generally represented by a vector, whose length is equal to the number of unique terms, also known as vocabulary <ref type="bibr" coords="3,403.84,527.48,11.58,10.91" target="#b4">[5]</ref>. The conventional term weighting scheme is known as term frequency and inverse document frequency or tfidf. Document frequency (df) is the number of documents in which a term appears. Inverse document frequency determines how frequently a term occurs in a corpus and it is defined as</p><formula xml:id="formula_0" coords="3,89.29,579.48,113.94,13.56">ğ‘–ğ‘‘ğ‘“ ğ‘¡ğ‘’ğ‘Ÿğ‘š = ğ‘™ğ‘œğ‘”( #ğ‘‘ğ‘œğ‘ğ‘¢ğ‘šğ‘’ğ‘›ğ‘¡ğ‘ </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ğ‘‘ğ‘“ğ‘¡ğ‘’ğ‘Ÿğ‘š</head><p>). The weight of a term in a document, is determined by multiplying its term frequency with inverse document frequency. Moreover, the entropy based term weighting technique is used by many researchers to form term-document matrix from a text collection <ref type="bibr" coords="3,492.63,608.77,11.28,10.91" target="#b6">[7]</ref>. This method developed in the spirit that the more important term is the more frequent one that occurs in fewer documents, taking the distribution of the term over the corpus into account <ref type="bibr" coords="3,492.62,635.87,11.28,10.91" target="#b6">[7]</ref>. The weight of a term in a document, is determined by the entropy 1 of term frequency of the term in that document <ref type="bibr" coords="4,192.23,86.97,11.43,10.91" target="#b6">[7]</ref>.</p><p>The BOW model generally creates sparse and high dimensional term-document matrices, which may affect the performance of the classifiers. Hence ğœ’<ref type="foot" coords="4,352.58,112.84,4.23,6.99" target="#foot_1">2</ref> -statistic <ref type="bibr" coords="4,397.09,114.06,17.76,10.91" target="#b18">[19]</ref> based term selection technique was used to identify important terms from the term-document matrix, which is a widely used technique for term selection <ref type="bibr" coords="4,274.32,141.16,16.30,10.91" target="#b18">[19]</ref>. We have considered different number of terms generated by ğœ’ 2 -statistic and evaluated the performance of individual classifiers using these set of terms on the training corpus. The best set of terms are used for experiments on the test data. These BOW features are used for the given data of task 1 and task 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Paragraph Embeddings Based Features</head><p>The unsupervised paragraph embeddings technique, also known as Doc2Vec model can express a document as a vector <ref type="bibr" coords="4,189.78,244.69,12.97,10.91" target="#b5">[6]</ref>, which can identify semantic similarity between two documents by comparing the corresponding vectors. It was developed based on unsupervised Continuous Bag of Words (CBOW) and Skip-grams model, which expresses a word as a vector <ref type="bibr" coords="4,453.65,271.79,17.82,10.91" target="#b19">[20]</ref> using a given corpus and combines them to learn paragraph or document level embeddings <ref type="bibr" coords="4,469.79,285.34,11.57,10.91" target="#b5">[6]</ref>. The Doc2Vec model is trained on the individual training corpora of task 1 and task 2 to generate the embeddings from individual documents of these corpora. Therefore it was used to generate the features for individual documents of the test data for task1 and task 2. The number of such features was fixed by performing 10-fold cross validation technique on the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">UMLS Features</head><p>We have also considered the UMLS (Unified Medical Language System) <ref type="bibr" coords="4,403.83,388.86,17.78,10.91" target="#b20">[21]</ref> concepts extracted from the text as features for task 1 only. We could not find many such features for task 2 data and hence did not use it for task 2. UMLS is a comprehensive list of biomedical terms for developing automated systems capable of understanding the specialized vocabulary used in biomedicine and health care <ref type="bibr" coords="4,233.80,443.06,16.42,10.91" target="#b20">[21]</ref>. In UMLS there are 133 semantic categories 2 related to biomedicine and health. The semantic category of a term can be identified using MetaMap <ref type="foot" coords="4,491.67,454.85,3.71,7.97" target="#foot_2">3</ref> , a tool to recognize UMLS concepts in text data <ref type="bibr" coords="4,287.64,470.16,16.09,10.91" target="#b21">[22]</ref>. MetaMap first breaks the text into terms and then for each term it returns different semantic categories and ranked these categories according to a confidence score. It generates a Concept Unique Identifier (CUI) for each term belong to a particular semantic category <ref type="bibr" coords="4,229.41,510.80,16.40,10.91" target="#b21">[22]</ref>. We have used these CUIs as features and they are called UMLS features in this paper. UMLS features belong to five relevant semantic categories e.g., acquired abnormality, mental and behavioral dysfunction, etc. were considered for experiments for task 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Text Classification Techniques for Task 1 and Task 2 2.2.1. Classical Methods</head><p>Different text classification methods were used for task 1 and task 2 using BOW features, features generated by Doc2Vec model and UMLS features. The Adaptive Boosting (AB), Logistic Regression (LR), Random Forest (RF) and Support Vector Machine (SVM) classifiers were used for task 1 and task 2. The significant parameters of the individual classifiers were selected by using the grid search technique<ref type="foot" coords="5,228.83,112.31,3.71,7.97" target="#foot_3">4</ref> following 10-fold cross validation model on the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Transformer Architecture Based Embeddings</head><p>Multiple transformer architecture based models were used for task 1 to get the best embeddings for the given training corpus. The aim was to capture long range dependency and context of the conversations effectively. The first model that we explored is BERT (Bidirectional Encoder Representations from Transformers), which is a contextualized word representation model that is based on a masked language model and pre-trained using bidirectional transformers <ref type="bibr" coords="5,462.27,216.69,11.28,10.91" target="#b7">[8]</ref>. It was pre-trained on general domain corpora i.e., English Wikipedia and books <ref type="bibr" coords="5,412.50,230.23,11.34,10.91" target="#b7">[8]</ref>. We also explored two widely used extensions of BERT i.e., BioBERT <ref type="bibr" coords="5,320.42,243.78,11.58,10.91" target="#b8">[9]</ref>, which is trained on PubMED articles and RoBERTa <ref type="bibr" coords="5,155.01,257.33,18.06,10.91" target="#b9">[10]</ref> that is trained on a news corpus by fixing some specific parameters and training strategies of BERT. Another alternative of BERT, the Longformer model has significant advantages over BERT to identify long term dependency in the given texts <ref type="bibr" coords="5,433.49,284.43,16.42,10.91" target="#b10">[11]</ref>. It presents a different attention mechanism that developed in conjunction with successive length of the document size using a sliding window technique <ref type="bibr" coords="5,308.93,311.53,16.25,10.91" target="#b10">[11]</ref>. We have used the pretrained models of BERT, BioBERT, RoBERTa and Longformer from the Hugging Face repository <ref type="foot" coords="5,433.87,323.32,3.71,7.97" target="#foot_4">5</ref> and fine-tuned them individually on the given training corpus of task 1 and another Reddit data for pathological gambling <ref type="bibr" coords="5,133.66,352.18,16.25,10.91" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Semantic Similarity Based Measures for Task 3</head><p>The objective of task 3 is to fill out a standard eating disorder questionnaire based on the evidence found in the history of postings of individual users. Hence the aim here is to find the contextual similarity between a given question and the posts of users for a period of time to generate a score between 0 to 6 to identify the severity of eating disorder. The performance of the following semantic similarity measures are explored in order to achieve this objective.</p><p>Jaccard similarity <ref type="bibr" coords="5,179.02,468.74,16.30,10.91" target="#b17">[18,</ref><ref type="bibr" coords="5,197.97,468.74,12.50,10.91" target="#b23">24,</ref><ref type="bibr" coords="5,213.11,468.74,13.95,10.91" target="#b24">25]</ref> is the ratio of common words between two sets of texts and the total unique words of these two sets. It ranges in [0,1], where 1 represents highest similarity and 0 represents no similarity between two sets of texts. Let X and Y be two sets of texts. The Jaccard similarity between X and Y can be defined as <ref type="bibr" coords="5,212.18,557.68,17.76,10.91" target="#b16">[17]</ref> is a string metric used for estimating the edit distance between two sets of texts. The lower the Jaro-Winkler distance for two strings is, the more similar the strings are. The score is normalized such that 1 means an exact match and 0 means there is no similarity. The Jaro-Winkler distance between X and Y is defined as follows:</p><formula xml:id="formula_1" coords="5,89.12,527.00,262.52,41.60">Jaccard (X,Y) = |ğ‘‹ âˆ© ğ‘Œ | |ğ‘‹ âˆª ğ‘Œ | Jaro-Winkler distance (ğ½ğ·)</formula><formula xml:id="formula_2" coords="5,158.69,616.79,276.69,32.67">Jaro-Winkler (X,Y) = 1 - {ï¸ƒ 0, if m = 0 1 3 (ï¸ ğ‘š |ğ‘‹| + ğ‘š |ğ‘Œ | + ğ‘š-ğ‘¡ ğ‘š )ï¸ , otherwise</formula><p>where ğ‘š and ğ‘¡ are respectively the number of common characters and number of transpositions between X and Y. Cahyono had shown that the Jaro-Winkler distance worked very well for plagiarism detection <ref type="bibr" coords="6,180.79,114.06,16.09,10.91" target="#b25">[26]</ref>. However, to our knowledge, this distance function has never used for identifying the severity of eating disorders or in any other such shared tasks of the earlier eRisk labs.</p><p>Cosine similarity between two documents <ref type="bibr" coords="6,291.25,154.71,12.88,10.91" target="#b4">[5]</ref> is measured as the similarity of the cosine of the angle between two document vectors. Cosine similarity between X and Y can be defined as</p><formula xml:id="formula_3" coords="6,253.23,189.36,87.62,29.06">Cos (X,Y) = ğ‘‹ âƒ— â€¢ ğ‘Œ âƒ— |ğ‘‹ âƒ— ||ğ‘Œ âƒ— |</formula><p>Here ğ‘‹ âƒ— and ğ‘Œ âƒ— are represented following the tf-idf weighting scheme of the BOW model. Cosine similarity <ref type="bibr" coords="6,134.58,244.95,12.81,10.91" target="#b4">[5]</ref> ranges in [0,1], where 1 indicates highest similarity and 0 indicates no similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>The organizers released individual corpora for the given tasks using the postings of the users over Reddit for a given time period. The data were released in XML format with the identity, timestamp, title and postings of individual users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Task 1:</head><p>The given training corpus of task 1 had two categories -pathological gambling and control group. In the training data, 164 users were marked as pathological gamblers and 2184 users were marked as control group, whereas in the test corpus, 81 users were marked as pathological gamblers, and 1998 users were marked as control group. The above statistics of the dataset clearly indicate that the users marked as pathological gamblers are observably smaller than the control group which during the training period creates preference by the models to treat the pathological gambling class as a stochastic error and created problems when generalising the values. In addition to the given training corpus, we have used two other Reddit corpora for pathological gambling <ref type="bibr" coords="6,192.41,508.75,18.01,10.91" target="#b22">[23]</ref> 67 and added them to the pathological gambling class of the given training data to train different classifiers. Posts in these two external Reddit corpora are mostly related to gambling addiction <ref type="bibr" coords="6,221.03,535.85,16.13,10.91" target="#b22">[23]</ref>. We had done the experiments using both the given training data for task 1 and adding the external Reddit corpus to the given training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Task 2:</head><p>The given training corpus of task2 had two categories: the depression and control groups. In the training data, 214 users were marked as depressed users and 1493 users were marked as control group, whereas in the test corpus, 98 users were marked as depressed group and 1302 users were marked as control group. No external data was used to train the classifiers for task 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Task 3:</head><p>The data set of task 3 comprises postings of individual users for a given period of time and a questionnaire having 22 different questions. The goal is to assess the degree of severity of eating disorders (scaled between 0 to 6) of a user for each of these questions based on Reddit postings. The ratings are an indication of the degree of agreement that the user has with the question, 0 meaning that the user in in disagreement with the hypothesis of the question and 6 meaning that the user is in maximum agreement to the hypothesis. Since no ground truths were provided for this data set, we used an anorexia data set of eRisk 2018 shared task 2 <ref type="bibr" coords="7,476.71,188.83,17.77,10.91" target="#b26">[27]</ref> to train the BERT model <ref type="bibr" coords="7,188.63,202.38,12.84,10.91" target="#b7">[8]</ref> in one of our runs submitted for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experimental Setup</head><p>We have submitted multiple runs following different frameworks for each of the tasks. For task 1 and task 2, we have evaluated the performance of different feature engineering techniques and the classifiers following 10 fold cross-validation method on the training corpus. We have chosen the five best frameworks to be tested on the test corpus. AB, LR, RF and SVM classifiers are implemented in Scikit-learn <ref type="foot" coords="7,278.59,304.55,3.71,7.97" target="#foot_7">8</ref> , a machine learning tool in Python. To overcome the effect of majority class over the classifiers, the balanced mode is used for each classifier, which automatically adjust weights of individual classes inversely proportional to the class frequencies in the training data <ref type="foot" coords="7,227.25,345.20,3.71,7.97" target="#foot_8">9</ref> . Doc2Vec is implemented in Gensim<ref type="foot" coords="7,393.21,345.20,7.41,7.97" target="#foot_9">10</ref> , a deep learning library package in Python. We have used BERT, Bio-BERT, RoBERTa and Longformer models from the HuggingFace library <ref type="foot" coords="7,180.93,372.29,7.41,7.97" target="#foot_10">11</ref> .</p><p>The performance of the proposed frameworks using the training set were evaluated in terms of precision, recall and F1 score <ref type="bibr" coords="7,217.49,414.70,16.09,10.91" target="#b27">[28]</ref>. In addition to that, the organizers evaluated the performance of the runs in terms of ERDE 5 <ref type="bibr" coords="7,219.55,428.25,17.76,10.91" target="#b28">[29]</ref> ERDE 50 <ref type="bibr" coords="7,274.16,428.25,16.09,10.91" target="#b28">[29]</ref>, latency ğ‘‡ ğ‘ƒ <ref type="bibr" coords="7,342.50,428.25,16.09,10.91" target="#b29">[30]</ref>, speed <ref type="bibr" coords="7,392.05,428.25,17.75,10.91" target="#b29">[30]</ref> and latency-weighted F1 score <ref type="bibr" coords="7,128.38,441.80,16.25,10.91" target="#b29">[30]</ref>.</p><p>The performance of the runs of task 3 was evaluated in terms of Mean Zero-One Error (MZOE), Mean Absolute Error (MAE), Macroaveraged Mean Absolute Error (MAE ğ‘šğ‘ğ‘ğ‘Ÿğ‘œ ), Restraint Subscale (RS), Eating Concern Subscale (ECS), Shape Concern Subscale (SCS), Weight Concern Subscale (WCS) and Global Eating Disorder (GED) <ref type="bibr" coords="7,328.83,509.54,16.41,10.91" target="#b15">[16]</ref>. These evaluation techniques are described in the overview paper of the eRisk 2022 shared task <ref type="bibr" coords="7,366.41,523.09,16.25,10.91" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Analysis of Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Task 1: Early Prediction of Pathological Gambling</head><p>Initially, we have implemented four classifiers using three different feature engineering schemes individually on the given training corpus. Moreover, we have used two relevant Reddit data sets collected from different resources <ref type="bibr" coords="8,263.45,441.11,18.05,10.91" target="#b22">[23]</ref> and appended them to the pathological gambling category of the given training data. Subsequently, experiments conducted using all three feature engineering schemes and the classifiers on this appended data set. However, only entropy-based BOW features worked well on this appended data set and hence we reported these results in Table <ref type="table" coords="8,116.10,495.31,3.78,10.91" target="#tab_0">1</ref>. Moreover, we reported the results by combining BOW and UMLS features following entropy-based term weighting scheme for all classifiers. We had implemented all the classifiers using just UMLS features following entropy-based term weighting scheme. However, none of the classifiers performed reasonably well, and hence we did not report these results in Table <ref type="table" coords="8,89.04,549.51,3.78,10.91" target="#tab_0">1</ref>. The performances of these frameworks are reported in Table <ref type="table" coords="8,375.49,549.51,5.13,10.91" target="#tab_0">1</ref> in terms of precision, recall and F1-score. These results help to analyze the performance of proposed frameworks on the training set. Thereafter, the top five frameworks from Table <ref type="table" coords="8,350.98,576.61,4.97,10.91" target="#tab_0">1</ref> in terms of F1-score were selected and subsequently implemented on the given test corpus. Eventually, the performance of these five frameworks on the test corpus was communicated as official results of our team for task 1.</p><p>It can be seen from Table <ref type="table" coords="8,219.80,630.80,5.17,10.91" target="#tab_0">1</ref> that RF performs better than the other classifiers in terms of F1 score following the Entropy-based term weighting scheme of the BOW model using both the given training data and a relevant Reddit dataset collected from another resource <ref type="bibr" coords="8,487.15,657.90,16.41,10.91" target="#b22">[23]</ref>. RF outperforms the other classifiers following the TF-IDF based term weighting scheme for BOW features in terms of F1 score. AB classifier beats other classifiers in terms of F1 score using the UMLS features following Entropy-based weighing scheme. The Longformer model performs better than the other transformer-based models based on the F1 score. Following their performance on the training corpus in terms of F1 score, these five frameworks had been implemented on the test corpus. For Doc2Vec based features, LR and AB classifiers beat the other classifiers based on F1 score, but these scores do not belong to the top five F1 scores in Table <ref type="table" coords="9,115.79,385.85,5.07,10.91" target="#tab_0">1</ref> and hence these models were not implemented on the test set.</p><p>The decision based results of the five runs on the test corpus in terms of precision, recall, F1 score, ğ¸ğ‘…ğ·ğ¸ 5 <ref type="bibr" coords="9,159.06,426.50,16.12,10.91" target="#b15">[16]</ref>, ğ¸ğ‘…ğ·ğ¸ 50 <ref type="bibr" coords="9,228.07,426.50,16.12,10.91" target="#b15">[16]</ref>, ğ¿ğ‘ğ‘¡ğ‘’ğ‘›ğ‘ğ‘¦ ğ‘‡ ğ‘ƒ <ref type="bibr" coords="9,304.46,426.50,17.79,10.91" target="#b15">[16]</ref> and speed <ref type="bibr" coords="9,371.60,426.50,16.12,10.91" target="#b15">[16]</ref>, are reported in Table <ref type="table" coords="9,488.87,426.50,3.68,10.91" target="#tab_1">2</ref>. It can be seen from this table that the NLPGroup-IISERB 4 run achieves the best precision score (1.0) among the precision scores of all 41 submissions for task 1 of the eRisk2022 challenge. The recall scores of NLPGroup-IISERB 1 (1.0), NLPGroup-IISERB 2 (1.0), and NLPGroup-IISERB 3 (1.0) runs are equal, and these are the best recall scores for task 1 among all submissions. The performance of NLPGroup-IISERB 2 run in terms of latency ğ‘‡ ğ‘ƒ (1.0) and speed (1.0) performs better than all submissions for task 1. However, none of our submissions performs reasonably well in terms of F1 score, ğ¸ğ‘…ğ·ğ¸ 5 , ğ¸ğ‘…ğ·ğ¸ 50 , and latency-weighted F1 score.</p><p>The ranking based results of the five runs on the test corpus in terms of precision, recall, F1 score, ğ¸ğ‘…ğ·ğ¸ 5 <ref type="bibr" coords="9,159.06,561.99,16.12,10.91" target="#b15">[16]</ref>, ğ¸ğ‘…ğ·ğ¸ 50 <ref type="bibr" coords="9,228.07,561.99,16.12,10.91" target="#b15">[16]</ref>, ğ¿ğ‘ğ‘¡ğ‘’ğ‘›ğ‘ğ‘¦ ğ‘‡ ğ‘ƒ <ref type="bibr" coords="9,304.46,561.99,17.79,10.91" target="#b15">[16]</ref> and speed <ref type="bibr" coords="9,371.60,561.99,16.12,10.91" target="#b15">[16]</ref>, are reported in Table <ref type="table" coords="9,488.87,561.99,3.68,10.91">3</ref>. It can be seen from this table that none of our runs performs reasonably well in terms of all the evaluation metrics. We may consider this as one of the limitations of the proposed models for task 1, and we plan to investigate them further in future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Task 2: Early Detection of Depression</head><p>We have implemented four different classifiers using three different feature engineering techniques individually on the training corpus. The performance of each of these models was best score in terms of NDCG@100 score among all the runs. We could not submit the results for 1000 writings for task 2 within the given deadline and hence we could not achieve any score for the same.</p><p>It may be noted from Table <ref type="table" coords="12,221.75,502.08,4.98,10.91">4</ref> and Table <ref type="table" coords="12,274.67,502.08,4.98,10.91">5</ref> that the proposed frameworks using SVM classifier have high recall scores, but random forest classifier based models achieved high precision scores. Moreover, the SVM classifier using the BOW features following entropy based term weighting scheme consistently performed the best in terms of most of the decision based and ranking based evaluation metrics. Hence we may conclude that proposed model using entropy based BOW features and SVM classifier is an effective and robust model for early prediction of depression over social media.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Task 3: Measuring the severity of the signs of Eating Disorders</head><p>The performance of the four runs on the test corpus in terms of different evaluation measures <ref type="bibr" coords="12,89.29,646.26,18.04,10.91" target="#b15">[16]</ref> as described in section 3.2 are reported in Table <ref type="table" coords="12,327.05,646.26,3.80,10.91">7</ref>. It can be seen from this table that the NLPGroup-IISERB 2 run, which is a combination of cosine similarity and BERT model fine-tuned on anorexia dataset from eRisk 2018 shared task 2 <ref type="bibr" coords="13,339.26,86.97,17.80,10.91" target="#b26">[27]</ref> performed the best among all the other runs for task 3 in terms of all the evaluation metrics except MZOE metric. The proposed models performed well in terms of GED score indicate that they identify eating disorder and its side-effects reasonably well. The reason is that GED is indicative of the overall score of the 4 metrics RS, ECS, SCS and WCS respectively, which relate to restraint, eating, shape and weight concerns that are further associated with psychological effects of eating disorder. Moreover, NLPGroup-IISERB 1 and NLPGroup-IISERB 3 runs, respectively, achieved the second best and third best scores among all the other submissions for task 2 in terms of all metrics except the MZOE metric. Being unsupervised in nature, the proposed models for task 3 performed reasonably well in measuring the severity of eating disorders. These results indicate the value and validity of the proposed models for task 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>The eRisk 2022 shared task highlights various challenges for early detection of depression and pathological gambling using the data of different users over Reddit for a given time period. We have proposed various text mining frameworks using different features from the given corpora to accomplish the given tasks. It has been observed from the empirical analysis that the classical BOW model performs better than all the deep learning-based models on the given data except the longformer model. Note that the embeddings were generated following the Doc2Vec model and transformer-based architecture using the given training corpus of the individual tasks, which have a reasonably low number of documents compared to the other pre-trained deep learning-based embeddings e.g., fasttext, which were trained on huge text collections. Consequently, these deep learning models cannot correctly represent the semantic interpretations of the given documents, and hence their performances are not as good as the classical BOW model. The Longformer model performed as good as the BOW model for Task1, but we could not explore its performance for task2 owing to time limitations. In the future, we plan to build a large training corpus by collecting data from Reddit and similar forums for early prediction of risks of different mental illnesses to develop pretrained longformer based embeddings to further improve the performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,88.99,90.49,411.71,326.27"><head>Table 1</head><label>1</label><figDesc>Task1: Performance of Different Frameworks on the Training Corpus</figDesc><table coords="8,94.58,118.58,406.13,298.18"><row><cell>Feature Types</cell><cell>Classifier</cell><cell cols="3">Precision Recall F1 Score</cell></row><row><cell></cell><cell>AdaBoost</cell><cell>0.98</cell><cell>0.99</cell><cell>0.98</cell></row><row><cell>Entropy Based BOW Features</cell><cell>Logistic Regression</cell><cell>0.91</cell><cell>0.95</cell><cell>0.93</cell></row><row><cell>(Using given training data)</cell><cell>Random Forest</cell><cell>1.00</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell></cell><cell>Support Vector Machine</cell><cell>0.95</cell><cell>0.92</cell><cell>0.94</cell></row><row><cell></cell><cell>AdaBoost</cell><cell>0.97</cell><cell>0.88</cell><cell>0.92</cell></row><row><cell>Entropy Based BOW Features</cell><cell>Logistic Regression</cell><cell>0.81</cell><cell>0.95</cell><cell>0.86</cell></row><row><cell cols="2">(Using a Reddit data from another resource Random Forest</cell><cell>0.98</cell><cell>0.97</cell><cell>0.97</cell></row><row><cell>along with given training data)</cell><cell>Support Vector Machine</cell><cell>0.92</cell><cell>0.87</cell><cell>0.89</cell></row><row><cell></cell><cell>AdaBoost</cell><cell>0.98</cell><cell>0.98</cell><cell>0.98</cell></row><row><cell cols="2">Entropy Based BOW and UMLS Features Logistic Regression</cell><cell>0.88</cell><cell>0.96</cell><cell>0.92</cell></row><row><cell>(Using given training data)</cell><cell>Random Forest</cell><cell>0.96</cell><cell>0.96</cell><cell>0.96</cell></row><row><cell></cell><cell>Support Vector Machine</cell><cell>0.89</cell><cell>0.94</cell><cell>0.91</cell></row><row><cell></cell><cell>AdaBoost</cell><cell>0.98</cell><cell>0.98</cell><cell>0.98</cell></row><row><cell>TF-IDF Based BOW Features</cell><cell>Logistic Regression</cell><cell>0.86</cell><cell>0.95</cell><cell>0.90</cell></row><row><cell>(Using given training data)</cell><cell>Random Forest</cell><cell>1.00</cell><cell>0.95</cell><cell>0.97</cell></row><row><cell></cell><cell>Support Vector Machine</cell><cell>0.93</cell><cell>0.95</cell><cell>0.94</cell></row><row><cell></cell><cell>AdaBoost</cell><cell>0.92</cell><cell>0.89</cell><cell>0.90</cell></row><row><cell>Doc2Vec Based Features</cell><cell>Logistic Regression</cell><cell>0.90</cell><cell>0.96</cell><cell>0.92</cell></row><row><cell>(Using given training data)</cell><cell>Random Forest</cell><cell>0.98</cell><cell>0.86</cell><cell>0.91</cell></row><row><cell></cell><cell>Support Vector Machine</cell><cell>0.89</cell><cell>0.95</cell><cell>0.92</cell></row><row><cell></cell><cell>BERT</cell><cell>0.98</cell><cell>0.77</cell><cell>0.84</cell></row><row><cell>Transformer Based Features</cell><cell>RoBERTa</cell><cell>0.98</cell><cell>0.74</cell><cell>0.82</cell></row><row><cell>(Using given training data)</cell><cell>Longformer</cell><cell>0.94</cell><cell>0.89</cell><cell>0.91</cell></row><row><cell></cell><cell>BioBERT</cell><cell>0.7</cell><cell>0.85</cell><cell>0.75</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,88.99,90.49,407.66,177.81"><head>Table 2</head><label>2</label><figDesc>Task1: Decision-Based Results obtained on the test set This model is trained using two Reddit data sets collected from two other resources</figDesc><table coords="9,89.29,116.35,407.36,151.59"><row><cell>Runs</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell cols="3">ERDE 5 ERDE 50 latency ğ‘‡ ğ‘ƒ speed</cell><cell>latency</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>weighted F1</cell></row><row><cell cols="5">NLPGroup-IISERB 0 0.107 0.642 0.183 0.030</cell><cell>0.025</cell><cell>2.0</cell><cell>0.996</cell><cell>0.182</cell></row><row><cell>(BOW+TF-IDF+RF)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">NLPGroup-IISERB 1 0.044 1.000 0.084 0.046</cell><cell>0.033</cell><cell>3.0</cell><cell>0.992</cell><cell>0.083</cell></row><row><cell>(BOW+Entropy+RF)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">NLPGroup-IISERB 2 0.043 1.000 0.083 0.041</cell><cell>0.034</cell><cell>1.0</cell><cell>1.000</cell><cell>0.083</cell></row><row><cell>(BOW+Entropy+RF)  â€ </cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">NLPGroup-IISERB 3 0.140 1.000 0.246 0.025</cell><cell>0.014</cell><cell>2.0</cell><cell>0.996</cell><cell>0.245</cell></row><row><cell>(Longformer)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">NLPGroup-IISERB 4 1.000 0.074 0.138 0.038</cell><cell>0.037</cell><cell>41.5</cell><cell>0.843</cell><cell>0.116</cell></row><row><cell>(UMLS+Entropy +AB)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>â€ </cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,108.93,671.04,241.37,8.97"><p>https://radimrehurek.com/gensim/models/logentropy_model.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,108.93,660.07,200.94,8.97"><p>https://mmtx.nlm.nih.gov/MMTx/semanticTypes.shtml</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,108.93,671.03,106.05,8.97"><p>https://metamap.nlm.nih.gov</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,108.93,660.08,343.36,8.97"><p>https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,108.93,671.04,84.73,8.97"><p>https://huggingface.co/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="6,108.93,660.08,171.46,8.97"><p>https://www.reddit.com/r/GamblingAddiction/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="6,108.93,671.04,164.00,8.97"><p>https://www.reddit.com/r/problemgambling/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="7,108.93,638.14,199.52,8.97"><p>http://scikit-learn.org/stable/supervised_learning.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="7,108.93,649.10,263.05,8.97"><p>https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9" coords="7,108.93,660.06,327.64,8.97"><p>https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.Doc2Vec</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10" coords="7,108.93,671.02,84.73,8.97"><p>https://huggingface.co/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p><rs type="person">Tanmay Basu</rs> acknowledges the support of the seed funding (<rs type="grantNumber">PPW/R&amp;D/2010006</rs>) provided by <rs type="funder">Indian Institute of Science Education and Research Bhopal, India</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pebjeVE">
					<idno type="grant-number">PPW/R&amp;D/2010006</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>reported in Table <ref type="table" coords="10,166.61,537.48,4.97,10.91">4</ref> in terms of precision, recall and F1 score. These results were used to analyze the performance of the proposed models on the training set. Subsequently, the best five models from Table <ref type="table" coords="10,141.31,564.57,5.17,10.91">4</ref> in terms of F1-score had been selected and then implemented on the given test corpus. Finally, the performances of these five models on the test corpus were communicated as official results of our team.</p><p>It can be seen from Table <ref type="table" coords="10,219.28,605.22,5.17,10.91">4</ref> that Entropy-based BOW features yielded better results than TF-IDF and Doc2Vec based features for all the classifiers. The AB and SVM classifiers for entropy based BOW features performed better than all the other models in terms of F1 score. Table <ref type="table" coords="10,501.08,632.32,5.15,10.91">4</ref> shows that the performance of RF classifier using Entropy-based feature engineering scheme is reasonably well. Hence we have selected these three models to be implemented on the test data. It may be noted from Table <ref type="table" coords="11,235.76,281.48,5.01,10.91">4</ref> that the LR classifier performed better than the RF classifier using BOW features following Entropy-based term weighting scheme in terms of F1 score. However, we did not select it to implement on the test corpus as LR often performs the same as of SVM. We also selected the best models of the TF-IDF based feature engineering scheme and the Doc2Vec based model to implement on the test data. Thus we have submitted a total of five runs using the test data for evaluation. Note that for Doc2Vec based model we could not implement the AB classifier within the deadline and hence this result is not reported in Table <ref type="table" coords="11,89.04,376.33,3.81,10.91">4</ref>. Moreover, we could not implement the transformer based models for this task due to the limitation of time.</p><p>The decision based results of the five runs on the test corpus in terms of precision, recall, F1 score, ğ¸ğ‘…ğ·ğ¸ 5 <ref type="bibr" coords="11,171.71,416.97,16.09,10.91" target="#b15">[16]</ref>, ğ¸ğ‘…ğ·ğ¸ 50 <ref type="bibr" coords="11,240.59,416.97,16.09,10.91" target="#b15">[16]</ref>, ğ¿ğ‘ğ‘¡ğ‘’ğ‘›ğ‘ğ‘¦ ğ‘‡ ğ‘ƒ <ref type="bibr" coords="11,316.87,416.97,17.76,10.91" target="#b15">[16]</ref> and speed <ref type="bibr" coords="11,383.66,416.97,16.09,10.91" target="#b15">[16]</ref>, are reported in Table <ref type="table" coords="11,500.34,416.97,3.66,10.91">5</ref>. It may be noted that the NLPGroup-IISERB 0 run performed best in terms of F1 score (0.712) and latency weighted F1 score (0.690) among all the 62 runs submitted for task 2. Moreover, the performance of NLPGroup-IISERB 3 run performed second best in terms of F1 score (0.566) among all other runs. The precision scores of NLPGroup-IISERB 0 and NLPGroup-IISERB 2 runs respectively were the second (0.682) and third best (0.662) among all the submissions. The proposed models performed reasonably well in terms of other evaluation metrics for task 2, but could achieve a place in the top three positions. These results indicate the effectiveness of the proposed models.</p><p>Ranking based evaluation ranks the users in decreasing estimation of risk with the help of standard IR metrics, such as P@10 or Normalized Discounted Cumulative Gain (NDCG) <ref type="bibr" coords="11,89.29,566.02,16.27,10.91" target="#b15">[16]</ref>. Table <ref type="table" coords="11,139.70,566.02,5.09,10.91">6</ref> shows that the scores are not reasonably well for the first writing, except for the NLPGroup-IISERB 2 run. However, considering 100 writings, NLPGroup-IISERB 0, NLPGroup-IISERB 1 and NLPGroup-IISERB 4 runs outperform all other submissions in terms P@10 metric for task2. NLPGroup-IISERB 0 and NLPGroup-IISERB 4 runs performed the second best among all other runs in terms of NDCG@10 score, while the NLPGroup-IISERB 4 run performed second best among all submissions in terms of NDCG@100 score. For 500 writings, NLPGroup-IISERB 0 and NLPGroup-IISERB 4 runs perform better than all other submissions of the challenge in terms of P@10 and NDCG@10 metrics. Moreover, NLPGroup-IISERB 4 run achieves the second </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="13,112.66,615.12,393.73,10.91;13,112.14,628.67,394.26,10.91;13,112.66,642.22,205.59,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,485.40,615.12,20.99,10.91;13,112.14,628.67,394.26,10.91;13,112.66,642.22,78.45,10.91">New WHO prevalence estimates of mental disorders in conflict settings: a systematic review and meta-analysis</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Charlson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Van Ommeren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Flaxman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cornett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Whiteford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,199.59,642.22,29.65,10.91">Lancet</title>
		<imprint>
			<biblScope unit="volume">394</biblScope>
			<biblScope unit="page" from="240" to="248" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,86.97,393.32,10.91;14,112.66,100.52,393.33,10.91;14,112.66,114.06,152.38,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,359.92,86.97,146.07,10.91;14,112.66,100.52,362.92,10.91">Relationship between household income and mental disorders: findings from a population-based longitudinal study</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sareen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">O</forename><surname>Afifi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">A</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J</forename><surname>Asmundson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,484.00,100.52,21.98,10.91;14,112.66,114.06,68.44,10.91">Arch Gen Psychiatry</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="419" to="427" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,127.61,393.33,10.91;14,112.66,141.16,82.88,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">De</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Counts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
		<title level="m" coord="14,361.08,127.61,144.91,10.91;14,112.66,141.16,27.03,10.91">Predicting depression via social media</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,154.71,393.33,10.91;14,112.66,168.26,125.14,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="14,291.36,154.71,214.63,10.91;14,112.66,168.26,64.22,10.91">Social media as a measurement tool of depression in populations</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">De</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Counts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,181.81,379.45,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schutze</surname></persName>
		</author>
		<title level="m" coord="14,296.50,181.81,163.69,10.91">Introduction to information retrieval</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,195.36,394.53,10.91;14,112.41,208.91,22.69,10.91" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="14,195.19,195.36,252.84,10.91">Distributed representations of sentences and documents</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,222.46,394.53,10.91;14,112.66,236.01,393.33,10.91;14,112.66,249.56,134.32,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,112.66,236.01,328.30,10.91">Modified frequency-based term weighting schemes for text classification</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sabbah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Selamat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">H</forename><surname>Selamat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">S</forename><surname>Al-Anzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H</forename><surname>Viedma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Krejcar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fujita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,449.29,236.01,56.69,10.91;14,112.66,249.56,50.39,10.91">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="193" to="206" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,263.11,393.33,10.91;14,112.66,276.66,363.59,10.91" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="14,353.43,263.11,152.55,10.91;14,112.66,276.66,181.08,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,290.20,393.33,10.91;14,112.66,303.75,393.98,10.91;14,112.41,317.30,48.96,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="14,361.64,290.20,144.35,10.91;14,112.66,303.75,268.25,10.91">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,394.29,303.75,66.92,10.91">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,330.85,394.53,10.91;14,112.30,344.40,393.68,10.91;14,112.66,357.95,107.17,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="14,173.53,344.40,256.77,10.91">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,371.50,393.59,10.91;14,112.66,385.05,146.44,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m" coord="14,268.64,371.50,203.75,10.91">Longformer: The long-document transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,112.66,398.60,393.61,10.91;14,112.66,412.15,179.74,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,249.42,398.60,140.03,10.91">A short introduction to boosting</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Abe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,397.86,398.60,108.41,10.91;14,112.66,412.15,111.93,10.91">Journal-Japanese Society For Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">1612</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,425.70,393.33,10.91;14,112.66,439.25,219.24,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="14,287.30,425.70,218.68,10.91;14,112.66,439.25,61.39,10.91">Large-scale bayesian logistic regression for text categorization</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Genkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Madigan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,182.36,439.25,65.61,10.91">Technometrics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="291" to="304" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,452.79,394.53,10.91;14,112.48,466.34,105.47,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="14,243.07,452.79,255.77,10.91">An improved random forest classifier for text categorization</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,112.48,466.34,16.46,10.91">JCP</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2913" to="2920" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,479.89,393.33,10.91;14,112.66,493.44,297.45,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,205.22,479.89,300.77,10.91;14,112.66,493.44,55.98,10.91">Support vector machine active learning with applications to text classification</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,176.56,493.44,164.83,10.91">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,506.99,393.61,10.91;14,112.66,520.54,146.74,10.91" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Martin-Rodilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<title level="m" coord="14,354.99,506.99,151.28,10.91;14,112.66,520.54,114.82,10.91">Overview of erisk 2022: Early risk prediction on the internet</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,534.09,393.33,10.91;14,112.66,547.64,394.53,10.91;14,112.39,561.19,116.36,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="14,279.00,534.09,226.99,10.91;14,112.66,547.64,49.73,10.91">A comparison of string metrics for matching names and records</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fienberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,185.24,547.64,317.51,10.91">Proceedings of Kdd workshop on data cleaning and object consolidation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="73" to="78" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,574.74,356.96,10.91" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="14,188.25,574.74,217.72,10.91">Similarity measures for text document clustering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="9" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,588.29,394.53,10.91;14,112.66,601.84,342.82,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="14,199.57,588.29,303.32,10.91">A supervised term selection technique for effective text categorization</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,112.66,601.84,263.96,10.91">International Journal of Machine Learning and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="877" to="892" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,615.39,393.33,10.91;14,112.26,628.93,285.09,10.91" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<title level="m" coord="14,369.78,615.39,136.21,10.91;14,112.26,628.93,203.88,10.91">Distributed representations of words and phrases and their compositionality</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,642.48,393.33,10.91;14,112.66,656.03,258.51,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="14,190.92,642.48,315.07,10.91;14,112.66,656.03,51.86,10.91">The unified medical language system (umls): integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,173.16,656.03,98.78,10.91">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="267" to="D270" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,669.58,393.33,10.91;15,112.66,86.97,384.72,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="14,236.62,669.58,269.37,10.91;15,112.66,86.97,38.24,10.91">An overview of metamap: historical perspective and recent advances</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Aronson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,159.61,86.97,253.83,10.91">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="229" to="236" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,100.52,393.33,10.91;15,112.66,114.06,393.61,10.91;15,112.66,127.61,147.26,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="15,366.61,100.52,139.38,10.91;15,112.66,114.06,339.47,10.91">Early risk detection of self-harm and depression severity using bert-based transformers: ilab at clef erisk 2020</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>MartÃ­nez-CastaÃ±o</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Htait</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Moshfeghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,460.60,114.06,45.67,10.91;15,112.66,127.61,115.34,10.91">Early Risk Prediction on the Internet</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,141.16,393.71,10.91;15,112.66,154.71,393.32,10.91;15,112.66,168.26,300.88,10.91" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kalyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jayaswal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pettifer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Jonnalagadda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06424</idno>
		<title level="m" coord="15,498.63,141.16,7.73,10.91;15,112.66,154.71,393.32,10.91;15,112.66,168.26,118.55,10.91">A novel framework to expedite systematic reviews by automatically building information extraction training corpora</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="15,112.66,181.81,393.61,10.91;15,112.66,195.36,393.32,10.91;15,112.66,208.91,174.74,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="15,384.62,181.81,121.65,10.91;15,112.66,195.36,339.69,10.91">Towards effective discovery of natural communities in complex networks and implications in e-commerce</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">C</forename><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,461.35,195.36,44.63,10.91;15,112.66,208.91,90.81,10.91">Electronic Commerce Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="917" to="954" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,222.46,393.32,10.91;15,112.66,236.01,316.29,10.91" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="15,168.73,222.46,337.25,10.91;15,112.66,236.01,266.42,10.91">Comparison of document similarity measurements in scientific writing using jaro-winkler distance method and paragraph vector method</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cahyono</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">662</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,249.56,393.53,10.91;15,112.66,263.11,293.09,10.91" xml:id="b26">
	<monogr>
		<title level="m" type="main" coord="15,257.72,249.56,248.47,10.91;15,112.66,263.11,256.11,10.91">Early detection of signs of anorexia and depression over social media using effective machine learning frameworks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Jandhyala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Basu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,276.66,393.61,10.91;15,112.66,290.20,395.00,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="15,293.24,276.66,213.03,10.91;15,112.66,290.20,271.02,10.91">A sentence classification framework to identify geometric errors in radiation therapy from relevant literature</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Goldsworthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Gkoutos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,391.55,290.20,53.46,10.91">Information</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">139</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,303.75,393.33,10.91;15,112.33,317.30,58.19,10.91" xml:id="b28">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<title level="m" coord="15,228.32,303.75,277.67,10.91">A test collection for research on depression and language use</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,330.85,393.61,10.91;15,112.66,344.40,146.74,10.91" xml:id="b29">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Martin-Rodilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
		<title level="m" coord="15,354.99,330.85,151.28,10.91;15,112.66,344.40,114.82,10.91">Overview of erisk 2021: Early risk prediction on the internet</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
