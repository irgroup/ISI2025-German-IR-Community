<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,416.57,15.42;1,89.29,106.66,265.82,15.42">ZHAW at eRisk 2022: Predicting Signs of Pathological Gambling -GloVe for Snowy Days</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,72.58,11.96"><forename type="first">Samuel</forename><surname>Stalder</surname></persName>
							<email>samuel.stalder1@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">ZHAW Zurich University of Applied Sciences</orgName>
								<address>
									<addrLine>Technikumstrasse 9</addrLine>
									<postCode>8401</postCode>
									<settlement>Winterthur</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,174.52,134.97,72.65,11.96"><forename type="first">Erman</forename><surname>Zankov</surname></persName>
							<email>zankoverman@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">ZHAW Zurich University of Applied Sciences</orgName>
								<address>
									<addrLine>Technikumstrasse 9</addrLine>
									<postCode>8401</postCode>
									<settlement>Winterthur</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,416.57,15.42;1,89.29,106.66,265.82,15.42">ZHAW at eRisk 2022: Predicting Signs of Pathological Gambling -GloVe for Snowy Days</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">BD561B44E4F2E82C9E418741564A40B8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Early Detection System</term>
					<term>Natural Language Processing</term>
					<term>GloVe</term>
					<term>SVM</term>
					<term>XGBoost</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of our team from ZHAW in the eRisk 2022 shared task. eRisk provides three challenges in the field of "Early Risk Prediction on the Internet". These are text classification problems to detect mental disorders. This paper focuses on addressing the first task whose main objective is the early detection of users at risk of pathological gambling, based on their Reddit history. We addressed this issue by developing models with GloVe as feature extraction and SVM and XGBoost as Classification. It has been found that XGBoost without dart boost has slightly better decision-based evaluation results than SVM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The eRisk lab is the "Early Risk Prediction on the Internet" challenge in CLEF. The various tasks relate to textual analysis of social media data. The main goal of eRisk is to study topics such as evaluation methods, metrics, and other factors necessary for the development of research collections and the identification of problems for early risk detection. In turn, the participants, research teams from around the world analyze user data through appropriate methods to detect mental illnesses of any kind early and precisely. With CLEF 2022, eRisk is already entering its sixth round. Early Detection of Signs of Pathological Gambling was first introduced as a challenge in 2021 <ref type="bibr" coords="1,163.57,463.47,13.96,10.91" target="#b0">[1]</ref> and continues this year. The only difference is that this year they provided labeled training data <ref type="bibr" coords="1,177.61,477.02,12.62,10.91" target="#b1">[2]</ref>. The work presented in this paper has been carried out in the course of a bachelor's thesis at the ZHAW Zurich University of Applied Sciences. Our work is based on the solution of team UNSL from 2021 <ref type="bibr" coords="1,254.81,504.11,13.54,10.91" target="#b2">[3]</ref>. We looked for possible improvements by replacing single subsystems. This paper describes our team's participation in CLEF eRisk 2022 Task 1. The methodology and associated results are presented in this paper, as well as a discussion of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Related Work</head><p>Our experiments are based on the results submitted by team UNSL in 2021. They achieved good results last year using three different approaches. In their first approach, they used standard classification models to identify high-risk users using a simple rule-based model. In the second approach, they used a deep learning model for classification and reinforcement learning for early detection. In the last approach, they used a simple and interpretable model that identifies high-risk users and alerts them with a global risk level. Since they did not receive any labeled training data last year, they had to obtain it themselves by using a web crawler that collects gambling and non-gambling users from Reddit.</p><p>For our setup, we used the same web crawler, preprocessing, and rule-based early detection system as in the first approach. Our customizations are mainly in the area of feature extraction and classification. For a good contextual extraction of the features, GloVe <ref type="bibr" coords="2,406.11,267.74,14.44,10.91" target="#b3">[4]</ref> is used in combination with SVM <ref type="bibr" coords="2,149.01,281.29,31.64,10.91">[5][6]</ref> and XGBoost <ref type="bibr" coords="2,237.69,281.29,15.04,10.91" target="#b6">[7]</ref>, whereas in the original they chose BoW with SVM and doc2vec with logistic regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Preprocessing</head><p>In order to test our models, we were given training data by the organizers of this year's challenge. Each file is a XML file that contains all of a user's posts: The user ID for identification and a list of the user's postings. Each writing has a title, date, text, and info field. The title is optional, so many posts have no title. The date contains both the date and time when the post was written. The text represents the post itself, and the info field is just information about the type of postin this case, all posts are Reddit posts. To compare our results with the results of team UNSL, we used the data received from eRisk only for testing. The training data was collected with the help of the UNSL web crawler, just like in last year's challenge. It is important to note that our participation in this year's challenge is based on our bachelor's thesis, which compares UNSL's feature extraction with our implementation of GloVe. Hence, the training of our models was done with the collected data from the web crawler.</p><p>The web crawler uses Reddit's API to download the newest posts from several subreddits. For the acquisition of the positive cases, the subreddit "r/problemgambling" was used. For the negative cases, a mixture of different subreddits, not related to gambling, was used, such as "r/sports", "r/jokes", "r/gaming", "r/politics", "r/news" and "r/LifeProTips". After downloading a user's post, comments written by the same user on the same subreddit were downloaded in order to acquire a sufficient number of posts per user. The limit for the maximum number of posts is 100. The resulting training set contains 411 positive cases and 999 negative cases. We aimed at having a balanced positive to negative ratio.</p><p>For the preprocessing of our training and testing data, we used the preprocessing script written by team UNSL. Given that their team had the best scores in several performance metrics in last year's challenge, we decided not to change it in the interest of having the best possible comparison later down the line with our models. The preprocessing steps include setting all characters in the acquired posts to lowercase, replacing Unicode values with their corresponding symbols, HTML codes with their symbols, web links with the token "weblink", numbers with the token "number" and removing repeated white spaces, tabs and new lines. In case the post ends up empty after the above-mentioned preprocessing steps, the token "empty" is used instead. After each post by a user, the token "$END_OF_POST$" is appended to signal its end. We conducted experiments to ensure that there is no overlap between the training and testing datasets. The importance of our system is not the classification of online users with gambling addiction, but the early detection of such. In the real world, it is more important to detect emerging signs of addiction rather than simply detecting addiction after it is too late, or significantly more difficult to treat. To do that, we trained our model with a system designed to detect gambling addiction as early as possible based on the user's posts. Hence, the testing of the system differs from the standard testing using a test set. Rather than giving all of a user's posts at once, we are simulating a real-life situation, where each post is given to our system one by one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Early Detection System</head><p>Figure <ref type="figure" coords="4,131.22,548.21,5.06,10.91" target="#fig_1">2</ref> visualizes the Early Detection System conceptually. The first step consists of determining if a post is classified as positive and its corresponding probability. Then, we keep track of the number of posts per user our model evaluates. All evaluations made from less than 𝑛 posts are ignored. This ensures the reduction of false positive cases, i.e., having our model make a prediction based on too little input. If the threshold of 𝑛 posts is reached and the probability of the post being positive is higher than threshold 𝛿, then this user is classified as positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Models</head><p>This section describes the details of the models used by our team for this task. The following five models differ in hyperparameter and classification method. They all used GloVe for feature extraction. GloVe contains 685k keys, 343k unique vectors with 300 dimensions. Model #0 and #1 use SVM with the parameter C is "1.0", the type of kernel the algorithm used is "rbf", the kernel coefficient is "scale" and the tolerance of stopping criterion is "0.001". Model #2 and #3 use XGBoost with the loss function "log_loss", learning rate of 0.1, number of boosting stages is 100 and a "friedman_mse" criterion. The last model uses an additional dart boost. We decided to take over the thresholds from UNSL's paper, considering that we wanted to make a direct comparison between our models and theirs, by only changing the feature extraction and classification methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>With our 5 models we have processed the first 30 posts, due to time constraints for all 2079 people. For an optimal outlook, all posts of the 2079 people would have had to be evaluated. Unfortunately, the prediction with our models took too long to fully process all posts, and additionally there were some server problems. Table <ref type="table" coords="5,127.75,506.99,5.17,10.91">2</ref> shows the performance and the number of processed posts of each team. In total, only three of the nine participating groups were able to process all posts. We processed 30 posts with a total time of 12 hours and 30 minutes. With 5 min, the prediction time is one of the slowest. Only team SINAI and team RELAI reported slower execution times. The fastest was team UNED-NLP with 6s.</p><p>Early classification decision-based performance: Table <ref type="table" coords="5,375.85,574.74,5.17,10.91" target="#tab_2">3</ref> shows the results obtained for the decision-based performance metrics. As can be observed, our team was able to achieve solid average performance for the metrics P, R, F1, ERDE5 and ERDE50. Our model #2 was able to achieve slightly better results compared to the rest of our models. Normally, precision and recall have an inversely proportional behavior. So, if one of them is high, the other one is low. This is also reflected in our results. The recall measure is high, but the precision is low. This means that our models could find almost all users addicted to gambling, but many of the users classified as addicted to gambling do not belong to this class. A good mix between P and R is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Details of the participating teams. For each team, the number of submitted models, number of processed posts and their total time is shown. In addition, the processing time for each post was calculated.  Ranking-based performance: Table <ref type="table" coords="6,276.84,571.30,5.17,10.91">4</ref> shows the results obtained for the ranking-based performance metrics. Team UNED-NLP, BLUE and UNSL achieved the best results here. The only differences here are seen in NDCG100. For one writing processed, model BLUE#0 performs best, for 100 writings processed, model UNSL#1 performs best, and for 500 and 1000 writings processed, model UNED-NLP performs best. Our models #2, #3 and #4 are better than the average after processing the first writing. For the metrics P10, NDCG10 and NDCG100 after 100, 500 and 1000 writings processed, we have a score of 0. The reason for this is that we processed a total of only 30 out of 2001 posts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,572.44,338.93,8.93;2,198.01,434.66,199.26,125.22"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Distribution of positive and negative cases of training data and test data</figDesc><graphic coords="2,198.01,434.66,199.26,125.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,400.03,416.70,8.93;4,89.29,411.77,416.69,9.14;4,89.29,423.72,199.98,9.14;4,158.16,116.93,278.95,275.68"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Policy to determine when to raise an alarm. It issues an alarm if the current document is predicted as positive, its probability of belonging to the positive class is greater than threshold 𝛿 and the number of posts read is over the threshold 𝑛.</figDesc><graphic coords="4,158.16,116.93,278.95,275.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,258.60,416.99,129.65"><head>Table 1</head><label>1</label><figDesc>Details of our models. For each run, the type of feature extraction, the type of Classification method and the thresholds 𝛿 and 𝑛 for early detection policy. Our models are designated "stezmo3#[run] (ZHAW)" in this and in the following tables.</figDesc><table coords="5,151.73,313.86,291.81,74.39"><row><cell>Model</cell><cell>Feature Extraction</cell><cell>Classfication</cell><cell>𝛿</cell><cell>𝑛</cell></row><row><cell>stezmo3#0 (ZHAW)</cell><cell>GloVe</cell><cell>SVM</cell><cell cols="2">0.85 3</cell></row><row><cell>stezmo3#1 (ZHAW)</cell><cell>GloVe</cell><cell>SVM</cell><cell cols="2">0.75 10</cell></row><row><cell>stezmo3#2 (ZHAW)</cell><cell>GloVe</cell><cell>XGBoost</cell><cell cols="2">0.85 3</cell></row><row><cell>stezmo3#3 (ZHAW)</cell><cell>GloVe</cell><cell>XGBoost</cell><cell cols="2">0.75 10</cell></row><row><cell>stezmo3#4 (ZHAW)</cell><cell>GloVe</cell><cell cols="3">XGBoost + dart 0.85 5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,88.99,336.36,460.11,213.73"><head>Table 3</head><label>3</label><figDesc>Decision-based evaluation results. For comparison, the best models in the range P, R, F1, ERDE5, ERDE50 and latency-weighted F1 are listed together with the median and mean values of all models. The best values among all participating models are shown in bold.</figDesc><table coords="6,95.27,391.89,453.84,158.20"><row><cell>Team</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell cols="5">ERDE5 ERDE50 latencyTP speed latency-weighted F1</cell></row><row><cell>UNED-NLP#4</cell><cell cols="3">0.809 0.938 0.869</cell><cell>0.02</cell><cell>0.008</cell><cell>3</cell><cell>0.992</cell><cell>0.862</cell></row><row><cell>SINAI#0</cell><cell cols="3">0.425 0.765 0.546</cell><cell>0.015</cell><cell>0.011</cell><cell>1</cell><cell>1</cell><cell>0.546</cell></row><row><cell>BioNLP-UniBuc#0</cell><cell>0.039</cell><cell>1</cell><cell>0.075</cell><cell>0.038</cell><cell>0.037</cell><cell>1</cell><cell>1</cell><cell>0.075</cell></row><row><cell>UNSL#1</cell><cell cols="3">0.461 0.938 0.618</cell><cell>0.041</cell><cell>0.008</cell><cell>11</cell><cell>0.961</cell><cell>0.594</cell></row><row><cell>NLPGroup-IISERB#1</cell><cell>1</cell><cell cols="2">0.074 0.138</cell><cell>0.038</cell><cell>0.037</cell><cell>41.5</cell><cell>0.843</cell><cell>0.116</cell></row><row><cell>stezmo3#0 (ZHAW)</cell><cell cols="3">0.116 0.864 0.205</cell><cell>0.034</cell><cell>0.015</cell><cell>5</cell><cell>0.984</cell><cell>0.202</cell></row><row><cell>stezmo3#1 (ZHAW)</cell><cell cols="3">0.116 0.864 0.205</cell><cell>0.049</cell><cell>0.015</cell><cell>12</cell><cell>0.957</cell><cell>0.196</cell></row><row><cell>stezmo3#2 (ZHAW)</cell><cell cols="3">0.152 0.914 0.261</cell><cell>0.033</cell><cell>0.011</cell><cell>5</cell><cell>0.984</cell><cell>0.257</cell></row><row><cell>stezmo3#3 (ZHAW)</cell><cell cols="2">0.139 0.864</cell><cell>0.24</cell><cell>0.047</cell><cell>0.013</cell><cell>12</cell><cell>0.957</cell><cell>0.229</cell></row><row><cell>stezmo3#4 (ZHAW)</cell><cell>0.16</cell><cell cols="2">0.901 0.271</cell><cell>0.043</cell><cell>0.011</cell><cell>7</cell><cell>0.977</cell><cell>0.265</cell></row><row><cell>Median</cell><cell cols="3">0.116 0.963 0.205</cell><cell>0.037</cell><cell>0.015</cell><cell>2.75</cell><cell>0.993</cell><cell>0.211</cell></row><row><cell>Mean</cell><cell cols="3">0.226 0.846 0.282</cell><cell>0.03</cell><cell>0.0209</cell><cell>4.82</cell><cell>0.985</cell><cell>0.300</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6.">Acknowledgments</head><p>This research was enabled by the support of <rs type="person">Prof. Dr. Martin Braschler</rs> at <rs type="affiliation">ZHAW Zurich University of Applied Sciences</rs>.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and future work</head><p>In this paper we presented the results of our team's participation in the eRisk2022 Task 1. Due to complications, not all posts could be evaluated. Server problems and the long time needed for the prediction, due to the low processing power of our hardware, made the whole evaluation difficult. A complete evaluation might have yielded better results. In addition, a complete ranking-based evaluation would have been possible. Despite these obstacles, we were able to achieve competitive results. With stronger hardware and more time, a complete prediction could have been done. This year, there was a training set for task 1 from the organizer. However, we did not use it for training but only for testing. Possibly better results could have been achieved by mixing the data from the web crawler with the training data from eRisk and using an 80-20 split.</p><p>The potential of our approach has not been exhausted and future work could be about improving the decision-based performance with hyperparameter tuning of the thresholds 𝛿 and 𝑛. In addition, the preprocessing could be improved by filtering out the non-English sentences. Another approach would be to divide the prediction into two subpredictions. In the first step, it is determined whether the person is at risk of addiction and in the second step whether it is pathological gambling. This would be especially beneficial in scenarios where all different types of addictive behavior should be detected.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,107.59,111.28,399.68,10.91;8,107.59,124.83,399.68,10.91;8,107.59,138.38,152.66,10.91" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Martín-Rodilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<ptr target="https://www.dc.fi.udc.es/~parapar" />
		<title level="m" coord="8,357.20,111.28,150.06,10.91;8,107.59,124.83,262.02,10.91">Overview of eRisk at CLEF 2021: Early Risk Prediction on the Internet (Extended Overview)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="8,107.59,151.93,399.68,10.91;8,107.59,165.48,400.08,10.91;8,107.59,179.03,54.35,10.91" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Parapar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Martín-Rodilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<ptr target="https://www.dc.fi.udc.es/~parapar" />
		<title level="m" coord="8,358.07,151.93,149.19,10.91;8,107.59,165.48,162.71,10.91">Evaluation Report of eRisk 2022: Early Risk Prediction on the Internet</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="8,107.59,192.57,398.78,10.91;8,107.59,206.12,398.40,10.91;8,107.59,219.67,399.10,10.91;8,107.59,233.22,155.31,10.91" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,405.01,192.57,64.60,10.91;8,498.69,192.57,7.68,10.91;8,107.59,206.12,398.40,10.91;8,107.59,219.67,160.71,10.91">A Comparison of Three Early Alert Policies for Early Risk Detection under Creative Commons License Attribution 4.0 International</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Loyola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Burdisso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cagnina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Errecalde</surname></persName>
		</author>
		<ptr target="https://github.com/jmloyola/unsl_erisk_2021http://ceur-ws.org" />
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note>UNSL at eRisk. CC BY 4.0</note>
</biblStruct>

<biblStruct coords="8,107.59,246.77,399.60,10.91;8,107.26,260.32,175.48,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,291.99,246.77,210.50,10.91">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://nlp" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="8,107.59,273.87,399.59,10.91;8,107.26,287.42,103.29,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="8,274.13,273.87,229.12,10.91">A Training Algorithm for Optimal Margin Classifiers</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">M</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="8,107.59,300.97,398.40,10.91;8,107.59,314.52,400.08,10.91;8,107.59,328.07,164.72,10.91" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="8,269.56,300.97,236.43,10.91;8,107.59,314.52,209.46,10.91">New Benchmark Corpus and Models for Fine-grained Event Classification: To BERT or not to BERT?</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Piskorski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Haneczok</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jacquet</surname></persName>
		</author>
		<ptr target="https://tac.nist.gov/2017/KBP/Event/index.html" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="8,107.59,341.62,398.40,10.91;8,107.59,355.17,400.24,10.91;8,107.34,368.71,399.84,10.91;8,107.59,384.71,91.42,7.90" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,200.84,341.62,177.78,10.91">XGBoost: A scalable tree boosting system</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939785</idno>
	</analytic>
	<monogr>
		<title level="m" coord="8,400.62,341.62,105.36,10.91;8,107.59,355.17,342.99,10.91">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016-08">August-2016. 2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
