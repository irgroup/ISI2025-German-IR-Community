<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.02,75.44,451.23,17.04;1,72.02,96.20,450.78,17.04;1,72.02,116.96,112.67,17.04">CYUT at eRisk 2022: Early Detection of Depression Based-on Concatenating Representation of Multiple Hidden Layers of RoBERTa Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,72.02,149.86,72.96,10.80"><forename type="first">Shih-Hung</forename><surname>Wu</surname></persName>
							<email>shwu@cyut.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution">Chaoyang University of Technology</orgName>
								<address>
									<settlement>Taichung</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,172.58,149.86,66.39,10.80"><forename type="first">Zhao-Jun</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Chaoyang University of Technology</orgName>
								<address>
									<settlement>Taichung</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">CLEF</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.02,75.44,451.23,17.04;1,72.02,96.20,450.78,17.04;1,72.02,116.96,112.67,17.04">CYUT at eRisk 2022: Early Detection of Depression Based-on Concatenating Representation of Multiple Hidden Layers of RoBERTa Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C8889CCE1E9B0DFB311ACB09EB0D45F6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>RoBERTa</term>
					<term>Depression Detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Depression has been seen as a global crisis, with hundreds of millions of people around the world suffering from it. By analyzing people's writings on social media, a system has the opportunity to detect depression and can alert the person to seek medical help. Our team participated the CELF 2022 eRisk Task 2: Early Detection of Depression, a mission designed to detect people early for depression tendencies. Our research methodology focuses on improving the pre-training model RoBERTa. We ran a total of five experiments this year. The first one is regarded as a baseline using the pre-trained language model. Experiment two is to extract the output of hidden layers as a new representation. Experiment three is to obtain keyword features by extracting two categories of single word features. Experiment four is to train two models for the title and text separately, and integrate the results to make predictions. Experiment five is to integrate the methods of experiment two and experiment four. According to the results of the task evaluation, the method of experiment two is indeed better than using the pre-trained model. Experiments 4 and 5 performed well on the Task's Ranking-based evaluation after testing 1000 writings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="594.96" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Depression is a popular disease of civilization in the 21st century, and according to data published on the website of the World Health Organization (WHO), 264 million people worldwide is suffered from depression in 2020. Nowadays, people are accustomed to sharing their living through social media, and analyzing these posts can observe the depression tendencies of the authors. In 2018, Eichstaedt, J.C., Smith, R.J., Merchant, R.M et al. used messages in Facebook posts to predict depression in their medical records <ref type="bibr" coords="1,144.86,540.09,11.77,9.94" target="#b0">[1]</ref>. In 2017, Reece, A.G., Reagan, A.J., Lix, K.L.M et al. used Twitter data to predict the onset and course of mental illness <ref type="bibr" coords="1,247.07,552.81,11.77,9.94" target="#b1">[2]</ref>. The eRisk organizers have organized related tasks in the CLEF lab. Last year's CLEF eRisk Task 3: Measuring the severity of the sign of depression <ref type="bibr" coords="1,489.55,565.41,12.78,9.94" target="#b2">[3]</ref> was also used posted writings on social media to predict the user's severity of depression. We have also participated in this lab in 2021 <ref type="bibr" coords="1,205.83,590.76,16.89,9.94" target="#b9">[10]</ref>, using the social media corpus to train BERT <ref type="bibr" coords="1,422.80,590.76,17.39,9.94" target="#b10">[11,</ref><ref type="bibr" coords="1,442.47,590.76,14.59,9.94" target="#b11">12]</ref> and RoBERTa <ref type="bibr" coords="1,72.02,603.36,11.77,9.94" target="#b6">[7]</ref>, and weighted the predictions of each post to calculate the degree of depression of the authors. From the overview of eRisk at CLEF 2021 results report <ref type="bibr" coords="1,298.09,616.08,16.79,9.94" target="#b12">[13]</ref>, we learned that our approach performed well, and we found that each team has its own methodology. Alhuzali et al. team ran three different pretrained language models to observe the practicality and strengths of each model, and it was learned from their experiments that the pre-trained model was trained for sentiment analysis, which helped to strengthen the model's judgment of the degree of depression <ref type="bibr" coords="1,335.91,666.60,16.88,9.94" target="#b13">[14]</ref>. <ref type="bibr" coords="1,359.59,666.60,77.50,9.94">Inkpen et al. team</ref> proposed two main approaches, the first approach is using of pre-trained models, and by analyzing the relevance of all posts and BDI answers, they believed it should be noted that not all categories were discussed in posts <ref type="bibr" coords="2,502.18,74.66,16.79,9.94" target="#b14">[15]</ref>. The second approach is to classify posts in different topics, and find the most relevant topics through the word vectors with the corpus. Bucur et al. team and Spartalis et al. team also used the pre-trained model approach <ref type="bibr" coords="2,146.15,112.60,17.45,9.94" target="#b15">[16,</ref><ref type="bibr" coords="2,163.59,112.60,13.09,9.94" target="#b16">17]</ref>, the difference being that one was trained to analyze post similarities and the other was to analyze feature-based transfer learning.</p><p>To analyze people's psychological conditions through a wide range of information in social media is widely appreciated. CLEF eRisk also gave three different tasks this year <ref type="bibr" coords="2,408.40,150.52,11.68,9.94" target="#b3">[4]</ref>, namely Task 1: Early Detection of Signs of Pathological Gambling, Task 2: Early Detection of Depression, Task 3: Measuring the severity of the signs of Eating Disorders. Our team is involved in Task 2, a task designed to detect people for depression tendencies. The eRisk server iteratively provides user writing to the participating teams by releasing data step by step. How to diagnose the tendency to depression early through some data is part of the evaluation indicator, that is, the evaluation not only considers the correctness of the system output, but also considers the time point at which its decision is published.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Data and Pre-processing</head><p>The data used in this paper is the dataset provided at eRisk 2022 Task 2: Early Detection of Depression <ref type="bibr" coords="2,123.96,296.83,29.74,9.94">[5][18]</ref>. The data contains text from multiple users, each of whom typically provides a large amount of written text in the XML format as in Figure <ref type="figure" coords="2,317.31,309.55,4.14,9.94">1</ref>. ID: Contains the anonymous ID of the user, title: the title of the post (keep blank for comments), INFO: the source of the post, TEXT: the content of the post or comment. Since the dataset was collected from the forum and has not been processed, it contains paths, URLs, some special characters, and so on. Therefore, we use regular expression do the preprocessing on the title and text of each document as shown in Figure <ref type="figure" coords="3,294.88,393.31,4.16,9.94" target="#fig_1">3</ref>. Special characters, paths, URLs, parentheses, and punctuation are removed. The number of training and verification after preprocessing is as shown in Table <ref type="table" coords="3,99.86,418.53,4.14,9.94" target="#tab_0">1</ref>.  The training materials came from a total of 820 people, of which the majority (741 people) were non-depression, which shows that the data is extremely unbalanced. The situation is often encountered in real world problems, how to effectively filter the post is an important issue, and it is also the main consideration of our research. According to the previous observation <ref type="bibr" coords="3,377.83,680.64,11.77,9.94" target="#b5">[6]</ref>, there is a difference between the length and amount of words used. We show them in Figure <ref type="figure" coords="3,361.36,693.24,5.52,9.94" target="#fig_3">4</ref> and Figure <ref type="figure" coords="3,422.03,693.24,4.08,9.94" target="#fig_4">5</ref>, respectively, for the length of the text and the number of words. Blue represents the post of non-depression ones, red represents post written by depression users, and the X axis is the total number of posts 538,389. The Yaxis indicates the length of the post and the number of words, respectively, and statistically there are indeed some posts that show that the posts by non-depression users have a longer length and more words than the posts by depression users. Therefore, according to this data, we removed the posts with   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>We describe our system settings in sub-section 3.1 and how we evaluate our system in sub-section 3.2. The experiment settings of our 5 runs is shown in the following 5 sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Operating environment and model parameter settings</head><p>Model is trained on Google Colab Pro, the training data is listed in Table <ref type="table" coords="4,411.20,727.92,4.14,9.94" target="#tab_0">1</ref>, the data is divided into 80% for training, 20% verification, tokenizer and model are roberta-base. The hyper parameters settings are: max length is set to 128, batch size is set to 100, hidden size is set to 768, learning rate is set to 1e-5, weight decay is set to 1e-2, and epoch of fine-tuning is set to 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation model method</head><p>Evaluation processes is shown in Figure <ref type="figure" coords="5,267.98,145.12,4.15,9.94" target="#fig_5">6</ref>, there are two evaluation modules. One is to predict the outcome of each data's depression tendency, and the other is to predict whether the statistical prediction results are for the corresponding person to determine whether there is a depression tendency. The process is to give test set data to the experimental model to predict whether it is a tendency to be depressed, and calculate the model Precision, Recall, and F1-score scores. And the data results are statistically judged by the corresponding person, adjusted from 1% to 99% of the symptomatic data, and calculate the Precision, Recall and F1-score scores under different proportions to find the best F1score score. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Experiment 1: RoBERTa</head><p>The pre-trained RoBERTa model <ref type="bibr" coords="5,242.33,571.41,12.91,9.94" target="#b6">[7]</ref> was used as a baseline model for evaluating model score changes against subsequent comparisons. The flowchart of experiment one is shown in Figure <ref type="figure" coords="5,498.08,584.16,4.14,9.94" target="#fig_7">7</ref>, the only preprocessing is focus on the data imbalance issue  The main idea of experiment 2 is to change the embedding representation of an input sentence in the RoBERTa model. The first tokens of each of the last four hidden layers are extracted from the model for improvement <ref type="bibr" coords="6,148.42,100.00,11.69,9.94" target="#b7">[8]</ref>. This token represents the corresponding output vector of each layer, which means that this token is the result of the model's representations in each hidden layer. In this experiment, the results given by the last layer vector will be used for linear classification (see Figure <ref type="figure" coords="6,455.84,125.32,4.01,9.94" target="#fig_8">8</ref>). We want to know if the model prediction can be improved by extracting multiple output vectors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Experiment 3: Building feature dictionary + RoBERTa</head><p>The experiment 3 setting is not to unconditionally discard the non-depression data, but to filter the data by creating a feature dictionary (to retain the information that can be matched by the dictionary). From the progressive reference <ref type="bibr" coords="6,214.10,585.12,11.69,9.94" target="#b8">[9]</ref>, when people's thoughts and emotional reactions are different the usage of words will be different too, that is why the negative emotion dictionary has been used in the past. However, since that it is easy to publish posts using social media, new buzzwords or lists may be generated at any time. Therefore; we try to extract a new dictionary of features by comparing the posts from depression users and non-depression users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1.">Build feature dictionary</head><p>The extraction process is shown in Figure <ref type="figure" coords="6,270.30,693.48,4.14,9.94" target="#fig_9">9</ref>, the frequency of words in training data from depression and non-depression users are counted separately. Some words only appear a few times, such as: personal names, place names, song names, etc., so two threshold values <ref type="bibr" coords="6,362.83,718.68,11.87,9.94" target="#b4">(5,</ref><ref type="bibr" coords="6,378.77,718.68,14.72,9.94" target="#b15">16)</ref> are set for the frequency of occurrence of words with depression and non-depression users. Two feature dictionaries are extracted, and the number of words in the characteristics of the non-depression is 19,214, and the number of words with the depression is 1,106. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2.">Experiment process</head><p>The flow chart of experiment 3 is shown in Figure <ref type="figure" coords="7,312.47,293.47,9.20,9.94" target="#fig_10">10</ref>, training data are screened by matching with a feature dictionary. After screening, a total of 129,544 non-depression data were screened, and the depression data was also screened for the purpose of strengthening the training of these data, a total of 902 cases. The processed training data contained all posts from depression users (40,353 posts) and more characteristic posts (129,544+902 posts) after screening, with a total of 170,799 posts. The training data is used to fine-tune the RoBERTa model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 4: Combining Title and Text Prediction Models</head><p>Experiment 4 is to train two models for the title and for the text separately. According to the observation of the dataset, some of the data is only containing the title and no text, and this experiment design is to deal this situation. Experimental process is shown in Figure <ref type="figure" coords="8,387.90,131.08,9.20,9.94" target="#fig_12">11</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Experiment 5: Combining experiments 2 and 4</head><p>We observed from the validation evaluation results of experiment 2 (Table <ref type="table" coords="8,427.21,336.19,4.60,9.94" target="#tab_1">2</ref>) that the method of extracting information from the hidden layer is effective, so we improved the process of experiment 4 according to the method of experiment 2 for experiment 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Discussion on System Development and eRisk task 2</head><p>The result of our five experimental processes according to the Section 3.2 evaluation methodology. Verify using the 2017 dataset test data. The first assessment is to determine whether there is a depression tendency result is shown in Table <ref type="table" coords="8,223.01,444.57,4.14,9.94" target="#tab_1">2</ref>, which is the result of the 401 users. According to the data results, we find that the decision proportion of depression posts is predicted to be different on whether the user has a tendency to be depressed. Figure <ref type="figure" coords="8,244.13,469.89,9.13,9.94" target="#fig_0">12</ref>, Table <ref type="table" coords="8,288.30,469.89,5.52,9.94" target="#tab_2">3</ref> are the metrics for all experiments at the proportion of the best F1-score score.  Where y-axis is the F1-score, and the x-axis is the proportions of posts that the system predicts as Pos. This figure shows that our model performs well when it finds that about 15% of a user's post is depression. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment 2: Discussion</head><p>The evaluation results show that extracting multiple output vectors can effectively improve the performance, which is more accurate than using only the last layer to predict the results. The result of experiment 2 in Table <ref type="table" coords="9,168.97,516.33,5.52,9.94" target="#tab_1">2</ref> is more outstanding than experiment 1 in most evaluation matrices. From Figure <ref type="figure" coords="9,72.02,528.93,9.20,9.94" target="#fig_0">12</ref>, it can be seen that the best result of F1-Score is 60.19% when the proportion of depression in this experimental model is 13%. In the comparison of Table <ref type="table" coords="9,314.79,541.65,5.52,9.94" target="#tab_2">3</ref> evaluation results, the F1-score of Experiment 2 is 2% better than that of Experiment 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiment 3: Discussion</head><p>The results of the assessment did not succeed in improving the prediction. The sharp increase in Recall was accompanied by a sharp decline in precision, which made it easy to make mistakes in judging the tendency to predict depression. As shown in Table <ref type="table" coords="9,319.54,637.32,4.14,9.94" target="#tab_2">3</ref>, accuracy, recall, and F1-score were among the worst of all experimental evaluations. The main reason for this situation is that the data is overscreened. In the establishment of feature dictionaries, too extreme methods are taken, and only words that appear in one of the categories are retained, which also leads to excessive exclusion of training materials. This in turn leads to insufficient model training. From Figure <ref type="figure" coords="9,390.54,687.96,9.20,9.94" target="#fig_0">12</ref>, it can be observed that the training model effect is very poor, when the proportion of symptoms is greater than 68%, the F1-score is greatly reduced. This is abnormal, it means that the model tends to predict that there is depression tendency result, but in fact, the number of people with non-depression tendency is much greater than the number of people with depression tendencies. This condition, as mentioned earlier, is due to overexcluding the results of the training data. And because the number of depression data is too rare after matching, and all the data on depression tendency are put back to the training data, this also leads to the training of the model to have a bias toward predicting depression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiment 4: Discussion</head><p>The evaluation results have not improved significantly, and it can be observed from Table <ref type="table" coords="10,482.39,145.12,5.52,9.94" target="#tab_1">2</ref> that the evaluation results of experiment 4 and experiment 1 are not much different, and only about 0.5% are improved in judging whether people have a depression tendency. This is slightly helpful, but the effect is not as effective as experiment 2. However, compared with the previous experiments, this model can predict the results of the title without the text, so it has different applications similar to the previous experimental models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experiment 5: Discussion</head><p>The results showed that instead of improving, they deteriorated, such as the results of experiment 4 in Figure <ref type="figure" coords="10,116.76,278.71,9.21,9.94" target="#fig_0">12</ref>, which were better than the evaluation results of experiment 5. And the reason for this might be too much different information, and finally the model is difficult to converge and make wrong judgments. From experiment 5 we found that the effect of this approach is limited, the vector size of RoBERTa hidden layer is 768, so the last four hidden layers a total of 6144 dimension vectors will be used. However, it might cause difficult to converge the results for a linear classification, so the model judgment ability is reduced. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5</head><p>Presents the ranking-based results <ref type="bibr" coords="10,226.82,506.35,17.43,11.04" target="#b17">[18]</ref> 1 writing 100 writing 500 writing 1000 writing Experiment P@10 ùëÅùê∑ùê∂ùê∫@10 ùëÅùê∑ùê∂ùê∫@100 P@10 ùëÅùê∑ùê∂ùê∫@10 ùëÅùê∑ùê∂ùê∫@100 P@10 ùëÅùê∑ùê∂ùê∫@10 ùëÅùê∑ùê∂ùê∫@100 P@10 ùëÅùê∑ùê∂ùê∫@10 ùëÅùê∑ùê∂ùê∫@100 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Formal Results in eRisk 2022 Task 2</head><p>We ran the above five experimental models on this Task 2, processing a total of 2000 iterations of user writing, which took 7 days and 12 hours to complete. The Decision-based evaluation results were not particularly pronounced (Table <ref type="table" coords="10,227.58,757.94,4.00,9.94" target="#tab_3">4</ref>), and the Recall was on the high side of each experimental model. We believe that the reasons for this result are due to the different way of evaluation. During the system development phase, all of the writing are given at once. While the task is to give each user a writing post at a time in an iterative way, and predict the data the user's depression tendencies early. However, we have a good performance in the ranking-based results, and from Table <ref type="table" coords="11,405.89,112.60,4.14,9.94">5</ref>, we can observe that the more information our model gets, the evaluation score continues to rise, and P@10 the best performance out of 1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>During the system developing phase, we use all of the user's writing training to train the model, which is different from task 2, which is to give one post at a time in an iterative manner. Therefore, our model is weaker in early detection of user depression, but has a good performance in the ranking-based results. Compared with the baseline of our experiment one, the results of experiment three are not as expected. We learned from it that the statistical common word count ratio as a classification feature might be overfitting. Extracting the output vector in the hidden layers in Experiment 2 as a new representation has indeed been effectively improved, and it is more accurate to predict the result than experiment 1 directly using a pre-trained model. In the design of experiment four, we combined the model trained on the body text with the model that trained on the titles. Although this method has not significantly improved in the evaluation effect, but compared to only the body text, the combined model can handle special cases that title is missing or body text is missing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,72.02,623.02,414.25,11.04;2,72.02,636.46,340.76,11.04;2,242.88,362.76,113.52,238.80"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: The XML format of each post in the dataset [5], where ID is the anonymous user ID, TITLE is the post title, INFO is the source, and TEXT is the content of the post</figDesc><graphic coords="2,242.88,362.76,113.52,238.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,72.02,511.75,143.23,11.04"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Data set normalization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,111.14,188.30,21.11,11.04;3,198.77,125.78,51.90,11.04;3,194.93,139.22,59.71,11.04;3,198.77,251.33,51.90,11.04;3,203.57,264.77,42.32,11.04;3,315.91,164.54,15.95,11.04;3,316.03,290.95,15.33,9.94;3,315.91,227.92,15.94,9.94;3,315.67,102.52,15.94,9.94;3,390.43,84.23,61.57,9.96;3,390.43,96.47,61.57,9.96;3,419.86,108.92,2.72,9.05;3,389.35,120.11,63.78,9.96;3,390.43,148.67,61.62,9.96;3,390.43,160.79,61.62,9.96;3,419.86,173.36,2.72,9.05;3,389.35,184.55,63.75,9.96;3,394.63,210.59,77.03,9.96;3,394.63,222.83,77.03,9.96;3,431.86,235.40,2.72,9.05;3,391.03,246.62,84.30,9.96;3,393.07,274.05,80.33,8.96;3,390.55,285.57,85.25,8.96;3,431.86,296.99,2.72,9.05;3,390.55,308.61,85.25,8.96;3,111.26,465.04,19.06,9.96;3,196.97,457.36,28.40,9.96;3,192.41,469.60,37.70,9.96;3,194.45,481.84,33.54,9.96;3,284.21,457.36,75.85,9.96;3,283.97,469.60,76.52,9.96;3,277.97,481.84,88.49,9.96;3,413.98,452.80,83.42,9.96;3,433.06,465.04,45.16,9.96;3,427.90,477.28,57.94,9.96;4,72.02,74.66,451.01,9.94;4,72.02,87.28,269.16,9.94"><head></head><label></label><figDesc>char, and white space Convert the file in to TSV format (ID, Title, text)length more than 1000 and the number of words over 500. But this distinction is still limited, and most of the posts are still similar in length to the number of words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,72.02,330.41,430.66,11.04;4,135.75,113.10,321.75,213.80"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Statistics on the length of each post (text_n: number of posts, text_len: length of post).</figDesc><graphic coords="4,135.75,113.10,321.75,213.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="4,72.02,571.63,451.26,11.04;4,72.02,585.10,31.06,11.04;4,131.25,355.79,327.75,212.29"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Statistics of the number of words in each post (text_n: number of posts, text_len: number of words)</figDesc><graphic coords="4,131.25,355.79,327.75,212.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="5,72.02,499.75,125.36,11.04;5,141.96,420.84,143.04,62.40"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Evaluation process</figDesc><graphic coords="5,141.96,420.84,143.04,62.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="5,313.11,596.76,210.05,9.94;5,72.02,609.36,451.11,9.94;5,72.02,622.08,383.50,9.94"><head></head><label></label><figDesc>. The treatment is to reduce the number of posts extracted from the documents of the non-depression people in the training set (up to 500 posts per person). The total training posts are 268,866 and use the TEXT part for model training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="5,72.02,706.06,138.14,11.04"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Experiment 1 process</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="6,72.02,474.67,450.97,11.04;6,72.02,488.11,274.50,11.04"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Extracts the output vector from the last four layers of the model's hidden layer and joins the four output vectors as the input vector of the linear classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="7,72.02,221.78,161.46,11.04"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Building feature dictionary</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="7,72.02,721.54,144.02,11.04"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Experiment 3 process</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="8,397.10,131.08,126.07,9.94;8,72.02,143.68,451.13,9.94;8,72.02,156.40,393.08,9.94"><head></head><label></label><figDesc>, the system extracts the title and body of each post from the training data, and the title and body each haw a separate RoBERTa model for training, and the results are integrated to make judgments by a linear classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="8,72.02,277.13,144.02,11.04;8,135.36,190.80,89.16,70.32"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Experiment 4 process</figDesc><graphic coords="8,135.36,190.80,89.16,70.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="9,72.02,264.65,451.17,11.04;9,72.02,278.09,451.19,11.04;9,72.02,291.53,327.78,11.04"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Changes in F1-score under different proportions of Pos.Where y-axis is the F1-score, and the x-axis is the proportions of posts that the system predicts as Pos. This figure shows that our model performs well when it finds that about 15% of a user's post is depression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,72.02,539.47,429.63,70.31"><head>Table 1 Number of training and validation</head><label>1</label><figDesc></figDesc><table coords="3,89.66,569.13,411.99,40.65"><row><cell></cell><cell>Pos(People)</cell><cell>Neg(People)</cell><cell>Pos(posts)</cell><cell>Neg(posts)</cell></row><row><cell>Training Set</cell><cell>79</cell><cell>741</cell><cell>40,385</cell><cell>498,004</cell></row><row><cell>Test set</cell><cell>52</cell><cell>349</cell><cell>17,431</cell><cell>157,433</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,72.02,188.90,424.12,428.84"><head>Table 2</head><label>2</label><figDesc>The evaluation experiment judges the results of each data</figDesc><table coords="8,98.54,538.03,386.81,79.71"><row><cell>Run</cell><cell>Precision</cell><cell>Recall</cell><cell>F1-score</cell></row><row><cell>Experiment 1</cell><cell>30.94%</cell><cell>12.62%</cell><cell>17.93%</cell></row><row><cell>Experiment 2</cell><cell>32.55%</cell><cell>13.38%</cell><cell>18.97%</cell></row><row><cell>Experiment 3</cell><cell>10.54%</cell><cell>60.92%</cell><cell>17.98%</cell></row><row><cell>Experiment 4</cell><cell>30.21%</cell><cell>12.27%</cell><cell>17.46%</cell></row><row><cell>Experiment 5</cell><cell>26.77%</cell><cell>9.77%</cell><cell>14.32%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,72.02,319.25,424.61,108.86"><head>Table 3</head><label>3</label><figDesc>The best results of an evaluation experiment for predicting people's tendency to depression</figDesc><table coords="9,87.26,348.41,409.37,79.70"><row><cell>Run</cell><cell>Pos Percentage</cell><cell>Precision</cell><cell>Recall</cell><cell>F1-score</cell></row><row><cell>Experiment 1</cell><cell>11%</cell><cell>53.12%</cell><cell>65.38%</cell><cell>58.62%</cell></row><row><cell>Experiment 2</cell><cell>13%</cell><cell>60.78%</cell><cell>59.61%</cell><cell>60.19%</cell></row><row><cell>Experiment 3</cell><cell>68%</cell><cell>28.81%</cell><cell>32.69%</cell><cell>30.63%</cell></row><row><cell>Experiment 4</cell><cell>11%</cell><cell>53.96%</cell><cell>65.38%</cell><cell>59.13%</cell></row><row><cell>Experiment 5</cell><cell>6%</cell><cell>39.60%</cell><cell>76.92%</cell><cell>52.28%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,72.02,366.77,428.28,108.74"><head>Table 4</head><label>4</label><figDesc></figDesc><table coords="10,72.02,380.21,428.28,95.30"><row><cell cols="2">Decision-based evaluation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Run</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>ERDE5</cell><cell>ERDE50</cell></row><row><cell>Experiment 1</cell><cell>0.165</cell><cell>0.918</cell><cell>0.280</cell><cell>0.053</cell><cell>0.032</cell></row><row><cell>Experiment 2</cell><cell>0.162</cell><cell>0.898</cell><cell>0.274</cell><cell>0.053</cell><cell>0.032</cell></row><row><cell>Experiment 3</cell><cell>0.106</cell><cell>0.867</cell><cell>0.189</cell><cell>0.056</cell><cell>0.047</cell></row><row><cell>Experiment 4</cell><cell>0.149</cell><cell>0.878</cell><cell>0.255</cell><cell>0.075</cell><cell>0.040</cell></row><row><cell>Experiment 5</cell><cell>0.142</cell><cell>0.918</cell><cell>0.245</cell><cell>0.082</cell><cell>0.041</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,107.66,393.31,415.27,9.94;11,107.66,406.03,415.52,9.94;11,110.42,418.65,32.14,9.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,313.47,393.31,209.46,9.94;11,107.66,406.03,46.32,9.94">Facebook language predicts depression in medi cal records</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Eichstaedt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Merchant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,161.13,406.03,258.35,9.94">Proceedings of the National Academy of Sciences (PNAS)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">44</biblScope>
			<biblScope unit="page" from="11203" to="11208" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,107.66,431.37,415.49,9.94;11,107.66,443.97,415.52,9.94;11,107.66,456.57,17.40,9.94" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,312.43,431.37,210.72,9.94;11,107.66,443.97,86.60,9.94">Forecasting the onset and course of mental illne ss with Twitter data</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Reece</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Reagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L M</forename><surname>Lix</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-017-12961-9</idno>
		<ptr target="https://doi.org/10.1038/s41598-017-12961-9" />
	</analytic>
	<monogr>
		<title level="j" coord="11,201.58,443.97,34.76,9.94">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13006</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,107.66,469.29,415.52,9.94;11,107.66,481.89,33.42,9.94" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="11,168.70,469.29,157.24,9.94">Early risk prediction on the Internet</title>
		<author>
			<persName coords=""><surname>Clef Erisk</surname></persName>
		</author>
		<ptr target="https://erisk.irlab.org/2021/index.html" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,107.66,494.61,415.47,9.94;11,107.66,507.21,224.05,9.94" xml:id="b3">
	<monogr>
		<ptr target="https://clef2022.clef-initiative.eu/index.php?page=Pages/labs.html#erisk" />
		<title level="m" coord="11,107.66,494.61,256.76,9.94">CLEF 2022 Conference and Labs of the Evaluation Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,107.66,519.81,390.57,9.94" xml:id="b4">
	<monogr>
		<ptr target="https://erisk.irlab.org/eRisk2022.html" />
		<title level="m" coord="11,107.66,519.81,162.87,9.94">eRisk 2022 Text Research Collection</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,107.66,532.53,415.46,9.94;11,107.66,545.13,413.76,9.94" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="11,425.16,532.53,97.95,9.94;11,107.66,545.13,165.78,9.94">Analysis and Experim ents on Early Detection of Depression</title>
		<author>
			<persName coords=""><forename type="first">Fidel</forename><surname>Cacheda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diego</forename><surname>Fern√°ndez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francisco</forename><forename type="middle">J</forename><surname>Novoa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V√≠ctor</forename><surname>Carneiro</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2125/paper_69.pdf" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,107.66,557.85,415.13,9.94;11,107.66,570.45,412.39,9.94;11,107.66,583.08,413.84,9.94" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="11,337.71,570.45,182.34,9.94;11,107.66,583.08,92.73,9.94">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<author>
			<persName coords=""><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Mi Ke Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Veselin</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692.version1</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>Computing Research Repository</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="11,107.66,595.80,415.43,9.94;11,107.66,608.40,261.35,9.94" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="11,250.36,595.80,151.43,9.94">BERT Word Embeddings Tutorial</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Mccormick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nick</forename><surname>Ryan</surname></persName>
		</author>
		<ptr target="https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,107.42,621.01,415.50,10.05;11,107.42,633.72,415.65,9.94" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="11,277.69,621.01,245.23,10.04;11,107.42,633.72,194.70,9.94">Predicting Web User&apos;s Tendency of Depression Using Negative Thought-Driven Depression Model</title>
		<author>
			<persName coords=""><forename type="first">Yen-Shuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wen-Hsiang</forename><surname>Lu</surname></persName>
		</author>
		<ptr target="https://hdl.handle.net/11296/uskn27" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.78,646.32,412.21,9.94;11,107.42,659.04,328.69,9.94" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="11,256.21,646.32,266.78,9.94;11,107.42,659.04,82.67,9.94">A RoBERTa-based model on measuring the severity of the signs of depression</title>
		<author>
			<persName coords=""><forename type="first">Shih-Hung</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhao-Jun</forename><surname>Qiu</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-86.pdf" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.78,671.64,412.36,9.94;11,107.42,684.36,415.47,9.94;11,107.42,696.96,415.68,9.94;11,107.42,709.68,390.54,9.94" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,402.90,671.64,120.24,9.94;11,107.42,684.36,235.61,9.94">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,362.42,684.36,160.47,9.94;11,107.42,696.96,415.68,9.94;11,107.42,709.68,196.29,9.94">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">June 2-7, (2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.78,722.28,412.16,9.94;11,107.42,734.77,414.50,10.05" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="11,254.73,734.77,111.50,10.04">Attention IsAll You Need</title>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">≈Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762v56</idno>
		<imprint>
			<date type="published" when="2017-12">Dec 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.78,74.66,412.49,9.94;12,107.42,87.28,415.72,9.94;12,107.42,100.00,183.34,9.94" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Javier</forename><surname>Parapar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patricia</forename><surname>Mart√≠n-Rodilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabio</forename><surname>Crestani</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-72.pdf" />
		<title level="m" coord="12,439.90,74.66,83.37,9.94;12,107.42,87.28,350.39,9.94">Overview of eRisk at CLEF 2021: Early Risk Prediction on the Internet (Extended Overview)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.78,112.84,412.26,9.94;12,107.42,126.88,415.72,9.94;12,107.42,139.60,81.68,9.94" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="12,346.38,112.84,176.66,9.94;12,107.42,126.88,251.28,9.94">Predicting Sign of Depression via Using Frozen Pre-trained Models and Random Forest Classifier</title>
		<author>
			<persName coords=""><forename type="first">Hassan</forename><surname>Alhuzali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tianlin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-73.pdf" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.78,152.20,412.33,9.94;12,107.42,164.81,415.27,10.04;12,107.42,177.52,401.08,9.94" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="12,172.90,164.81,349.79,10.04;12,107.42,177.52,154.82,9.94">uOttawa at eRisk 2021: Automatic Filling of the Beck&apos;s Depression Inventory Questionnaire using Deep Learning</title>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruba</forename><surname>Skaik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prasadith</forename><surname>Buddhitha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dimo</forename><surname>Angelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maxwell</forename><forename type="middle">Thomas</forename><surname>Fredenburgh</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-79.pdf" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.78,190.24,412.13,9.94;12,107.42,202.84,415.84,9.94;12,107.42,215.44,81.68,9.94" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Ana-Maria</forename><surname>Bucur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Cosma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Liviu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Dinu</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-77.pdf" />
		<title level="m" coord="12,350.66,190.24,172.25,9.94;12,107.42,202.84,239.76,9.94">Early Risk Detection of Pathological Gambling, Self-Harm and Depression Using BERT</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.78,228.40,409.60,9.94;12,107.42,242.44,404.44,9.94" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="12,372.66,228.40,147.73,9.94;12,107.42,242.44,158.25,9.94">Transfer Learning for Automated Responses to the BDI Questionnaire</title>
		<author>
			<persName coords=""><forename type="first">Christoforos</forename><surname>Spartalis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">George</forename><surname>Drosatos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Avi</forename><surname>Arampatzis</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-84.pdf" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,110.78,255.07,412.32,9.94;12,107.42,267.79,415.67,9.94;12,107.42,280.39,415.63,9.94;12,107.42,293.11,273.05,9.94" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,442.54,255.07,80.56,9.94;12,107.42,267.79,222.29,9.94">Evaluation Report of eRisk 2022: Early Risk Prediction on the internet</title>
		<author>
			<persName coords=""><forename type="first">Javier</forename><surname>Parapar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patricia</forename><surname>Mart√≠n-Rodilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabio</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,350.22,267.79,172.86,9.94;12,107.42,280.39,415.63,9.94;12,107.42,293.11,19.87,9.94">Experimental IR Meets Multilinguality, Multimodality, and Interaction. 13th International Conference of the CLEF Association, CLEF 2022</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
