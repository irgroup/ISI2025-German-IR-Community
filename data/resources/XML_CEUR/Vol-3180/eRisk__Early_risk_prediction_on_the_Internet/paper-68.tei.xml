<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,412.26,15.42;1,88.78,106.66,338.40,15.42">UNED-MED at eRisk 2022: depression detection with TF-IDF, linguistic features and Embeddings</title>
				<funder ref="#_xkQ8YMH">
					<orgName type="full">UNED -Santander</orgName>
				</funder>
				<funder ref="#_YqWg8CW">
					<orgName type="full">MCI/AEI/FEDER, UE)</orgName>
				</funder>
				<funder ref="#_qMTzbch">
					<orgName type="full">Spanish Ministry of Science and Innovation</orgName>
				</funder>
				<funder ref="#_x6qWaDx #_pQBYJHp #_AyyCBsA">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,115.30,11.96"><forename type="first">Elena</forename><surname>Campillo-Ageitos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dpto. Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="laboratory">NLP &amp; IR Group</orgName>
								<orgName type="institution">Universidad Nacional de Educación a Distancia (UNED)</orgName>
								<address>
									<addrLine>Juan del Rosal 16</addrLine>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,217.23,134.97,101.54,11.96"><forename type="first">Juan</forename><surname>Martinez-Romo</surname></persName>
							<email>juaner@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">Dpto. Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="laboratory">NLP &amp; IR Group</orgName>
								<orgName type="institution">Universidad Nacional de Educación a Distancia (UNED)</orgName>
								<address>
									<addrLine>Juan del Rosal 16</addrLine>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">IMIENS: Instituto Mixto de Investigación</orgName>
								<orgName type="institution">Escuela Nacional de Sanidad</orgName>
								<address>
									<addrLine>Monforte de Lemos 5</addrLine>
									<postCode>28019</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,355.05,134.97,75.93,11.96"><forename type="first">Lourdes</forename><surname>Araujo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dpto. Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="laboratory">NLP &amp; IR Group</orgName>
								<orgName type="institution">Universidad Nacional de Educación a Distancia (UNED)</orgName>
								<address>
									<addrLine>Juan del Rosal 16</addrLine>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">IMIENS: Instituto Mixto de Investigación</orgName>
								<orgName type="institution">Escuela Nacional de Sanidad</orgName>
								<address>
									<addrLine>Monforte de Lemos 5</addrLine>
									<postCode>28019</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,412.26,15.42;1,88.78,106.66,338.40,15.42">UNED-MED at eRisk 2022: depression detection with TF-IDF, linguistic features and Embeddings</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">0CEC0ACC331999FC56E91A7E3C916E4F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>early risk detection</term>
					<term>depression detection</term>
					<term>natural language processing</term>
					<term>data extraction</term>
					<term>data relabeling</term>
					<term>CEUR-WS</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mental health problems, such as depression and anxiety, are conditions that can have very serious consequences if untreated, and cause the patient a lot of suffering. Research suggests that the way people write can reflect mental well-being and mental health risks, and social media provides a source of user-generated text to study. Early detection is crucial for mental health problems, and with this in mind the shared task eRisk was created. This paper describes the participation of the group UNED-MED on the 2022 T2 subtask. Participants were asked to create systems to detect early signs of depression on users from Reddit. Our team proposes two approaches: feature-driven classifiers with features based on text data, TF-IDF terms, first-person pronoun use, sentiment analysis and depression terminology; and a Deep Learning classifier with pretrained Embeddings. The official task results show modest results that show the difficulty of working with depression data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Mental health problems such as anxiety and depression are conditions that affect millions of people every year. People with depression may not seek medical attention in time, causing them unnecessary suffering. Some patients forego medical attention because they are not aware that they need it, but some still avoid it because of the stigma associated with it. Whatever the cause, untreated mental illnesses can worsen with time and lead to serious consequences, such as substance abuse, or even death.</p><p>Language is a tool we use to communicate with one another. Besides transmitting the intended message with it, we also transmit information about ourselves: our upbringing, our mood, our emotional well-being, etc. Several studies have shown a correlation between differences in the way people talk and write, and having a mental health condition <ref type="bibr" coords="1,370.96,556.30,11.24,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,384.62,556.30,7.49,10.91" target="#b1">2]</ref>. This use of language can be studied with Natural Language Processing (NLP) to detect untreated mental health problems.</p><p>Social media sites such as Twitter, Tumblr or Reddit present a natural collection of usergenerated texts. They use these sites to communicate with their friends, follow celebrities, and to express themselves. There is an insurmountable amount of information that can be applied to NLP techniques with various purposes. Recent research has been applying these techniques to automatically detect users who suffer from several mental health issues.</p><p>In the case of mental health, early treatment is especially important, since it increases the probabilities of a good prognosis. The longer a patient suffers without medical treatment, the more likely they are to suffer from associated risks. Early detection helps detect these cases before they become bigger problems. Most work done in the literature focus on detecting people who already suffer from conditions, but we believe focusing on early detection is important to allow for a faster diagnostic and faster intervention.</p><p>The eRisk shared task was created with this objective in mind. The shared task focuses on the early detection of mental health problems in social networks. Previous editions have focused on anorexia, self-harm, and pathological gambling. The 2022 eRisk shared task proposed three subtasks, and this paper presents our participation to the subtask T2: early detection of depression. An overview of the task and the overall results for all participating teams can be found on <ref type="bibr" coords="2,132.26,303.75,11.43,10.91" target="#b2">[3]</ref>.</p><p>The following sections are organized as follows: Section 2 describes the data we used to train our models; Section 3 details the models we proposed for this task; Section 4 explains the experiment setup; Section 5 summarizes our results after the test stage; finally, Section 6 presents our conclusions and ideas for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Dataset Description</head><p>Our system for the T2 task was trained and evaluated with two different datasets: 1) The official eRisk 2022 shared Task 2 dataset, and 2) The UNED-MED depression Reddit dataset. This section briefly describes each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">eRisk 2022 shared Task 2 dataset</head><p>This is the official eRisk dataset given to the participants. It is a Reddit early risk detection dataset first presented by Losada et al. at <ref type="bibr" coords="2,286.66,506.95,11.58,10.91" target="#b3">[4]</ref>. The dataset is comprised of a collection of documents. Each document contains the post submission history of a user from Reddit. The users are labeled as one of two classes: Positive (at risk of depression) and negative (control group).</p><p>This year, the training data was formed with the training and testing data of 2017 and 2018 eRisk depression tasks. We decided to use the data from 2017 to train our model, and the data from 2018 for evaluation.</p><p>Each post in a user document is either a text submission or a comment to another users' submission. The posts are ordered chronologically, from earliest to latest. The number of posts for an user is indeterminate, as is the length of each post. Additional data comes with each post, such as the date of submission.</p><p>The classes are deeply unbalanced, as can be seen in table <ref type="table" coords="2,353.06,655.99,4.97,10.91" target="#tab_0">1</ref> represented by the Original eRisk column: there are 7 times more negative users than positive users. This can be a challenge when training classifiers, especially when the most important class is the least represented one.</p><p>Table <ref type="table" coords="3,126.76,100.52,5.02,10.91" target="#tab_1">2</ref> shows some statistics about the dataset. The average length of messages for positive users is 206, while for negative users it is 170. The 25%, 50%, and 75% values show that, while below the median, most posts are short for both groups of users, positive users tend to write longer posts, even if the longest post was written by a negative user.</p><p>One brief exploration of the data shows some challenges from the textual data. This is no surprise, since they are not formal texts; they are sentences written by people on the Internet to communicate between each other. It is widely observed that Internet speech has deep particularities, such as loose grammar, incorrect spelling (sometimes in purpose, and with meaning), emoji use, etc. A whole paper could be written on social cues only observed on Internet speech. We also find metadata, such as hyperlinks, references to other users, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">UNED-MED 2022 depression Reddit dataset</head><p>One of the most challenging aspects of the official dataset is the deeply unbalanced aspect of the classes. Detecting positive users is arguably much more important than detecting negative users, but they are underrepresented in the training dataset. We have curbed this problem in previous editions of the shared task by applying data oversampling. In this occasion, we obtained additional data from Reddit, following the strategies described in <ref type="bibr" coords="3,421.99,325.96,11.43,10.91" target="#b3">[4]</ref>.</p><p>We used the PRAW Python Reddit API Wrapper to extract new data <ref type="foot" coords="3,407.79,337.76,3.71,7.97" target="#foot_0">1</ref> . We searched Reddit submissions with the following search queries:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• diagnosed AND depression • I AND am AND diagnosed AND depression</head><p>We searched on r/all, and on the following subreddits related to mental health and depression:</p><formula xml:id="formula_0" coords="3,107.28,434.78,89.24,138.28">• r/addiction • r/adultdepression • r/anxiety • r/anxietyhelp • r/depression • r/depression_help • r/mentalhealth • r/mentalillness • r/sad • r/suicidewatch</formula><p>Results were manually reviewed to make sure users were talking about themselves, and had been officially diagnosed with clinical depression. A list was compiled, and we obtained the post and comment history of each user. Users with less than ten submissions were discarded.</p><p>The resulting dataset is a collection of 235 users. Table <ref type="table" coords="3,335.17,625.62,4.97,10.91" target="#tab_1">2</ref> shows some statistics for the resulting dataset. We can see that the statistics resemble those of the positive users of the original eRisk dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Model</head><p>We present three early risk models, depending on the classifier algorithm we use: 1) Random Forest, 2) XGBoost, and 3) CNN. Models 1 and 2 are based on traditional machine learning techniques, while model 3 applies Deep Learning. Features for models 1 and 2 are a combination of TF-IDF and text-based. Features for model 3 were Embeddings. Each model is conformed of three stages: 1) Data pre-processing, 2) features, and 3) classification. The models take one message by one user and predict whether this user is at risk of depression <ref type="bibr" coords="4,140.51,468.26,11.75,10.91" target="#b0">(1)</ref> or not (0). As is established by the eRisk task, a positive decision is final, but a negative decision may be rectified later.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Training data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Relabeled eRisk</head><p>Labels in the Original eRisk dataset are applied at user level, not at post level. This means that every post from a positive user is labeled positive, and vice-versa. We propose the hypothesis that not all posts by users at risk contain relevant information that can be detected by an early risk system, and that training a system with these posts labeled as positive makes the system perform worse. We could approach this hypothesis in different ways: for example, we could apply unsupervised learning, or treat the problem as a zero-shot classification problem. As a first approach to the problem, we chose to re-classify only posts labeled as positive by using sentiment analysis. Posts with a negative sentiment analysis above a certain threshold would keep their classification as positive, while others would be re-classified as negative.</p><p>This training set was created with this strategy applied to the Original eRisk training set. Posts from positive users were relabeled based on the sentiment analysis strategy. We applied a twitter-XLM-roBERTa-base model trained on tweets and finetuned for Sentiment Analysis<ref type="foot" coords="5,485.80,429.23,3.71,7.97" target="#foot_1">2</ref>  <ref type="bibr" coords="5,492.63,430.98,11.28,10.91" target="#b4">[5]</ref>.</p><p>Table <ref type="table" coords="5,127.03,444.53,5.07,10.91" target="#tab_2">3</ref> shows statistics of the dataset after applying the relabeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Preprocessing</head><p>Standard text preprocessing was applied to the text from each post. Posts were cleaned, tokenized, and stems were obtained. Stop words were kept as part of the text, since we believe they are important for this particular task. We used the Python library redditcleaner<ref type="foot" coords="5,282.24,533.15,3.71,7.97" target="#foot_2">3</ref> to clean the textual data. We removed Markdown formatting, separated contractions, removed hyperlinks, HTML tags, numbers and multiple spaces. Finally, all text was made lowercase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Windowfying</head><p>Some texts are long, while others are exceptionally short. To curve this difference and make sure a significant length of text is processed in each step without compromising speed, we applied a sliding window to the posts. After cleaning, we joined the text of a post from its previous 𝑤 messages, where 𝑤 is a configurable parameter. Features, explained next, were calculated on this message window instead of only on the text of the particular post.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Features</head><p>We used two different strategies for features, depending on whether the classifier algorithm was a traditional machine learning algorithm (models 1 and 2), or a Deep Learning one (model 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Traditional features</head><p>The features applied to traditional machine learning were an adapted version of the ones used for the eRisk 2021 T2 <ref type="bibr" coords="6,186.01,226.67,11.43,10.91" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text-based features</head><p>We applied two features in this category: 1) text length and 2) number of words. We showed in section 2 that positive users are more likely to write longer texts, so we keep track of this information with these two features.</p><p>Similarly to our previous eRisk participations, we applied a collection features tailored to the depression dataset. Features were normalized by text length and discretized to a fixed number of bins.</p><p>First-person pronouns First-person pronouns: Several works <ref type="bibr" coords="6,381.03,351.93,12.85,10.91" target="#b6">[7]</ref>  <ref type="bibr" coords="6,396.61,351.93,12.85,10.91" target="#b7">[8]</ref> have established that people with mental health problems such as depression tend to use more first-person pronouns when they speak. We create a feature that counts the number of times a first-person singular pronoun is used in a text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depression-related words</head><p>In previous editions of the shared task, we applied a wordset of self-harm related terms as a feature. This year, we applied a collection of words related to clinical depression, and the moods and topics associated with it extracted from <ref type="bibr" coords="6,435.21,448.44,11.28,10.91" target="#b8">[9]</ref>. This feature counts the number of depression-related words that appear in a text.</p><p>We combined these features to TF-IDF-based features using Scipy Hparse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">TF-IDF features</head><p>Similarly to previous years, we trained a TF-IDF featurizer on the positive users of the data and used this featurizer to obtain TF-IDF features for each message window. The featurizer was trained exclusively on positive data (in the case of the relabeled dataset, it was trained only on those messages that remained positive) because we want to detect words used frequently by positive users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Embeddings</head><p>Embeddings were used for the Deep Learning model. We applied Standford's pre-trained GloVe <ref type="bibr" coords="6,89.29,641.94,18.07,10.91" target="#b9">[10]</ref> Wikipedia 2014 100d word Embeddings. Posts were windowfied and then padded to a sufficiently long length in order to include the longest messages before applying the Embeddings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Classifier Algorithms</head><p>We worked with traditional machine learning models, and with one Deep Learning model. The classifiers predict whether a message window belongs to a user at risk of being "positive" or "negative". Like the task specifies, a positive decision is final, but a negative decision may be revised later.</p><p>We worked with two traditional machine learning models: Random Forest and XGBoost; and one Deep Learning model: a Convolutional Neural Network (CNN).</p><p>• Random Forest: We used the scikit-learn<ref type="foot" coords="7,292.10,324.04,3.71,7.97" target="#foot_3">4</ref> implementation of the Random Forest Classifier </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">Training strategy</head><p>We applied descending training weights to positive posts. This was in order to make our system prioritize earlier messages, and detect positive users as fast as possible. Messages created by negative users were all assigned the same training weight <ref type="bibr" coords="7,447.73,486.22,10.29,10.91" target="#b0">(1)</ref>. Messages created by positive users were assigned descending weights, from oldest to most recent message, through a fixed rate (2 to 1). Our working notes from the eRisk 2021 task <ref type="bibr" coords="7,407.86,513.32,12.69,10.91" target="#b5">[6]</ref> present a thorough explanation of the algorithm used to calculate these training weights.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Discussion</head><p>In this section we analyze the task results of our participation. Table <ref type="table" coords="8,127.18,319.74,5.10,10.91" target="#tab_5">5</ref> shows our five runs metric results, plus the results for the best groups according to different metrics. Our results this year were modest, with a best latency-weighted F1 of 0.233 compared to NLPGroup-IISERB's 0.690.</p><p>When comparing our different runs, we observe that run 1 obtained the best results overall in all the available metrics, followed by run 3. In the following ranking metrics we will also observe our best results in these two runs. Run 1 used the Random forest model trained on the augmented dataset, while run 3 was an XGBoost model trained on the relabeled dataset. Other differences between these runs are the sizes of the feature windows: 10 for run 1, and 10 for run 3.</p><p>While modest, we believe these differences show that the strategies to improve the training dataset worked favorably. Our best results were obtained with a model trained on the augmented dataset, which used an increased amount of positive users during training. It would be interesting to see how the models would behave if we trained the XGBoost model on the augmented dataset instead, and the Random Forest model on the relabeled dataset. Unfortunately, due to the amount of combinations we wanted to test, we could not fit these combinations in the official task.</p><p>Smaller feature window sizes appear to yield better results, as can be also seen by the ranking of our results. Run 1 was trained on window sizes of size 10, and evaluated on sliding window sizes of the same size. Run 3 was trained on window sizes of size 100, but evaluated on sliding window sizes of size 10. It appears that using smaller sizes on test is better, but it may not be necessary during training.</p><p>Our worst results were obtained with run 4, the Deep Learning model. We can only wonder as to why this happened, since usually Deep Learning models perform better than traditional learning models in similar circumstances. Despite the sliding window size being 10, the same as runs 1 and 3, the latency value is also very high compared to our other runs (251 compared to less than 20 for all other runs). This makes us think that maybe something went wrong with</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 6</head><p>Ranking-based evaluation. Our team's results (UNED-MED) are compared to the best team's results. The best results overall are bolded.</p><p>1 writing 100 writings team run P@10 NDCG@10 NDCG@100 P@10 NDCG@10 NDCG@100 UNED-MED the implementation of this model. Overall, we believe the depression task has been significantly more challenging than previous edition of the eRisk task. We observed this too while preparing our systems during the training phase, and we believe this might be due to the nature of the data. eRisk datasets are obtained by searching users on Reddit that have explicitly said that they were diagnosed with the mental health problem the task is about (depression, in this case). While other problems such as anorexia and self-harm still have a lot of stigma, openly talking about one's depression is seen more and more in this day and age. Therefore, it is more possible that most positive users in previous years, where anorexia or self-harm were detected, were accounts created exclusively to talk about that specific problem (Reddit users call these kind of accounts "throwaways"), while positive users in the depression dataset are normal users that use their Reddit account to talk about their hobbies, interests, etc.</p><p>Table <ref type="table" coords="9,127.67,516.20,5.17,10.91">6</ref> shows our ranked results compared to teams that obtained the best results in this category. In this case we can see some better results in some of the categories for runs 1 and 3. We can see that our results are better in the beginning, when only one message has been processed, and they decrease as time goes by. This might indicate that our system performs better when only a limited amount of messages for every user are observed, and that observing too many messages results in yielding too many false positive results. This is in concordance with results from table 5, where we can see that Recall is very high for four of the five runs, while Precision is very low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 6</head><p>Ranking-based evaluation. Our team's results (UNED-MED) are compared to the best team's results. The best results overall are bolded. Continuation 1 writing 100 writings team run P@10 NDCG@10 NDCG@100 P@10 NDCG@10 NDCG@100 UNED-MED </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Work</head><p>This paper presented the UNED-MED participation for the eRisk 2022 T2 task. We developed several classifier models based on TF-IDF, text and specially-tailored features, and a Deep Learning classifier model with Embeddings. We also implemented several strategies to reduce the imbalance of the training data: we obtained more data from Reddit, and we relabeled the original training data. The test results show that our systems obtain modest results, and that more effort is needed to achieve state-of-art results.</p><p>In future work, we would like to keep exploring strategies to relabel the data, and maybe experiment with zero-shot learning. This would allow the system to be portable from one kind of disease to another with minimal effort.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,531.54,357.93,10.91;4,107.28,557.04,398.71,10.91;4,116.56,570.59,100.16,10.91;4,107.28,585.49,398.70,10.91;4,116.56,599.04,264.65,10.91;4,107.28,613.95,398.70,10.91;4,116.56,627.50,233.98,10.91"><head></head><label></label><figDesc>Based on the data described in section 2, we created three different training sets: • Original eRisk: This training set was formed by combining the eRisk 2022 shared Task 2 train and test datasets. • Augmented eRisk: This training set was created by incorporating the UNED-MED 2022 depression Reddit dataset to the Original eRisk training set. • Relabeled eRisk: This training set was created by applying a relabelling strategy based on sentiment analysis to the Original eRisk training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,116.56,339.34,20.31,10.91;7,107.28,354.25,48.73,10.91;7,156.01,352.49,3.71,7.97;7,160.21,354.25,345.77,10.91;7,116.56,367.79,308.76,10.91;7,107.28,382.70,398.99,10.91;7,116.16,396.25,389.82,10.91;7,116.56,409.80,277.71,10.91"><head></head><label></label><figDesc><ref type="bibr" coords="7,116.56,339.34,16.25,10.91" target="#b10">[11]</ref>. • XGBoost 5 : This model is a type of ensemble model. It learns by optimizing a distributed gradient on learning algorithms of the Gradient Boosting framework. • CNN: We implemented a Convolutional Neural Network using Keras. The Neural Network was formed by a CNN layer of size 64, a GlobalMaxPooling layer, a Dense layer with relu activation, and an output Dense layer with sigmoid activation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,90.49,416.99,82.95"><head>Table 1</head><label>1</label><figDesc>Breakdown of the Original eRisk 2022 dataset's number of positive and negative users, plus the additional extracted data.</figDesc><table coords="4,177.49,128.31,240.29,45.13"><row><cell></cell><cell cols="3">Original eRisk New eRisk Combined</cell></row><row><cell>Positive users</cell><cell>214</cell><cell>235</cell><cell>449</cell></row><row><cell>Negative users</cell><cell>1493</cell><cell>0</cell><cell>1493</cell></row><row><cell>All users</cell><cell>1707</cell><cell>235</cell><cell>1942</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,88.98,193.34,418.10,144.95"><head>Table 2</head><label>2</label><figDesc>Analysis of the messages text length of the Original eRisk 2022 dataset and the UNED-MED 2022 dataset, represented as New eRisk</figDesc><table coords="4,141.58,233.39,312.11,104.91"><row><cell></cell><cell cols="4">Original eRisk Original positive Original negative New eRisk</cell></row><row><cell>count</cell><cell>1076582</cell><cell>90222</cell><cell>986360</cell><cell>106588</cell></row><row><cell>mean</cell><cell>172.91</cell><cell>205.54</cell><cell>169.92</cell><cell>208.87</cell></row><row><cell>std</cell><cell>538.48</cell><cell>398.54</cell><cell>549.41</cell><cell>481.77</cell></row><row><cell>min</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>25%</cell><cell>40</cell><cell>40</cell><cell>40</cell><cell>33</cell></row><row><cell>50%</cell><cell>73</cell><cell>90</cell><cell>72</cell><cell>79</cell></row><row><cell>75%</cell><cell>160</cell><cell>213</cell><cell>156</cell><cell>199</cell></row><row><cell>max</cell><cell>38663</cell><cell>18175</cell><cell>38663</cell><cell>31638</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,88.98,90.49,394.75,133.00"><head>Table 3</head><label>3</label><figDesc>Analysis of the messages text length of the Original eRisk 2022 dataset after applying relabeling.</figDesc><table coords="5,171.18,118.58,252.91,104.91"><row><cell></cell><cell cols="3">Relabel eRisk Relabel positives Relabel negatives</cell></row><row><cell>count</cell><cell>531394</cell><cell>11521</cell><cell>519873</cell></row><row><cell>mean</cell><cell>160.67</cell><cell>203.26</cell><cell>159.73</cell></row><row><cell>std</cell><cell>397.83</cell><cell>224.69</cell><cell>400.77</cell></row><row><cell>min</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>25%</cell><cell>39</cell><cell>61</cell><cell>39</cell></row><row><cell>50%</cell><cell>72</cell><cell>134</cell><cell>72</cell></row><row><cell>75%</cell><cell>159</cell><cell>262</cell><cell>157</cell></row><row><cell>max</cell><cell>38216</cell><cell>4033</cell><cell>38216</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,88.99,90.49,397.34,97.13"><head>Table 4</head><label>4</label><figDesc>UNED-MED eRisk 2022 T2 runs configurations.</figDesc><table coords="7,108.95,118.58,377.38,69.04"><row><cell>run</cell><cell>dataset</cell><cell>model</cell><cell cols="2">training window size test sliding window size</cell></row><row><cell>0</cell><cell>Original eRisk</cell><cell>XGBoost</cell><cell>30</cell><cell>30</cell></row><row><cell>1</cell><cell cols="2">Augmented eRisk Random forest</cell><cell>10</cell><cell>10</cell></row><row><cell>2</cell><cell>Relabeled eRisk</cell><cell>XGBoost</cell><cell>100</cell><cell>100</cell></row><row><cell>3</cell><cell>Relabeled eRisk</cell><cell>XGBoost</cell><cell>100</cell><cell>10</cell></row><row><cell>4</cell><cell>Original eRisk</cell><cell>CNN</cell><cell>10</cell><cell>10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,88.96,585.50,417.02,38.01"><head>Table 4</head><label>4</label><figDesc>shows the parameter configurations for the five different runs. Each run uses a different combination of training data, classifier model and training and test window size. They all used weighted training.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,88.99,90.49,465.96,167.05"><head>Table 5</head><label>5</label><figDesc>eRisk 2022 T2 decision-based evaluation. Our teams' results (UNED-MED) are compared to the best results for each metric. Our best results for each metric and the overall best results for the rest of the teams are bolded.</figDesc><table coords="8,89.29,139.99,465.66,117.54"><row><cell>team name</cell><cell>run id</cell><cell>𝑃</cell><cell>𝑅</cell><cell cols="5">𝐹 1 𝐸𝑅𝐷𝐸 5 𝐸𝑅𝐷𝐸 50 𝑙𝑎𝑡𝑒𝑛𝑐𝑦 𝑡𝑝 𝑠𝑝𝑒𝑒𝑑</cell><cell>𝑙𝑎𝑡𝑒𝑛𝑐𝑦-𝑤𝑒𝑖𝑔ℎ𝑡𝑒𝑑 𝐹 1</cell></row><row><cell>UNED-MED</cell><cell>0</cell><cell cols="3">.119 .969 .212</cell><cell>.091</cell><cell>.056</cell><cell>18</cell><cell>.934</cell><cell>.198</cell></row><row><cell>UNED-MED</cell><cell>1</cell><cell cols="3">.139 .980 .244</cell><cell>.079</cell><cell>.046</cell><cell>13</cell><cell>.953</cell><cell>.233</cell></row><row><cell>UNED-MED</cell><cell>2</cell><cell cols="3">.122 .939 .215</cell><cell>.086</cell><cell>.057</cell><cell>15</cell><cell>.945</cell><cell>.204</cell></row><row><cell>UNED-MED</cell><cell>3</cell><cell cols="3">.131 .949 .231</cell><cell>.084</cell><cell>.051</cell><cell>15</cell><cell>.945</cell><cell>.218</cell></row><row><cell>UNED-MED</cell><cell>4</cell><cell cols="3">.084 .163 .111</cell><cell>.079</cell><cell>.078</cell><cell>251</cell><cell>.252</cell><cell>.028</cell></row><row><cell>NLPGroup-IISERB</cell><cell>0</cell><cell cols="3">.682 .745 .712</cell><cell>.055</cell><cell>.032</cell><cell>9</cell><cell>.969</cell><cell>.690</cell></row><row><cell>BLUE</cell><cell>0</cell><cell cols="3">.395 .898 .548</cell><cell>.047</cell><cell>.027</cell><cell>5</cell><cell>.984</cell><cell>.540</cell></row><row><cell>UNSL</cell><cell>2</cell><cell cols="3">.400 .755 .523</cell><cell>.045</cell><cell>.026</cell><cell>3</cell><cell>.992</cell><cell>.519</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,108.93,671.04,138.57,8.97"><p>https://praw.readthedocs.io/en/stable/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,108.93,660.08,252.83,8.97"><p>https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,108.93,671.04,139.64,8.97"><p>https://pypi.org/project/redditcleaner/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="7,108.93,660.03,82.18,8.97"><p>https://scikit-learn.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="7,108.93,670.98,149.13,8.97"><p>https://xgboost.readthedocs.io/en/stable/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has been partially supported by the <rs type="funder">Spanish Ministry of Science and Innovation</rs> within the <rs type="projectName">DOTT-HEALTH</rs> Project (<rs type="funder">MCI/AEI/FEDER, UE)</rs> under Grant <rs type="grantNumber">PID2019-106942RB-C32</rs>, as well as project <rs type="projectName">EXTRAE II</rs> (<rs type="grantNumber">IMIENS 2019</rs>), the research network <rs type="grantNumber">AEI</rs> <rs type="grantNumber">RED2018-102312-T</rs> (<rs type="grantNumber">IA-Biomed</rs>), and a predoctoral contract <rs type="funder">UNED -Santander</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_qMTzbch">
					<orgName type="project" subtype="full">DOTT-HEALTH</orgName>
				</org>
				<org type="funded-project" xml:id="_YqWg8CW">
					<idno type="grant-number">PID2019-106942RB-C32</idno>
					<orgName type="project" subtype="full">EXTRAE II</orgName>
				</org>
				<org type="funding" xml:id="_x6qWaDx">
					<idno type="grant-number">IMIENS 2019</idno>
				</org>
				<org type="funding" xml:id="_pQBYJHp">
					<idno type="grant-number">AEI</idno>
				</org>
				<org type="funding" xml:id="_AyyCBsA">
					<idno type="grant-number">RED2018-102312-T</idno>
				</org>
				<org type="funding" xml:id="_xkQ8YMH">
					<idno type="grant-number">IA-Biomed</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,112.66,642.48,393.33,10.91;10,112.66,656.03,393.33,10.91;10,111.79,669.58,357.93,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,288.91,642.48,217.08,10.91;10,112.66,656.03,61.60,10.91">Social Media as a Measurement Tool of Depression in Populations</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">De</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Counts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,197.26,656.03,308.73,10.91;10,111.79,669.58,11.60,10.91">Proceedings of the 5th Annual ACM Web Science Conference, WebSci &apos;13</title>
		<meeting>the 5th Annual ACM Web Science Conference, WebSci &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,86.97,393.33,10.91;11,112.66,100.52,394.00,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,352.82,86.97,153.17,10.91;11,112.66,100.52,168.04,10.91">Psychological Aspects of Natural Language Use: Our Words, Our Selves</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Mehl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">G</forename><surname>Niederhoffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,288.90,100.52,133.83,10.91">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="547" to="577" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,114.06,393.33,10.91;11,112.66,127.61,395.00,10.91;11,112.41,141.16,393.58,10.91;11,112.66,154.71,75.17,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,289.90,114.06,80.39,10.91;11,399.32,114.06,106.67,10.91;11,112.66,127.61,49.72,10.91">Early risk prediction on the internet</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">D</forename><surname>Parapar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martín</forename><surname>Rodilla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,189.28,127.61,318.38,10.91;11,112.41,141.16,289.13,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction. 13th International Conference of the CLEF Association, CLEF 2022</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Overview of erisk</note>
</biblStruct>

<biblStruct coords="11,112.66,168.26,393.32,10.91;11,112.66,181.81,393.33,10.91;11,112.66,195.36,315.09,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,224.53,168.26,281.46,10.91;11,112.66,181.81,24.22,10.91">A Test Collection for Research on Depression and Language Use CLEF</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">E</forename><surname>Losada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
		<ptr target="https://tec.citius.usc.es/ir/pdf/evora.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="11,165.09,181.81,340.89,10.91;11,112.66,195.36,49.00,10.91">Évora (Portugal), Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="28" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,208.91,393.33,10.91;11,112.66,222.46,395.01,10.91;11,112.66,236.01,167.31,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="11,322.63,208.91,183.36,10.91;11,112.66,222.46,188.36,10.91">Xlm-t: Multilingual language models in twitter for sentiment analysis and beyond</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">E</forename><surname>Anke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2104.12250</idno>
		<ptr target="https://arxiv.org/abs/2104.12250.doi:10.48550/ARXIV.2104.12250" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,249.56,394.62,10.91;11,112.66,263.11,353.75,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Campillo-Ageitos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fabregat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martínez-Romo</surname></persName>
		</author>
		<title level="m" coord="11,401.68,249.56,105.60,10.91;11,112.66,263.11,279.24,10.91">Nlp-uned at erisk 2021: self-harm early risk detection with tf-idf and linguistic features</title>
		<imprint>
			<publisher>CLEF</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,276.66,393.61,10.91;11,112.66,290.20,51.08,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
		<title level="m" coord="11,190.22,276.66,255.90,10.91">The Secret Life of Pronouns What Our Words Say About Us</title>
		<imprint>
			<publisher>Bloomsbury Press</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,303.75,393.33,10.91;11,112.66,317.30,394.62,10.91;11,112.66,330.85,185.78,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,239.28,303.75,266.71,10.91;11,112.66,317.30,129.06,10.91">A meta-analysis of correlations between depression and first person singular pronoun use</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">S</forename><surname>Holtzman</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jrp.2017.02.005</idno>
		<ptr target="http://dx.doi.org/10.1016/j.jrp.2017.02.005" />
	</analytic>
	<monogr>
		<title level="j" coord="11,250.92,317.30,155.27,10.91">Journal of Research in Personality</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="63" to="68" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,344.40,394.53,10.91;11,112.66,357.95,393.33,10.91;11,112.66,371.50,394.03,10.91;11,112.66,385.05,165.61,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,112.66,357.95,393.33,10.91;11,112.66,371.50,22.42,10.91">Understanding depressive symptoms and psychosocial stressors on twitter: A corpus-based study</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mowery</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Stoddard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Coppersmith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Conway</surname></persName>
		</author>
		<idno type="DOI">10.2196/jmir.6895</idno>
		<ptr target="https://doi.org/10.2196/jmir.6895.doi:10.2196/jmir.6895" />
	</analytic>
	<monogr>
		<title level="j" coord="11,143.54,371.50,166.16,10.91">Journal of Medical Internet Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">48</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,398.60,394.52,10.91;11,112.66,412.15,395.01,10.91;11,112.66,425.70,224.18,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,295.45,398.60,207.11,10.91">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m" coord="11,128.37,412.15,277.22,10.91">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,439.25,56.11,10.91;11,188.76,439.25,318.52,10.91;11,112.41,452.79,226.52,10.91" xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<idno type="DOI">10.1023/a:1010933404324</idno>
		<ptr target="https://doi.org/10.1023/a:1010933404324.doi:10.1023/a:1010933404324" />
	</analytic>
	<monogr>
		<title level="j" coord="11,188.76,439.25,86.76,10.91">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
