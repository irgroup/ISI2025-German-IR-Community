<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,379.74,15.42;1,89.29,106.66,398.33,15.42;1,89.29,128.58,272.06,15.43">SimBa at CheckThat! 2022: Lexical and Semantic Similarity Based Detection of Verified Claims in an Unsupervised and Supervised Way</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,156.89,87.85,11.96"><forename type="first">Alica</forename><surname>Hövelmeyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Heinrich-Heine-Universität Düsseldorf (HHU)</orgName>
								<address>
									<addrLine>Universitätsstraße 1</addrLine>
									<postCode>40225</postCode>
									<settlement>Düsseldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">GESIS -Leibniz Institute for the Social Sciences</orgName>
								<address>
									<addrLine>Unter Sachsenhausen 6-8</addrLine>
									<postCode>50667</postCode>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,194.65,156.89,78.96,11.96"><forename type="first">Katarina</forename><surname>Boland</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Heinrich-Heine-Universität Düsseldorf (HHU)</orgName>
								<address>
									<addrLine>Universitätsstraße 1</addrLine>
									<postCode>40225</postCode>
									<settlement>Düsseldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">GESIS -Leibniz Institute for the Social Sciences</orgName>
								<address>
									<addrLine>Unter Sachsenhausen 6-8</addrLine>
									<postCode>50667</postCode>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.89,156.89,64.71,11.96"><forename type="first">Stefan</forename><surname>Dietze</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Heinrich-Heine-Universität Düsseldorf (HHU)</orgName>
								<address>
									<addrLine>Universitätsstraße 1</addrLine>
									<postCode>40225</postCode>
									<settlement>Düsseldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">GESIS -Leibniz Institute for the Social Sciences</orgName>
								<address>
									<addrLine>Unter Sachsenhausen 6-8</addrLine>
									<postCode>50667</postCode>
									<settlement>Cologne</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,379.74,15.42;1,89.29,106.66,398.33,15.42;1,89.29,128.58,272.06,15.43">SimBa at CheckThat! 2022: Lexical and Semantic Similarity Based Detection of Verified Claims in an Unsupervised and Supervised Way</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">9FEB959E9CA2E2511A8CB6FF2D7760E6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>fact-checking</term>
					<term>STS</term>
					<term>semantic similarity</term>
					<term>lexical similarity</term>
					<term>sentence embeddings</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One step in many automated fact-checking pipelines is verified claim retrieval, i.e. checking whether a claim has been fact-checked before. We approach this task as a semantic textual similarity problem. For this, we examine the extent to which an input claim and a verified claim are similar at semantic, textual, lexical and referential levels using a variety of NLP tools. We rank similar pairs based on these features using a supervised and an unsupervised model. We participate in two subtasks and compare our results for subtask 2A: detecting previously fact-checked claims from tweets and subtask 2B: detecting previously fact-checked claims in political debates for English data. We find that the combination of semantic and lexical similarity features performs best in finding relevant claim pairs for both subtasks. Furthermore, our unsupervised method is on par with the supervised one and seems to generalize well over similar tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The dissemination of true or false information through traditional channels, such as political speeches, or channels that have emerged in recent years, such as social media, is a powerful tool for shaping public opinion. Therefore, the analysis of claims made online or by public speakers is a popular field of research. The CLEF CheckThat Lab <ref type="bibr" coords="1,329.94,478.03,14.53,9.72" target="#b0">[1]</ref> contributes by offering shared tasks related to it. This paper reports on our submission for Task 2: Detecting previously fact-checked claims for English language.</p><p>We approach this task as a semantic textual similarity problem that we solve by combining different kinds of similarity features. We want to build on the success of sentence embedding models by trying out different models, their combinations and different ways of weighing them. In addition, we also want to contribute to a better understanding of sentence embeddings by investigating what kinds of possible similarities between sentences they capture and how their performance can be improved by adding complementary information. The combination of lexical and semantic similarity features proves to be particularly helpful. The great strength of CLEF 2022: Conference and Labs of the Evaluation Forum, September 5-8, 2022, Bologna, Italy alica.hoevelmeyer@hhu.de (A. Hövelmeyer); katarina.boland@hhu.de (K. Boland); stefan.dietze@hhu.de (S. Dietze) our approach is that we compare a supervised and an unsupervised method to rank the given data by similarity and are able to propose an unsupervised method that is on par with supervised approaches. Furthermore, there is evidence that our unsupervised method generalizes well over similar tasks. The code for both subtasks is available on github <ref type="foot" coords="2,368.62,125.86,3.71,7.97" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This submission is part of the 5th edition of the CheckThat! Lab. Previous editions, also held in conjunction with the Conference and Labs of the Evaluation Forum (CLEF) that also featured the task of Detecting Previously Fact-Checked Claims / Claim Retrieval, took place in 2020 <ref type="bibr" coords="2,491.76,213.34,14.22,10.91" target="#b1">[2]</ref> and 2021 <ref type="bibr" coords="2,125.48,226.89,13.10,10.91" target="#b2">[3]</ref>. The approaches proposed by the participants are similar to ours in various aspects.</p><p>For the lab in 2020 the data to be processed exclusively consisted of tweets as input claims. Many of the participants used pre-processing and cleaned the tweets, removing tweet-specific characters like hashtags <ref type="bibr" coords="2,204.04,267.54,12.99,10.91" target="#b3">[4]</ref>  <ref type="bibr" coords="2,220.83,267.54,13.00,10.91" target="#b4">[5]</ref> [6] <ref type="bibr" coords="2,254.44,267.54,11.58,10.91" target="#b6">[7]</ref>. Some teams solely made use of lexical and string similarity features <ref type="bibr" coords="2,170.87,281.08,13.35,10.91" target="#b4">[5]</ref>  <ref type="bibr" coords="2,186.96,281.08,11.53,10.91" target="#b5">[6]</ref>, whereas other teams used pre-trained language models to evaluate semantic similarity. These teams fine-tuned RoBERTa <ref type="bibr" coords="2,319.34,295.65,14.93,9.72" target="#b7">[8]</ref> <ref type="bibr" coords="2,334.26,295.65,14.93,9.72" target="#b3">[4]</ref> or used Sentence-BERT <ref type="bibr" coords="2,452.11,294.63,12.76,10.91" target="#b8">[9]</ref> [10] <ref type="bibr" coords="2,488.15,294.63,17.83,10.91;2,89.29,308.18,12.85,10.91">[11] [7]</ref> or Universal Sentence Encoder <ref type="bibr" coords="2,233.70,309.20,18.89,9.72" target="#b11">[12]</ref> <ref type="bibr" coords="2,252.60,309.20,18.89,9.72" target="#b12">[13]</ref> in order to calculate the distances between sentence embeddings. Different variations of Blocking-techniques were also used <ref type="bibr" coords="2,408.09,321.73,18.76,10.91" target="#b9">[10]</ref>[5] <ref type="bibr" coords="2,443.66,321.73,11.50,10.91" target="#b6">[7]</ref>. Similar to our approach, some teams combined lexical and semantic similarity features <ref type="bibr" coords="2,431.52,335.28,12.84,10.91" target="#b3">[4]</ref>  <ref type="bibr" coords="2,447.09,335.28,17.91,10.91" target="#b12">[13]</ref>  <ref type="bibr" coords="2,467.73,335.28,16.25,10.91" target="#b10">[11]</ref>.</p><p>In 2021 all teams made use of the sentence embedding model Sentence-BERT. Team NLytics <ref type="bibr" coords="2,123.07,362.38,19.30,10.91" target="#b13">[14]</ref> offered an unsupervised approach based on the distances of sentence embeddings gained using Sentence-BERT. This approach performed well for only one of the proposed subtasks.</p><p>Team DIPS <ref type="bibr" coords="2,146.83,403.03,19.85,10.91" target="#b14">[15]</ref> and Team Aschern <ref type="bibr" coords="2,249.88,403.03,19.84,10.91" target="#b15">[16]</ref> made use of the combination of a semantic similarity feature (also gained using the sentence embedding model Sentence-BERT) and a string (BM25 by Team DIPS) or lexical (TF.IDF by Team Aschern) similarity feature. Different from us, they only presented supervised approaches to rank the data based on these features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task Definition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Detection of previously fact-checked claims</head><p>One of the tasks that arise in the broader context of automated fact-checking is to check whether a claim has been fact-checked before. This can be considered the second step of a claim retrieval and verification pipeline, after the detection of check-worthy claims in different kinds of textual utterances and before the verification of those claims. This is addressed by task 2 <ref type="bibr" coords="2,462.67,563.92,11.58,10.91" target="#b2">[3]</ref>. More precisely, the task is to rank the most relevant verified claims out of a collection of already verified claims for a given input claim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data</head><p>The subtasks cover two different types of media that are used to disseminate claims. Subtask A deals with tweets, subtask B with political debates and speeches. Both types of text sequences containing claim utterances will simply be referred to as input claims in the following. For both tasks different kinds of already fact-checked claims are made available. These will be called verified claims. <ref type="bibr" coords="3,154.46,162.75,12.93,9.72" target="#b0">[1]</ref> Both input claims and verified claims consist of one or a few coherent sentences. The input claims of subtask A are given as strings, divided into a training dataset of 1167 input claims, a development test dataset of 201 input claims and a final test dataset of 209 input claims. A human-annotated mapping from every input claim to the most relevant verified claim (query relevance or qrels-file) constitutes the gold standard. Verified claims are crawled from the fact-checking website Snopes and are provided in JSON format containing title, subtitle, author, date and a vclaim-entry with the content of the claim.</p><p>The input claims of subtask B are also provided as strings, divided into a training dataset of 702 input claims, a development test dataset of 79 input claims and a final test dataset of 65 input claims. Here, a human-annotated mapping from every input claim to one or more relevant verified claims is given in addition to the training data and as a gold standard for the test data. Furthermore, transcripts of the debates or speeches the input claims are obtained from are given for the test data. 19250 verified claims are taken from the fact-checking website PolitiFact and made available in JSON format containing the entries vclaim_id, vclaim, date, truth_label, speaker, url, title and text.</p><p>The mappings of input claims to verified claims will be referred to as input-ver-claim pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Similarity-Based Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Semantic Similarity</head><p>The task is formulated as a ranking-problem, where input-ver-claim-pairs are ranked depending on the relevance of the verified claim for fact-checking the input claim. Thus, the task can be considered a semantic textual similarity problem (STS) where sentences are compared by their semantic content to rank sentences containing similar claims highest (cf. <ref type="bibr" coords="3,416.09,498.54,15.71,10.91" target="#b16">[17]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Sentence Embeddings</head><p>One promising way to deal with STS-problems is the usage of sentence embeddings. Sentence embeddings are fixed-sized vector representations that capture the meaning of sentences in so far that embeddings of semantically similar sentences are close in the corresponding vector space (cf. <ref type="bibr" coords="3,105.28,588.29,15.53,10.91" target="#b17">[18]</ref>). Sentence embedding models are usually trained on a huge amount of natural language data or rely on models that are trained on such. Thus they reflect the empirical distribution of linguistic elements and can be viewed as an appropriate method to investigate semantic similarity. That's because relying on the distributional hypothesis, "there is a correlation between distributional similarity and meaning similarity" <ref type="bibr" coords="3,344.31,642.48,16.68,10.91" target="#b18">[19]</ref>.</p><p>The usefulness of the application of sentence embeddings has already been demonstrated by the participants of last year's lab. The sentence embedding model Sentence-BERT <ref type="bibr" coords="3,449.72,669.58,12.86,10.91" target="#b8">[9]</ref> was used by the top-ranked teams of both subtask A and subtask B <ref type="bibr" coords="4,345.62,86.97,17.87,10.91" target="#b15">[16]</ref>  <ref type="bibr" coords="4,366.23,86.97,16.20,10.91" target="#b14">[15]</ref>. Therefore, we use them as starting points for different components of our application.</p><p>Sentence-BERT (SBERT) is a modification of the transformer-based pre-trained language models BERT <ref type="bibr" coords="4,150.49,127.61,18.07,10.91" target="#b19">[20]</ref> or RoBERTa <ref type="bibr" coords="4,222.33,128.63,16.01,9.72" target="#b7">[8]</ref> using a Siamese network structure. The language models are trained on natural language inference (NLI) data and a pooling operation is added to their outputs in order to derive fixed-sized vector representations of the input sentences.</p><p>The idea of training on NLI data in a supervised way in order to get meaningful sentence embeddings was introduced by the authors of the sentence embedding model InferSent <ref type="bibr" coords="4,488.08,182.82,17.90,9.72" target="#b17">[18]</ref> (InferSent). However they did not build their model upon a tranformer-based language model, but on an encoder based on a bi-directional LSTM architecture fed with pre-trained word embeddings (GloVe <ref type="bibr" coords="4,173.16,222.46,18.39,10.91" target="#b20">[21]</ref> or fastText <ref type="bibr" coords="4,239.81,223.47,16.62,9.72" target="#b21">[22]</ref>).</p><p>Similarly, the model Universal Sentence Encoder <ref type="bibr" coords="4,309.64,237.02,19.28,9.72" target="#b11">[12]</ref> (UniversalSE) averages together word and bi-gram level embeddings, passes the representations through a feed-forward deep neural network (DNN) and is trained on NLI data.</p><p>The authors of SimCSE <ref type="bibr" coords="4,202.48,277.67,21.98,9.72" target="#b22">[23]</ref>(SimCSE) also train their model on NLI data, but within a contrastive learning framework. Otherwise their model is similar to Sentence-BERT, relying on the pre-trained language models BERT and RoBERTa and adding a pooling operation to one of their output layers.</p><p>All sentence embedding models are also able encode small paragraphs instead of just sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Measuring Semantic Similarity Using Sentence Embeddings</head><p>For all of these sentence embeddings methods, there are pre-trained models available that can be used out of the box. For Sentence-BERT we used sentence-transformers/all-mpnet-base-v2, because it performs best for STS tasks compared to the other pretrained models <ref type="foot" coords="4,438.51,418.92,3.71,7.97" target="#foot_1">2</ref> . For InferSent we experimented with both versions, but report here only on the results obtained using version 2, which works with fastText <ref type="bibr" coords="4,213.19,448.78,16.63,9.72" target="#b21">[22]</ref>, because we got better results than using the GloVe-vocabulary in pre-liminary experiments. For Universal Sentence Encoder we used TF2.0 Saved Model (v4) <ref type="foot" coords="4,500.56,459.56,3.71,7.97" target="#foot_2">3</ref> , because this is the most widely used model available for Universal Sentence Encoder and for SimCSE we used princeton-nlp/sup-simcse-roberta-large <ref type="foot" coords="4,338.17,486.66,3.71,7.97" target="#foot_3">4</ref> , because this also performs best for STS tasks compared to the other pretrained models <ref type="foot" coords="4,319.56,500.21,3.71,7.97" target="#foot_4">5</ref> .</p><p>Since sentence embeddings are vector representations of sentences within the same vector space, their similarity can be measured applying cosine similarity (CosSim), resulting in similarity scores which are rational numbers ∈ [-100, 100]. These similarity scores should be referred to as SentEmb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Other Measures of Similarity</head><p>In the following other measures of similarity are presented. An overview of their corresponding metrics can be found in Table <ref type="table" coords="4,223.54,619.28,3.74,10.91" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">String Similarity</head><p>In addition to the study of semantic similarity using sentence embeddings, there are other ways in which the similarity of sentences can be measured.</p><p>The most naive approach to measure the similarity of two sentences is to compare them at the string level, i.e. to see how far the characters and strings that make up a sentence differ from those of other sentences. We used three different methods to measure the string similarity of sentences: Levenshtein Distance, Jaccard Distance and Sequence Matching.</p><p>Levenshtein Distance (LevDist) is a metric to measure the distance between two strings by counting the number of operations (insertions, deletions or substitutions) needed to change one string into the other. Sentences which are similar thus have a small Levenshtein Distance. In order to adjust this distance score to the other similarity scores, such that a higher value signifies a higher similarity, we multiplied the Levenshtein Distance by -1. In practice, we thereby get negative three-or two-digit integers as similarity scores for almost all input-ver-claim pairs.</p><p>In general, Jaccard Distance is used to measure the similarity of sets. It is computed by dividing the size of the intersection by the size of the union of the sets. The closer this value is to one, the more similar are the sets. In context of sentence-similarity it can be applied in two ways: either regarding the characters (JaccChar) or the tokens (JaccTok) a sentence consists of as elements of a set.</p><p>The Sequence Matching-algorithm (SeqMat) provided by the Python library difflib works by comparing "the longest contiguous matching subsequence that contains no 'junk' elements" and recursively repeating this on the remaining subsequences. Junk elements are determined heuristically based on the frequency of their duplicates in the text sequence <ref type="foot" coords="6,428.67,85.21,3.71,7.97" target="#foot_5">6</ref> .</p><p>Both the application of Jaccard Distance and Sequence Matching generate rational numbers ∈ [0, 1]. These similarity scores will be referred to as StringSim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Lexical Similarity</head><p>Another type of similarity, which is not clearly distinguishable from semantic and string similarity, is lexical similarity or similarity of words. We used one method to capture lexical similarity between sentences and simply counted how often two claims contained the same words.</p><p>For this, we tokenized all claims using NLTK's word tokenizer <ref type="bibr" coords="6,378.31,217.03,18.01,10.91" target="#b23">[24]</ref>, filtered out stop words and counted how often two claims contained the same tokens (WordCount). In order to value the number of equal tokens of shorter sentences higher than those of longer ones, we also computed a normalized ratio. For this we divided 100 by the number of tokens of both claims and multiplied the obtained value by two times the number of equal tokens. <ref type="foot" coords="6,425.50,269.47,3.71,7.97" target="#foot_6">7</ref> We did this both including stop words (WordTokRatio) and not including them (WordRatio).</p><p>Counting equal tokens we gained a positive integer similarity score, usually with less than three digits. We call this kind of discrete score SimCount. Computing the ratios we obtained percentages similar to the SentEmb-scores ∈ [0, 100]. This kind of scores will be referred to as SimRatio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Referential Similarity</head><p>Another way to think of similarity between sentences is to examine whether they refer to the same objects. To represent this kind of similarity we used two methods. Similar to the lexical similarity approach, we counted how often two claims contained words which are synonyms of each other. Additionally, we counted how often two claims contain the same named entities (NEs).</p><p>To compare the synonyms, we used WordNet <ref type="bibr" coords="6,304.16,455.48,21.58,10.91" target="#b24">[25]</ref> and looked for all available synsets the tokens mentioned in a claim are part of. We tokenized the sentences the same way as above. Then we counted how often two claims contained the same synsets (SynCount). Here we also computed the ratio of the count of synonyms regarding all synonyms (SynRatio) and all tokens (SynTokRatio) in the two sentences.</p><p>In order to compare NEs we used the entity-fishing system <ref type="bibr" coords="6,366.14,523.22,18.98,10.91" target="#b25">[26]</ref>, which recognizes named entities mentioned in a text and disambiguates them using Wikidata. The system is able to return the the Wikipedia and Wikidata identifiers of those mentions. We counted how often two claims contained named entities related to the same Wikipedia or Wikidata entry (NE). We also additionally computed the ratio of the count of NEs regarding all NEs (NERatio) and all tokens (NETokRatio) in the two sentences.</p><p>Similarly to the lexical similarity scores, we obtained two different kinds of metrics for these similarities: SimCount and SimRatio (see Table <ref type="table" coords="6,310.99,618.07,3.57,10.91" target="#tab_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Pre-Processsing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Cleaning tweets</head><p>For both subtasks we experimented with different ways of pre-processing the input claims. We cleaned the tweets given in subtask 2a to get rid of redundant information. We removed URLs, @-symbols and user-information (see Table <ref type="table" coords="7,286.11,155.20,3.57,10.91">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Example of a cleaned tweet from the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Tweet Cleaned Tweet</head><p>Starlink -here. Thanks, @elonmusk pic.twitter.com/dZbaYqWYCf -Mykhailo Fedorov (@FedorovMykhailo) February 28, 2022</p><p>Starlink -here. Thanks, elonmusk</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Including context</head><p>For subtask 2b, we tried incorporating the input claims' contexts within the speech or debate they were obtained from. We included the lines that were spoken before and after the relevant claim and integrated information about the current speaker by prepending "speaker X said" to the line of speech, where X is substituted by the name of the respective speaker (see Table <ref type="table" coords="7,492.99,368.21,3.57,10.91" target="#tab_1">3</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Unsupervised Approach</head><p>We tried out an unsupervised and a supervised method to utilize the information we gained on the different kinds of similarity. The main idea of the unsupervised approach is to rank the input-ver-claim pairs by the different similarity scores described above. Therefore a general similarity score is computed, combining the varying metrics (see Table <ref type="table" coords="7,409.77,613.78,3.61,10.91" target="#tab_0">1</ref>). This general score can roughly be compared to the percentage to which two sentences are similar where two exactly equal sentences would have a score of roughly 100. However, our way of combining the different similarity scores does not ensure that the resulting score is smaller than 100. It can sometimes be a little higher.</p><p>The general similarity score is computed the following way:</p><p>• taking the mean of all SentEmb-, SimRatio-and StringSim-scores normalized to [0, 100] • incorporating the LevDist: First the LevDist is divided by -100, which generates a positive factor that is smaller the more similar two sentences are. Then the similarity score obtained by computing the mean, is divided by this factor. <ref type="foot" coords="8,378.31,150.54,3.71,7.97" target="#foot_7">8</ref>• adding the SimCount-scores to the obtained score For the output the five most similar verified claims for an input claim are computed relying on the general similarity score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Supervised Approach</head><p>For the supervised approach we built a feature set out of the different similarity scores in order to classify if a verified claim is relevant for an input claim. We experimented with different methods to optimize our classification results. We used Blocking and Balancing in order to optimize our training results. Additionally we tried out different Classifiers and applied Feature Selection to further improve our output. Lastly we also made use of a heuristic based on our supervised approach to find relevant verified claims for all input claims.</p><p>To optimize the training, we used a Blocking approach. Instead of generating negative training instances by pairing each input claim with all but the true matching verified claims in the dataset, we computed the 50 most similar verified claims according to either of the four SentEmb scores and generated negative training instances using only those. More specifically, we extracted 4 sets of input-ver-claim-pairs, one set for each SentEmb method, with each set containing the 50 most similar verified claims identified by this method. Then we used the union of these sets as our final training set. We observed that all true input-ver-claims were covered. Besides the computational advantage of a smaller training set, this way the model may better learn to distinguish cases that are similar on the surface as all very dissimilar pairs have been filtered out before training.</p><p>Then all similarity scores, (also the SentEmb-scores) were added as features. As targets we obtained the relevance scores from the qrels-file of the training data. An unlabeled feature set was built for the test data.</p><p>After blocking, the percentage of true positives in our training data was still beneath 1% for both subtasks. That's why we applied Random Undersampling as a Balancing method and experimented with different parameters (see Tables <ref type="table" coords="8,320.47,538.35,5.07,10.91" target="#tab_2">4</ref> and<ref type="table" coords="8,347.41,538.35,3.57,10.91" target="#tab_3">5</ref>).</p><p>Then a Classifier was trained on the training data to predict relevance scores for the test data. We also experimented with different classifiers suited for binary classification, such as KNN, Logistic Regression, Linear SVC and a Decision Tree (see Tables <ref type="table" coords="8,365.46,579.00,7.47,10.91" target="#tab_4">6,</ref><ref type="table" coords="8,375.66,579.00,3.57,10.91">7</ref>).</p><p>We experimented with different selections of features out of the similarity features presented above. The influence of the ensemble of features is shown in Tables <ref type="table" coords="8,388.46,606.10,10.00,10.91" target="#tab_10">13</ref> and<ref type="table" coords="8,420.10,606.10,8.25,10.91" target="#tab_11">14</ref>. Additionally we If no relevant verified claim was predicted for an input claim, we relied on our unsupervised approach heuristically and chose the five most similar verified claims based on the mean of sentence embedding similarity scores. For 2A we chose SBERT, InferSent and SimCSE as SentEmb scores, for 2B all four models, including UniversalSE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Evaluation Metric</head><p>The task is considered a ranking task and is evaluated as such. The official ranking evaluation measure is Mean Average Precision at 5 (MAP@5). Additionally the provided scorer computes the measures MAP@k for k=1, 3, 5, 10, MRR and Precision@k for k= 3, 5, 10 (cf. <ref type="bibr" coords="9,464.79,641.85,11.26,10.91" target="#b2">[3]</ref>). The MAP@k metric measures the mean of correctly classified pairs in the top k of the returned output. MRR or Mean Reciprocal Rank measures how far the assigned rank of a correct pair  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Subtask 2A</head><p>For Subtask 2A we got the best result with our unsupervised approach, combining the similarity scores of SBERT, SimCSE, WordCount and WordTokRatio with a MAP@5 of 0.9175 (see Table <ref type="table" coords="11,89.04,408.33,7.90,10.91" target="#tab_10">13</ref>).</p><p>However the output we submitted made use of SBERT, SimCSE and WordCount and scored slightly worse (0.9075) (see Table <ref type="table" coords="11,243.50,435.43,3.62,10.91" target="#tab_5">8</ref>). We still achieved a score above the baselines utilizing a simple and fast unsupervised ranking method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Subtask 2B</head><p>For Subtask 2B we got the best results using a supervised approach. All similarity features were included, except from JaccChar (see Table <ref type="table" coords="12,279.11,121.08,7.93,10.91" target="#tab_11">14</ref>). We made use of Random Undersampling to increase the percentage of positives in the training data (relevant input-ver-claim pairs) to 8%. Then a Logistic Regression Classifier was trained and predicted 111 input-ver-claim pairs. The unsupervised heuristic described above was used to find relevant verified claims for the remaining input claims. This way the output achieved a MAP@5 of 0.4882.</p><p>The output we submitted also scored slightly worse than our best result with a MAP@5 of 0.459. To generate this output we used Linear Support Vector Classification and sampled to 14% positives. The considered features were SimCSE, JaccTok, WordCount, WordRatio, SynCount and SynRatio. This is still the best result for subtask 2B (see Table <ref type="table" coords="12,382.83,229.48,3.57,10.91" target="#tab_6">9</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Result of Pre-Processing</head><p>It turned out that our pre-processing approach did not improve our results on the test data for Subtask 2A (see Table <ref type="table" coords="12,207.45,418.53,8.06,10.91" target="#tab_7">10</ref>), although it did for the development test data. This is an issue worth investigating in future work. Tweet-specific units of text such as user-information were removed and it showed that it would have been useful to incorporate this kind of information for solving the task 2A. Nevertheless the pre-processing ensured that the data of both tasks was more similar and thereby helped assessing similarity of claims in general contexts. The incorporation of context for subtask 2B also did not improve the results on the development test data and on the final test data. That is why we used the original data for subtask 2B. The observation of the results of using the supervised approach on single features (see Table <ref type="table" coords="13,89.04,449.78,8.87,10.91" target="#tab_0">11</ref>) gives a good overview of their independent performance. As expected, the most successful features for both subtasks are the cosine similarities of the sentence embeddings. Especially SBERT, UniversalSE and SimCSE performed best on both task. That's because, as explained above, sentence embeddings are really useful to capture STS.</p><p>Interestingly SBERT is the most powerful feature for Subtask 2A and SimCSE the most powerful one for Subtask 2B. It would be worth further investigations to identify the reason for this difference. Both models are pre-trained on a large share of the same data, so maybe the contrastive training objective of SimCSE is partly responsible for it.</p><p>Another important observation is the fact that the lexical similarity features WordCount, WordRatio and WordTokRatio perform also really well for both tasks. This is kind of surprising, because these features are generated in such a simple way.</p><p>In contrast, the Jaccard Similarity of characters JaccChar is the weakest similarity feature. This can be explained by the fact that the consideration of equal characters, regardless of their order, doesn't have much informational value for the meaning of a sentence as a whole.</p><p>One interesting finding regarding the differences between the subtasks is the varying performance of string similarity features. The string similarity features LevDist and SeqMat are the only features that produce a higher MAP@5 for Subtask 2B than for Subtask 2A. Looking at the data, it is noticeable that the input claims and the verified claims provided for Subtask 2B often share long, continuous strings (see Table <ref type="table" coords="14,273.13,100.52,7.90,10.91" target="#tab_9">12</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2.">Feature Set</head><p>One of the most intriguing observations is the fact that both the unsupervised and the supervised approach perform best if lexical similarity is considered besides semantic similarity (see Tables <ref type="table" coords="14,89.04,387.69,10.01,10.91" target="#tab_10">13</ref> and<ref type="table" coords="14,120.71,387.69,7.80,10.91" target="#tab_11">14</ref>). The SentEmb features do not seem to cover lexical similarity and their performance benefits from the additional information contained by lexical similarity features. This is also supported by the observation that these two types of features do not have a strong correlation (see Tables <ref type="table" coords="14,139.98,428.34,10.15,10.91" target="#tab_12">15</ref> and<ref type="table" coords="14,171.99,428.34,7.90,10.91" target="#tab_13">16</ref>). Also it can be observed that especially for subtask B it is helpful to consider the combination of almost all similarity features in the supervised approach (see Table <ref type="table" coords="14,400.57,455.44,7.90,10.91" target="#tab_11">14</ref>).</p><p>Overall a higher number of features mostly increases the performance of the supervised approach and decreases it for the unsupervised approach as relatively uninformative features have a too high impact on the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Supervised vs Unsupervised Approach</head><p>One important observation with respect to our results is the fact that the unsupervised approach performs nearly as good as the unsupervised approach for subtask B and even better than the unsupervised approach for subtask A.</p><p>Since the task is a ranking problem, the unsupervised approach seems to perform sufficiently well for the given task. For similar tasks with the constraint to only find pairs that are relevant with a high certainty, the supervised approach might be more helpful.</p><p>Also it is reasonable to assume that the unsupervised approach generalizes well over similar tasks, because it is independent of the training data. This assumption is supported by the fact that the features that produce the best outputs are almost the same for both subtask A and  subtask B for the unsupervised approach (see Tables <ref type="table" coords="17,321.64,86.97,9.99,10.91" target="#tab_10">13</ref> and<ref type="table" coords="17,353.24,86.97,7.78,10.91" target="#tab_11">14</ref>), while the supervised approach relies on different features for the subtasks to produce good outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Future Work</head><p>It would be interesting to investigate the generalizability of our approach and to check if the assumption that the unsupervised approach generalizes better than the supervised approach is true. Also a detailed assessment of the impact of pre-processing would be beneficial for related works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusion</head><p>We treated the task to detect previously fact-checked claims as a STS-task. To solve it, we investigated different kinds of similarity measures between sentences, covering semantic, lexical and referential similarity. We found that it is beneficial to combine semantic similarity measures gained by calculating the distance of sentence embeddings with lexical similarity measures gained by counting shared words. Furthermore, we found that an unsupervised approach can be even more successful than a supervised approach for this task. Overall, our proposed approaches provide very good results for both subtasks with a MAP@5 of 0.907 for subtask A and a MAP@5 of 0.459 for subtask B, both scoring above the baselines and even being the top-ranked output for subtask B.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,90.49,346.32,265.63"><head>Table 1</head><label>1</label><figDesc>Metrics of similarity features.</figDesc><table coords="5,159.96,122.10,275.35,234.01"><row><cell>Kind of Similarity</cell><cell>Group</cell><cell>Feature</cell><cell>Metric</cell></row><row><cell>Semantic Similarity</cell><cell>SentEmb</cell><cell>SBERT</cell><cell>∈ [-100, 100]</cell></row><row><cell></cell><cell></cell><cell>InferSent</cell><cell></cell></row><row><cell></cell><cell></cell><cell>UniversalSE</cell><cell></cell></row><row><cell></cell><cell></cell><cell>SimCSE</cell><cell></cell></row><row><cell>String Similarity</cell><cell>LevDist</cell><cell>LevDist</cell><cell>∈ -Z</cell></row><row><cell></cell><cell>StringSim</cell><cell>SeqMat</cell><cell>∈ [0, 1]</cell></row><row><cell></cell><cell></cell><cell>JaccChar</cell><cell></cell></row><row><cell></cell><cell></cell><cell>JaccTok</cell><cell></cell></row><row><cell>Lexical Similarity</cell><cell>SimCount</cell><cell>WordCount</cell><cell>∈ N</cell></row><row><cell></cell><cell>SimRatio</cell><cell>WordRatio</cell><cell>∈ [0, 100]</cell></row><row><cell></cell><cell></cell><cell>WordTokRatio</cell><cell></cell></row><row><cell cols="2">Referential Similarity SimCount</cell><cell>SynCount</cell><cell>∈ N</cell></row><row><cell></cell><cell>SimRatio</cell><cell>SynRatio</cell><cell>∈ [0, 100]</cell></row><row><cell></cell><cell></cell><cell>SynTokRatio</cell><cell></cell></row><row><cell></cell><cell>SimCount</cell><cell>NE</cell><cell>∈ N</cell></row><row><cell></cell><cell>SimRatio</cell><cell>NERatio</cell><cell>∈ [0, 100]</cell></row><row><cell></cell><cell></cell><cell>NeTokRatio</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,88.99,397.22,384.70,92.38"><head>Table 3</head><label>3</label><figDesc>Example of a contextualized claim from the development test data.</figDesc><table coords="7,121.58,428.83,296.04,42.83"><row><cell>Original Input Claim</cell><cell>Contextualized Input Claim</cell></row><row><cell>He wouldn't send anything else.</cell><cell></cell></row></table><note coords="7,303.62,444.77,170.07,8.96;7,303.62,456.73,170.07,8.96;7,303.62,468.68,170.08,8.96;7,303.62,480.64,102.08,8.96"><p>donald trump said "And Obama would send pillows and sheets." donald trump said "He wouldn't send anything else. " donald trump said "It's the whole thing. "</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,88.99,90.49,416.99,401.38"><head>Table 4</head><label>4</label><figDesc>Subtask 2A: Impact of balancing using KNN.</figDesc><table coords="9,135.11,121.67,325.07,320.30"><row><cell>Positives</cell><cell></cell><cell>Selected Features</cell><cell cols="2">Classified Positives MAP@5</cell></row><row><cell>0.62 %</cell><cell cols="2">Semantic Similarity: SBERT, InferSent, SimCSE Lexical Similarity: WordCount, WordTokRatio</cell><cell>138</cell><cell>0.8865</cell></row><row><cell>1 %</cell><cell></cell><cell>"</cell><cell>196</cell><cell>0.8865</cell></row><row><cell>2 %</cell><cell></cell><cell>"</cell><cell>247</cell><cell>0.8760</cell></row><row><cell></cell><cell cols="2">Semantic Similarity: SBERT, InferSent, SimCSE</cell><cell></cell><cell></cell></row><row><cell>3 %</cell><cell cols="2">Lexical Similarity: WordCount, WordRatio, Word-</cell><cell>287</cell><cell>0.8664</cell></row><row><cell>4 %</cell><cell>TokRatio</cell><cell>"</cell><cell>340</cell><cell>0.8712</cell></row><row><cell>5 %</cell><cell></cell><cell>"</cell><cell>373</cell><cell>0.8784</cell></row><row><cell>6 %</cell><cell cols="2">Semantic Similarity: SBERT, InferSent, SimCSE Lexical Similarity: WordRatio, WordTokRatio</cell><cell>402</cell><cell>0.8656</cell></row><row><cell>7 %</cell><cell></cell><cell>"</cell><cell>433</cell><cell>0.8652</cell></row><row><cell>8 %</cell><cell></cell><cell>"</cell><cell>468</cell><cell>0.8628</cell></row><row><cell>9 %</cell><cell></cell><cell>"</cell><cell>486</cell><cell>0.8628</cell></row><row><cell>10 %</cell><cell></cell><cell>"</cell><cell>520</cell><cell>0.8700</cell></row><row><cell>20 %</cell><cell cols="2">Semantic Similarity: SBERT, InferSent, SimCSE Lexical Similarity: WordRatio</cell><cell>714</cell><cell>0.8776</cell></row><row><cell>30 %</cell><cell cols="2">Semantic Similarity: SBERT, InferSent, SimCSE</cell><cell>973</cell><cell>0.8836</cell></row><row><cell>40 %</cell><cell></cell><cell>"</cell><cell>1158</cell><cell>0.8884</cell></row><row><cell>50 %</cell><cell></cell><cell>"</cell><cell>1368</cell><cell>0.8896</cell></row><row><cell>60 %</cell><cell cols="2">Semantic Similarity: SBERT, InferSent, SimCSE String Similarity: LevDist</cell><cell>1532</cell><cell>0.8805</cell></row><row><cell>70 %</cell><cell></cell><cell>"</cell><cell>1660</cell><cell>0.8829</cell></row><row><cell>80 %</cell><cell></cell><cell>"</cell><cell>1852</cell><cell>0.8805</cell></row><row><cell></cell><cell cols="2">Semantic Similarity: SBERT, InferSent, SimCSE</cell><cell></cell><cell></cell></row><row><cell>90 %</cell><cell cols="2">String Similarity: LevDist</cell><cell>2111</cell><cell>0.8896</cell></row><row><cell></cell><cell cols="2">Referential Similarity: SynCount, SynTokRatio</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Semantic Similarity: SBERT, InferSent, SimCSE</cell><cell></cell><cell></cell></row><row><cell>100 %</cell><cell cols="2">String Similarity: LevDist</cell><cell>2102</cell><cell>0.8713</cell></row><row><cell></cell><cell cols="2">Referantial Similarity: SynCount</cell><cell></cell><cell></cell></row></table><note coords="9,89.29,467.41,416.69,10.91;9,89.02,480.96,63.04,10.91"><p>included the feature TokenCount which represents the sum of tokens of both input claim and verified claim.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,88.99,90.49,370.14,534.81"><head>Table 5</head><label>5</label><figDesc>Subtask 2B: Impact of balancing using Logistic Regression.</figDesc><table coords="10,136.14,121.67,323.00,467.27"><row><cell>Positives</cell><cell></cell><cell>Selected Features</cell><cell cols="2">Classifed Positives MAP@5</cell></row><row><cell></cell><cell cols="2">Semantic Similarity: SimCSE</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">String Similarity: LevDist</cell><cell></cell><cell></cell></row><row><cell>0.65 %</cell><cell cols="2">Lexical Similarity: WordCount, WordRatio, Word-</cell><cell>11</cell><cell>0.4721</cell></row><row><cell></cell><cell>TokRatio</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>TokenCount</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 %</cell><cell></cell><cell>"</cell><cell>15</cell><cell>0.4669</cell></row><row><cell></cell><cell cols="2">Semantic Similarity: SimCSE</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">String Similarity: LevDist</cell><cell></cell><cell></cell></row><row><cell>2 %</cell><cell cols="2">Lexical Similarity: WordCount, WordRatio, Word-TokRatio</cell><cell>22</cell><cell>0.4669</cell></row><row><cell></cell><cell cols="2">Referential Similarity: SynTokRatio</cell><cell></cell><cell></cell></row><row><cell></cell><cell>TokenCount</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Semantic Similarity: SimCSE</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">String Similarity: LevDist</cell><cell></cell><cell></cell></row><row><cell>3 %</cell><cell cols="2">Lexical Similarity: WordCount, WordRatio, Word-</cell><cell>26</cell><cell>0.4503</cell></row><row><cell></cell><cell>TokRatio</cell><cell></cell><cell></cell><cell></cell></row><row><cell>4 %</cell><cell>TokenCount</cell><cell>"</cell><cell>34</cell><cell>0.4579</cell></row><row><cell>5 %</cell><cell></cell><cell>"</cell><cell>43</cell><cell>0.4464</cell></row><row><cell></cell><cell cols="2">Semantic Similarity: SimCSE</cell><cell></cell><cell></cell></row><row><cell>6 %</cell><cell cols="2">String Similarity: LevDist Lexical Similarity: WordRatio, WordTokRatio</cell><cell>61</cell><cell>0.4531</cell></row><row><cell>7 %</cell><cell>TokenCount</cell><cell>"</cell><cell>68</cell><cell>0.4531</cell></row><row><cell>8 %</cell><cell></cell><cell>"</cell><cell>82</cell><cell>0.4608</cell></row><row><cell>9 %</cell><cell></cell><cell>"</cell><cell>92</cell><cell>0.4608</cell></row><row><cell>10 %</cell><cell></cell><cell>"</cell><cell>106</cell><cell>0.4454</cell></row><row><cell></cell><cell cols="2">Semantic Similarity: SimCSE</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">String Similarity: LevDist</cell><cell></cell><cell></cell></row><row><cell>20 %</cell><cell cols="2">Lexical Similarity: WordRatio, WordTokRatio</cell><cell>258</cell><cell>0.4569</cell></row><row><cell></cell><cell cols="2">Referential Similarity: SynRatio</cell><cell></cell><cell></cell></row><row><cell></cell><cell>TokenCount</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Semantic Similarity: SimCSE</cell><cell></cell><cell></cell></row><row><cell>30 %</cell><cell cols="2">Lexcial Similarity: WordRatio, WordTokRatio</cell><cell>453</cell><cell>0.4436</cell></row><row><cell></cell><cell cols="2">Referential Similarity: SynTokRatio</cell><cell></cell><cell></cell></row><row><cell>40 %</cell><cell></cell><cell>"</cell><cell>637</cell><cell>0.4359</cell></row><row><cell>50 %</cell><cell></cell><cell>"</cell><cell>809</cell><cell>0.4332</cell></row><row><cell>60 %</cell><cell></cell><cell>"</cell><cell>981</cell><cell>0.4324</cell></row><row><cell>70 %</cell><cell></cell><cell>"</cell><cell>1171</cell><cell>0.4436</cell></row><row><cell></cell><cell cols="2">Semantic Similarity: SimCSE</cell><cell></cell><cell></cell></row><row><cell>80 %</cell><cell cols="2">Lexical Similarity: WordCount, WordTokRatio</cell><cell>1265</cell><cell>0.4436</cell></row><row><cell></cell><cell cols="2">Referential Similarity: SynCount, SynTokRatio</cell><cell></cell><cell></cell></row><row><cell>90 %</cell><cell></cell><cell>"</cell><cell>1393</cell><cell>0.4551</cell></row><row><cell></cell><cell cols="2">Semantic Similarity: SimCSE</cell><cell></cell><cell></cell></row><row><cell>100 %</cell><cell cols="2">Lexcial Similarity: WordRatio, WordTokRatio</cell><cell>1547</cell><cell>0.4551</cell></row><row><cell></cell><cell cols="2">Referential Similarity: SynCount, SynTokRatio</cell><cell></cell><cell></cell></row></table><note coords="10,89.29,614.39,321.85,10.91"><p>differs from its correct rank (i.e. the first rank for subtask A) on average.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,88.81,90.49,418.27,243.46"><head>Table 6</head><label>6</label><figDesc>Subtask 2A: Impact of Classifier without balancing using features SBERT, InferSent, SimCSE, WordCount, WordTokRatio and with balancing to 50% positives using features SBERT, InferSent, SimCSE.</figDesc><table coords="11,88.81,134.06,418.27,199.89"><row><cell>Classifier</cell><cell></cell><cell cols="2">MAP@5 No Balancing MAP@5 with Balancing</cell></row><row><cell>KNN</cell><cell></cell><cell>0.8865</cell><cell>0.8896</cell></row><row><cell cols="2">Logistic Regression</cell><cell>0.8832</cell><cell>0.8844</cell></row><row><cell>Linear SVC</cell><cell></cell><cell>0.8792</cell><cell>0.8820</cell></row><row><cell>Decision Tree</cell><cell></cell><cell>0.8502</cell><cell>0.8478</cell></row><row><cell>Table 7</cell><cell></cell><cell></cell></row><row><cell cols="4">Subtask 2B: Impact of classifier with balancing to 50% Positives using features SimCSE, SynTokRatio,</cell></row><row><cell cols="4">WordRatio, WordTokRatio and to 8% Positives using features SimCSE, LevDist, WordRatio, WordTokRatio,</cell></row><row><cell>TokenCount.</cell><cell></cell><cell></cell></row><row><cell>Classifier</cell><cell cols="3">MAP@5 with 50% Positives MAP@5 with 8% Positives</cell></row><row><cell>KNN</cell><cell></cell><cell>0.4328</cell><cell>0.4179</cell></row><row><cell>Logistic Regression</cell><cell></cell><cell>0.4332</cell><cell>0.4608</cell></row><row><cell>Linear SVC</cell><cell></cell><cell>0.4340</cell><cell>0.4485</cell></row><row><cell>Decision Tree</cell><cell></cell><cell>0.4136</cell><cell>0.3538</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="11,88.99,477.98,318.65,151.34"><head>Table 8</head><label>8</label><figDesc>Subtask 2A: Results.</figDesc><table coords="11,187.64,507.38,220.00,121.94"><row><cell>User/Team</cell><cell cols="3">MAP@5 P@5 RR</cell></row><row><cell>mshlis</cell><cell>0.956</cell><cell cols="2">0.322 0.957</cell></row><row><cell>watheq9</cell><cell>0.921</cell><cell cols="2">0.189 0.923</cell></row><row><cell>Viktor</cell><cell>0.922</cell><cell cols="2">0.190 0.922</cell></row><row><cell>Team_SimBa</cell><cell>0.907</cell><cell cols="2">0.190 0.907</cell></row><row><cell>motlogelwan</cell><cell>0.873</cell><cell cols="2">0.187 0.878</cell></row><row><cell>fraunhofersit_checkthat22</cell><cell>0.610</cell><cell cols="2">0.141 0.624</cell></row><row><cell>Team_Vax_Misinfo</cell><cell>0.020</cell><cell cols="2">0.011 0.096</cell></row><row><cell>Random Baseline</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>BM25 Baseline</cell><cell>0.8179</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="12,88.99,258.47,303.16,91.56"><head>Table 9</head><label>9</label><figDesc>Subtask 2B: Results.</figDesc><table coords="12,203.13,287.86,189.02,62.16"><row><cell>User/Team</cell><cell cols="3">MAP@5 P@5 RR</cell></row><row><cell>Team_SimBa</cell><cell>0.459</cell><cell cols="2">0.126 0.475</cell></row><row><cell>Team_Vax_Misinfo</cell><cell>0.091</cell><cell cols="2">0.040 0.131</cell></row><row><cell>Random Baseline</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>BM25 Baseline</cell><cell>0.3207</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="12,88.99,539.93,279.43,69.87"><head>Table 10</head><label>10</label><figDesc>Subtask 2A: Impact of pre-processing.</figDesc><table coords="12,226.86,571.55,141.56,38.25"><row><cell></cell><cell>MAP@5</cell></row><row><cell>with Pre-Processing</cell><cell>0.9143</cell></row><row><cell>without Pre-Processing</cell><cell>0.9270</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="13,88.99,111.96,350.50,303.16"><head>. Evaluation of Features 7.1.1. Powerful Features for Subtask A and Subtask B</head><label></label><figDesc></figDesc><table coords="13,88.99,165.92,350.50,249.20"><row><cell>Table 11</cell><cell></cell><cell></cell></row><row><cell cols="3">Supervised Approach: Comparison of performance of similarity scores independently.</cell></row><row><cell>Similarity Scores</cell><cell cols="2">MAP@5 Subtask 2A MAP@5 Subtask 2B</cell></row><row><cell>CosSim SBERT</cell><cell>0.8711</cell><cell>0.3664</cell></row><row><cell>CosSim InferSent</cell><cell>0.4208</cell><cell>0.1846</cell></row><row><cell>CosSim UniversalSE</cell><cell>0.7153</cell><cell>0.3872</cell></row><row><cell>CosSim SimCSE</cell><cell>0.7973</cell><cell>0.3946</cell></row><row><cell>LevDist</cell><cell>0.1271</cell><cell>0.1833</cell></row><row><cell>JaccChar</cell><cell>0.0522</cell><cell>0.0569</cell></row><row><cell>JaccTok</cell><cell>0.4014</cell><cell>0.2763</cell></row><row><cell>SeqMat</cell><cell>0.2698</cell><cell>0.2790</cell></row><row><cell>WordCount</cell><cell>0.5667</cell><cell>0.2731</cell></row><row><cell>WordRatio</cell><cell>0.6454</cell><cell>0.2967</cell></row><row><cell>WordTokRatio</cell><cell>0.6630</cell><cell>0.2954</cell></row><row><cell>SynCount</cell><cell>0.3228</cell><cell>0.2024</cell></row><row><cell>SynRatio</cell><cell>0.3196</cell><cell>0.2508</cell></row><row><cell>SynTokRatio</cell><cell>0.3071</cell><cell>0.2359</cell></row><row><cell>NE</cell><cell>0.4549</cell><cell>0.1600</cell></row><row><cell>NERatio</cell><cell>0.4357</cell><cell>0.1556</cell></row><row><cell>NETokRatio</cell><cell>0.4620</cell><cell>0.1654</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="14,88.99,129.52,418.09,177.85"><head>Table 12</head><label>12</label><figDesc>Comparison of input-ver-claim-pairs of subtask A and subtask B. The claims of subtask B share a long, continuous string.</figDesc><table coords="14,147.42,173.10,300.44,134.28"><row><cell></cell><cell>Input Claim</cell><cell>Verified Claim</cell></row><row><cell></cell><cell></cell><cell>Time magazine compared</cell></row><row><cell>Subtask 2A</cell><cell>TIME's new cover: How Putin shattered Europe's dreams</cell><cell>Russian President Vladimir Putin to Adolf Hitler on the cover of the March 14 / March</cell></row><row><cell></cell><cell></cell><cell>21, 2022, issue.</cell></row><row><cell></cell><cell>160 million people like</cell><cell></cell></row><row><cell>Substask 2B</cell><cell>their private insurance, and if they don't like it, they can buy into a Medicare-like</cell><cell>160 million people like their private insurance.</cell></row><row><cell></cell><cell>proposal..</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="15,88.99,90.49,418.65,549.18"><head>Table 13</head><label>13</label><figDesc>Subtask 2A: Comparison of the combination of different similarity scores in a supervised and unsupervised way.</figDesc><table coords="15,106.67,133.62,381.94,506.05"><row><cell cols="2">Kinds of Similarity Scores</cell><cell cols="2">Similarity Scores</cell><cell>MAP@5 UnSup</cell><cell>MAP@5 KNN 50% positives KNN no balancing MAP@5</cell></row><row><cell cols="2">Semantic Similarity</cell><cell cols="2">SBERT, InferSent, Univer-salSE, SimCSE SBERT, UniversalSE, Sim-CSE SBERT, InferSent, SimCSE</cell><cell>0.8883 0.8972 0.8793</cell><cell>0.8734 0.8748 0.8896</cell><cell>0.8672 0.8829 0.8664</cell></row><row><cell></cell><cell></cell><cell cols="2">SBERT, SimCSE</cell><cell>0.8896</cell><cell>0.8781</cell><cell>0.8748</cell></row><row><cell cols="2">Semantic Similarity and Lexical Similarity</cell><cell cols="2">SBERT, SimCSE, Word-Count SBERT, SimCSE, Word-TokRatio SBERT, SimCSE, Word-Count, WordTokRatio SBERT, InferSent, SimCSE,</cell><cell>0.9075 0.9151 0.9175</cell><cell>0.8839 0.8877 0.8955</cell><cell>0.8792 0.8792 0.8801</cell></row><row><cell></cell><cell></cell><cell cols="2">WordCount, WordTokRa-</cell><cell>0.8911</cell><cell>0.8832</cell><cell>0.8865</cell></row><row><cell></cell><cell></cell><cell cols="2">tio SBERT, InferSent, SimCSE, WordCount</cell><cell>0.8941</cell><cell>0.8817</cell><cell>0.8780</cell></row><row><cell cols="2">Semantic Lexical Similarity and Similarity, Referential Similarity</cell><cell cols="2">SBERT, SIMCSE, Word-TokRatio, NETokRatio</cell><cell>0.9172</cell><cell>0.8863</cell><cell>0.8825</cell></row><row><cell cols="2">Semantic String Similarity and Similarity, Lexical Similarity</cell><cell cols="2">SimCSE, LevDist, all Word-Sims</cell><cell>0.8027</cell><cell>0.8521</cell><cell>0.8742</cell></row><row><cell></cell><cell></cell><cell cols="2">SimCSE, LevDist, WordRa-tio, WordTokRatio</cell><cell>0.7986</cell><cell>0.8305</cell><cell>0.8670</cell></row><row><cell>Semantic</cell><cell>Similarity,</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">String Similarity, Lex-ical Similarity and</cell><cell cols="2">SBERT, WordCount, Jacc-Tok, NETokRatio</cell><cell>0.8929</cell><cell>0.8748</cell><cell>0.8744</cell></row><row><cell cols="2">Referential Similarity</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">SBERT, WordTokRatio, Jac-cTok, NETokRatio SimCSE, JaccTok, all Word-Sims, all SynSims</cell><cell>0.9001 0.5509</cell><cell>0.8720 0.8473</cell><cell>0.8844 0.8650</cell></row><row><cell></cell><cell></cell><cell cols="2">SimCSE, SeqMat, JaccTok, all WordSims, all SynSims</cell><cell>0.5490</cell><cell>0.8417</cell><cell>0.8518</cell></row><row><cell></cell><cell></cell><cell cols="2">SimCSE, SeqMat, JaccTok,</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">all WordSims, SynRatio,</cell><cell>0.6540</cell><cell>0.8323</cell><cell>0.8602</cell></row><row><cell></cell><cell></cell><cell>SynTokRatio</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">SimCSE, LevDist, all Word-Sims, SynTokRatio</cell><cell>0.7448</cell><cell>0.8628</cell><cell>0.8550</cell></row><row><cell></cell><cell></cell><cell cols="2">SimCSE, LevDist, WordRa-</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">tio, WordTokRatio, SynRa-</cell><cell>0.7425</cell><cell>0.8501</cell><cell>0.8778</cell></row><row><cell></cell><cell></cell><cell cols="2">tio SimCSE, WordRatio, WordTokRatio SynTokRatio, SimCSE, WordCount,</cell><cell>0.7030</cell><cell>0.8573</cell><cell>0.8610</cell></row><row><cell></cell><cell></cell><cell cols="2">WordTokRatio, SynCount,</cell><cell>0.5850</cell><cell>0.8537</cell><cell>0.8650</cell></row><row><cell></cell><cell></cell><cell>SynTokRatio</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">SimCSE, SynCount, Syn-</cell><cell></cell></row><row><cell></cell><cell></cell><cell>TokRatio,</cell><cell>WordRatio,</cell><cell>0.5842</cell><cell>0.8585</cell><cell>0.8586</cell></row><row><cell></cell><cell></cell><cell>WordTokRatio</cell><cell></cell><cell></cell></row><row><cell>ALL</cell><cell></cell><cell cols="2">ALL Except JaccChar, NERatio</cell><cell>0.6323</cell><cell>0.8521</cell><cell>0.8754</cell></row><row><cell></cell><cell></cell><cell cols="2">ALL Except JaccChar</cell><cell>0.6540</cell><cell>0.8620</cell><cell>0.8754</cell></row><row><cell></cell><cell></cell><cell>ALL</cell><cell></cell><cell>0.6376</cell><cell>0.8642</cell><cell>0.8793</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="16,88.99,90.49,418.65,554.91"><head>Table 14</head><label>14</label><figDesc>Subtask 2B: Comparison of the combination of different similarity scores in a supervised and unsupervised way.</figDesc><table coords="16,124.46,133.62,346.35,511.77"><row><cell cols="2">Kinds of Similarity Scores</cell><cell cols="2">Similarity Scores</cell><cell>MAP@5 UnSup</cell><cell>MAP@5 LogReg 8% positives LinearSVC MAP@5</cell></row><row><cell cols="2">Semantic Similarity</cell><cell cols="2">SBERT, InferSent, Univer-salSE, SimCSE SBERT, UniversalSE, Sim-CSE SBERT, InferSent, SimCSE</cell><cell>0.4721 0.4672 0.4310</cell><cell>0.4190 0.4190 0.4190</cell><cell>0.4454 0.4454 0.4454</cell></row><row><cell></cell><cell></cell><cell cols="2">SBERT, SimCSE</cell><cell>0.4190</cell><cell>0.4344</cell><cell>0.4454</cell></row><row><cell cols="2">Semantic Similarity and Lexical Similarity</cell><cell cols="2">SBERT, SimCSE, Word-Count SBERT, SimCSE, Word-TokRatio SBERT, SimCSE, Word-Count, WordTokRatio SBERT, InferSent, SimCSE,</cell><cell>0.4395 0.4654 0.4718</cell><cell>0.4554 0.4479 0.4479</cell><cell>0.4531 0.4537 0.4332</cell></row><row><cell></cell><cell></cell><cell cols="2">WordCount, WordTokRa-</cell><cell>0.4654</cell><cell>0.4479</cell><cell>0.4332</cell></row><row><cell></cell><cell></cell><cell cols="2">tio SBERT, InferSent, SimCSE, WordCount</cell><cell>0.4583</cell><cell>0.4554</cell><cell>0.4562</cell></row><row><cell cols="2">Semantic Lexical Similarity and Similarity, Referential Similarity</cell><cell cols="2">SBERT, SIMCSE, Word-TokRatio, NETokRatio</cell><cell>0.4190</cell><cell>0.4479</cell><cell>0.4691</cell></row><row><cell>Semantic</cell><cell>Similarity,</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">String Similarity, Lex-ical Similarity and</cell><cell cols="2">SBERT, WordCount, Jacc-Tok, NETokRatio</cell><cell>0.4595</cell><cell>0.4056</cell><cell>0.4590</cell></row><row><cell cols="2">Referential Similarity</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">SBERT, WordTokRatio, Jac-cTok, NETokRatio SimCSE, JaccTok, all Word-Sims, all SynSims</cell><cell>0.4415 0.3205</cell><cell>0.4338 0.4646</cell><cell>0.4295 0.4308</cell></row><row><cell></cell><cell></cell><cell cols="2">SimCSE, SeqMat, JaccTok, all WordSims, all SynSims</cell><cell>0.3195</cell><cell>0.4646</cell><cell>0.4308</cell></row><row><cell></cell><cell></cell><cell cols="2">SimCSE, SeqMat, JaccTok,</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">all WordSims, SynRatio,</cell><cell>0.3367</cell><cell>0.4646</cell><cell>0.4308</cell></row><row><cell></cell><cell></cell><cell>SynTokRatio</cell><cell></cell><cell></cell></row><row><cell cols="2">Semantic String Similarity and Similarity, Lexical Similarity</cell><cell cols="2">SimCSE, LevDist, all Word-Sims</cell><cell>0.3731</cell><cell>0.4608</cell><cell>0.4428</cell></row><row><cell></cell><cell></cell><cell cols="2">SimCSE, LevDist, all Word-Sims, SynTokRatio</cell><cell>0.3477</cell><cell>0.4646</cell><cell>0.4308</cell></row><row><cell></cell><cell></cell><cell cols="2">SimCSE, LevDist, WordRa-tio, WordTokRatio SimCSE, LevDist, WordRa-</cell><cell>0.3641</cell><cell>0.4608</cell><cell>0.4569</cell></row><row><cell></cell><cell></cell><cell cols="2">tio, WordTokRatio, SynRa-</cell><cell>0.3542</cell><cell>0.4646</cell><cell>0.4340</cell></row><row><cell></cell><cell></cell><cell cols="2">tio SimCSE, WordRatio, WordTokRatio SynTokRatio, SimCSE, WordCount,</cell><cell>0.3355</cell><cell>0.4646</cell><cell>0.4340</cell></row><row><cell></cell><cell></cell><cell cols="2">WordTokRatio, SynCount,</cell><cell>0.3118</cell><cell>0.4531</cell><cell>0.4269</cell></row><row><cell></cell><cell></cell><cell>SynTokRatio</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">SimCSE, SynCount, Syn-</cell><cell></cell></row><row><cell></cell><cell></cell><cell>TokRatio,</cell><cell>WordRatio,</cell><cell>0.3301</cell><cell>0.4646</cell><cell>0.4385</cell></row><row><cell></cell><cell></cell><cell>WordTokRatio</cell><cell></cell><cell></cell></row><row><cell>ALL</cell><cell></cell><cell cols="2">ALL Except JaccChar, NER-atio ALL Except JaccChar</cell><cell>0.3301 0.3147</cell><cell>0.4869 0.4882</cell><cell>0.4436 0.4436</cell></row><row><cell></cell><cell></cell><cell>ALL</cell><cell></cell><cell>0.3147</cell><cell>0.4749</cell><cell>0.4513</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="20,88.99,90.49,465.15,647.76"><head>Table 15</head><label>15</label><figDesc>Subtask 2A: Spearman correlation between different similiarity scores for the test data.</figDesc><table coords="20,95.27,122.06,458.87,616.19"><row><cell>SimScore</cell><cell>SBERT</cell><cell>InferSent</cell><cell>UniversalSE</cell><cell>SimCSE</cell><cell>LevDist</cell><cell>JaccChar</cell><cell cols="2">JaccTok SeqMat</cell></row><row><cell>SBERT</cell><cell>1.0</cell><cell>-0.1</cell><cell>0.43</cell><cell>0.51</cell><cell>0.06</cell><cell>0.0</cell><cell>0.02</cell><cell>0.05</cell></row><row><cell>InferSent</cell><cell>-0.1</cell><cell>1.0</cell><cell>0.09</cell><cell>-0.09</cell><cell>-0.54</cell><cell>0.38</cell><cell>0.44</cell><cell>-0.36</cell></row><row><cell>UniversalSE</cell><cell>0.43</cell><cell>0.09</cell><cell>1.0</cell><cell>0.36</cell><cell>0.08</cell><cell>0.05</cell><cell>0.15</cell><cell>0.08</cell></row><row><cell>SimCSE</cell><cell>0.51</cell><cell>-0.09</cell><cell>0.36</cell><cell>1.0</cell><cell>0.05</cell><cell>0.01</cell><cell>0.02</cell><cell>0.03</cell></row><row><cell>LevDist</cell><cell>0.06</cell><cell>-0.54</cell><cell>0.08</cell><cell>0.05</cell><cell>1.0</cell><cell>-0.28</cell><cell>-0.23</cell><cell>0.74</cell></row><row><cell>JaccChar</cell><cell>0.0</cell><cell>0.38</cell><cell>0.05</cell><cell>0.01</cell><cell>-0.28</cell><cell>1.0</cell><cell>0.37</cell><cell>-0.14</cell></row><row><cell>JaccTok</cell><cell>0.02</cell><cell>0.44</cell><cell>0.15</cell><cell>0.02</cell><cell>-0.23</cell><cell>0.37</cell><cell>1.0</cell><cell>-0.06</cell></row><row><cell>SeqMat</cell><cell>0.05</cell><cell>-0.36</cell><cell>0.08</cell><cell>0.03</cell><cell>0.74</cell><cell>-0.14</cell><cell>-0.06</cell><cell>1.0</cell></row><row><cell>WordCount</cell><cell>0.1</cell><cell>0.51</cell><cell>0.19</cell><cell>0.07</cell><cell>-0.43</cell><cell>0.37</cell><cell>0.67</cell><cell>-0.28</cell></row><row><cell>WordRatio</cell><cell>0.17</cell><cell>0.24</cell><cell>0.24</cell><cell>0.13</cell><cell>-0.12</cell><cell>0.29</cell><cell>0.63</cell><cell>-0.04</cell></row><row><cell cols="2">WordTokRatio 0.19</cell><cell>0.2</cell><cell>0.26</cell><cell>0.15</cell><cell>-0.05</cell><cell>0.25</cell><cell>0.59</cell><cell>0.02</cell></row><row><cell>SynCount</cell><cell>0.07</cell><cell>0.5</cell><cell>0.16</cell><cell>0.05</cell><cell>-0.42</cell><cell>0.31</cell><cell>0.48</cell><cell>-0.27</cell></row><row><cell>SynRatio</cell><cell>0.14</cell><cell>0.28</cell><cell>0.23</cell><cell>0.12</cell><cell>-0.14</cell><cell>0.22</cell><cell>0.45</cell><cell>-0.05</cell></row><row><cell>SynTokRatio</cell><cell>0.14</cell><cell>0.26</cell><cell>0.23</cell><cell>0.12</cell><cell>-0.11</cell><cell>0.21</cell><cell>0.42</cell><cell>-0.04</cell></row><row><cell>NE</cell><cell>0.28</cell><cell>0.11</cell><cell>0.38</cell><cell>0.24</cell><cell>-0.0</cell><cell>0.09</cell><cell>0.23</cell><cell>0.02</cell></row><row><cell>NERatio</cell><cell>0.28</cell><cell>0.1</cell><cell>0.38</cell><cell>0.24</cell><cell>0.0</cell><cell>0.09</cell><cell>0.23</cell><cell>0.02</cell></row><row><cell>NeTokRatio</cell><cell>0.28</cell><cell>0.1</cell><cell>0.38</cell><cell>0.24</cell><cell>0.01</cell><cell>0.08</cell><cell>0.23</cell><cell>0.03</cell></row><row><cell>SimScore</cell><cell cols="6">WordCount WordRatio WordTokRatio SynCount SynRatio SynTokRatio</cell><cell></cell><cell></cell></row><row><cell>SBERT</cell><cell>0.1</cell><cell>0.17</cell><cell>0.19</cell><cell>0.07</cell><cell>0.14</cell><cell>0.14</cell><cell></cell><cell></cell></row><row><cell>InferSent</cell><cell>0.51</cell><cell>0.24</cell><cell>0.2</cell><cell>0.5</cell><cell>0.28</cell><cell>0.26</cell><cell></cell><cell></cell></row><row><cell>UniversalSE</cell><cell>0.19</cell><cell>0.24</cell><cell>0.26</cell><cell>0.16</cell><cell>0.23</cell><cell>0.23</cell><cell></cell><cell></cell></row><row><cell>SimCSE</cell><cell>0.07</cell><cell>0.13</cell><cell>0.15</cell><cell>0.05</cell><cell>0.12</cell><cell>0.12</cell><cell></cell><cell></cell></row><row><cell>LevDist</cell><cell>-0.43</cell><cell>-0.12</cell><cell>-0.05</cell><cell>-0.42</cell><cell>-0.14</cell><cell>-0.11</cell><cell></cell><cell></cell></row><row><cell>JaccChar</cell><cell>0.37</cell><cell>0.29</cell><cell>0.25</cell><cell>0.31</cell><cell>0.22</cell><cell>0.21</cell><cell></cell><cell></cell></row><row><cell>JaccTok</cell><cell>0.67</cell><cell>0.63</cell><cell>0.59</cell><cell>0.48</cell><cell>0.45</cell><cell>0.42</cell><cell></cell><cell></cell></row><row><cell>SeqMat</cell><cell>-0.28</cell><cell>-0.04</cell><cell>0.02</cell><cell>-0.27</cell><cell>-0.05</cell><cell>-0.04</cell><cell></cell><cell></cell></row><row><cell>WordCount</cell><cell>1.0</cell><cell>0.9</cell><cell>0.86</cell><cell>0.72</cell><cell>0.64</cell><cell>0.62</cell><cell></cell><cell></cell></row><row><cell>WordRatio</cell><cell>0.9</cell><cell>1.0</cell><cell>0.98</cell><cell>0.6</cell><cell>0.67</cell><cell>0.64</cell><cell></cell><cell></cell></row><row><cell cols="2">WordTokRatio 0.86</cell><cell>0.98</cell><cell>1.0</cell><cell>0.57</cell><cell>0.66</cell><cell>0.64</cell><cell></cell><cell></cell></row><row><cell>SynCount</cell><cell>0.72</cell><cell>0.6</cell><cell>0.57</cell><cell>1.0</cell><cell>0.91</cell><cell>0.92</cell><cell></cell><cell></cell></row><row><cell>SynRatio</cell><cell>0.64</cell><cell>0.67</cell><cell>0.66</cell><cell>0.91</cell><cell>1.0</cell><cell>0.97</cell><cell></cell><cell></cell></row><row><cell>SynTokRatio</cell><cell>0.62</cell><cell>0.64</cell><cell>0.64</cell><cell>0.92</cell><cell>0.97</cell><cell>1.0</cell><cell></cell><cell></cell></row><row><cell>NE</cell><cell>0.32</cell><cell>0.36</cell><cell>0.36</cell><cell>0.2</cell><cell>0.24</cell><cell>0.23</cell><cell></cell><cell></cell></row><row><cell>NERatio</cell><cell>0.31</cell><cell>0.36</cell><cell>0.36</cell><cell>0.19</cell><cell>0.24</cell><cell>0.23</cell><cell></cell><cell></cell></row><row><cell>NeTokRatio</cell><cell>0.31</cell><cell>0.36</cell><cell>0.36</cell><cell>0.19</cell><cell>0.24</cell><cell>0.23</cell><cell></cell><cell></cell></row><row><cell>SimScore</cell><cell>NE</cell><cell>NERatio</cell><cell>NETokRatio</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SBERT</cell><cell>0.28</cell><cell>0.28</cell><cell>0.28</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>InferSent</cell><cell>0.11</cell><cell>0.1</cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UniversalSE</cell><cell>0.38</cell><cell>0.38</cell><cell>0.38</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SimCSE</cell><cell>0.24</cell><cell>0.24</cell><cell>0.24</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LevDist</cell><cell>-0.0</cell><cell>0.0</cell><cell>0.01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>JaccChar</cell><cell>0.09</cell><cell>0.09</cell><cell>0.08</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>JaccTok</cell><cell>0.23</cell><cell>0.23</cell><cell>0.23</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SeqMat</cell><cell>0.02</cell><cell>0.02</cell><cell>0.03</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>WordCount</cell><cell>0.32</cell><cell>0.31</cell><cell>0.31</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>WordRatio</cell><cell>0.36</cell><cell>0.36</cell><cell>0.36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">WordTokRatio 0.36</cell><cell>0.36</cell><cell>0.36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SynCount</cell><cell>0.2</cell><cell>0.19</cell><cell>0.19</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SynRatio</cell><cell>0.24</cell><cell>0.24</cell><cell>0.24</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SynTokRatio</cell><cell>0.23</cell><cell>0.23</cell><cell>0.23</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NE</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NERatio</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NeTokRatio</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="21,88.99,90.49,465.15,647.76"><head>Table 16</head><label>16</label><figDesc>Subtask 2B: Spearman correlation between different similiarity scores for the test data.</figDesc><table coords="21,95.27,122.06,458.87,616.19"><row><cell>SimScore</cell><cell>SBERT</cell><cell>InferSent</cell><cell>UniversalSE</cell><cell>SimCSE</cell><cell>LevDist</cell><cell>JaccChar</cell><cell cols="2">JaccTok SeqMat</cell></row><row><cell>SBERT</cell><cell>1.0</cell><cell>0.26</cell><cell>0.59</cell><cell>0.61</cell><cell>-0.06</cell><cell>0.21</cell><cell>0.04</cell><cell>0.04</cell></row><row><cell>InferSent</cell><cell>0.26</cell><cell>1.0</cell><cell>0.39</cell><cell>0.15</cell><cell>-0.59</cell><cell>0.47</cell><cell>0.38</cell><cell>-0.14</cell></row><row><cell>UniversalSE</cell><cell>0.59</cell><cell>0.39</cell><cell>1.0</cell><cell>0.48</cell><cell>-0.09</cell><cell>0.28</cell><cell>0.25</cell><cell>0.09</cell></row><row><cell>SimCSE</cell><cell>0.61</cell><cell>0.15</cell><cell>0.48</cell><cell>1.0</cell><cell>-0.01</cell><cell>0.19</cell><cell>0.1</cell><cell>0.05</cell></row><row><cell>LevDist</cell><cell>-0.06</cell><cell>-0.59</cell><cell>-0.09</cell><cell>-0.01</cell><cell>1.0</cell><cell>-0.23</cell><cell>-0.03</cell><cell>0.57</cell></row><row><cell>JaccChar</cell><cell>0.21</cell><cell>0.47</cell><cell>0.28</cell><cell>0.19</cell><cell>-0.23</cell><cell>1.0</cell><cell>0.33</cell><cell>0.06</cell></row><row><cell>JaccTok</cell><cell>0.04</cell><cell>0.38</cell><cell>0.25</cell><cell>0.1</cell><cell>-0.03</cell><cell>0.33</cell><cell>1.0</cell><cell>0.21</cell></row><row><cell>SeqMat</cell><cell>0.04</cell><cell>-0.14</cell><cell>0.09</cell><cell>0.05</cell><cell>0.57</cell><cell>0.06</cell><cell>0.21</cell><cell>1.0</cell></row><row><cell>WordCount</cell><cell>0.25</cell><cell>0.6</cell><cell>0.43</cell><cell>0.2</cell><cell>-0.33</cell><cell>0.36</cell><cell>0.59</cell><cell>-0.01</cell></row><row><cell>WordRatio</cell><cell>0.11</cell><cell>0.11</cell><cell>0.28</cell><cell>0.14</cell><cell>0.23</cell><cell>0.16</cell><cell>0.59</cell><cell>0.28</cell></row><row><cell cols="2">WordTokRatio 0.16</cell><cell>0.04</cell><cell>0.29</cell><cell>0.16</cell><cell>0.33</cell><cell>0.08</cell><cell>0.5</cell><cell>0.33</cell></row><row><cell>SynCount</cell><cell>0.31</cell><cell>0.57</cell><cell>0.46</cell><cell>0.22</cell><cell>-0.37</cell><cell>0.32</cell><cell>0.41</cell><cell>-0.03</cell></row><row><cell>SynRatio</cell><cell>0.26</cell><cell>0.3</cell><cell>0.4</cell><cell>0.21</cell><cell>-0.01</cell><cell>0.18</cell><cell>0.43</cell><cell>0.16</cell></row><row><cell>SynTokRatio</cell><cell>0.27</cell><cell>0.29</cell><cell>0.42</cell><cell>0.22</cell><cell>0.0</cell><cell>0.18</cell><cell>0.4</cell><cell>0.18</cell></row><row><cell>NE</cell><cell>0.34</cell><cell>0.17</cell><cell>0.43</cell><cell>0.3</cell><cell>-0.07</cell><cell>0.18</cell><cell>0.21</cell><cell>0.05</cell></row><row><cell>NERatio</cell><cell>0.34</cell><cell>0.16</cell><cell>0.43</cell><cell>0.3</cell><cell>-0.05</cell><cell>0.17</cell><cell>0.21</cell><cell>0.06</cell></row><row><cell>NeTokRatio</cell><cell>0.34</cell><cell>0.16</cell><cell>0.43</cell><cell>0.3</cell><cell>-0.05</cell><cell>0.17</cell><cell>0.21</cell><cell>0.07</cell></row><row><cell>SimScore</cell><cell cols="6">WordCount WordRatio WordTokRatio SynCount SynRatio SynTokRatio</cell><cell></cell><cell></cell></row><row><cell>SBERT</cell><cell>0.25</cell><cell>0.11</cell><cell>0.16</cell><cell>0.31</cell><cell>0.26</cell><cell>0.27</cell><cell></cell><cell></cell></row><row><cell>InferSent</cell><cell>0.6</cell><cell>0.11</cell><cell>0.04</cell><cell>0.57</cell><cell>0.3</cell><cell>0.29</cell><cell></cell><cell></cell></row><row><cell>UniversalSE</cell><cell>0.43</cell><cell>0.28</cell><cell>0.29</cell><cell>0.46</cell><cell>0.4</cell><cell>0.42</cell><cell></cell><cell></cell></row><row><cell>SimCSE</cell><cell>0.2</cell><cell>0.14</cell><cell>0.16</cell><cell>0.22</cell><cell>0.21</cell><cell>0.22</cell><cell></cell><cell></cell></row><row><cell>LevDist</cell><cell>-0.33</cell><cell>0.23</cell><cell>0.33</cell><cell>-0.37</cell><cell>-0.01</cell><cell>0.0</cell><cell></cell><cell></cell></row><row><cell>JaccChar</cell><cell>0.36</cell><cell>0.16</cell><cell>0.08</cell><cell>0.32</cell><cell>0.18</cell><cell>0.18</cell><cell></cell><cell></cell></row><row><cell>JaccTok</cell><cell>0.59</cell><cell>0.59</cell><cell>0.5</cell><cell>0.41</cell><cell>0.43</cell><cell>0.4</cell><cell></cell><cell></cell></row><row><cell>SeqMat</cell><cell>-0.01</cell><cell>0.28</cell><cell>0.33</cell><cell>-0.03</cell><cell>0.16</cell><cell>0.18</cell><cell></cell><cell></cell></row><row><cell>WordCount</cell><cell>1.0</cell><cell>0.76</cell><cell>0.7</cell><cell>0.74</cell><cell>0.64</cell><cell>0.63</cell><cell></cell><cell></cell></row><row><cell>WordRatio</cell><cell>0.76</cell><cell>1.0</cell><cell>0.96</cell><cell>0.45</cell><cell>0.63</cell><cell>0.61</cell><cell></cell><cell></cell></row><row><cell cols="2">WordTokRatio 0.7</cell><cell>0.96</cell><cell>1.0</cell><cell>0.4</cell><cell>0.6</cell><cell>0.62</cell><cell></cell><cell></cell></row><row><cell>SynCount</cell><cell>0.74</cell><cell>0.45</cell><cell>0.4</cell><cell>1.0</cell><cell>0.87</cell><cell>0.9</cell><cell></cell><cell></cell></row><row><cell>SynRatio</cell><cell>0.64</cell><cell>0.63</cell><cell>0.6</cell><cell>0.87</cell><cell>1.0</cell><cell>0.95</cell><cell></cell><cell></cell></row><row><cell>SynTokRatio</cell><cell>0.63</cell><cell>0.61</cell><cell>0.62</cell><cell>0.9</cell><cell>0.95</cell><cell>1.0</cell><cell></cell><cell></cell></row><row><cell>NE</cell><cell>0.37</cell><cell>0.32</cell><cell>0.32</cell><cell>0.27</cell><cell>0.26</cell><cell>0.25</cell><cell></cell><cell></cell></row><row><cell>NERatio</cell><cell>0.37</cell><cell>0.33</cell><cell>0.32</cell><cell>0.26</cell><cell>0.27</cell><cell>0.25</cell><cell></cell><cell></cell></row><row><cell>NeTokRatio</cell><cell>0.37</cell><cell>0.32</cell><cell>0.32</cell><cell>0.26</cell><cell>0.2760.25</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SimScore</cell><cell>NE</cell><cell>NERatio</cell><cell>NETokRatio</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SBERT</cell><cell>0.34</cell><cell>0.34</cell><cell>0.34</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>InferSent</cell><cell>0.17</cell><cell>0.16</cell><cell>0.16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UniversalSE</cell><cell>0.43</cell><cell>0.43</cell><cell>0.43</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SimCSE</cell><cell>0.3</cell><cell>0.3</cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LevDist</cell><cell>-0.07</cell><cell>-0.05</cell><cell>-0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>JaccChar</cell><cell>0.18</cell><cell>0.17</cell><cell>0.17</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>JaccTok</cell><cell>0.21</cell><cell>0.21</cell><cell>0.21</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SeqMat</cell><cell>0.05</cell><cell>0.06</cell><cell>0.07</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>WordCount</cell><cell>0.37</cell><cell>0.37</cell><cell>0.37</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>WordRatio</cell><cell>0.32</cell><cell>0.33</cell><cell>0.32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">WordTokRatio 0.32</cell><cell>0.32</cell><cell>0.32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SynCount</cell><cell>0.27</cell><cell>0.26</cell><cell>0.26</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SynRatio</cell><cell>0.26</cell><cell>0.27</cell><cell>0.27</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SynTokRatio</cell><cell>0.25</cell><cell>0.25</cell><cell>0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NE</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NERatio</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NeTokRatio</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,670.91,383.26,8.97"><p>https://github.com/Alihoe/CLEFCheckThat2aSimBa, https://github.com/Alihoe/CLEFCheckThat2bSimBa</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,108.93,638.16,190.06,8.97"><p>https://www.sbert.net/docs/pretrained_models.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,108.93,649.12,199.56,8.97"><p>https://tfhub.dev/google/universal-sentence-encoder/4</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,108.93,660.08,231.08,8.97"><p>https://huggingface.co/princeton-nlp/sup-simcse-roberta-large</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,108.93,671.04,153.07,8.97"><p>https://github.com/princeton-nlp/SimCSE</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="6,108.93,638.16,163.15,8.97"><p>https://docs.python.org/3/library/difflib.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="6,108.93,649.12,398.66,8.97;6,89.00,660.08,416.98,8.97;6,89.29,671.04,416.69,8.97"><p>e.g.: If two claims consisted of ten tokens each and had ten tokens in common, we would obtain a Word-TokRatio of (100/20)*10*2 = 100. If they only had one token in common the obtained ratio would be (100/20)*1*2 = 10. If both claims consisted of 50 tokens each, the obtained ratios would be (100/100)*10*2 = 20 and (100/20)*1*2 = 2.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="8,108.93,627.20,397.05,8.97;8,89.29,638.16,416.70,8.97;8,89.29,649.12,416.69,8.97;8,89.07,660.08,417.90,8.97;8,89.29,671.04,140.02,8.97"><p>e.g.: Given is a SentEmb mean of 50.0. If two sentences consist of quite similar strings, one could imagine them having a LevDist of -50. If two sentences are not that similar, they could have a LevDist of -200. Applying the technique described, incorporating LevDist would result in the sim score 100 for the similar sentences and 25 for the varying sentences. This way it is not ensured that the obtained similarity score is ∈ [0, 100]. In practice, however, the calculated values are in this range.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="17,112.66,425.43,393.33,10.91;17,112.66,438.98,394.62,10.91;17,112.14,452.53,395.05,10.91;17,112.66,466.08,89.12,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="17,449.74,425.43,56.25,10.91;17,112.66,438.98,372.85,10.91">Overview of the CLEF-2022 CheckThat! lab task 2 on detecting previously fact-checked claims</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,112.14,452.53,390.83,10.91">Working Notes of CLEF 2022-Conference and Labs of the Evaluation Forum, CLEF &apos;2022</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,479.63,394.53,10.91;17,112.66,493.18,394.62,10.91;17,112.28,506.73,394.91,10.91;17,112.66,520.28,393.33,10.91;17,112.66,533.83,395.00,10.91;17,112.66,547.38,17.97,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="17,342.58,493.18,164.70,10.91;17,112.28,506.73,299.84,10.91">Overview of checkthat! 2020 english: Automatic identification and verification of claims in social media</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Suwaileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">D S</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_265.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="17,294.95,520.28,102.40,10.91">CLEF (Working Notes)</title>
		<title level="s" coord="17,480.05,521.29,25.94,9.72;17,112.66,533.83,122.95,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2696</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,560.93,394.52,10.91;17,112.33,574.48,393.65,10.91;17,112.66,588.02,393.59,10.91;17,112.66,601.57,393.33,10.91;17,112.33,615.12,277.18,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="17,213.75,574.48,292.24,10.91;17,112.66,588.02,267.07,10.91">Overview of the CLEF-2021 CheckThat! lab task 2 on detecting previously fact-checked claims in tweets and political debates</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-29.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="17,402.38,588.02,103.86,10.91;17,112.66,601.57,294.77,10.91">Working Notes of CLEF 2021-Conference and Labs of the Evaluation Forum, CLEF &apos;2021</title>
		<meeting><address><addrLine>Bucharest, Romania (online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,628.67,393.33,10.91;17,112.66,642.22,395.01,10.91;17,112.66,655.77,393.33,10.91;18,112.66,86.97,393.33,10.91;18,112.66,100.52,367.50,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="17,360.23,628.67,145.76,10.91;17,112.66,642.22,240.82,10.91">Team buster.ai at checkthat! 2020 insights and recommendations to improve fact-checking</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bouziane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Perrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cluzeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mardas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sadeq</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_134.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="17,209.87,655.77,296.12,10.91;18,112.66,86.97,26.91,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="18,432.75,87.98,73.24,9.72;18,112.66,100.52,77.32,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">0001. September 22-25, 2020. 2696. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,114.06,394.61,10.91;18,112.66,127.61,394.53,10.91;18,112.66,141.16,393.32,10.91;18,112.66,154.71,393.33,10.91;18,112.66,168.26,394.03,10.91;18,112.66,181.81,66.21,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="18,393.83,114.06,113.44,10.91;18,112.66,127.61,301.44,10.91">Ub_et at checkthat! 2020: Exploring ad hoc retrieval approaches in verified claims retrieval</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Thuma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">P</forename><surname>Motlogelwa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leburu-Dingalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mudongo</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_204.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="18,294.50,141.16,211.48,10.91;18,112.66,154.71,128.24,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="18,124.43,168.26,153.99,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">0001. September 22-25, 2020. 2696. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,195.36,394.53,10.91;18,112.33,208.91,393.65,10.91;18,112.66,222.46,393.59,10.91;18,112.66,236.01,393.53,10.91;18,112.66,249.56,394.61,10.91;18,112.31,263.11,172.79,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="18,112.33,208.91,393.65,10.91;18,112.66,222.46,27.63,10.91">The university of sheffield at checkthat! 2020: Claim identification and verification on twitter</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hampson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Leidner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Stevenson</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_162.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="18,402.00,222.46,104.24,10.91;18,112.66,236.01,237.01,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="18,237.91,249.56,151.60,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">0001. September 22-25, 2020. 2696. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,276.66,393.33,10.91;18,112.66,290.20,394.53,10.91;18,112.66,303.75,393.33,10.91;18,112.66,317.30,393.33,10.91;18,112.66,330.85,395.00,10.91;18,112.66,344.40,17.97,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="18,279.08,276.66,226.90,10.91;18,112.66,290.20,260.21,10.91">Check square at checkthat! 2020: Claim detection in social media via fusion of transformer and syntactic features</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">S</forename><surname>Cheema</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hakimov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ewerth</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_216.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="18,240.37,303.75,265.62,10.91;18,112.66,317.30,78.85,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="18,480.05,318.32,25.94,9.72;18,112.66,330.85,122.95,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">0001. September 22-25, 2020. 2696. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,357.95,394.53,10.91;18,112.30,371.50,394.97,10.91;18,112.31,385.05,288.85,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="18,170.33,371.50,252.97,10.91">Roberta: A robustly optimized bert pretraining approach</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1907.11692</idno>
		<ptr target="https://arxiv.org/abs/1907.11692.doi:10.48550/ARXIV.1907.11692" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,398.60,394.53,10.91;18,112.66,412.15,122.77,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m" coord="18,219.42,398.60,283.17,10.91">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,425.70,395.17,10.91;18,112.66,439.25,395.17,10.91;18,112.66,452.79,393.33,10.91;18,112.66,466.34,395.17,10.91;18,112.66,479.89,394.61,10.91;18,112.66,493.44,193.79,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="18,351.45,425.70,156.38,10.91;18,112.66,439.25,395.17,10.91;18,112.66,452.79,33.15,10.91">Unipi-nle at checkthat! 2020: Approaching fact checking from a sentence similarity perspective through the lens of transformers</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">C</forename><surname>Passaro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondielli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Marcelloni</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_169.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="18,424.56,452.79,81.43,10.91;18,112.66,466.34,254.88,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="18,257.53,479.89,154.19,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">0001. September 22-25, 2020. 2696. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,506.99,394.53,10.91;18,112.66,520.54,393.32,10.91;18,112.66,534.09,393.33,10.91;18,112.66,547.64,394.03,10.91;18,112.66,561.19,66.21,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="18,208.55,506.99,218.38,10.91">Tiet at clef checkthat! 2020: Verified claim retrieval</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_197.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="18,294.50,520.54,211.48,10.91;18,112.66,534.09,128.24,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="18,124.43,547.64,153.99,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">0001. September 22-25, 2020. 2696. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,574.74,395.17,10.91;18,112.66,588.29,394.53,10.91;18,112.66,601.84,393.49,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="18,380.20,588.29,122.23,10.91">Universal sentence encoder</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kurzweil</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1803.11175" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,615.39,395.17,10.91;18,112.66,628.93,393.33,10.91;18,112.66,642.48,59.30,10.91" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="18,318.90,615.39,188.93,10.91;18,112.66,628.93,393.33,10.91;18,112.66,642.48,28.01,10.91">Nlpir@uned at checkthat! 2020: A preliminary approach for check-worthiness and claim retrieval tasks using neural networks and graphs</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martinez-Rico</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martinez-Romo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,656.03,393.61,10.91;18,112.66,669.58,393.33,10.91;19,112.66,86.97,395.01,10.91;19,112.66,100.52,121.17,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="18,173.81,656.03,332.45,10.91;18,112.66,669.58,131.24,10.91">Nlytics at checkthat! 2021: Detecting previously fact-checked claims by measuring semantic similarity</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pritzkau</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-47.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="18,266.34,669.58,239.64,10.91;19,112.66,86.97,146.55,10.91">Working Notes of CLEF 2021-Conference and Labs of the Evaluation Forum, CLEF &apos;2021</title>
		<meeting><address><addrLine>Bucharest, Romania (online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,114.06,393.32,10.91;19,112.66,127.61,393.33,10.91;19,112.66,141.16,394.61,10.91;19,112.66,154.71,187.60,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="19,485.58,114.06,20.40,10.91;19,112.66,127.61,185.05,10.91">Dips at checkthat! 2021: Verified claim retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mihaylova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Borisova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chemishanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hadzhitsanev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hardalov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-45.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="19,320.07,127.61,185.91,10.91;19,112.66,141.16,206.77,10.91">Working Notes of CLEF 2021-Conference and Labs of the Evaluation Forum, CLEF &apos;2021</title>
		<meeting><address><addrLine>Bucharest, Romania (online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,168.26,393.33,10.91;19,112.66,181.81,393.33,10.91;19,112.66,195.36,395.01,10.91;19,112.66,208.91,121.17,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="19,298.31,168.26,207.68,10.91;19,112.66,181.81,99.87,10.91">Aschern at checkthat! 2021: Lambda-calculus of fact-checked claims</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chernyavskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ilvovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-38.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="19,238.60,181.81,267.39,10.91;19,112.66,195.36,135.79,10.91">Working Notes of CLEF 2021-Conference and Labs of the Evaluation Forum, CLEF &apos;2021</title>
		<meeting><address><addrLine>Bucharest, Romania (online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,222.46,393.32,10.91;19,112.66,236.01,393.33,10.91;19,112.66,249.56,393.33,10.91;19,112.66,263.11,393.33,10.91;19,112.66,276.66,395.01,10.91;19,112.66,290.20,177.28,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="19,322.26,222.46,183.72,10.91;19,112.66,236.01,72.11,10.91">SemEval-2012 task 6: A pilot on semantic textual similarity</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/S12-1051" />
	</analytic>
	<monogr>
		<title level="m" coord="19,212.39,236.01,293.60,10.91;19,112.66,249.56,51.96,10.91;19,212.11,249.56,236.41,10.91;19,124.59,263.11,381.40,10.91;19,112.66,276.66,22.05,10.91">Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012)</title>
		<meeting>the Sixth International Workshop on Semantic Evaluation (SemEval 2012)<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="385" to="393" />
		</imprint>
	</monogr>
	<note>SEM 2012: The First Joint Conference on Lexical and Computational Semantics -</note>
</biblStruct>

<biblStruct coords="19,112.66,303.75,393.33,10.91;19,112.66,317.30,395.00,10.91;19,112.66,330.85,257.17,10.91" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="19,364.80,303.75,141.18,10.91;19,112.66,317.30,279.66,10.91">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1705.02364</idno>
		<ptr target="https://arxiv.org/abs/1705.02364.doi:10.48550/ARXIV.1705.02364" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,344.40,393.98,10.91;19,112.66,357.95,28.67,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="19,173.77,344.40,131.48,10.91">The distributional hypothesis</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sahlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="19,314.04,344.40,149.38,10.91">The Italian Journal of Linguistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="33" to="54" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,371.50,393.33,10.91;19,112.66,385.05,393.33,10.91;19,112.66,398.60,393.32,10.91;19,112.66,412.15,393.33,10.91;19,112.66,425.70,394.03,10.91;19,112.66,439.25,185.51,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="19,323.15,371.50,182.83,10.91;19,112.66,385.05,186.91,10.91">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://aclanthology.org/N19-1423.doi:10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m" coord="19,327.87,385.05,178.11,10.91;19,112.66,398.60,393.32,10.91;19,112.66,412.15,99.97,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="19,112.66,452.79,394.61,10.91;19,112.66,466.34,394.62,10.91;19,112.66,479.89,199.51,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="19,290.62,452.79,197.56,10.91">Glove: Global vectors for word representation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m" coord="19,112.66,466.34,270.61,10.91">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,493.44,393.33,10.91;19,112.66,506.99,233.34,10.91" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<title level="m" coord="19,331.70,493.44,174.29,10.91;19,112.66,506.99,50.99,10.91">Enriching word vectors with subword information</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="19,112.66,520.54,394.62,10.91;19,112.66,534.09,300.45,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="19,221.21,520.54,265.81,10.91">SimCSE: Simple contrastive learning of sentence embeddings</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="19,112.66,534.09,269.60,10.91">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,547.64,393.33,10.91;19,112.66,561.19,253.14,10.91" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="19,228.64,547.64,277.34,10.91;19,112.66,561.19,122.59,10.91">Natural language processing with Python: analyzing text with the natural language toolkit</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>O&apos;Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,574.74,345.11,10.91" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="19,170.66,574.74,182.39,10.91">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Bradford Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,588.29,395.01,10.91;19,112.66,604.28,311.00,7.90" xml:id="b25">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lopez</surname></persName>
		</author>
		<idno>arXiv:1:dir:cb0ba3379413db12b0018b7</idno>
		<ptr target="https://github.com/kermitt2/entity-fishing" />
		<title level="m" coord="19,173.71,588.29,59.88,10.91">entity-fishing</title>
		<imprint>
			<date type="published" when="2016">2016-2022</date>
		</imprint>
	</monogr>
	<note>c3af8d0d2d864139c.</note>
</biblStruct>

<biblStruct coords="19,89.29,646.80,84.21,12.85" xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">A</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
