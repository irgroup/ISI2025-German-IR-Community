<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,233.65,15.42;1,89.29,106.66,340.04,15.42">ARC-NLP at CheckThat! 2022: Contradiction for Harmful Tweet Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,73.55,11.96"><forename type="first">Cagri</forename><surname>Toraman</surname></persName>
							<email>ctoraman@aselsan.com.tr</email>
							<affiliation key="aff0">
								<orgName type="institution">Aselsan Research Center</orgName>
								<address>
									<settlement>Ankara</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,175.48,134.97,83.59,11.96"><forename type="first">Oguzhan</forename><surname>Ozcelik</surname></persName>
							<email>ogozcelik@aselsan.com.tr</email>
							<affiliation key="aff0">
								<orgName type="institution">Aselsan Research Center</orgName>
								<address>
									<settlement>Ankara</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,271.72,134.97,76.53,11.96"><forename type="first">Furkan</forename><surname>ÅahinuÃ§</surname></persName>
							<email>fsahinuc@aselsan.com.tr</email>
							<affiliation key="aff0">
								<orgName type="institution">Aselsan Research Center</orgName>
								<address>
									<settlement>Ankara</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,379.24,134.97,71.83,11.96"><forename type="first">Umitcan</forename><surname>Sahin</surname></persName>
							<email>ucsahin@aselsan.com.tr</email>
							<affiliation key="aff0">
								<orgName type="institution">Aselsan Research Center</orgName>
								<address>
									<settlement>Ankara</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,233.65,15.42;1,89.29,106.66,340.04,15.42">ARC-NLP at CheckThat! 2022: Contradiction for Harmful Tweet Detection</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">6E16BEFEFF2B2784596EBD856C982DF7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Claim Detection</term>
					<term>Contradiction</term>
					<term>COVID-19</term>
					<term>Harmful Tweet Detection</term>
					<term>Language Model</term>
					<term>Tweet</term>
					<term>Worthiness Checking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The target task of our team in CLEF2022 CheckThat! Lab challenge is Task-1C, harmful tweet detection. We propose a novel approach, called ARC-NLP-contra, which is a contradiction check approach by using the idea that harmful tweets contradict with the real-life facts in the scope of COVID-19 pandemic. Besides, we propose and examine two other models. The first model, called ARC-NLP-hc, is a traditional approach that utilizes hand-crafted tweet and user features. The second model, called ARC-NLP-pretrain, pretrains a Transformer-based language model by using COVID-related Turkish tweets. We compare the performances of these three models, and submit the highest performing model in the preliminary experiments to the challenge. We make submissions for Task-1A, 1B, 1C in Turkish and Task-1C in English. We have the winning solution for Task-1C, harmful tweet detection in Turkish, using ARC-NLP-contra that is our contradiction check approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The effects of the COVID-19 pandemic maintain its existence for more than two years. The power of the social media is experienced more intensely when such global events break out. Although people can get news and collaborate via social platforms in during such events, undesired behaviors can also take place in these platforms such as spreading misinformation, along with false claims and sharing harmful content. In this regard, CLEF2022 CheckThat! Lab organizes a challenge that focuses on the detection of such undesired behaviors <ref type="bibr" coords="1,451.31,480.20,11.45,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,465.49,480.20,7.51,10.91" target="#b1">2,</ref><ref type="bibr" coords="1,475.73,480.20,7.51,10.91" target="#b2">3,</ref><ref type="bibr" coords="1,485.97,480.20,7.51,10.91" target="#b3">4,</ref><ref type="bibr" coords="1,496.21,480.20,7.64,10.91" target="#b4">5]</ref>. The lab consists of three main tasks that are Task-1: Identifying Relevant Claims in Tweets, Task-2: Detecting Previously Fact Checked Claims, and Task-3: Fake News Detection. We submit our solutions to Task-1 in this challenge.</p><p>The target task of our team, called ARC-NLP (Aselsan Research Center -Natural Language Processing team), in this challenge is Task-1C, harmful tweet detection, in Turkish. We propose a novel model, called ARC-NLP-contra, which is a contradiction check approach by using the idea that harmful tweets contradict with the real-life facts in the scope of COVID-19 pandemic. In other words, we assume that harmful tweets spread misinformation. We thereby check the claims in tweets with a manually generated fact list by using reliable sources, such as Government Offices and UNICEF. Our model placed first in the leaderboard for Turkish. Since this approach is language-independent, i.e. one can create other fact lists in other languages, we also submit our winning solution to Task-1C in English. However, our solution does not perform as expected in English, possibly due to our machine-translated fact list from Turkish to English.</p><p>We also propose and examine two other approaches for Task-1C, harmful tweet detection. The first model, called ARC-NLP-hc, is a traditional approach that utilizes hand-crafted tweet and user features. The second model, called ARC-NLP-pretrain, pretrains a Transformer-based language model, similarly to BERTweet <ref type="bibr" coords="2,270.62,195.36,11.57,10.91" target="#b5">[6]</ref>, by using COVID-related Turkish tweets that we have collected during the training stage of this challenge. We call our pretrained language model as RoBERTweetTurkCovid. To the best of our knowledge, RoBERTweetTurkCovid is the first Transformer-based language model pretrained on Turkish tweets, which can help other researchers to fine-tune models on various downstream tasks.</p><p>We compare the performances of these three models in the preliminary experiments, and submit the highest performing model, ARC-NLP-contra, to Task-1C. Besides, we make submissions to other subtasks, Task-1A (check-worthiness of tweets) and Task-1B (verifiable claim detection). Since our contradiction check approach is not directly applicable to other subtasks, we submit the results of ARC-NLP-pretrain for Task-1A and 1B in Turkish, since we train this language model by using a Turkish corpora.</p><p>Overall, we make submissions for Task-1C in both Turkish and English; and for Task-1A and 1B in Turkish. We have the winning solution for Task-1C, harmful tweet detection in Turkish, using ARC-NLP-contra that is our contradiction check approach. In this paper, we explain our proposed models; namely (i) ARC-NLP-hc with hand-crafted features, (ii) ARC-NLP-pretrain with a novel pretrained language model, and (iii) ARC-NLP-contra with a novel contradiction check approach. We then report the results of our preliminary experiments, along with our results in the final leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Model-1: ARC-NLP-hc</head><p>In order to perform statistical analysis of the challenge data, we extract hand-crafted features to classify the tweets into predefined classes, such as harmful or not in Task-1C. We extract the following hand-crafted features including tweet content and user attributes:</p><p>â€¢ N-grams (bigrams and trigrams), â€¢ Hashtags, â€¢ User features.</p><p>These features both separately and in combination are used in classifying whether a tweet is harmful or not. In the following subsections, the extraction process is detailed for each feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N-grams:</head><p>We use bigrams and trigrams to extract relevant features from the data. For the preprocessing step, we first transform Turkish characters (i.e., "Ã§ÄŸÄ±Ã¶ÅŸÃ¼") to English characters</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>The first 5 bigrams and trigrams of 1C Turkish harmful and unharmful data (including their translations in English) ranked according to their frequencies. (i.e., "cgiosu"). We then remove all URLs from the tweets. Furthermore, we also remove non alphabetical characters such as emojis and placeholders in the text. We remove punctuation and lowercase all the tweets. For the Turkish stop words, we use the collection list given in <ref type="bibr" coords="3,492.10,313.72,11.32,10.91" target="#b6">[7]</ref>, which are disregarded when forming bigrams and trigrams.</p><p>To construct n-gram table for the data, we divide bigrams and trigrams extracted from tweet text into the harmful and unharmful classes. Table <ref type="table" coords="3,315.80,354.36,5.05,10.91">1</ref> shows the first 5 of bigrams and trigrams ranked according to their number of occurrences in the data. Since the data is imbalanced with respect to the classes (i.e., there are more unharmful data instances than harmful ones), we also compute each n-gram's normalized frequency within their own category, which are given in Table <ref type="table" coords="3,115.17,408.56,3.66,10.91">1</ref>. The frequencies are computed dividing an n-gram's total number of occurrences by the number of all n-gram occurrences in its corresponding category. For instance, "doz asi" occurs 27 times in tweets that are labeled harmful. Since there are 1881 harmful n-gram occurrences in the data, its normalized frequency is computed as 0.0144.</p><p>We use the tweets in the data to construct the n-gram , respectively. ğ‘› ğ‘–,ğ‘— is the number of times n-gram ğ‘— occurs in tweet ğ‘–. In other words, each tweet in the data is represented with two-dimensional feature vector, whose dimensions correspond to harmful and unharmful n-gram occurrences extracted from the data. As seen from Table <ref type="table" coords="3,115.70,669.58,3.72,10.91">1</ref>, there are some contradictory n-grams in the data. For example, "asi karsiti" translated as "vaccine opposer" can be somehow thought to be a harmful bigram; however, it's normalized frequency values in both categories (i.e., .0096 vs .0089) are very close. This suggests that n-gram features might not suffice on their own to classify harmful tweets correctly.</p><p>Hashtag Analysis: Similarly to n-gram feature extraction, we perform hashtag analysis on the data. For the preprocessing step, we first transform Turkish characters (i.e., "Ã§ÄŸÄ±Ã¶ÅŸÃ¼") to English characters (i.e., "cgiosu"). We then lowercase all the tweets in the data. Similarly to the n-gram feature analysis, we divide the data into two categories: harmful and unharmful. Table <ref type="table" coords="4,89.29,371.04,4.97,10.91" target="#tab_2">2</ref> shows the most frequently used first 8 hashtags that belong to harmful and unharmful classes.</p><p>There are 180 and 411 hashtags in harmful and unharmful classes, respectively. As given in Table <ref type="table" coords="4,89.29,398.14,3.66,10.91" target="#tab_2">2</ref>, some hashtags belong to both classes; thus, their frequencies (i.e., the number of occurrences) in the data should be considered when extracting hashtag related features. We compute each hashtag's normalized frequency by dividing its total number of occurrences by the number of all hashtags in its corresponding category. For instance, hashtag "pcrdayatmasidurdurulsun" occurs 13 times in tweets that are labeled harmful. Since there are 180 harmful hashtags in the data, its normalized frequency is computed as 0.0722. We extract hashtags from the data to construct the hashtag table ğ’¯ ğ‘¡ğ‘ğ‘” (ğ’¯ ğ‘¡ğ‘ğ‘” 1 : harmful, ğ’¯ ğ‘¡ğ‘ğ‘” 2 : unharmful). Then, similarly to n-gram normalized frequency analysis, we compute each hashtag's normalized frequency within its own category. For the tweet ğ‘–, we allocate two features denoted by ğ‘“ ğ‘¡ğ‘ğ‘” 1,ğ‘– and ğ‘“ ğ‘¡ğ‘ğ‘” 2,ğ‘– . We compute these features as follows:</p><formula xml:id="formula_0" coords="4,229.48,545.68,136.31,56.60">ğ‘“ ğ‘¡ğ‘ğ‘” 1,ğ‘– = âˆ‘ï¸ ğ‘— ğ‘¤ 1,ğ‘— , ğ‘¤ 1,ğ‘— âˆˆ ğ’¯ ğ‘¡ğ‘ğ‘” 1 , ğ‘“ ğ‘¡ğ‘ğ‘” 2,ğ‘– = âˆ‘ï¸ ğ‘— ğ‘¤ 2,ğ‘— , ğ‘¤ 2,ğ‘— âˆˆ ğ’¯ ğ‘¡ğ‘ğ‘” 2 ,</formula><p>where ğ‘— is an extracted hashtag from the tweet ğ‘–. ğ‘¤ 1,ğ‘— and ğ‘¤ 2,ğ‘— are the corresponding normalized frequency values in hashtag tables ğ’¯ ğ‘¡ğ‘ğ‘” 1 and ğ’¯ ğ‘¡ğ‘ğ‘” 2 , respectively. As opposed to n-gram feature extraction, we do not consider the number of times a hashtag occurs in a given tweet. In this way, the same hashtags that occur multiple times in a given tweet are disregarded and counted as one. The problem of contradictory features is also present in hashtag feature extraction. For instance, "biontechyanetki", translated as "biontech side effects", might be considered as a harmful hashtag; but, it also frequently occurs in the tweets that are labeled unharmful. Furthermore, the cold start problem is also prevalent in hashtag extraction; since only a portion of the tweet data includes hashtags and extracted hashtags from the training data might not be present in the test data and vice versa <ref type="bibr" coords="5,215.73,277.13,11.58,10.91" target="#b7">[8]</ref>. Therefore, hashtag features might not be enough to classify harmful tweets on their own.</p><p>User Features: In order to obtain user features, we collect additional data from Twitter API's public access by using tweet ids. Since some users or tweets are deleted, we cannot extract user features for all tweets. A set of sample extracted features are shown in Table <ref type="table" coords="5,448.34,346.54,3.67,10.91" target="#tab_3">3</ref>. These user features are explained as follows.</p><p>â€¢ Age: This feature indicates account age, i.e., the time passed between creating the account and sending the tweet. If the age feature is not extracted from Twitter API, then the value is set to 0. The age values are normalized in all data. â€¢ Influential: This feature indicates account's logarithmic ratio of followees to followers.</p><p>If the influential feature is not extracted from Twitter API, then the value is set to 0. The influential values are normalized in all data. â€¢ Verified: This feature indicates whether the user account is verified or not. If verified, then the value is set to 1; otherwise, 0. â€¢ Status: This feature indicates account's total number of tweets, retweets, and replies. If status feature is not extracted from Twitter API, then the value is set to 0. The status values are normalized in all data. â€¢ Favorites: This feature indicates account's total number of favorites. If favorite feature is not extracted from Twitter API, then the value is set to 0. The favorite values are normalized in all data.</p><p>As our classification model, we use XGBoost, an optimized distributed gradient boosting library that is highly efficient, flexible, and widely used in machine learning applications <ref type="bibr" coords="5,492.24,600.22,11.57,10.91" target="#b8">[9]</ref>. The XGBoost model that we employ in Task-1C is trained with the hand-crafted features (i.e., n-gram, hashtag, and user features) separately and in combination. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Model-2: ARC-NLP-pretrain</head><p>Considering the domain of this challenge (i.e. COVID-19 pandemic), we argue that pretraining of a Transformer-based language model, called RoBERTweetTurkCovid, by using COVIDrelated Turkish tweets can be more effective than using existing language models pretrained on variety of text documents such as BERTurk <ref type="bibr" coords="6,284.30,390.96,16.25,10.91" target="#b9">[10]</ref>, or English tweets such as BERTweet <ref type="bibr" coords="6,471.74,390.96,11.43,10.91" target="#b5">[6]</ref>.</p><p>RoBERTweetTurkCovid-b-30k-wp<ref type="foot" coords="6,251.88,402.76,3.71,7.97" target="#foot_0">1</ref> is our uncased pretrained model whose corpus contains COVID-19 pandemic related Turkish tweets (b refers to base model, 30k refers to the vocabulary size, and wp refers to the WordPiece tokenizer). Figure <ref type="figure" coords="6,345.32,431.61,5.17,10.91" target="#fig_0">1</ref> describes the phases of obtaining our pretrained language model. We collect and preprocess COVID-related Turkish tweets by replacing tweet-specific tags such as usernames, links, and emojis. After preprocessing step, we train a WordPiece tokenizer and obtained our vocabulary contains 30k tokens. Since harmful tweet detection is a text sequence classification task, we consider a base model with optimized pretraining objective scheme, e.g., removing next sentence prediction. We therefore use the RoBERTa-base architecture <ref type="bibr" coords="6,216.51,512.91,18.07,10.91" target="#b10">[11]</ref> instead of other Transformer-based language models, such as BERT <ref type="bibr" coords="6,129.89,526.46,16.36,10.91" target="#b11">[12]</ref>, to pretrain our language model. Finally, our pretrained model is fine-tuned for different downstream tasks, i.e. Task-1A, 1B, and 1C in Turkish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tweet Collection and Preprocessing</head><p>We collect 5,851,000 Turkish tweets from Twitter API's academic access. We use Twitter API's language field to obtain Turkish tweets. In order to satisfy that tweets are related to the COVID-19 pandemic, we create a COVID-related list with 75 keywords, as given in Table <ref type="table" coords="6,252.63,609.19,3.77,10.91">4</ref>. The keywords are selected by the authors considering commonly used COVID-related words and phrases in Turkish. Note that we have different versions of the same keyword, since Turkish users write COVID-19 in different forms such as "kovid" and "covid19". Besides, Twitter API's academic access does not allow regular expressions, we therefore add different forms of the same phrase such as "aÅŸÄ± oldum" (translated as "I got vaccinated") and "aÅŸÄ± oldun" (translated as "you got vaccinated").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>The keyword list used for the collection of COVID-19 pandemic related Turkish tweets. Keywords are sorted according to their length. After obtaining tweets, we filter out near-duplicate tweets by using the Dice similarity with 85% threshold <ref type="bibr" coords="7,154.17,375.16,16.27,10.91" target="#b12">[13]</ref>. We empirically set the threshold value by investigating the outputs of the preliminary experiments. We then preprocess them following a similar approach to BERTweet <ref type="bibr" coords="7,89.29,402.26,11.28,10.91" target="#b5">[6]</ref>. We replace user mentions with @USER, and external links with HTTPURL. We also replace emoji with their text descriptions in Turkish. For that, we use the emoji package <ref type="foot" coords="7,453.59,414.05,3.71,7.97" target="#foot_1">2</ref> in Python. Since it does not support Turkish, we create an emoji list with Turkish descriptions, and modify the package to run with this list. We lastly replace the new-line character with the space character.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Keywords</head><p>Pretraining The model architecture is similar to RoBERTa-base <ref type="bibr" coords="7,380.63,485.22,16.09,10.91" target="#b10">[11]</ref>. There are 12 layers and 12 attention heads with a hidden dimension size of 768. Following <ref type="bibr" coords="7,390.28,498.76,16.39,10.91" target="#b13">[14]</ref>, we apply WordPiece <ref type="bibr" coords="7,89.29,512.31,17.80,10.91" target="#b14">[15]</ref> tokenizer with a vocabulary size of 30k. The pretraining details of our base model is given in Table <ref type="table" coords="7,129.64,525.86,3.81,10.91" target="#tab_5">5</ref>. For comparison, we also provide the details of BERTurk model <ref type="bibr" coords="7,434.75,525.86,16.41,10.91" target="#b9">[10]</ref>, which is a Turkish pretrained version of BERT-base <ref type="bibr" coords="7,273.26,539.41,16.25,10.91" target="#b11">[12]</ref>.</p><p>We use AdamW <ref type="bibr" coords="7,174.34,552.96,17.88,10.91" target="#b15">[16]</ref> optimizer (ğ›½ 1 is 0.90, ğ›½ 2 is 0.98, and ğœ– is 1e-6), linear scheduling with a warmup ratio of 1e-2 and peak learning rate of 5e-5, and gradient accumulation with 22 steps. Other hyperparameters are set to the RoBERTa configuration <ref type="bibr" coords="7,364.60,580.06,16.25,10.91" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Model-3: ARC-NLP-contra</head><p>When we examine the dataset, we notice that harmful tweets display similar patterns. In general, harmful tweets consist of misinformative content about COVID-19 such as conspiracy theories about pandemic or the vaccines' side-effects that do not exist. The common ground of the harmful tweets is that they contradict with the facts about COVID-19. At this point, we convert the problem from harmful tweet detection to contradiction detection. Detection of misinformative or harmful tweets is a difficult task due to several reasons. Although current state-of-the-art language models can capture contextual relations in a sentence <ref type="bibr" coords="8,89.29,350.45,16.09,10.91" target="#b11">[12]</ref>, they might not able to distinguish the true information from the false one by only looking at the sentence itself. There is a need for a reference point (i.e. fact) to find out which information is true. For instance, the sentence "COVID-19 vaccines include HIV virus. " does not give significant information about misinformation to the language models. It is also difficult to find a pattern among harmful tweets. On the other hand, providing a fact for every single instance in the dataset is not a feasible solution. We therefore propose to feed the facts to the language model in our contradiction method.</p><p>ARC-NLP-contra consists of three main stages, as illustrated in Figure <ref type="figure" coords="8,423.20,445.30,3.77,10.91">2</ref>. First, we extract important facts about the COVID-19 pandemic. Then, we associate those facts with every data instance. Finally, we train a language model based on contradiction detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Fact extraction</head><p>We gather COVID-related facts to provide more knowledge to the model. For reliability, we extract a list of facts from Turkish Ministry of Health 34 and Unicef Turkey<ref type="foot" coords="8,427.77,533.52,3.71,7.97" target="#foot_4">5</ref> . These facts are published in order to protect the society from misinformation spread regarding the COVID-19 pandemic. Some examples along with their English translations are given as follows:</p><p>â€¢ Covid-19 aÅŸÄ±sÄ± ile insanlara mikroÃ§ipler yerleÅŸtirileceÄŸi iddiasÄ±nÄ±n bilimsel bir dayanaÄŸÄ± yok. (There is no scientific basis for the claim that microchips will be implanted in people with the Covid-19 vaccine.)  â€¢ AÅŸÄ± olanlara HIV virÃ¼sÃ¼ bulaÅŸmÄ±yor ya da aÅŸÄ±lananlar bu enfeksiyona daha aÃ§Ä±k hale gelmiyor. (Those who are vaccinated do not become infected with HIV, or those who are vaccinated do not become more susceptible to this infection.) â€¢ VirÃ¼s yÃ¼ksek hava sÄ±caklÄ±ÄŸÄ±nda da dÃ¼ÅŸÃ¼k hava sÄ±caklÄ±ÄŸÄ±nda bulaÅŸabilmektedir. (The virus can be transmitted at high air temperature or low air temperature.)</p><p>As observed from examples, facts include the information that mostly aims to correct the misinformation about vaccines and COVID-19. We extract 41 facts in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">Fact association</head><p>Second phase of the ARC-NLP-contra method is to associate facts with related tweets. In order to associate facts and tweets, we check the similarity between two sentences (i.e. the sentences of a fact and tweet). We use the Cosine similarity measurement, and choose the fact that has the highest similarity score with a given tweet. We use two methods for encoding sentences into vector representations before calculating the Cosine similarity scores. We first utilize a deep learning approach, SBERT model for similarity <ref type="bibr" coords="9,330.54,533.04,16.41,10.91" target="#b16">[17]</ref>. Unlike original BERT model <ref type="bibr" coords="9,486.66,533.04,16.42,10.91" target="#b11">[12]</ref>, SBERT model is more appropriate for the Cosine similarity for obtained sentence embeddings. The second method is to encode sentences with conventional TF-IDF vectors.</p><p>In the submitted model, we use the TF-IDF vectors. The reason of this choice is that SBERT focuses on semantic similarity, while TF-IDF focuses on syntactic similarity (i.e. the bag-ofwords model). For instance, although a tweet and its related fact are in the same context (e.g., effectiveness of the vaccines), they can have opposite arguments (i.e. they are not semantically similar), which may result in less similarity. In this case, a harmful tweet may be associated with a fact of different context. However, our aim is to find the fact with the same context as harmful tweet rather than finding the most semantically similar fact. On the other hand, the bag-of-word model with TF-IDF vectors can be more immune to this opposite meaning drawback. Since it determines the similarity according to existence of the same words, facts and harmful tweets that are in the same context can be matched without considering semantic similarity between them. Furthermore, our early experiments that include SBERT in fact association phase do not give promising results. Therefore we follow the mentioned procedure only with TF-IDF method. Note that we do not detect harmful tweets in this phase, but associate a possibly related fact with our tweets. We detect harmful ones in the next phase called contradiction detection.</p><p>While extracting TF-IDF features of the tweets for fact matching, we follow some preprocessing steps. First, we remove URLs and punctuation from the tweets. We make each word lowercased. Since TF-IDF considers word counts, these steps are useful for reducing possible noise. The same steps are also applied to fact sentences. After extracting features for all facts and tweets, the fact giving the highest cosine similarity is chosen for each tweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3.">Contradiction detection</head><p>After providing facts implicitly, we approach to the problem as a kind of Natural Language Inference. In conventional Natural Language Inference, the task is to determine veracity of a hypothesis for a given premise. If the hypothesis is true, the relation between the premise and the hypothesis is named as entailment. If there is no relation between the premise and hypothesis, the relation becomes neutral. Lastly, the relation is considered as contradiction when hypothesis is not correct for the given premise. The sentences, "I have been in Turkey for a week. " and "I visited Eiffel Tower yesterday. " constitute an example for the contradiction.</p><p>In our problem, we try to create a contradiction relationship between the facts and harmful tweets, then to solve it via text classification. Since harmful tweets often contain statements that contradict general facts about COVID-19, we expect that such modelling can be more effective than standard text classification. Furthermore, we provide the reference information which is a critical aspect for the misinformation tasks with this method. At this point, it should be noted that we treat neutral and entailment categories as a single class because the main problem is a binary classification.</p><p>In classification, we utilize the state-of-the-art Transformer-based language models. We imported a pretrained model and fine-tune it via the harmful tweet dataset. We feed tweet and fact pairs as data instances separated by [SEP] token. We do not make any changes in labels because the pairs with the harmful tweets automatically indicate a contradiction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>The challenge task includes the datasets in six languages that are Arabic, Bulgarian, Dutch, English, Spanish and Turkish <ref type="bibr" coords="10,216.21,595.27,11.23,10.91" target="#b0">[1,</ref><ref type="bibr" coords="10,229.30,595.27,7.43,10.91" target="#b1">2,</ref><ref type="bibr" coords="10,238.58,595.27,7.49,10.91" target="#b2">3]</ref>. The datasets contain a training, development, development test splits. In the last part of the challenge, unlabeled split (Test) is released. The subtasks and data statistics are given in the following subsections. The dataset statistics are given for all tasks in Table <ref type="table" coords="10,152.97,635.91,3.74,10.91">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 6</head><p>The number of instances and ratio of the positive instances in Task-1A, 1B, and 1C Turkish (TR) dataset and 1C English (EN) dataset. Task-1A: Check-worthiness of tweets The aim of Task-1A is to detect whether a tweet is worthy for fact-checking. This classification task is defined with binary labels, worthy or not worthy. Model performances are measured by the F1 score of positive class for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task 1A-TR 1B-TR 1C-TR 1C-EN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task-1B: Verifiable factual claims detection</head><p>The aim of the Task-1B is to detect a tweet whether it contains a verifiable factual claim. This classification task is defined with binary labels, claim or no claim. Model performances are measured by the accuracy metric for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task-1C: Harmful tweet detection</head><p>The aim of Task-1C is to detect harmful tweets for society. Model performances are measured with the F1 score of the positive class for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experimental Setup</head><p>In this section, we explain the experimental setups for the preliminary experiments of each model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Model-1: ARC-NLP-hc</head><p>When extracting n-gram and hashtag features, we construct n-gram and hashtag tables ğ’¯ ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š and ğ’¯ ğ‘¡ğ‘ğ‘” using the tweet data in the Training and Development splits. We train the classifier with the features extracted from these tables as explained previously. We then report the classification results only on the Development Test data. For the user features, the collected tweet json objects from Twitter API are 1766 (out of 2417) for the Training split, 166 (out of 222) for the Development split, and 499 (out of 660) for the Development Test split. The reason for missing objects are that some user accounts and tweets are deleted or suspended.</p><p>The n-gram (N), hashtag (H), and user (U) features might not suffice on their own to achieve a good classification performance due to the problems of contradictory features and cold start. Therefore, we also report classification results on the combined features.</p><p>The classifier is trained with the user features collected from the Training and Development splits, while the Development Test split is spared for evaluation and reporting classification results for our preliminary experiments.</p><p>As the classifier, we use XGBoost <ref type="bibr" coords="12,251.44,86.97,12.88,10.91" target="#b8">[9]</ref> with a grid-search hyperparameter optimization. The grid-searched hyperparameters are as follows:</p><p>â€¢ max_depth (Maximum depth of a tree): <ref type="bibr" coords="12,292.90,123.76,11.52,9.57" target="#b2">[3,</ref><ref type="bibr" coords="12,306.23,123.76,8.48,9.57" target="#b5">6,</ref><ref type="bibr" coords="12,316.54,123.76,12.25,9.57" target="#b9">10]</ref>,</p><p>â€¢ learning_rate: [0.001, 0.01, 0.05, 0.1],</p><p>â€¢ n_estimators (Number of estimators): [100, 500, 1000],</p><p>â€¢ colsample_bytree (Subsample ratio of columns when constructing each tree): [0.3, 0.7, 0.9].</p><p>The classifier optimizes each hyperparameter for the hand-crafted features separately and in combination. The optimized hyperparameter list for each feature is given in Table <ref type="table" coords="12,456.85,203.81,3.74,10.91" target="#tab_8">7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Model-2: ARC-NLP-pretrain</head><p>During the fine-tuning of our Transformer-based language model, RoBERTweetTurkCovid, we employ PyTorch 1.11 <ref type="bibr" coords="12,207.66,442.50,16.41,10.91" target="#b17">[18]</ref>. In our preliminary experiments of this model, we use the Training split without making any changes on the given dataset, and evaluate the model on the Development Test split. We use the predefined hyperparameters for the fine-tuning process of our model. We use a batch size of 8 instances, and run training for 5 epochs with a learning rate of 5e-5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Model-3: ARC-NLP-contra</head><p>While extracting TF-IDF features for ARC-NLP-contra, we do not put any constraint on the number of features. Therefore, the number of TF-IDF features correspond to the number of unique words in the dataset. Since the dataset is quite small, the number of features does not create any computational overhead. Shrinking the already small features is also not a reasonable approach for the sake of information loss.</p><p>In fine-tuning the language model for ARC-NLP-contra, we use the BERTurk model <ref type="bibr" coords="12,486.71,613.77,16.38,10.91" target="#b9">[10]</ref>, which is a BERT-base model pretrained on Turkish corpora. We utilize Trainer API from Huggingface library <ref type="bibr" coords="12,180.46,640.87,16.13,10.91" target="#b18">[19]</ref>. We conduct a hyperparameter search by utilizing Optuna along with Trainer <ref type="bibr" coords="12,125.00,654.42,16.29,10.91" target="#b19">[20]</ref>. We optimize batch size, number of epochs, and learning rate. In total, we search hyperparameters along 20 trials. The number of epochs is searched in the interval <ref type="bibr" coords="12,448.35,668.70,11.52,9.57" target="#b4">[5,</ref><ref type="bibr" coords="12,461.69,668.70,12.22,9.57" target="#b14">15]</ref>. Batch size is searched in the set {4, 8, 16, 32}. Learning rate is searched in the interval [1e-5,1e-4]. The best hyperparameters are chosen by F1 score of positive class on the Development split. The best hyperparameters are 8 for batch size, 15 for epochs and â‰ˆ 2.5e-5 for learning rate. For our submission to English, we use translations of the Turkish facts to English by Google Translate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Experimental Results</head><p>In this section, we report our preliminary results obtained from the labeled Development Test split. We choose which model to submit by using the results of our preliminary experiments. We also report our best model's leaderboard results. In addition, we present results of baseline methods determined by the organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Baseline Results</head><p>In this part, we report the evaluation results of the baseline methods for Task-1 determined by the organizers <ref type="foot" coords="13,151.61,292.53,3.71,7.97" target="#foot_5">6</ref> . These methods are as follows: 1) random prediction, 2) majority, and 3) TF-IDF. 10 random seed initializations are performed for the random baseline. The best single results along with the average of 10 seeds are given in Table <ref type="table" coords="13,327.50,321.39,3.74,10.91">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 8</head><p>Baseline results for the tasks Turkish 1A-1B-1C and English 1C. Results are presented in terms of F1 score of positive class for the Tasks 1A and 1C, and accuracy for the Task-1B. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Preliminary Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARC-NLP-hc:</head><p>In this part, we share our preliminary classification results for the hand-crafted features: (N)-gram, (H)ashtag, and (U)ser features. We share the classification results for the features both separately and in combination in terms of positive F1 scores in Table <ref type="table" coords="13,460.71,638.55,3.76,10.91">9</ref>. As seen from Table <ref type="table" coords="14,139.48,86.97,3.67,10.91">9</ref>, the best positive class F1 score is achieved when the n-gram and hashtag features are combined together. This might be due to the fact that combining hashtags with n-gram features provides a partial solution for contradictory features and cold start problem. However, it is clear from Table <ref type="table" coords="14,183.47,127.61,5.06,10.91">9</ref> that user features alone do not provide any meaningful contribution to classify harmful tweets. Therefore, the model that is trained with all hand-crafted features does not achieve the best score. The best score is also reported in Table <ref type="table" coords="14,385.82,154.71,8.36,10.91" target="#tab_10">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 9</head><p>Results of our model, ARC-NLP-hc, in terms of positive class F1 score. The best (out of 10 runs) and average score of 10 runs are given in Table <ref type="table" coords="14,416.98,374.43,8.50,10.91" target="#tab_10">10</ref>. We observe that ARC-NLP-pretrain produces more robust scores than the BERTurk model. We use 10 random model initializations and report the average score. Although BERTurk achieves better results than ARC-NLP-pretrain, BERTurk's positive class F1 score dip to zero in some of the random initializations in Task-1A and 1C. This results in high deviance between the best and average scores for BERTurk. We argue that a pretrained model using a text corpus without noisy social media texts may not be adequate for such tasks having noisy texts (e.g., emoticons, abbreviations, and web urls) and up-to-date topics (e.g., COVID-19 pandemic). The robustness on noisy texts and the success of our ARC-NLP-pretrain model is promising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>ARC-NLP-contra: After obtaining the best hyperparameters, we implement 10 consecutive training and evaluate the model on the Development Test split. We use 10 random model initialization. The best (out of 10 runs) and average score of 10 runs are given in Table <ref type="table" coords="14,494.87,538.68,8.53,10.91" target="#tab_10">10</ref>. ARC-NLP-contra has better performance in terms of both the best and averaged results in Turkish harmful detection task (1C). ARC-NLP-pretrain has also competitive results with ARC-NLP-contra model but cannot outperform ARC-NLP-contra. Although ARC-NLP-hc exceeds the baseline scores, it obtains lower results compared to previous two models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Leaderboard Results</head><p>Since we obtain the highest positive class F1 score via ARC-NLP-contra method on the given test set, we decide to use its predictions for Task-1C in both Turkish and English. In order to improve our final training before the submission to leaderboard, we utilize the whole labeled The list of our submissions and name of the submitted models, along with their leaderboard results, are given in Table <ref type="table" coords="15,259.49,342.38,8.25,10.91">11</ref>. Our contradiction check approach for harmful tweet detection, ARC-NLP-contra, takes the first place in the leaderboard. Since the approach is language-independent, we also submit our winning solution to Task-1C in English. However, it does not perform as expected in English, possibly due to our machinetranslated fact list from Turkish to English. We collect COVID-related facts from the Turkish Web Sources (Unicef Turkey and Turkish Ministry of Health). Although some facts are universal ("Those who are vaccinated do not become infected with HIV "), the list includes some facts reflected by Turkish politics and cultural effects. An example fact is "There is no pork ingredient in the inactivated COVID-19 vaccine." We argue that the performance of our model in English probably decreases because of its fact list that includes direct translations from Turkish fact list. An extension or recollection of fact list for other languages can provide better performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 11</head><p>Leaderboard scores and ranks regarding our submissions in terms of the F1 score of positive class for Task-1A and 1C, and the accuracy score for the Task-1B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>Submitted Model Leaderboard Score Rank TR Task-1A ARC-NLP-pretrain 0.082 4 Task-1B ARC-NLP-pretrain 0.760 3 Task-1C ARC-NLP-contra 0.366 1 EN Task-1C ARC-NLP-contra 0.300</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>We propose and examine three models for checking worthiness, detecting claims, and detecting harmful tweets in the CLEF2022 CheckThat! Lab. We explain the details of our models and the results of our preliminary experiments in this paper. Our contradiction checking approach, ARC-NLP-contra, is the winning solution for the task of harmful tweet detection in Turkish. We plan to extend our experiments to other datasets and languages. We can also expand our COVID-related Turkish tweets corpus to train an improved version of ARC-NLP-pretrain. Furthermore, employing a fact list together with Transformer-based language models perform promising as demonstrated by ARC-NLP-contra. The performance can be improved by an extended fact database. We believe that ARC-NLP-contra model can be used for similar tasks requiring an external knowledge base, such as our COVID-19 fact list.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,89.29,221.37,416.69,8.93;6,89.29,233.38,416.70,8.87;6,89.29,245.33,416.70,8.87;6,89.29,257.29,417.79,8.87;6,89.29,269.24,416.69,8.87;6,89.29,281.20,416.70,8.87;6,89.29,293.10,241.76,8.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: ARC-NLP-pretrain model diagram. We collect and preprocess COVID-related Turkish by replacing tweet-specific tags such as usernames, links, and emojis. The vocabulary is obtained by training the Word-Piece tokenizer with the preprocessed text. We then use the RoBERTa-base architecture to pretrain our language model. The pretrained model can be fine-tuned for several tasks, such as check-worthiness of tweets and harmful tweet detection. The translation of the sample tweet in the figure is "@USER Since the covid incidents, there have been posts that have disturbed the public [worried face emoticon] for correct information HTTPURL".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="14,270.29,215.29,28.86,8.93;14,360.30,215.29,28.52,8.93;14,206.45,266.53,8.87,11.36;14,270.29,232.77,63.69,8.87;14,364.21,232.77,20.72,8.87;14,270.29,244.72,63.77,8.87;14,364.21,244.72,20.72,8.87;14,270.29,256.68,63.43,8.87;14,364.21,256.68,20.72,8.87;14,225.00,268.63,116.21,8.87;14,364.21,268.63,20.72,8.87;14,270.29,280.59,70.82,8.87;14,364.21,280.59,20.72,8.87;14,270.29,292.55,70.90,8.87;14,364.21,292.55,20.72,8.87;14,270.29,304.50,78.06,8.87;14,364.21,304.50,20.72,8.87;14,89.29,347.33,416.70,10.91;14,89.29,360.88,418.37,10.91"><head></head><label></label><figDesc>ARC-NLP-hc-NH 0.400 ARC-NLP-hc-NU 0.280 ARC-NLP-hc-HU 0.120 ARC-NLP-hc-NHU 0.320 ARC-NLP-pretrain: With the predefined hyperparameters, we implement 10 consecutive training and evaluate the model on the given Dev_test set. We use 10 random model initialization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,88.89,460.19,418.78,181.60"><head></head><label></label><figDesc>table ğ’¯ ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š (ğ’¯ ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š</figDesc><table coords="3,88.89,462.76,418.78,179.03"><row><cell cols="6">: harmful, : unharmful) aforementioned in the previous step. Then, for the tweet ğ‘– in the data, we 1 allocate two features denoted by ğ‘“ ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š ğ’¯ ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š 2 1,ğ‘– and ğ‘“ ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š 2,ğ‘– . We compute these features as follows:</cell></row><row><cell>ğ‘“ ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š 1,ğ‘–</cell><cell>=</cell><cell>âˆ‘ï¸</cell><cell>ğ‘› ğ‘–,ğ‘— ğ‘¤ 1,ğ‘— , ğ‘¤ 1,ğ‘— âˆˆ ğ’¯ ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š 1</cell><cell>,</cell></row><row><cell></cell><cell></cell><cell>ğ‘—</cell><cell></cell><cell></cell></row><row><cell>ğ‘“ ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š 2,ğ‘–</cell><cell>=</cell><cell>âˆ‘ï¸</cell><cell>ğ‘› ğ‘–,ğ‘— ğ‘¤ 2,ğ‘— , ğ‘¤ 2,ğ‘— âˆˆ ğ’¯ ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š 2</cell><cell>,</cell></row><row><cell></cell><cell></cell><cell>ğ‘—</cell><cell></cell><cell></cell></row><row><cell cols="6">where ğ‘— is an extracted n-gram from the tweet ğ‘–. We extract bigrams and trigrams in this study.</cell></row><row><cell cols="5">ğ‘¤ 1,ğ‘— and ğ‘¤ 2,ğ‘— are the corresponding normalized frequency values in n-grams tables ğ’¯ ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š 1 ğ’¯ ğ‘›ğ‘”ğ‘Ÿğ‘ğ‘š 2</cell><cell>and</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,88.99,90.49,417.00,142.92"><head>Table 2</head><label>2</label><figDesc>The most frequently used first 8 hashtags that belong to harmful and unharmful categories (including their translations in English) in Task-1C Turkish dataset</figDesc><table coords="4,96.01,133.83,403.25,99.58"><row><cell>Harmful #</cell><cell>Translation</cell><cell>Freq. Unharmful #</cell><cell>Translation</cell><cell>Freq.</cell></row><row><cell cols="2">pcrdayatmasidurdurulsun stop PCRs</cell><cell>.0722 biontech</cell><cell>-</cell><cell>.0511</cell></row><row><cell>biontech</cell><cell>-</cell><cell>.0500 turkovac</cell><cell>-</cell><cell>.0365</cell></row><row><cell>asivepcrdurdurulsun</cell><cell>stop vacc.</cell><cell cols="2">.0278 pcrdayatmasidurdurulsun stop PCRs</cell><cell>.0292</cell></row><row><cell>kalpkrizlerisalgini</cell><cell cols="2">heart diseases .0278 coronavirus</cell><cell>-</cell><cell>.0292</cell></row><row><cell>denekolmaturkiye</cell><cell>no tests</cell><cell>.0278 covid19</cell><cell>-</cell><cell>.0146</cell></row><row><cell>pcrhataliasizararli</cell><cell>harmful vacc.</cell><cell>.0278 sondakika</cell><cell>newsbreak</cell><cell>.0122</cell></row><row><cell>nepcrneasi</cell><cell>harmful vacc.</cell><cell>.0167 biontechyanetki</cell><cell>side effect</cell><cell>.0097</cell></row><row><cell>biontechyanetki</cell><cell>side effect</cell><cell>.0111 asihayatkurtarir</cell><cell cols="2">vacc. saves lifes .0073</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,88.99,90.49,373.51,105.74"><head>Table 3</head><label>3</label><figDesc>Sample user features that we extract from Twitter API's public access.</figDesc><table coords="5,130.29,122.05,332.21,74.17"><row><cell>Tweet ID</cell><cell>Age</cell><cell cols="4">Influential Verified Status Favorites</cell></row><row><cell>1423282834987372553</cell><cell>0.0</cell><cell>0.6357</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell cols="2">1428301747110522881 0.3333</cell><cell>0.5802</cell><cell>0.0</cell><cell>0.0012</cell><cell>0.0118</cell></row><row><cell cols="2">1443304873471221760 0.1966</cell><cell>0.6464</cell><cell>0.0</cell><cell>0.0782</cell><cell>0.1626</cell></row><row><cell cols="2">1421163651084562438 0.2133</cell><cell>0.7029</cell><cell>0.0</cell><cell>0.0018</cell><cell>0.0066</cell></row><row><cell cols="2">1428447916826501126 0.0188</cell><cell>0.7449</cell><cell>0.0</cell><cell>4.49e-05</cell><cell>0.0003</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,88.99,90.49,417.00,165.51"><head>Table 5</head><label>5</label><figDesc>Details of pre-training configurations for BERTurk and our model, RoBERTweetTurkCovid. (*Train time and hardware are given for a vocabulary size of 30k tokens.</figDesc><table coords="8,163.66,134.01,265.47,121.99"><row><cell>Configuration</cell><cell cols="2">BERTurk-base RoBERTweetTurkCovid</cell></row><row><cell>Total parameters</cell><cell>110.62 M</cell><cell>108.79 M</cell></row><row><cell>Train data</cell><cell>35 GB</cell><cell>1 GB</cell></row><row><cell>Layers</cell><cell>12</cell><cell>12</cell></row><row><cell>Heads</cell><cell>12</cell><cell>12</cell></row><row><cell>Hidden size</cell><cell>768</cell><cell>768</cell></row><row><cell>Batch size</cell><cell>n/a</cell><cell>264</cell></row><row><cell>Max length</cell><cell>512 tokens</cell><cell>514 tokens</cell></row><row><cell>Train time</cell><cell>9.63 days</cell><cell>12 hours</cell></row><row><cell>Hardware</cell><cell>TPU v3-8</cell><cell>4x Nvidia RTX2080 Ti</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,89.02,88.04,417.27,204.64"><head></head><label></label><figDesc>First, general facts related to COVID-19 pandemic are gathered from reliable sources. In fact association, TF-IDF vectors are obtained for facts and tweets. For each tweet, the fact with the highest cosine similarity is determined. After all pairs are determined. A Transformer based language model is fine-tuned as contradiction detection task.</figDesc><table coords="9,89.29,88.04,393.95,168.78"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Training data</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>t 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>t 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Tweets and facts</cell><cell></cell></row><row><cell>Unicef Turkey</cell><cell>Turkish Ministry of Health</cell><cell>TWEETS FACTS</cell><cell>TF-IDF vectors</cell><cell>t 3 t 2417 f 1</cell><cell>Cosine similarity</cell><cell>Î¸</cell><cell>t i</cell><cell>f j</cell><cell cols="3">associated data Tweets Facts Label t 1 f 32 0 t 2 f 12 0 t 3 f 3 1 t 4 f 6 0 ... ... ...</cell><cell>Tweet</cell><cell>Fact</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>f40 f41</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>t 2417</cell><cell>f 12</cell><cell>1</cell></row><row><cell cols="6">Figure 2: ARC-NLP-contra model diagram.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="12,88.99,232.82,418.09,141.60"><head>Table 7</head><label>7</label><figDesc>Hyperparameter list for our model trained with hand-crafted features: (N)-grams, (H)ashtags, (U)sers, separately and in combination</figDesc><table coords="12,110.20,276.34,374.87,98.08"><row><cell>Model</cell><cell cols="4">max_depth learning_rate n_estimators colsample_bytree</cell></row><row><cell>ARC-NLP-hc-N</cell><cell>6</cell><cell>0.1</cell><cell>100</cell><cell>0.3</cell></row><row><cell>ARC-NLP-hc-H</cell><cell>3</cell><cell>0.05</cell><cell>100</cell><cell>0.3</cell></row><row><cell>ARC-NLP-hc-U</cell><cell>3</cell><cell>0.01</cell><cell>100</cell><cell>0.3</cell></row><row><cell>ARC-NLP-hc-NH</cell><cell>6</cell><cell>0.1</cell><cell>100</cell><cell>0.3</cell></row><row><cell>ARC-NLP-hc-NU</cell><cell>3</cell><cell>0.05</cell><cell>100</cell><cell>0.7</cell></row><row><cell>ARC-NLP-hc-HU</cell><cell>3</cell><cell>0.0</cell><cell>100</cell><cell>0.7</cell></row><row><cell>ARC-NLP-hc-NHU</cell><cell>6</cell><cell>0.1</cell><cell>500</cell><cell>0.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="15,88.99,90.49,418.09,249.26"><head>Table 10</head><label>10</label><figDesc>Preliminary results of our proposed models in terms of the F1 score of positive class for Task-1A and 1C, and the accuracy score for the Task-1B. The highest scores are given in bold.data by merging the Training, Development, and Development Test splits. We obtain our final model's predictions on the leaderboard Test split. On the other hand, we decide to use ARC-NLP-pretrain for Task-1A and 1B. We only use the Training split for the final training of this model.</figDesc><table coords="15,185.71,134.01,223.85,129.28"><row><cell></cell><cell>Task</cell><cell>Model</cell><cell cols="2">Best Average</cell></row><row><cell></cell><cell>Task-1A</cell><cell cols="2">BERTurk ARC-NLP-pretrain 0.348 0.400</cell><cell>0.273 0.289</cell></row><row><cell>TR</cell><cell>Task-1B</cell><cell cols="2">BERTurk ARC-NLP-pretrain 0.756 0.794 BERTurk 0.572</cell><cell>0.775 0.745 0.375</cell></row><row><cell></cell><cell>Task-1C</cell><cell cols="2">ARC-NLP-hc ARC-NLP-pretrain 0.566 0.400</cell><cell>-0.518</cell></row><row><cell></cell><cell></cell><cell>ARC-NLP-contra</cell><cell>0.600</cell><cell>0.555</cell></row><row><cell>EN</cell><cell cols="2">Task-1C ARC-NLP-contra</cell><cell>0.391</cell><cell>0.323</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="6,108.93,671.04,238.18,8.97"><p>Our models will be published at https://huggingface.co/ctoraman</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="7,108.93,671.03,136.72,8.97"><p>https://github.com/carpedm20/emoji/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="8,108.93,649.09,249.25,8.97"><p>https://covid19asi.saglik.gov.tr/TR-77694/sikca-sorulan-sorular.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="8,108.93,660.05,294.11,8.97"><p>https://covid19.saglik.gov.tr/TR-66125/sikca-sorulan-sorular-halka-yonelik.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="8,108.93,671.01,403.16,8.97"><p>https://www.unicef.org/turkey/gercek-mi-efsane-mi-koronavirus-covid-19-hakkinda-ne-kadar-bilgi-sahibisiniz</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="13,108.93,660.08,369.41,8.97;13,89.29,671.04,83.49,8.97"><p>https://gitlab.com/checkthat_lab/clef2022-checkthat-lab/clef2022-checkthat-lab/-/blob/main/task1/ baselines/subtask_1.py</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank the organizers and other participants in the challenge.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="16,112.66,350.47,394.53,10.91;16,112.33,364.02,394.85,10.91;16,112.66,377.57,393.33,10.91;16,112.66,391.12,394.53,10.91;16,112.66,404.67,393.53,10.91;16,112.66,418.22,225.55,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="16,284.29,377.57,221.70,10.91;16,112.66,391.12,203.11,10.91">The CLEF-2022 CheckThat! Lab on fighting the COVID-19 infodemic and fake news detection</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>BarrÃ³n-CedeÃ±o</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>StruÃŸ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>MÃ­guez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Kartal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>BeltrÃ¡n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,313.54,404.67,148.39,10.91">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Hagen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Verberne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Seifert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Balog</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>NÃ¸rvÃ¥g</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Setty</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="416" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,431.77,394.53,10.91;16,112.33,445.32,394.85,10.91;16,112.66,458.87,393.33,10.91;16,112.66,472.42,394.53,10.91;16,112.66,485.97,394.53,10.91;16,112.66,499.52,393.33,10.91;16,112.66,513.06,393.33,10.91;16,112.66,526.61,366.90,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="16,432.00,458.87,73.99,10.91;16,112.66,472.42,390.24,10.91">Overview of the CLEF-2022 CheckThat! Lab on fighting the COVID-19 infodemic and fake news detection</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>BarrÃ³n-CedeÃ±o</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>StruÃŸ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>MÃ­guez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Kartal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>BeltrÃ¡n</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>KÃ¶hler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,397.65,499.52,108.34,10.91;16,112.66,513.06,393.33,10.91;16,112.66,526.61,270.79,10.91">Proceedings of the 13th International Conference of the CLEF Association: Information Access Evaluation meets Multilinguality, Multimodality, and Visualization, CLEF &apos;2022</title>
		<editor>
			<persName><forename type="first">A</forename><surname>BarrÃ³n-CedeÃ±o</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Degli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Esposti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sebastiani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Macdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Pasi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hanbury</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Potthast</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><surname>Nicola</surname></persName>
		</editor>
		<meeting>the 13th International Conference of the CLEF Association: Information Access Evaluation meets Multilinguality, Multimodality, and Visualization, CLEF &apos;2022<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,540.16,394.52,10.91;16,112.14,553.71,394.25,10.91;16,112.66,567.26,394.62,10.91;16,112.14,580.81,395.05,10.91;16,112.66,594.36,89.12,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="16,462.25,553.71,44.14,10.91;16,112.66,567.26,372.69,10.91">Overview of the CLEF-2022 CheckThat! Lab Task 1 on identifying relevant claims in tweets</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>BarrÃ³n-CedeÃ±o</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>MÃ­guez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Kartal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>BeltrÃ¡n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,112.14,580.81,390.83,10.91">Working Notes of CLEF 2022-Conference and Labs of the Evaluation Forum, CLEF &apos;2022</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,607.91,393.33,10.91;16,112.66,621.46,394.62,10.91;16,112.14,635.01,395.05,10.91;16,112.66,648.56,89.12,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,449.74,607.91,56.25,10.91;16,112.66,621.46,374.33,10.91">Overview of the CLEF-2022 CheckThat! Lab Task 2 on detecting previously fact-checked claims</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,112.14,635.01,390.83,10.91">Working Notes of CLEF 2022-Conference and Labs of the Evaluation Forum, CLEF &apos;2022</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,86.97,393.33,10.91;17,112.66,100.52,393.58,10.91;17,112.66,114.06,382.34,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="17,431.30,86.97,74.69,10.91;17,112.66,100.52,262.44,10.91">Overview of the CLEF-2022 CheckThat! Lab Task 3 on fake news detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>KÃ¶hler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>StruÃŸ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,398.36,100.52,107.88,10.91;17,112.66,114.06,286.23,10.91">Working Notes of CLEF 2022-Conference and Labs of the Evaluation Forum, CLEF &apos;2022</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,127.61,393.53,10.91;17,112.66,141.16,393.33,10.91;17,112.66,154.71,394.53,10.91;17,112.66,168.26,300.78,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="17,296.79,127.61,209.39,10.91;17,112.66,141.16,62.84,10.91">BERTweet: A pre-trained language model for English tweets</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Tuan</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.2</idno>
	</analytic>
	<monogr>
		<title level="m" coord="17,198.47,141.16,307.51,10.91;17,112.66,154.71,390.37,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,181.81,393.32,10.91;17,112.66,195.36,395.01,10.91;17,112.66,208.91,119.84,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="17,205.58,181.81,300.41,10.91;17,112.66,195.36,67.96,10.91">Discovering story chains: A framework based on zigzagged search and news actors</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Toraman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Can</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.23885</idno>
	</analytic>
	<monogr>
		<title level="j" coord="17,187.94,195.36,288.82,10.91">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,222.46,393.71,10.91;17,112.66,236.01,393.33,10.91;17,112.66,249.56,241.73,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="17,339.03,222.46,167.34,10.91;17,112.66,236.01,253.93,10.91">Understanding social engagements: A comparative analysis of user and text features in Twitter</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Toraman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>ÅahinuÃ§</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">B</forename><surname>Akkaya</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13278-022-00872-1</idno>
	</analytic>
	<monogr>
		<title level="j" coord="17,375.02,236.01,130.97,10.91;17,112.66,249.56,32.35,10.91">Social Network Analysis and Mining</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,263.11,393.33,10.91;17,112.66,276.66,394.52,10.91;17,112.66,290.20,395.00,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="17,211.45,263.11,186.67,10.91">XGBoost: A scalable tree boosting system</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939785</idno>
	</analytic>
	<monogr>
		<title level="m" coord="17,421.80,263.11,84.19,10.91;17,112.66,276.66,394.52,10.91;17,112.66,290.20,35.50,10.91">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,303.75,394.96,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schweter</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.3770924</idno>
		<title level="m" coord="17,167.79,303.75,158.34,10.91">BERTurk -BERT models for Turkish</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,317.30,393.33,10.91;17,112.66,330.85,107.17,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="17,170.60,317.30,263.12,10.91">RoBERTa: A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="17,112.66,344.40,393.33,10.91;17,112.66,357.95,393.33,10.91;17,112.66,371.50,393.32,10.91;17,112.66,385.05,393.33,10.91;17,112.66,398.60,394.88,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="17,323.15,344.40,182.83,10.91;17,112.66,357.95,186.91,10.91">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m" coord="17,327.87,357.95,178.11,10.91;17,112.66,371.50,393.32,10.91;17,112.66,385.05,99.97,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="17,112.66,412.15,393.33,10.91;17,112.66,425.70,393.33,10.91;17,111.79,439.25,398.35,10.91;17,112.36,455.24,121.09,7.90" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="17,292.59,412.15,213.40,10.91;17,112.66,425.70,141.51,10.91">BlackLivesMatter 2020: An analysis of deleted and suspended users in Twitter</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Toraman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>ÅahinuÃ§</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H</forename><surname>Yilmaz</surname></persName>
		</author>
		<idno type="DOI">10.1145/3501247.3531539</idno>
	</analytic>
	<monogr>
		<title level="m" coord="17,280.27,425.70,225.72,10.91;17,111.79,439.25,11.57,10.91">14th ACM Web Science Conference 2022, WebSci &apos;22</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="290" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,466.34,393.33,10.91;17,112.66,479.89,394.51,10.91;17,112.36,495.88,97.35,7.90" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="17,343.05,466.34,162.94,10.91;17,112.66,479.89,140.18,10.91">Impact of tokenization on language models: An analysis for Turkish</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Toraman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>ÅahinuÃ§</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Ozcelik</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2204.08832</idno>
		<idno type="arXiv">arXiv:2204.08832</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="17,112.66,506.99,393.33,10.91;17,112.66,520.54,395.01,10.91;17,112.66,534.09,179.18,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="17,230.32,506.99,147.86,10.91">Japanese and Korean voice search</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nakajima</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2012.6289079</idno>
	</analytic>
	<monogr>
		<title level="m" coord="17,424.07,506.99,81.92,10.91;17,112.66,520.54,294.64,10.91">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="5149" to="5152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,547.64,395.17,10.91;17,112.66,561.19,222.68,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="17,218.88,547.64,171.50,10.91">Decoupled weight decay regularization</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,413.17,547.64,94.66,10.91;17,112.66,561.19,148.97,10.91">International Conference on Learning Representations</title>
		<meeting><address><addrLine>LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,574.74,395.17,10.91;17,112.66,588.29,393.33,10.91;17,112.66,601.84,393.33,10.91;17,112.66,615.39,394.53,10.91;17,112.66,628.93,263.48,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="17,231.60,574.74,276.23,10.91;17,112.66,588.29,39.98,10.91">Sentence-BERT: Sentence embeddings using Siamese BERTnetworks</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1410</idno>
	</analytic>
	<monogr>
		<title level="m" coord="17,182.44,588.29,323.55,10.91;17,112.66,601.84,393.33,10.91;17,112.66,615.39,330.76,10.91">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,642.48,394.53,10.91;17,112.66,656.03,394.52,10.91;17,112.66,669.58,65.30,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="17,188.92,642.48,314.25,10.91">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="17,127.23,656.03,232.24,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
			<date type="published" when="2019">2019</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,86.97,395.17,10.91;18,112.66,100.52,394.61,10.91;18,112.66,114.06,395.01,10.91;18,112.66,127.61,228.88,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="18,186.87,86.97,271.39,10.91">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m" coord="18,488.38,86.97,19.45,10.91;18,112.66,100.52,394.61,10.91;18,112.66,114.06,308.61,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,141.16,395.17,10.91;18,112.66,154.71,393.32,10.91;18,112.66,168.26,393.33,10.91;18,112.66,181.81,389.06,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="18,340.16,141.16,167.68,10.91;18,112.66,154.71,140.88,10.91">Optuna: A next-generation hyperparameter optimization framework</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yanase</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3330701</idno>
	</analytic>
	<monogr>
		<title level="m" coord="18,276.93,154.71,229.05,10.91;18,112.66,168.26,269.23,10.91">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;19</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2623" to="2631" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
