<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,80.66,75.32,433.53,17.04;1,229.54,95.96,136.03,17.04">AI Rational at CheckThat! 2022: Using transformer models for tweet classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.02,141.37,98.39,10.80"><forename type="first">Aleksandar</forename><surname>Savchev</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sofia University</orgName>
								<address>
									<country key="BG">Bulgaria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,80.66,75.32,433.53,17.04;1,229.54,95.96,136.03,17.04">AI Rational at CheckThat! 2022: Using transformer models for tweet classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CFB84D87A42D18F8D9447C2B7995F326</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Check-worthiness</term>
					<term>COVID-19</term>
					<term>transformer models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper is an overview of the approach taken by team AI Rational in CheckThat! 2022 for Task1 in English, Bulgarian, Dutch and Turkish. Task 1 is about classifying COVID-19 tweets and has four subtasks: 1A Check-worthiness; 1B Verifiable factual claims detection; 1C Harmful tweet detection; 1D Attention-worthy tweet detection. This document will focus on the experiments done for 1A English where the team got first place out of 13 teams however the same techniques are done for the other languages and subtasks. This document will show our data preprocessing and data augmentation as well as the use of transformer models BERT, DistilBERT and RoBERTa for text classification and how we fined-tuned them for best results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In today's day and age with the mass spread of the internet, access to large quantities of information is easily achieved. With the increase of data, the amount of disinformation increases as well. People rely on trustworthy news sites where the articles are redacted and checked by publishers to satisfy their need for information. However, most people use social media sites as well where people can freely post information about different topics which is not fact-checked or can be malicious in nature. Vulnerable topics include but are not limited to finance, healthcare and politics. To combat the spread of misinformation in social media sites Task 1 in CheckThat! 2022 is about classifying tweeter tweets related to COVID-19 in English, Bulgarian, Turkish, Dutch and Arabic. Task 1 has four subtasks:  1A Check-worthiness of tweets: Given a tweet, predict whether it is worth fact-checking.  1B Verifiable factual claims detection: Given a tweet, predict whether it contains a verifiable factual claim.  1C Harmful tweet detection: Given a tweet, predict whether it is harmful to society and why.  1D Attention-worthy tweet detection: Given a tweet, predict whether it should get the attention of policy makers and why.</p><p>Models have been trained on all 4 of these subtasks in the languages: English, Bulgarian, Dutch and Turkish. However, this document will show the approach and experiments on subtask 1A English only. For the other subtasks and languages, the same approach was used.</p><p>Section 2 of this document will show the most successful models from previous editions of this task. The models used to solve the task: DistilBERT, BERT and RoBERTa. The method of fine-tuning these models. Section 2 also contains what kind of data preprocessing and data augmentation are used on the training data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Usage of transformer models 2.1. Previous successful approaches for CheckThat!</head><p>In CheckThat! 2021 the most successful participants have used different transformer models as seen in <ref type="bibr" coords="2,83.53,135.31,12.80,9.94" target="#b0">[1]</ref> such models include BERT, RoBERTa, BETO, AraBERT, BERTurk. Some teams have applied data augmentation, and most have done simple data preprocessing. For deeper research papers from different participants were analyzed. In <ref type="bibr" coords="2,250.66,160.75,12.80,9.94" target="#b1">[2]</ref> the team has tried different transformer models. For their final submission they have used BERT pre-trained on tweets and have gotten first place in English and fourth in Spanish for task 1A which is the equivalent of clef 2022 task 1A. <ref type="bibr" coords="2,402.38,185.95,12.80,9.94" target="#b6">[7]</ref> have experimented with fine-tuning pretrained models BETO and RoBERTa. They have achieved fifth and first place in English and Spanish languages on task 1A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Dataset</head><p>For task1 English the "train" corpus consists of 2122 entries. Each entry is a tweet related to COVID-19 which is labeled with 0 or 1 on whether it is worth fact-checking or not. The entries also consist of the tweet's id and URL. The organizers have also provided "dev" set of 195 entries and "dev_test" set of 574 entries.</p><p>For data preprocessing, all links in the data were replaced with "@link". Other kinds of text edits were not done since these models have their own tokenizer. For English, to increase the training set it was experimented with back translation (English tweets were translated to French and then translated back to English), tweets that remained the same were removed.</p><p>For training both entries from the "train" set and "dev" set were used, which total to 2317 entries. With back translation the train set increased to 4439 entries. For validation set the "dev_test" corpus was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Experiments done</head><p>Experiments for task1A in English were made with three different pre-trained transformer models: BERT <ref type="bibr" coords="2,102.96,463.24,11.43,9.94" target="#b2">[3]</ref>, DistilBERT <ref type="bibr" coords="2,175.87,463.24,12.80,9.94" target="#b3">[4]</ref> (a distilled version of BERT) and RoBERTa <ref type="bibr" coords="2,387.98,463.24,11.61,9.94" target="#b4">[5]</ref>. All models used have been taken from huggingface 1 . Fine-tuning of parameters was done on DistilBERT model since it is the fastest one to train. The more notable hyper parameters are number of epochs 15, warm up ratio 6% and learning ratio 3e-5. These discoveries were used when training the different models. The same settings were used for the other subtasks and languages.</p><p>For the other subtasks the same experiments were done where for English the final submissions were with RoBERTa while for the other languages XLM-RoBERTa <ref type="bibr" coords="2,378.79,539.11,12.80,9.94" target="#b5">[6]</ref> (a cross-lingual version of RoBERTa) was used. For Dutch and Turkish data augmentation was not used while for Bulgarian the test set was increased with back translation, translating Bulgarian to English and then back to Bulgarian to have more data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Results from experiments</head><p>Table <ref type="table" coords="2,114.29,647.61,5.52,9.94" target="#tab_0">1</ref> shows the results of the different experiments on task 1A English with the given train and test set from the organizers. As expected, the best results are with RoBERTa model. Data augmentation helped improve the metrics slightly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Placements</head><p>Table <ref type="table" coords="3,117.60,283.17,5.52,9.94" target="#tab_1">2</ref> shows the placements in the official leaderboards 2 . The rankings are written in format X/N where X is the place taken by the model and N is the number of participants in that subtask. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion and future work</head><p>This paper presents the experiments done with transformer models BERT, DistilBERT, RoBERTa and data augmentation technique back translation on task1A in English. The results achieved are satisfactory. For future work experiments with transformer models pre-trained on tweets is advised and experimenting with different data augmentation techniques to increase the data set may yield better results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,72.02,716.19,3.48,6.26;1,75.38,720.01,315.73,7.21;1,72.02,729.93,132.97,7.13;1,72.02,739.05,146.89,7.13;2,72.02,794.06,98.08,9.94;2,169.99,792.29,3.48,6.26"><head>1 CLEF 2022 -</head><label>12022</label><figDesc>Conference and Labs of the Evaluation Forum, September 5-8, 2022, Bologna, Italy EMAIL: alex1alex@abv.bg (A. Savchev) ORCID: 0000-0003-4626-643X (A. Savchev) https://huggingface.co1   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,72.02,74.30,421.49,122.19"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="3,72.02,87.74,421.49,108.75"><row><cell>Experiments</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Accuracy</cell><cell>F1(for positive class)</cell></row><row><cell>BERT</cell><cell>0.8118</cell><cell>0.8833</cell></row><row><cell>DistilBERT</cell><cell>0.8101</cell><cell>0.8836</cell></row><row><cell>RoBERTa</cell><cell>0.8432</cell><cell>0.9010</cell></row><row><cell>BERT + Data augmentation</cell><cell>0.8118</cell><cell>0.8838</cell></row><row><cell>DistilBERT+Data augmentation</cell><cell>0.8153</cell><cell>0.8862</cell></row><row><cell>RoBERTa + Data augmentation</cell><cell>0.8536</cell><cell>0.9070</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,72.02,333.58,430.48,95.30"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="3,72.02,347.02,430.48,81.86"><row><cell>Placements</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Subtask</cell><cell>English</cell><cell>Bulgarian</cell><cell>Dutch</cell><cell>Turkish</cell></row><row><cell>1A</cell><cell>1/13</cell><cell>3/5</cell><cell>2/5</cell><cell>-</cell></row><row><cell>1B</cell><cell>4/9</cell><cell>1/2</cell><cell>1/2</cell><cell>2/4</cell></row><row><cell>1C</cell><cell>2/10</cell><cell>1/2</cell><cell>2/2</cell><cell>3/4</cell></row><row><cell>1D</cell><cell>2/5</cell><cell>1/2</cell><cell>1/2</cell><cell>1/2</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5.">Acknowledgements</head><p>This work has been supported by professor <rs type="person">Preslav Nakov</rs>, professor <rs type="person">Ivan Koychev</rs> and assistant <rs type="person">Momchil Hardalov. Victor Kostov</rs> (worked on task 2) and <rs type="person">Kristian Zhelyazkov</rs> (worked on task 3) from the AI Rational team helped this work as well. All the before mentioned people are from <rs type="institution">Sofia University</rs></p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="4,93.41,119.71,429.42,9.94;4,93.41,132.19,430.06,9.94;4,93.41,144.91,429.58,9.94;4,93.41,157.63,429.65,9.94;4,93.41,170.11,221.41,9.94" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Shaden</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maram</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bayan</forename><surname>Hamdan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zien</forename><surname>Sheikh Ali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fatima</forename><surname>Haouari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mucahid</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yavuz</forename><surname>Selim Kartal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Firoj</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rubén</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Javier</forename><surname>Míguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tamer</forename><surname>Beltrán</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Preslav</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Nakov</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-28.pdf" />
		<title level="m" coord="4,447.34,144.91,75.65,9.94;4,93.41,157.63,429.65,9.94;4,93.41,170.11,35.23,9.94">Overview of the CLEF-2021 CheckThat! Lab Task 1 on Check-Worthiness Estimation in Tweets and Political Debates</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,93.41,182.83,429.93,9.94;4,93.41,195.55,429.39,9.94;4,93.41,208.29,183.23,9.94" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Juan</forename><surname>Martinez-Rico</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lourdes</forename><surname>Martinez-Romo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nlp&amp;ir</forename><surname>Araujo</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-44.pdf" />
		<title level="m" coord="4,477.89,182.83,45.45,9.94;4,93.41,195.55,51.87,9.94;4,174.71,195.55,348.09,9.94">Check-worthiness estimation and fake news detection using transformer models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>UNED at CheckThat!</note>
</biblStruct>

<biblStruct coords="4,93.41,220.77,429.90,9.94;4,93.41,233.49,412.37,9.94" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="4,430.97,220.77,92.34,9.94;4,93.41,233.49,248.44,9.94">Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1810.04805.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,93.41,246.21,429.43,9.94;4,93.41,258.69,361.72,9.94" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1910.01108.pdf" />
		<title level="m" coord="4,385.21,246.21,137.64,9.94;4,93.41,258.69,197.38,9.94">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,93.41,271.65,429.82,9.94;4,93.41,284.37,429.72,9.94;4,93.41,296.88,207.08,9.94" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roberta</forename></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1907.11692.pdf" />
		<title level="m" coord="4,343.03,284.37,180.10,9.94;4,93.41,296.88,43.42,9.94">A Robustly Optimized BERT Pretraining Approach</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,93.41,309.49,429.61,10.04;4,93.41,322.32,429.99,9.94;4,93.41,335.04,187.48,9.94" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="4,279.73,322.32,243.66,9.94;4,93.41,335.04,23.85,9.94">Unsupervised Cross-lingual Representation Learning at Scale</title>
		<author>
			<persName coords=""><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francisco</forename><surname>Guzmὰn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1911.02116.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="4,93.41,347.52,429.97,9.94;4,93.41,360.24,250.21,9.94" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Robiert</forename><surname>Sepúlveda-Torres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Estela</forename><surname>Saquete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gplsi</forename></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/paper-52.pdf" />
		<title level="m" coord="4,318.32,347.52,88.54,9.94;4,439.17,347.52,84.22,9.94;4,93.41,360.24,64.61,9.94">Fine-tuning BETO and RoBERTa</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>team at CheckThat!</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
