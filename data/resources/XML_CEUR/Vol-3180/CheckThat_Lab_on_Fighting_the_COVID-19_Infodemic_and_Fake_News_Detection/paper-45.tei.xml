<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,365.88,15.42;1,89.29,106.66,348.65,15.42;1,89.29,128.58,76.99,15.43">COURAGE at CheckThat! 2022: Harmful Tweet Detection using Graph Neural Networks and ELECTRA</title>
				<funder ref="#_hg8pD79">
					<orgName type="full">COURAGE -A</orgName>
				</funder>
				<funder>
					<orgName type="full">Volkswagen Foundation in the topic Artificial Intelligence and the Society of the Future</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,156.89,102.34,11.96"><forename type="first">Francesco</forename><surname>Lomonaco</surname></persName>
							<email>f.lomonaco5@campus.unimib.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica</orgName>
								<orgName type="institution" key="instit1">Università degli Studi di Milano Bicocca</orgName>
								<orgName type="institution" key="instit2">Sistemistica e Comunicazione</orgName>
								<address>
									<postCode>20126</postCode>
									<settlement>Milano</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,204.27,156.89,91.17,11.96"><forename type="first">Gregor</forename><surname>Donabauer</surname></persName>
							<email>gregor.donabauer@ur.de</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica</orgName>
								<orgName type="institution" key="instit1">Università degli Studi di Milano Bicocca</orgName>
								<orgName type="institution" key="instit2">Sistemistica e Comunicazione</orgName>
								<address>
									<postCode>20126</postCode>
									<settlement>Milano</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Information Science</orgName>
								<orgName type="institution">University of Regensburg</orgName>
								<address>
									<postCode>93053</postCode>
									<settlement>Regensburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,331.72,156.89,58.76,11.96"><forename type="first">Marco</forename><surname>Siino</surname></persName>
							<email>marco.siino@unipa.it</email>
							<affiliation key="aff1">
								<orgName type="department">Dipartimento di Ingegneria</orgName>
								<orgName type="institution">Università degli Studi di Palermo</orgName>
								<address>
									<postCode>90128</postCode>
									<settlement>Palermo</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,365.88,15.42;1,89.29,106.66,348.65,15.42;1,89.29,128.58,76.99,15.43">COURAGE at CheckThat! 2022: Harmful Tweet Detection using Graph Neural Networks and ELECTRA</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">05912E4E4A6002731849DB3AA85E3E41</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>graph classification</term>
					<term>harmful tweets</term>
					<term>Twitter</term>
					<term>ELECTRA</term>
					<term>COVID-19</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we propose a deep learning model based on graph machine learning (i.e. Graph Attention Convolution) and a pretrained transformer language model (i.e. ELECTRA). Our model was developed to detect harmful tweets about COVID-19 and was used to tackle subtask 1C (harmful tweet detection) at the CheckThat!Lab shared task organized as part of CLEF 2022. In this binary classification task, our proposed model reaches a binary F1 score (positive class label, i.e. harmful tweet) of 0.28 on the test set. We demonstrate that our approach outperforms the official baseline by 8% and describe our model as well as the experimental setup and results in detail. We also refer to limitations of the approach and future research directions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Along the COVID-19 outbreak the spread of misleading information online related to news on the pandemic could be observed, for example on social media. The three tasks proposed for the CheckThat! Lab@CLEF2022 <ref type="bibr" coords="1,233.72,465.46,11.32,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,247.76,465.46,8.93,10.91" target="#b1">2]</ref> aim at addressing related issues, namely: (1) Identifying relevant claims in tweets, (2) Detecting previously fact checked claims and (3) Fake news detection. All tasks are framed as classification problems and build on collective effort to tackle the COVID-19 related infodemic.</p><p>The aim of this paper is to propose a model to address the first task <ref type="bibr" coords="1,391.79,519.66,11.27,10.91" target="#b2">[3]</ref>. Furthermore, this task includes four different subtasks. Subtask 1A is about determining check-worthiness of tweets (i.e. given a tweet, predict whether it is worth fact-checking). Subtask 1B is about verifiable factual claims detection: given a tweet, predict whether it contains a verifiable factual claim. In Subtask 1C the focus is on harmful tweet detection: given a tweet, predict whether it is harmful to the society and why. Finally, Subtask 1D is related to attention-worthy tweet detection: given a tweet, predict whether it should get the attention of policy makers and why. This task is defined with eight class labels. For the subtasks 1A and 1C the official evaluation metric is the binary F1 score with respect to the positive class, for subtask 1B the metric used is the accuracy and for subtask 1D the metric used is the weighted F1 score.</p><p>We present our model proposed for Subtask 1C -English language. The model takes advantage of an ELECTRA-based document embedding as well as a text graph that is processed using a Graph Convolutional Network (GCN). Our goal is to introduce a novel method that can handle different types of heterogeneous textual or social information. We show how a first version of such a model performs on the proposed task, leaving room for improvements on future research in the domain. To support reproducibility and future research directions we make our Code publicly available <ref type="foot" coords="2,167.20,220.70,3.71,7.97" target="#foot_0">1</ref> .</p><p>This paper is organized as follows: in Section 2 we present some related work about the usage of deep learning methods for similar text classification tasks. In Section 3 we describe our approach in detail, explaining our choices and the configuration of each layer of the model. In Section 5 we report the results we obtain on the official test set. In Section 6 we discuss some interesting future directions to investigate before closing our work with concluding remarks in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Motivated by the notable performances reached in various text classification tasks, where deep AI models <ref type="bibr" coords="2,139.46,375.93,11.48,10.91" target="#b3">[4,</ref><ref type="bibr" coords="2,154.52,375.93,9.03,10.91" target="#b4">5]</ref> outperformed classic techniques used in natural language processing (e.g. Bayes, Decision Tree, K-Nearest Neighbour, Support Vector Machine) as also reported in <ref type="bibr" coords="2,481.86,389.48,11.29,10.91" target="#b5">[6,</ref><ref type="bibr" coords="2,495.89,389.48,7.53,10.91" target="#b6">7]</ref>, we decided to use a deep learning-based approach for our submission.</p><p>Our proposed model is based on both, a transformer-based document embedding and a GCN (i.e. Graph Attention Convolution) applied to a text graph representing the structure of each tweet. The transformer-based embedding we used is ELECTRA a language model presented by <ref type="bibr" coords="2,103.32,457.22,11.59,10.91" target="#b7">[8]</ref>. ELECTRA does not mask the input as BERT but replaces some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, a discriminative model is trained to predict whether each token in the corrupted input is replaced by a generator sample or not.</p><p>GCNs take advantage of graph data structures that are made of vertices and edges (also called nodes and link). Nowadays, graphs are used in many real-world applications, including traffic prediction <ref type="bibr" coords="2,137.56,538.52,11.43,10.91" target="#b8">[9]</ref>, computer vision <ref type="bibr" coords="2,230.61,538.52,16.25,10.91" target="#b9">[10]</ref>, social networks <ref type="bibr" coords="2,326.12,538.52,16.43,10.91" target="#b10">[11,</ref><ref type="bibr" coords="2,345.27,538.52,14.03,10.91" target="#b11">12]</ref> and many more.</p><p>GCNs for text classification are discussed in <ref type="bibr" coords="2,298.27,552.07,16.40,10.91" target="#b12">[13]</ref>: the authors propose a novel graph neural network method and model a whole corpus as a heterogeneous graph to learn word and document embeddings with graph neural networks jointly. Results on several benchmark datasets demonstrate that the proposed method outperforms state-of-the-art text classification methods, without using pre-trained word embeddings or external knowledge. The model proposed also learns predictive word and document embeddings automatically.</p><p>In <ref type="bibr" coords="2,112.77,633.36,18.07,10.91" target="#b13">[14]</ref> a more sophisticated approach is proposed. In particular, the authors discussed a flexible Heterogeneous Information Network (HIN) for modeling short texts. The model can integrate any type of additional information as well as capturing their relations to address the semantic sparsity. Additionally, the authors propose heterogeneous graph attention networks to embed the HIN for short text classification based on a dual-level attention mechanism, including node-level and type-level attention. The attention mechanism can learn the importance of different neighboring nodes as well as the importance of different node (information) types to a current node.</p><p>A broader overview over a range of text classification applications using GCNs is given in <ref type="bibr" coords="3,89.29,181.81,16.25,10.91" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed model</head><p>The model architecture with input and output shapes of each layer is shown in Figure <ref type="figure" coords="3,473.50,239.92,5.06,10.91" target="#fig_0">1</ref> along with parameter distributions of each layer. The proposed model is composed by two modules:</p><p>• Graph creation and embedding • Pretrained document embedding Geometric deep learning <ref type="bibr" coords="3,213.52,656.03,16.46,10.91" target="#b15">[16,</ref><ref type="bibr" coords="3,232.72,656.03,14.05,10.91" target="#b16">17]</ref> has led to a growing number of new architectures as well as novel applications, including text modelling <ref type="bibr" coords="3,303.54,669.58,16.41,10.91" target="#b17">[18]</ref>. The representation of each tweet into a graph starts with text preprocessing and Part Of Speech (POS) tagging. After these steps each unique tagged word in the tweet corresponds to a node in the graph and the adjacency matrix is populated connecting each node with all words in a window equal to 3. Each node is annotated with various features discussed in 3.2. The proposed architecture is composed of two graph attention convolution (i.e. GATV2Conv) layers proposed by <ref type="bibr" coords="4,354.34,141.16,16.29,10.91" target="#b18">[19]</ref>; the node-wise representation outputted by the GATV2Conv layers is passed to a max pooling operator and a dropout layer. The output is then concatenated with the document embedding generated using ELECTRA <ref type="bibr" coords="4,492.54,168.26,11.34,10.91" target="#b7">[8]</ref>. Finally, two dense layers and a rectified linear unit (ReLu) activation function between them output the predictions of the model for each class.</p><p>Before discussing the network architecture as well as our hyperparameter settings, we want to mention that each split of the dataset (training and test per language) consists of individual tweets and their corresponding labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Graph creation</head><p>The graph module takes as input a raw sample (tweet) and outputs the tweet represented as an undirected, attributed graph. In Figure <ref type="figure" coords="4,268.04,299.28,5.17,10.91" target="#fig_1">2</ref> each step of the preprocessing pipeline is depicted. Our custom preprocessing function uses the python NLTK package <ref type="bibr" coords="4,392.23,312.83,16.27,10.91" target="#b19">[20]</ref>. Below we list all the preprocessing steps involved:</p><p>• Lowercasing. This step is used to get the same embedding e.g. for the words Hello and hello • Removing Stopwords. Stopwords are generally speaking used with high frequency but they are in many cases not really informative, e.g. preposition and articles belong to this category. • POS Tagging. In this step each word in the tweet is classified into its parts of speech class and labelled accordingly using a one-hot encoding. These vectors correspond to the respective POS tag out of all 43 POS classes in the NLTK package. • URL removing. All URLs in each tweet have been removed.</p><p>• Hashtag symbol and tagged accounts. All hashtag symbols have been removed along with tagged users. Starting from the output of POS-tagging we adopt a strategy that associates an edge to each word with all words in a window equal to 3. If a word is repeated more then once, only the first occurrence is considered as node while edges are updated accordingly. Edges are unweighted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Node characterisation</head><p>As mentioned above in Figure <ref type="figure" coords="5,220.78,163.79,4.97,10.91" target="#fig_0">1</ref> each node is characterized as a 815-dimensional vector. The first 768 features correspond to the pretrained ELECTRA document embedding, obtained using FLAIR, applying the introduced preprocessing steps<ref type="foot" coords="5,286.58,189.14,3.71,7.97" target="#foot_1">2</ref>  <ref type="bibr" coords="5,290.78,190.89,16.14,10.91" target="#b20">[21]</ref>. To select the best embeddings we evaluated different transformer-based models using the tweet text data as input. The official evaluation metric was used during this experiment and it turned out that ELECTRA outperformed other pretrained language models. Using ELECTRA we collected both word embedding for each node in the graph as well as document embedding for the whole tweet. Each node was also annotated with the corresponding one-hot-encoded vector of its POS tag (45 features). Given the fact that graph networks are order invariant w.r.t the nodes processed during message passing the order of words in the original tweet is lost. To maintain this information we characterized each node with a feature vector of two dimensions that encodes the distance from the origin of each node (word) in the graph using sine and cosine positional encoding of a transformer model <ref type="bibr" coords="5,463.53,312.83,16.09,10.91" target="#b21">[22]</ref>. This vector is concatenated to the other node features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Graph attention convolution and max pooling layer</head><p>In our model we use two GATV2Conv <ref type="bibr" coords="5,255.20,376.11,17.76,10.91" target="#b18">[19]</ref> layers. This layer is characterized by the computation of dynamic attention scores. Moreover we adopt multiple heads in the first layer where the number of heads is set to four, because (as demonstrated previously by <ref type="bibr" coords="5,424.86,403.21,17.11,10.91" target="#b22">[23]</ref>) the learning process can benefit from employing multi-head attention and concatenating their outputs. As highlighted in Figure <ref type="figure" coords="5,186.12,430.31,5.08,10.91" target="#fig_0">1</ref> the number of features used to represent each node is halved between the 2 layers. The output of the graph attention layer is a 2D matrix with shape: Number of nodes (𝑑) * Number of features (𝑁 ). The maximum value is calculated along the dimension of size 𝑁 , in fact reducing the dimension of the input tensor by one. As an example consider the following matrix 𝑋. Providing 𝑋 as input to a 1D-max pooling layer returns the following array 𝑌 as output, where each 𝑦 𝑖 is computed taking the maximum of all the values along the 𝑖-th column of the matrix 𝑋.</p><formula xml:id="formula_0" coords="5,234.54,626.97,126.20,12.44">𝑌 = [︀ 𝑦 1 𝑦 2 𝑦 3 . . . 𝑦 𝑛 ]︀</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Dense</head><p>The max pooling layer's output is concatenated with the tweet embedding obtained from ELECTRA. This vector is fully connected to a dense layer which is followed by a rectified linear unit function element-wise (e.g., 𝑅𝑒𝑙𝑢(𝑥) = 𝑚𝑎𝑥(0, 𝑥)) and finally a dense layer with two units as output. This float values correspond to the softmax logits, a vector of raw (non-normalized) predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental setup</head><p>We participated in Subtask C (harmful tweet detection) of CheckThat!'s Task 1 for the English language. Before addressing experimental setup and model training we briefly describe the provided dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>The corpus includes a list of tweets that are either labeled as harmful <ref type="bibr" coords="6,396.76,297.18,11.48,10.91" target="#b0">(1)</ref> or not (0). In addition the ID of the tweets and its urls are available. All samples are related to COVID-19. In general, a train, development and dev-test set are provided as well as an official test set that was used for evaluation of the submissions. While for the first three dataset splits the gold labels were available, those of the official test set were held out till the end of the evaluation phase. In general, the number of samples for all parts of the dataset are distributed as shown in table <ref type="table" coords="6,500.12,364.93,3.77,10.91" target="#tab_1">1</ref>.</p><p>The data were released as multiple tab-separated files (one per split) Dataset statistics of all provided splits for English.</p><p>An exploratory analysis shows the dataset imbalance with respect to the class labels. For the training set the positive class samples correspond to only 8% of the total entries. Using the Tweet IDs provided we crawled twitter data via the official Twitter API <ref type="foot" coords="6,400.28,535.60,3.71,7.97" target="#foot_2">3</ref> . However, only a small subset of the samples (w.r.t. the training set only 20% of the original tweets) was still available as the rest of this information was already deleted by Twitter. Given this observation, we choose to discard including social context information such as the number of tweet interaction (favourites, shares), author features (follower following relationships as well as user timeline tweets). Besides using the graph based approach introduced in Section 1 we performed further experiments on the dataset featuring transformer based methods as well as alternative graph construction techniques. We present details on the results of these experiments in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model training</head><p>The hyperparameter settings are in line with many of the decisions made in a study conducted in <ref type="bibr" coords="7,100.76,121.08,17.81,10.91" target="#b23">[24]</ref> and used to subsequently fine-tune our proposed model. To initialize the weights of the model we used a Glorot uniform initializer <ref type="bibr" coords="7,284.40,134.63,16.38,10.91" target="#b24">[25]</ref>. The model was compiled using binary cross entropy loss; this function calculates the loss with respect to two classes (i.e., 0 and 1) as defined in 1.</p><formula xml:id="formula_1" coords="7,136.56,185.55,369.42,33.58">𝐿𝑜𝑠𝑠 𝐵𝐶𝐸 = - 1 𝑁 𝑁 ∑︁ 𝑛=1 [𝑦 𝑛 × log (ℎ 𝜃 (𝑥 𝑛 )) + (1 -𝑦 𝑛 ) × log (1 -ℎ 𝜃 (𝑥 𝑛 ))]<label>(1)</label></formula><p>where:</p><p>• N is the number of training examples;</p><p>• 𝑦 𝑛 is the target for the training sample 𝑛;</p><p>• 𝑥 𝑛 is the input sample 𝑛;</p><p>• ℎ 𝜃 is the neural network model with weights 𝜃.</p><p>To improve the model performance and counteract class imbalance we set up class weights that correspond to a manual rescaling weight assigned to each class. Optimization is performed using the Adam optimizer <ref type="bibr" coords="7,212.46,337.51,16.41,10.91" target="#b25">[26]</ref>. To reduce overfitting we take advantage of a learning rate scheduler reducing the learning rate by a factor of 0.9 with a step size of 25 epochs. The model architecture is depicted in figure <ref type="figure" coords="7,234.40,364.61,3.68,10.91" target="#fig_0">1</ref>, where the number of the various network hyperparameters are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Baseline</head><p>The organizers provide official baseline results based on random predictions. The metric to evaluate the task is the binary F1 score of the positive class label (harmful tweet). The official baseline result amounts to 0.200 binary F1 on the evaluation test set. We compare the baseline results to the values our approaches did achieve in Table <ref type="table" coords="7,343.63,498.16,3.74,10.91" target="#tab_2">2</ref>.</p><p>We established an additional, strong baseline based on a fine-tuned transformer model. We evaluated different transformer architectures including BERT, RoBERTa and ELECTRA regarding the classification task. It turned out that ELECTRA achieves the best results among these models (using 3 epochs for fine-tuning as recommended by <ref type="bibr" coords="7,388.35,552.35,15.88,10.91" target="#b26">[27]</ref>). The performance of this approach amounts to 0.250 binary F1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Our approach</head><p>Our main experiments are based on the model proposed in Section 3. We will report results on the test set used for evaluation using the official binary F1 metric, as well as binary precision and binary recall of the positive class label.</p><p>As presented in Table <ref type="table" coords="7,200.56,656.03,5.15,10.91" target="#tab_2">2</ref> our submitted approach (GCN+ELECTRA) outperforms the official baseline by 8%. The official baseline approach generates class labels in random order. Compared to the performance of our own baseline using ELECTRA, the GCN+ELECTRA outperforms this approach by 3%. We also evaluated a ELECTRA fine-tuning setup using 50 epochs resulting in a performance almost as good as the finally submitted approach. However, the high number of epochs lead to strong overfitting on the training data.</p><p>In addition we report results obtained by experimental setups that we evaluated as part of the development process of the submitted approach. In Table <ref type="table" coords="8,336.10,305.62,4.97,10.91" target="#tab_2">2</ref> GCN+POS w/o word embeddings refers to a setting where we omit word embeddings and represent graph nodes by only considering one-hot encoded POS-tag vectors. In the GCN+3-gram-ELECTRA model we characterize graph nodes by mean-pooled word embeddings of 3 subsequent words at each position. Thus we also wanted to take into account word orders that can be lost during graph convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and future work</head><p>All other approaches used during our experiments result in a lower performance compared to the actually submitted model. This observation strengthens our decision of choosing to submit predictions generated with the model proposed in this paper as it outperforms a range of other setups we used during our experiments. Obviously, we can report a high recall score compared to low precision using our proposed model as it tends to frequently predict the positive class label and only a few times the negative one.</p><p>As already mentioned above, the inclusion of social and user information in the model (as in <ref type="bibr" coords="8,89.29,513.29,16.88,10.91" target="#b27">[28]</ref>) can improve the classification accuracy. Deleted tweets that can not be crawled anymore limit the usage of those information in the actual training set.</p><p>In future work, it would be interesting to analyze the contribution of such social context features. In addition, different types of embeddings (stacked or pooled) could be compared regarding the characterization of each node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we describe our approach to Subtask-1C at CheckThat!Lab@CLEF2022 based on graph data structures and transformer language models. The proposed hybrid solution of a text graph and pretrained document emebddings should be studied in more detail as it leads to improvements over fine-tuning transformer-based models as demonstrated on the given dataset. In addition there is room left for including additional types of information (e.g. social media context) in these data structures which could be helpful in solving other tasks. Overall, as reported by the organizers, our approach -achieving an F1-score of the positive class of 0.280 ranked eight in the CheckThat!Lab@CLEF2022 -Subtask-1C.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,603.57,416.69,8.93;3,89.29,615.57,416.69,8.87;3,89.29,627.53,310.69,8.87;3,180.00,314.55,235.28,281.59"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Model parameters (Top) numbers in brackets indicate parameters' tensor dimensions; last column indicates the number of parameters in each layer. Model architecture (bottom) model input and output shapes in each layer (figure taken from our Google Colab notebook).</figDesc><graphic coords="3,180.00,314.55,235.28,281.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,637.52,416.69,8.93;4,89.29,649.53,31.05,8.87;4,500.89,661.86,2.19,8.87;4,180.00,521.49,235.27,109.45"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Graph representation: each tweet is represented as a graph after pre-processing and POS tagging .</figDesc><graphic coords="4,180.00,521.49,235.27,109.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,220.76,507.62,153.76,58.56"><head></head><label></label><figDesc>𝑥 11 𝑥 12 𝑥 13 . . . 𝑥 1𝑛 𝑥 21 𝑥 22 𝑥 23 . . . 𝑥 2𝑛</figDesc><table coords="5,220.76,507.62,153.76,58.56"><row><cell></cell><cell>⎡</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>⎤</cell></row><row><cell>𝑋 =</cell><cell>⎢ ⎢ ⎢ ⎣</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell><cell>⎥ ⎥ ⎥ ⎦</cell></row><row><cell></cell><cell cols="6">𝑥 𝑑1 𝑥 𝑑2 𝑥 𝑑3 . . . 𝑥 𝑑𝑛</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.99,403.37,328.25,76.20"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="6,178.03,403.37,239.21,58.33"><row><cell>Dataset</cell><cell cols="3">Number of Samples Label 0 Label 1</cell></row><row><cell>Train</cell><cell>3323</cell><cell>3031</cell><cell>292</cell></row><row><cell>Development</cell><cell>307</cell><cell>276</cell><cell>31</cell></row><row><cell>Dev-Test</cell><cell>910</cell><cell>828</cell><cell>82</cell></row><row><cell>Test</cell><cell>251</cell><cell>211</cell><cell>40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,88.93,86.37,417.05,124.81"><head>Table 2</head><label>2</label><figDesc>Results (binary Precision, Recall and F1 of the positive class label) on the official test set for English with respect to different approaches.</figDesc><table coords="8,120.32,86.37,354.63,83.04"><row><cell>Approach</cell><cell cols="3">Binary Precision Binary Recall Binary F1</cell></row><row><cell>Baseline</cell><cell>0.200</cell><cell>0.200</cell><cell>0.200</cell></row><row><cell>GCN+3-gram-ELECTRA</cell><cell>0.138</cell><cell>0.625</cell><cell>0.226</cell></row><row><cell>ELECTRA (3 epochs)</cell><cell>0.263</cell><cell>0.250</cell><cell>0.256</cell></row><row><cell>GCN+POS w/o embeddings</cell><cell>0.166</cell><cell>0.650</cell><cell>0.264</cell></row><row><cell>ELECTRA (50 epochs)</cell><cell>0.275</cell><cell>0.275</cell><cell>0.275</cell></row><row><cell>GCN+ELECTRA</cell><cell>0.166</cell><cell>0.875</cell><cell>0.280</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,671.03,208.63,8.97"><p>https://github.com/sagacemente/CLEF2022CheckThat.git</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="5,108.93,670.99,233.24,8.97"><p>Documentation available here: https://github.com/flairNLP/flair</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,108.93,671.00,309.99,8.97"><p>API Documentation available here: https://developer.twitter.com/en/docs/twitter-api</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thanks reviewers for precious insight to increase readability of the paper and remove typos. This work has been partially founded by <rs type="funder">COURAGE -A</rs> <rs type="programName">social media companion safeguarding and educating students</rs> (no. <rs type="grantNumber">95567</rs>), funded by the <rs type="funder">Volkswagen Foundation in the topic Artificial Intelligence and the Society of the Future</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_hg8pD79">
					<idno type="grant-number">95567</idno>
					<orgName type="program" subtype="full">social media companion safeguarding and educating students</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,112.66,285.51,394.53,10.91;9,112.33,299.06,394.85,10.91;9,112.66,312.61,393.33,10.91;9,112.66,326.16,394.52,10.91;9,112.66,339.71,393.33,10.91;9,112.28,353.26,394.91,10.91;9,112.66,366.81,89.12,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,432.00,312.61,73.99,10.91;9,112.66,326.16,390.20,10.91">Overview of the CLEF-2022 CheckThat! lab on fighting the COVID-19 infodemic and fake news detection</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Struß</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Míguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Kartal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Beltrán</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Köhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,126.92,339.71,379.07,10.91;9,112.28,353.26,390.55,10.91">Proceedings of the 13th International Conference of the CLEF Association: Information Access Evaluation meets Multilinguality, Multimodality, and Visualization, CLEF &apos;2022</title>
		<meeting>the 13th International Conference of the CLEF Association: Information Access Evaluation meets Multilinguality, Multimodality, and Visualization, CLEF &apos;2022<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,380.36,394.53,10.91;9,112.33,393.91,394.85,10.91;9,112.66,407.46,393.32,10.91;9,112.66,421.01,394.53,10.91;9,112.66,434.55,395.17,10.91;9,112.66,448.10,193.11,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,272.14,407.46,233.84,10.91;9,112.66,421.01,154.75,10.91">The clef-2022 checkthat! lab on fighting the covid-19 infodemic and fake news detection</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Struß</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Míguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Kartal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Beltrán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,272.10,434.55,151.66,10.91">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Hagen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Verberne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Seifert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Balog</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Nørvåg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Setty</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="416" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,461.65,394.52,10.91;9,112.14,475.20,393.85,10.91;9,112.66,488.75,393.33,10.91;9,112.66,502.30,394.52,10.91;9,112.66,515.85,47.34,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,451.89,475.20,54.10,10.91;9,112.66,488.75,333.06,10.91">Overview of the CLEF-2022 CheckThat! lab task 1 on identifying relevant claims in tweets</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Míguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Kartal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Beltrán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,467.81,488.75,38.18,10.91;9,112.66,502.30,349.04,10.91">Working Notes of CLEF 2022-Conference and Labs of the Evaluation Forum, CLEF &apos;2022</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,529.40,393.32,10.91;9,112.66,542.95,393.82,10.91;9,112.66,556.50,212.77,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,320.75,529.40,185.23,10.91;9,112.66,542.95,130.87,10.91">Detection of hate speech spreaders using convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Siino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Di Nuovo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ilenia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">La</forename><surname>Cascia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,265.62,542.95,240.86,10.91;9,112.66,556.50,21.05,10.91">PAN 2021 Profiling Hate Speech Spreaders on Twitter@ CLEF</title>
		<imprint>
			<publisher>CEUR</publisher>
			<date type="published" when="2021">2936. 2021</date>
			<biblScope unit="page" from="2126" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,570.05,393.33,10.91;9,112.66,583.60,395.17,10.91;9,112.66,597.15,394.53,10.91;9,112.28,610.69,216.04,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,280.44,570.05,225.55,10.91;9,112.66,583.60,352.53,10.91">McRock at SemEval-2022 Task 4: Patronizing and Condescending Language Detection using Multi-Channel CNN and DistilBERT</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Siino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>La Cascia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Tinnirello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,488.58,583.60,19.25,10.91;9,112.66,597.15,389.70,10.91">Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)</title>
		<meeting>the 16th International Workshop on Semantic Evaluation (SemEval-2022)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,624.24,395.17,10.91;9,112.66,637.79,244.76,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,223.22,624.24,247.47,10.91">Review of text classification methods on deep learning</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,480.36,624.24,27.47,10.91;9,112.66,637.79,150.68,10.91">CMC-Computers, Materials &amp; Continua</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="1309" to="1321" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,651.34,393.33,10.91;10,112.26,86.97,393.93,10.91;10,112.66,100.52,107.04,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,257.56,651.34,248.43,10.91;10,112.26,86.97,199.87,10.91">Classifying tweets using convolutional neural networks with multi-channel distributed representation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hashida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,320.53,86.97,185.66,10.91;10,112.66,100.52,33.25,10.91">IAENG International Journal of Computer Science</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="68" to="75" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,114.06,393.33,10.91;10,112.66,127.61,347.38,10.91" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m" coord="10,335.14,114.06,170.85,10.91;10,112.66,127.61,165.13,10.91">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,141.16,395.17,10.91;10,112.66,154.71,289.92,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01926</idno>
		<title level="m" coord="10,252.12,141.16,255.71,10.91;10,112.66,154.71,107.88,10.91">Diffusion convolutional recurrent neural network: Datadriven traffic forecasting</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,168.26,395.17,10.91;10,112.66,181.81,395.17,10.91;10,112.66,195.36,394.52,10.91;10,112.66,208.91,90.72,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,261.82,168.26,246.02,10.91;10,112.66,181.81,277.34,10.91">Graph neural network (gnn) in image and video understanding using deep learning for computer vision applications</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pradhyumna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Shreya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,413.12,181.81,94.71,10.91;10,112.66,195.36,363.55,10.91">2021 Second International Conference on Electronics and Sustainable Communication Systems (ICESC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1183" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,222.46,393.33,10.91;10,112.66,236.01,393.33,10.91;10,112.66,249.56,189.68,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,230.39,222.46,275.60,10.91;10,112.66,236.01,79.44,10.91">Supervised random walks: predicting and recommending links in social networks</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Backstrom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,215.71,236.01,290.28,10.91;10,112.66,249.56,101.49,10.91">Proceedings of the fourth ACM international conference on Web search and data mining</title>
		<meeting>the fourth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="635" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,263.11,393.73,10.91;10,112.66,276.66,393.33,10.91;10,112.66,290.20,272.03,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,279.25,263.11,227.14,10.91;10,112.66,276.66,235.00,10.91">Whosnext: Recommending twitter users to follow using a spreading activation network based approach</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Siino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>La Cascia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Tinnirello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,371.10,276.66,134.89,10.91;10,112.66,290.20,166.48,10.91">2020 International Conference on Data Mining Workshops (ICDMW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="62" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,303.75,393.33,10.91;10,112.66,317.30,360.96,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,209.52,303.75,222.71,10.91">Graph convolutional networks for text classification</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,452.94,303.75,53.05,10.91;10,112.66,317.30,212.36,10.91">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7370" to="7377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,330.85,395.17,10.91;10,112.66,344.40,393.32,10.91;10,112.66,357.95,393.33,10.91;10,112.66,371.50,306.64,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,282.58,330.85,225.26,10.91;10,112.66,344.40,152.03,10.91">Heterogeneous graph attention networks for semisupervised short text classification</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Linmei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,287.24,344.40,218.75,10.91;10,112.66,357.95,393.33,10.91;10,112.66,371.50,207.85,10.91">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4821" to="4830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,385.05,393.33,10.91;10,112.66,398.60,328.88,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,445.36,385.05,60.62,10.91;10,112.66,398.60,214.17,10.91">Graph neural networks: A review of methods and applications</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,335.08,398.60,37.73,10.91">AI Open</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="57" to="81" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,412.15,394.61,10.91;10,112.66,425.70,355.64,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,397.17,412.15,110.11,10.91;10,112.66,425.70,125.75,10.91">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,246.60,425.70,147.91,10.91">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,439.25,394.52,10.91;10,112.66,452.79,173.79,10.91" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="10,214.39,439.25,287.96,10.91">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,466.34,393.32,10.91;10,112.66,479.89,321.63,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,448.31,466.34,57.68,10.91;10,112.66,479.89,193.61,10.91">Graph-based siamese network for authorship verification</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Embarcadero-Ruiz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gómez-Adorno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Embarcadero-Ruiz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sierra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,314.29,479.89,57.26,10.91">Mathematics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">277</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,493.44,393.33,10.91;10,112.66,506.99,107.17,10.91" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yahav</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14491</idno>
		<title level="m" coord="10,236.75,493.44,197.03,10.91">How attentive are graph attention networks?</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,520.54,393.33,10.91;10,112.66,534.09,76.23,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,197.73,520.54,155.18,10.91">Nltk: the natural language toolkit</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,364.44,520.54,141.55,10.91;10,112.66,534.09,46.58,10.91">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,547.64,393.33,10.91;10,112.66,561.19,393.32,10.91;10,112.28,574.74,394.91,10.91;10,112.66,588.29,70.43,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="10,413.84,547.64,92.15,10.91;10,112.66,561.19,151.61,10.91">Flair: An easy-to-use framework for state-of-the-art nlp</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schweter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,287.12,561.19,218.86,10.91;10,112.28,574.74,390.09,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="54" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,601.84,395.17,10.91;10,112.66,615.39,393.33,10.91;10,112.33,628.93,29.19,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="10,484.04,601.84,23.79,10.91;10,112.66,615.39,143.41,10.91">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,264.71,615.39,228.49,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Polosukhin</note>
</biblStruct>

<biblStruct coords="10,112.66,642.48,393.33,10.91;10,112.66,656.03,221.81,10.91" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m" coord="10,433.05,642.48,72.94,10.91;10,112.66,656.03,39.20,10.91">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,669.58,393.32,10.91;11,112.66,86.97,195.18,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="10,227.00,669.58,278.99,10.91;11,112.66,86.97,78.84,10.91">Mb-courage@ exist: Gcn classification for sexism identification in social networks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wilkens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ognibene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,200.32,86.97,75.60,10.91">IberLEF@ EXIST</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,100.52,339.24,10.91" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="11,142.69,100.52,106.81,10.91">Layer weight initializers</title>
		<author>
			<persName coords=""><surname>Keras</surname></persName>
		</author>
		<ptr target="https://keras.io/api/layers/initializers/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,114.06,395.01,10.91" xml:id="b25">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m" coord="11,227.57,114.06,157.91,10.91">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,127.61,393.33,10.91;11,112.66,141.16,395.01,10.91;11,112.66,154.71,167.31,10.91" xml:id="b26">
	<monogr>
		<title level="m" type="main" coord="11,352.90,127.61,153.09,10.91;11,112.66,141.16,186.90,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1810.04805</idno>
		<ptr target="https://arxiv.org/abs/1810.04805.doi:10.48550/ARXIV.1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,168.26,393.54,10.91;11,112.66,181.81,393.98,10.91;11,112.41,195.36,32.84,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="11,292.65,168.26,213.55,10.91;11,112.66,181.81,234.29,10.91">Improving cyberbullying detection using twitter users&apos; psychological features and machine learning</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">R</forename><surname>Arabnia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,358.85,181.81,102.86,10.91">Computers &amp; Security</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page">101710</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
