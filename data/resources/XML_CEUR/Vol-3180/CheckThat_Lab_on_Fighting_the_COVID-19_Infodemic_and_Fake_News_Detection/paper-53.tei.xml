<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,403.17,15.42;1,89.29,106.66,417.06,15.42">AIT_FHSTP at CheckThat! 2022: Cross-Lingual Fake News Detection with a Large Pre-Trained Transformer</title>
				<funder ref="#_CAdmJaN">
					<orgName type="full">WWTF Austria</orgName>
				</funder>
				<funder ref="#_KpE6EfJ">
					<orgName type="full">Federal Ministry of Agriculture, Regions and Tourism (BMLRT)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,60.74,11.96"><forename type="first">Mina</forename><surname>Sch√ºtz</surname></persName>
							<email>mina.schuetz@ait.ac.at</email>
							<affiliation key="aff0">
								<orgName type="institution">Austrian Institute of Technology</orgName>
								<address>
									<addrLine>Giefinggasse 4</addrLine>
									<postCode>1210</postCode>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,162.68,134.97,72.60,11.96"><forename type="first">Jaqueline</forename><surname>B√∂ck</surname></persName>
							<email>jaqueline.boeck@fhstp.ac.at</email>
							<affiliation key="aff1">
								<orgName type="institution">St.P√∂lten University of Applied Sciences</orgName>
								<address>
									<addrLine>3100 St</addrLine>
									<settlement>P√∂lten</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,247.93,134.97,83.38,11.96"><forename type="first">Medina</forename><surname>Andresel</surname></persName>
							<email>medina.andresel@ait.ac.at</email>
							<affiliation key="aff0">
								<orgName type="institution">Austrian Institute of Technology</orgName>
								<address>
									<addrLine>Giefinggasse 4</addrLine>
									<postCode>1210</postCode>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,343.95,134.97,90.20,11.96"><forename type="first">Armin</forename><surname>Kirchknopf</surname></persName>
							<email>armin.kirchknopf@fhstp.ac.at</email>
							<affiliation key="aff1">
								<orgName type="institution">St.P√∂lten University of Applied Sciences</orgName>
								<address>
									<addrLine>3100 St</addrLine>
									<settlement>P√∂lten</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,148.92,83.12,11.96"><forename type="first">Daria</forename><surname>Liakhovets</surname></persName>
							<email>daria.liakhovets@ait.ac.at</email>
							<affiliation key="aff0">
								<orgName type="institution">Austrian Institute of Technology</orgName>
								<address>
									<addrLine>Giefinggasse 4</addrLine>
									<postCode>1210</postCode>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,185.06,148.92,91.31,11.96"><forename type="first">Djordje</forename><surname>Slijepƒçeviƒá</surname></persName>
							<email>djordje.slijepcevic@fhstp.ac.at</email>
							<affiliation key="aff1">
								<orgName type="institution">St.P√∂lten University of Applied Sciences</orgName>
								<address>
									<addrLine>3100 St</addrLine>
									<settlement>P√∂lten</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,307.37,148.92,99.42,11.96"><forename type="first">Alexander</forename><surname>Schindler</surname></persName>
							<email>alexander.schindler@ait.ac.at</email>
							<affiliation key="aff0">
								<orgName type="institution">Austrian Institute of Technology</orgName>
								<address>
									<addrLine>Giefinggasse 4</addrLine>
									<postCode>1210</postCode>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,403.17,15.42;1,89.29,106.66,417.06,15.42">AIT_FHSTP at CheckThat! 2022: Cross-Lingual Fake News Detection with a Large Pre-Trained Transformer</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">1953B0C13251AD9A9264F3D42C7560DD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fake News Detection</term>
					<term>Pre-Training</term>
					<term>Transformer</term>
					<term>Cross-Lingual</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The increase of fake news in today's society, partially due to the accelerating digital transformation, is a major problem in today's world. This year's CheckThat! Lab 2022 challenge addresses this problem as a Natural Language Processing (NLP) task aiming to detect fake news in English and German texts. Within this paper, we present our methodology and results for both, the monolingual (English) and cross-lingual (German) tasks of the CheckThat! challenge in 2022. We applied the multilingual transformer model XLM-RoBERTa to solve these tasks by pre-training the models on additional datasets and fine-tuning them on the original data as well as its translations for the cross-lingual task. Our final model achieves a macro F1-score of 15,48% and scores the 22 ùë°‚Ñé rank in the benchmark. Regarding the second task, i.e., the cross-lingual German classification, our final model achieves an F1-score of 19.46% and reaches the 4 ùë°‚Ñé rank in the benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Due to the information overload on the web and the rapid spread of content on social media platforms, fake news articles circulate faster and are difficult to distinguish from journalistic articles <ref type="bibr" coords="1,123.55,455.50,11.28,10.91" target="#b0">[1]</ref>. The term fake news is commonly used since the US presidential election in 2016 <ref type="bibr" coords="1,493.29,455.50,12.69,10.91" target="#b1">[2]</ref> and can include multiple aspects of incorrect information propagation, such as propaganda, pure fabrications, hoaxes, click-bait and rumors <ref type="bibr" coords="1,303.98,482.60,11.36,10.91" target="#b1">[2,</ref><ref type="bibr" coords="1,318.07,482.60,7.47,10.91" target="#b2">3,</ref><ref type="bibr" coords="1,328.27,482.60,7.57,10.91" target="#b3">4]</ref>.</p><p>In this year's shared task at CLEF2022 CheckThat! Lab <ref type="bibr" coords="1,336.81,496.14,12.68,10.91" target="#b4">[5]</ref> the third task is fake news detection with four classes <ref type="bibr" coords="1,164.72,509.69,11.23,10.91" target="#b5">[6,</ref><ref type="bibr" coords="1,178.58,509.69,7.42,10.91" target="#b6">7,</ref><ref type="bibr" coords="1,188.63,509.69,7.60,10.91" target="#b7">8]</ref>: false, partly false, true, and other. We decided to take part in both fake news detection sub-tasks: a) English and b) German. The latter was proposed as a cross-lingual task without training data in German language. We propose a large pre-trained XLM-RoBERTa model <ref type="bibr" coords="1,119.37,550.34,11.39,10.91" target="#b8">[9]</ref>, which we additionally pre-trained on a non-publicly available dataset with roughly 200,000 news articles from journalistic as well as citizen sources, such as blogs. After pre-training the model, we fine-tuned it with the given English training data as well as their translations into German to increase its generalization ability.</p><p>Our paper is structured as follows: The first Section 1.1 presents the current state-of-the-art and related work. Section 1.2 describes our methodological approach, including the employed datasets and models. Our experimental setup is explained in Section 2, followed by a documentation of the results (Section 3) and a discussion with final conclusions (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>Detecting fake news is a task that is becoming increasingly important due to digitalization and the rapid spread of information. Current approaches to detect fake news can be categorized into: feature-based approaches which describe the task of learning different writing styles and knowledge-based approaches, where the model learns latent information about the text and its domain <ref type="bibr" coords="2,125.84,231.54,16.33,10.91" target="#b9">[10]</ref>. Gasparetto et al. <ref type="bibr" coords="2,225.46,231.54,17.99,10.91" target="#b10">[11]</ref> provided a structured and comprehensive overview of the existing methods for text classification. In the past, supervised machine learning (ML) models such as stochastic gradient descent (SGD), support vector machines (SVM), linear support vector machines (LSVM), k-nearest neighbour (KNN) and decision trees (DT) have been used for solving that task <ref type="bibr" coords="2,165.15,285.73,16.16,10.91" target="#b11">[12]</ref>. These methods have been overtaken soon by deep learning (DL) models like long short-term memory (LSTM), convolutional neural networks (CNN) and attention-based bidirectional long short-term memory (BiLSTM with Attention) models. However, transformer models including BERT, ALBERT and XLNET outperformed recent ML and DL algorithms <ref type="bibr" coords="2,487.51,326.38,16.12,10.91" target="#b12">[13]</ref>. These are becoming increasingly popular as pre-trained models trained on large corpora of data <ref type="bibr" coords="2,110.25,353.48,17.75,10.91" target="#b13">[14]</ref> are made publicly available (e.g., https://huggingface.co/). Another popular transfomer is T5, which was used by Sabry et al. <ref type="bibr" coords="2,264.39,367.03,16.41,10.91" target="#b14">[15]</ref>. The authors trained an English T5 transformer (t5-base) for an English hate speech classification task and compared the results to several other state-of-the-art classification models; The authors stated that their T5 model outperformed the RoBERTa model in all tasks. This results show that the use of sequence-to-sequence models like T5 can be beneficial when it comes to text classification.</p><p>Fine-tuning a pre-trained ML models to the target data is used in many deep learning applications, especially for small datasets. Previous studies have shown that models which got pre-trained and fine-tuned on similar data as the task-specific one leads to improvements in performance of the models <ref type="bibr" coords="2,228.49,475.42,16.56,10.91" target="#b15">[16,</ref><ref type="bibr" coords="2,247.80,475.42,12.42,10.91" target="#b16">17]</ref>. This has also been demonstrated in our work, where we have used additional datasets (external datasets and translations) for pre-training and finetuning <ref type="bibr" coords="2,121.49,502.52,16.25,10.91" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Methodological Approach</head><p>In this paper we propose a feature-based approach for fake news detection. We define pretraining as unsupervised re-training of a transformer model and fine-tuning as supervised training on the specific classification task. For training our models we used additional as well as translated data.</p><p>‚Ä¢ Pre-Training Strategy: Transformer models are usually already pre-trained on a large set of generic text data <ref type="bibr" coords="2,221.66,631.95,16.35,10.91" target="#b13">[14]</ref>. However, to adapt these models to a specific classification task, we experiment with further pre-training on domain-related data (which might be relevant to the classification task).</p><p>‚Ä¢ Fine-Tuning Strategy: Training pre-trained models for a given downstream task on the given training data for classification is also called fine-tuning. This can be performed either on the upper layers of the model or on all layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">CheckThat! 2022 Data (CT)</head><p>This year's training data consisted of 900 news articles and an additional development set containing 364 instances -both only in English <ref type="bibr" coords="3,315.49,177.34,16.41,10.91" target="#b18">[19]</ref>. The test set in English (sub-task a) contained 612 articles, the one in German 586 (sub-task b). The dataset contained in total four different classes with the following split for the training set <ref type="bibr" coords="3,356.47,204.44,16.43,10.91" target="#b19">[20,</ref><ref type="bibr" coords="3,375.63,204.44,12.45,10.91" target="#b20">21]</ref>: partially false (217), false (465), true (142), and other (76); and the development set: partially false (141), false (113), true (69), and other (41). Since the provided datasets do not include German data, we translated the original English CheckThat data to German using Google Translator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.">External Data:</head><p>In this section, we describe briefly the external data we used for pre-training our models. The abbreviations for the datasets AD and FN are later used to describe which datasets we used for pre-training. FN is used during the following sections for the combination of both presented fake news datasets.</p><p>Article Dataset (AD) This dataset was collected over a period of 1,5 years as part of a nationally funded Austrian research project, i.e., Defalsif-AI https://science.apa.at/project/ defalsifai/, and therefore is not publicly available. It contains 194,332 gathered news articles from different sources. The articles are multilingual, however the majority are either in English or German. The articles are not annotated in terms of whether they are Fake News or not, and are only used for pre-training the transformer models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fake News Dataset German (FN)</head><p>The Fake News Dataset German <ref type="bibr" coords="3,410.95,460.73,18.03,10.91" target="#b21">[22]</ref> contains approximately 63,000 fake and non-fake news articles from the fields of economics and sports. As some of the text include HTML and JavaScript snippets we removed lines which contained such snippets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fake and real news dataset (FN)</head><p>This dataset can be found on Kaggle.com <ref type="bibr" coords="3,432.77,530.13,17.75,10.91" target="#b22">[23]</ref> and consists of text data from news, political and other articles. This dataset comprises approximately 19,000 fake and 21,000 non-fake texts. More details on the described data can be found in the papers <ref type="bibr" coords="3,89.29,570.78,16.43,10.91" target="#b23">[24,</ref><ref type="bibr" coords="3,108.45,570.78,12.32,10.91" target="#b24">25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Experimental Setup</head><p>We employed and experimented with two different transformer models: XLM-R <ref type="bibr" coords="3,439.48,629.41,12.68,10.91" target="#b8">[9]</ref> and T5 <ref type="bibr" coords="3,487.55,629.41,16.09,10.91" target="#b25">[26]</ref>. The experimental setup is depicted in Figure <ref type="figure" coords="3,291.58,642.96,3.77,10.91" target="#fig_0">1</ref>. We evaluated each experiment with the same training (90%) and validation split (10%).</p><p>‚Ä¢ XLM-R is a multilingual model, trained on 100 languages, which is designed for standard NLP tasks. The underlying architecture is a combination of RoBERTa <ref type="bibr" coords="4,424.01,100.52,17.83,10.91" target="#b26">[27]</ref> and XLM <ref type="bibr" coords="4,488.16,100.52,17.83,10.91" target="#b27">[28]</ref> which leads to very good performance, outperforming other state-of-the-art models like the multilingual version of BERT (mBERT) <ref type="bibr" coords="4,302.79,127.61,17.75,10.91" target="#b28">[29]</ref> without the use of Next Sentence Prediction (relying only on Masked Language Modeling) as pre-training strategy. ‚Ä¢ T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. The small variant of the English T5 model is pre-trained on the English C4 <ref type="bibr" coords="4,422.52,181.72,17.76,10.91" target="#b25">[26]</ref> dataset as well as the Wiki-DPR <ref type="bibr" coords="4,192.96,195.27,17.83,10.91" target="#b29">[30]</ref> data. This publicly available model has been fine-tuned in the past for several downstream tasks using different datasets. In our work, this small version of the T5 got further fine-tuned for detecting fake news.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Unsupervised Pre-Training</head><p>For pre-training our models, which we obtained from HuggingFace <ref type="foot" coords="4,397.18,276.33,3.71,7.97" target="#foot_0">1</ref> we experimented with two different additional datasets (AD, FN).</p><p>‚Ä¢ T5-PRET: The smaller version of the T5 model (T5-small) that can be found on Huggingface <ref type="bibr" coords="4,137.20,322.55,17.91,10.91" target="#b25">[26]</ref> was trained on:</p><p>1. the original CheckThat data: T5-PRET-CT 2. the original + translated CheckThat data: T5-PRET-CT-TL 3. as well as on a combination of the original CheckThat and the additional fake news dataset (FN ): T5-PRET-CT-FN</p><p>Since the additional fake news datasets (FN ) are relatively large, experiments were conducted smaller subsets. In this paper, only the results for one of these splits are mentioned, since similar results were obtained for all splits. The additional data did not show better results than the model only re-trained on the original CheckThat data. The mentioned T5-PRET-CT-FN model was re-trained on a split of the fake news datasets (FN ) which had a length of about 10 million characters. The distribution of English and German texts is about 50%. All models got re-trained with a batch size of 8 and a learning rate of 1ùëí -4 . Each model was trained for about 8 to 15 epochs. ‚Ä¢ XLM-R-PRET-AD: We trained the available XLM-R model provided by HuggingFace with the AD dataset. It was trained for 5 epochs, with a batch size of 16 and a learning rate of 2ùëí -5 . The probability for masked language modeling was 15% as used in the original BERT paper <ref type="bibr" coords="4,172.70,545.36,16.24,10.91" target="#b13">[14]</ref>, where this type of pre-training was introduced. We only trained it for such a low amount of epochs because the training time took roughly 55 hours on one GPU for all articles. ‚Ä¢ XLM-R-PRET-FN: The second pre-trained XLM-R transformer we trained was with the other fake news datasets (FN ). We used a similar strategy by also using the 15% probability for the Masked Language Modeling and a character length of 40 million. Since the available fake news datasets (FN ) are less than the additional dataset (AD), the training time on GPU was only around 13 minutes. We also trained it with a learning rate of 2ùëí -5 and for 5 epochs and a smaller batch size of 8. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Supervised Fine-Tuning</head><p>For fine-tuning our pre-trained models we experimented with different hyperparameters and data combinations. For the fine-tuning of the XLM-R we used the titles as well as the article content. If a title was available it was added to the front of the content, due to the maximum sequence length of models. Using transformer models, usually the content after the maximum sequence length gets padded and not cropped off. This has shown to push the performance on fake news classification tasks in other settings <ref type="bibr" coords="5,293.51,606.67,17.76,10.91" target="#b30">[31]</ref> and was also used for the official baseline of this year's shared task <ref type="bibr" coords="5,191.64,620.22,16.29,10.91" target="#b31">[32]</ref>. This approach was not used for the T5 model, for which only the article content was used for training.</p><p>Our experiments show that the T5 was not performing well due the low amount of data, even though it was pre-trained as well. Hence, after some experiments with the re-trained T5 models fine-tuned on the original CheckThat data, we did not continue with further experiments and focused on the better performing XLM-R models. Since we only had a small dataset for the English language we found the best amount of epochs for fine-tuning the XLM-R models is 30, even though it has been shown that usually 3-5 epochs are enough or sometimes even 5-10 epochs -depending on the available dataset size <ref type="bibr" coords="6,304.26,363.57,16.19,10.91" target="#b30">[31]</ref>. We additionally experimented with only using the training data as well as the German translations. Table <ref type="table" coords="6,384.71,377.12,5.17,10.91" target="#tab_0">1</ref> shows the final results of our experiments, which we used to determine our best model for the submission. Experiment 4 (XLM-PRET-AD) shows the best performance in terms of accuracy. For a more detailed overview we documented all investigated hyperparameters in Table <ref type="table" coords="6,500.05,417.76,3.80,10.91" target="#tab_1">2</ref>. For each experiment we evaluated different learning rates, i.e., 1ùëí -5 and 2ùëí -5 . Our assumption was that a pre-trained model needs a lower learning rate during fine-tuning, because of the additional data it was pre-trained on. As shown in Table <ref type="table" coords="6,353.03,458.41,5.17,10.91" target="#tab_1">2</ref> and Table <ref type="table" coords="6,409.21,458.41,5.17,10.91" target="#tab_0">1</ref> the higher learning rate of 2ùëí -5 did result in significantly worse predictions on the development set in two setups. For the second pre-trained model XLM-R-PRET-FN, we only performed experiments with the translated data, as preliminary results showed that using the translated data yielded more stable results with both learning rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>Our submitted models for subtask 3a and subtask 3b are both pre-trained on the large additional dataset (AD) and only fine-tuned on the CheckThat data as well as its translations into German (see Table <ref type="table" coords="6,137.70,598.33,3.65,10.91" target="#tab_2">3</ref>). For subtask 3a we rank 22 ùë°‚Ñé out of 25 and for the cross-lingual task we rank 4 ùë°‚Ñé of 8. The best performing models from other teams for subtask 3a achieved an F1-score of 33.91% and for subtask 3b 29.98%. These results indicate that none of the models achieves a performance that is suitable for real-world applications. An interesting observation is that our model performed better in the cross-language task, even without using training data in German.  The low results for some classes are probably due to the unbalanced class distributions. In general, studies have shown that even humans have difficulty distinguishing between the different fake-news related categories, even for binary classification tasks <ref type="bibr" coords="7,418.53,595.67,16.25,10.91" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Subtask 3b: Cross-Lingual</head><p>To train one model for both subtasks, we translated the original English data into German to train and fine-tune our multilingual XLM-R model on the specific classes. In comparison to our approach, the baseline approach of the organizers was a standard BERT model, trained on the English CheckThat dataset. For the cross-lingual task, they translated the German test data to English and then evaluated the performance <ref type="bibr" coords="8,303.36,237.02,16.35,10.91" target="#b31">[32]</ref>. Even though there was no training data available, our model performs slightly better for each class compared to our results for subtask 3a. The results are shown in Table <ref type="table" coords="8,249.42,264.11,3.81,10.91" target="#tab_6">5</ref>. The results for each class are similar to the results for subtask 3a, where the model performed best for the True class (F1: 37.84%) and worst for the Other class (F1: 8.08%). We assume that this behavior is due to the fact that the class Other class was significantly underrepresented in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion &amp; Conclusion</head><p>In this paper, we provide the details on our submission to the CheckThat! 2022 Lab for Task 3: Fake News Detection, which consists of two tasks on the classification of fake content. Our experiments show that the unsupervised pre-training strategy of the XLM-R model with additional generic (not task-specific) data with more data instances is the more promising strategy compared to using domain-specific data with fewer training instances. Our model -XLM-R-PRET-AD achieves an F1-score of 15.48% in subtask 3a and 19.46% in subtask 3b. However, the model shows great signs of overfitting, especially on the class Other. We conclude that using translations of the original data and using similar content for fine-tuning increases the performance of these models, rather than just fine-tuning them on the provided training data. In future work, we want to compare the influence of pre-training models with more domain-specific data than general content data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,89.29,468.80,416.69,8.93;5,89.29,480.81,337.81,8.87;5,89.29,84.19,416.69,378.03"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the experimental setup for training the two transformer architectures, including both training strategies, i.e., unsupervised pre-training and supervised fine-tuning.</figDesc><graphic coords="5,89.29,84.19,416.69,378.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,88.99,90.49,417.00,192.17"><head>Table 1</head><label>1</label><figDesc>Experiment results (in %). The models were all evaluated on a merge of the original English development set and its German translations. The evaluated performance metrics are accuracy, macro-averaged precision, recall, and F1-score.</figDesc><table coords="6,100.97,146.01,393.33,136.65"><row><cell cols="2">No. Model</cell><cell>Dataset</cell><cell cols="3">Accuracy Precision Recall</cell><cell>F1</cell></row><row><cell>1</cell><cell>XLM-R-PRET-AD</cell><cell>CheckThat</cell><cell>53.80</cell><cell>56.77</cell><cell>51.68</cell><cell>51.68</cell></row><row><cell>2</cell><cell>XLM-R-PRET-AD</cell><cell>CheckThat</cell><cell>42.93</cell><cell>39.94</cell><cell>36.29</cell><cell>34.00</cell></row><row><cell>3</cell><cell>XLM-R-PRET-AD</cell><cell>CheckThat + Translated</cell><cell>53.85</cell><cell>54.31</cell><cell>52.04</cell><cell>50.65</cell></row><row><cell>4</cell><cell cols="2">XLM-R-PRET-AD CheckThat + Translated</cell><cell>54.81</cell><cell>52.56</cell><cell cols="2">51.59 50.29</cell></row><row><cell>5</cell><cell>XLM-R-PRET-FN</cell><cell>CheckThat + Translated</cell><cell>53.30</cell><cell>53.45</cell><cell>50.67</cell><cell>50.16</cell></row><row><cell>6</cell><cell>XLM-R-PRET-FN</cell><cell>CheckThat + Translated</cell><cell>43.96</cell><cell>29.03</cell><cell>35.32</cell><cell>29.09</cell></row><row><cell>7</cell><cell>T5-PRET</cell><cell>CheckThat</cell><cell>48.35</cell><cell>48.53</cell><cell>44.70</cell><cell>44.24</cell></row><row><cell>8</cell><cell>T5-PRET-CT</cell><cell>CheckThat</cell><cell>54.40</cell><cell>49.70</cell><cell>50.89</cell><cell>49.76</cell></row><row><cell>9</cell><cell>T5-PRET-CT-TL</cell><cell>CheckThat</cell><cell>49.18</cell><cell>45.59</cell><cell>46.14</cell><cell>45.26</cell></row><row><cell cols="2">10 T5-PRET-CT-FN</cell><cell>CheckThat</cell><cell>53.02</cell><cell>55.06</cell><cell>48.48</cell><cell>48.98</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,88.99,90.49,340.50,168.26"><head>Table 2</head><label>2</label><figDesc>Investigated hyperparameters.</figDesc><table coords="7,165.78,122.10,263.71,136.65"><row><cell cols="5">No. Epochs Batch Size Learning Rate Max. Seq. Length</cell></row><row><cell>1</cell><cell>30</cell><cell>8</cell><cell>1ùëí -5</cell><cell>512</cell></row><row><cell>2</cell><cell>30</cell><cell>8</cell><cell>2ùëí -5</cell><cell>512</cell></row><row><cell>3</cell><cell>30</cell><cell>8</cell><cell>1ùëí -5</cell><cell>512</cell></row><row><cell>4</cell><cell>30</cell><cell>8</cell><cell>2ùëí -5</cell><cell>512</cell></row><row><cell>5</cell><cell>30</cell><cell>8</cell><cell>1ùëí -5</cell><cell>512</cell></row><row><cell>6</cell><cell>30</cell><cell>8</cell><cell>2ùëí -5</cell><cell>512</cell></row><row><cell>7</cell><cell>7</cell><cell>4</cell><cell>1ùëí -4</cell><cell>512</cell></row><row><cell>8</cell><cell>10</cell><cell>4</cell><cell>1ùëí -4</cell><cell>512</cell></row><row><cell>9</cell><cell>4</cell><cell>4</cell><cell>1ùëí -4</cell><cell>512</cell></row><row><cell>10</cell><cell>9</cell><cell>4</cell><cell>1ùëí -4</cell><cell>512</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,88.99,281.01,385.19,70.05"><head>Table 3</head><label>3</label><figDesc>Results, model, rank. The F1-score is macro-averaged and shown in percent (%).</figDesc><table coords="7,121.09,312.62,353.09,38.43"><row><cell>Model</cell><cell>Dataset</cell><cell>Task</cell><cell>F1</cell><cell cols="2">Accuracy Rank</cell></row><row><cell cols="4">XLM-R-PRET-AD CheckThat + Translated Subtask 3a 15.48</cell><cell>19.93</cell><cell>22 ùë°‚Ñé</cell></row><row><cell cols="4">XLM-R-PRET-AD CheckThat + Translated Subtask 3b 19.46</cell><cell>25.42</cell><cell>4 ùë°‚Ñé</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,88.99,373.32,313.90,131.48"><head>Table 4</head><label>4</label><figDesc>Results for subtask 3a. Values are macro-averaged and shown in percent (%).</figDesc><table coords="7,89.29,404.93,296.64,99.86"><row><cell>Class</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>False</cell><cell>35.80</cell><cell>9.20</cell><cell>14.65</cell></row><row><cell>Other</cell><cell>2.81</cell><cell>6.45</cell><cell>3.92</cell></row><row><cell>Partially False</cell><cell>11.50</cell><cell cols="2">23.21 15.38</cell></row><row><cell>True</cell><cell>22.47</cell><cell cols="2">37.14 28.07</cell></row><row><cell>3.1</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,102.92,494.09,115.86,10.71"><head>. Subtask 3a: English</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,88.96,514.38,418.31,51.56"><head>Table 4</head><label>4</label><figDesc></figDesc><table /><note coords="7,124.00,514.38,381.99,10.91;7,89.29,527.93,417.98,10.91;7,89.04,541.48,416.94,10.91;7,89.29,555.02,67.28,10.91"><p>shows the overall results per class for subtask 3a (English data). The proposed model performs best for the class True (F1: 28.07%). The False (F1: 14.65%) and Partially False (F1: 15.38%) classes are classified considerably worse. However, XLM-R fails to model the class Other at all (F1: 3.92).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,88.99,90.49,313.93,93.78"><head>Table 5</head><label>5</label><figDesc>Results for subtask 3b. Values are macro-averaged and shown in percent (%).</figDesc><table coords="8,209.34,122.10,176.60,62.16"><row><cell>Class</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>False</cell><cell>22.03</cell><cell cols="2">13.61 16.82</cell></row><row><cell>Other</cell><cell>9.09</cell><cell>7.27</cell><cell>8.08</cell></row><row><cell>Partially False</cell><cell>13.28</cell><cell cols="2">17.52 15.11</cell></row><row><cell>True</cell><cell>34.45</cell><cell cols="2">41.97 37.84</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,108.93,671.04,84.73,8.97"><p>https://huggingface.co/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This contribution has been funded by the <rs type="programName">FFG Project "Defalsif-AI" (Austrian security research programme KIRAS of</rs> the <rs type="funder">Federal Ministry of Agriculture, Regions and Tourism (BMLRT)</rs>, grant no. <rs type="grantNumber">879670</rs>) and Project "<rs type="projectName">Young People Against Online Hate: Computer-assisted Strategies for Facilitating Citizen-generated Counter Speech</rs>", <rs type="funder">WWTF Austria</rs>, grant no. <rs type="grantNumber">ICT-20-016</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_KpE6EfJ">
					<idno type="grant-number">879670</idno>
					<orgName type="project" subtype="full">Young People Against Online Hate: Computer-assisted Strategies for Facilitating Citizen-generated Counter Speech</orgName>
					<orgName type="program" subtype="full">FFG Project &quot;Defalsif-AI&quot; (Austrian security research programme KIRAS of</orgName>
				</org>
				<org type="funding" xml:id="_CAdmJaN">
					<idno type="grant-number">ICT-20-016</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,656.03,393.33,10.91;8,112.66,669.58,394.53,10.91;9,112.66,86.97,241.69,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,306.83,656.03,199.16,10.91;8,112.66,669.58,87.72,10.91">Fake news on social media: Brief review on detection techniques</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">I</forename><surname>Mahid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Manickam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Karuppayah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,244.06,669.58,263.12,10.91;9,112.66,86.97,173.35,10.91">Fourth International Conference on Advances in Computing, Communication Automation (ICACCA)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,100.52,393.33,10.91;9,112.66,114.06,393.33,10.91;9,112.66,127.61,169.44,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,300.38,100.52,205.61,10.91;9,112.66,114.06,42.01,10.91">The use and abuse of social media for spreading fake news</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">H</forename><surname>Alkawaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">M</forename><surname>Zangana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,201.40,114.06,304.59,10.91;9,112.66,127.61,81.43,10.91">IEEE International Conference on Automatic Control and Intelligent Systems (I2CACIS)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="145" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,141.16,169.95,10.91;9,308.02,141.16,197.97,10.91;9,112.66,154.71,102.31,10.91;9,246.98,154.71,260.30,10.91;9,112.31,168.26,185.54,10.91;9,316.24,168.26,191.42,10.91;9,112.66,184.25,305.06,7.90" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,308.02,141.16,197.97,10.91;9,112.66,154.71,98.08,10.91">Defining &quot;fake news&quot;: A typology of scholarly definitions</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Tandoc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ling</surname></persName>
		</author>
		<idno type="DOI">10.1080/21670811.2017.1360143</idno>
		<idno>doi:</idno>
		<ptr target="https://doi.org/10.1080/21670811.2017.1360143" />
	</analytic>
	<monogr>
		<title level="j" coord="9,246.98,154.71,90.32,10.91">Digital Journalism</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="137" to="153" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,195.36,393.71,10.91;9,112.66,208.91,393.33,10.91;9,112.33,222.46,308.77,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,393.98,195.36,112.39,10.91;9,112.66,208.91,224.19,10.91">Combating fake news: A survey on identification and mitigation techniques</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ruchansky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3305260</idno>
		<ptr target="https://doi.org/10.1145/3305260.doi:10.1145/3305260" />
	</analytic>
	<monogr>
		<title level="j" coord="9,345.40,208.91,147.66,10.91">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,236.01,394.53,10.91;9,112.33,249.56,394.85,10.91;9,112.66,263.11,393.32,10.91;9,112.66,276.66,394.53,10.91;9,112.66,290.20,395.17,10.91;9,112.66,303.75,193.11,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,272.14,263.11,233.84,10.91;9,112.66,276.66,154.75,10.91">The clef-2022 checkthat! lab on fighting the covid-19 infodemic and fake news detection</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barr√≥n-Cede√±o</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Stru√ü</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>M√≠guez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Kartal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Beltr√°n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,272.10,290.20,151.66,10.91">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Hagen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Verberne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Seifert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Balog</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>N√∏rv√•g</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Setty</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="416" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,317.30,394.53,10.91;9,112.33,330.85,394.85,10.91;9,112.66,344.40,393.33,10.91;9,112.66,357.95,394.52,10.91;9,112.66,371.50,393.33,10.91;9,112.28,385.05,394.91,10.91;9,112.66,398.60,89.12,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,432.00,344.40,73.99,10.91;9,112.66,357.95,390.20,10.91">Overview of the CLEF-2022 CheckThat! lab on fighting the COVID-19 infodemic and fake news detection</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barr√≥n-Cede√±o</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Stru√ü</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>M√≠guez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Kartal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Beltr√°n</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>K√∂hler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,126.92,371.50,379.07,10.91;9,112.28,385.05,390.55,10.91">Proceedings of the 13th International Conference of the CLEF Association: Information Access Evaluation meets Multilinguality, Multimodality, and Visualization, CLEF &apos;2022</title>
		<meeting>the 13th International Conference of the CLEF Association: Information Access Evaluation meets Multilinguality, Multimodality, and Visualization, CLEF &apos;2022<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,412.15,393.33,10.91;9,112.66,425.70,393.58,10.91;9,112.66,439.25,382.34,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,431.30,412.15,74.69,10.91;9,112.66,425.70,259.59,10.91">Overview of the CLEF-2022 CheckThat! lab task 3 on fake news detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>K√∂hler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Stru√ü</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,397.40,425.70,108.85,10.91;9,112.66,439.25,286.23,10.91">Working Notes of CLEF 2022-Conference and Labs of the Evaluation Forum, CLEF &apos;2022</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,452.79,393.33,10.91;9,112.66,466.34,393.33,10.91;9,112.66,479.89,387.87,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,220.22,452.79,285.77,10.91;9,112.66,466.34,49.79,10.91">FakeCovid -a multilingual cross-domain fact check news dataset for covid-19</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nandini</surname></persName>
		</author>
		<ptr target="http://workshop-proceedings.icwsm.org/pdf/2020_14.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="9,183.76,466.34,322.22,10.91;9,112.66,479.89,73.20,10.91">Workshop Proceedings of the 14th International AAAI Conference on Web and Social Media</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,493.44,394.53,10.91;9,112.66,506.99,178.78,10.91;9,307.62,506.99,198.37,10.91;9,112.66,520.54,395.01,10.91;9,112.66,536.53,97.35,7.90" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,307.62,506.99,198.37,10.91;9,112.66,520.54,75.38,10.91">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Guzm√°n</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1911.02116" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,547.64,393.33,10.91;9,112.66,561.19,394.51,10.91;9,112.36,577.18,79.55,7.90" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,314.72,547.64,191.27,10.91;9,112.66,561.19,47.12,10.91">Fake news detection using machine learning approaches</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Khanam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">N</forename><surname>Alwasel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sirafi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rashid</surname></persName>
		</author>
		<idno>doi:0.1088/1757-899X/ 1099/1/012040</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,168.43,561.19,131.80,10.91">IOP Conf. Ser. Mater. Sci. Eng</title>
		<imprint>
			<biblScope unit="page">12040</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,588.29,395.17,10.91;9,112.66,601.84,358.66,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Di≈æo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Blatnick√Ω</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Melnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">K</forename></persName>
		</author>
		<ptr target="https://orcid.org/00000003-4677-2535" />
		<title level="m" coord="9,450.20,588.29,57.62,10.91;9,112.66,601.84,259.51,10.91">A mathematical model of operation of a semi-trailer tractor powertrain</title>
		<imprint>
			<publisher>Komunik√°cie</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,615.39,393.33,10.91;9,112.66,628.93,394.53,10.91;9,112.66,642.48,315.89,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="9,263.62,615.39,169.51,10.91">Covid-19 fake news detection system</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Malhotra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mahur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Achint</forename></persName>
		</author>
		<idno type="DOI">10.1109/Confluence52989.2022.9734144</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,461.86,615.39,44.13,10.91;9,112.66,628.93,332.24,10.91">2022 12th International Conference on Cloud Computing, Data Science Engineering</title>
		<meeting><address><addrLine>Confluence)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="428" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,656.03,394.53,10.91;9,112.66,669.58,364.20,10.91" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="9,216.70,656.03,285.80,10.91">Transformer based automatic covid-19 fake news detection system</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gundapu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mamidi</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2101.00180</idno>
		<ptr target="https://arxiv.org/abs/2101.00180.doi:10.48550/ARXIV.2101.00180" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,86.97,395.17,10.91;10,112.66,100.52,393.33,10.91;10,112.28,114.06,393.71,10.91;10,112.33,127.61,393.65,10.91;10,112.28,141.16,395.55,10.91;10,112.41,154.71,243.89,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,335.31,86.97,172.52,10.91;10,112.66,100.52,156.13,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1423,doi=10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m" coord="10,291.87,100.52,214.11,10.91;10,112.28,114.06,393.71,10.91;10,112.33,127.61,57.49,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="10,112.66,168.26,393.32,10.91;10,112.66,181.81,395.01,10.91;10,112.66,195.36,197.80,10.91" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="10,413.84,168.26,92.15,10.91;10,112.66,181.81,225.13,10.91">Hat5: Hate language identification using text-to-text transfer transformer</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">S</forename><surname>Sabry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Adewumi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kovacs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2202.05690</idno>
		<ptr target="https://arxiv.org/abs/2202.05690.doi:10.48550/ARXIV.2202.05690" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,208.91,393.32,10.91;10,112.66,222.46,393.33,10.91;10,112.66,236.01,58.35,10.91" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="10,348.66,208.91,157.32,10.91;10,112.66,222.46,178.11,10.91">Improved fine-tuning by leveraging pre-training data: Theory and practice</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=kQns9y_JH6" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,249.56,395.17,10.91;10,112.66,263.11,393.33,10.91;10,112.66,276.66,394.53,10.91;10,112.66,290.20,365.89,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,269.62,249.56,238.20,10.91;10,112.66,263.11,99.09,10.91">To tune or not to tune? adapting pretrained representations to diverse tasks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-4302</idno>
		<ptr target="https://aclanthology.org/W19-4302.doi:10.18653/v1/W19-4302" />
	</analytic>
	<monogr>
		<title level="m" coord="10,234.29,263.11,271.70,10.91;10,112.66,276.66,109.99,10.91">Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)</title>
		<meeting>the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,303.75,394.53,10.91;10,112.66,317.30,393.33,10.91;10,112.66,330.85,301.26,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,298.35,317.30,207.64,10.91;10,112.66,330.85,174.98,10.91">Automatic sexism detection with multilingual transformer models ait fhstp@exist2021</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sch√ºtz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Boeck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Liakhovets</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Slijepcevic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kirchknopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hecht</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bogensperger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schlarb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zeppelzauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,310.90,330.85,72.06,10.91">IberLEF@SEPLN</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,344.40,393.33,10.91;10,112.66,357.95,211.28,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,263.99,344.40,241.99,10.91;10,112.66,357.95,65.16,10.91">Overview of the clef-2021 checkthat! lab task 3 on fake news detection</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Stru√ü</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,186.11,357.95,105.90,10.91">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,371.50,393.33,10.91;10,112.66,385.05,283.01,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,292.17,371.50,213.82,10.91;10,112.66,385.05,42.26,10.91">An exploratory study of covid-19 misinformation on twitter</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dirkson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Majchrzak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,162.86,385.05,154.86,10.91">Online Social Networks and Media</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">100104</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,398.60,393.59,10.91;10,112.66,412.15,146.44,10.91" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00502</idno>
		<title level="m" coord="10,168.48,398.60,305.06,10.91">Amused: An annotation framework of multi-modal social media data</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,425.70,394.04,10.91;10,112.66,439.25,162.20,10.91" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="10,166.31,425.70,121.39,10.91">Fake news dataset german</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Str√∂ckl</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/datasets/astoeckl/fake-news-dataset-german" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,452.79,394.03,10.91;10,112.66,466.34,289.91,10.91" xml:id="b22">
	<monogr>
		<title level="m" type="main" coord="10,182.12,452.79,116.90,10.91">Fake and real news dataset</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bisaillon</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset?resource=download" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,479.89,395.16,10.91;10,112.66,493.44,57.53,10.91" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="10,241.92,479.89,265.91,10.91;10,112.66,493.44,25.61,10.91">Detecting opinion spams and fake news using text classification 1</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Traore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Saad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,506.99,393.33,10.91;10,112.66,520.54,393.33,10.91;10,112.66,534.09,349.12,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="10,247.17,506.99,258.82,10.91;10,112.66,520.54,129.18,10.91">Detection of online fake news using n-gram analysis and machine learning techniques</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Traore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Saad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="10,266.37,520.54,239.61,10.91;10,112.66,534.09,75.54,10.91">Lecture Notes in Computer Science, Lecture notes in computer science</title>
		<imprint>
			<biblScope unit="page" from="127" to="138" />
			<date type="published" when="2017">2017</date>
			<publisher>Springer International Publishing</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,547.64,394.53,10.91;10,112.66,561.19,393.59,10.91;10,112.66,574.74,166.85,10.91" xml:id="b25">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<title level="m" coord="10,112.66,561.19,358.56,10.91">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,588.29,395.17,10.91;10,112.66,601.84,395.01,10.91" xml:id="b26">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="10,137.85,601.84,241.29,10.91">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,615.39,394.53,10.91;10,112.66,628.93,393.32,10.91;10,112.66,642.48,394.62,10.91;10,112.31,656.03,387.34,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="10,232.50,615.39,190.84,10.91">Cross-lingual language model pretraining</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="s" coord="10,420.63,628.93,85.36,10.91;10,112.66,642.48,144.34,10.91">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alch√©-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,669.58,393.33,10.91;11,112.66,86.97,373.82,10.91" xml:id="b28">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Turc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08962</idno>
		<title level="m" coord="10,323.12,669.58,182.87,10.91;11,112.66,86.97,191.23,10.91">Well-read students learn better: On the importance of pre-training compact models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,100.52,393.33,10.91;11,112.66,114.06,370.24,10.91" xml:id="b29">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Oƒüuz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04906</idno>
		<title level="m" coord="11,477.87,100.52,28.12,10.91;11,112.66,114.06,239.86,10.91">Dense passage retrieval for open-domain question answering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,127.61,393.33,10.91;11,112.66,141.16,393.33,10.91;11,112.66,154.71,394.52,10.91;11,112.39,168.26,359.08,10.91" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="11,336.85,127.61,169.13,10.91;11,112.66,141.16,141.88,10.91">Automatic fake news detection with pre-trained transformer models</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sch√ºtz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nazemi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-68787-8_45</idno>
	</analytic>
	<monogr>
		<title level="m" coord="11,385.67,141.16,120.32,10.91;11,112.66,154.71,226.40,10.91">Pattern Recognition. ICPR International Workshops and Challenges. ICPR 2021</title>
		<title level="s" coord="11,346.24,154.71,156.59,10.91">Lecture Notes in Computer Sciences</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Bimbo</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12667</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,181.81,394.62,10.91;11,112.31,195.36,302.76,10.91" xml:id="b31">
	<monogr>
		<title level="m" type="main" coord="11,214.57,181.81,205.53,10.91">Baseline for clef2022 -checkthat! lab task 3</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sch√ºtz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Siegel</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.6362498</idno>
		<ptr target="https://doi.org/10.5281/zenodo.6362498.doi:10.5281/zenodo.6362498" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,208.91,393.33,10.91;11,112.66,222.46,63.75,10.91;11,194.27,222.46,313.40,10.91;11,112.66,238.45,97.35,7.90" xml:id="b32">
	<monogr>
		<title level="m" type="main" coord="11,227.05,208.91,278.94,10.91;11,112.66,222.46,59.19,10.91">Fake news: A survey of research, detection methods, and opportunities</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zafarani</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1812.00315" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
