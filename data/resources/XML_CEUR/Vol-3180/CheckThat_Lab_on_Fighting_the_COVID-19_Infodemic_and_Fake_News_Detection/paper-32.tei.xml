<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,105.41,75.32,384.13,17.04;1,201.67,95.96,191.85,17.04">SCUoL at CheckThat! 2022: Fake News Detection Using Transformer-Based Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.02,128.89,69.80,10.80"><forename type="first">Saud</forename><surname>Althabiti</surname></persName>
							<email>salthabiti@kau.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Leeds</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">King Abdulaziz University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,159.91,128.89,136.87,10.80"><forename type="first">Mohammad</forename><forename type="middle">Ammar</forename><surname>Alsalka</surname></persName>
							<email>m.a.alsalka@leeds.ac.uk</email>
							<affiliation key="aff4">
								<orgName type="institution">University of Leeds</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,326.28,128.89,55.41,10.80"><forename type="first">Eric</forename><surname>Atwell</surname></persName>
							<email>e.s.atwell@leeds.ac.uk</email>
							<affiliation key="aff6">
								<orgName type="institution">University of Leeds</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Leeds</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<settlement>Jeddah</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<address>
									<settlement>Leeds</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<address>
									<settlement>Leeds</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,105.41,75.32,384.13,17.04;1,201.67,95.96,191.85,17.04">SCUoL at CheckThat! 2022: Fake News Detection Using Transformer-Based Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DCED3B3C9FD5803CFF7BC015D6AA3248</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fake News Detection</term>
					<term>Misinformation</term>
					<term>Misleading Information</term>
					<term>CLEF 2022 1</term>
					<term>CheckThat! Lab</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The fifth edition of the "CheckThat! Lab" is one of the 2022 Conference and Labs of the Evaluation Forum (CLEF) and aims to evaluate advances supporting three factuality-related tasks, covering several languages. Our team (SCUoL) participated in task 3A, which concentrates on multi-class fake news detection of English news articles. This paper describes our approach, including several experiments exploring different machine learning and transformer-based models. Furthermore, we employed an additional dataset to support our proposed model. During the validation results phase, the experiments highlight the best performing machine learning classifier, which achieved cross-validation scores of over 60% for the LinearSVC compared to the pre-trained BERT model that exceeds other models in this task. While in the testing results, we obtained an F1 of approximately 0.305 compared to the other participants' average F1 of 0.252.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.2" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fake news has evolved into a severe threat which may cause increased political, financial and societal losses making detection of such news significant because, unlike traditional newspapers, online information could mislead a broader range of communities. In addition, it may include both facts and parts of incorrect contents in one statement, which can be challenging to identify. Therefore, various research studies have developed models to find a beneficial solution to tackle this difficulty.</p><p>The CheckThat! Lab <ref type="bibr" coords="1,182.95,520.36,13.30,9.94" target="#b0">[1]</ref>- <ref type="bibr" coords="1,200.68,520.36,13.30,9.94" target="#b4">[5]</ref> provides annual competitions divided into three categories, namely, check-worthiness estimation, verified claim retrieval, and fake news detection (FND). In this experiment, we participated in the third task <ref type="bibr" coords="1,270.82,545.56,11.63,9.94" target="#b5">[6]</ref>. The performed task aims to detect fake news articles written in the English language and their topical domains. We focused only on the first part (Task 3A) in this experiment. This subtask provided a dataset including almost 1,300 articles divided into training and development datasets to determine whether each article's claim is false, partially false, true, or other <ref type="bibr" coords="1,72.02,596.23,12.65,9.94" target="#b6">[7]</ref>- <ref type="bibr" coords="1,88.90,596.23,12.65,9.94" target="#b8">[9]</ref>. Therefore, the main objective of this experiment is to classify real-world news articles into predefined categories. Each text has been labelled with a specific rating; hence, this is a supervised text classification problem aiming to rank upcoming news based on the article's content.</p><p>In this paper, we firstly discuss some of the related topics and previous studies in section 2. Then, we analyze the datasets and describe the methodology in sections 3 and 4, respectively. The following section discusses the results obtained after training the model, and finally, we conclude and suggest future work in the last section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Defining claims' credibility is a research problem that has drawn considerable attention in past years, and various studies have been developing methods to overcome this issue <ref type="bibr" coords="2,394.94,118.24,13.09,9.94" target="#b8">[9]</ref>- <ref type="bibr" coords="2,412.39,118.24,17.45,9.94" target="#b13">[14]</ref>. Since this topic has become trending in many languages, several surveys have attempted to review the various suggested techniques and practical approaches systematically. For instance, the study by Shu and Liu <ref type="bibr" coords="2,471.53,143.47,18.32,9.94" target="#b14">[15]</ref> details fake news detection methods in five categories: linguistics, topic-agnostic, knowledge-based, traditional machine learning, and hybrid approaches. Furthermore, they examined how these methods could interlink to be used jointly.</p><p>In another study by <ref type="bibr" coords="2,174.07,194.11,16.80,9.94" target="#b11">[12]</ref>, <ref type="bibr" coords="2,198.05,194.11,16.82,9.94" target="#b15">[16]</ref>, they provided a descriptive tutorial that reevaluated FND techniques and methods from four different viewpoints. The first viewpoint evaluates authenticity by extracting the facts and comparing them with knowledge. An additional perspective is apprehending the writing style of given news since manipulators who aim to distribute fake news usually spread distorted messages intended to persuade others <ref type="bibr" coords="2,240.34,244.77,16.80,9.94" target="#b15">[16]</ref>. Furthermore, the propagation of the spread of information is the third perspective. To explain, the route of widespread news messages forms a network that could hold indications for early fake news detection. Finally, a source-based method is another employed idea. This method mainly relies on the source of a particular post, such as the original news authors, the publishers who conveyed that news, or the person who shared the posted news <ref type="bibr" coords="2,436.49,295.41,16.80,9.94" target="#b16">[17]</ref>.</p><p>The CheckThat! lab is part of the conference and labs of the evaluation forum (CLEF) <ref type="bibr" coords="2,476.11,308.16,11.61,9.94" target="#b3">[4]</ref>, <ref type="bibr" coords="2,495.04,308.16,11.45,9.94" target="#b6">[7]</ref>. It has provided contests since 2018 <ref type="bibr" coords="2,222.34,320.64,12.80,9.94" target="#b0">[1]</ref> and attempts to assess competitors' systems each year related to factuality in different languages. It is divided into three challenges <ref type="bibr" coords="2,367.34,333.36,12.02,9.94" target="#b3">[4]</ref>: the first one is to predict which tweets are worth fact-checking <ref type="bibr" coords="2,211.03,346.08,16.82,9.94" target="#b17">[18]</ref>. Our team (SCUoL) participated in this competition last year and achieved the third-best result among eight other participating team <ref type="bibr" coords="2,367.34,358.56,16.80,9.94" target="#b17">[18]</ref>, <ref type="bibr" coords="2,391.32,358.56,16.84,9.94" target="#b18">[19]</ref>. The second challenge is to decide whether a posted claim can be verified or not <ref type="bibr" coords="2,326.04,371.28,16.80,9.94" target="#b19">[20]</ref>. The last task is task 3, which aims to predict the veracity of a news article <ref type="bibr" coords="2,234.82,384.00,11.61,9.94" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Datasets 3.1. Dataset provided from the competition</head><p>In this competition, the data was collected from 2010 to 2022 with more than one topic, such as elections, COVID-19 etc. <ref type="bibr" coords="2,189.91,471.38,16.80,9.94" target="#b20">[21]</ref>. The provided English dataset is about 1,300 articles for the training data with four main features: public id, the text, the title, and the rating or class-label of each article. In addition, the testing dataset includes more than 600 English articles with similar features except for the rating, which our proposed model aims to predict. The text provides the most important features that can help solve this multi-classification problem and determine the veracity of an article <ref type="bibr" coords="2,453.77,522.04,16.80,9.94" target="#b21">[22]</ref>. Therefore, we decided to use the article text only to be analyzed along with the rating. There are four different classes in the provided dataset (false, partially false, true, and other). We initially represent each category as a number since the proposed transformer model only accepts numerical classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">External datasets</head><p>Team NoFake, the winner of last year's competition, used additional datasets <ref type="bibr" coords="2,428.09,617.83,16.80,9.94" target="#b22">[23]</ref>, which increased their model performance. Therefore, we used an external dataset called the Fakenews Classification Datasets 2 from a Kaggle competition in our experiments. It contains more than 21,000 factual articles and over 23,000 fake articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Text pre-processing and machine learning models</head><p>2 https://www.kaggle.com/datasets/liberoliber/onion-notonion-datasets We conducted several experiments using various methods, including traditional machine learning and transformer-based models. Initially, the training datasets were divided into features and labels. In order for machine learning (ML) algorithms to be able to make predictions, all words included within each article's text have to be transformed into vectors. In this experiment, we used a statistical measure called TF-IDF. This measure stands for (Term Frequency -Inverse Document Frequency) and aims to evaluate how relevant each word is to a specific document in several documents <ref type="bibr" coords="3,440.33,137.71,16.80,9.94" target="#b23">[24]</ref>. The repeated words that may show in most or all documents, such as the words that, who, and which, will have a lower weight because these kinds of words will not add any valuable information for our predictions. The importance of each word will be determined by multiplying TF by IDF. In addition, we set some hyperparameters in the used vectorizer to minimize the unnecessary words, such as the 'stop words'. Then, we unified labels that indicate the same meaning and converted them into numerical features. For example, False, false, and untrue are represented as "1", and we also applied this to other labels. The last step in the pre-processing is splitting the data into training and testing with approximate ratios of 70% and 30%, respectively.</p><p>After that, we used four frequently used machine learning algorithms in classification problems: Random Forest Classifier (RFC), Linear Support Vector Classifier (SVC), Multinomial Naive Bayes (MNB), and Logistic Regression (LR). Then, we examined these models on the split training dataset and estimated the average prediction score over five folds of cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Transformer-based models</head><p>In addition to the traditional machine learning algorithms, we aimed to utilize transformer-based models. To simplify the process of using such models, we employed an NLP library called "Simple Transformers", which includes multiple models, each intended to perform a particular task. For example, the "ClassificationModel" is designed to implement binary and multi-class text classification tasks. It can also implement other tasks, such as named entity recognition, multi-label text classification, question answering, language generation, and more tasks. The applied simpletransformer-based model contains a classification layer on top of the chosen transformer model. This layer has four output neurons corresponding to each class (true, false, partially false, or other). After creating the model, we specified the selected pre-trained model types and architectures based on the following supported models:</p><p>• BERT: The acronym indicates Bidirectional Encoder Representations from Transformers. The BERT model differs from other language models because it is open-source and developed to pretrain deep bidirectional language representations exclusively by using a plain unsupervised text <ref type="bibr" coords="3,502.27,504.04,16.80,9.94" target="#b24">[25]</ref>.</p><p>• XLNet is a generalised autoregressive pretraining model. It combines a bidirectional context and avoids independent predictions to overcome the limitations of BERT based on its autoregressive formulation <ref type="bibr" coords="3,140.69,542.20,16.82,9.94" target="#b25">[26]</ref>.</p><p>• RoBERTa: A Robustly Optimised BERT Pretraining Approach is built based on BERT with a modification on the hyperparameters; for instance, it is trained for a more extended time on bigger batches and learning rates <ref type="bibr" coords="3,202.87,580.39,16.82,9.94" target="#b26">[27]</ref>.</p><p>• DistilBERT is another pre-trained transformer model. However, unlike previous models, this model is a distilled version that aims to reduce the size of a BERT model by 40 per cent and make it faster by 60 per cent while having more than 95 per cent of its language understanding abilities <ref type="bibr" coords="3,86.18,631.03,16.82,9.94" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Discussion</head><p>This section illustrates and discusses the results of the various experiments we conducted in this competition. As described in section 4, we investigated multiple ML algorithms (RFC, SVC, MNB, and LR) and transformer-based models (BERT, XLNet, RoBERTa, and DistilBERT). Firstly, we assessed the average prediction scores over five cross-validation folds. We only used the accuracy metric in the first experiment to choose the best performing ML classifier and then compare this selected classifier with transformer-based models based on the macro F1 score. The initial results indicate that the SVC usually outperforms other traditional ML models, as exhibited in Figure <ref type="figure" coords="4,442.49,74.56,5.52,9.94" target="#fig_2">1</ref> with an average accuracy score of 0.61 compared to LR, MNB, and RFC with 0.57, 0.52, and 0.52, respectively. After that, we employed a simple transformer multi-classification model and fine-tuned it on both the learning rate and the number of epochs hyperparameters. Since the BERT model has been proven to provide state-of-the-art results in many studies, we employed it as the simple transformer model type. We conducted over 20 experiments, using BERT-large-cased with a learning rate ranging from 1.00E-4 to 1.00E-6 and several epochs ranging from 5 to 25. We concluded that the best combination of these hyperparameters was using the 1.00E-5 learning rate with five epochs. Then, we examined the other transformer-based models using these parameters. However, the comparison shows that the used combination delivers better scores with the BERT model than other transformer-based models, as presented in Table <ref type="table" coords="4,155.81,201.07,4.15,9.94" target="#tab_0">1</ref>.</p><p>In addition, we attempt to enhance the model using additional datasets. We used the Fakenews Classification Datasets described in subsection 3.2 with different numbers of samples. We only used 500 fake news and 500 real news in the first attempt and combined them with the provided dataset. Accordingly, we increased the amount of news to 1000 and 2000 samples in two different tries. Nevertheless, the outcomes still indicate that the previous fine-tuned model using only the provided dataset from the contest outperformed all other attempts. The reason here could be that the additional dataset only includes binary labels (false or real), while this task aims to classify a multiclassification problem. As a result, we decided to train the final model with only the provided dataset from the competition so that the model can only see similar data in the same format. The testing results released on the leaderboard for this task show that our team achieved 0.305 on the F1 measurement. Our score nearly reached the highest, which is 0.339, and it is higher than the average F1 of other participants at 0.252.  For evaluation, we used a classification report and confusion matrix to assess our model during the validation phase. We also use these matrices to evaluate the submitted predictions on the released gold labels. Our observations show that our model performed better during the validation phase than in the testing, as illustrated in Figure <ref type="figure" coords="4,206.95,752.52,4.14,9.94" target="#fig_0">2</ref>. The received validation F1 scores are 0.77, 0.5, 0.57, and 0.28 for the   <ref type="figure" coords="5,281.14,112.48,5.52,9.94" target="#fig_1">3</ref> that the model mostly predicted the label 'false' with about 65% of the total predictions. In contrast, the positive labels are much fewer than the 'predictedas-false' labels. One possible reason is that the labelling criteria in the training set differ from the last released testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>Obtaining reliable information is considered an essential factor in our daily lives, especially when it comes to reading news. Due to the extensive number of published online articles, many developers have investigated and developed various models to tackle infodemic. This paper describes our system and participation in the "CLEF CheckThat! Lab" Task-3A competition. We examined an English dataset labelled as whether a particular article is 'true', 'false', 'partially false' and 'other'. We investigated four ML algorithms and pre-trained transformers to solve this multi-classification problem. Additionally, we attempted to use an external dataset from Kaggle to help improve the model. However, the additional dataset did not increase the performance, even though we used a different number of samples in each attempt. Finally, our findings from over 30 experiments show that the BERT model outperforms other models. The obtained testing results on the leaderboard indicate that we got an F1 of around 0.305, which slightly differs from the highest participant's score with only about 0.03. In future work, we recommend that finding an additional dataset with a similar format may help improve the model. Also, using an ensemble method, which considers both rule-based and deep learning methods, could significantly enhance the proposed system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,77.30,674.71,194.48,11.04;4,77.30,688.15,147.99,11.04;4,77.30,552.25,204.00,120.00"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: F1 scores for evaluating the model on the validation and testing sets</figDesc><graphic coords="4,77.30,552.25,204.00,120.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,307.08,674.71,198.74,11.04;4,306.95,552.25,214.55,120.00"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Confusion matrix on the testing set</figDesc><graphic coords="4,306.95,552.25,214.55,120.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,77.30,350.62,204.63,11.04;4,77.30,364.06,141.76,11.04;4,77.40,223.84,205.50,124.00"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Applying cross validation score function on the four ML models</figDesc><graphic coords="4,77.40,223.84,205.50,124.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,311.40,226.27,207.43,143.07"><head>Table 1</head><label>1</label><figDesc>The highest scores received from the employed models on the validation set 'true', 'partially false', and 'other', respectively. However, on the testing set, the model mispredicted most of the 'partially false' once, and none of the 'other' labels was predicted correctly, although it behaved close enough when predicting the 'false' and 'true' articles. Moreover, we observed from the presented confusion matrix in Figure</figDesc><table coords="4,313.56,268.75,205.27,100.59"><row><cell>Model Name</cell><cell>Acc.</cell><cell>F1 Score</cell></row><row><cell>LinearSVC</cell><cell>0.62</cell><cell>0.51</cell></row><row><cell>bert-base-cased</cell><cell>0.43</cell><cell>0.46</cell></row><row><cell>bert-large-cased</cell><cell>0.63</cell><cell>0.53</cell></row><row><cell>xlnet-large-cased</cell><cell>0.53</cell><cell>0.38</cell></row><row><cell>roberta.large</cell><cell>0.59</cell><cell>0.47</cell></row><row><cell>distilbert-base-cased</cell><cell>0.58</cell><cell>0.45</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">Acknowledgements</head><p>We would like to acknowledge and give our warmest thanks to the <rs type="institution">Ministry of Education in Saudi Arabia, King Abdulaziz University</rs>, and <rs type="institution">University of Leeds</rs> for their continuous support.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="5,103.97,500.81,419.15,10.05;5,103.97,513.29,419.62,10.05;5,103.97,526.12,225.07,9.94" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,181.32,500.81,341.80,10.05;5,103.97,513.29,133.71,10.04">Overview of the CLEF-2018 CheckThat! Lab on automatic identification and verification of political claims</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,262.66,513.40,260.93,9.94;5,103.97,526.12,132.17,9.94">International conference of the cross-language evaluation forum for european languages</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="372" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,103.97,538.73,419.18,10.05;5,103.97,551.21,301.17,10.04" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="5,187.58,538.73,335.57,10.05;5,103.97,551.21,96.65,10.04">Overview of the CLEF-2019 CheckThat! Lab: Automatic Identification and Verification of Claims</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-28577-7_25</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,103.97,563.93,419.05,10.05;5,103.97,576.68,365.76,10.05" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="5,220.01,563.93,303.01,10.04;5,103.97,576.79,161.15,9.94">CheckThat! at CLEF 2020: Enabling the automatic identification and verification of claims in social media</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-45442-5_65</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,103.97,589.16,419.29,10.05;5,103.97,601.88,419.53,10.05;5,103.97,614.71,57.96,9.94" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,179.54,589.16,343.72,10.05;5,103.97,601.88,154.30,10.05">The CLEF-2021 CheckThat! lab on detecting check-worthy claims, previously fact-checked claims, and fake news</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,282.82,601.99,208.55,9.94">European Conference on Information Retrieval</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="639" to="649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,103.97,627.32,419.18,10.05;5,103.97,639.80,188.40,10.04" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="5,188.50,627.32,334.65,10.05;5,103.97,639.80,152.09,10.04">Overview of the CLEF-2022 CheckThat! lab on fighting the COVID-19 infodemic and fake news detection</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,103.97,652.52,418.87,10.05;5,103.97,665.26,408.50,10.05" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Öhler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kishore</forename><surname>Gautam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julia</forename><forename type="middle">Maria</forename><surname>Struß</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sch</surname></persName>
		</author>
		<title level="m" coord="5,147.49,665.26,328.64,10.05">Overview of the CLEF-2022 CheckThat! lab task 3 on fake news detection</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,103.97,677.74,419.26,10.05;5,103.97,690.46,136.12,10.04" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="5,286.44,677.74,236.78,10.05;5,103.97,690.46,99.82,10.04">Overview of the CLEF-2021 CheckThat! Lab: Task 3 on fake news detection</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Struß</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,103.97,703.18,419.46,10.05;5,103.97,715.66,387.12,10.05" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,178.59,703.18,344.84,10.05;5,103.97,715.66,64.56,10.04">The CLEF-2022 CheckThat! lab on fighting the COVID-19 infodemic and fake news detection</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,191.11,715.77,207.79,9.94">European Conference on Information Retrieval</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="416" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,103.97,728.38,419.18,10.04;5,103.97,741.10,362.62,10.05" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="5,359.19,728.38,163.96,10.04;5,103.97,741.10,112.57,10.04">An exploratory study of covid-19 misinformation on twitter</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dirkson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Majchrzak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,227.62,741.21,120.96,9.94">Online Soc. networks media</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">100104</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,103.97,753.85,418.77,10.04;6,103.97,74.56,130.87,9.94" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="5,226.07,753.85,238.30,10.04">Survey on Steps of Truth Detection on Arabic Tweets</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mouty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gazdar</surname></persName>
		</author>
		<idno type="DOI">10.1109/NCG.2018.8593060</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,103.97,86.93,418.90,10.04;6,103.97,99.65,377.56,10.05" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="6,360.52,86.93,162.35,10.04;6,103.97,99.65,183.38,10.05">Classifying Arabic tweets based on credibility using content and user features</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jardaneh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Abdelhaq</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Buzz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.1109/JEEIT.2019.8717386</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,103.97,112.37,418.89,10.04;6,103.97,125.09,234.42,10.05" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,240.65,112.37,282.21,10.04;6,103.97,125.09,58.90,10.04">Fake News: a survey of research, Detection Methods, and Opportunities</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zafarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,174.79,125.20,87.80,9.94">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,103.97,137.60,419.02,10.05;6,103.97,150.32,342.13,10.04" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="6,467.99,137.60,55.00,10.04;6,103.97,150.32,129.52,10.04">Detection of Online Fake News : A Survey</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gaonkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Itagi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Chalippatt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gaonkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Aswale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Shetgaonkar</surname></persName>
		</author>
		<idno type="DOI">10.1109/ViTECoN.2019.8899556</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,103.97,163.04,419.22,10.05;6,103.97,175.63,240.89,9.94" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="6,235.87,163.04,287.32,10.05;6,103.97,175.63,61.19,9.94">FakeCovid--A multilingual cross-domain fact check news dataset for COVID-19</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nandini</surname></persName>
		</author>
		<idno>arXiv2006.11343</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr.</note>
</biblStruct>

<biblStruct coords="6,103.97,188.24,419.13,10.05;6,103.97,201.07,266.18,9.94" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="6,199.59,188.24,171.89,10.04">Detecting Fake News on Social Media</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.2200/s00926ed1v01y201906dmk018</idno>
	</analytic>
	<monogr>
		<title level="j" coord="6,385.82,188.35,137.28,9.94;6,103.97,201.07,31.60,9.94">Synth. Lect. Data Min. Knowl. Discov</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,103.97,213.44,418.56,10.04;6,103.97,226.29,311.64,9.94" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="6,223.42,213.44,299.11,10.04;6,103.97,226.29,77.64,9.94">A Survey of Fake News: Fundamental Theories, Detection Methods, and Opportunities</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zafarani</surname></persName>
		</author>
		<idno type="DOI">10.1145/3395046</idno>
	</analytic>
	<monogr>
		<title level="j" coord="6,193.51,226.29,87.80,9.94">ACM Comput. Surv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,103.97,238.90,418.59,10.04;6,103.97,251.62,419.45,10.05;6,103.97,264.21,237.41,9.94" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="6,389.93,238.90,132.63,10.04;6,103.97,251.62,323.39,10.05">Credibility detection in twitter using word n-gram analysis and supervised machine learning techniques</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">Y</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">H</forename><surname>Gomaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Khoriba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">H</forename><surname>Haggag</surname></persName>
		</author>
		<idno type="DOI">10.22434/IFAMR2019.0020</idno>
	</analytic>
	<monogr>
		<title level="j" coord="6,440.33,251.73,83.09,9.94;6,103.97,264.21,54.35,9.94">Int. Food Agribus. Manag. Rev</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,103.97,276.82,419.10,10.05;6,103.97,289.54,305.95,10.05" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="6,184.92,276.82,338.15,10.05;6,103.97,289.54,179.53,10.04">Overview of the CLEF-2021 CheckThat! lab task 1 on check-worthiness estimation in tweets and political debates</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Work. Notes CLEF</note>
</biblStruct>

<biblStruct coords="6,103.97,302.02,418.86,10.05;6,103.97,314.77,187.45,10.05" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="6,287.53,302.02,235.30,10.05;6,103.97,314.77,150.94,10.05">SCUoL at CheckThat! 2021: An AraBERT model for check-worthiness of Arabic tweets</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Althabiti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Alsalka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Atwell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,103.97,327.49,419.10,10.05;6,103.97,339.97,419.54,10.05;6,103.97,352.80,105.02,9.94" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="6,178.46,327.49,344.62,10.05;6,103.97,339.97,227.05,10.05">Overview of the CLEF-2021 CheckThat! Lab Task 2 on detecting previously fact-checked claims in tweets and political debates</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,356.30,340.08,133.85,9.94">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2021">2021. 2936</date>
			<biblScope unit="page" from="393" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,103.97,365.41,419.76,10.05;6,103.97,378.24,138.13,9.94" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="6,171.37,365.41,314.68,10.04">Amused: An annotation framework of multi-modal social media data</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<idno>arXiv2010.00502</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr.</note>
</biblStruct>

<biblStruct coords="6,103.97,390.61,418.93,10.04;6,103.97,403.35,364.32,10.05" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="6,342.89,390.61,180.01,10.04;6,103.97,403.35,78.63,10.05">Content based fake news detection using knowledge graphs</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pavlova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,205.99,403.46,169.32,9.94">International semantic web conference</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="669" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,103.97,416.07,419.48,10.05;6,103.97,428.66,107.17,9.94" xml:id="b22">
	<monogr>
		<title level="m" type="main" coord="6,164.05,416.07,285.12,10.04">NoFake at CheckThat! 2021: fake news detection using BERT</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kumari</surname></persName>
		</author>
		<idno>arXiv2108.05419</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr.</note>
</biblStruct>

<biblStruct coords="6,103.97,441.27,419.39,10.05;6,103.97,453.99,419.17,10.05;6,103.97,466.82,140.33,9.94" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="6,249.98,441.27,273.37,10.05;6,103.97,453.99,96.80,10.04">Comparison of various machine learning models for accurate detection of fake news</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Poddar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">S</forename><surname>Umadevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,224.98,454.10,298.16,9.94;6,103.97,466.82,38.93,9.94">2019 Innovations in Power and Advanced Computing Technologies (i-PACT)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,103.97,479.19,419.22,10.05;6,103.97,491.93,354.46,10.05" xml:id="b24">
	<monogr>
		<title level="m" type="main" coord="6,348.02,479.19,175.17,10.05;6,103.97,491.93,176.66,10.05">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>arXiv1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr.</note>
</biblStruct>

<biblStruct coords="6,103.97,504.65,418.86,10.04;6,103.97,517.13,419.52,10.05;6,103.97,529.96,24.84,9.94" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="6,439.39,504.65,83.44,10.04;6,103.97,517.13,234.89,10.04">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,351.26,517.24,132.61,9.94">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,103.97,542.57,419.00,10.05;6,103.97,555.16,107.17,9.94" xml:id="b26">
	<monogr>
		<title level="m" type="main" coord="6,181.66,542.57,266.29,10.04">Roberta: A robustly optimized bert pretraining approach</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno>arXiv1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr.</note>
</biblStruct>

<biblStruct coords="6,103.97,567.77,419.03,10.04;6,103.97,580.52,328.03,10.05" xml:id="b27">
	<monogr>
		<title level="m" type="main" coord="6,332.42,567.77,190.58,10.04;6,103.97,580.52,151.45,10.04">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno>arXiv1910.01108</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Prepr</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
