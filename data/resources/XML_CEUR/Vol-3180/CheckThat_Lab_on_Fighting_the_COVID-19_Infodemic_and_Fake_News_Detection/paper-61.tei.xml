<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,410.26,15.42;1,89.29,106.66,320.79,15.42">Awakened at CheckThat! 2022: Fake News Detection using BiLSTM and Sentence Transformer</title>
				<funder ref="#_jTMAPHA">
					<orgName type="full">German Federal Ministry of Education and Research (BMBF)</orgName>
				</funder>
				<funder ref="#_PrewBHx">
					<orgName type="full">German Academic Exchange Service (DAAD)</orgName>
				</funder>
				<funder ref="#_wuMdVWJ">
					<orgName type="full">EU CEF</orgName>
				</funder>
				<funder ref="#_CEhTxnD">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_FwnnBEg">
					<orgName type="full">European Union</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,119.22,11.96"><forename type="first">Ciprian-Octavian</forename><surname>TruicƒÉ</surname></persName>
							<email>ciprian-octavian.truica@it.uu.se</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Technology</orgName>
								<orgName type="institution">Uppsala University</orgName>
								<address>
									<addrLine>L√§gerhyddsv√§gen 1</addrLine>
									<postCode>75105</postCode>
									<settlement>Uppsala</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Computer Science and Engineering Department</orgName>
								<orgName type="department" key="dep2">Faculty of Automatic Control and Computers</orgName>
								<orgName type="institution">University Politehnica of Bucharest</orgName>
								<address>
									<addrLine>Splaiul Independent, ei 313</addrLine>
									<postCode>060042</postCode>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,226.44,134.97,108.83,11.96"><forename type="first">Elena-Simona</forename><surname>Apostol</surname></persName>
							<email>elena-simona.apostol@it.uu.se</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Technology</orgName>
								<orgName type="institution">Uppsala University</orgName>
								<address>
									<addrLine>L√§gerhyddsv√§gen 1</addrLine>
									<postCode>75105</postCode>
									<settlement>Uppsala</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Computer Science and Engineering Department</orgName>
								<orgName type="department" key="dep2">Faculty of Automatic Control and Computers</orgName>
								<orgName type="institution">University Politehnica of Bucharest</orgName>
								<address>
									<addrLine>Splaiul Independent, ei 313</addrLine>
									<postCode>060042</postCode>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,371.55,134.97,76.44,11.96"><forename type="first">Adrian</forename><surname>Paschke</surname></persName>
							<email>adrian.paschke@fokus.fraunhofer.de</email>
							<affiliation key="aff2">
								<orgName type="institution">Fraunhofer Institute for Open Communication Systems</orgName>
								<address>
									<postCode>10589</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,410.26,15.42;1,89.29,106.66,320.79,15.42">Awakened at CheckThat! 2022: Fake News Detection using BiLSTM and Sentence Transformer</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">10D0016437C087F80274F0B997EB3458</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fake News Detection</term>
					<term>Neural Networks</term>
					<term>Sentence Transformers</term>
					<term>Transfer Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, online social networks and online news venues have become some of the main news and event-related information spreading mediums. Although using these mediums has facilitated the speed of accessing information, it also created a new phenomenon used for propaganda and disinformation: fake news. As fake news has detrimental consequences to society, new technologies need to be developed in order to stop their harmful effects. In this paper, we propose two Bidirectional Long Short-Term Memory (BiLSTM) architectures with sentence transformers to solve two tasks: (1) a multi-class mono-lingual fake news detection task (i.e., mono-lingual task); and (2) a multi-class cross-lingual fake news detection task (i.e., cross-lingual task). For the mono-lingual task, we train and test a BiLSTM with BART sentence transformers model on an English dataset and obtain an accuracy of ‚àº 0.53 and an F1-Score of ‚àº 0.32. For the cross-lingual task, we train a BiLSTM with XLM sentence transformers model on an English dataset and test the model using transfer learning on a German dataset. For this task, we obtain an accuracy of ‚àº 0.28 and an F1-Score of ‚àº 0.19.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the digital age, new mass media paradigms for information distribution have been adopted by the general public. The current paradigms have shifted from the journalistic rigorous imposed by editors to personalized social media where anyone can spread event related news. This new approach aggravates the risk of fake news <ref type="bibr" coords="1,308.52,500.93,11.49,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,323.19,500.93,7.65,10.91" target="#b1">2]</ref>, which has detrimental consequences to society by facilitating the spread of misinformation in the form of fake news, propaganda, conspiracy theories, political bias, etc. The practices of spreading misinformation online by malicious actors need to be tackled from different points of view, i.e., from a journalistic and fact-checking perspective to a more technological-based one.</p><p>In this paper, we address the problem of detecting fake news from a technological perspective by using the CheckThat! 2022: Fake News Detection Challenge datasets. To tackle the problem we propose two neural network with sentence transformer models for (1) multi-class monolingual fake news detection; and (2) multi-class cross-lingual fake news detection. We use transfer learning in order to train a model on an English dataset and test it on German text.</p><p>In this work, we aim to answer the following two research questions:</p><p>(ùëÑ 1 ) Does a simple neural network with sentence transformers offer good results for multi-class mono-lingual fake news detection? (ùëÑ 2 ) Can cross-lingual sentence transforms be used through transfer learning in multi-class cross-lingual fake news detection?</p><p>To answer question (ùëÑ 1 ), we propose the use of a Bidirectional Long Short-Term Memory (BiLSTM) neural network with BART sentence transformers. While to answer question (ùëÑ 2 ), we train a BiLSTM neural network with XLM sentence transformers on English textual data and use transfer learning to solve a cross-lingual fake news detection task by testing the model on German textual data.</p><p>This paper is structured as follows. In Section 2, we discuss some of the current literature on fake news detection. In Section 3, we present our approach for mono-lingual and cross-lingual fake news detection. In Section 4, we present the datasets, experimental setup, and results. Finally, in Section 5, we summarize our findings and hint at future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The task of fake news detection has been examined from various perspectives and using various models from traditional Machine Learning to more elaborate yet more powerful Neural Network based models. Extensive work in the field of fake news detection has led to many solutions focusing on either model or data-driven approaches. Among the traditional Machine Learning models (e.g., Support Vector Machine, Logistic Regression, Decision Trees, AdaBoost, Na√Øve Bayes), the model that performs very good in many cases is Multinomial Na√Øve Bayes <ref type="bibr" coords="2,472.43,449.41,11.43,10.91" target="#b2">[3]</ref>.</p><p>Several current solutions use complex Deep Neural Network architectures for this task. Many solutions for multi-class mono-lingual fake news detection show promising results when using Convolutional Neural Network (CNN) based architectures. FNDNet <ref type="bibr" coords="2,390.60,490.06,12.79,10.91" target="#b3">[4]</ref> is such an architecture that obtains good results in comparison even with recurrent networks, i.e., LSTM. OPCNN-FAKE <ref type="bibr" coords="2,117.44,517.16,12.89,10.91" target="#b4">[5]</ref> is an optimized CNN based solution that uses a hyperopt optimization technique to adapt the values of parameters for each component layer in order to achieve high performance. Other Deep Learning solutions focus on recurrent networks, e.g., (Bi)GRU, (Bi)GRU, (Bi)LSTM, obtaining the best results when also using attention mechanisms <ref type="bibr" coords="2,380.43,557.81,11.43,10.91" target="#b0">[1]</ref>. BiLSTM based solutions (e.g., Samantaray and Kumar <ref type="bibr" coords="2,216.41,571.36,11.39,10.91" target="#b5">[6]</ref>, Trueman et al. <ref type="bibr" coords="2,299.13,571.36,12.02,10.91" target="#b6">[7]</ref>) are very promising as this type of recurrent network is able to capture both past and future information.</p><p>In multi-class classification, the employed embedding model is very important. As shown in Ilie et al. <ref type="bibr" coords="2,141.42,612.00,11.42,10.91" target="#b0">[1]</ref>, many Deep Learning models have an increase in accuracy when using custom trained word embeddings versus pre-trained ones. Other models use advanced pre-trained transformers instead of the more classical word embeddings. Different transformer models can be applied for fake news detection, e.g., BERT (Bidirectional Encoder Representations from Transformers) <ref type="bibr" coords="2,156.01,666.20,11.53,10.91" target="#b7">[8]</ref>, RoBERTa (A Robustly Optimized BERT pre-training Approach) <ref type="bibr" coords="2,460.97,666.20,11.53,10.91" target="#b8">[9]</ref>, BART (Bidirectional and Autoregressive Transformer) <ref type="bibr" coords="3,305.37,86.97,16.36,10.91" target="#b9">[10]</ref>. As such, MisRoBAERTa <ref type="bibr" coords="3,434.73,86.97,12.95,10.91" target="#b1">[2]</ref> is a complex architecture that combines BART and RoBERTa for a multi-class classification task. Another solution to the multi-class classification problem is proposed by Liu et al. <ref type="bibr" coords="3,429.45,114.06,17.91,10.91" target="#b10">[11]</ref> that offers a two-stage BERT-based model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we present the methodology used for fake news detection. For encoding the text, we used two sentence transformer <ref type="bibr" coords="3,245.27,199.79,17.93,10.91" target="#b11">[12]</ref> approaches. (1) For multi-class fake news detection of news articles in English, we use BART (Bidirectional and Auto-Regressive Transformers) <ref type="bibr" coords="3,488.06,213.34,17.92,10.91" target="#b9">[10]</ref> sentence transformers. (2) For cross-lingual news articles, we use XLM (Cross-Lingual Language Model) <ref type="bibr" coords="3,123.63,240.44,18.01,10.91" target="#b12">[13]</ref> sentence transformers. We employed a BiLSTM (Bidirectional Long Short-Terms Memory) as the classification model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sentence Transformers</head><p>Sentence transformers are a modification of the pre-trained BERT networks that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity <ref type="bibr" coords="3,269.54,330.81,16.14,10.91" target="#b11">[12]</ref>. We construct BART <ref type="bibr" coords="3,383.07,330.81,17.82,10.91" target="#b9">[10]</ref> and XLM <ref type="bibr" coords="3,447.12,330.81,17.81,10.91" target="#b12">[13]</ref> sentence transformers for the mono-lingual and cross-lingual classification tasks, respectively.</p><p>XLM <ref type="bibr" coords="3,124.31,357.91,17.76,10.91" target="#b12">[13]</ref> is a Transformer architecture that uses two approaches during pre-training depending on the type of data. For mono-lingual data, it uses an unsupervised modeling technique such as Casual Language Modeling (CLM) or Masked Language Modeling (MLM). For cross-lingual data, XLM employs a supervised modeling technique that combines MLM with Translation Language Modeling (TLM).</p><p>BART <ref type="bibr" coords="3,129.91,425.66,17.94,10.91" target="#b9">[10]</ref> is a generalized BERT that uses a transformer-based neural machine translation architecture. The architecture uses a left-to-right decoder (as in GPT <ref type="bibr" coords="3,400.52,439.20,18.02,10.91" target="#b13">[14]</ref> architecture) and a standard Sequence-to-Sequence bidirectional encoder (as in BERT <ref type="bibr" coords="3,385.22,452.75,11.09,10.91" target="#b7">[8]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Classification Models</head><p>For classification, we propose a deep neural network architecture that contains the following layers: (1) Input layer; (2) BiLSTM layer; and (3) Dense layer.</p><p>The input layer instantiates the neural network. It is used to produce a symbolic tensor-like object that has the size of the sentence transformer.</p><p>LSTM (Long Short-Term Memory)) <ref type="bibr" coords="3,265.67,556.68,18.07,10.91" target="#b14">[15]</ref> is a recurrent neural network that process past information using two state components: (1) a hidden layer for the short-term memory; and</p><p>(2) an internal cell state for long-term memory. The BiLSTM layer encapsulates both past and future information through the use of two hidden states. The forward hidden state processes the past information using a forward LSTM, while the backwards hidden state process the future information provided by employing a backward LSTM. To encode both the past and future, the BiLSTM concatenate into on hidden state the forward and backward hidden state at every time-step. The number of units for this layer can be determined experimentally using ablation and hyperparameter testing (see <ref type="bibr" coords="3,239.77,665.07,12.99,10.91" target="#b1">[2]</ref> for more details). For the LSTM cell, we use the classic implementation presented in <ref type="bibr" coords="4,219.09,86.97,16.36,10.91" target="#b14">[15,</ref><ref type="bibr" coords="4,238.18,86.97,12.27,10.91" target="#b15">16]</ref>. For this cell, the recurrent activation function is sigmoid, the kernel weights are initialized using the Glorot uniform linear transformation <ref type="bibr" coords="4,449.74,100.52,16.22,10.91" target="#b16">[17]</ref>, and the bias vector is initialized with zeros.</p><p>The Dense Layer is a fully connected Perceptron layer used for classification. The number of units in this layer is equal to the number of classes. The activation function for this layer is the sigmoid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we present the experimental results of the proposed models for the CheckThat! 2022 Fake News Detection task for both the mono-lingual and cross-lingual challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>The CheckThat! 2022 Task 3 <ref type="bibr" coords="4,218.00,276.61,16.39,10.91" target="#b17">[18,</ref><ref type="bibr" coords="4,237.13,276.61,14.01,10.91" target="#b18">19]</ref> consists of two subtasks as follows: (1) multi-class monolingual fake news detection of news articles (English) <ref type="bibr" coords="4,329.39,290.16,16.43,10.91" target="#b19">[20,</ref><ref type="bibr" coords="4,348.54,290.16,14.03,10.91" target="#b20">21]</ref> (i.e., mono-lingual task); and (2) multi-class cross-lingual fake news detection task (German) (i.e., cross-lingual task). The steps used in the data collection are defined in Shahi <ref type="bibr" coords="4,300.28,317.26,16.25,10.91" target="#b21">[22]</ref>.</p><p>For the mono-lingual task, the English training data is the same as from the CheckThat! 2021 version <ref type="bibr" coords="4,125.49,344.36,16.41,10.91" target="#b22">[23]</ref>. The number of classes for this task is four: false, partially false, other, and true. The number of labels has been defined after a thorough study of 83 classes was conducted by fact-checkers <ref type="bibr" coords="4,151.12,371.46,16.25,10.91" target="#b23">[24]</ref>. The dataset contains an English training, development, and testing set.</p><p>For the cross-lingual task, a new test dataset in German is introduced. The main focus of this task is to use transfer learning to detect fake news content in low resource languages. Thus, the training for this task is done on the English training and development datasets, and then it is tested on the German testing set. For this task, we use the same labels as for the mono-lingual task.</p><p>As we used the same training data for both the mono-lingual and cross-lingual tasks, we concatenated the English training and development sets to train the model. Table <ref type="table" coords="4,453.38,466.30,5.07,10.91" target="#tab_0">1</ref> shows the label distribution for the training dataset. We observe that the dataset is highly imbalanced. Table <ref type="table" coords="4,115.79,493.40,5.07,10.91">2</ref> presents the label distribution for the test dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Setup</head><p>For the sentence transformer, we used the pre-trained BART (facebook/bart-large) and XLM (sentence-transformers/stsb-xlm-r-multilingual) from HuggingFace Transformer <ref type="bibr" coords="5,443.86,260.31,16.28,10.91" target="#b24">[25]</ref>. We train the sentence transformers using the SentenceTransformers Python 3 package <ref type="bibr" coords="5,435.94,273.85,16.25,10.91" target="#b11">[12]</ref>. For classification, the BiLSTM layer uses 100 LSTM units configured as in <ref type="bibr" coords="5,435.94,287.40,16.42,10.91" target="#b14">[15]</ref>. The dense layer contains 4 units (equal to the number of classes), and the sigmoid function as activation. We used the ADAM optimizer and a 64 batch size. The model is trained for 100 epochs. To prevent overfitting, we used an early stopping mechanism that monitors the Accuracy during training. We use Keras with TensorFlow as backend for implementing the neural model.</p><p>The implementation is available online on GitHub at the following url: https://github.com/ elena-apostol/AwakenedCheckThat2022.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>Table <ref type="table" coords="5,115.87,418.43,5.09,10.91" target="#tab_1">3</ref> presents the overall results for both the mono-lingual (i.e., both train and test sets use English texts) and cross-lingual (i.e., the train set is in English and the test set is in German). We observe that, by training the BiLSTM with BART sentence embeddings on English, we obtain an accuracy of ‚àº 0.53 for the mono-lingual task and an accuracy of ‚àº 0.28 for the cross-lingual task. With these results, the Awakened team obtained the 3 ùëüùëë and the 5 ùë°‚Ñé place in the competition for the mono-lingual task and cross-lingual task, respectively. As a general observation, the results are highly influenced by the dataset's size and class imbalance. For the mono-lingual task, the low performance of the model is directly impacted by two dataset related aspects: (1) the dataset size is small, being inadequate for a neural network approach; and (2) the dataset is highly imbalanced, miss-classification being a real challenge. When analyzing the evaluation metrics per class (Table <ref type="table" coords="5,344.23,655.78,3.65,10.91" target="#tab_3">4</ref>), these two aspects are even more emphasized by the results obtained per class. For the false label, the model obtains ‚àº 0.67 precision and ‚àº 0.83 recall. Thus, the interpretation of these results shows that the models manage to correctly determine fake news. For the true label, the model obtains ‚àº 0.76 precision and ‚àº 0.21 recall. This shows that the model also manages to discriminate well between true news and the other 3 types of texts. Also, these results show that the contextual, semantic, and syntactic information encoded by the sentence transformer for the true and false labels are very specific to this classes. Thus, the textual dissimilarities between fake news and real news are more prominent. The precision and recall for the partially true (‚àº 0.13 precision and ‚àº 0.32 recall) and other (‚àº 0.04 precision and ‚àº 0.03 recall) classes are very small. These results indicate that these textual data are more similar to the other two classes. Thus, the model does not manage to discriminate correctly between these two labels and the true and false ones. For the cross-lingual task, we observe that the model manages to obtain an accuracy of ‚àº 0.28 and an F1-Score of ‚àº 0.19 (Table <ref type="table" coords="6,240.42,364.00,3.64,10.91" target="#tab_1">3</ref>). These results are also impacted by the transfer learning algorithm besides the dataset's size and the imbalanced labels. Based on these observations, we can conclude that the multi-lingual sentence transformers do not manage to correctly find similarities between the English and German texts that are labeled with the same class. When analyzing the per class results, we observe that the true labeled German documents are predicted with a high precision (‚àº 0.59), but the recall for these labeled documents is ‚àº 0.05. The interpretation of these values for the true label is that the model manages to determine the true positives more accurately than false negatives. In other words, for the true labeled documents, the model manages to return more relevant results to this label than irrelevant ones but does not manages to return most of the relevant results for this label. For the false labeled German documents, the interpretation of the results is, as expected, in reverse as for the true labeled documents. Thus, with a precision of ‚àº 0.35 and a recall of 0.65, the model manages to return most of the relevant results for this label but does not manage to return all relevant results to this label. The model does not manage to classify any of the German documents in the test set labeled with other and it has a very low F1-Score for the prediction of documents labeled with partially true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we trained two BiLSTM neural networks with sentence transformers for data encoding models to detect the veracity of fake news. The first model is trained and tested on an English news article dataset for multi-class mono-lingual fake news detection. This model encodes the textual data using BART sentence transformer. The second model is trained on the same English dataset and tested on a German dataset for multi-class cross-lingual fake news detection. The model encodes the textual data using XLM sentence transformer and takes advantage of transfer learning to solve the task of cross-lingual fake news detection. We use the first model to answer our first research question (ùëÑ 1 ). With an accuracy of ‚àº 0.53 and a F1-Score of ‚àº 0.32, we conclude that it is worth investigating more the use of simple neural networks with sentence transformers for mono-lingual fake news detection task. The second model is used to answer our second research question (ùëÑ 2 ). With an accuracy of ‚àº 0.28 and a F1-Score of ‚àº 0.19, we conclude that the BiLSTM XML sentence transform model does not manage to correctly find similarities between the English and German texts. Although, the use of cross-lingual transformers and transfer learning for multi-class classification in theory could prove useful, for the multi-class cross-lingual fake news detection task at head, they perform poorly. For both mono-lingual and cross-lingual tasks, we observed that: (1) the dataset size needs to be large to be adequate for a neural network approach; and (2) the dataset needs a balanced label distribution to mitigate against miss-classification.</p><p>In future work, we aim to use transformer embeddings instead of sentence transformers. We also plan to test other cross-lingual transformers for transfer learning in a larger study to determine if the conclusions obtained on this small dataset generalize or are obtained by this data-driven approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,522.41,308.50,108.99"><head>Table 1</head><label>1</label><figDesc>Train dataset statistics</figDesc><table coords="4,197.78,551.75,199.71,79.65"><row><cell>Label</cell><cell cols="2">No. Documents Percentage</cell></row><row><cell>false</cell><cell>578</cell><cell>45.73%</cell></row><row><cell>partially false</cell><cell>358</cell><cell>28.32%</cell></row><row><cell>true</cell><cell>211</cell><cell>16.69%</cell></row><row><cell>other</cell><cell>117</cell><cell>9.26%</cell></row><row><cell>Total</cell><cell>1 264</cell><cell>100.00%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,88.99,528.73,131.31,20.87"><head>Table 3</head><label>3</label><figDesc>Overall results for the two tasks</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,138.51,558.07,318.25,38.30"><head>Task Dataset Sentence Transformer Accuracy F1-Score</head><label></label><figDesc></figDesc><table coords="5,138.51,575.56,313.89,20.82"><row><cell>Mono-lingual English</cell><cell>BART</cell><cell>0.531045</cell><cell>0.323094</cell></row><row><cell cols="2">Cross-lingual German XML</cell><cell>0.283276</cell><cell>0.185991</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,88.99,237.92,314.68,93.79"><head>Table 4</head><label>4</label><figDesc>Detailed results for the multi-class mono-lingual task using English</figDesc><table coords="6,191.61,269.48,212.06,62.22"><row><cell>Class</cell><cell cols="2">Precision Recall</cell><cell>F1-score</cell></row><row><cell>false</cell><cell>0.673521</cell><cell cols="2">0.831746 0.744318</cell></row><row><cell cols="2">partially false 0.129496</cell><cell cols="2">0.321428 0.184615</cell></row><row><cell>true</cell><cell>0.758620</cell><cell cols="2">0.209523 0.328358</cell></row><row><cell>other</cell><cell>0.038461</cell><cell cols="2">0.032258 0.035087</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,88.99,90.49,314.68,93.79"><head>Table 5</head><label>5</label><figDesc>Detailed results for the multi-class cross-lingual task using German</figDesc><table coords="7,191.61,122.05,212.06,62.22"><row><cell>Class</cell><cell cols="2">Precision Recall</cell><cell>F1-score</cell></row><row><cell>false</cell><cell>0.345303</cell><cell cols="2">0.654450 0.452079</cell></row><row><cell cols="2">partially false 0.145833</cell><cell cols="2">0.288659 0.193771</cell></row><row><cell>true</cell><cell>0.590909</cell><cell cols="2">0.053497 0.098113</cell></row><row><cell>other</cell><cell>0.000000</cell><cell cols="2">0.000000 0.000000</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The research presented in this paper was supported in part by the <rs type="funder">German Academic Exchange Service (DAAD)</rs> through the projects "<rs type="projectName">AWAKEN: content-Aware and netWork-Aware faKE News mitigation</rs>" (grant no. <rs type="grantNumber">91809005</rs>) "<rs type="projectName">Deep-Learning Anomaly Detection for Human and Automated Users Behavior"</rs> (grant no. <rs type="grantNumber">91809358</rs>), in part by the <rs type="funder">German Federal Ministry of Education and Research (BMBF)</rs> project "<rs type="projectName">PANQURA -a technology platform for more information transparency in times of crisis</rs>" under Grant <rs type="grantNumber">03COV03F</rs>, in part by the <rs type="funder">European Union</rs> project "<rs type="projectName">FAST-LISA -Fighting hAte Speech Through a Legal, ICT and Sociolinguistic approach</rs>" under Grant <rs type="grantNumber">101049342</rs>, and in part by the <rs type="funder">EU CEF</rs> project "<rs type="projectName">NORDIS -NORdic observatory for digital media and information DISorder</rs>" under Grant <rs type="grantNumber">number2394203</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_PrewBHx">
					<idno type="grant-number">91809005</idno>
					<orgName type="project" subtype="full">AWAKEN: content-Aware and netWork-Aware faKE News mitigation</orgName>
				</org>
				<org type="funded-project" xml:id="_jTMAPHA">
					<idno type="grant-number">91809358</idno>
					<orgName type="project" subtype="full">Deep-Learning Anomaly Detection for Human and Automated Users Behavior&quot;</orgName>
				</org>
				<org type="funded-project" xml:id="_FwnnBEg">
					<idno type="grant-number">03COV03F</idno>
					<orgName type="project" subtype="full">PANQURA -a technology platform for more information transparency in times of crisis</orgName>
				</org>
				<org type="funded-project" xml:id="_wuMdVWJ">
					<idno type="grant-number">101049342</idno>
					<orgName type="project" subtype="full">FAST-LISA -Fighting hAte Speech Through a Legal, ICT and Sociolinguistic approach</orgName>
				</org>
				<org type="funded-project" xml:id="_CEhTxnD">
					<idno type="grant-number">number2394203</idno>
					<orgName type="project" subtype="full">NORDIS -NORdic observatory for digital media and information DISorder</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,112.66,111.28,394.61,10.91;8,112.28,124.83,393.71,10.91;8,112.33,138.38,280.68,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,322.94,111.28,184.33,10.91;8,112.28,124.83,321.05,10.91">Context-Aware Misinformation Detection: A Benchmark of Deep Learning Architectures Using Word Embeddings</title>
		<author>
			<persName coords=""><forename type="first">V.-I</forename><surname>Ilie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-O</forename><surname>TruicƒÉ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E.-S</forename><surname>Apostol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paschke</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2021.3132502</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,442.63,124.83,55.43,10.91">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="162122" to="162146" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,151.93,395.17,10.91;8,112.66,165.48,259.73,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,236.65,151.93,230.20,10.91">MisRoBAERTa: Transformers versus Misinformation</title>
		<author>
			<persName coords=""><forename type="first">C.-O</forename><surname>TruicƒÉ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E.-S</forename><surname>Apostol</surname></persName>
		</author>
		<idno type="DOI">10.3390/math10040569</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,475.59,151.93,32.24,10.91;8,112.66,165.48,28.93,10.91">Mathematics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">569</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,179.03,393.33,10.91;8,112.66,192.57,340.55,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,323.83,179.03,182.15,10.91;8,112.66,192.57,167.87,10.91">Performance of Bernoulli&apos;s Na√Øve Bayes classifier in the detection of fake news</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">W</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">S</forename><surname>Bedi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,289.56,192.57,131.73,10.91">Materials Today: Proceedings</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,206.12,393.33,10.91;8,112.66,219.67,397.48,10.91;8,112.36,235.66,150.76,7.90" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,327.97,206.12,178.01,10.91;8,112.66,219.67,146.27,10.91">FNDNet -A deep convolutional neural network for fake news detection</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">K</forename><surname>Kaliyar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cogsys.2019.12.005</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,268.39,219.67,128.14,10.91">Cognitive Systems Research</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="32" to="44" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,246.77,393.33,10.91;8,112.66,260.32,314.51,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,287.02,246.77,218.96,10.91;8,112.66,260.32,142.54,10.91">OPCNN-FAKE: Optimized convolutional neural network for fake news detection</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Alharbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Alsamhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,263.50,260.32,54.37,10.91">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="129471" to="129489" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,273.87,393.33,10.91;8,112.66,287.42,394.53,10.91;8,112.66,300.97,55.16,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,235.93,273.87,270.06,10.91;8,112.66,287.42,152.26,10.91">Bi-directional Long Short-Term Memory Network for Fake News Detection from Social Media</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Samantaray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,288.55,287.42,145.03,10.91">Intelligent and Cloud Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="463" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,314.52,393.32,10.91;8,112.66,328.07,397.48,10.91;8,112.36,344.06,38.01,7.90" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,348.89,314.52,157.10,10.91;8,112.66,328.07,63.86,10.91">Attention-based C-BiLSTM for fake news detection</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">E</forename><surname>Trueman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Narayanasamy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vidya</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.asoc.2021.107600</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,184.64,328.07,106.52,10.91">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page">107600</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,355.17,393.33,10.91;8,112.33,368.71,393.65,10.91;8,112.66,382.26,395.01,10.91;8,112.66,395.81,138.14,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,321.08,355.17,184.91,10.91;8,112.33,368.71,196.41,10.91">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m" coord="8,341.70,368.71,164.28,10.91;8,112.66,382.26,264.96,10.91">Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,409.36,395.17,10.91;8,112.66,422.91,395.00,10.91;8,112.66,438.90,97.35,7.90" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m" coord="8,203.56,422.91,273.34,10.91">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,450.01,395.17,10.91;8,112.66,463.56,395.17,10.91;8,112.66,477.11,393.32,10.91;8,112.66,490.66,394.52,10.91;8,112.66,504.21,285.00,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,146.14,463.56,361.68,10.91;8,112.66,477.11,174.42,10.91">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m" coord="8,311.16,477.11,194.82,10.91;8,112.66,490.66,200.08,10.91">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,517.76,393.53,10.91;8,112.66,531.30,395.17,10.91;8,112.66,544.85,394.51,10.91;8,112.07,560.85,20.20,7.90" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,338.46,517.76,167.73,10.91;8,112.66,531.30,115.92,10.91">A Two-Stage Model Based on BERT for Short Fake News Detection</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-29563-9_17</idno>
	</analytic>
	<monogr>
		<title level="m" coord="8,250.69,531.30,207.32,10.91">International Conference on Knowledge Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="172" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,571.95,395.17,10.91;8,112.66,585.50,393.33,10.91;8,112.66,599.05,393.33,10.91;8,112.66,612.60,394.52,10.91;8,112.66,626.15,163.56,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,230.89,571.95,276.95,10.91;8,112.66,585.50,41.53,10.91">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d19-1410</idno>
	</analytic>
	<monogr>
		<title level="m" coord="8,183.50,585.50,322.49,10.91;8,112.66,599.05,393.33,10.91;8,112.66,612.60,319.59,10.91">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,112.66,639.70,393.33,10.91;8,112.66,653.25,395.01,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,217.62,639.70,182.11,10.91">Cross-lingual Language Model Pretraining</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<ptr target="https://proceedings" />
	</analytic>
	<monogr>
		<title level="j" coord="8,421.15,639.70,84.84,10.91;8,112.66,653.25,144.90,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,86.97,393.33,10.91;9,112.66,100.52,253.81,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="9,412.10,86.97,93.89,10.91;9,112.66,100.52,141.16,10.91">Language models are unsupervised multitask learners</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,262.00,100.52,56.95,10.91">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,114.06,393.98,10.91;9,112.41,127.61,224.93,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="9,250.98,114.06,114.99,10.91">Long Short-Term Memory</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,375.74,114.06,93.13,10.91">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,141.16,393.33,10.91;9,112.66,154.71,394.91,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="9,298.40,141.16,207.59,10.91;9,112.66,154.71,23.29,10.91">Learning to forget: Continual prediction with LSTM</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Cummins</surname></persName>
		</author>
		<idno type="DOI">10.1162/089976600300015015</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,145.70,154.71,91.81,10.91">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,168.26,393.33,10.91;9,112.66,181.81,393.33,10.91;9,112.66,195.36,393.32,10.91;9,112.66,208.91,394.61,10.91;9,112.66,222.46,213.95,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,210.80,168.26,295.18,10.91;9,112.66,181.81,38.41,10.91">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v9/glorot10a.html" />
	</analytic>
	<monogr>
		<title level="m" coord="9,317.06,181.81,188.93,10.91;9,112.66,195.36,227.00,10.91;9,402.73,196.37,103.25,9.72;9,112.66,209.92,79.11,9.72">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Titterington</surname></persName>
		</editor>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>PMLR, Chia Laguna Resort, Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct coords="9,112.66,236.01,394.53,10.91;9,112.33,249.56,394.85,10.91;9,112.66,263.11,395.17,10.91;9,112.41,276.66,393.79,10.91;9,112.66,290.20,383.73,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="9,264.49,263.11,243.34,10.91;9,112.41,276.66,171.06,10.91">The CLEF-2022 CheckThat! Lab on Fighting the COVID-19 Infodemic and Fake News Detection</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barr√≥n-Cede√±o</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">D S</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Stru√ü</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>M√≠guez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Kartal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Beltr√°n</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-99739-7_52</idno>
	</analytic>
	<monogr>
		<title level="s" coord="9,306.51,276.66,154.43,10.91">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="416" to="428" />
			<date type="published" when="2022">2022</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,303.75,394.53,10.91;9,112.33,317.30,394.85,10.91;9,112.66,330.85,393.33,10.91;9,112.66,344.40,394.53,10.91;9,112.66,357.95,393.33,10.91;9,112.28,371.50,394.91,10.91;9,112.66,385.05,89.12,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,432.00,330.85,73.99,10.91;9,112.66,344.40,390.04,10.91">Overview of the CLEF-2022 CheckThat! Lab on Fighting the COVID-19 Infodemic and Fake News Detection</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barr√≥n-Cede√±o</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Stru√ü</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>M√≠guez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Kartal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Beltr√°n</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>K√∂hler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,126.92,357.95,379.07,10.91;9,112.28,371.50,390.55,10.91">Proceedings of the 13th International Conference of the CLEF Association: Information Access Evaluation meets Multilinguality, Multimodality, and Visualization, CLEF &apos;2022</title>
		<meeting>the 13th International Conference of the CLEF Association: Information Access Evaluation meets Multilinguality, Multimodality, and Visualization, CLEF &apos;2022<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,398.60,393.33,10.91;9,112.66,412.15,395.17,10.91;9,112.66,425.70,395.01,10.91;9,112.66,439.25,240.48,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="9,228.62,398.60,277.37,10.91;9,112.66,412.15,94.66,10.91">FakeCovid -A Multilingual Cross-domain Fact Check News Dataset for COVID-19</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nandini</surname></persName>
		</author>
		<idno type="DOI">10.36190/2020.14</idno>
		<ptr target="http://workshop-proceedings.icwsm.org/pdf/2020_14.pdf.doi:10.36190/2020.14" />
	</analytic>
	<monogr>
		<title level="m" coord="9,231.18,412.15,276.64,10.91;9,112.66,425.70,169.63,10.91">Workshop Proceedings of the 14th International AAAI Conference on Web and Social Media, ICWSM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,452.79,393.33,10.91;9,112.66,466.34,393.58,10.91;9,112.66,479.89,382.34,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="9,431.30,452.79,74.69,10.91;9,112.66,466.34,263.95,10.91">Overview of the CLEF-2022 CheckThat! Lab Task 3 on Fake News Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>K√∂hler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Stru√ü</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,399.84,466.34,106.40,10.91;9,112.66,479.89,286.23,10.91">Working Notes of CLEF 2022-Conference and Labs of the Evaluation Forum, CLEF &apos;2022</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,493.44,394.53,10.91;9,112.66,506.99,173.79,10.91" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="9,172.47,493.44,329.93,10.91">AMUSED: An Annotation Framework of Multi-modal Social Media Data</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00502</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="9,112.66,520.54,393.32,10.91;9,112.66,534.09,232.83,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="9,266.92,520.54,239.07,10.91;9,112.66,534.09,86.71,10.91">Overview of the CLEF-2021 CheckThat! lab task 3 on fake news detection</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Stru√ü</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,207.67,534.09,105.91,10.91">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,547.64,395.17,10.91;9,112.66,561.19,306.09,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="9,295.92,547.64,211.91,10.91;9,112.66,561.19,65.02,10.91">An exploratory study of COVID-19 misinformation on Twitter</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dirkson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Majchrzak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,185.94,561.19,154.85,10.91">Online Social Networks and Media</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">100104</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,112.66,574.74,394.53,10.91;9,112.66,588.29,394.53,10.91;9,112.66,601.84,393.33,10.91;9,112.66,615.39,393.33,10.91;9,112.66,628.93,393.33,10.91;9,112.66,642.48,324.18,10.91" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="9,303.43,601.84,202.56,10.91;9,112.66,615.39,119.84,10.91">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m" coord="9,254.10,615.39,251.89,10.91;9,112.66,628.93,249.59,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
