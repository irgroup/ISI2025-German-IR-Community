<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,364.98,15.42;1,89.29,106.66,277.79,15.42">NLPIR-UNED at CheckThat! 2022: Ensemble of Classifiers for Fake News Detection</title>
				<funder ref="#_ttwmMj4">
					<orgName type="full">Spanish Ministry of Science and Innovation</orgName>
				</funder>
				<funder ref="#_cZ7VA44">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.10,134.97,107.06,11.96"><forename type="first">Juan</forename><forename type="middle">R</forename><surname>Martinez-Rico</surname></persName>
							<email>jrmartinezrico@invi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">Dpto. Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="laboratory">NLP &amp; IR Group</orgName>
								<orgName type="institution">Universidad Nacional de Educación a Distancia (UNED)</orgName>
								<address>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,208.80,134.97,101.54,11.96"><forename type="first">Juan</forename><surname>Martinez-Romo</surname></persName>
							<email>juaner@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">Dpto. Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="laboratory">NLP &amp; IR Group</orgName>
								<orgName type="institution">Universidad Nacional de Educación a Distancia (UNED)</orgName>
								<address>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Instituto Mixto de</orgName>
								<orgName type="institution">Investigación -Escuela Nacional de Sanidad (IMIENS)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,346.62,134.97,75.93,11.96"><forename type="first">Lourdes</forename><surname>Araujo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dpto. Lenguajes y Sistemas Informáticos</orgName>
								<orgName type="laboratory">NLP &amp; IR Group</orgName>
								<orgName type="institution">Universidad Nacional de Educación a Distancia (UNED)</orgName>
								<address>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Instituto Mixto de</orgName>
								<orgName type="institution">Investigación -Escuela Nacional de Sanidad (IMIENS)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,364.98,15.42;1,89.29,106.66,277.79,15.42">NLPIR-UNED at CheckThat! 2022: Ensemble of Classifiers for Fake News Detection</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">F48C1897BCB019C7922C4DB8E5ECDBDE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fake News Detection</term>
					<term>Transformer Models</term>
					<term>Ensemble of Classifiers</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes the different approaches used by the NLPIR-UNED team 1 in the CLEF2022 Check-That! Lab to tackle the Task 3 -English. The goal of this task is to determine the veracity of the main claim made in a news article. It is a multi-class classification problem with four possible values: true, partially false, false, and other. For this task, we have evaluated three different approaches. The first has been based on a Longformer transformer model that supports larger input sequence sizes than other transformer models such as BERT. The second approach uses transformer models where an extension of the training set has been carried out. The last approach uses an ensemble classifier composed of a transformer model fed with the sequence of words of the article to be evaluated, and a feed forward neural network fed with features related, among other things, to the number of named entities in the article, and features extracted using the LIWC text analysis tool. With this last approach, we have made our main submission reaching the second position among the twenty-five participating teams.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As events unfold: elections, pandemic, war, etc., the proliferation of fake news that manipulates public opinion becomes more evident. This kind of disinformation does not need to be too sophisticated as it is usually targeted at a biased audience that is receptive to these messages.</p><p>Therefore, tools that allow this type of news to be automatically intercepted before they can produce undesirable consequences are increasingly necessary. It can also be of great help in achieving this goal to have forums in which different approaches to this task can be evaluated and shared. One of the events in which this type of activity takes place is the CheckThat! Lab organized at the CLEF conference <ref type="bibr" coords="1,239.24,530.29,11.23,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,253.14,530.29,7.49,10.91" target="#b1">2]</ref>, and specifically in Task 3 <ref type="bibr" coords="1,379.23,530.29,11.27,10.91" target="#b2">[3]</ref>, which is dedicated to the detection of fake news. In this task, the organizers, who have previously worked on the study of fake-news and the development of datasets related to this problem <ref type="bibr" coords="1,401.07,557.39,30.25,10.91">[4, 5? ]</ref>, provide a set of around 1,300 articles, for which it is necessary to determine if the main claim they contain can be classified as false, partially false, true or other. This dataset has been compiled from various fact-checking sites, retrieving the original articles and unifying the labeling.</p><p>We have organized the rest of the article as follows: in Section 2 we make a brief review of the different approaches carried out in recent years to the task of detecting fake news and disinformation, in Section 3 we explain our different approaches to this task, Section 4 discuss the results obtained, and Section 5 contains our conclusions and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We can approach the fake news detection task in many ways but they can generally be grouped into three main strategies: use the information around the article (metadata, author information, publisher information, spatial and temporal dispersion, etc.), use only the information present in the article (textual content, images, writing style, etc.), or try to extract the claims made in the article to verify them in external knowledge bases, understanding that the presence of one or more false claims would also make the article false or at least partially false.</p><p>Perhaps the last of them would be the desirable approach since each prediction given by a classifier based on this strategy can provide a direct explanation for said prediction, but the great drawback is the lack of completeness of the available knowledge bases. This means that most of the claims that can be found in an article cannot be verified.</p><p>Given the format of Task 3, we will focus on reviewing the approaches made recently based solely on the content of the article. Following this strategy Pérez-Rosas et al. <ref type="bibr" coords="2,428.88,362.38,11.33,10.91" target="#b5">[6]</ref>, in addition to systematizing the process of creating a corpus that can be used to train fake news detection systems, they propose the use of features such as n-grams encoded as TF-IDF vectors, features extracted using the Linguistic Inquiry and Word Count (LIWC) <ref type="bibr" coords="2,370.49,403.03,12.78,10.91" target="#b6">[7]</ref> text analysis tool, syntactic features created from grammar production rules that are also encoded as TF-IDF values, and features associated with readability such as the number of complex or long words, number of paragraphs, etc., using a linear SVM as classifier. Other types of features used in the detection of misleading content compare the coherence and structure of the discourse between misleading and true narratives <ref type="bibr" coords="2,176.01,470.77,11.34,10.91" target="#b7">[8]</ref>. Sentiment analysis scoring of news content or social media posts <ref type="bibr" coords="2,483.38,470.77,12.75,10.91" target="#b8">[9]</ref> is another method that can give clues about misleading content and suspicious authors such as bots, that are frequently linked to the proliferation of fake news.</p><p>With the advent of transformer models <ref type="bibr" coords="2,275.73,511.42,16.18,10.91" target="#b9">[10]</ref>, which have become the default choice in many of the tasks related to natural language processing, numerous proposals have emerged to apply these models to the detection of fake news. These models are pre-trained with a large amount of textual information with learning objectives, such as predicting the next sentence or predicting a masked word in the sentence, that do not require manual annotation of these large corpora. Schütz et al. <ref type="bibr" coords="2,147.45,579.17,18.07,10.91" target="#b10">[11]</ref> evaluate different transformer models: BERT <ref type="bibr" coords="2,374.45,579.17,16.41,10.91" target="#b11">[12]</ref>, ALBERT <ref type="bibr" coords="2,440.06,579.17,16.42,10.91" target="#b12">[13]</ref>, RoBERTa <ref type="bibr" coords="2,89.29,592.72,16.41,10.91" target="#b13">[14]</ref>, DistilBERT <ref type="bibr" coords="2,165.89,592.72,18.07,10.91" target="#b14">[15]</ref> and XLNet <ref type="bibr" coords="2,238.48,592.72,18.07,10.91" target="#b15">[16]</ref> on the FakeNewsNet dataset <ref type="bibr" coords="2,391.38,592.72,16.42,10.91" target="#b16">[17]</ref>, using the text of the article, the title and the concatenation of both as input. In their conclusions, they confirm that performing a fine adjustment of these models with a dataset oriented to the detection of fake news allows obtaining promising results, with an accuracy greater than 80% in almost all configurations. Gundapu and Mamidi <ref type="bibr" coords="2,261.07,646.91,18.00,10.91" target="#b17">[18]</ref> after experimenting with different classifier types: Linear Regression (LR), Support Vector Machines (SVM), Multi-Layer Perceptron (MLP), Long Short-Term Memory (LSTM), Convolution Neural Network (CNN), etc., finally found that an ensemble of three transformer models BERT, ALBERT and XLNet, fed with the sequence of words of the news item to be analyzed outperforms all the alternatives studied. Instead, Mehta et al. <ref type="bibr" coords="3,115.66,127.61,18.07,10.91" target="#b18">[19]</ref> uses three BERT models with shared weights whose outputs are concatenated in order to be able to feed the first one with the sequence of text tokens, the second with the metadata associated with this text, and the third model is fed with the justification given for its truth value. This architecture also shows superior behavior than other types of classifiers like LR, SVM, CNN, or Bi-LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approaches to Fake News Detection Task</head><p>To tackle Task 3 in this 2022 edition of the CheckThat! Lab, our team has evaluated three strategies. The first of them is, given that this task is posed with articles with full text (the maximum number of tokens detected is 5816), to use a type of transformer model that allows sequence sizes greater than those admitted for example by BERT, whose maximum sequence size in its standard implementation is 512.</p><p>The second strategy can be considered the opposite of the first: using a small sequence size such as 128 tokens, and assuming that a true or false instance of the training dataset still holds the same truth value if we chop it into 128-token chunks, we have expanded the training dataset and evaluated different transformer models on this extended dataset.</p><p>Our last strategy has tried to take advantage of the transformer models for their ability to extract latent features present in the text, with the use of explicit features such as those provided by the LIWC text analysis tool, which have also given good results in detecting misleading content. For this, we have developed an ensemble classifier that can be fed with both types of inputs simultaneously.</p><p>We detail each of these approaches below. All used pre-trained transformer models have been downloaded from https://huggingface.co/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Using Transformers With Larger Sequence Sizes</head><p>To evaluate this option we have made use of a Longformer <ref type="bibr" coords="3,371.49,493.40,18.07,10.91" target="#b19">[20]</ref> pre-trained model. This architecture is a variation of the standard transformer models with the particularity that the self-attention operation scales linearly with the sequence length, unlike others where this operation scales quadratically and limits its performance with long sequences.</p><p>The implementation of a sequence classifier with this model does not differ from that carried out with any other transformer model. We have used the pre-trained model allenai/longformerbase-4096 that supports sequences of length up to 4096.</p><p>To carry out the training process, we have unified the provided files task3_english_training and task_3a_sample_data and then we have left 33% of the instances as a test dataset. Of the remaining instances, we have reserved 20% as a dev dataset. No preprocessing has been done to the input text.</p><p>This process has been carried out with maximum sequence sizes of 512 and 1024, and a batch size of 8. To determine the configuration with the best performance, the system was configured to use deterministic algorithms and the tests were repeated for 10 different random seeds, obtaining the average of the precision, recall, and F1 measurements.</p><p>An early stopping mechanism has also been implemented. This mechanism stores the updated state of parameters after each epoch and stops training when there have been no improvements in the measure F1 over the dev dataset in the last n epochs (default value 2), then selecting the saved configuration with the best F1 measure during that interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training Dataset Expansion</head><p>Our second option to address this task has been the use of various transformer models but with an extended training dataset. To do this expansion, we have partitioned the data in a similar way as described in the previous section, except to extract the dev dataset from training dataset since expanding the latter has allowed us to experiment with smaller size partitions on the dev dataset: 20% and 10%.</p><p>We have configured a maximum sequence size of 128 tokens and during the training process the instances of the training dataset are dynamically partitioned into 128-word chunks. The same early stopping mechanism and random management configuration described above has also been used.</p><p>Transformer models funnel-transformer/intermediate, bert-base-cased, bert-base-uncased, and albert-base-v2 have been evaluated by repeating the training process for 10 pre-selected random seeds and obtaining the average of the precision, recall and F1 measurements. In this case the batch size has been 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Ensemble Classifier</head><p>In our last approach we wanted to check if using a transformer classifier with its typical input, i.e. a sequence of words, together with a feed forward neural network (FFNN) classifier with discrete features, gave better performance than using only a transformer model.</p><p>For this, we have built an ensemble classifier that is internally composed of a transformer classifier and an FFNN classifier. The input of the ensemble classifier is therefore composed of a section in which the sequence of words of the article is introduced, and a section we feed with a vector of discrete features extracted from that article.</p><p>The hidden layer of the FFNN and the first token of the last hidden layer of the transformer (classification token) are concatenated and form the first layer of the ensemble classifier. Behind this concatenation layer are two hidden layers and one output layer (Figure <ref type="figure" coords="4,418.92,538.70,3.50,10.91" target="#fig_0">1</ref>). It is also possible to disable one of the hidden layers by configuration.</p><p>Before training the ensemble classifier, the transformer and FFNN models are trained separately on the same dataset and stored in binary files. These models are then loaded in evaluation mode in the ensemble classifier to prevent their parameters from being modified during ensemble training.</p><p>On the other hand, we have selected two different sets as discrete features: the 93 features generated by the LIWC text analysis tool, and a series of features elaborated from the syntactic analysis, dependencies, named entities, and sentiment score information which generates the  Stanza NLP tool <ref type="bibr" coords="5,161.01,551.57,17.76,10.91" target="#b20">[21]</ref> from the article text. The complete list of features that make up this second group can be seen in Table <ref type="table" coords="5,210.93,565.12,3.74,10.91" target="#tab_0">1</ref>.</p><p>To select the appropriate configuration of the FFNN classifier, a grid search has been carried out by varying the following hyper-parameters:</p><p>• FFNN activation function: relu, sigmoid, tanh.</p><p>• Hidden layer size: 100, 500, 1000.</p><p>After evaluating the results, the sigmoid activation function has been selected and a hidden layer size of 500 has been configured. These parameters have been set in the ensemble classifier. In all cases, the early stopping mechanism described above has continued to be used. The result obtained is detailed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>This section describes the results obtained with the three strategies described. In the evaluation of results we have used F1-macro measure, which is the measure proposed by the organizers of the task.</p><p>Table <ref type="table" coords="6,127.68,337.51,5.17,10.91" target="#tab_1">2</ref> shows the averaged results of the transformer models after 10 runs with different random seeds. These results have been separated into three groups: the transformer models with a sequence size of 128 tokens without expansion of the training dataset, the Longformer model, and the transformer models with a sequence size of 128 tokens in which the training dataset has been expanded. Using the Longformer, the highest average F1 (0.486) is obtained with a sequence size of 512 tokens and the two configurations of this model clearly outperform all the models in the first group. This average value of F1 measure in the best Longformer configuration is exceeded (0.496) by the funnel transformer model with expansion of the training dataset and leaving a size of 10% in the dev dataset.</p><p>Table <ref type="table" coords="6,128.37,459.46,5.17,10.91" target="#tab_2">3</ref> shows the results of the ensemble classifier also averaged for executions on ten different random seeds. On this occasion, and given the large number of hyper-parameters evaluated in the grid search, we have ordered the results by average F1 measure, showing only the most significant configurations, although the rank column allows us to place each configuration displayed in its absolute position.</p><p>In this way, we can see that the best result (0.477) is obtained when the ensemble is configured with two hidden layers, a dropout of 0, the activation function tanh, the transformer model bert-base-uncased is loaded, and the two sets of discrete features: LIWC and Stanza are used in the FFNN input.</p><p>This average value of F1 is lower than that of the funnel transformer model alone, but higher than that obtained with the loaded model (bert-base-uncased) when used alone. In our tests we have detected that the ensemble with the funnel transformer model behaves quite unstable when it is executed with different random seeds, which means that the average value of the ensemble classifier is lower than expected with this configuration.</p><p>On the other hand, in tables 4 and 5 we can see, respectively, the best values obtained for any random seed in transformer models and in the ensemble classifier. In this case, the Longformer model and the transformer models with extended training dataset obtain a very similar F1 measure: 0.536 for the Longformer with a maximum sequence size of 1024 tokens and a random seed of 42, and 0.535 for the funnel transformer model with a dev partition of 10% and random seed 79. The transformer models without expansion of the training dataset fall behind the Longformer model although they surpass some configurations that use the extended dataset.</p><p>Table <ref type="table" coords="7,127.20,648.21,5.11,10.91" target="#tab_4">5</ref> shows the results of the grid search for the most significant configurations ordered by F1 measure. Here, the extreme behavior of the ensemble classifier is clearly seen when the funnel transformer model is loaded: the best and worst results (the latter are not shown in the table) are for this configuration. In addition, the best value of F1 (0.543) associated with the configuration where the ensemble uses two hidden layers, a dropout of 0, the activation function sigmoid, and the discrete LIWC features, clearly exceeds that obtained by the transformers alone (0.536). Seeing these results on our test dataset and after verifying that the average and maximum values differ mainly due to the behavior of the funnel transformer model when it is loaded in the ensemble classifier, we have chosen to send our submissions guided by the highest absolute value on any random seed.</p><p>In this way, we have made a submission with the best value obtained with the Longformer model, with the best value obtained with the transformers with expanded training dataset, and our final submission has been the one corresponding to the ensemble classifier shown in first position in Table <ref type="table" coords="9,166.70,141.16,3.74,10.91" target="#tab_4">5</ref>.</p><p>With this latest submission we have obtained the second best F1 measure (0.3324) among the 25 participating teams with a difference of 0.0066 compared to the team ranked first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>To tackle the task of detecting fake news in this edition of the CheckThat! Lab, our team has evaluated three strategies that involve the use of transformer models. The first one is the use of an architecture such as the Longformer model, which is specially designed to work with long input sequences without penalizing the performance obtained during the training process. This capability seems adequate to the task, given that we are working with news items that can have thousands of words.</p><p>Comparing its results with those of transformer models with smaller sequence sizes, we find that both in absolute measures of F1 and averaged over different random seeds, the Longformer model has superior performance.</p><p>The second strategy used has been to expand the training dataset in chunks of 128 words, which is the sequence size that we have configured by default in all models. For this, we have assumed that if an article is true, partially false, or false, it will still have the same truth value if we chop it up, since the latent features that the transformer model extracts must be similar. Making this assumption, our results show that in certain configurations this expansion is beneficial, surpassing even the Longformer model in the average F1 measurement.</p><p>Finally, we wanted to test if the combination of discrete features and a multi-layer classifier together with a transformer model with its typical input add an additional ability to classify news items according to their truth value. For this goal, have developed an ensemble classifier that is loaded with two pre-trained models separately in the training dataset: an FFNN that has as inputs the features extracted by the LIWC text analysis tool together with features elaborated from the information returned by the NLP Stanza tool and, on the other hand, a transformer model. With this architecture, we have performed a grid search, verifying that in absolute values using the LIWC features and the sequence of words from the article as inputs obtains the best F1 value among all the evaluated strategies.</p><p>In the future, we plan to continue investigating other ways of integrating different models in an ensemble classifier, as well as including knowledge-based features that, on the one hand, help to identify this type of news, and on the other hand, allow to justify the predictions of the classifier.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,89.29,279.23,163.55,8.93"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Transformer-FFNN ensemble.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,100.20,86.97,405.78,10.91;6,89.29,100.52,114.55,10.91;6,107.28,126.02,399.90,10.91;6,116.56,140.58,63.55,9.72;6,107.28,154.47,228.43,10.91;6,107.28,169.38,153.36,10.91;6,107.28,184.28,95.45,10.91;6,107.28,199.19,247.85,10.91"><head>Finally, a second</head><label></label><figDesc>grid search has been carried out by altering the following hyper-parameters of the ensemble classifier: • Pretrained model: funnel-transformer/intermediate, bert-base-cased, bert-base-uncased, albert-base-v2. • Ensemble activation function: relu, sigmoid, tanh. • Number of hidden layers: 1 or 2. • Dropout: 0, 0.2, 0.5. • Discrete features: LIWC, Stanza, All (LIWC + Stanza).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,312.10,430.44,213.33"><head>Table 1</head><label>1</label><figDesc>Features generated from Stanza tool data</figDesc><table coords="5,95.27,343.72,424.17,181.72"><row><cell>Name</cell><cell>Description</cell></row><row><cell>no_sentences</cell><cell>Number of sentences in the article</cell></row><row><cell>no_subj_sentence</cell><cell>Ratio between number of subjects detected and number of sentences</cell></row><row><cell>no_pred_sentence</cell><cell>Ratio between number of predicates detected and number of sentences</cell></row><row><cell>no_obj_sentence</cell><cell>Ratio between number of objects detected and number of sentences</cell></row><row><cell>no_ne_sentence</cell><cell>Named entities per sentence</cell></row><row><cell>no_ne_person_sentence</cell><cell>Number of person-type named entities per sentence</cell></row><row><cell>no_ne_product_sentence</cell><cell>Number of product-type named entities per sentence</cell></row><row><cell>no_ne_org_sentence</cell><cell>Number of org-type named entities per sentence</cell></row><row><cell>no_ne_norp_sentence</cell><cell>Number of norp-type named entities per sentence</cell></row><row><cell>no_ne_fac_sentence</cell><cell>Number of fac-type named entities per sentence</cell></row><row><cell>no_ne_gpe_sentence</cell><cell>Number of gpe-type named entities per sentence</cell></row><row><cell>no_ne_loc_sentence</cell><cell>Number of loc-type named entities per sentence</cell></row><row><cell cols="2">no_ne_work_of_art_sentence Number of work-of-art-type named entities per sentence</cell></row><row><cell>sentiment</cell><cell>Sentiment score</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,88.98,90.49,327.73,214.13"><head>Table 2</head><label>2</label><figDesc>Average results on our test dataset with transformer models</figDesc><table coords="7,178.56,122.05,238.16,182.56"><row><cell>Configuration</cell><cell cols="2">Precision. Recall</cell><cell>F1</cell></row><row><cell>funnel 128 tokens</cell><cell>0.483</cell><cell>0.453</cell><cell>0.448</cell></row><row><cell>bert cased 128 tokens</cell><cell>0.464</cell><cell>0.423</cell><cell>0.409</cell></row><row><cell>bert uncased 128 tokens</cell><cell>0.464</cell><cell>0.432</cell><cell>0.420</cell></row><row><cell>albert 128 tokens</cell><cell>0.487</cell><cell>0.447</cell><cell>0.448</cell></row><row><cell>longformer max length 512</cell><cell>0.517</cell><cell cols="2">0.488 0.486</cell></row><row><cell>longformer max length 1024</cell><cell>0.533</cell><cell>0.490</cell><cell>0.484</cell></row><row><cell>funnel dev split 20%</cell><cell>0.489</cell><cell>0.481</cell><cell>0.479</cell></row><row><cell>bert cased dev split 20%</cell><cell>0.441</cell><cell>0.441</cell><cell>0.438</cell></row><row><cell>bert uncased dev split 20%</cell><cell>0.447</cell><cell>0.439</cell><cell>0.437</cell></row><row><cell>albert dev split 20%</cell><cell>0.443</cell><cell>0.437</cell><cell>0.435</cell></row><row><cell>funnel dev split 10%</cell><cell>0.503</cell><cell cols="2">0.496 0.496</cell></row><row><cell>bert cased dev split 10%</cell><cell>0.464</cell><cell>0.457</cell><cell>0.455</cell></row><row><cell>bert uncased dev split 10%</cell><cell>0.471</cell><cell>0.465</cell><cell>0.464</cell></row><row><cell>albert dev split 10%</cell><cell>0.473</cell><cell>0.463</cell><cell>0.463</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,88.98,326.87,408.54,213.33"><head>Table 3</head><label>3</label><figDesc>Average results on our test dataset with ensemble classifier sorted by F1</figDesc><table coords="7,97.75,358.44,399.77,181.77"><row><cell>Configuration</cell><cell cols="2">Precision. Recall</cell><cell>F1</cell><cell>Rank</cell></row><row><cell>bert uncased+all, 2 hidden layers, tanh, dropout=0</cell><cell>0.485</cell><cell cols="2">0.479 0.477</cell><cell>1</cell></row><row><cell>bert uncased+Stanza, 2 hidden layers, sigmoid, dropout=0.2</cell><cell>0.477</cell><cell>0.477</cell><cell>0.474</cell><cell>2</cell></row><row><cell>bert uncased+Stanza, 2 hidden layers, sigmoid, dropout=0</cell><cell>0.477</cell><cell>0.478</cell><cell>0.473</cell><cell>3</cell></row><row><cell>albert+LIWC, 1 hidden layer, relu, dropout=0.5</cell><cell>0.480</cell><cell>0.475</cell><cell>0.473</cell><cell>4</cell></row><row><cell>albert+LIWC, 1 hidden layer, sigmoid, dropout=0.5</cell><cell>0.480</cell><cell>0.475</cell><cell>0.473</cell><cell>5</cell></row><row><cell>albert+all, 2 hidden layers, tanh, dropout=0.2</cell><cell>0.483</cell><cell>0.472</cell><cell>0.472</cell><cell>10</cell></row><row><cell>albert+Stanza, 1 hidden layer, relu, dropout=0</cell><cell>0.478</cell><cell>0.473</cell><cell>0.472</cell><cell>12</cell></row><row><cell>funnel+all, 2 hidden layers, relu, dropout=0.5</cell><cell>0.466</cell><cell>0.467</cell><cell>0.461</cell><cell>90</cell></row><row><cell>funnel+LIWC, 2 hidden layers, sigmoid, dropout=0.5</cell><cell>0.457</cell><cell>0.469</cell><cell>0.458</cell><cell>98</cell></row><row><cell>bert cased+all, 2 hidden layers, relu, dropout=0.2</cell><cell>0.462</cell><cell>0.459</cell><cell>0.457</cell><cell>99</cell></row><row><cell>bert cased+Stanza, 2 hidden layers, relu, dropout=0.2</cell><cell>0.465</cell><cell>0.458</cell><cell>0.456</cell><cell>101</cell></row><row><cell>bert uncased+LIWC, 1 hidden layer, sigmoid, dropout=0</cell><cell>0.448</cell><cell>0.452</cell><cell>0.441</cell><cell>158</cell></row><row><cell>funnel+Stanza, 2 hidden layers, relu, dropout=0.2</cell><cell>0.437</cell><cell>0.457</cell><cell>0.438</cell><cell>164</cell></row><row><cell>bert cased+LIWC, 1 hidden layer, relu, dropout=0</cell><cell>0.435</cell><cell>0.453</cell><cell>0.435</cell><cell>175</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,88.99,90.49,342.35,211.91"><head>Table 4</head><label>4</label><figDesc>Best results on our test dataset with transformer models</figDesc><table coords="8,163.93,119.83,267.41,182.56"><row><cell>Configuration</cell><cell cols="2">Precision Recall</cell><cell>F1</cell><cell>Seed</cell></row><row><cell>funnel 128 tokens</cell><cell>0.512</cell><cell>0.490</cell><cell>0.488</cell><cell>33</cell></row><row><cell>bert cased 128 tokens</cell><cell>0.578</cell><cell>0.471</cell><cell>0.470</cell><cell>85</cell></row><row><cell>bert uncased 128 tokens</cell><cell>0.523</cell><cell>0.499</cell><cell>0.496</cell><cell>0</cell></row><row><cell>albert 128 tokens</cell><cell>0.520</cell><cell>0.469</cell><cell>0.480</cell><cell>54</cell></row><row><cell>longformer max length 512</cell><cell>0.530</cell><cell>0.529</cell><cell>0.516</cell><cell>54</cell></row><row><cell>longformer max length 1024</cell><cell>0.564</cell><cell cols="2">0.524 0.536</cell><cell>42</cell></row><row><cell>funnel dev split 20%</cell><cell>0.520</cell><cell>0.527</cell><cell>0.521</cell><cell>42</cell></row><row><cell>bert cased dev split 20%</cell><cell>0.462</cell><cell>0.459</cell><cell>0.457</cell><cell>0</cell></row><row><cell>bert uncased dev split 20%</cell><cell>0.464</cell><cell>0.461</cell><cell>0.462</cell><cell>42</cell></row><row><cell>albert dev split 20%</cell><cell>0.474</cell><cell>0.465</cell><cell>0.465</cell><cell>85</cell></row><row><cell>funnel dev split 10%</cell><cell>0.532</cell><cell cols="2">0.549 0.535</cell><cell>79</cell></row><row><cell>bert cased dev split 10%</cell><cell>0.481</cell><cell>0.467</cell><cell>0.470</cell><cell>63</cell></row><row><cell>bert uncased dev split 10%</cell><cell>0.498</cell><cell>0.487</cell><cell>0.491</cell><cell>0</cell></row><row><cell>albert dev split 10%</cell><cell>0.502</cell><cell>0.498</cell><cell>0.496</cell><cell>12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,88.99,324.65,421.23,213.33"><head>Table 5</head><label>5</label><figDesc>Best results on our test dataset with ensemble classifier sorted by F1</figDesc><table coords="8,95.27,356.22,414.95,181.77"><row><cell>Configuration</cell><cell cols="2">Precision Recall</cell><cell>F1</cell><cell cols="2">Seed Rank</cell></row><row><cell>funnel+LIWC, 2 hidden layers, sigmoid, dropout=0</cell><cell>0.544</cell><cell cols="2">0.559 0.546</cell><cell>79</cell><cell>1</cell></row><row><cell>funnel+LIWC, 2 hidden layers, sigmoid, dropout=0.5</cell><cell>0.542</cell><cell>0.560</cell><cell>0.544</cell><cell>79</cell><cell>2</cell></row><row><cell>funnel+Stanza, 2 hidden layers, sigmoid, dropout=0</cell><cell>0.542</cell><cell>0.557</cell><cell>0.543</cell><cell>79</cell><cell>3</cell></row><row><cell>funnel+all, 2 hidden layers, sigmoid, dropout=0</cell><cell>0.540</cell><cell>0.558</cell><cell>0.541</cell><cell>79</cell><cell>4</cell></row><row><cell>funnel+all, 2 hidden layers, sigmoid, dropout=0.2</cell><cell>0.536</cell><cell>0.553</cell><cell>0.541</cell><cell>79</cell><cell>5</cell></row><row><cell>bert uncased+all, 2 hidden layers, tanh, dropout=0</cell><cell>0.529</cell><cell>0.526</cell><cell>0.518</cell><cell>79</cell><cell>116</cell></row><row><cell>albert+LIWC, 2 hidden layers, sigmoid. dropout=0.2</cell><cell>0.506</cell><cell>0.516</cell><cell>0.507</cell><cell>12</cell><cell>165</cell></row><row><cell>albert+Stanza, 2 hidden layers, relu, dropout=0.2</cell><cell>0.515</cell><cell>0.509</cell><cell>0.506</cell><cell>33</cell><cell>166</cell></row><row><cell>albert+all, 1 hidden layer, relu, dropout=0</cell><cell>0.524</cell><cell>0.512</cell><cell>0.504</cell><cell>12</cell><cell>183</cell></row><row><cell>bert uncased+Stanza, 2 hidden layers, relu, dropout=0</cell><cell>0.514</cell><cell>0.508</cell><cell>0.498</cell><cell>79</cell><cell>238</cell></row><row><cell>bert cased+all, 2 hidden layers, relu, dropout=0.2</cell><cell>0.499</cell><cell>0.494</cell><cell>0.496</cell><cell>0</cell><cell>249</cell></row><row><cell>bert cased+Stanza, 2 hidden layers, sigmoid, dropout=0</cell><cell>0.498</cell><cell>0.497</cell><cell>0.493</cell><cell>0</cell><cell>281</cell></row><row><cell>bert uncased+LIWC, 1 hidden layer, relu, dropout=0</cell><cell>0.503</cell><cell>0.496</cell><cell>0.492</cell><cell>0</cell><cell>304</cell></row><row><cell>bert cased+LIWC, 2 hidden layers, sigmoid, dropout=0.5</cell><cell>0.579</cell><cell>0.493</cell><cell>0.490</cell><cell>85</cell><cell>355</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has been partially supported by the <rs type="funder">Spanish Ministry of Science and Innovation</rs> within the projects <rs type="projectName">DOTT-HEALTH</rs> (<rs type="grantNumber">PID2019-106942RB-C32</rs>) and <rs type="projectName">RAICES</rs> (<rs type="grantNumber">IMIENS 2022</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_ttwmMj4">
					<idno type="grant-number">PID2019-106942RB-C32</idno>
					<orgName type="project" subtype="full">DOTT-HEALTH</orgName>
				</org>
				<org type="funded-project" xml:id="_cZ7VA44">
					<idno type="grant-number">IMIENS 2022</idno>
					<orgName type="project" subtype="full">RAICES</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,112.66,111.28,394.53,10.91;10,112.33,124.83,394.85,10.91;10,112.66,138.38,393.33,10.91;10,112.66,151.93,394.52,10.91;10,112.66,165.48,393.53,10.91;10,112.66,179.03,225.55,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,283.05,138.38,222.93,10.91;10,112.66,151.93,205.76,10.91">The CLEF-2022 CheckThat! Lab on Fighting the COVID-19 Infodemic and Fake News Detection</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Struß</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Míguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Kartal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Beltrán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,313.54,165.48,148.39,10.91">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Hagen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Verberne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Seifert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Balog</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Nørvåg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Setty</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="416" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,192.57,394.53,10.91;10,112.33,206.12,394.85,10.91;10,112.66,219.67,393.33,10.91;10,112.66,233.22,394.53,10.91;10,112.66,246.77,393.33,10.91;10,112.28,260.32,394.91,10.91;10,112.66,273.87,89.12,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,432.00,219.67,73.99,10.91;10,112.66,233.22,390.04,10.91">Overview of the CLEF-2022 CheckThat! Lab on Fighting the COVID-19 Infodemic and Fake News Detection</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Struß</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Míguez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Babulkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Kartal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Beltrán</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Köhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,126.92,246.77,379.07,10.91;10,112.28,260.32,390.57,10.91">Proceedings of the 13th International Conference of the CLEF Association: Information Access Evaluation meets Multilinguality, Multimodality, and Visualization, CLEF &apos;2022</title>
		<meeting>the 13th International Conference of the CLEF Association: Information Access Evaluation meets Multilinguality, Multimodality, and Visualization, CLEF &apos;2022<address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,287.42,393.73,10.91;10,112.66,300.97,393.33,10.91;10,112.66,314.52,394.53,10.91;10,112.66,328.07,22.69,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,462.91,287.42,43.48,10.91;10,112.66,300.97,291.21,10.91">Overview of the CLEF-2022 CheckThat! Lab Task 3 on Fake News Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Köhler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Struß</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schütz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,426.99,300.97,78.99,10.91;10,112.66,314.52,321.74,10.91">Working Notes of CLEF 2022-Conference and Labs of the Evaluation Forum, CLEF &apos;2022</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,341.62,395.17,10.91;10,112.66,355.17,394.62,10.91;10,112.66,368.71,37.08,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,311.78,341.62,196.05,10.91;10,112.66,355.17,94.89,10.91">An exploratory study of COVID-19 misinformation on Twitter</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dirkson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Majchrzak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,219.00,355.17,157.57,10.91">Online social networks and media</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">100104</biblScope>
			<date type="published" when="2021">2021</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,382.26,393.33,10.91;10,112.66,395.81,242.73,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">K</forename><surname>Shahi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nandini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11343</idno>
		<title level="m" coord="10,220.97,382.26,285.02,10.91;10,112.66,395.81,59.52,10.91">FakeCovid-A multilingual cross-domain fact check news dataset for COVID-19</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,112.66,409.36,394.52,10.91;10,112.66,422.91,394.52,10.91;10,112.66,436.46,65.30,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,350.28,409.36,151.49,10.91">Automatic Detection of Fake News</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pérez-Rosas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lefevre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,127.24,422.91,350.43,10.91">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3391" to="3401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,450.01,393.32,10.91;10,112.66,463.56,214.69,10.91" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="10,349.13,450.01,156.85,10.91;10,112.66,463.56,103.23,10.91">The development and psychometric properties of LIWC</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Blackburn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="10,112.66,477.11,393.33,10.91;10,112.66,490.66,394.62,10.91;10,112.14,504.21,97.01,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,235.85,477.11,229.76,10.91">Truth and deception at the rhetorical structure level</title>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">L</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lukoianova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,473.38,477.11,32.61,10.91;10,112.66,490.66,263.08,10.91">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="905" to="917" />
			<date type="published" when="2015">2015</date>
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,517.76,394.62,10.91;10,112.28,531.30,393.70,10.91;10,112.66,544.85,394.53,10.91;10,112.66,558.40,397.48,10.91;10,112.66,574.39,73.62,7.90" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,314.91,517.76,192.37,10.91;10,112.28,531.30,183.95,10.91">Using sentiment to detect bots on Twitter: Are humans more opinionated than bots?</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Subrahmanian</surname></persName>
		</author>
		<idno type="DOI">10.1109/ASONAM.2014.6921650</idno>
		<ptr target="http://ieeexplore.ieee.org/document/6921650/.doi:10.1109/ASONAM.2014.6921650" />
	</analytic>
	<monogr>
		<title level="m" coord="10,342.21,531.30,163.77,10.91;10,112.66,544.85,308.11,10.91">IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2014)</title>
		<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="620" to="627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,585.50,395.17,10.91;10,112.66,599.05,394.53,10.91;10,112.66,612.60,90.72,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,148.82,599.05,106.21,10.91">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">\</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,278.07,599.05,224.48,10.91">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,626.15,395.17,10.91;10,112.66,639.70,394.53,10.91;10,112.66,653.25,80.57,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,322.40,626.15,185.44,10.91;10,112.66,639.70,116.53,10.91">Automatic fake news detection with pretrained transformer models</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Schütz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nazemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,251.36,639.70,209.65,10.91">International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="627" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,666.80,393.33,10.91;11,112.33,86.97,393.65,10.91;11,112.66,100.52,393.32,10.91;11,112.66,114.06,357.71,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,321.08,666.80,184.91,10.91;11,112.33,86.97,192.59,10.91">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,330.38,86.97,175.60,10.91;11,112.66,100.52,393.32,10.91;11,112.66,114.06,102.30,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct coords="11,112.66,127.61,393.53,10.91;11,112.66,141.16,394.61,10.91;11,112.66,154.71,223.37,10.91" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="11,392.89,127.61,113.30,10.91;11,112.66,141.16,236.99,10.91">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942[cs</idno>
		<idno>arXiv: 1909.11942</idno>
		<ptr target="http://arxiv.org/abs/1909.11942" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,168.26,395.17,10.91;11,112.66,181.81,393.33,10.91;11,112.66,195.36,299.20,10.91" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692[cs</idno>
		<idno>arXiv: 1907.11692</idno>
		<ptr target="http://arxiv.org/abs/1907.11692" />
		<title level="m" coord="11,141.36,181.81,277.91,10.91">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,208.91,394.52,10.91;11,112.66,222.46,395.01,10.91;11,112.66,236.01,111.33,10.91" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108[cs</idno>
		<idno>arXiv: 1910.01108</idno>
		<ptr target="http://arxiv.org/abs/1910.01108" />
		<title level="m" coord="11,295.22,208.91,211.96,10.91;11,112.66,222.46,116.00,10.91">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,249.56,393.33,10.91;11,112.28,263.11,393.95,10.91;11,112.33,276.66,29.19,10.91" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="11,415.32,249.56,90.66,10.91;11,112.28,263.11,244.97,10.91">XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,112.66,290.20,393.32,10.91;11,112.66,303.75,393.32,10.91;11,112.66,317.30,376.46,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,341.22,290.20,164.77,10.91;11,112.66,303.75,393.32,10.91;11,112.66,317.30,52.57,10.91">Fakenewsnet: A data repository with news content, social context, and spatiotemporal information for studying fake news on social media</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mahudeswaran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,174.07,317.30,36.49,10.91">Big data</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="171" to="188" />
			<date type="published" when="2020-06-01">2020. June 1, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,330.85,395.17,10.91;11,112.66,344.40,394.62,10.91;11,112.66,357.95,50.45,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gundapu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mamidi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00180[cs</idno>
		<idno>arXiv: 2101.00180</idno>
		<ptr target="http://arxiv.org/abs/2101.00180" />
		<title level="m" coord="11,230.74,330.85,277.09,10.91;11,112.66,344.40,51.54,10.91">Transformer based Automatic COVID-19 Fake News Detection System</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,371.50,393.54,10.91;11,112.66,385.05,394.62,10.91;11,112.31,398.60,392.68,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="11,340.37,371.50,165.82,10.91;11,112.66,385.05,105.65,10.91">A transformer-based architecture for fake news classification</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Anand</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13278-021-00738-y</idno>
		<ptr target="https://link.springer.com/10.1007/s13278-021-00738-y.doi:10.1007/s13278-021-00738-y" />
	</analytic>
	<monogr>
		<title level="j" coord="11,226.93,385.05,167.31,10.91">Social Network Analysis and Mining</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,412.15,394.53,10.91;11,112.66,425.70,377.18,10.91" xml:id="b19">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150[cs</idno>
		<idno>arXiv: 2004.05150</idno>
		<ptr target="http://arxiv.org/abs/2004.05150" />
		<title level="m" coord="11,284.61,412.15,217.67,10.91">Longformer: The Long-Document Transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,439.25,393.32,10.91;11,112.66,452.79,395.17,10.91;11,112.66,466.34,393.33,10.91;11,112.66,479.89,394.03,10.91;11,112.66,493.44,326.09,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="11,344.94,439.25,161.05,10.91;11,112.66,452.79,205.87,10.91">Stanza: A Python Natural Language Processing Toolkit for Many Human Languages</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-demos.14</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-demos.14.doi:10.18653/v1/2020.acl-demos.14" />
	</analytic>
	<monogr>
		<title level="m" coord="11,341.66,452.79,166.17,10.91;11,112.66,466.34,393.33,10.91;11,112.66,479.89,132.32,10.91">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="101" to="108" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
