<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,405.89,15.42;1,89.29,106.66,240.38,15.42">HULAT-UC3M at SimpleText@CLEF-2022: Scientific text simplification using BART</title>
				<funder>
					<orgName type="full">Madrid Government (Comunidad de Madrid-Spain)</orgName>
				</funder>
				<funder ref="#_jePdFhC">
					<orgName type="full">of the Ministry of Science and Innovation -Government of Spain</orgName>
				</funder>
				<funder ref="#_8BmTf4Q">
					<orgName type="full">Multiannual Agreement</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,134.97,65.36,11.96"><forename type="first">Adrián</forename><surname>Rubio</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering Department</orgName>
								<orgName type="institution" key="instit1">Universidad Carlos III de Madrid</orgName>
								<orgName type="institution" key="instit2">Av de la Universidad</orgName>
								<address>
									<addrLine>30</addrLine>
									<postCode>28911</postCode>
									<settlement>Leganés</settlement>
									<region>Madrid</region>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,166.88,134.97,82.86,11.96"><forename type="first">Paloma</forename><surname>Martínez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering Department</orgName>
								<orgName type="institution" key="instit1">Universidad Carlos III de Madrid</orgName>
								<orgName type="institution" key="instit2">Av de la Universidad</orgName>
								<address>
									<addrLine>30</addrLine>
									<postCode>28911</postCode>
									<settlement>Leganés</settlement>
									<region>Madrid</region>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,405.89,15.42;1,89.29,106.66,240.38,15.42">HULAT-UC3M at SimpleText@CLEF-2022: Scientific text simplification using BART</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">AEC3AFD3CA9CEAF869126C4616A74EE4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Scientific text simplification</term>
					<term>Summarization</term>
					<term>Deep Learning</term>
					<term>BART</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the proposed system developed by HULAT-UC3M research group from Universidad Carlos III de Madrid to solve Task 3 of SimpleText@CLEF-2022 on scientific text simplification. We present an abstractive approach implemented with BART. BART is a sequence-to-sequence model trained as a denoising autoencoder. In order to fulfill this specific task proposed, it has been fine-tuned to simplify text passages provided by the task organizers. The proposed system obtained a loss value of 1.232 , a SARI value of 47.83, and a rouge L value of 0.615 on the validation set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Science is becoming harder to understand to non-scientists. With the ever increasing use of jargon, long sentences and scientific terms research papers are hermetic to the average person. Moreover it is also paramount to make texts more accessible to people with reading disabilities. Simplifying text is a process that involves reducing the complexity of the text to make it more accessible, while at the same time maintaining the information it holds. Moreover, because the high volume of scientific literature there is also a great interest on developing tools capable of summarizing such papers, so that they allow to process more data in less time.</p><p>Summarizing text is an essential process in text simplification together with lexical and sentence simplification. Text summarization techniques were implemented to perform the proposed text simplification task. Currently there are two main approaches for text summarization, which are the extractive approach and abstractive approach <ref type="bibr" coords="1,327.74,504.11,11.36,10.91" target="#b0">[1,</ref><ref type="bibr" coords="1,341.83,504.11,7.57,10.91" target="#b1">2]</ref>.</p><p>The extractive method involves tokenizing the text into sentences and ranking them using a variety of different methods. The sentences regarded as more relevant or that encapsulate more meaning of the text are ranked higher than those that don't. These sentences are later bundled up together to form the summarization. No new sentences are generated, therefore the main idea behind this approach is to reduce the text to the sentences that carry the most meaning.</p><p>The abstractive approach is more complex and more computationally intensive than extractive approach, however it is the only approach that can provide cohesion and it is more similar to human-like text summarization. Human-like text summarization involves reading the whole text, grow ones understanding of the text, and then rewrite the text in such a way that it is more understandable than the original text. Computers do not have such knowledge or language capability, therefore this makes abstractive text summarization a very difficult and non-trivial task. However this approach yields better results as it manages to understand the meaning of the text, thus adding more cohesion and context to the simplifications produced <ref type="bibr" coords="2,441.94,141.16,11.30,10.91" target="#b2">[3]</ref>. This is the approach followed in the HULAT-UC3M system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Dataset</head><p>The fine-tunning process of the model was carried out using the dataset provided by the organization. The dataset provided is divided into two sets <ref type="bibr" coords="2,352.01,226.89,11.40,10.91" target="#b3">[4]</ref>. Firstly there is the training set, which consists of scientific passages with their corresponding simplified passage. The source passage and the simplified passage are provided in two different CSV files. The first file contains the source passage, identified with an ID, a document ID which provides information on the origin of the passage, and a query text which can be interpreted as the topic of the passage. The query text is identified with an ID as well. The second file contains the simplified version of the passage, identified with its ID as well. In total there are 647 entries in the training set. The other part consists of the test set, which has the same attributes as the first file of the training dataset, but includes 116742 entries. The training set was used to fine-tune the model to the task, whereas the test set was used to generate the simplified passages which were evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Resources</head><p>To develop this system, a Python 3.8 notebook was utilized in Google Colaboratory, as it provides a sufficient amount of GPU to build the system. A pre-trained BART model from HuggingFace <ref type="bibr" coords="2,89.29,434.55,13.00,10.91" target="#b4">[5]</ref> was used. The model has been pre-trained using the CNN/Daily Mail dataset. Several libraries were necessary for the adequate development of the system. Pandas, in its current version 1.4.2, is a versatile library used to process data. PyTorch 1.11, Transformers 4.19.2 and Blurr 1.0.0 were necessary to use in order to being able to fine-tune the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methods and system description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data pre-processing</head><p>As the training data was provided in two different files, it was necessary to combine them into one file which then could be used for training the model. Furthermore, a key aspect of the data is the query text; it is paramount to include this information in the training set, as it was thought that it would improve the results. However this had to be included somehow in the source passage, as for training it was required that the entries be composed of three attributes: source identification, source passage and simplified passage.</p><p>In terms of how to include the query into the source passage, there are several options:</p><p>• Option 0: Not including the query.</p><p>• Option 1: Add the query at the end of the passage: source passage + query text.</p><p>• Option 2: Include the query separated from the source passage by " related to": source passage + "related to" + query text. • Option 3: Include the query separated from the source passage by ",related to ": source passage + ", related to" + query text.</p><p>Data was generated using all the options described to later experiment and observe which yields the best results. From the first data file, shown as File 1 in Figure <ref type="figure" coords="3,419.45,182.93,3.81,10.91" target="#fig_0">1</ref>, we extracted the values of sentence id, source sentence and query text into a dataframe. Then depending on the option it was necessary to add text to the data (for example options 2 and 3), this was done with a simple concatenating operation. Up next we perform the aggregate operation to join source sentence and query text into source sentence. The dataframe is redefined to being just the sentence id and source sentence. From the other data file, shown as File 2 in Figure <ref type="figure" coords="3,482.56,250.67,3.79,10.91" target="#fig_0">1</ref>, we extracted sentence id and simplified sentence into another dataframe. Both dataframes are joined using the sentence id as key into a single dataframe. This dataframe is later converted to a easy to work CSV file, which consists of three attributes. The sentence id, which identifies the sentence, the source sentence itself and the simplified sentence. This format was used as it is handy to work with the data in this way, using the pre-trained model which will be explained up next. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Deep Learning Model</head><p>This model implements a pre-trained BART architecture from Huggingface. As explained in the original paper <ref type="bibr" coords="3,186.74,654.97,11.58,10.91" target="#b4">[5]</ref>, BART is a sequence-to-sequence model trained as a denoising autoencoder. The main component of BART are transformers, as BART stands for Bidirectional AutoRegressive Transformers. A transformer is a sequence-to-sequence component based in a encoder-decoder architecture. This means that BART that can be fine-tuned for Conditional Text Generation, which in essence takes a text sequence and produces a text sequence as an output <ref type="bibr" coords="4,122.75,127.61,11.58,10.91" target="#b5">[6]</ref>. BART was chosen as it is one of the current state-of-the-art techniques in NLP. It is a model which is particularly effective when fine-tunned for text generation, as it has demonstrated to excel the results of other models in text summarization tasks <ref type="bibr" coords="4,452.07,154.71,13.00,10.91" target="#b4">[5]</ref> and text simplification task <ref type="bibr" coords="4,173.92,168.26,11.43,10.91" target="#b6">[7]</ref>.</p><p>On the supervised training phase the training data set was used to fine-tune the model. The inputs to the model are the source sentence and the simplified sentence, they are in a dataframe format. Computers don't understand sentences, therefore the first step is to tokenize each sentence, and obtain the embeddings of the inputs to the transformers.</p><p>With word embeddings, the aim is to map every word in the sentence to a point in space where similar words in meaning are physically closer to each other. The space in which they are present is called an embedding space. In this case the pre-trained model used incorporates a pre-trained embedding space <ref type="bibr" coords="4,222.23,276.66,11.49,10.91" target="#b4">[5]</ref>. This embedding space maps each word in the sentence to a vector. Furthermore it is important to represent the location of the word within the sentence, as the location of a word in the sentence may result in different meanings, this is where positional encoders are used. It is a vector that has information on the distances between words in the sentence <ref type="bibr" coords="4,129.52,330.85,11.23,10.91" target="#b7">[8,</ref><ref type="bibr" coords="4,142.94,330.85,7.49,10.91" target="#b8">9]</ref>. After passing a word through word embedding and applying positional encoding we obtain the word vectors that have positional information, i.e. context. The architecture for this specific model is detailed in Figure <ref type="figure" coords="4,282.97,357.95,3.78,10.91" target="#fig_1">2</ref>. With a transformer encoder there is no need to pass each word individually through the input embedding, all words of the sentence are passed simultaneously and determine the word embeddings simultaneously. This process is applied to the source and simplified sentence, thus obtaining the embedded text (source sentence) and target (simplified sentence), which are passed to the next transformer layer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimenting and results</head><p>Firstly several experiments were carried out to see which of the data processing options yields better results. Table <ref type="table" coords="5,182.13,124.83,5.17,10.91" target="#tab_0">1</ref> shows that adding the query information into the training data yields better results. The difference between the losses of Option 0 and Option 1 is substantial. Having acknowledged this, we can observe that introducing this information in a way that would be equivalent to natural language (Option 3), generates better the best results among the described options. Once the text is transformed to input sequences, it can then be used to fine-tune the system for the task. As described in section 2, the training data consists of the sentences and their respective simplifications. The training dataset is further split into training (80%) and validation (20%). The objective of fine-tunning with this data is to create a model capable of generating the most similar simplifications to the simplifications provided. To achieve this purpose several experiments were carried out modifying the value of the hyperparameters in order to obtain the best metrics.</p><p>The Blurr library provides a built-in method which slowly ramps-up the value for the learning rate in a log-linear way <ref type="bibr" coords="5,198.63,419.20,16.41,10.91" target="#b9">[10]</ref>. The loss is recorded for every iteration. In Figure <ref type="figure" coords="5,449.35,419.20,3.81,10.91" target="#fig_2">3</ref>, the values are plotted in order to find the best value for the learning rate. This process is very handy if the model is being developed in a Python Notebook. One cell can be dedicated to find the best learning rate, and the following cell performs the fine-tuning after having obtained the value of the previous cell. Because of this resource there was no need to experiment with the value of the learning rate ourselves. The experimenting was performed modifying the values of other hyperparameters, namely batch size, number of epochs and the length of the output.  As table <ref type="table" coords="6,139.55,180.91,5.16,10.91" target="#tab_1">2</ref> shows, when it comes to the adequacy of the hyperparameters, it was observed that the optimal number of epochs is 2, as with more epochs the model started overfitting to the training data, the loss value on validation incremented at the same time as the loss value on training decreased.</p><p>Regarding the batch size, smaller sizes are beneficial for two main reasons. Firstly smaller batch sizes are noisy, offering a regularizing effect and lower generalization error; secondly it makes it easier to fit one batch worth of training data in memory. This is paramount since resources on Google Colab are limited, namely GPU RAM size.</p><p>As for the minimum and maximum length of the output simplification, this was more of a eyeballing task. In several experiments it was observed that the output did not have sufficient length to adequately simplify larger passages, thus the decision was made to set the maximum length to 50 words.</p><p>The metrics obtained with the optimal hyperparameters shown in table <ref type="table" coords="6,420.95,343.50,3.82,10.91" target="#tab_2">3</ref>: After the model had been fine-tunned, the test data file was used to generate the simplifications of the passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>Developing this kind of models is no easy task. Throughout the development process there has been some setbacks and dead ends. Nevertheless the developed model fulfills the task competently. The generated simplified passages have obtained a desired evaluation in the validation set and an direct inspection of several of the simplified passages shows that the passages generated have grammatical correctness and adequately simplify the original sentences. Moreover the hypothesis that adding the query text as a topic marker would improve the results was validated.</p><p>However there were some options which could not be implemented or explored due to time constraints. Namely it was devised to use another external set of data to further the scope and variety of the training data. Some sources that could be used are Simply Wikipedia, Turk corpus or Asset corpus. Further work may be carried out with regard to this.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,89.29,575.59,186.99,8.93;3,193.47,354.68,208.35,208.35"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of data pre-processing</figDesc><graphic coords="3,193.47,354.68,208.35,208.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,89.29,654.63,198.76,8.93;4,89.29,433.71,416.70,208.35"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Embedding architecture of the system</figDesc><graphic coords="4,89.29,433.71,416.70,208.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,89.29,660.78,185.32,8.93;5,214.30,523.21,166.68,125.01"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Loss value for a given learning rate</figDesc><graphic coords="5,214.30,523.21,166.68,125.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,208.03,372.08,86.37"><head>Table 1</head><label>1</label><figDesc>Metrics obtained using the different options described (two epochs using training dataset).</figDesc><table coords="5,160.07,236.12,275.14,58.28"><row><cell>Option</cell><cell cols="5">Train loss Valid loss Rouge 1 Rouge 2 Rouge L</cell></row><row><cell>Option 0</cell><cell>2.91</cell><cell>2.15</cell><cell>0.655</cell><cell>0.51</cell><cell>0.625</cell></row><row><cell>Option 1</cell><cell>1.24</cell><cell>1.417</cell><cell>0.652</cell><cell>0.5</cell><cell>0.619</cell></row><row><cell>Option 2</cell><cell>1.179</cell><cell>1.346</cell><cell>0.652</cell><cell>0.5</cell><cell>0.62</cell></row><row><cell>Option 3</cell><cell>1.137</cell><cell>1.231</cell><cell>0.659</cell><cell>0.494</cell><cell>0.615</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.97,90.49,351.36,74.02"><head>Table 2</head><label>2</label><figDesc>Values tested and optimal value for each hyperparameter.</figDesc><table coords="6,154.94,118.58,285.40,45.93"><row><cell>Hyperparameter</cell><cell>Range</cell><cell>Optimal</cell></row><row><cell>Batch size</cell><cell>[1,2,3,4,5]</cell><cell>2</cell></row><row><cell>Epochs</cell><cell>[1,2,3,4,5]</cell><cell>2</cell></row><row><cell cols="2">Max length and Min length [(10,30)(15,30)(10,40)(10,50)]</cell><cell>(10,50)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,88.99,372.50,383.16,49.31"><head>Table 3</head><label>3</label><figDesc>Obtained metrics with optimal hyperparameters.</figDesc><table coords="6,123.12,400.59,349.03,21.22"><row><cell cols="7">Validation loss Rouge 1 Rouge 2 Rouge L SARI Precision Recall</cell><cell>F1</cell></row><row><cell>1.231</cell><cell>0.659</cell><cell>0.494</cell><cell>0.615</cell><cell>47.83</cell><cell>0.938</cell><cell>0.94</cell><cell>0.938</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has been supported by the <rs type="funder">Madrid Government (Comunidad de Madrid-Spain)</rs> under the <rs type="funder">Multiannual Agreement</rs> with UC3M in the line <rs type="grantName">of Excellence</rs> <rs type="projectName">of University Professors</rs> (<rs type="grantNumber">EPUC3M17</rs>) and by the <rs type="programName">Research Program</rs> <rs type="funder">of the Ministry of Science and Innovation -Government of Spain</rs> (<rs type="projectName">ACCESS2MEET</rs> project<rs type="grantNumber">-PID2020-116527RB-I00</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_8BmTf4Q">
					<idno type="grant-number">EPUC3M17</idno>
					<orgName type="grant-name">of Excellence</orgName>
					<orgName type="project" subtype="full">of University Professors</orgName>
					<orgName type="program" subtype="full">Research Program</orgName>
				</org>
				<org type="funded-project" xml:id="_jePdFhC">
					<idno type="grant-number">-PID2020-116527RB-I00</idno>
					<orgName type="project" subtype="full">ACCESS2MEET</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Online Resources</head><p>The source code is accessible via • https://github.com/Adrubio12/text-simplification</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,112.66,210.55,393.33,10.91;7,112.66,224.10,364.00,10.91" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Allahyari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Pouriyeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Assefi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Safaei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Trippe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">B</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Kochut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02268</idno>
		<title level="m" coord="7,486.97,210.55,19.02,10.91;7,112.66,224.10,181.74,10.91">Text summarization techniques: a brief survey</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,237.65,65.97,10.91;7,210.38,237.65,296.81,10.91;7,112.66,251.20,370.80,10.91;7,112.66,264.75,89.58,10.91" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chiusiano</surname></persName>
		</author>
		<ptr target="https://medium.com/nlplanet/two-minutes-nlp-quick-intro-to-text-simplification-f5f9d7be4" />
		<title level="m" coord="7,210.38,237.65,292.50,10.91">Two minutes nlp -quick intro to text simplification</title>
		<imprint>
			<date type="published" when="2021">a3c (2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,278.30,395.00,10.91" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Mago</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08612</idno>
		<title level="m" coord="7,194.39,278.30,134.04,10.91">A survey on text simplification</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,291.85,394.52,10.91;7,112.66,305.40,394.62,10.91;7,112.66,318.95,394.53,10.91;7,112.66,332.50,393.33,10.91;7,112.66,346.05,164.71,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,311.78,305.40,195.50,10.91;7,112.66,318.95,115.12,10.91">Automatic simplification of scientific texts: Simpletext lab at clef-2022</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ermakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nurbakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ovchinnikova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mathurin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Araújo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hannachi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Poinsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,240.17,332.50,156.37,10.91">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Hagen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Verberne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Seifert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Balog</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Norvaag</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Setty</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="364" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,359.59,395.17,10.91;7,112.66,373.14,394.53,10.91;7,112.66,386.69,321.31,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<title level="m" coord="7,145.01,373.14,362.18,10.91;7,112.66,386.69,138.52,10.91">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,400.24,394.53,10.91;7,112.66,413.79,393.33,10.91;7,112.66,427.34,303.98,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dangati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Uppaal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Windsor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Dotterrer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00751</idno>
		<title level="m" coord="7,217.41,413.79,288.57,10.91;7,112.66,427.34,121.39,10.91">Long document summarization in a low resource setting using pretrained language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,440.89,393.33,10.91;7,112.66,454.44,387.22,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">É</forename><surname>De La Clergerie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sagot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00352</idno>
		<title level="m" coord="7,361.89,440.89,144.10,10.91;7,112.66,454.44,204.83,10.91">Muss: multilingual unsupervised sentence simplification by mining paraphrases</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="7,112.66,467.99,395.17,10.91;7,112.66,481.54,393.33,10.91;7,112.33,495.09,29.19,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,484.04,467.99,23.79,10.91;7,112.66,481.54,143.41,10.91">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,264.71,481.54,228.49,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Polosukhin</note>
</biblStruct>

<biblStruct coords="7,112.66,508.64,394.03,10.91;7,112.33,522.18,29.19,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Alammar</surname></persName>
		</author>
		<ptr target="https://jalammar.github.io/illustrated-transformer/" />
		<title level="m" coord="7,165.34,508.64,114.17,10.91">The illustrated transformer</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,112.66,535.73,393.32,10.91;7,112.66,549.28,385.46,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09820</idno>
		<title level="m" coord="7,168.08,535.73,337.90,10.91;7,112.66,549.28,203.16,10.91">A disciplined approach to neural network hyper-parameters: Part 1-learning rate, batch size, momentum, and weight decay</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
