<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,72.00,77.76,450.63,14.99;1,72.00,98.51,105.74,14.99">Using a Pre-Trained SimpleT5 Model for Text Simplification in a Limited Corpus</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,72.00,136.88,68.33,10.54"><forename type="first">José</forename><surname>Monteiro</surname></persName>
							<email>jdiogoxmonteiro@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Minho</orgName>
								<address>
									<settlement>Braga</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,151.92,136.88,74.97,10.54"><forename type="first">Micaela</forename><surname>Aguiar</surname></persName>
							<email>maguiar60@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Minho</orgName>
								<address>
									<settlement>Braga</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,255.81,136.88,64.32,10.54"><forename type="first">Sílvia</forename><surname>Araújo</surname></persName>
							<email>saraujo@elach.uminho.pt</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Minho</orgName>
								<address>
									<settlement>Braga</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,72.00,77.76,450.63,14.99;1,72.00,98.51,105.74,14.99">Using a Pre-Trained SimpleT5 Model for Text Simplification in a Limited Corpus</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">63E1DBF981699CF63EC2F319702F1664</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Text Simplification</term>
					<term>SimpleT5</term>
					<term>Sentence Transformers 1</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe our approach for solving Task 3 of the SimpleText Lab, organized as part of the Clef 2022 conference. The SimpleText Lab addresses issues of automatic text simplification of scientific texts in order to make scientific knowledge more accessible to everyone. To address Task 3, we trained Simple T5. In the first experiment, Simple T5 was trained with the small training dataset (648 entries) provided by the SimpleText team. Although there was a high number of unchanged sentences (49%), the sentences that were simplified and evaluated by SimpleText gathered the best overall result among the other participants. Nevertheless, we decided to run a new experiment training T5 with different datasets, namely, the original SimpleText training data set (trained with new parameters), WikiLarge, and a combination of WikiLarge and the SimpleText training dataset (WikiLast). We used EASSE metrics to compare the three models. Firstly, we tested the model with the TurkCorpus for reference and afterwards we tested them with the SimpleText Corpus. We wanted to know if a small but highly specialized dataset of the same discourse genre (abstracts) of the sentences that will be simplified combined with a larger general dataset would produce better results. WikiLast yielded the best SARI and BLEU results, however the BLEU results correlate with a higher percentage of exact matches. It seems that creating a small but highly specialized dataset may not make up for the investment, since the difference between the scores of the three models is not considerable.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper describes an approach for building and designing a methodology for solving Task 3 of the SimpleText lab <ref type="bibr" coords="1,164.37,497.93,11.68,9.66" target="#b0">[1]</ref>, organized as part of the Clef 2022 conference. The SimpleText 2022 Lab addressed text simplification approaches and proposed three taks: TASK 1 What is in (or out)? Select passages to include in a simplified summary, given a query; TASK 2 What is unclear? Given a passage and a query, rank terms/concepts that are required to be explained for understanding this passage (definitions, context, applications…) and TASK 3 Rewrite this! Given a query, simplify passages (sentences) from scientific abstracts. In this paper, we will propose a solution to address Task 3, using a pre-trained SimpleT5 model in the limited corpus of scientific abstracts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Text Simplification</head><p>The first article on text simplification dates back to 1975 and was written in the field of social sciences <ref type="bibr" coords="1,112.87,628.86,11.68,9.66" target="#b1">[2]</ref>. Indeed, the fields of foreign and second language teaching have been interested in text simplification, as teachers need texts adapted to the level of their students and often end up having to create their own simplified materials. In the last two decades, (automated) text simplification has become a field of natural language processing alongside matters of text summarization, machine translation and paraphrasing <ref type="bibr" coords="2,199.29,87.64,11.68,9.66" target="#b1">[2]</ref>.</p><p>Text simplification relates to the process of modifying a text <ref type="bibr" coords="2,354.86,100.28,12.82,9.66" target="#b2">[3]</ref> -whether its syntax or lexicon or both -, in order to produce a simpler version of the text <ref type="bibr" coords="2,332.23,112.93,11.68,9.66" target="#b3">[4]</ref>, that retains its original meaning <ref type="bibr" coords="2,493.24,112.93,11.68,9.66" target="#b4">[5]</ref>, all the while improving its readability and understandability. Readability refers to the level of ease with which someone may read a text (it usually pertains to aspects such as grammar complexity or sentence length) <ref type="bibr" coords="2,106.63,150.88,11.68,9.66" target="#b2">[3]</ref>. Understandability (or comprehensibility) concerns the "amount of information a user may gain from a piece of text" <ref type="bibr" coords="2,194.86,163.53,12.82,9.66" target="#b2">[3]</ref> and it is influenced by factors such as the background knowledge the reader may have about a certain subject.</p><p>The applications of automated text simplification are many. Text simplification may be widely useful for second language learners <ref type="bibr" coords="2,237.90,201.48,11.68,9.66" target="#b5">[6]</ref>, but also for assisted technology design to help people with Aphasia or lay readers faced with technical or specialized documents <ref type="bibr" coords="2,424.93,214.12,11.68,9.66" target="#b2">[3]</ref>. Automated text simplification can tackle, as well, the problem of scientific literacy <ref type="bibr" coords="2,383.07,226.77,11.68,9.66" target="#b0">[1]</ref>. The SimpleText Lab arises from the need to address the ever growing amount of scientific information people have to manage nowadays. Indeed, it is estimated that scientific knowledge doubles every five years <ref type="bibr" coords="2,461.79,252.07,11.68,9.66" target="#b6">[7]</ref>. Scientific texts are usually not easy to read and understand -they are complex and full of specialized terminology -since they are a product of a specialized discourse community mostly uninterested in science communication or popularization. Nowadays, it is more important than ever to make scientific texts more accessible to everyone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Training SimpleT5 for Text Simplification</head><p>Text-to-Text transfer transformers (T5) are gaining popularity due to their competitive performance, effective scaling to larger model sizes, and ease of use in solving tasks as simple text-to-text mapping problems <ref type="bibr" coords="2,212.96,370.35,11.68,9.66" target="#b7">[8]</ref>. T5 is a model pre-trained in multiple NLP tasks, that proposes a unified approach, that is, the authors propose "to treat every text processing problem as a "text-to-text" problem, i.e. taking text as input and producing new text as output" <ref type="bibr" coords="2,435.20,395.65,11.68,9.66" target="#b7">[8]</ref>. T5 is trained on Colossal Clean Crawled Corpus (C4), a dataset created by applying a set of filters to English-language text sourced from the public Common Crawl web scrape. The T5 model is trained on several datasets for different tasks, such as Text Summarization, Question Answering Translation and Sentiment analysis <ref type="bibr" coords="2,113.16,446.25,11.68,9.66" target="#b8">[9]</ref>. To address Task 3, we face a series of constraints: lack of computing power, little development time and a fairly low amount of data available for pre-training the model. T5 is not trained for text simplification. However, after testing various models (mainly from the hugging face library <ref type="bibr" coords="2,105.55,484.19,16.85,9.66" target="#b9">[10]</ref>, such as distilbart <ref type="bibr" coords="2,211.99,484.19,16.21,9.66" target="#b10">[11]</ref>), simpleT5 (base) seemed to give the best results with the least amount of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Initial Solution</head><p>In the first experiment, we trained SimpleT5 (base) with the data provided by the SimpleText Lab. The pre-training data consisted of a parallel corpus of 648 simplified sentences from Medicine and Computed Science scientific abstracts <ref type="bibr" coords="2,241.71,564.53,11.68,9.66" target="#b0">[1]</ref>.</p><p>The sentences were simplified by experts and/or specialized translators. The test data set consisted of 116,763 sentences retrieved from the DBLP Citation Network Dataset.</p><p>The methodology we used was adapted from the process streamlined in this article <ref type="bibr" coords="2,467.99,602.47,18.32,9.66" target="#b11">[12]</ref> and the SimpleT5 documentation: 1. Pre-process the available training data and turn it into the acceptable by SimpleT5 format. The "simpletext_task3_train" was turned into our "source" and "simpletext_task3_decorated_run" into our "target"; 2. Split the full processed set between a training set and a testing set; 3. Train the model, using the simple methods given by SimpleT5; 4. Run every query in the "simpletext_task3_test" dataset through our model; 5. Evaluate the results; 6. If the results don't meet our standards: Go back to step 3. with updated arguments for the model; Else: Go to next step; 7. Build a script that pushes every result into the file t format required by the SimpleText Lab. This was certified using the available script; 8. Submit the Results.</p><p>The final parameters for the model were the following: source_max_token_len = 128, target_max_token_len = 50, batch_size = 8, max_epochs = 5. The actual training dataset was composed of 80% of the original training set (518 entries) and the other entries (130) were needed for validation.</p><p>The general results showed that from the total 116,763 sentences our model didn't change 42,189 sentences (49%) and 852 (0,72%) sentences were truncated. 3,217 sentences became longer (2,7%) and the syntax complexity (2.94) and the lexical complexity (3.06) scores were average when comparing to the total of runs submitted: the highest score for syntax complexity was 4.69 and the lowest 2.10 and the highest score for lexical complexity was 3.69 and the lowest 2.42. Regarding informational loss, our run had the lowest score 1.50, the highest being 3.84. 564 sentences were evaluated for information distortion: there were 9 instances of non-sense (1,5%), 3 instances of contresens (0,5%), 4 instances of topic shifts (0,70%), 3 instances of wrong synonyms, 19 instances of ambiguity (3,3%), 94 instances of omission of essential details (16,6%), 9 instances of overgeneralization (1,5%), 13 instances of oversimplification (2,3%), 2 instances of unsupported information (0,3%) and 2 of unnecessary details (0,3%), 5 instances of redundancy (0,8%) and 1 instances of style (0,1%). In the final ranking for Task 3, we placed first with a score of 0.149 (the second and third places score respectively 0.122 and 0.119).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training SimpleT5 with Different Datasets</head><p>Despite our classification, we decided to run a second experiment training T5 with different datasets. In order to evaluate/test the new experiments, we used the Python package EASSE <ref type="bibr" coords="3,485.20,281.81,18.32,9.66" target="#b12">[13]</ref> that offers a single-point access to popular automatic metrics for sentence simplification, such as BLEU, SARI and FKGL. BLEU is used to assess grammaticality and meaning preservation; SARI examines simplicity gain (word added, deleted and kept) and FKGL indicates sentence length and number of syllables. Additionally, EASSE makes available three datasets for automatic sentence simplification evaluation: PWKP, TurkCorpus and HSplit. EASSE also offers quality estimation metrics (the compression ratio, Levenshtein similarity, average number of sentence splits, proportion of exact matches, proportion of added words, deleted words, and lexical complexity score) <ref type="bibr" coords="3,435.24,370.35,16.85,9.66" target="#b12">[13]</ref>.</p><p>We decided to train T5 with three different datasets. The first model (New SimpleText) uses, as the name suggests, the original SimpleText dataset. This time we opted to follow the most popular state of the art models both in terms of parameters and validation sets. That being said, for training we used the original set with all the entries (648), and for validation TurkCorpus was used (2000 entries). The validation set available from TurkCorpus and the final parameters were the same for all three models: source_max_token_len = 256, target_max_token_len = 128, batch_size = 8, max_epochs = 5, precision = 32. The second model (WikiLarge) used the WikiLarge dataset <ref type="bibr" coords="3,427.65,458.90,16.85,9.66" target="#b13">[14]</ref>. WikiLarge is a popular dataset used for text simplification: it contains 296,402 sentence pairs from English Wikipedia and Simple Wikipedia. The third and final model (WikiLast) was trained with WikiLarge + SimpleText Training Dataset. We wanted to know if a small but highly specialized dataset of the same discourse genre (abstracts) of the sentences that will be simplified combined with a larger general dataset would gather different and better results. If our tests reveal that the merger of a large generic set and a small specialized one for model training have a significant increase in the model accuracy, we could make the case that focusing resources on building small specialized sets could be a good investment.</p><p>Having the three final models trained and ready, we tested them with TurkCorpus, in order to have benchmark values. Table <ref type="table" coords="3,195.41,585.38,5.50,9.66" target="#tab_0">1</ref> shows the EASSE metrics results of the three models. The New SimpleText Model scores the highest value of BLEU (94.897), however as Sulem, Abend &amp; Rappoport point out, BLEU "gives high scores to sentences that are close or even identical to the input" <ref type="bibr" coords="3,102.97,623.33,16.85,9.66" target="#b14">[15]</ref>. Indeed, the New SimpleText Model has a high value of exact copies (47%). Between the WikiLarge and the WikiLast Models the differences do not appear to be significant. It is noteworthy that the WikiLast gathered the best SARI result.  <ref type="table" coords="4,114.75,74.99,5.50,9.66" target="#tab_1">2</ref> shows an example taken from the TurkCorpus and simplified by the models. The major transformation in this sentence was the substitution of the word "victorious" with a simpler word: New SimpleText replaced it with "elected" and WikiLarge and WikiLast replaced it with "won". Arguably, "elected" is less complex and more common than "victorious", but more complex and less common than "won". This example illustrates well the lexical complexity scores from Table <ref type="table" coords="4,479.86,125.58,4.12,9.66" target="#tab_0">1</ref>. Next, the models were tested with a selection of the original SimpleText dataset (10,000 entries). Table <ref type="table" coords="4,100.50,262.74,5.50,9.66" target="#tab_2">3</ref> shows the EASSE metrics results of the three models tested with the SimpleText dataset. In comparison with the other two models, WikiLast produces the best SARI results. Wikilast also presented the best BLEU results, even though they correlate with an increase in exact matches (54%). In this run, the NewSimpleText Model and WikiLarge are very similar in most metrics.  <ref type="table" coords="4,170.12,425.20,5.50,9.66" target="#tab_5">5</ref> show examples taken from the three models. In the first example (Table <ref type="table" coords="4,499.57,425.20,3.97,9.66" target="#tab_3">4</ref>), in the New SimpleText Model the sentence remains unchanged. We can also see that both WikiLarge and WikiLast unabbreviate the verb form, but only WikiLast replaces the verb form "developed" with a simple and more common one ("made"). In Table <ref type="table" coords="4,126.66,641.38,4.12,9.66" target="#tab_5">5</ref>, the sentence remains unchanged in the New Simple Text and WikiLarge models, while WikiLast simplifies the sentence, by replacing "browse" with "use". </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion and Conclusion</head><p>Unsurprisingly, when the models were tested with the TurkCorpus, the New Simple Text model yielded the worst SARI results, since we are using a model trained on specialized data to simplify more generic data. WikiLast produced the best SARI result, which we think could be simply attributed to its larger data set. In the TurkCorpus case, just as we expected, the difference in results between WikiLarge and WikiLast are too small and unnoticeable to justify the need for the specialized set.</p><p>When the models were tested with the SimpleText Corpus, the New Simple Text model yielded the worst SARI results of both runs. WikiLast had the best SARI and BLEU results, however, as mentioned before, the high BLEU results correlate with a higher percentage of exact copies. These results have the limitation of stemming from automatic metric. As Shardlow observes, automatic metrics are largely ineffective <ref type="bibr" coords="5,208.88,218.57,11.68,9.66" target="#b2">[3]</ref>, and they are not a substitute for human judgment. Nevertheless, if we take into account the results, it seems that creating a small but highly specialized dataset may not make up for the investment, since the difference between the scores of the three models is not considerable. Evidently, it is possible that if the specialized dataset was larger the results would be better and that larger specialized datasets (of scientific texts, judicial texts or academic texts, for example) could produce better results than generic datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,72.00,674.67,440.55,83.03"><head>Table 1 EASSE</head><label>1</label><figDesc></figDesc><table coords="3,80.33,688.10,432.22,69.60"><row><cell cols="3">metrics: Models Tested with the TurkCorpus</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Metrics</cell><cell>BLEU</cell><cell>SARI</cell><cell>FKGL</cell><cell>Exact Cop.</cell><cell>Lexic. Comp.</cell></row><row><cell>New SimpleT</cell><cell>94.897</cell><cell>35.127</cell><cell>9.586</cell><cell>0.479</cell><cell>8.268</cell></row><row><cell>WikiLarge</cell><cell>86.706</cell><cell>37.652</cell><cell>9.17</cell><cell>0.295</cell><cell>8.18</cell></row><row><cell>WikiLast</cell><cell>88.063</cell><cell>38.165</cell><cell>9.056</cell><cell>0.312</cell><cell>8.186</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,72.00,151.62,415.71,80.59"><head>Table 2</head><label>2</label><figDesc>Example of Simplified Sentence of Models Tested with TurkCorpus Original In 1998, Culver ran for Iowa Secretary of State and was victorious. New SimpleT Culver ran for Iowa Secretary of State in 1998 and was elected. WikiLarge In 1998, Culver ran for Iowa Secretary of State and won. WikiLast In 1998, Culver ran for Iowa Secretary of State and won.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,72.00,326.73,440.55,80.59"><head>Table 3 EASSE</head><label>3</label><figDesc></figDesc><table coords="4,80.33,340.16,432.22,67.16"><row><cell cols="4">metrics: Models Tested with the SimpleText Corpus</cell><cell></cell><cell></cell></row><row><cell>Metrics</cell><cell>BLEU</cell><cell>SARI</cell><cell>FKGL</cell><cell>Exact Cop.</cell><cell>Lexic. Comp.</cell></row><row><cell>New SimpleT</cell><cell>81.084</cell><cell>29.847</cell><cell>13.469</cell><cell>0.374</cell><cell>8.794</cell></row><row><cell>WikiLarge</cell><cell>83.915</cell><cell>30.395</cell><cell>14.067</cell><cell>0.478</cell><cell>8.778</cell></row><row><cell>WikiLast</cell><cell>88.511</cell><cell>31.269</cell><cell>13.949</cell><cell>0.542</cell><cell>8.787</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,86.25,425.20,80.37,9.66"><head>Table 4 and</head><label>4</label><figDesc>Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,72.00,489.19,428.60,134.30"><head>Table 4</head><label>4</label><figDesc>Example 1 of Simplified Sentence of Models Tested with SimpleText Corpus</figDesc><table coords="4,78.83,518.30,421.77,105.19"><row><cell>Original</cell><cell>It's developed for Windows Mobile® for use in mobile devices such as PDA's</cell></row><row><cell></cell><cell>and Smartphone's.</cell></row><row><cell>New SimpleT</cell><cell>It's developed for Windows Mobile® for use in mobile devices such as PDA's</cell></row><row><cell></cell><cell>and Smartphone's.</cell></row><row><cell>WikiLarge</cell><cell>It was developed for Windows Mobile® for use in mobile devices such as PDAs</cell></row><row><cell></cell><cell>and Smartphones.</cell></row><row><cell>WikiLast</cell><cell>It was made for Windows Mobile® for use in mobile devices such as PDAs and</cell></row><row><cell></cell><cell>Smartphones.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="4,72.00,680.06,397.72,82.23"><head>Table 5</head><label>5</label><figDesc>Example 2 of Simplified Sentence of Models Tested with SimpleText Corpus OriginalThe BioWAP service can be browsed with any WAP terminal.New SimpleTThe BioWAP service can be browsed with any WAP terminal.</figDesc><table /><note coords="4,93.53,738.35,43.95,9.70;4,199.78,738.35,268.70,9.70;4,96.72,752.60,37.57,9.70;4,208.36,752.60,251.53,9.70"><p><p><p><p>WikiLarge</p>The BioWAP service can be browsed with any WAP terminal.</p>WikiLast</p>The BioWAP service can be used with any WAP terminal.</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,108.00,336.85,398.41,9.66;5,108.00,349.50,404.03,9.66;5,108.00,362.14,407.92,9.66;5,108.00,374.79,371.88,9.66;5,108.00,387.44,388.02,9.66;5,108.00,400.09,401.96,9.66;5,108.00,412.74,286.68,9.66" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,261.55,349.50,250.48,9.66;5,108.00,362.14,144.22,9.66">Overview of the CLEF 2022 SimpleText Lab: Automatic Simplification of Scientific Texts</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Liana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nurbakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ovchinnikova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mathurin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Araújo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Hannachi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Huet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,273.72,362.14,242.20,9.66;5,108.00,374.79,371.88,9.66;5,108.00,387.44,111.59,9.66">Proceedings of the Thirteenth International Conference of the CLEF Association (CLEF 2022)</title>
		<editor>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mirko</forename><surname>Martino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fabrizio</forename><surname>Degli Esposti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Craig</forename><surname>Sebastiani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gabriella</forename><surname>Macdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Allan</forename><surname>Pasi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Martin</forename><surname>Hanbury</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Guglielmo</forename><surname>Potthast</surname></persName>
		</editor>
		<editor>
			<persName><surname>Faggioli</surname></persName>
		</editor>
		<meeting>the Thirteenth International Conference of the CLEF Association (CLEF 2022)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="5,108.00,425.39,407.38,9.66;5,108.00,438.04,396.22,9.66;5,108.00,450.69,193.88,9.66" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,208.29,425.39,307.08,9.66;5,108.00,438.04,72.76,9.66">A Bibliometric Analysis of Articles on Text Simplification: Sample of Scopus Database</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">Z</forename><surname>Özcan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Batur</surname></persName>
		</author>
		<idno type="DOI">10.7575/aiac.ijels.v.9n.2p.24</idno>
		<ptr target="https://doi.org/10.7575/aiac.ijels.v.9n.2p.24" />
	</analytic>
	<monogr>
		<title level="j" coord="5,188.28,438.04,243.59,9.66">International Journal of Education and Literacy Studies</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,108.00,463.34,408.62,9.66;5,108.00,475.98,216.44,9.66;5,108.00,488.63,227.27,9.66" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,169.58,463.34,190.05,9.66">A Survey of Automated Text Simplification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shardlow</surname></persName>
		</author>
		<idno type="DOI">10.14569/SpecialIssue.2014.040109</idno>
		<ptr target="https://doi.org/10.14569/SpecialIssue.2014.040109" />
	</analytic>
	<monogr>
		<title level="j" coord="5,366.75,463.34,149.87,9.66;5,108.00,475.98,157.83,9.66">International Journal of Advanced Computer Science and Applications</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,108.00,501.28,368.49,9.66;5,108.00,513.93,358.64,9.66;5,108.00,526.58,141.06,9.66" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="5,108.00,513.93,291.14,9.66">Reference-Less Quality Estimation of Text Simplification Systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Humeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P.-E</forename><surname>Mazaré</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">É</forename><forename type="middle">V</forename><surname>De La Clergerie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sagot</surname></persName>
		</author>
		<idno>arXiv</idno>
		<ptr target="http://arxiv.org/abs/1901.10746" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,108.00,539.23,391.29,9.66;5,108.00,551.88,403.87,9.66;5,108.00,564.53,398.68,9.66;5,108.00,577.18,289.56,9.66" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,459.61,539.23,39.68,9.66;5,108.00,551.88,277.74,9.66">Sentence Alignment Methods for Improving Text Simplification Systems</title>
		<author>
			<persName coords=""><forename type="first">Š</forename><surname>Sanja</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franco-Salvador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">P</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Stuckenschmidt</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2016</idno>
		<ptr target="https://doi.org/10.18653/v1/P17-2016" />
	</analytic>
	<monogr>
		<title level="m" coord="5,407.77,551.88,104.10,9.66;5,108.00,564.53,287.56,9.66">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,108.00,589.83,406.64,9.66;5,108.00,602.47,384.59,9.66;5,108.00,615.12,366.18,9.66" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,291.03,589.83,223.60,9.66;5,108.00,602.47,36.62,9.66">Text Readability Assessment for Second Language Learners</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Menglin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kochmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Briscoe</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-0502</idno>
		<ptr target="https://doi.org/10.18653/v1/W16-0502" />
	</analytic>
	<monogr>
		<title level="m" coord="5,166.29,602.47,326.29,9.66;5,108.00,615.12,109.03,9.66">Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the 11th Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,108.00,627.77,369.85,9.66;5,108.00,640.42,87.64,9.66" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cribb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sari</surname></persName>
		</author>
		<title level="m" coord="5,182.71,627.77,249.81,9.66">Open Science: Sharing Knowledge in the Global Century</title>
		<meeting><address><addrLine>Victoria, Collingwood</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,108.00,653.07,396.24,9.66;5,108.00,665.72,388.72,9.66;5,108.00,678.37,233.46,9.66" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>arXiv</idno>
		<ptr target="http://arxiv.org/abs/1910.10683" />
		<title level="m" coord="5,128.76,665.72,363.24,9.66">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,108.00,691.02,400.71,9.66;5,108.00,703.67,223.18,9.66" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Digaari</surname></persName>
		</author>
		<ptr target="https://bit.ly/3NAp24W" />
		<title level="m" coord="5,159.89,691.02,348.82,9.66;5,108.00,703.67,55.81,9.66">Solved! Google&apos;s Text-To-Text Transfer Transformer (T5) Bottleneck, Towards Data Science</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,126.00,716.31,196.30,9.66" xml:id="b9">
	<monogr>
		<ptr target="https://huggingface.co/" />
		<title level="m" coord="5,126.00,716.31,60.02,9.66">Hugging Face</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,107.91,728.96,380.02,9.66" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<ptr target="https://huggingface.co/sshleifer/distilbart-cnn-6-6" />
		<title level="m" coord="5,159.46,728.96,75.48,9.66">distilbart-cnn-6-6</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,126.00,741.61,323.37,9.66;5,108.00,754.26,100.47,9.66" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="5,160.72,741.61,228.20,9.66">SimpleT5 -Train T5 Models in Just 3 Lines of Code</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<ptr target="https://bit.ly/38MS7eZ" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,126.00,74.99,391.40,9.66;6,108.00,87.64,405.41,9.66;6,108.00,100.28,412.98,9.66;6,108.00,112.93,383.48,9.66;6,108.00,125.58,390.63,9.66" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="6,359.57,74.99,157.83,9.66;6,108.00,87.64,111.43,9.66">EASSE: Easier Automatic Sentence Simplification Evaluation</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Alva-Manchego</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-3009</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-3009" />
	</analytic>
	<monogr>
		<title level="m" coord="6,241.11,87.64,272.30,9.66;6,108.00,100.28,412.98,9.66;6,108.00,112.93,383.48,9.66;6,108.00,125.58,47.30,9.66">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations,Association for Computational Linguistics</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations,Association for Computational Linguistics<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,126.00,138.23,376.68,9.66;6,108.00,150.88,203.64,9.66" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="6,235.90,138.23,262.07,9.66">Sentence Simplification with Deep Reinforcement Learning</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Xingxing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<idno>arXiv</idno>
		<ptr target="http://arxiv.org/abs/1703.10931" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,126.00,163.53,344.48,9.66;6,108.00,176.18,269.28,9.66" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Elior</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Abend</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rappoport</surname></persName>
		</author>
		<idno>arXiv</idno>
		<ptr target="http://arxiv.org/abs/1810.05022" />
		<title level="m" coord="6,295.62,163.53,174.86,9.66;6,108.00,176.18,61.27,9.66">Semantic Structural Evaluation for Text Simplification</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
