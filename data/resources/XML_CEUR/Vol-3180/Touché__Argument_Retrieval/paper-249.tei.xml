<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,416.58,15.42;1,89.29,106.66,217.62,15.42;1,89.29,132.57,324.97,5.42">SEUPD@CLEF: Team INTSEG on Argument Retrieval for Controversial Questions Notebook for the Touché Lab on Argument Retrieval at CLEF 2022</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,158.47,76.49,5.42"><forename type="first">Sepide</forename><surname>Bahrami</surname></persName>
							<email>sepide.bahrami@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,178.42,158.47,97.72,5.42"><forename type="first">Gnana</forename><forename type="middle">Prakash</forename><surname>Goli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,288.79,158.47,65.30,5.42"><forename type="first">Andrea</forename><surname>Pasin</surname></persName>
							<email>andrea.pasin.1@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,366.73,158.47,91.54,5.42"><forename type="first">Neemol</forename><surname>Rajkumari</surname></persName>
							<email>neemol.rajkumari@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,172.42,146.64,5.42"><forename type="first">Mohammad</forename><forename type="middle">Muzammil</forename><surname>Sohail</surname></persName>
							<email>mohammadmuzammil.sohail@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,248.58,172.42,58.46,5.42"><forename type="first">Paria</forename><surname>Tahan</surname></persName>
							<email>paria.tahan@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,338.03,172.42,60.31,5.42"><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
							<email>ferro@dei.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,416.58,15.42;1,89.29,106.66,217.62,15.42;1,89.29,132.57,324.97,5.42">SEUPD@CLEF: Team INTSEG on Argument Retrieval for Controversial Questions Notebook for the Touché Lab on Argument Retrieval at CLEF 2022</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">5E4AFFA5758D704FBDC1E51B6729D15A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information Retrieval</term>
					<term>Information Retrieval System</term>
					<term>Search Engine</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Search Engines play important roles in helping users to rapidly retrieve relevant information. The technology underlying Search Engines has been improved in the last years, both in terms of hardware capabilities and in terms of software. However, they are still affected by many issues due to the continuously growing amount of data and the various forms in which it comes. In this paper we discuss our solution to the Information Retrieval problem proposed by the CLEF 2022 Touché Task 1. We first describe in general the considered problem and subsequently present our Information Retrieval System implemented through Apache Lucene illustrating the various phases and methods applied to fulfil the objectives of the task. Eventually, we provide the obtained experimental results and possible explanations for them. In particular, we investigate the reasons for which some methods performed worse than others and describe possible ways to improve the system in the future.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Nowadays Search Engines are used by the vast majority of people in their daily routine to retrieve any kind of information in a practical and efficient way. Even though there exist several advanced implementations of them, there are still many big challenges such as the argument retrieval for controversial <ref type="bibr" coords="1,206.07,479.28,12.84,4.94" target="#b0">[1]</ref> or comparative questions <ref type="bibr" coords="1,336.95,479.28,11.43,4.94" target="#b1">[2]</ref>.</p><p>This report aims at providing an information retrieval system that has been developed to solve the task "Argument Retrieval for Controversial Questions" provided by the third Touché lab <ref type="bibr" coords="1,104.92,519.93,12.69,4.94" target="#b2">[3]</ref> on argument retrieval at CLEF 2022. The ultimate goal of this system is to retrieve a pair of coherent arguments through a huge dataset of online debate portals in response to a query about a controversial topic.</p><p>The paper is organized as follows: Section 2 describes the previous studies that have been done in this area; Section 3 describes our approach; Section 4 explains our experimental setup; Section 5 discusses our main findings; Section 6 investigates the relevance judgements, finally, Section 7 draws some conclusions; Section 8 outlooks for future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Researchers have conducted a number of studies in the field of information retrieval <ref type="bibr" coords="2,475.83,189.50,13.00,4.94" target="#b3">[4]</ref>  <ref type="bibr" coords="2,491.74,189.50,11.58,4.94" target="#b4">[5]</ref>, each one concerning a different application in use. To the extent that it relates to our work as an argument retrieval task for decision making purposes <ref type="bibr" coords="2,348.41,216.60,11.57,4.94" target="#b2">[3]</ref>, much of the knowledge base is drawn from the work done in the previous year, which was summarized in the overview of Touché lab in 2021 <ref type="bibr" coords="2,174.81,243.70,11.43,4.94" target="#b5">[6]</ref>.</p><p>One critical thing to notice in an information retrieval task is the amount of time needed to perform the task. A number of studies have investigated the effectiveness of document organization in helping a user locate the relevant material among the retrieved documents as quickly as possible. These studies have used a set of clustering algorithms, and various experiments have demonstrated that clustering can be significantly more effective than the traditional ranked list approach <ref type="bibr" coords="2,232.70,324.99,12.84,4.94" target="#b6">[7]</ref> like the one we have implemented.</p><p>The researchers, who tackled a task similar to ours, believed one premise could be formulated differently <ref type="bibr" coords="2,138.76,352.09,11.48,4.94" target="#b7">[8]</ref>. Therefore, the system must avoid retrieving duplicate results and thus rely on some form of clustering. To achieve this, they propose a probabilistic ranking framework for premises based on the idea of TF-IDF that, given a query claim, the system will consider clusters of premises with the same meaning instead of isolated claims and premises.</p><p>As the retrieval task involves text documents, different word classes are likely to be significant. We can see the use of part of speech (POS) in order to compute a term weight <ref type="bibr" coords="2,428.14,419.84,11.27,4.94" target="#b8">[9]</ref>. Term weights are mathematical computations of how informative words are, and constitute an integral part of the statistical modelling of documents by information retrieval systems. Inspired by this reference, we will assign weights to noun and not-noun classes of words in our proposed methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In our system, the source file includes a parser, indexer, analyzer and a searcher in order to have a robust project structure. The general structure of the system can be divided into several parts. In particular each component has different aims as follows:</p><p>1. Parser: It parses the corpus CSV file and extracts documents. 2. Analyzer: It processes the extracted documents to apply tokenizers and stemmers. 3. Indexer: It creates indexable fields based on the parsed documents. 4. Searcher: It tries to retrieve relevant documents for each provided topic as a query.</p><p>The details of each part are separately reported in sections 3.2, 3.3, 3.4, 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Our System in General</head><p>Our information retrieval system starts by parsing the collection obtaining the parsed documents. Each parsed document is analyzed and indexed keeping only the necessary information. Documents are indexed in 2 different folders:</p><p>• First Folder: We store for each document its context field and its sentences field after having extracted only their ids. • Second Folder: We store each sentence found in the sentences field with additional data such as its stance retrieved from the conclusion and premises fields. Topics are then parsed to formulate 2 queries for each of them: The first query will be used to retrieve the relevant sentences' ids according only to the corresponding contexts while the second query will be used to retrieve relevant sentences. Query expansion can be applied to formulate queries also by means of synonyms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Parser</head><p>Our utmost goal at this step was to extract as much data as possible and not to lose any meaningful data. To achieve the aimed completeness, our parsed document consists of 5 fields which are document id, hash-table of premise ids and their texts, premise stances, hash-table of conclusion ids and their texts and context. Document ids were easy to extract while premise stances and hash-tables needed more effort. Our first approach to extract premise stances was done by means of regular expressions to find patterns of 'stance': 'PRO' or 'stance': 'CON' in the premises column of the corpus. Moreover, the same was applied to extract premises/conclusions along with their ids. Here we have used regular expressions that iterates the sentences column of the corpus and tries to find patterns of 'sent-id' and 'sent-text' as the starting points and then extracts only the clean id and text of each premise/conclusion. However, in the second approach we decided to simplify the use of regular expressions by combining them with Jackson JSON parser <ref type="bibr" coords="4,392.52,144.43,16.41,4.94" target="#b9">[10]</ref>. Finally, we kept the context part for later experiments to check whether a retrieved sentence is ideally the most representative sentence of its corresponding argument or not. Keeping contexts was very tricky because it affects the indexing runtime.</p><p>Through our experiments we noticed that in almost 10 percent of the whole corpus, there exists a conclusion in the conclusion column but it does not appear in the sentences column. This happens because some conclusion texts are less than 3 words and do not actually contain any stance information. For all these cases we have considered the conclusion as empty in the hash-table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Analyzer</head><p>We implemented a custom analyzer which extends the abstract class Analyzer provided in Apache Lucene <ref type="bibr" coords="4,160.84,316.09,16.38,4.94" target="#b10">[11]</ref>. We made the architecture flexible by parametrizing our analyzer which allows us to call it with different types of filters, tokenizers and stemmers. In this way the analyzer can be modified from the top layer without changing the core code each time. By changing the settings of these parameters it is possible to achieve better results in terms of precision. In addition, it is possible to increase or decrease both the index size and the indexing time required. We allow to choose from the possible parameters: • English Possessive: Allows specifying if to apply or not the English Possessive Filter.</p><p>• Shingles: Allows specifying the size of the word n-grams if used.</p><p>• N-grams Size: Allows specifying the size of n-grams if used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Custom Stop-List</head><p>We decided to analyze the indexed collection and try to create custom stop-lists composed of the most frequent terms in the indexed collection. Thus we parsed, analyzed without any kind of stemming and indexed the collection to inspect it with Luke <ref type="bibr" coords="4,370.90,615.84,16.25,4.94" target="#b11">[12]</ref>. Luke is a tool that can be used to inspect the index and obtain some statistics of the indexed files. After having analyzed the entire indexed collection with Luke, we managed to create 2 different stop-lists composed of 100 terms each: one that can be used for the context field of the collection and one for the sentences field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Indexer</head><p>The arguments of the collection are stored in the index using four fields provided by the parser. It is possible to avoid indexing some sentences that are recognized as spam. By looking at the document collection, in fact, we found many sentences that contained random characters or words in other languages. Therefore we decided to apply a language model provided by Apache Open NLP <ref type="bibr" coords="5,138.32,164.99,17.92,4.94" target="#b12">[13]</ref> to retrieve the language of a sentence: if a sentence is not recognized as being written in English, the sentence is discarded. This resulted in having roughly 300 thousands sentences removed from the indexed documents but this requires additional time when indexing. It is possible to see the details obtained from some of our runs regarding the index size and indexing time based on the parameters used in the analyzer and based on the utilization of the spam method used in the following table: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Searcher</head><p>The searcher is the component that is in charge of retrieving, from a given topic, a ranked list of documents from the most to the least relevant. In this case the searcher aims at retrieving the best 1000 pairs of sentences for a given topic. Each couple of sentences should have the same stance that can be either PRO in case the pair of sentences is supporting the topic or CON vice versa. Our developed searcher integrates different tools:</p><p>• Query Expansion: This is a technique that allows us to reformulate a query in order to increase the number of matched documents. • Term Weighting: This is a technique consisting of assigning different weights to each term of the query based on its discriminant power on the collection. • Document Re-Ranking: This is a technique that is applied after having retrieved a large set of relevant documents. It involves applying different operations to rank the set of retrieved document once again and keep only a smaller subset composed by the most relevant ones.</p><p>We added the possibility to formulate queries based on both topics' title and description. After running our system we found that formulating queries using also the topics' description was deteriorating the precision of our system, therefore we decided to consider only the topics' title and to not further implement the retrieval based also on the description field. The searcher first parses the provided topics to extract their fields and then formulates 2 queries:</p><p>1. First Query: It is used to retrieve relevant sentences' id that can be either premises or conclusions based only on the context field of the collection. 2. Second Query: It is used to retrieve relevant sentences that can be either premises or conclusions based only on the sentences field of the collection.</p><p>After obtaining the 2 lists of relevant documents corresponding to each query, we only keep the best 2000 sentences based on the results of the 2 queries. This is shown in the Algorithm 1.</p><p>Algorithm 1 Filter ranked documents finalListSentences.orderByDescendingWeight()</p><formula xml:id="formula_0" coords="6,91.93,459.56,10.45,4.06">12:</formula><p>finalListSentences=finalListSentences.keepOnlyFirst2000()</p><formula xml:id="formula_1" coords="6,91.93,473.11,10.45,4.06">13:</formula><p>return finalListSentences</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1.">Query Expansion</head><p>We tried to implement the query expansion part by using the WordNet <ref type="bibr" coords="6,410.82,538.07,18.00,4.94" target="#b13">[14]</ref> thesaurus which provides a list of words and their corresponding synonyms. Each topic is processed in a way that when applying query expansion, we try to retrieve the possible synonyms for each term in the topic and consider them when formulating the final query.</p><p>Since each term can have different grammatical meaning based on the context (e.g. can be either a noun or a verb) and consequently the list of synonyms can be different, we first use the Apache Open NLP <ref type="bibr" coords="6,191.40,619.37,17.91,4.94" target="#b12">[13]</ref> library to mark up each term as corresponding to a particular part of speech (POS). We tested the POS-Tagger with 2 different models provided by Apache Open NLP that are en-pos-maxent.bin and en-pos-perceptron.bin and we eventually decided to use only the first one because it was generally more accurate.</p><p>After having tagged each term, we retrieve from the WordNet dataset the list of synonyms associated with each term based on its tag. This is achieved by keeping an in-memory representation of the dataset through a HashMap. Thus we can obtain search operations in O <ref type="bibr" coords="7,462.90,115.08,12.77,9.72" target="#b0">(1)</ref>.</p><p>The final query is then formulated by means of the Lucene <ref type="bibr" coords="7,355.36,130.88,17.75,4.94" target="#b10">[11]</ref> query classes BooleanQuery, BoostQuery, TermQuery and PhraseQuery in the following way:</p><p>1. Each term of the topic title considering both the original terms of the topic and synonyms is wrapped in a TermQuery object. If a synonym is composed of many terms (e.g. one of the synonyms for the term 'search' is 'look for' ), then we use a PhraseQuery instead of a TermQuery when creating the second query (the one used to retrieve sentences based only on the topics' sentences field). In fact, we index sentences storing offset values as well, and this allows PhraseQuery for multiple terms matching. 2. Each TermQuery/PhraseQuery object is then wrapped in a BoostQuery object that allows to specify a weight. These parameters can be set in our ISearcher class as parameters that depend on the POS tag of the term and whether it is a synonym or an original term of the topic. 3. Each BoostQuery object is then added to a final BooleanQuery that represents the OR of all the added BoostQueries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2.">Term Weighting</head><p>When formulating a query, term weighting can be incredibly useful in emphasizing the importance of some terms over others. In fact, after having inspected the collection, we found that many terms appeared in nearly every document while others were found in only some of them. It is possible to see that terms appearing in almost every document do not have a high discriminant power: a query based only on that term would retrieve almost every document in the collection. We implemented our Searcher with 2 possible Term Weighting measures:</p><p>• TF-IDF: Over the whole collection. It is a customized version of the normal TF-IDF in which the term frequency TF is calculated over all the terms in the collection and not for the terms in the considered query. We also tried to smooth it by means of the fifth root. Let 𝑡𝑜𝑡 𝑡𝑘 be the total number of tokens in the collection, 𝑡𝑜𝑡 𝑑𝑜𝑐 be the total number of documents in the collection, |𝑡𝑜𝑘𝑒𝑛 𝑖 | be the number of occurrences of the 𝑖-th considered token over the whole collection and |𝑑𝑜𝑐 𝑡𝑜𝑘𝑒𝑛 𝑖 | be the number of documents containing at least one occurrence of the 𝑖-th considered token. Then the weight of a term according to this measure can be calculated as follows:</p><formula xml:id="formula_2" coords="7,137.13,553.29,349.56,21.77">𝑇 𝐹 𝐼𝐷𝐹 (𝑡𝑜𝑘𝑒𝑛 𝑖 ) = 𝑇 𝐹 (𝑡𝑜𝑘𝑒𝑛 𝑖 ) • 𝐼𝐷𝐹 (𝑡𝑜𝑘𝑒𝑛 𝑖 ) = 5 √︁ |𝑡𝑜𝑘𝑒𝑛 𝑖 | 𝑡𝑜𝑡 𝑡𝑘 • (log 𝑡𝑜𝑡 𝑑𝑜𝑐 |𝑑𝑜𝑐 𝑡𝑜𝑘𝑒𝑛 𝑖 | )</formula><p>• I-Coefficient: This is a coefficient that we came up with to calculate the discriminant power of a term. Let 𝑡𝑜𝑡 𝑑𝑜𝑐 be the total number of documents in the collection, |𝑡𝑜𝑘𝑒𝑛 𝑖 | be the number of occurrences of the 𝑖-th considered token over the whole collection and |𝑑𝑜𝑐 𝑡𝑜𝑘𝑒𝑛 𝑖 | be the number of documents containing at least one occurrence of the 𝑖-th considered token. Then the weight of a term according to this measure can be calculated as follows:</p><formula xml:id="formula_3" coords="7,201.51,665.21,220.81,18.46">𝐼 𝑐𝑜𝑒𝑓 (𝑡𝑜𝑘𝑒𝑛 𝑖 ) = (1 - |𝑑𝑜𝑐 𝑡𝑜𝑘𝑒𝑛 𝑖 | 𝑡𝑜𝑡 𝑑𝑜𝑐 ) • (1 - |𝑑𝑜𝑐 𝑡𝑜𝑘𝑒𝑛 𝑖 | 2•|𝑡𝑜𝑘𝑒𝑛 𝑖 | )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3.">Document Re-Ranking</head><p>We implemented document re-ranking using IBM Project Debater. This is an AI system developed by IBM that was developed to debate humans on complex topics <ref type="bibr" coords="8,429.53,124.35,16.41,4.94" target="#b14">[15]</ref>. It has been in development since 2012 and it is based on a massive dataset. To use project debater it is necessary to have an internet connection since each API call is done through http requests, then IBM servers elaborate the corresponding answer and send back an http response containing it. Using project debater therefore requires almost no computation by the computer that is running our system, but it takes quite some time to obtain an answer for all the topics. We used the project debater Java library to exploit the following 2 APIs:</p><p>• Pro/Cons API: This API is used to understand if a sentence most strongly opposes the topic or most strongly supports it. This is used to obtain which of the 2000 retrieved sentences by our system are PRO or CON a given topic. In fact, the final output of our program needs to have a couple of sentences having same stance. This API is used by default in our system; the time taken to obtain the result for 50 topics considering only this part of our system is in the order of 1 hour. • Evidence Detection API: This API is used to determine if a sentence is likely to contain evidence relating to a topic and vice versa. By means of this API we saw a significant improvement in the results, especially considering the best ranked sentences. Using this API in our system is optional; the time taken to obtain the result for 50 topics considering only this part of our system is in the order of 4-5 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>Our work is based on the following experimental setups:</p><p>• Repository: https://bitbucket.org/upd-dei-stud-prj/seupd2122-intseg/src/master/ • Collection: The corpora at https://webis.de/events/touche-22/shared-task-1.html • Evaluation Measure: LMDirichlet • System Hardware: We have used different computers to test our system but we report in this document only the results obtained with a machine having the following specifications:</p><p>-CPU: Intel i5-8600k not overclocked -GPU: Zotac Nvidia Gtx 1060 AMP 6 GB -RAM: 32 GB ddr4 3000 MHz -SSD: Samsung 960 evo 1 TB nvme, sequential read: 3,200MB/s, sequential write: 1,900MB/s</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In order to determine which configuration provided better results, we tested the system with a variety of configurations. Additionally, we also tried adapting it to last year's Touché Task 1 <ref type="bibr" coords="8,492.63,669.42,11.27,4.94" target="#b5">[6]</ref>.</p><p>This has been done by modifying the searcher part so that queries are only applied to contexts and stance detection is not being performed.</p><p>Here we report the results of 3 systems with different configurations based on last year's topics and relevance judgements following some important evaluation measures. The 3 systems reported here have been chosen to demonstrate how different configurations lead to variant results. However these 3 systems are different than the one submitted to CLEF <ref type="bibr" coords="9,441.24,157.97,11.43,4.94" target="#b2">[3]</ref>. We have observed that, in general, the system performs better when configured to not use stop-lists and Porter stemmer. As well, we noticed that using synonyms and POS tags could improve our precision levels, whereas the use of stemmers decreased overall. It is possible to see from the above results that our IR system does not perform very well on the topics from last year. However, we tried to change different parameters to find which configurations were the most effective ones in order to submit our runs to the CLEF <ref type="bibr" coords="10,373.63,157.97,11.43,4.94" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Configurations</head><p>We inspected how the different queries were created. This was in order to understand if there were some problems in our system. However, queries seemed to be formulated in a correct way, by assigning different weights to different terms based on their discriminant power and part of speech tag. We also tried to inspect which topics were obtaining the best results and therefore we tried to see if there were problems with our stop-lists or particular stemmers/tokenizers adapting our system as a consequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Statistical Analysis</head><p>This section contains a summary of the hypothesis testing we did using ANOVA and boxplots for our 5 different runs submitted to CLEF <ref type="bibr" coords="10,284.79,310.68,13.00,4.94" target="#b2">[3]</ref> which their configurations are listed in Table <ref type="table" coords="10,89.29,324.23,3.81,4.94" target="#tab_4">3</ref>. The mean value is calculated using two metrics: Average Precision (AP) and Normalized Discounted Cumulative Gain at 10 (nDCG@10).  Based on the Average Precision in Figure <ref type="figure" coords="10,281.53,578.00,4.97,4.94" target="#fig_4">3</ref> (a) we can notice that run 1, run 2, run 3 and run 4 all show a similar structure in terms of their median values and interquartile ranges. However, with respect to the length of the whiskers, we can note some differences. Moreover, as shown in run 1 and run 4, both of which use stoplists, there are many outliers (the ones represented as circles). The presence of these outliers leads to deteriorated performance.</p><p>When considering nDCG@10 in Figure <ref type="figure" coords="10,271.18,645.75,17.56,4.94" target="#fig_4">3 (b)</ref>, run 1 and run 3 have an almost identical structure. Since run 2 was the one with the highest mean value and had the smallest standard deviation, run 2 in this case is the one with the best performance.  On the other hand, the ANOVA test is used to determine if the means of some groups are equal or not. It assesses whether it is feasible to reject or not reject a null hypothesis H0 (hypothesis that all means are equal between the groups). The following two tables demonstrate that the p-value obtained when considering both AP and nDCG is below the value alpha that we set to 0.05. We can therefore conclude that the null hypothesis is rejected.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In this paper we reviewed the structure of an Information Retrieval system and we proposed a solution to the Information Retrieval problem proposed by the CLEF 2022 Touchè Task 1 <ref type="bibr" coords="12,492.26,411.84,11.56,4.94" target="#b2">[3]</ref>. We started by describing each of the different components in our system and further we showed how they interact with each other. We also discussed the results obtained by running our system with various configurations and we highlighted which could have been some of its major issues. We finally observe that there is the need for a more complex evaluation measure. In fact, there may be many combinations of sentences that could be matched together forming couple of relevant sentences having same stance. Therefore before using trec_eval <ref type="bibr" coords="12,417.56,493.14,18.03,4.94" target="#b15">[16]</ref> to evaluate our system, it could be useful to modify the relevance judgements/quality judgements given by CLEF <ref type="bibr" coords="12,89.29,520.23,12.82,4.94" target="#b2">[3]</ref> according to the coherence judgements to account also for different possible combinations of sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Discussions and Future Work</head><p>Our IR system is far from perfect and there are many aspects that can be improved to make it more effective. Extracting data that can be used to formulate queries from the topics' description field may improve performances. This must be done in a way that only the relevant terms are actually added to the query because some terms may introduce noise and decrease precision.</p><p>Another improvement that can be done is to parse contexts in a better way to index only the fields that can actually be useful. In this way, the index size can be decreased and the precision of the system may be improved by formulating queries to more fields.</p><p>In addition, another feature that could be implemented is to apply query expansion by retrieving some terms from documents that resulted the most relevant from a first search phase and formulate a new query accordingly. In this way we could create a second query containing more terms and that would likely increase the recall (similar approach to pseudo-relevance feedback).</p><p>Lastly machine learning and in particular systems like BERT's <ref type="bibr" coords="13,379.14,157.97,17.90,4.94" target="#b16">[17]</ref> could be used to further improve the results when doing re-ranking and stance detection. Machine learning is, in fact, a field in which many researchers are interested and therefore new well performing solutions will be proposed. Thus integrating those solutions in future IR systems will probably be a key factor to further improve their effectiveness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,102.95,359.63,389.38,9.76;3,99.21,222.64,396.85,124.79"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Schema of our system representing the parsing, analyzing and indexing parts</figDesc><graphic coords="3,99.21,222.64,396.85,124.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,154.84,539.51,285.60,9.76;3,99.21,448.37,396.85,78.94"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Schema of our system representing the searching part</figDesc><graphic coords="3,99.21,448.37,396.85,78.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,107.28,407.02,398.91,9.76;4,116.56,422.89,61.18,4.94;4,107.28,435.47,399.91,9.76;4,116.56,451.35,86.74,4.94;4,107.28,463.93,398.71,9.76;4,116.56,479.80,30.84,4.94;4,107.28,492.38,258.66,9.76"><head>•</head><label></label><figDesc>Tokenizer: Allows choosing from different tokenizers to use such as Whitespace, Letter and Standard. • Stemmer: Allows choosing from different stemmers to use such as none, EnglishMinimal, Porter and Krovetz. • Token Length: Allows specifying the minimum length for a token and its maximum length. • Stop-List: Allows specifying a possible stop-list if used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="11,167.29,233.65,52.04,8.92;11,359.58,233.65,84.15,8.92"><head></head><label></label><figDesc>Using nDCG@10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="11,89.29,268.01,416.70,9.76;11,99.71,84.19,187.51,141.73"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Boxplots of runs (X: Runs in decreasing order of mean performance, Y: Performance)</figDesc><graphic coords="11,99.71,84.19,187.51,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="12,91.55,324.65,412.18,9.76;12,99.71,114.31,187.52,170.09"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: ANOVA tables (The means of runs 2 and 5 in the X-axis are significantly different)</figDesc><graphic coords="12,99.71,114.31,187.52,170.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,100.13,266.33,392.85,176.40"><head>Table 1 :</head><label>1</label><figDesc>Indexing with different settings for the analyzer's parameters</figDesc><table coords="5,100.13,266.33,392.85,140.82"><row><cell>Tokenizer</cell><cell cols="2">Stemmer Minimum</cell><cell>Maximum</cell><cell cols="2">Stoplist Spam</cell><cell cols="2">Time Total</cell></row><row><cell></cell><cell></cell><cell>token</cell><cell>token</cell><cell></cell><cell>detection</cell><cell></cell><cell>size</cell></row><row><cell></cell><cell></cell><cell>length</cell><cell>length</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">WhiteSpace Krovetz</cell><cell>3</cell><cell>10</cell><cell>Yes</cell><cell>No</cell><cell>527 s</cell><cell>1.01 GB</cell></row><row><cell cols="2">WhiteSpace Porter</cell><cell>2</cell><cell>10</cell><cell>Yes</cell><cell>No</cell><cell>559 s</cell><cell>1.00 GB</cell></row><row><cell>Standard</cell><cell>English</cell><cell>2</cell><cell>20</cell><cell>No</cell><cell>No</cell><cell>451 s</cell><cell>1.10 GB</cell></row><row><cell></cell><cell>Minimal</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">WhiteSpace English</cell><cell>2</cell><cell>20</cell><cell>No</cell><cell>Yes</cell><cell cols="2">1057 s 1.06 GB</cell></row><row><cell></cell><cell>Minimal</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,114.92,202.17,365.22,464.56"><head>Table 2 :</head><label>2</label><figDesc>Touché Task 1 2021 [6] Results</figDesc><table coords="9,114.92,202.17,365.22,427.69"><row><cell>Evaluation Measure</cell><cell>System 1:</cell><cell>System 2:</cell><cell>System 3:</cell></row><row><cell></cell><cell>Letter Tok.</cell><cell>Whitespace Tok.</cell><cell>Whitespace Tok.</cell></row><row><cell></cell><cell>English Stem.</cell><cell>Krovetz Stem.</cell><cell>Porter Stem.</cell></row><row><cell></cell><cell>No Stop-List</cell><cell>Stop-List</cell><cell>No Stop-List</cell></row><row><cell></cell><cell>POS Tag</cell><cell>POS Tag</cell><cell>POS Tag</cell></row><row><cell></cell><cell>WordNet</cell><cell>WordNet</cell><cell>WordNet</cell></row><row><cell></cell><cell>Evidence Det.</cell><cell>Evidence Det.</cell><cell>TFIDF</cell></row><row><cell></cell><cell>ICoefficient</cell><cell>ICoefficient</cell><cell>LMDirichlet</cell></row><row><cell></cell><cell>LMDirichlet</cell><cell>LMDirichlet</cell><cell></cell></row><row><cell>num_ret</cell><cell>43397</cell><cell>47895</cell><cell>48054</cell></row><row><cell>num_rel</cell><cell>1818</cell><cell>1818</cell><cell>1818</cell></row><row><cell>num_rel_ret</cell><cell>1113</cell><cell>1048</cell><cell>806</cell></row><row><cell>map</cell><cell>0.0329</cell><cell>0.0240</cell><cell>0.0197</cell></row><row><cell>Rprec</cell><cell>0.0458</cell><cell>0.0269</cell><cell>0.0316</cell></row><row><cell>bpref</cell><cell>0.4731</cell><cell>0.4416</cell><cell>0.3645</cell></row><row><cell>iprec_at_recall_0.00</cell><cell>0.1695</cell><cell>0.0954</cell><cell>0.1081</cell></row><row><cell>iprec_at_recall_0.20</cell><cell>0.0512</cell><cell>0.0408</cell><cell>0.0404</cell></row><row><cell>iprec_at_recall_0.40</cell><cell>0.0396</cell><cell>0.0344</cell><cell>0.0240</cell></row><row><cell>iprec_at_recall_0.60</cell><cell>0.0279</cell><cell>0.0244</cell><cell>0.0135</cell></row><row><cell>iprec_at_recall_0.80</cell><cell>0.0156</cell><cell>0.0118</cell><cell>0.0048</cell></row><row><cell>P_5</cell><cell>0.0760</cell><cell>0.0280</cell><cell>0.0240</cell></row><row><cell>P_10</cell><cell>0.0640</cell><cell>0.0360</cell><cell>0.0220</cell></row><row><cell>P_100</cell><cell>0.0370</cell><cell>0.0238</cell><cell>0.0276</cell></row><row><cell>P_1000</cell><cell>0.0223</cell><cell>0.0210</cell><cell>0.0161</cell></row><row><cell>recall_5</cell><cell>0.0101</cell><cell>0.0038</cell><cell>0.0030</cell></row><row><cell>recall_10</cell><cell>0.0164</cell><cell>0.0101</cell><cell>0.0065</cell></row><row><cell>recall_100</cell><cell>0.1063</cell><cell>0.0667</cell><cell>0.0888</cell></row><row><cell>recall_1000</cell><cell>0.6134</cell><cell>0.5753</cell><cell>0.4369</cell></row><row><cell>ndcg_cut_5</cell><cell>0.0428</cell><cell>0.0187</cell><cell>0.0214</cell></row><row><cell>ndcg_cut_10</cell><cell>0.0417</cell><cell>0.0227</cell><cell>0.0188</cell></row><row><cell>ndcg_cut_100</cell><cell>0.0715</cell><cell>0.0430</cell><cell>0.0550</cell></row><row><cell>ndcg_cut_1000</cell><cell>0.2567</cell><cell>0.2256</cell><cell>0.1830</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,206.47,541.48,181.99,9.76"><head>Table 3 :</head><label>3</label><figDesc>The runs submitted to CLEF<ref type="bibr" coords="10,375.61,543.81,12.84,4.94" target="#b2">[3]</ref> </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="11,95.67,463.38,394.10,98.95"><head>Table 4 :</head><label>4</label><figDesc>The one-way ANOVA table using AP</figDesc><table coords="11,95.67,510.73,394.10,51.61"><row><cell>'Source'</cell><cell>'SS'</cell><cell>'df'</cell><cell>'MS'</cell><cell>'F'</cell><cell>'Prob&gt;F'</cell></row><row><cell>'Columns'</cell><cell>1.1896</cell><cell>4</cell><cell>0.2974</cell><cell>7.6717</cell><cell>7.7149e-06</cell></row><row><cell>'Error'</cell><cell>9.4976</cell><cell>245</cell><cell>0.0388</cell><cell>[]</cell><cell>[]</cell></row><row><cell>'Total'</cell><cell>10.6872</cell><cell>249</cell><cell>[]</cell><cell>[]</cell><cell>[]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="11,176.94,588.44,241.04,9.76"><head>Table 5 :</head><label>5</label><figDesc>The one-way ANOVA table using nDCG@10 From the following charts, we can see that run 5 is quite different from the others.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="13,112.66,270.80,394.62,4.94;13,112.31,284.35,241.35,4.94" xml:id="b0">
	<monogr>
		<ptr target="https://webis.de/events/touche-22/shared-task-1.html" />
		<title level="m" coord="13,112.66,270.80,360.21,4.94">Touché task 1: Argument retrieval for controversial questions</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Touchè@CLEF</note>
</biblStruct>

<biblStruct coords="13,112.66,297.90,394.04,4.94;13,112.26,311.44,234.30,4.94" xml:id="b1">
	<monogr>
		<ptr target="https://webis.de/events/touche-22/shared-task-2.html" />
		<title level="m" coord="13,112.66,297.90,352.99,4.94">Touché task 2: Argument retrieval for comparative questions</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Touchè@CLEF</note>
</biblStruct>

<biblStruct coords="13,112.66,324.99,395.17,4.94;13,112.66,338.54,395.17,4.94;13,112.66,352.09,395.01,4.94;13,112.41,365.64,393.57,4.94;13,112.66,379.19,339.15,4.94" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,361.25,338.54,146.58,4.94;13,112.66,352.09,62.60,4.94">Overview of Touché 2022: Argument Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gurcke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,197.77,352.09,309.90,4.94;13,112.41,365.64,309.42,4.94">Experimental IR Meets Multilinguality, Multimodality, and Interaction. 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<title level="s" coord="13,429.80,365.64,76.18,4.94;13,112.66,379.19,78.83,4.94">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="13,112.66,392.74,395.17,4.94;13,112.66,406.29,394.53,4.94;13,112.66,419.84,70.43,4.94" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,161.24,392.74,304.73,4.94">Evaluating document clustering for interactive information retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Leuski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,488.61,392.74,19.21,4.94;13,112.66,406.29,389.17,4.94">Proceedings of the tenth international conference on Information and knowledge management</title>
		<meeting>the tenth international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,433.39,393.57,4.94;13,112.33,446.94,58.19,4.94" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,190.27,433.39,205.24,4.94">Modern information retrieval: A brief overview</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,404.12,433.39,89.43,4.94">IEEE Data Eng. Bull</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="35" to="43" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,460.49,394.52,4.94;13,112.66,474.04,393.33,4.94;13,112.66,487.58,315.37,4.94" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,335.85,474.04,92.96,4.94">Overview of Touché</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-85251-1_28</idno>
	</analytic>
	<monogr>
		<title level="m" coord="13,459.33,474.04,46.66,4.94;13,112.66,487.58,38.01,4.94">Argument Retrieval</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="450" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,501.13,394.62,4.94;13,112.66,514.68,395.17,4.94;13,112.66,528.23,395.01,4.94;13,112.66,541.78,371.62,4.94" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,167.59,501.13,315.37,4.94">Evaluating document clustering for interactive information retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Leuski</surname></persName>
		</author>
		<idno type="DOI">10.1145/502585.502592</idno>
		<ptr target="https://doi.org/10.1145/502585.502592.doi:10.1145/502585.502592" />
	</analytic>
	<monogr>
		<title level="m" coord="13,112.66,514.68,395.17,4.94;13,112.66,528.23,84.39,4.94">Proceedings of the Tenth International Conference on Information and Knowledge Management, CIKM &apos;01</title>
		<meeting>the Tenth International Conference on Information and Knowledge Management, CIKM &apos;01<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,555.33,394.53,4.94;13,112.66,568.88,393.33,4.94;13,112.66,582.43,367.73,4.94" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,286.75,555.33,156.05,4.94">A framework for argument retrieval</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dumani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,449.96,568.88,56.03,4.94;13,112.66,582.43,94.24,4.94">Advances in Information Retrieval</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Magalhães</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Castells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Silva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Martins</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="431" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,595.98,393.33,4.94;13,112.66,609.53,393.05,4.94" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,247.03,595.98,222.09,4.94">Part of speech n-grams and information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Van Rijsbergen</surname></persName>
		</author>
		<idno type="DOI">10.3917/rfla.131.0009</idno>
		<ptr target="https://doi.org/10.3917/rfla.131.0009" />
	</analytic>
	<monogr>
		<title level="j" coord="13,478.15,595.98,27.84,4.94;13,112.66,609.53,152.42,4.94">Revue française de linguistique appliquée</title>
		<imprint>
			<biblScope unit="volume">XIII</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,623.08,344.97,4.94" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><surname>Fasterxml</surname></persName>
		</author>
		<ptr target="https://github.com/FasterXML/jackson" />
		<title level="m" coord="13,166.81,623.08,83.92,4.94">Jackson json parser</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,636.63,252.35,4.94" xml:id="b10">
	<monogr>
		<ptr target="https://lucene.apache.org/" />
		<title level="m" coord="13,112.66,636.63,101.86,4.94">Apache, Apache lucene</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,650.17,208.44,4.94" xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Luke</surname></persName>
		</author>
		<ptr target="http://www.getopt.org/luke/" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,663.72,270.02,4.94" xml:id="b12">
	<monogr>
		<ptr target="https://opennlp.apache.org/" />
		<title level="m" coord="13,112.66,663.72,112.34,4.94">Apache, Apache open nlp</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,90.23,395.01,4.94;14,112.28,103.78,96.76,4.94" xml:id="b13">
	<monogr>
		<ptr target="https://wordnet.princeton.edu/download" />
		<title level="m" coord="14,112.66,90.23,175.70,4.94">Wordnet: A lexical database for english</title>
		<imprint>
			<date type="published" when="2005">2005. 2022-04-25</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,117.33,394.04,4.94;14,112.66,130.88,93.01,4.94" xml:id="b14">
	<monogr>
		<ptr target="https://research.ibm.com/interactive/project-debater/how-it-works/" />
		<title level="m" coord="14,145.23,117.33,102.39,4.94">Ibm project debater</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
		<respStmt>
			<orgName>IBM</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,144.43,346.27,4.94" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">I</forename><surname>Standards</surname></persName>
		</author>
		<ptr target="https://trec.nist.gov/trec_eval/" />
		<title level="m" coord="14,194.40,144.43,95.21,4.94">Technology, trec_eval</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,157.97,393.32,4.94;14,112.66,171.52,383.96,4.94" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="14,356.77,157.97,149.22,4.94;14,112.66,171.52,181.08,4.94">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">N</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
