<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.69,84.74,365.76,15.42;1,89.29,106.66,257.21,15.42;1,89.29,129.00,324.97,11.96">Aramis at TouchÃ© 2022: Argument Detection in Pictures using Machine Learning Notebook for the TouchÃ© Lab on Argument Retrieval at CLEF 2022</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.10,154.90,51.63,11.96"><forename type="first">Jan</forename><surname>Braker</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Student at</orgName>
								<orgName type="department" key="dep2">Computer Science (M.Sc</orgName>
								<orgName type="institution">Leipzig University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,153.38,154.90,93.57,11.96"><forename type="first">Lorenz</forename><surname>Heinemann</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science (M.Sc</orgName>
								<orgName type="institution">Student at Leipzig University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,277.95,154.90,86.21,11.96"><forename type="first">Tobias</forename><surname>Schreieder</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Data Science (M.Sc</orgName>
								<orgName type="institution">Student at Leipzig University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.69,84.74,365.76,15.42;1,89.29,106.66,257.21,15.42;1,89.29,129.00,324.97,11.96">Aramis at TouchÃ© 2022: Argument Detection in Pictures using Machine Learning Notebook for the TouchÃ© Lab on Argument Retrieval at CLEF 2022</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">2FEA35D25028C30E1AC21EC982C3C255</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>machine learning</term>
					<term>search engine</term>
					<term>argument retrieval</term>
					<term>neural network</term>
					<term>image search</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work deals with the classifying and retrieving images in a data set. Images that argue for or against a topic should be recognized and ordered according to their argumentativeness. Therefor, different approaches are tested and compared with each other. The best results are provided by a neural network, which has been trained to recognize argumentative images with a total of 10,000 labeled images. The model received various features as input, including color, image text and other features. In addition, initial attempts are made to classify the images and their websites in relation to a given question according to their stance into "pro" (the thesis from the question is supported), "con" (the thesis from the question is attacked) and "neutral" (the thesis from the question is supported to the same extent as it is attacked).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The availability of information on the Internet is constantly growing. All topics are represented on the Internet with public statements of various points of view. Due to the unequal distribution of opinions and one-sided reports, it is often difficult to guarantee a neutral and balanced search result. Even the evaluation of such a search result causes problems. The retrieval of thematically critical search queries is a particular noticeable effort. For this purpose, there are special argument search engines that filter arguments related to a topic <ref type="bibr" coords="1,475.89,493.16,12.99,10.91" target="#b0">[1]</ref> [2] <ref type="bibr" coords="1,89.29,506.70,11.58,10.91" target="#b2">[3]</ref>. However, this is often only possible in text form at the moment <ref type="bibr" coords="1,400.21,506.70,11.58,10.91" target="#b3">[4]</ref>. Although there are opinions that images cannot be argumentative on their own <ref type="bibr" coords="1,361.91,520.25,11.52,10.91" target="#b4">[5]</ref>, there are also hypotheses to the contrary. <ref type="bibr" coords="1,153.33,533.80,64.75,10.91">Kjeldsen et al.</ref> describe in their work the argumentative character of images and graphics and their various functions <ref type="bibr" coords="1,287.23,547.35,11.58,10.91" target="#b5">[6]</ref>. Through the visual component, they can clarify a problem to the viewer and highlight arguments in text form. Certain facts can be presented more convincingly by means of a picture than would be possible in written form <ref type="bibr" coords="1,490.95,574.45,11.28,10.91" target="#b6">[7]</ref>. Source: Tomasz Markowski/Associated Press <ref type="bibr" coords="2,266.97,267.28,11.83,8.87" target="#b7">[8]</ref> For example, in professional cycling, there is a debate about whether the barriers in the finisharea should be improved and replaced, as too many accidents are caused by the old fences <ref type="bibr" coords="2,492.41,317.99,11.44,10.91" target="#b7">[8]</ref>. A presentation of last year's accident statistics can be a useful and convincing argument for increasing safety in cycling. It is also possible to describe the consequences to highlight the scale of the accident. "In a professional race, six riders crashed heavily, with three of them suffering brain and bone damage. " <ref type="bibr" coords="2,132.41,385.74,12.84,10.91" target="#b7">[8]</ref> People can generally imagine text worse than a photo <ref type="bibr" coords="2,333.01,399.29,11.48,10.91" target="#b5">[6]</ref>. A picture of the accident increases the visualization and may reinforce the importance of renewing safety measures. A photo of the accident can be seen in Figure <ref type="figure" coords="2,242.75,426.39,3.74,10.91" target="#fig_0">1</ref>. The effective interplay of text and visual graphics is also exploited by the law of mandatory pictorial warnings on cigarette packs in Germany. The dissuasive images of long-term consequences of smoking are intended to discourage people from smoking. According to research by the German Bundestag, these warnings are more effective than a simple warning in text form. Especially the combination of picture and text achieves high efficiency <ref type="bibr" coords="2,405.83,494.13,11.43,10.91" target="#b6">[7]</ref>. For this reason, it is of great interest to highlight such arguments and argumentative images in a search query. This would be a useful extension, especially for special argument search engines. A method that can classify pixel-based representations as argumentative has not yet been extensively researched in the literature. The aim of this work is to develop a system that can assess and evaluate images according to their argumentative power in order to drive development forward. One possible application would be to obtain arguments from a search query not only in text but also in image form, to gain an even more detailed overview of the searched problem. Finally, an attempt should be made to assign these arguments to a supportive, neutral or negative stance, whereby the focus of this work is clearly on the recognition of argumentativeness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Many previous works have dealt with the problem of finding arguments in text collections. The three largest search engines specifically designed for this task are args.me <ref type="bibr" coords="3,435.45,124.83,11.33,10.91" target="#b0">[1]</ref>, IBM-debater <ref type="bibr" coords="3,89.29,138.38,13.00,10.91" target="#b1">[2]</ref> and ArgumenText <ref type="bibr" coords="3,189.96,138.38,11.58,10.91" target="#b2">[3]</ref>. They all make it possible to search for arguments in texts for a controversial issue and provide arguments in an ordered and clear manner. <ref type="bibr" coords="3,429.53,151.93,78.13,10.91">Wachsmuth et al.</ref> have already dealt with this problem in many works <ref type="bibr" coords="3,319.61,165.48,11.23,10.91" target="#b8">[9,</ref><ref type="bibr" coords="3,333.46,165.48,12.50,10.91" target="#b9">10,</ref><ref type="bibr" coords="3,348.58,165.48,8.88,10.91" target="#b3">4]</ref> and have successfully shown that it is basically possible to extract text excerpts reliably and determine their stance to the topic. However, they are limited to arguments in text form. Currently there is no published argument search engine that includes images in a search, but conventional image search engines can also achieve adequate results if clearly structured search queries are used <ref type="bibr" coords="3,417.38,219.67,16.25,10.91" target="#b10">[11]</ref>. This work will investigate whether it is possible to integrate images as a result of argument search queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Image search and image features</head><p>Compared to working with texts, the steps of indexing and feature extraction have to be adjusted in the retrieval process for images. <ref type="bibr" coords="3,251.03,296.50,45.83,10.91">Latif et al.</ref> summarize in their review article the current technologies and procedures very well <ref type="bibr" coords="3,269.14,310.05,16.41,10.91" target="#b11">[12]</ref>. The different features of images are presented. Particularly important for this work are the color-based properties. The concept of dominant colors is presented and referred to the work of Shao et al. <ref type="bibr" coords="3,340.30,337.15,16.09,10.91" target="#b12">[13]</ref>. They reduce the countless colors of an image to a few representative colors to search for color-level images faster and more effectively. From this it can be concluded that only a few colors are enough to represent the color conditions of a picture. According to <ref type="bibr" coords="3,276.30,377.79,42.51,10.91">Solli et al.</ref> it is also possible to establish a connection between emotions and colors. <ref type="bibr" coords="3,224.97,391.34,16.25,10.91" target="#b13">[14]</ref>. They show that people experience similar emotions when looking at certain shades in pictures. Emotions are an important part of <ref type="bibr" coords="3,409.43,404.89,17.87,10.91" target="#b13">[14]</ref> arguments. From this it can be concluded that colors might also play a central role in the assessment of the argumentativeness of an image. In addition, represented objects are important for the message of the image. These can be reliably detected by object recognition. Mokshin et al. use distinctive structures and shapes in the images <ref type="bibr" coords="3,140.92,472.64,16.39,10.91" target="#b14">[15]</ref>. However, detection often requires specially trained neural networks, which are trained to reliably detect only a few objects using large training data sets <ref type="bibr" coords="3,447.53,486.19,16.41,10.91" target="#b15">[16]</ref>. For the assessment and viewing of many different images this method is not suitable for this work, since for each object to be recognized a training data set with several images of it would be necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Can pictures contain an argument?</head><p>Finding arguments in pictures is indirectly also critically considered by many works. Fleming et al. have the opinion that images cannot contain an argument on their own, and only can be supportive <ref type="bibr" coords="3,138.62,603.66,16.08,10.91" target="#b16">[17]</ref>. According to Fleming et al., an argument must consist of a claim and a support for that claim. An image cannot perform both functions at the same time. This view is also supported by Champagne et. al <ref type="bibr" coords="3,231.40,630.76,11.44,10.91" target="#b4">[5]</ref>. Both complain that a visual representation lacks a textual component to be a clear argument.</p><p>However, many pictures also contain texts and diagrams. This would allow them to meet the requirements by Flemming et al. for an argument. The text on images gives the visual elements a context in which to understand and classify them, which according to Kjeldsen et al. is an important part when looking at pictures <ref type="bibr" coords="4,372.26,127.61,16.42,10.91" target="#b17">[18]</ref>. This fact must be taken into account in the evaluation and creation of the retrieval system and will be dealt with later in this work. However, it can be clearly stated that it is of great importance to examine the additional elements, such as texts and diagrams, on a picture in order to be able to assess the argumentativeness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Argument search in pictures</head><p>Image search has been widespread for many years and has been enhanced by several algorithms and technologies. The most common methods are described and summarized by Meharban et al. <ref type="bibr" coords="4,102.74,258.64,16.25,10.91" target="#b18">[19]</ref>. The search for argumentative images is much less researched than the search for arguments in text form. One approach for image search is to use a search query extension <ref type="bibr" coords="4,444.20,285.73,16.30,10.91" target="#b10">[11]</ref>, whereby the index for the elements to be searched was created only on the texts of the pages of the images. In a search query, only the words good and anti were added and searched for matches in the documents. The result were already on a good level. However, no information from the image was used and the process is based on the assumption that the text used on a website is representative for each image embedded in it. This assumption has to be questioned critically: several images of different content can be integrated on one page, which makes it difficult to assign the texts clearly. This paper aims to take a different approach and to focus exclusively on the picture when assessing argumentativeness. Information from the website should only be used later, when classifying the stance of an argument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Retrieval System</head><p>To determine the relevance of an image to a given query, Kiesel et al. <ref type="bibr" coords="4,389.55,479.85,17.76,10.91" target="#b10">[11]</ref> distinguish three level of relevance. An image is considered relevant to the search engine if it is both topic-relevant, argumentative, and stance-relevant <ref type="bibr" coords="4,254.22,506.95,16.41,10.91" target="#b10">[11]</ref>. Stance-relevance refers to the attitude within the discussion. The tripartite division was adopted for this work and a separate model was developed for each of the three level. The goal of the retrieval system is to find the top k relevant images for both the "pro" and the "con" side. Figure <ref type="figure" coords="4,255.42,561.15,5.17,10.91" target="#fig_1">2</ref> shows the retrieval system in a simplified form. The starting point is always a query as input. The query and most of the texts processed by the system first go through a preprocessing. It uses the Spacy (en-core-web-sm) language model <ref type="bibr" coords="4,316.83,601.80,18.06,10.91" target="#b19">[20]</ref> to get a tokenization on the text first. Subsequently, punctuation marks and stopwords are removed. The remaining tokens are finally lemmatized before they are passed on to the appropriate part of the retrieval model.</p><p>A preprocessed query is first entered into the topic model, which calculates the affiliation to a topic for each image in the data set by using a DirichletLM model. According to the studies of Potthast et al. the DirichletLM performs significantly better in argument retrieval compared to TFIDF and BM25 <ref type="bibr" coords="5,170.12,327.41,16.41,10.91" target="#b20">[21]</ref>. The text of the HTML page of the image is used as input. Since the retrieval of the topic relevance is not be focused by this work, further information provided by the underlying data set is taken into account. This includes, that each image has already been assigned to at least one topic. Additionally, each topic has a handful of example queries. With these and the user query, the topic of the query is determined and a DirichletLM retrieval is performed on the images of the determined topic. Because of that, it is assumed that the topic model returns subject-related images. No further evaluation will take place. As a result, the topic model returns a list of image IDs with a calculated score representing the topic relevance in relation to the query. The next step is to pass the list of image IDs in parallel to the argument model and the stance model. The argument model now calculates a score for the argumentativeness of the respective image. At the same time, the stance model classifies the same images into the classes "pro", "con" and "neutral". Note, however, that only the image IDs that have been classified as either "pro" or "con" are returned by the model in two separate lists. Neutral images are ignored. In some cases, this can lead to an unbalanced retrieval result of the search engine if fewer images are classified for one of the sides. This approach was deliberately chosen because the number of arguments per page gives the user additional value in the search query. In the final step, both the "pro" and "con" lists of the stance model are sorted according to the scores calculated by the argument model. The results are two lists of image IDs in descending order, which ideally contain the images, which support or attack the thesis sought the most. The argument and stance models are discussed in much more detail in the following two chapters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Argument Model</head><p>To be able to rank images according to their argumentativeness, each image must be given a score. The higher this score is, the better an image argues in relation to a topic. An argument that is critical and/or supportive results in a high score. The position for which it argues is not considered. This score makes it possible to search for argumentative images without having to specifically understand their content and assign it to the issue. A diagram for example often has an argumentative supporting character. This can be considered in the argument model and evaluated accordingly. In this case, only the diagram must be recognized, since the expression of the stance is unimportant for the argument model. There are several such features that can be searched for on an image. Whether and how strongly the occurrence and use of these features correlates with the argumentativeness of an image is to be examined later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image Features</head><p>The first thing to look at are the features that can be obtained from the image alone. All images are in PNG format and have a rather low resolution, since they are downloads of embedded images on web pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Color Features</head><p>Colors can be represented on a computer by the RGB color model. It uses three numerical values between 0 and 255 for the colors red, green and blue. From them, a color is described exactly by means of an additive color model. Each color value was normalized between 0 and 1 by means of the maximum value equal to 255.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average Color</head><p>The first feature used for the argument score is the average color of an image. For this purpose, all color values of the pixels of an image are averaged. Possibly, a general color mood and also an emotion when looking at the image, can be detected via this. One hypothesis to be tested is that certain colors are used more often in argumentative images than others. For example, the colors red and green, following the colors of a traffic light, could be used to highlight positive and negative elements as indicators <ref type="bibr" coords="6,249.58,521.97,16.20,10.91" target="#b13">[14]</ref>. This feature consists of three values, respectively for the red, green and blue values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dominant Color</head><p>However, the average color has a decisive disadvantage. If an image has many red and green elements, which could possibly stand for a strong argumentativeness, this cannot be detected by this feature. The average color mixes the colors together to a new color. Thus, in the additive color model, red and green make yellow, making it impossible to distinguish whether the image is yellow, or red and green. One solution is to use dominant colors. Here, the most used colors of an image are considered. The color values of the pixels are grouped and the most used color is output as the dominant color. To avoid grouping by exact colors, the image can optionally be reduced to fewer colors beforehand, creating color intervals, which results in higher accuracy, especially for photographs and color gradients. As a feature, only the first dominant color is used, which in turn consists of the three RGB values. Consequently, the second most used color is no longer considered. The effects of this decision could be analyzed in further studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Percentage Color</head><p>In order to investigate the hypothesis that different colors are used more frequently in argumentative images than in non-argumentative images, the color proportions are determined. The colors red, green and blue as the three colors of the color model were considered, additionally yellow as a neutral color between red and green was added. The color proportion is determined by examining each pixel color to see whether it lies within the specified color interval of the color to be examined. The interval is necessary because, for example, the color green describes a variety of hues. These intervals were specified in the HSV model because the brightness and saturation of a color can be considered independently. These have no effect on the hue. A binary image mask is then created, where a pixel is colored white if the pixel color value is in the interval, and black if not. The ratio of black and white pixels in the image gives the color portion of the color being searched for. The following color intervals were defined (Hue, Saturation, Light value):</p><p>â€¢ Red: (0, 50, 80) to <ref type="bibr" coords="7,195.32,342.81,15.80,10.91" target="#b19">(20,</ref><ref type="bibr" coords="7,213.84,342.81,17.62,10.91">255,</ref><ref type="bibr" coords="7,234.19,342.81,13.85,10.91">255</ref> Since the color red lies in the middle of the zero point of the color scale, two color intervals are necessary to capture the entire color. The same procedure can be applied to the brightness of the image. The HSV color model makes it very easy to filter the light and dark color areas. The thesis in this context is that possibly light colors could be used more in the positive context and dark colors in the negative context.</p><p>â€¢ Light: (0, 0, 200) to (255, 60, 255) â€¢ Dark: (0, 0, 0) to (255, 255, 60)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Optical Character Recognition</head><p>Optical Character Recognition is a common technique that Google Inc. used to develop their text recognition Tesseract <ref type="bibr" coords="7,185.08,570.51,16.25,10.91" target="#b21">[22]</ref>. This open-source software is used to recognize text on images. In this work, each image was checked for recognizable texts using Tesseract. It is noticeable that handwriting is not recognized. Standard fonts with high contrast to the background are found best on images. This is problematic for demonstrations with handwritten posters or photographs with low-contrast fonts. Text in languages other than English is also recognized more poorly. Normally, Tesseract only outputs letters that could be identified with a high probability. However, for this work, the threshold for the probability of a match was lowered, which allowed more text to be detected on the images. The output contains many single letters and symbols, making it advisable to filter for words with a letter length greater than two. In addition, the words found were checked against a lexicon and words not in the English lexicon were also filtered out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Length</head><p>Text length is a feature, which corresponds to the number of words found after post-processing.</p><p>The assumption is that a long text has a higher potential to be argumentative. The text length is then normalized with the following function, where ğ‘¥ is the number of words found:</p><formula xml:id="formula_0" coords="8,216.84,193.59,289.15,12.68">ğ‘¡ğ‘’ğ‘¥ğ‘¡ğ¿ğ‘’ğ‘›ğ‘”ğ‘¡â„ğ¹ ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ = 1 -ğ‘’ -0.01â€¢ğ‘¥ (1)</formula><p>This ensures that a text twice as long not results in a feature value twice as large. Furthermore, from a text length of 200 onwards, there is no longer a large change and the value is asymptotically one, because the subtrahend becomes zero by a large divisor.</p><p>Sentiment Score Furthermore, the text is given into a sentiment analysis, which returns a sentiment score between -1 and 1. A sentiment analysis is a topic of text mining and examines a text snippet for a positive or negative stance. Basically, different words are associated with a certain stance. For example, the word "anti" might indicate a negative stance. The average stance of a text is returned as a continuous numerical value, where -1 indicates a negative text, and 1 indicates a positive text. In this work, a lexicographic approach was taken using the VADER lexicon, with image texts evaluated using NLTK sentiment analysis <ref type="bibr" coords="8,128.57,377.98,16.42,10.91" target="#b22">[23]</ref>. However, many images do not include text, or it was not detected. In addition, sometimes individual words can strongly influence the sentiment score, even though the context would suggest the opposite influence. For the argument model, the absolute value of the score was used, since it is irrelevant whether the argument is for the positive or negative side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Area</head><p>In addition to the individual letters, Tesseract also returns the position and size of the text. Two points on the image plane describe the upper left and lower right point of the smallest possible rectangle that can be placed around the font elements. These areas can be added and put into a ratio with the total area of the image. The resulting percentage value describes how much area of the image is taken up by the text (seen in Figure <ref type="figure" coords="8,320.74,513.47,3.62,10.91" target="#fig_3">3</ref>). Possibly, the area taken up by the text should be understood as a kind of weighting of the sentiment score. For example, logos or source citations are often included, which are displayed small at the bottom. These may be less important than large headlines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Position</head><p>Furthermore, the text position on the image is examined. The theory behind this is that many images contain text captions or headings, with many texts found on the edges of the images. These texts could possibly be attributed a different meaning than the text found in the middle of the image. This was realized using a heat map, with the image divided into an even 8x8 grid. For each field, the amount of text is determined and the 64 values are stored in a two-dimensional array. This can be seen as an example in Figure <ref type="figure" coords="8,302.52,662.51,3.74,10.91" target="#fig_3">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Additional features</head><p>Two additional features were created after an analysis of the data set revealed that many images were graphics and not photographs: Image Type and Diagram Detection. The assumption was made that graphics have a higher potential to be argumentative than photographs. This was said to be due to the more frequent presence of text and diagrams, which form assertion and/or support of the argument <ref type="bibr" coords="9,234.22,436.44,16.42,10.91" target="#b16">[17]</ref>. To do this, it must be recognized what type of image is involved. Also, it would be reasonable to assume that multiple and larger diagrams have a higher argumentative character.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Type</head><p>A distinction is made between graphics (cartoons, clipart) and photographs. Abd Zaid et al. showed in their work that cartoons generally consist of significantly fewer colors than photographs <ref type="bibr" coords="9,147.01,531.29,16.09,10.91" target="#b23">[24]</ref>. This can be used to build a classifier which distinguish between cartoons and photographs. The classifier looks at the ten dominant colors and their image proportion. If these take up more than 30% of the image, it is assumed to be a graphic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diagram Detection</head><p>To recognize diagrams, the image is preprocessed in several steps. The procedure was described this way by the user nathancy on the page Stackoverflow.com <ref type="bibr" coords="9,358.75,612.58,16.11,10.91" target="#b24">[25]</ref>. First, the image is converted to a binary image by a threshold-value. Now the image consists only of black and white pixels. If the contrast between text and background is high enough, the text is clearly visible. The text can be removed using an extended horizontal kernel. The kernel removes all elements and image-areas, which looks like small horizontal lines, just like a line of text. By assuming that the text was written horizontally, all lines of text will be removed and colored black. All remaining image elements, which are the left over white areas, are no text and combined and extracted. The ratio of the size of these elements to the total image size forms the actual feature. It becomes problematic if several diagrams are contained and recognized as one by the algorithm. Since the smallest possible bounding box is determined, the area portion can have a large error. Also problematic are logos in the corners of the images, which makes the bounding box unnecessarily large if additional diagrams are included on the image. Colored diagrams are recognized worse due to the binary filter, if they are not sufficiently different from the background. This filter is necessary, however, because otherwise the kernel cannot recognize the horizontal structures. Overall, the diagram recognition works well enough to add a benefit to the project. However, the percentage value as a feature implies that a diagram that takes up the entire image is the best. Because of the described error, the feature value should be urgently 0 for a diagram image percentage of 100%. The optimal image proportion is assumed to be 80%. From these defaults, a function can be derived, which converts the image portion accordingly. For this purpose, a log-normal distribution was used. The variable ğ‘¥ is the determined value of the diagram recognition between 0 and 1. The value range of the function is also between 0 and 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Argument Standard Model</head><p>The first attempt was to build a formula with the normalized features, whose output is a numerical value that describes the argumentativeness of an image. For this purpose, an attempt was made to implement the assumptions made within the formula. A higher result should be associated with a higher argumentativeness.</p><formula xml:id="formula_1" coords="10,126.65,405.28,379.33,10.91">ğ‘ğ‘Ÿğ‘”ğ‘¢ğ‘šğ‘’ğ‘›ğ‘¡ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ = ğ›¼(ğ‘ğ‘œğ‘™ğ‘œğ‘Ÿğ‘†ğ‘ğ‘œğ‘Ÿğ‘’) + ğ›½(ğ‘¡ğ‘’ğ‘¥ğ‘¡ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’) + ğ›¾(ğ‘‘ğ‘–ğ‘ğ‘”ğ‘Ÿğ‘ğ‘šğ¹ ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’)<label>(2)</label></formula><p>The factors ğ›¼, ğ›½, ğ›¾ describe the influence of the individual scores on the argumentScore. They should lie between 0 and 1, whereby the optimal weighting is to be determined later by an evaluation. The colorScore implements the assumption that light and green colors are more likely to be found on positive images, and dark and red colors on negative images. However, this only applies to photographs, as cliparts contradict the assumption due to their often white background. For this reason, the colorScore is included when calculating the image-type. Since the stance of the image does not matter in a discussion, the positive and negative assumptions do not need to be distinguished. Therefore, they are added in the formula.</p><p>if Image Type == 'photo':</p><formula xml:id="formula_2" coords="10,162.22,572.92,343.76,24.43">ğ‘ğ‘œğ‘™ğ‘œğ‘Ÿğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ = ğœ(%ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘› + %ğ‘Ÿğ‘’ğ‘‘) + 1 ğœ (%ğ‘ğ‘Ÿğ‘–ğ‘”â„ğ‘¡ + %ğ‘‘ğ‘ğ‘Ÿğ‘˜)<label>(3)</label></formula><p>ğœ is the weighting ratio between the color component (red, green) and the brightness (light, dark). In this work, a ğœ of 0.8 is used, which weights the colors significantly higher.</p><p>if Image Type == 'clipArt': The textScore describes the assumption that longer texts have a higher potential to be argumentative. Furthermore, the sentiment score is included.</p><formula xml:id="formula_3" coords="10,222.80,669.58,283.19,10.91">ğ‘ğ‘œğ‘™ğ‘œğ‘Ÿğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ = (%ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘› + %ğ‘Ÿğ‘’ğ‘‘)<label>(4)</label></formula><formula xml:id="formula_4" coords="11,197.46,404.72,308.52,10.91">ğ‘¡ğ‘’ğ‘¥ğ‘¡ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ = ğ‘¡ğ‘’ğ‘¥ğ‘¡ğ¿ğ‘’ğ‘›ğ‘”ğ‘¡â„ â€¢ |ğ‘¡ğ‘’ğ‘¥ğ‘¡ğ‘†ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘šğ‘’ğ‘›ğ‘¡|<label>(5)</label></formula><p>The formula thus formed for calculating the argumentScore evaluates images in the value range 0 to 3, where each term can become a maximum of 1.</p><p>This formula is based on many assumptions and theories that could not be substantiated in this work due to time constraints. This would require an extensive analysis on a data set labeled in this regard. For this reason, another approach was tested which is based on a neural network and is described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Argument NeuralNet</head><p>The goal in using a neural network is to not have to set the parameters ğ›¼, ğ›½, and ğ›¾ yourself.</p><p>In addition, the network should ignore possibly incorrect assumptions, such as the color assumptions on argumentativeness. It first needs a topology that allows all features to be reasonably available to the network.</p><p>The simplest variant is to use a fully-connected network, where all neurons of one layer are connected to the neurons of the next layer. The relu function serves as an activation function of the neurons. However, it is not recommended to put all features into the input layer. Although they are already all normalized, the color features receive a presumably higher importance  <ref type="figure" coords="12,120.36,432.57,3.57,10.91" target="#fig_4">4</ref>). The evaluated images in the data set serve as training data, where an evaluation with strong is mapped to a value of 1 and an evaluation with weak was mapped to 0.5. The rating scale is defined in more detail in chapter 6.1. To prevent overfitting, an automatic stopping was used that terminates the training as soon as the validation accuracy stops increasing within 10 epochs.</p><p>Since in summary 10 features are given to the network, it is advisable to evaluate which features actually add value and which do not. Each feature is based on an assumption for the detection of argumentativeness. However, since these are not proven, the evaluation is at the same time a hypothesis testing. The features were evaluated by training a baseline model. This contains all the features presented. Subsequently, each feature was omitted once in the training. The average of accuracy was calculated from 10 trained and evaluated models. The results are shown in Figure <ref type="figure" coords="12,467.85,595.16,3.81,10.91" target="#fig_5">5</ref>. It can be seen that the network relies heavily on the textLengths feature, as the accuracy decreases when the feature is left out. Furthermore, it can be seen that the accuracies increase when Percentage blue&amp;yellow, Image Type and Text Position are excluded during training. In terms of assumptions, this means that argumentativeness does not depend on the yellow and blue content. The same is true for the text position on the image. The argumentativeness of an image seems to be relatively strongly dependent on the text length found on the image. The result from the evaluation is that Percentage blue&amp;yellow, Image Type and Text Position are no longer considered. It was also tried not to include Average Color, but the accuracy decreased in relation to the baseline when excluding all four features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Stance Model</head><p>Even though the focus of this work is on the recognition of argumentativeness, an attempt was made to recognize the expression of stance relevance within the discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Stance detection features</head><p>An image can be supportive, neutral or rejective towards a question. To recognize this, information about the question itself is required. Since it is difficult to make the content of the question understandable to an algorithm, the following features try to establish a connection between image text and query. However, on many images no text is recognized, or they do not possess any, whereby a comparison is not possible and an attitude concerning the question cannot be recognized. For this reason, the HTML page of the image was included. There, the surrounding text was extracted and special emphasis was placed on captions. Thus, HTML text and image text can be compared with the query regarding features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Equality</head><p>The Query Equality describes the equality between the query and the image or HTML text. This is based on the assumption that if the query itself, or parts of it, are found in the text, there is a high correlation between them. This is implemented by a simple term index, for which the occurrences of the preprocessed query terms in the text are counted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Alignment</head><p>Similar to the Query Equality, the Query Alignment also searches for matches between query and text. The Needleman-Wunsch algorithm is used to find optimal alignments <ref type="bibr" coords="13,436.83,479.85,16.08,10.91" target="#b25">[26]</ref>. Due to the computation time and the sometimes very long HTML texts, the feature was only computed between query and image text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Context</head><p>However, finding query terms in the text is not enough to make statements about the attitude of the text. Negations, which can reverse statements using a word, pose a major problem. The Query Context feature looks at query term occurrences in the text and evaluates the surrounding words in a ğœ environment with respect to the sentiment score. ğœ is to be chosen depending on the text at hand. Since in this work the text was preprocessed, ğœ is chosen quite low, since filler and stop words are no longer included. Recommended is ğœ = 6. The assumption underlying the feature is that if a negative sentiment score is found around query term occurrences, there is also a negative correlation between the text excerpt and the query term. For each occurrence in the text, the sentiment score is determined and the average is calculated at the end. This results in a total of 5 features, since Query Equality and Query Context are each calculated between query and either image text or HTML text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Stance NeuralNet</head><p>Similar to the argument model, an attempt was made to determine the attitude of an image with respect to a search query by means of a formula. This formula included the described features from section 5.1, but also some features from the standard argument model. However, the accuracy of the used formula and a random assignment couldn't be distinguished. For this reason, a neural network was also trained, which has similarities to the argument model. It uses the stance detection features from section 5.1 and some argument model features. The stance NeuralNet model can be seen in Figure <ref type="figure" coords="14,296.03,502.52,3.74,10.91" target="#fig_6">6</ref>.</p><p>The three classes have different occurrences in the data set. Because of this, they were weighted differently in the training. The weight of the pro and con classes is the number of neutral images divided by the number of pro/con images. The neutral class has a weight of 1. These weights ensure that the network don't prefer the neutral class due to its frequency.</p><p>It should be emphasized that unlike the argument model, no continuous value is to be predicted. The stance model functions as a classifier. Each output neuron represents one of the three stance expressions of the image "con", "neutral" and "pro". During training, the information is converted into binary vectors (0, 0, 1) for "pro", (0, 1, 0) for "neutral", and (1, 0, 0) for "con". In prediction, the largest value of the three neurons is interpreted as the predicted class. An evaluation of the features can also be made for this network. Only the sentiment score of the query does seem to be disadvantageous for the network. The accuracy in relation to the baseline when excluding this feature increases significantly (to be seen in Figure <ref type="figure" coords="15,411.45,378.38,3.59,10.91" target="#fig_7">7</ref>). Consequently, the feature is not further used in the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Evaluation</head><p>Subsequent to the presentation of the various models and features, these should now be evaluated. For this purpose, the labeling process and the resulting outcomes will be presented. In connection with this, both the argument model and the stance model will be evaluated and their performance is to be determined using suitable metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Data Set</head><p>This work is based on a data set with a total of over 23,000 images, divided into 50 different topics. The data set contains both the images and the HTML text in which each image appears, as well as some additional information. In order to be able to train models on this data, the data must first be labeled. For texts, there are ready-made platforms for semantic annotation, such as the INCEpTION <ref type="bibr" coords="15,173.87,595.12,17.79,10.91" target="#b26">[27]</ref> platform. In order to also label images in terms of their topic relevance, argumentativeness, and stance relevance, a separate web frontend was first developed, as can be seen in Figure <ref type="figure" coords="15,168.83,622.22,3.80,10.91" target="#fig_8">8</ref>. This allowed a large portion of the data set to be annotated as efficiently as possible. The recognizable tripartition of the labels refers to the considerations of Kiesel et al. <ref type="bibr" coords="15,102.78,649.32,16.27,10.91" target="#b10">[11]</ref>, whereas the different expressions were adapted to the questions and problems of this work. Below is an explanation of the different labels <ref type="bibr" coords="16,295.69,307.25,16.39,10.91" target="#b10">[11]</ref>:</p><p>â€¢ Topic -True: From the image (recognizable content) you can see the topic.</p><p>-False: From the image (recognizable content) you can not see the topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¢ Argumentativeness</head><p>-None: There is no argument recognizable in the image that argues for any position in a topic. If the image does not belong to the topic, the argumentativeness must be set to "none". -Weak: Few arguments are recognizable in the image or/and the arguments are not clear. -Strong: Several arguments are recognizable in the image or/and a clear stance is recognizable in each argument.</p><p>â€¢ Stance -Pro: A clear attitude can be seen throughout the image, which supports the thesis of the topic. -Neutral: The picture is not argumentative or arguments are made in equal measure for and against a topic. If the image does not belong to the topic, the stance must be "neutral". -Con: A clear attitude can be seen throughout the picture, which attacks the thesis of the topic.</p><p>When assigning labels, it is important to note that images that are not topic relevant (topic = false) cannot be argumentative and are assigned a neutral stance. The underlying assumption is that both argumentativeness and stance relevance are directly dependent on topic relevance.</p><p>For example, an image is labeled "neutral" in terms of stance relevance if the topic reference is missing, even if the image could be "pro" or "con" to another topic. With regard to argumentativeness, a distinction is made between strongly and weakly argumentative images. This is intended to train the model in such a way that strongly argumentative images get a higher score and can therefore ideally be shown first by the search engine. In the further course, a more precise distinction is made in the evaluation with the addition "strong", which means that only strongly argumentative images are considered here.</p><p>A total of almost 10,000 images were annotated through the labeling process. There are clear differences between the various topics in the distribution of the label characteristics. Table <ref type="table" coords="17,501.08,208.91,5.16,10.91" target="#tab_0">1</ref> shows the label results averaged across all annotated images. First of all, it can be deduced from the results that only 72% of the images in the data set that have already been assigned to a topic are actually topic relevant. Since argumentative images must also be topic relevant, the proportion of argumentative images is obviously lower at 46%. However, a clear difference can also be seen with the strongly argumentative images. With 14% of the images, these are represented rather rarely in the data set. A clear stance relevance is still recognisable in 34% of the images, whereby "con" images are represented less frequently in the data set with 14% compared to "pro" images with 20%. In conclusion, 34% of the images are topic relevant, argumentative as well as stance-relevant and are therefore relevant for the search engine. When considering only the strongly argumentative images here, the proportion is reduced to 13%. Even more serious are the differences between the various topics. For example, in the topic "Should the penny stay in circulation?" only 9% of the images are argumentative and only 2% of the images have a stance relevance of the type "pro" (see Table <ref type="table" coords="17,390.02,385.05,5.17,10.91">8</ref> in the appendix). These outliers mean that the retrieval system can only find very few relevant images. In a rank-based evaluation, the Precision@k evaluates the performance of the model for the best k results. If k is chosen to be greater than the number of relevant images contained, the performance of the model is underestimated. This is already the case from a k of 20 for some topics. The analysis shows the ratio of the number of images that fulfil the respective property to the total number of images. All 20 labeled topics are taken into account. The addition of "strong" to the argumentative and relevant images means that only the strong argumentative images are taken into account, whereas without the addition both strong and weak argumentative images are meant. Relevant images are all images that show a topic relevance, are argumentative and have a stance relevance with the characteristic "pro" or "con". In the best case, these should be displayed by the search engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Evaluation Argument Model</head><p>In the following, the argument model will be evaluated. A differentiation is made between the presented standard and NeuralNet model. All calculated and presented values were cross-validated and averaged from multiple runs to obtain representative data.</p><p>For the training of the model, topics are needed that contain enough relevant images. Based on the analysis of the labels, skip topics were defined, which are ignored for the training of the models and partly also for the subsequent testing. The analysis of the data set has shown that some topics (skip topics) lead to a degradation of the model performance when they are used. This is due to the fact that they contain too few argumentative images for the network to train. However, there are two separate skip lists for the argument model and the stance model. In the argument model, all topics that do not have at least 20 strong argumentative images are added to the list and later ignored during training. The stance skip list contains topics that do not have at least 20 "pro" and 20 "con" images. The choice of the value 20 is taken into account in the following subsections for calculating the model performance.</p><p>There are two topics in the argument skip list. If the images of these topics are removed from the data, the remaining data is called valid. For the evaluation of the standard model, a separate examination and evaluation takes place with all and the valid data. A Precision@20 is calculated in each case. Strong@20 evaluates how many of the best 20 search results were actually labeled as strong. Since the valid data for each topic contains at least 20 strongly argumentative images in the data set, in the optimal case these images would also be at the top of the ranking. For Both@20, both strong and weak argumentative images are considered correct and are not distinguished.</p><formula xml:id="formula_5" coords="18,222.91,415.53,283.08,24.43">ğ‘ƒ ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›@ğ‘˜ = |{ğ‘Ÿ âˆˆ ğ¼|ğ‘Ÿ â‰¤ ğ‘˜}| |ğ¼|<label>(6)</label></formula><p>The Precision@k describes according to Berrendorf et al. <ref type="bibr" coords="18,338.20,444.92,17.75,10.91" target="#b27">[28]</ref> the proportion of hits or instances for which the true entity appears among the first ğ‘˜ entities in a sorted list. The ğ‘Ÿ represents the respective rank from the set of individual rank scores ğ¼. The presented model assumes that the inputs are topic relevant. Due to the fact that there is no further check for this, images without topic relevance can also be found in the output. Therefore, only the first k topic relevant images are considered when calculating the Precision@k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1.">Argument Standard Model</head><p>The results from table <ref type="table" coords="18,187.21,574.74,4.97,10.91">2</ref> show that the model performance decreases slightly when all topics are considered. This is because the topics in the skip list do not contain 20 strongly argumentative images, and the Precision@k for these topics can never become 1. With a precision Both@20 of 0.875, this means that among the 20 top-rated images, on average 17.5 images are actually strongly or weakly argumentative. The other predicted images were classified as "none" in the labeling process. The predictions can thus be considered acceptable, but not particularly good, since the precision decreases especially with increasing k.</p><p>Topics Strong@20 Both@20 all 0.5225 0.8475 valid 0.5472 0.8750</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Results of the argument standard model: To calculate the results, the features described in chapter 4.2 are included in the model with equal weighting. Strong@20 means Precision@k with k=20, whereby only the strongly argumentative images are considered. With Both@20, both strong and weak argumentative images are considered. With topics "all" all labeled topics are used, while with topics "valid" all topics are used excluding the previously defined skip topics. Figure <ref type="figure" coords="19,121.29,445.87,5.17,10.91" target="#fig_9">9</ref> shows the scores assigned by the standard argument model for two representative topics in the form of histograms. Ideally, strongly argumentative images should be found in the right part of each plot (green), weakly argumentative images in the middle part (red) and non-argumentative images in the left part (blue). Three distributions would thus be visible, which are spatially separated from each other on the x-axis. The argument standard model does not produce such clearly independent distributions. The right-skewed distributions produce good Precision@20 values, but this performance drops sharply as k is increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2.">Argument NeuralNet</head><p>For the NeuralNet model, data splits for training and testing must be defined. This is realised in two different ways. On the one hand, there is a split at the image level, where all labeled topics are taken into account and the images of all topics are divided into 2/3 training data and 1/3 test data. This split allows the model to learn the characteristics of each topic. In order to investigate possible overfitting to the given topics, another split is made at topic level. Here, individual topics are left out from training and only used for testing. These test topics contain enough argumentative and strong argumentative images and are taken from the valid topics.</p><p>The retrieval system is set up in such a way that individual topics, but not individual images, can be processed in the evaluation. As a result, a correct evaluation on only the test data is not possible in case of a split on image level. Since the training data is also included in the evaluation data, overfitting is difficult to detect. For this reason, the ratio of images serving as train and test data was varied and different models were trained with them. The test data serves the neural network as a validation data set and, with the accuracy calculated on it, determines when the training is stopped in order to avoid overfitting. It was discovered, that already from a training share of the data of 10% no remarkable changes in the model performance can be recognized (For further results see figure <ref type="figure" coords="20,275.62,195.36,10.28,10.91" target="#fig_12">12</ref> in the appendix). Accordingly, this proportion is already sufficient for the model to learn the essential features of argumentative images. Since, in the worst case, overfitting occurs on the training data, and the test data is significantly worse predicted, there should be a change in overall precision when varying the split between the two. Since this is not the case, overfitting on the data can be excluded with sufficient certainty and an evaluation can also take place with training data contained in the test data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>Results of the argument NeuralNet model. The table shows the Precision@20 values determined on the one hand for exclusively strong argumentative images (Strong@20) and on the other hand for strong and weak argumentative images (Both@20). A distinction is made as to whether a data split was applied at image level or at topic level, and whether all topics, the valid topics or only the test topics were taken into account.</p><p>Table <ref type="table" coords="20,115.72,452.04,5.06,10.91">3</ref> shows the results for the NeuralNet model for both an image level and topic level data split. The column topics describes which data was used for the evaluation. The model for the data split at image and topic level was the same in each case and was trained with the valid data (skip topics excluded). As can already be seen with the standard model, the results are also slightly better here when only the valid topics are considered. Furthermore, it can be seen that the results of the data split at the image level are roughly comparable with those at the topic level. This indicates that no overfitting takes place and that the features of certain topics are not learned by heart. With the split at topic level, it can be seen that when predicting unknown data (topics = test), approximately the same precision is achieved as when predicting data that has already been partially seen in training (topics = all).</p><p>Compared to the standard model, the results of the NeuralNet model in Figure <ref type="figure" coords="20,447.72,601.08,10.35,10.91" target="#fig_10">10</ref> show that separate distributions can be seen for all three classes. The majority of the non-argumentative images are found on the left of the plots and the strongly argumentative images on the right. Thus, good Precision@k results can be expected even for larger k values. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Evaluation Stance Model</head><p>Analogous to the argument NeuralNet model, the stance NeuralNet model should now also be evaluated. For this purpose, all valid topics are first searched for, whereby seven topics are no longer considered. The metric accuracy is used for the stance NeuralNet model to measure the performance, since the images are to be classified into the classes "pro", "con" and "neutral".</p><formula xml:id="formula_6" coords="21,211.62,413.57,294.37,24.43">ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ = ğ‘‡ ğ‘ƒ + ğ‘‡ ğ‘ ğ‘‡ ğ‘ƒ + ğ‘‡ ğ‘ + ğ¹ ğ‘ƒ + ğ¹ ğ‘<label>(7)</label></formula><p>In this application, TP (True Positive) and TN (True Negative) represent the number of images that were correctly classified. The number of incorrectly classified images, FP (False Positive) and FN (False Negative), are added to the denominator of the sum, which means that the total number of all classified images is used here. The quotient of both sums calculates the accuracy. Similar conclusions as for the argument models can be drawn from the results in table <ref type="table" coords="21,484.93,638.09,5.17,10.91" target="#tab_2">4</ref> for the stance NeuralNet model. The model performs better with the split at image level if only valid topics are considered. This is not the case with the split at topic level. Here, the model is 6 percentage points better when all topics are considered than when predicting only unknown topics. This difference could be due to the variance of the trained models. However, it is also conceivable that the model increasingly classifies "neutral", which results in a higher accuracy for the non-valid topics, which achieves a noticeable difference on average across all topics. A slight overfitting would also be conceivable.</p><p>Figure <ref type="figure" coords="22,121.60,361.51,10.35,10.91" target="#fig_11">11</ref> shows the results of the out-of-sample prediction on the test data in the form of confusion matrices. Ideally, a dark blue diagonal line from top-left to bottom-right should be visible. This would be the case if the actual stance of the images corresponds to the majority of the predicted stance. This diagonal is not visible in the plots shown. Mostly one or two of the classes "pro", "con" or "neutral" are predicted well, but it is not possible for the neural network to predict all stances of a topic well. It can also be seen that "neutral" is classified more often, which can explain the differences in the table 4 at topic level. It can be seen that the stance model is not able to distinguish "pro" from "con". The features do not seem to make it possible to establish a connection between query and stance. The results are significantly worse than those of the argument model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Evaluation of the overall system</head><p>First of all, it should be noted once again that the focus of this work was on the argument model. No solution was found to improve the stance model and bring it to a similar quality. Since the overall system is based on both models, the results of the overall system are not satisfactory due to a poor stance model. Table <ref type="table" coords="22,115.75,587.37,5.06,10.91">5</ref> shows that the system offers on average only 5 strongly argumentative images among the first 20 images viewed for a search query, which were also assigned to the correct stance ("pro" or "con"). If the weakly argumentative ones are also evaluated, the value increases to 8 out of 20. A more precise evaluation of the overall system is considered to make little sense, since the main focus was on the argument model. The stance model is clearly the limiting factor for a better precision here.</p><p>Precision Strong@20 0.2510 Both@20 0.4030</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5</head><p>Results of the overall system consisting of the argument NeuralNet model and stance NeuralNet model trained with all topics and a data split at the image level. A Precision@20 was calculated for strong argumentative images only, as well as for strong and weak argumentative images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">External evaluation with Tira</head><p>In addition to our own evaluation, another external evaluation of the approaches via Tira took place. Tira is a software that tries to solve the problem of reproducibility of scientific work, especially for shared tasks <ref type="bibr" coords="23,206.07,245.94,16.09,10.91" target="#b28">[29]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 7</head><p>Results of the Tira runs compared to the Minsc baseline. The quality of topic relevance, argumentativeness and stance relevance is calculated with a Precision@10. To relate the values to the evaluation shown in the previous subsections, the argumentativeness and stance relevance have to be adjusted by dividing them with the topic precision. This is indicated by the addition of "adj".</p><p>To measure the performance, a Precision@10 was calculated for topic relevance, argumentativeness, and stance relevance. Comparing only these precision values with the baseline, they are significantly lower than the precision values for the argument and stance model from the previous subsections. This is mainly due to the fact that the topic model was not evaluated for this work and therefore only topic relevant images were used for the evaluation of the argument and stance model. For better comparability, the Tira Precision@10 values for the argument and stance model were additionally adjusted by dividing the respective value by the topic precision. This is possible because there is a continuous dependency between the values. This means that an image can only be argumentative if it is also topic relevant and an image can only have a positive or negative stance value if it is also argumentative.</p><p>If we now look at the adjusted values, we can see, as in the previous evaluation, that the NeuralNet argument model perform better than the standard argument model, although the deviations between the different runs are small. Furthermore, the four runs with the adjusted precision values achieve about the same high performance as the Minsc baseline. The opposite is evident in the stance models. Here, the standard stance model performs better than the NeuralNet, although the precision can generally be rated as low. Thus, the Tira results also confirm the conclusions of the evaluations in the previous subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>The work has shown that it is in principle possible to recognise argumentative images. Two feature-based approaches were tested. Both involve information derived exclusively from images. These include colour, image text and structural features such as the recognition of diagrams. The standard argument model, which is based on a formula, achieves worse results in this work than a trained neural network with the same features. This is because too many assumptions have to be made in a single formula to accommodate the complexity of an image. The interaction of colours and text cannot be considered because each feature is added up individually and as part of the formula. The neural network, on the other hand, achieves remarkable results and can deliver on average more than 17 matching images among the first 20 results for a search query. Among them, more than 10 images are even strongly argumentative. As the number of search results considered increases, the neural network also performs significantly better than the standard model. It has been shown that not all information on the images is equally important for the network. The presence of text seems to be particularly decisive for an argumentative image. However, this should be seen as positive, since according to the considerations in chapter 2.2, an image can only be argumentative in interaction with text or diagrams. The word count of the recognised text is the most important feature for the argument model. However, this feature is based on the unreliable text recognition of Tesseract, which can neither recognise handwriting nor low-contrast writing. Perhaps an improvement of this optical character recognition technique would also lead to an improvement of the model. In further work, other features could be integrated and examined for their usefulness. It might be conceivable to recognise simple symbols, which could indicate an argumentative character.</p><p>As a result of this work, in addition to the argument model, the analysis and labeling of the data set provided for the TouchÃ© Lab task should be mentioned. There it was shown that many topics are not suitable for evaluating an argument search engine. Too few images are actually argumentative (46%). Furthermore, 28% of the images that were indicated as topic relevant are not topic relevant. With actually only 13% strongly argumentative and at the same time topic relevant images, the data set is only of very limited use for a good training of a neural network and for evaluation for this task. A better data set could possibly lead to better results here as well.</p><p>Even though it was not the focus of this work, some attempts were made to create a stance model. However, only slightly less than 50% of the images are correctly classified as "pro", "neutral" and "con". This is probably due to the distinction between "pro" and "con". The neural network was not able to establish the connection between the given query and the image. Even after including the HTML pages of the images, the performance improved only slightly. This classification requires a deep understanding of the question, including negations and other rhetorical devices. For this reason, the overall system cannot achieve good results. On average, only 5 of the first 20 images in a search query are strongly argumentative and assigned to the correct side ("pro" or "con") and thus of great interest to the user. Improvements must therefore be made primarily to the stance model for a good overall system. One possibility would be to use a language model such as BERT <ref type="bibr" coords="25,309.11,222.46,16.38,10.91" target="#b29">[30]</ref>. This might make it possible to process the complexities of the language and establish a connection between query and text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Reproducibility</head><p>Our complete code basis and our results of the labeling process can be found in our GitLab repository <ref type="foot" coords="28,137.37,123.07,3.71,7.97" target="#foot_0">1</ref> . We used the dataset from the TouchÃ© 2022 Task 3: Image Retrieval for Arguments. It can be found on the official task website 2 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Complete analysis of the labeled data set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 8</head><p>Results of the analysis of the data set for all labeled topics. The addition "strong" means that only strong argumentative images are used here, while otherwise strong and weak argumentative images are considered. Relevant images are images which are topic relevant, argumentative as well as stancerelevant. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,89.29,255.28,408.49,8.93;2,89.29,267.23,189.50,8.96;2,172.63,84.19,250.00,164.31"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Accident at the finish sprint at a professional bike race. Six drivers were seriously injured.Source: Tomasz Markowski/Associated Press<ref type="bibr" coords="2,266.97,267.28,11.83,8.87" target="#b7">[8]</ref> </figDesc><graphic coords="2,172.63,84.19,250.00,164.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,89.29,242.61,416.70,8.93;5,89.29,254.62,416.70,8.87;5,89.29,266.57,416.69,8.87;5,88.93,278.53,117.57,8.87;5,130.96,84.19,333.34,151.00"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of the retrieval system with the sketched sequence of the image search via the IDs. Argument and stance models evaluate the images independently. After combining the scores, the pictures are sorted accordingly. The images with the highest score can be considered as pro, and those with the lowest score as con.</figDesc><graphic coords="5,130.96,84.19,333.34,151.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,248.04,342.81,155.57,10.91;7,107.28,357.71,159.53,10.91;7,107.28,372.62,163.41,10.91;7,107.28,387.52,162.22,10.91"><head></head><label></label><figDesc>) and (160, 50, 80) to (255, 255, 255) â€¢ Green: (36, 50, 80) to (70, 255, 255) â€¢ Blue: (100, 50, 80) to (130, 255, 255) â€¢ Yellow: (20, 50, 80) to (36, 255, 255)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,89.29,300.16,309.95,8.93;9,88.99,312.17,416.99,8.87;9,89.29,324.12,196.39,8.87;9,110.13,84.19,375.02,208.55"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (left) Text Area calculated based on the area occupied by the text. (right) Text Position calculated as an example of a heat map over a 4x4 grid. In the work, a 8x8 grid was used. A red color represents a high text content.</figDesc><graphic coords="9,110.13,84.19,375.02,208.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="11,89.29,329.06,292.53,8.93;11,110.13,84.19,375.01,232.31"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Topology of the used neural network for argument detection.</figDesc><graphic coords="11,110.13,84.19,375.01,232.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="12,89.29,290.68,417.05,8.93;12,88.93,302.69,418.59,8.87;12,89.29,314.64,154.21,8.87;12,110.13,84.19,375.01,199.07"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Evaluation of the significance of the input features. If the accuracy of the network is low without a certain feature, this feature has high influence on the predictive performance of the network. For the baseline, all features are used.</figDesc><graphic coords="12,110.13,84.19,375.01,199.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="14,89.29,306.79,277.19,8.93;14,110.13,84.19,375.02,210.04"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Topology of the neural network for stance determination.</figDesc><graphic coords="14,110.13,84.19,375.02,210.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="15,89.29,290.68,417.05,8.93;15,88.93,302.69,417.23,8.87;15,89.29,314.64,138.18,8.87;15,110.13,84.19,375.01,199.07"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Evaluation of the significance of the input features. If the accuracy of the network is low without a certain feature, this feature has high influence on the predictive power of the network. For the baseline, all features are used.</figDesc><graphic coords="15,110.13,84.19,375.01,199.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="16,89.29,247.96,416.70,8.93;16,89.29,259.97,416.87,8.87;16,89.29,271.92,69.34,8.87;16,110.13,84.19,374.99,156.34"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: HTML frontend for labeling the images of a selected topic. Different topics can be labeled by several users at the same time in terms of their topic relevance, their argumentativeness and their stance relevance.</figDesc><graphic coords="16,110.13,84.19,374.99,156.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="19,89.29,360.44,416.69,8.93;19,89.29,372.45,416.70,8.87;19,89.29,384.40,416.94,8.87;19,89.29,396.36,416.70,8.87;19,89.29,408.31,350.28,8.87;19,89.29,212.38,416.70,140.64"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Histograms for the argument standard model over a selected sample of valid topics with the distributions of the scores assigned by the model and the calculated Precision@20 values. PBoth@20 shows the determined precision for weakly and strongly argumentative images. With PStrong@20, only strongly argumentative images are considered. ImageCount shows the absolute number of images that are available in the data set for the category ("Strong" or "Both") under consideration.</figDesc><graphic coords="19,89.29,212.38,416.70,140.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="21,89.29,232.25,416.69,8.93;21,89.29,244.25,416.70,8.87;21,89.29,256.21,416.69,8.87;21,89.29,268.16,416.94,8.87;21,89.29,280.12,416.70,8.87;21,89.29,292.07,335.02,8.87;21,89.29,84.19,416.70,140.64"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Histograms for the argument NeuralNet model over a selected section of valid topics with the distributions of the scores assigned by the model and the calculated Precision@20 values. The topics shown were not trained by the model, but are used exclusively as test data. PBoth@20 shows the determined precision for weakly and strongly argumentative images. With PStrong@20, only strongly argumentative images are considered. ImageCount shows the absolute number of images that are available in the data set for the category ("Strong" or "Both") under consideration.</figDesc><graphic coords="21,89.29,84.19,416.70,140.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="22,89.29,232.25,416.70,8.93;22,88.93,244.25,417.05,8.87;22,89.29,256.21,179.35,8.87;22,89.29,84.19,416.70,140.64"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Confusion matrices for the stance NeuralNet model over a selected section of valid topics with the predicted classes and the calculated accuracy values. The topics shown were not trained by the model, but are used exclusively as test data.</figDesc><graphic coords="22,89.29,84.19,416.70,140.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="30,89.29,353.92,416.69,8.93;30,89.06,365.92,416.93,8.87;30,89.29,377.88,416.70,8.87;30,89.29,389.83,268.63,8.87;30,110.13,121.23,375.00,225.26"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Precision@20 values for the argument NeuralNet model with image level data split, with the x-axis showing the proportion of training data. All data points represent an arithmetic mean over the results of 10 trained models. It can be seen that already from a training share of the data of 10% no remarkable changes in the model performance can be recognized.</figDesc><graphic coords="30,110.13,121.23,375.00,225.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="17,88.99,463.79,317.61,134.34"><head>Table 1</head><label>1</label><figDesc></figDesc><table coords="17,188.67,463.79,217.93,116.86"><row><cell>Category</cell><cell>Percentage in data set</cell></row><row><cell>Topic Relevance</cell><cell>72%</cell></row><row><cell>Argumentativeness</cell><cell>46%</cell></row><row><cell>Argumentativeness (Strong)</cell><cell>14%</cell></row><row><cell>Stance Relevance</cell><cell>34%</cell></row><row><cell>Stance Pro</cell><cell>20%</cell></row><row><cell>Stance Con</cell><cell>14%</cell></row><row><cell>Stance Neutral</cell><cell>66%</cell></row><row><cell>Relevant Images</cell><cell>34%</cell></row><row><cell>Relevant Images (Strong)</cell><cell>13%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="21,88.99,508.93,417.00,110.42"><head>Table 4</head><label>4</label><figDesc>Results of the stance NeuralNet model. The table shows the calculated accuracy values. A distinction is made as to whether a data split was applied at image level or at topic level, and whether all topics, the valid topics or only the test topics were taken into account.</figDesc><table coords="21,229.14,508.93,137.01,57.09"><row><cell>Data Split</cell><cell cols="2">Topics Accuracy</cell></row><row><cell>Image Level</cell><cell>all</cell><cell>0.4723</cell></row><row><cell>Image Level</cell><cell>valid</cell><cell>0.4961</cell></row><row><cell>Topic Level</cell><cell>all</cell><cell>0.4812</cell></row><row><cell>Topic Level</cell><cell>test</cell><cell>0.4215</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="23,88.89,245.94,417.76,244.87"><head>Table 6</head><label>6</label><figDesc>Table 6 and table 7 show the results for the submitted runs. The four runs each come from combining one of the two argument models (NeuralNet or standard) with one of the two stance models (NeuralNet or standard). The table shows the Tira timestamps and the argument and stance models (standard or NeuralNet) used in each case.</figDesc><table coords="23,122.94,296.74,349.39,194.07"><row><cell>ID</cell><cell cols="2">Tira Timestamp</cell><cell></cell><cell></cell><cell>Models used</cell></row><row><cell>1</cell><cell cols="6">2022-02-25-11-07-15 NeuralNet argument model and NeuralNet stance model</cell></row><row><cell>2</cell><cell cols="2">2022-02-25-11-49-41</cell><cell cols="4">NeuralNet argument model and standard stance model</cell></row><row><cell>3</cell><cell cols="2">2022-02-25-19-11-54</cell><cell cols="4">standard argument model and NeuralNet stance model</cell></row><row><cell>4</cell><cell cols="2">2022-02-25-09-41-56</cell><cell cols="4">standard argument model and standard stance model</cell></row><row><cell></cell><cell>ID</cell><cell cols="5">Topic Argument Stance Argument (adj) Stance (adj)</cell></row><row><cell cols="3">Minsc -Baseline 0.736</cell><cell>0.686</cell><cell>0.407</cell><cell>0.932</cell><cell>0.553</cell></row><row><cell></cell><cell>1</cell><cell>0.673</cell><cell>0.624</cell><cell>0.354</cell><cell>0.927</cell><cell>0.526</cell></row><row><cell></cell><cell>2</cell><cell>0.687</cell><cell>0.632</cell><cell>0.365</cell><cell>0.920</cell><cell>0.531</cell></row><row><cell></cell><cell>3</cell><cell>0.664</cell><cell>0.609</cell><cell>0.344</cell><cell>0.917</cell><cell>0.518</cell></row><row><cell></cell><cell>4</cell><cell>0.701</cell><cell>0.634</cell><cell>0.381</cell><cell>0.904</cell><cell>0.544</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="28,108.93,660.07,283.90,8.97"><p>https://git.informatik.uni-leipzig.de/jb64vyso/aramis-image-argument-search</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank all the people who helped with the evaluation. Without them, we would not have been able to use nearly 10,000 labeled images as a data set. We would like to thank <rs type="person">Lena</rs>, <rs type="person">Yasmin</rs>, <rs type="person">SÃ¶ren</rs> and <rs type="person">Roman.</rs> Additionally, we would like to thank <rs type="person">Theresa Elstner</rs> for the detailed and fast review of our paper.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Should the voting age be lowered?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 9</head><p>Assignment of topic numbers to topic names for all labeled topics.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="25,112.66,421.01,393.33,10.91;25,112.66,434.55,395.17,10.91;25,112.66,448.10,394.04,10.91;25,112.66,461.65,279.36,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="25,431.16,421.01,74.83,10.91;25,112.66,434.55,189.57,10.91">Data acquisition for argument search: The args.me corpus</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-30179-8_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-30179-8_4.doi:10.1007/978-3-030-30179-8_4" />
	</analytic>
	<monogr>
		<title level="m" coord="25,330.87,434.55,176.97,10.91;25,112.66,448.10,23.87,10.91">KI 2019: Advances in Artificial Intelligence</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="48" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,475.20,393.32,10.91;25,112.66,488.75,394.62,10.91;25,112.31,502.30,132.63,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="25,347.73,475.20,158.25,10.91;25,112.66,488.75,172.97,10.91">Towards an argumentative content search engine using weak supervision</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gretz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Aharonov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/C18-1176/" />
	</analytic>
	<monogr>
		<title level="m" coord="25,312.97,488.75,36.02,10.91">COLING</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2066" to="2081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,515.85,394.53,10.91;25,112.28,529.40,393.71,10.91;25,112.66,542.95,393.33,10.91;25,112.66,556.50,394.53,10.91;25,112.66,570.05,394.51,10.91;25,112.66,586.04,50.37,7.90" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="25,112.28,529.40,288.95,10.91">ArgumenText: Searching for arguments in heterogeneous sources</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Stab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Daxenberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Stahlhut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tauchmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Eger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-5005</idno>
		<ptr target="https://aclanthology.org/N18-5005.doi:10.18653/v1/N18-5005" />
	</analytic>
	<monogr>
		<title level="m" coord="25,424.06,529.40,81.93,10.91;25,112.66,542.95,393.33,10.91;25,112.66,556.50,325.24,10.91">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, Association for Computational Linguistics</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, Association for Computational Linguistics<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,597.15,394.53,10.91;25,112.30,610.69,395.37,10.91;25,112.66,624.24,333.37,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="25,261.71,610.69,214.17,10.91">Building an argument search engine for the web</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">A</forename><surname>Khatib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Puschmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dorsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Morari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w17-5106</idno>
		<ptr target="https://doi.org/10.18653/v1/w17-5106.doi:10.18653/v1/w17-5106" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,112.66,637.79,393.33,10.91;25,112.66,651.34,395.01,10.91;25,112.66,664.89,174.74,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="25,267.42,637.79,238.57,10.91;25,112.66,651.34,23.48,10.91">Why images cannot be arguments, but moving ones might</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Champagne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A.-V</forename><surname>Pietarinen</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10503-019-09484-0</idno>
		<ptr target="https://doi.org/10.1007/s10503-019-09484-0.doi:10.1007/s10503-019-09484-0" />
	</analytic>
	<monogr>
		<title level="j" coord="25,143.87,651.34,67.17,10.91">Argumentation</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="207" to="236" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,86.97,393.32,10.91;26,112.66,100.52,394.62,10.91;26,112.31,114.06,335.01,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="26,177.24,86.97,328.75,10.91;26,112.66,100.52,166.90,10.91">The rhetoric of thick representation: How pictures render the importance and strength of an argument salient</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">E</forename><surname>Kjeldsen</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10503-014-9342-2</idno>
		<ptr target="https://doi.org/10.1007/s10503-014-9342-2.doi:10.1007/s10503-014-9342-2" />
	</analytic>
	<monogr>
		<title level="j" coord="26,292.25,100.52,69.91,10.91">Argumentation</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="197" to="215" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,127.61,70.27,10.91;26,216.32,127.61,291.51,10.91;26,112.66,141.16,64.35,10.91;26,195.11,141.16,29.78,10.91;26,242.99,141.16,22.39,10.91;26,283.49,141.16,223.21,10.91;26,112.66,154.71,287.35,10.91;26,442.82,154.71,64.36,10.91;26,112.66,170.70,104.78,7.90" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bundestag</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10503-014-9342-2</idno>
		<ptr target="https://www.bundestag.de/resource/blob/511122/8ae51b807ef2d0ebd58e4f4747c4bee7/wd-5-024-17-pdf-data.pdf" />
		<title level="m" coord="26,216.32,127.61,291.51,10.91;26,112.66,141.16,64.35,10.91">Wirksamkeit von bildlichen warnhinweisen auf zigarettenpackungen</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,181.81,394.62,10.91;26,112.31,195.36,347.69,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="26,158.63,181.81,263.32,10.91">How a horrifying cycling crash set up a battle over safety</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Busca</surname></persName>
		</author>
		<ptr target="https://www.nytimes.com/2021/01/30/sports/cycling/riders-crashes-uci-safety.html" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,208.91,394.53,10.91;26,112.66,222.46,393.33,10.91;26,112.66,236.01,393.33,10.91;26,112.66,249.56,394.61,10.91;26,112.66,263.11,305.42,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="26,112.66,222.46,314.54,10.91">Computational argumentation quality assessment in natural language</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Naderi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bilu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Thijm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/e17-1017</idno>
		<ptr target="https://doi.org/10.18653/v1/e17-1017.doi:10.18653/v1/e17-1017" />
	</analytic>
	<monogr>
		<title level="m" coord="26,450.78,222.46,55.20,10.91;26,112.66,236.01,393.33,10.91;26,112.66,249.56,45.81,10.91">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct coords="26,112.66,276.66,394.53,10.91;26,112.66,290.20,393.33,10.91;26,112.66,303.75,394.62,10.91;26,112.66,317.30,337.56,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="26,167.84,290.20,77.43,10.91">Argument search</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Euchner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>HeilenkÃ¶tter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Weidmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331327</idno>
		<ptr target="https://doi.org/10.1145/3331184.3331327.doi:10.1145/3331184.3331327" />
	</analytic>
	<monogr>
		<title level="m" coord="26,274.02,290.20,231.97,10.91;26,112.66,303.75,308.07,10.91">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,330.85,393.33,10.91;26,112.66,344.40,394.53,10.91;26,112.28,357.95,395.39,10.91;26,112.66,371.50,265.65,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="26,339.01,330.85,166.98,10.91;26,112.66,344.40,130.10,10.91">Image retrieval for arguments using stance-aware query expansion</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reichenbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.argmining-1.4</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.argmining-1.4.doi:10.18653/v1/2021.argmining-1.4" />
	</analytic>
	<monogr>
		<title level="m" coord="26,265.19,344.40,242.00,10.91;26,112.28,357.95,189.53,10.91">Proceedings of the 8th Workshop on Argument Mining, Association for Computational Linguistics</title>
		<meeting>the 8th Workshop on Argument Mining, Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,385.05,394.53,10.91;26,112.33,398.60,394.85,10.91;26,112.66,412.15,394.04,10.91;26,112.66,425.70,202.09,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="26,155.85,398.60,346.82,10.91">Content-based image retrieval and feature extraction: A comprehensive review</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rasheed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Sajid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">I</forename><surname>Ratyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Zafar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">H</forename><surname>Dar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sajid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Khalil</surname></persName>
		</author>
		<idno type="DOI">10.1155/2019/9658350</idno>
		<ptr target="https://doi.org/10.1155/2019/9658350.doi:10.1155/2019/9658350" />
	</analytic>
	<monogr>
		<title level="j" coord="26,112.66,412.15,178.31,10.91">Mathematical Problems in Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,439.25,393.53,10.91;26,112.66,452.79,394.53,10.91;26,112.66,466.34,359.68,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="26,276.53,439.25,229.66,10.91;26,112.66,452.79,41.65,10.91">Image retrieval based on MPEG-7 dominant color descriptor</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/icycs.2008.89</idno>
		<ptr target="https://doi.org/10.1109/icycs.2008.89.doi:10.1109/icycs.2008.89" />
	</analytic>
	<monogr>
		<title level="m" coord="26,198.45,452.79,278.90,10.91">The 9th International Conference for Young Computer Scientists</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,479.89,393.33,10.91;26,112.66,493.44,377.96,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="26,189.67,479.89,176.52,10.91">Color emotions for multi-colored images</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Solli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Lenz</surname></persName>
		</author>
		<idno type="DOI">10.1002/col.20604</idno>
		<ptr target="https://doi.org/10.1002/col.20604.doi:10.1002/col.20604" />
	</analytic>
	<monogr>
		<title level="j" coord="26,374.87,479.89,131.12,10.91">Color Research &amp; Application</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="210" to="221" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,506.99,393.33,10.91;26,112.66,520.54,393.33,10.91;26,112.33,534.09,394.84,10.91;26,112.66,550.08,109.22,7.90" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="26,341.58,506.99,164.41,10.91;26,112.66,520.54,205.39,10.91">Object detection in the image using the method of selecting significant structures</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Mokshin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sayfudinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yudina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Sharnin</surname></persName>
		</author>
		<idno type="DOI">10.14419/ijet.v7i4.38.27759</idno>
		<ptr target="https://doi.org/10.14419/ijet.v7i4.38.27759.doi:10.14419/ijet.v7i4.38.27759" />
	</analytic>
	<monogr>
		<title level="j" coord="26,328.14,520.54,177.85,10.91;26,112.33,534.09,51.12,10.91">International Journal of Engineering &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">1187</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,561.19,393.32,10.91;26,112.66,574.74,395.01,10.91;26,112.66,588.29,137.64,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="26,167.73,561.19,338.25,10.91;26,112.66,574.74,63.99,10.91">Image classification and object detection algorithm based on convolutional neural network</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">K L</forename></persName>
		</author>
		<idno type="DOI">10.15354/si.19.re117</idno>
		<ptr target="https://doi.org/10.15354/si.19.re117.doi:10.15354/si.19.re117" />
	</analytic>
	<monogr>
		<title level="j" coord="26,184.11,574.74,69.17,10.91">Science Insights</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="85" to="100" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,601.84,393.04,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="26,167.56,601.84,121.27,10.91">Can pictures be arguments?</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fleming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="26,297.68,601.84,134.24,10.91">Argumentation and Advocacy</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="11" to="22" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,615.39,393.33,10.91;26,112.66,628.93,169.40,10.91" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="26,177.83,615.39,328.15,10.91;26,112.66,628.93,138.81,10.91">Virtues of visual argumentation: How pictures make the importance and strength of an argument salient</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">E</forename><surname>Kjeldsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,112.66,642.48,393.32,10.91;26,112.48,656.03,394.21,10.91;26,112.66,669.58,184.32,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="26,220.78,642.48,174.98,10.91">A review on image retrieval techniques</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Meharban</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Priya</surname></persName>
		</author>
		<idno type="DOI">10.9756/bijaip.8136</idno>
		<ptr target="https://doi.org/10.9756/bijaip.8136.doi:10.9756/bijaip.8136" />
	</analytic>
	<monogr>
		<title level="j" coord="26,404.38,642.48,101.60,10.91;26,112.48,656.03,188.14,10.91">Bonfring International Journal of Advances in Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="7" to="10" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,86.97,394.53,10.91;27,112.66,100.52,123.72,10.91" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="27,221.18,86.97,281.20,10.91">spacy -industrial-strength natural language processing in python</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Montani</surname></persName>
		</author>
		<ptr target="https://spacy.io/" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,114.06,394.53,10.91;27,112.66,127.61,395.17,10.91;27,112.66,141.16,393.33,10.91;27,112.33,154.71,236.65,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="27,160.31,127.61,209.54,10.91">Argument search: Assessing argument relevance</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Euchner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>HeilenkÃ¶tter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Weidmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331327</idno>
	</analytic>
	<monogr>
		<title level="m" coord="27,377.36,127.61,130.47,10.91;27,112.66,141.16,393.33,10.91">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1117" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,168.26,393.33,10.91;27,112.66,181.81,394.62,10.91;27,112.31,195.36,338.84,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="27,157.13,168.26,183.60,10.91">An overview of the tesseract OCR engine</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1109/icdar.2007.4376991</idno>
		<ptr target="https://doi.org/10.1109/icdar.2007.4376991.doi:10.1109/icdar.2007.4376991" />
	</analytic>
	<monogr>
		<title level="m" coord="27,364.09,168.26,141.90,10.91;27,112.66,181.81,252.74,10.91">Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,208.91,393.33,10.91;27,112.66,222.46,394.53,10.91;27,112.66,236.01,397.48,10.91;27,112.66,252.00,26.14,7.90" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="27,277.75,208.91,228.24,10.91;27,112.66,222.46,98.68,10.91">A comprehensive study on lexicon based approaches for sentiment analysis</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bonta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Kumaresh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Janardhan</surname></persName>
		</author>
		<idno type="DOI">10.51983/ajcst-2019.8.s2.2037</idno>
		<ptr target="https://doi.org/10.51983/ajcst-2019.8.s2.2037.doi:10.51983/ajcst-2019.8.s2.2037" />
	</analytic>
	<monogr>
		<title level="j" coord="27,219.91,222.46,235.08,10.91">Asian Journal of Computer Science and Technology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,263.11,394.52,10.91;27,112.66,276.66,393.33,10.91;27,112.66,290.20,65.99,10.91" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="27,266.68,263.11,235.66,10.91">Distinguishing cartoons images from real-life images</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zaid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Al-Khafaji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="27,112.66,276.66,393.33,10.91">International Journal of Advanced Research in Computer Science and Software Engineering</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="91" to="95" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,303.75,393.33,10.91;27,112.66,317.30,287.18,10.91" xml:id="b24">
	<monogr>
		<ptr target="https://stackoverflow.com/a/59315026" />
		<title level="m" coord="27,112.66,303.75,292.40,10.91">user12526469 nathancy, How to detect diagram region and extract(</title>
		<imprint/>
	</monogr>
	<note>crop) it from a research paper&apos;s image, 2022</note>
</biblStruct>

<biblStruct coords="27,112.66,330.85,393.32,10.91;27,112.66,344.40,397.48,10.91;27,112.66,360.39,38.01,7.90" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="27,236.65,330.85,173.44,10.91">The string-to-string correction problem</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Fischer</surname></persName>
		</author>
		<idno type="DOI">10.1145/321796.321811</idno>
		<ptr target="https://doi.org/10.1145/321796.321811.doi:10.1145/321796.321811" />
	</analytic>
	<monogr>
		<title level="j" coord="27,418.92,330.85,87.07,10.91">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="168" to="173" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,371.50,393.33,10.91;27,112.66,385.05,395.17,10.91;27,112.66,398.60,393.33,10.91;27,112.66,412.15,394.91,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="27,164.81,385.05,343.02,10.91;27,112.66,398.60,67.69,10.91">The inception platform: Machine-assisted and knowledge-oriented interactive annotation</title>
		<author>
			<persName coords=""><forename type="first">Jan-Christoph</forename><surname>Klie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Bugert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Beto</forename><surname>Boullosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Eckart De Castilho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/C18-2002/" />
	</analytic>
	<monogr>
		<title level="m" coord="27,191.96,398.60,314.02,10.91;27,112.66,412.15,159.75,10.91">Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations</title>
		<meeting>the 27th International Conference on Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,425.70,393.33,10.91;27,112.66,439.25,395.01,10.91" xml:id="b27">
	<monogr>
		<title level="m" type="main" coord="27,320.29,425.70,185.70,10.91;27,112.66,439.25,198.12,10.91">On the ambiguity of rank-based evaluation of entity alignment or link prediction methods</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Berrendorf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Faerman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Vermue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/2002.06914" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,452.79,394.53,10.91;27,112.66,466.34,393.33,10.91;27,112.66,479.89,394.51,10.91;27,112.66,495.88,123.08,7.90" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="27,327.46,452.79,175.13,10.91">TIRA Integrated Research Architecture</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-22948-1_5</idno>
	</analytic>
	<monogr>
		<title level="m" coord="27,240.99,466.34,264.99,10.91;27,112.66,479.89,123.97,10.91">Information Retrieval Evaluation in a Changing World, The Information Retrieval Series</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,112.66,506.99,393.33,10.91;27,112.66,520.54,393.33,10.91;27,112.66,534.09,393.32,10.91;27,112.66,547.64,393.33,10.91;27,112.66,561.19,394.03,10.91;27,112.66,574.74,185.51,10.91" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="27,323.15,506.99,182.83,10.91;27,112.66,520.54,186.91,10.91">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://aclanthology.org/N19-1423.doi:10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m" coord="27,327.87,520.54,178.11,10.91;27,112.66,534.09,393.32,10.91;27,112.66,547.64,99.97,10.91">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
