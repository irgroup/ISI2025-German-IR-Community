<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,347.46,15.42;1,89.29,106.66,345.41,15.42;1,88.78,128.58,363.59,15.43;1,89.29,150.91,324.97,11.96">Boromir at Touch√© 2022: Combining Natural Language Processing and Machine Learning Techniques for Image Retrieval for Arguments Notebook for the Touch√© Lab on Argument Retrieval at CLEF 2022</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.93,176.82,88.30,11.96"><forename type="first">Thilo</forename><surname>Brummerloh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Leipzig University</orgName>
								<address>
									<addrLine>Augustusplatz 10</addrLine>
									<postCode>04109</postCode>
									<settlement>Leipzig</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,189.14,176.82,105.52,11.96"><forename type="first">Miriam</forename><forename type="middle">Louise</forename><surname>Carnot</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Leipzig University</orgName>
								<address>
									<addrLine>Augustusplatz 10</addrLine>
									<postCode>04109</postCode>
									<settlement>Leipzig</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,305.93,176.82,62.20,11.96"><forename type="first">Shirin</forename><surname>Lange</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Leipzig University</orgName>
								<address>
									<addrLine>Augustusplatz 10</addrLine>
									<postCode>04109</postCode>
									<settlement>Leipzig</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,395.60,176.82,75.28,11.96"><forename type="first">Gregor</forename><surname>Pf√§nder</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Leipzig University</orgName>
								<address>
									<addrLine>Augustusplatz 10</addrLine>
									<postCode>04109</postCode>
									<settlement>Leipzig</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,347.46,15.42;1,89.29,106.66,345.41,15.42;1,88.78,128.58,363.59,15.43;1,89.29,150.91,324.97,11.96">Boromir at Touch√© 2022: Combining Natural Language Processing and Machine Learning Techniques for Image Retrieval for Arguments Notebook for the Touch√© Lab on Argument Retrieval at CLEF 2022</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">70C97F82F3D63327C0D2A5FBC2F5529E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>argument retrieval</term>
					<term>images</term>
					<term>information retrieval</term>
					<term>search engines</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the frequent information overload when scrolling the web, little information sticks with the reader. In argumentation, images are often used to leave a formative impression. Until now, there has been little research focusing on search engines specifically devoted to finding argumentative images. Argumentative images help the viewer to form an opinion by implicitly or explicitly giving an argument that either supports or invalidates a thesis. We built a search engine that assists users to overview a controversial topic with supporting and opposing images. With this goal in mind, we compare different techniques from the fields of Natural Language Processing and Machine Learning to cluster the images, extract the text from the images and evaluate the sentiment of the page the image appears on. The best retrieval system uses a BERT model to determine stance, query Preprocessing, optical character recognition, and image clustering to detect the image content. Over 50% of the images found by this retrieval system are relevant, argumentative, and assigned to the correct stance according to automatic and manual evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The internet contains a vast assortment of opinions and arguments in social media posts, discussion forums, and news pages that are presented, challenged, and evaluated by contributors and readers. But mostly these textual argumentation are not structured as arguments <ref type="bibr" coords="1,461.38,492.36,11.27,10.91" target="#b0">[1]</ref>. Those arguments are commonly expressed verbally. However, it is possible to present some of them as images with the argument written on them or simply through visual communication e.g., symbolism <ref type="bibr" coords="1,141.73,533.01,11.58,10.91" target="#b1">[2]</ref>. A common example of visual arguments are memes which amongst others became popular as a method to influence the 2016 presidential primaries in the U.S. <ref type="bibr" coords="1,462.95,546.56,11.43,10.91" target="#b2">[3]</ref>.</p><p>Our research addresses the above-stated problems with the implementation of an image-based argument search engine in the course of Shared Task 3 from Touch√© 2022. The topic of the task is "Image retrieval to corroborate and strengthen textual arguments and to provide a quick overview of public opinions on controversial topics" <ref type="bibr" coords="1,324.26,600.75,11.43,10.91" target="#b3">[4]</ref>.</p><p>Our work pursues the fulfillment of the evaluation criteria defined by the Shared Task 3<ref type="foot" coords="2,97.09,98.76,3.71,7.97" target="#foot_0">1</ref> :</p><p>‚Ä¢ topic relevance ‚Ä¢ argumentativeness ‚Ä¢ stance relevance of the retrieved images regarding the query. These three evaluation criteria represent the assessment of the relevance of an image to a given query that should be considered for an image argument search engine.</p><p>Topic relevance indicates the extent to which an image matches the content of the entered search query. Argumentativeness states whether the image is suitable for defending a position within the debate on the searched topic. And finally, stance relevance evaluates if the image supports the stance (pro or con) it is assigned to <ref type="bibr" coords="2,307.90,256.65,11.51,10.91" target="#b4">[5]</ref>. For each of the three evaluation criteria, we compare several approaches: To retrieve a high number of topic relevant images our focus is on preprocessing the query and optical character recognition (OCR). OCR is also used to filter the most argumentative images together with clustering the images according to the type of image (e.g., statistic, text image, image with people). To assign the images to the correct stance we use sentiment analysis. We assume that the sentiment of the web page text the image appears on, correlates with supporting (positive sentiment) or opposing (negative sentiment) a controversial question. In Section 2, we review related work covering argument search, OCR, image clustering and sentiment analysis. Section 3 will present our methodology and Section 4 will discuss our results. Section 5 will summarize this work and point out the limitations of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we clarify the context of Argument Search, Optical Character Recognition, Image Clustering and Sentiment Analysis. It should allow the reader to better comprehend the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Argument Search</head><p>Wachsmuth et al. <ref type="bibr" coords="2,168.04,527.59,12.72,10.91" target="#b5">[6]</ref> propose a framework for argument search. An argument is saved with an ID, the URL of the web page they are found on, and the page's full text. The index is created with Apache Lucene using the argument representations as input. Elasticsearch can be used via a REST API as a user interface to Apache Lucene to create the index and to search it. To allow fast search responses Elasticsearch searches an index instead of the whole text. It creates the so-called inverted index with keywords from the document's text using Apache Lucene <ref type="bibr" coords="2,89.29,608.89,11.49,10.91" target="#b6">[7]</ref>. To provide results ranked by relevance to the query Lucene has several standard ranking functions such as Okapi BM25 which relies on term frequency-inverse document frequency (TF-IDF). With slight adaptions, this framework can be applied to results represented as images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Optical Character Recognition</head><p>OCR is used to retrieve text from images automatically. Hamad et al. <ref type="bibr" coords="3,423.80,107.54,12.99,10.91" target="#b7">[8]</ref> discuss OCR's challenges, important steps in the pipeline, use cases, and historical details. One of the major challenges is called scene complexity where the intricate image content makes it difficult to distinguish the text from the rest.</p><p>One widely-used OCR system was developed by Fedor et al. <ref type="bibr" coords="3,357.43,161.73,12.69,10.91" target="#b8">[9]</ref> in 2018. The so-called "Rosetta" is deployed on Facebook and Instagram to evaluate memes in real-time and at scale. First, the system detects text regions of an image using a Faster-R-CNN model <ref type="bibr" coords="3,412.66,188.83,16.41,10.91" target="#b9">[10]</ref>, and afterwards recognizes letters contained in those regions by applying a CNN model. They point out that high-quality training data containing a variety of fonts, styles, and font sizes is very important for training the OCR model. Even though the procedure is described in detail, the source code has not been made available so we can not try its performance on our data set.</p><p>Memes are a very popular image type on the internet. They often contain an opinion and take stance with one side or the other <ref type="bibr" coords="3,256.51,270.13,16.09,10.91" target="#b10">[11]</ref>. Beskow et al. <ref type="bibr" coords="3,340.50,270.13,17.76,10.91" target="#b11">[12]</ref> worked on characterizing memes and elaborating families of memes. Among other things, they used meme-specific OCR for identifying the text written in the image. The biggest challenge using OCR algorithms is that most are trained on text with black font and white background which is usually not the case for images from the web. To solve the problem, Beskow et al. preprocess the images before using Google's Tesseract OCR <ref type="foot" coords="3,232.05,336.12,3.71,7.97" target="#foot_1">2</ref> . As our dataset also contains many memes, we take their Preprocessing approach as a guideline and also decide for Google Tesseract.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Image Clustering</head><p>In our work we use image clustering to group together images of similar types such as bar plots, pie charts, images of persons demonstrating or persons doing sport. From these image types we derive the importance of the images as an argument depending on their type. Omran et al. <ref type="bibr" coords="3,89.29,441.80,17.91,10.91" target="#b12">[13]</ref> give a general overview of clustering methods and addresses some of the most important similarity measures and clustering algorithms. We use the k-Means clustering, which aims to minimize the intra-cluster distance, by adding new data points to the cluster whose centroid has the shortest distance to the new point. Since having to consider a few special things when working with images, Rahmani et al. <ref type="bibr" coords="3,254.63,495.99,17.87,10.91" target="#b13">[14]</ref> give a short introduction of how k-means clustering with image data works.</p><p>Clustering methods for images mostly use image feature vectors, which are created to represent the contents of a image in form of a multidimensional numerical vector, as input data. Those vectors can be created in multiple ways, for example using machine learning methods like VGG16 <ref type="bibr" coords="3,141.36,563.74,16.08,10.91" target="#b14">[15]</ref>. VGG16 is a convolutional network designed for image classification. It was the winning method of the 2014 ImageNet Large-ScaleVisual Recognition Challenge (ILSVRC) for the localization and classification tracks <ref type="bibr" coords="3,269.43,590.84,16.25,10.91" target="#b15">[16]</ref>. The net consists of 16 convolutional layers with filters of a receptive field size of 3 x 3. We use a pre-trained VGG16 model for our clustering approach to create image feature vectors.</p><p>Another noteworthy approach for creating image feature vectors is the Scale Invariant Feature Transform (SIFT) from Lowe <ref type="bibr" coords="3,219.22,645.03,16.24,10.91" target="#b16">[17]</ref>. This method finds and localizes key points in an image and transforms them into image features. Csurka et al. <ref type="bibr" coords="4,318.35,86.97,17.99,10.91" target="#b17">[18]</ref> and Yanai <ref type="bibr" coords="4,385.81,86.97,17.99,10.91" target="#b18">[19]</ref> use SIFT to generate a so-called bag-of-keypoints. A bag-of-keypoints can be seen as a histogram of the number of occurrences of particular image patterns in an image, similar to the bag-of-words representation for documents. The bag-of-keypoints can then be used to classify an image using a classifier method like SVM. The strength of bag-of-keypoints lies in object detection. We decided against a bag-of-keypoints approach because the aim of our clustering is to separate the different types of images (e.g. pie chart, bar plot, persons demonstrating, ...) rather then identifying the objects pictured on them.</p><p>In many cases, the scale of the image collection is too large for computing on a standard processor. Liu et al. <ref type="bibr" coords="4,179.15,208.91,17.95,10.91" target="#b19">[20]</ref> propose a method that allows nearest neighbor clustering for a large number of images with the help of parallel distributed hybrid spill trees which are implemented by combining MapReduce operations with a intelligent data partitioning. Even though it is not relevant for the amount of data the Touch√© Task involves, the aspect of scalability must be kept in mind when working with even larger data sets. As an illustration, modern image search engines can contain billions of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Sentiment Analysis</head><p>We determine an image's stance towards a topic by performing a sentiment analysis on the associated web page's text. We assume that sentiments reveal if the image supports or opposes a given query. Nielsen <ref type="bibr" coords="4,190.60,353.48,17.85,10.91" target="#b20">[21]</ref> proposes a lexicon based approach to determine the sentiment of a text. The proposed lexicon "AFINN" consists of words that are labeled with a certain sentiment score. The latest version of AFINN consists of 2477 unique words (including phrases) that were manually labeled with an integer value between -5 (very negative) and 5 (very positive). The overall sentiment of a text is the sum of sentiment scores of all words contained in that text. Another approach to sentiment analysis uses a machine learning classifier with a pre-trained BERT-model (Bidirectional Encoder Representations from Transformers) introduced by Devlin et al. <ref type="bibr" coords="4,113.17,448.32,16.09,10.91" target="#b21">[22]</ref>. BERT is a language representation model that can be customized to a variety of tasks by only adding one extra output layer. Possible applications include answering questions or sentiment analysis <ref type="bibr" coords="4,170.88,475.42,17.28,10.91" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we describe our approach guided by the three evaluation criteria: topic relevance, argumentativeness, and stance relevance. We start with a summary of the data we used and the workflow. Afterward, we explain in depth the methods used, namely document and query Preprocessing, optical character recognition, image clustering, and sentiment analysis. At the end of the chapter, we propose an evaluation method for comparing the implemented techniques. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Workflow</head><p>Our workflow begins by building the index. As can be seen in Figure <ref type="figure" coords="5,393.42,492.38,4.99,10.91" target="#fig_0">1</ref> we use the crawled web pages' title, visible text and the image that is associated with that webpage. The title is fed into a Machine Learning algorithm to derive a sentiment that can either be positive or negative.</p><p>The pre-processed text of the webpage is used to calculate a sentiment, though here we use a dictionary based approach to determine the score of the text. Furthermore the text is also directly fed into the index to execute queries on it.</p><p>The image of a webpage is also used twofold. We scan it for text and put any readable text into the index to be queried with higher weighting than the text of the webpage. Additionally, we analyze the image with a neural network trained for image classification. We use the output of the network to assign the image to an image type.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Document And Query Preprocessing</head><p>In our initial approach to pre-process we use the text of the web page as this document is used for building the index. We use the Natural Language Toolkit (NLTK) <ref type="bibr" coords="6,420.49,332.70,17.94,10.91" target="#b23">[24]</ref> for the desired adjustments: First, we convert the document to lowercase and remove URLs. We only keep letters and tokenize the text. Next, we lemmatize all tokens and remove tokens consisting of only one letter and tokens included in the stop word list of the NLTK library. Finally, we eliminate the tokens that appear only once or twice in the text, which is a method based on Zipf's law <ref type="bibr" coords="6,89.29,400.45,16.23,10.91" target="#b24">[25]</ref>. We find this step necessary because many web pages contain additional information like navigation or a footer. Words from those sections do not appear often on the web page, whereas words important to the main topic of the page will show up frequently. In the next step we pre-process the query. Not all words contained in a query have the same importance to the topic and thereby for the retrieval. Less important words can negatively influence the search. If web pages are found that use only these less important words, those pages might be less relevant to the topic. Therefore, we decide to create our own stop word list with selected words that do not contribute to the statement of the query e.g., "be", "for", "in". We eliminate these words from the query. Furthermore, we lemmatize the remaining words as we did with the text of the web pages. We believe that it is more effective to perform the retrieval when the words in the query and the page text are restored to their non-inflected form in the same way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Optical Character Recognition</head><p>Our research team shares the impression that images containing text are in the majority of cases more argumentative than images without because the text directly indicates what the image should express. We expect images which contain text from a query to be more relevant than other images. Optical character recognition is used to identify image text. We decide to build our pipeline using Google's Tesseract OCR <ref type="bibr" coords="6,222.35,653.41,17.76,10.91" target="#b25">[26]</ref> as it is one of the most popular freely-available OCR engines and is widely used <ref type="bibr" coords="6,174.09,666.96,16.96,10.91" target="#b26">[27]</ref> <ref type="bibr" coords="6,191.04,666.96,16.96,10.91" target="#b11">[12]</ref>. The implemented text identification pipeline is depicted in Figure <ref type="figure" coords="7,89.29,86.97,3.68,10.91" target="#fig_1">2</ref>. First, we binarize the image as OCR techniques work better on black and white pictures <ref type="bibr" coords="7,487.51,86.97,16.12,10.91" target="#b11">[12]</ref>. Afterward, we extract the text using Tesseract. It often occurs that random symbols and letters are being extracted from parts of the image where there is no text. Therefore, we decide to keep only letters and check for each word of the extracted text if it is included in an English dictionary.  There are clear limitations for handwritten text e.g., images containing demonstration posters. However, our goal is not to identify the text from each image perfectly but to get an impression of whether OCR can generally improve image retrieval for arguments. It is possible to use commercial OCR engines that claim to achieve more accurate results or to train neural networks for text extraction. The latter would be much more time-consuming and is an ongoing field of research on its own. Based on our results, Tesseract OCR is improving the retrieval and it would be interesting to find out in the future if different OCR systems could improve it even further. After extracting the text of each image, we add it to the index. For each query, we retrieve images based on the web pages text and the text extracted from the images via OCR. The image text gets a boost of five i.e. its score is multiplied by five. We choose five after having tried different numbers. For each image, we compare the boosted image text score with the page text score and use the higher one for the retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Image Clustering</head><p>Another approach to increase the argumentativeness of the results is to categorize images into classes of different image types. There are plots, memes, landscape photos, portraits, and many more possible classes. The idea behind using image clustering is, that some of the classes are more argumentative than others. Therefore images of these classes should get higher attention than images of less argumentative classes. This can be shown by the exemplary topic "Should people become vegetarian?". Figure <ref type="figure" coords="7,250.53,514.88,5.09,10.91">3</ref> (a) shows a bar plot of the protein intake of vegetarians and non-vegetarians. Images like this can easily be used to argue for or against a topic. In comparison, Figure <ref type="figure" coords="7,177.79,541.98,5.08,10.91">3</ref> (b) shows different kinds of fruits and vegetables in the shape of a heart. Compared to the argumentative benefit of a bar plot and other statistics, the argumentativeness of a symbolic image like this is limited, because it does not provide additional facts to support a textual argument with.</p><p>Our goal is to find clusters containing images with the same level of argumentativeness. We do this by using an image clustering method based on feature vectors and k-means clustering 4 . The detail of the processing is as follows:</p><p>1. Load a pre-trained VGG16 model for image classification Figure <ref type="figure" coords="8,132.36,425.70,5.17,10.91" target="#fig_3">4</ref> shows the elbow plot from which we derive the number of clusters to be found by k-means. The plot shows the Within Cluster Sum of Squared Errors (WCSS) when using different numbers of clusters <ref type="bibr" coords="8,226.92,452.79,16.42,10.91" target="#b27">[28]</ref>. In theory, the optimal number of clusters is a number where the WCSS stops decreasing significantly with the addition of one cluster. This point is usually recognizable by a clear inflection point in the displayed curve. Our plot shows no clear indication for one optimal number of clusters, but because of the flattening character of the curve in the range between 10 and 20, anywhere between 10 clusters and 20 clusters seems reasonable.</p><p>To find the final number we need a deeper inspection of the differences in the clustering results, when choosing different numbers of clusters. We repeat the k-means clustering for all possible numbers of clusters between 10 and 20. Then, for each possible number, we look into examples of images that are assigned to the different clusters and compare them to other results. After manually inspecting all possibilities between 10 and 20 clusters, we found that 14 clusters is a good number to build our image clustering up on.</p><p>After the identification of the clusters, we define a weight for each cluster, based on the level of argumentativeness of the image type each cluster represents. A strength of this approach is, that the determination of the weights is variable. However, it is difficult to find an objective heuristic for determining a cluster's weight, because the perceived level of argumentativeness of each cluster is subjective. In our case we determine weights between 1 (for lowest level of argumentativeness) to 5 (for the highest level of argumentativeness) for each cluster. Table <ref type="table" coords="9,500.85,323.33,5.14,10.91" target="#tab_3">2</ref> gives a overview of the clusters and the weights we use in our retrieval system. It shows that we give the highest weight of 5.0 to the clusters 5,8,10 and 13 which contain a high amount of statistical graphics or other graphics with text on them. We give a weight of 3.0 to the clusters 3,6,9 and 12 which contain cartoons, memes or other graphics with text. Cluster 7 is the only one with a weight of 2.0 and contains photos of protesters with posters. The remaining clusters 0,1,2,4 and 11 get a weight of 1.0 and are mostly containing some sort of photos without text on them.</p><p>The information to which cluster each image belongs is added to the index. After that, when retrieving images, the weight for each cluster is implemented into the query in a way that it will be multiplied with the relevance score of the retrieval. Hence, the method will influence the relevance ranking of an image, depending on the cluster the image belongs to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Sentiment Analysis</head><p>Our approach to stance classification assumes an underlying sentiment of the web page's content that can be used to determine whether the web page is supporting or opposing a query. In a first experiment, we use dictionary-based sentiment analysis on the whole text content of the web pages embedding the image. In a second experiment, we use a BERT-based method to label the titles of web pages that embed the images to positive and negative sentiments.</p><p>Dictionary-Based Approach: We use the complete textual content of the web page from the text.txt file to compare each word with the AFINN dictionary and get a corresponding score that is either positive (from 1 to 5), negative (from -1 to -5), or neutral (0). The sum of scores of all words gives the overall score of a page's content. We assume that a positive score represents a pro stance, a negative score a contra stance, and zero no stance. We add the scores to the index and modify the query to give two result sets, one for positive scores (higher than zero) as the pro side and one for negatives scores (lower than zero) as the con side. Images with the score zero are not considered in the results. Machine-Learning-based method: For this approach, we fine-tune a pre-trained BERT model with movie reviews from the Internet Movie Database (IMDB) to specialize the model on sentiment <ref type="bibr" coords="10,152.73,453.64,16.41,10.91" target="#b28">[29]</ref>. This model accepts input phrases of up to 512 words only. Because the entire text of the web page is usually longer than the web page's title, we used the latter to determine the web page's sentiment. The assumption is that the title will represent a web page's content and thus, their sentiment. The BERT model we use is the BERT-base-uncased model from "Hugging Face".<ref type="foot" coords="10,183.96,506.08,3.71,7.97" target="#foot_3">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Evaluation Method</head><p>Corresponding to the three evaluation criteria (topic relevance, argumentativeness and stance), our evaluation method is threefold. The easiest case is evaluating if an image was relevant for the topic. The dataset is provided by the organizers of the shared task. They obtained the images by conducting Google Image searches for all topics from the provided topics.xml file in the dataset as queries. To verify if a retrieved image fits the entered query, we check from which Google image search query it was obtained. This information is given in the rankings.jsonl file. Our retrieval systems retrieve ten images supporting and ten images opposing a query. We perform the retrieval for the 50 given topics and calculate the percentage of fitting images based on their ranking in the Google Image search results for the same query.</p><p>There is no information given if an image is argumentative or if it has the correct stance. Therefore, we evaluate those two evaluation criteria manually. We pick out five topics to evaluate. The team members evaluate the retrieved images for the five topics independently. For each run, we indicate how many of the images on the pro side are argumentative and how many of those are on the correct side. We repeat the same procedure for the con side. We then form the average across all topics and individual ratings and calculate the percentages of retrieved images that are argumentative respective stance relevant.</p><p>To get an idea of our overall consensus in the evaluation, we calculate several statistics used for ratio data and more than two raters: Fleiss' Kappa, Krippendorff's Alpha, and Gwet's coefficients AC1 and AC2 <ref type="bibr" coords="11,205.56,385.52,16.25,10.91" target="#b29">[30]</ref>.</p><p>Table <ref type="table" coords="11,127.01,399.07,5.07,10.91" target="#tab_4">3</ref> shows the calculated coefficients. The percent agreement increases from left: Fleiss' kappa with 5.9% to right: AC2 with 84.9%. The general representation of percent agreement is shown in <ref type="bibr" coords="11,132.47,426.17,11.35,10.91" target="#b0">(1)</ref> and can be summarized with: the actual agreement that was not caused by chance divided by perfect agreement not caused by chance.</p><formula xml:id="formula_0" coords="11,277.31,462.45,224.81,25.50">ùê¥ ùë§ -ùê¥ ùëê 1 -ùê¥ ùëê (<label>1</label></formula><formula xml:id="formula_1" coords="11,502.13,469.10,3.86,10.91">)</formula><p>where ùê¥ ùë§ = weighted percent agreement ùê¥ ùëê = percent chance agreement It is known that Fleiss' kappa, Krippendorff's alpha, and AC1 "overstate the percent chance agreement" resulting in an understated percent agreement <ref type="bibr" coords="11,359.34,547.54,16.41,10.91" target="#b29">[30]</ref>. We can also see that the pvalues of those methods are higher than a 5% alpha-value while the p-value of AC2 with 0.00057 is even lower than a 1% alpha-value. That is why we assume an 84.9% agreement within our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>The following chapter compares the performance of the implemented methods. We aim to find out which methods work best together, considering the three evaluation criteria and the overall performance. We opt for six different retrieval systems which we compare to the method proposed by Kiesel et al. <ref type="bibr" coords="12,115.54,114.06,11.58,10.91" target="#b4">[5]</ref>. We replicate their approach which expands the query with the term "pro" for the supporting side and the term "anti" for the opposing side. This retrieval system is denoted with the number 0 in the following graphics. System number 1 uses only the dictionary-based sentiment analysis for stance allocation. The second system extends system 1 by usage of OCR, whereas the number 3 uses image clustering on top of the dictionary-based sentiment analysis. System number 4 combines the same sentiment analysis, the OCR, and the image clustering. Additionally, system number 5 adds query Preprocessing. For retrieval system number 6 we use the same components as number 5 but exchange the dictionary-based sentiment analysis with the machine-learning-based sentiment analysis.</p><p>We deployed our systems within the "TIRA" platform that was set up by the workshop officials <ref type="bibr" coords="12,89.29,249.56,16.41,10.91" target="#b30">[31]</ref>. There, we submitted five different approaches at first, that can be recognized by their timestamps in table 4. We included a short description of every system. A sixth system was submitted, that we did not anticipate to perform as well as it did. This sixth combination of methods ultimately performed best out of all in Touch√© Task 3. We did not analyze this system further because we did not expect that it would perform better without the clustering. Figure <ref type="figure" coords="12,131.87,466.34,5.17,10.91">5</ref> reveals the results of the seven retrieval systems regarding the topic relevance of the retrieved images. Only the replicated system 0 obtains less than 80% of topic-relevant images. Especially the retrieval systems 1, 2, 3 and 5 perform best with over 84%. However, the distances are not large. Retrieval systems number 6 perform better for the con-side than any other retrieval systems.</p><p>The next evaluation criterion is to retrieve argumentative images. Figure <ref type="figure" coords="12,426.70,534.09,5.07,10.91">6</ref> shows how well the retrieval systems perform regarding this criterion. It is important to note that in our point of view, only topic-relevant images can also be argumentative for this topic. This means that the reached percentage of topic-relevant images corresponds to 100% in the evaluation of the argumentativeness. By doing so, we can assess the evaluation criteria independently from each other and find out which retrieval systems are best regarding each evaluation criterion separately. The retrieval system's performances for retrieving argumentative images are far apart. As to be expected the retrieval systems 0 and 1 perform poorly, both of them are based on techniques made for boosting the topic relevance and not the argumentativeness. The more techniques we add to the retrieval systems the better the results. The retrieval systems 4 and 6 perform best. We record an improvement from 64% with retrieval system 0 to 89% with system 6 -a clear sign that the applied methods have a positive effect on retrieving argumentative images.</p><p>The most difficult evaluation criterion to achieve is assigning the correct stance (pro or contra) to the images. As can be observed in figure <ref type="figure" coords="13,327.09,651.29,5.17,10.91" target="#fig_5">7</ref> all retrieval systems are struggling to figure out whether an image is supporting the query or not. On the pro-side, the retrieval systems do similarly well with about four out of five correct assignments. But the images on the contra-side often do not oppose the query. Retrieval system 6 using the machine-learning-based sentiment analysis managed to classify a majority of contra-images correctly. We accomplish an improvement from 48% with system 0 to 71% using system 6.</p><p>Regarding all three evaluation criteria, we obtain the results depicted in Figure <ref type="figure" coords="14,453.23,406.99,3.74,10.91" target="#fig_6">8</ref>. To understand the plot, it is important to know that only images that correspond to the topic can be argumentative for that topic and only those images argumentative for a topic can be assigned a correct side. Images in the red part on the bottom of the plot achieve all three evaluation criteria, whereas the brown part in the middle is argumentative but does not have the correct stance. The beige top part contains images that match the topic but are not argumentative. Retrieval systems 0 and 1 which only use the query expansion respective the dictionary-based sentiment analysis perform weakest with around 26% and 21% of images fulfilling all evaluation criteria. The three following retrieval systems 2, 3, and 4 score 39%, 39%, and 42%. Finally, the best results are produced by retrieval systems 5 with 48% and 6 with 52%. Comparing the replicated system designed by Kiesel et al. <ref type="bibr" coords="14,230.08,542.48,11.48,10.91" target="#b4">[5]</ref> to our best retrieval system, we achieve an improvement of the retrieval performance by 26%, which doubles that of system 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Intending to retrieve argumentative images supporting or opposing an entered query, we built six retrieval systems using ElasticSearch. The implemented retrieval systems use different combinations of the following techniques: Query Preprocessing, Optical Character Recognition, Image Clustering, dictionary-based, and machine-learning-based sentiment analysis. All six retrieval systems were evaluated regarding topic relevance, argumentativeness, and stance relevance of the retrieved images. The last two evaluation criteria were evaluated manually by four independent annotators. Our best retrieval system used a combination of all mentioned techniques except the dictionary-based sentiment analysis. We compared our retrieval systems to the only search engine we found that was implemented for this sort of image retrieval designed by Kiesel et al. <ref type="bibr" coords="15,198.97,427.29,11.41,10.91" target="#b4">[5]</ref>. We replicated their approach. Our best retrieval system is able to improve it by 26% regarding all three evaluation criteria.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Limitations</head><p>Nevertheless, our approach is constrained by some limitations. We observed that several images were informative and thus useful to build an opinion on the topic but did not clearly represent either side (pro or con). This often is the case with statistics showing the results of a survey. Depending on the viewer's interpretation the image can support the pro or the con side.</p><p>As an example, Figure <ref type="figure" coords="15,199.66,544.76,4.97,10.91" target="#fig_7">9</ref> presents people's opinions on abortion depending on the time of the abortion and the political opinion of the respondents. The plot opens up different interpretation possibilities. Particularly, the viewer's political opinion may influence if the plot is interpreted as supporting or opposing abortion. Nevertheless, this graph can contribute to answering the question of whether abortion should be legal as it contains a lot of information. Another observation was that even though the retrieval systems work well on many topics some topics do not have as many supporting arguments in form of images as opposing them or the other way around. An example of this is the ban on bottled water. Many arguments support the ban such as environmental friendliness and the equal quality of tab and bottled water, while the opposing side does not reveal any relevant arguments. We conclude that not all arguments are equally represented on the internet thus in our database and results. This limits the representativeness of an argument search engine to topics and opinions that are frequently discussed with a diversity of arguments. Also, more annotators would improve the significance of the results because the agreement would be more reliable. A more diverse group of annotators could help represent the view of a bigger part of society.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Future Work</head><p>In the future, we want to add more approaches to our list and work on the limitations. One idea is to train Convolutional Neural Networks to perform the OCR to improve text identification. Another idea is to analyze an image's colors and their distribution to find out how argumentative the image is and which stance it supports. We want to introduce an additional stance category besides pro and con, that contains informative images that can not be assigned to either pro or con. We would also like to have more people conduct the manual evaluation to increase the validity of the results. We would expect the retrieval performance to improve further by adding synonyms to the query. Thereby, we could also retrieve images that appear on web pages treating the same topic but use different words. Instead of applying sentiment analysis only on the title or the entire webpage we want to use it also on the image text or only on an excerpt of the webpage that is of higher importance to the image. Also, we would like to find out, if the retrieval performance could be enhanced by classifying the images into predefined classes instead of clustering them.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,89.29,260.98,208.34,8.93"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Methods to build the index and query it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,89.29,216.14,168.30,8.93"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The constructed OCR pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,89.29,297.56,327.87,8.93;8,307.60,94.15,184.25,170.08"><head>( a )Figure 3 :</head><label>a3</label><figDesc>Figure 3: Retrieval candidates for the topic 'Should people become vegetarian?'</figDesc><graphic coords="8,307.60,94.15,184.25,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,89.29,248.59,416.70,8.93;9,89.29,260.60,416.70,8.87;9,89.29,272.55,46.09,8.87;9,188.96,84.19,217.35,156.98"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The elbow method graph indicates the optimal number of clusters of image types in the argumentative image dataset is between 10 and 20. After manually inspecting the clusters, we decided on 14 (red).</figDesc><graphic coords="9,188.96,84.19,217.35,156.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="13,89.29,255.41,416.69,8.93;13,89.29,267.42,416.70,8.87;13,89.29,279.37,416.69,8.87;13,89.29,291.33,416.70,8.87;13,89.29,303.28,416.70,8.87;13,89.29,315.24,286.36,8.87;13,151.72,84.19,291.85,163.80"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Percentage of how many topic-relevant images are retrieved by the different retrieval systems for the pro (blue) and the contra (red) side. (0 -query expansion, 1 -dictionary-based sentiment analysis, 2 -dictionary-based sentiment analysis + OCR, 3 -dictionary-based sentiment analysis + Image Clustering, 4 -dictionary-based sentiment analysis + OCR + Image Clustering, 5 -dictionary-based sentiment analysis + OCR + Image Clustering + Query Preprocessing, 6 -machine-learning-based sentiment analysis + OCR + Image Clustering + Query Preprocessing)</figDesc><graphic coords="13,151.72,84.19,291.85,163.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="14,89.29,255.41,416.69,8.93;14,89.29,267.42,416.69,8.87;14,89.29,279.37,416.69,8.87;14,89.29,291.33,416.70,8.87;14,89.29,303.28,416.70,8.87;14,89.29,315.24,286.36,8.87;14,151.72,84.19,291.85,163.80"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Percentage of how many correctly classified images are retrieved by the different retrieval systems for the pro (blue) and the con (red) side. (0 -query expansion, 1 -dictionary-based sentiment analysis, 2 -dictionary-based sentiment analysis + OCR, 3 -dictionary-based sentiment analysis + Image Clustering, 4 -dictionary-based sentiment analysis + OCR + Image Clustering, 5 -dictionary-based sentiment analysis + OCR + Image Clustering + Query Preprocessing, 6 -machine-learning-based sentiment analysis + OCR + Image Clustering + Query Preprocessing)</figDesc><graphic coords="14,151.72,84.19,291.85,163.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="15,89.29,275.71,416.69,8.93;15,89.29,287.72,418.36,8.87;15,89.29,299.68,418.35,8.87;15,89.29,311.63,416.69,8.87;15,87.98,323.59,419.32,8.87;15,89.29,335.54,199.54,8.87;15,118.79,84.19,357.70,184.10"><head>Figure 8 :</head><label>8</label><figDesc>Figure8: Percentage of how many images meet all three evaluation criteria (pink) retrieved by the different retrieval systems (0 -query expansion, 1 -dictionary-based sentiment analysis, 2 -dictionarybased sentiment analysis + OCR, 3 -dictionary-based sentiment analysis + Image Clustering, 4dictionary-based sentiment analysis + OCR + Image Clustering, 5 -dictionary-based sentiment analysis + OCR + Image Clustering + Query Preprocessing, 6 -machine-learning-based sentiment analysis + OCR + Image Clustering + Query Preprocessing)</figDesc><graphic coords="15,118.79,84.19,357.70,184.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="16,89.29,314.63,337.40,8.93;16,154.58,84.19,286.13,217.88"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Legality of abortion depending on time of abortion and political opinion</figDesc><graphic coords="16,154.58,84.19,286.13,217.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,88.99,90.49,411.76,220.64"><head>Table 1</head><label>1</label><figDesc>Data used in our project</figDesc><table coords="5,89.29,122.10,411.46,189.03"><row><cell>Name</cell><cell>Content</cell><cell>Usage</cell></row><row><cell></cell><cell></cell><cell>Evaluation</cell></row><row><cell>text.txt</cell><cell>Text content of the webpage with the image</cell><cell>Indexing/Sentiment/</cell></row><row><cell></cell><cell></cell><cell>Retrieval</cell></row><row><cell>dom.html</cell><cell>html source of the webpage</cell><cell>Sentiment</cell></row><row><cell cols="2">one file for the entire dataset:</cell><cell></cell></row><row><cell>topics.xml</cell><cell>Title, description, and narrative of Touch√© topics</cell><cell>Evaluation/Development</cell></row><row><cell></cell><cell>1 to 50</cell><cell></cell></row><row><cell>3.1. Data</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,88.99,90.49,416.99,282.84"><head>Table 2</head><label>2</label><figDesc>Identified clusters by k-means, the weights specified by us and the number N of images within each cluster</figDesc><table coords="10,95.27,131.84,101.10,8.87"><row><cell>Cluster</cell><cell>Description</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,88.99,90.49,408.33,105.74"><head>Table 3</head><label>3</label><figDesc></figDesc><table coords="11,89.29,102.49,408.03,93.73"><row><cell cols="2">Inter-annotator Agreement</cell><cell></cell><cell></cell><cell></cell></row><row><cell>coefficient name</cell><cell>Fleiss' kappa</cell><cell>Krippendorff's</cell><cell>AC1 (Gwet's)</cell><cell>AC2 (Gwet's)</cell></row><row><cell></cell><cell></cell><cell>Alpha</cell><cell>identity weights</cell><cell>quadratic weights</cell></row><row><cell>coefficient value</cell><cell>5.9%</cell><cell>10.6%</cell><cell>13.2%</cell><cell>84.9%</cell></row><row><cell cols="2">confidence interval (-0.265, 0.384)</cell><cell>(-0.218, 0.431)</cell><cell>(-0.066, 0.33)</cell><cell>(0.613, 1)</cell></row><row><cell>p-value</cell><cell>0.638</cell><cell>0.414</cell><cell>0.137</cell><cell>0.00057</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,88.99,331.33,391.63,117.69"><head>Table 4</head><label>4</label><figDesc>Systems submitted as runs on the TIRA-platform</figDesc><table coords="12,112.76,362.95,71.93,8.87"><row><cell>No. Description</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,671.01,195.04,8.97"><p>https://webis.de/events/touche-22/shared-task-3.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,108.93,671.03,152.51,8.97"><p>https://github.com/tesseract-ocr/tesseract</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,108.93,670.99,285.20,8.97"><p>https://files.webis.de/corpora/corpora-webis/corpus-touche-image-search-22/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="10,108.93,660.08,8.76,8.97;10,151.02,660.08,356.48,8.97;10,89.29,671.04,50.05,8.97"><p>cf. https://towardsdatascience.com/sentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank the <rs type="institution">Webis research group</rs> for giving helpful advice and always being available for upcoming questions. A special thanks goes to <rs type="person">Theresa Elstner</rs>, who took time for us every week to discuss the current status.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="17,112.66,197.00,394.53,10.91;17,112.28,210.55,371.20,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="17,253.17,197.00,248.78,10.91">Laying the foundations for a world wide argument web</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Rahwan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Zablith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Reed</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artint.2007.04.015</idno>
	</analytic>
	<monogr>
		<title level="j" coord="17,112.28,210.55,94.35,10.91">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="page" from="897" to="921" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,224.10,393.33,10.91;17,112.66,237.65,394.51,10.91;17,112.66,253.64,123.08,7.90" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="17,171.60,224.10,228.73,10.91">The possibility and actuality of visual arguments</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Blair</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-94-007-2363-4_16</idno>
	</analytic>
	<monogr>
		<title level="m" coord="17,434.03,224.10,71.96,10.91;17,112.66,237.65,135.24,10.91">Groundwork in the Theory of Argumentation</title>
		<meeting><address><addrLine>Dordrecht</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="205" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,264.75,393.32,10.91;17,112.33,278.30,157.69,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="17,176.75,264.75,141.95,10.91">Meme-ing electoral participation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Heiskanen</surname></persName>
		</author>
		<idno type="DOI">10.4000/ejas.12158</idno>
	</analytic>
	<monogr>
		<title level="j" coord="17,326.81,264.75,166.49,10.91">European journal of American studies</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,291.85,395.17,10.91;17,112.66,305.40,393.33,10.91;17,112.66,318.95,393.33,10.91;17,112.33,332.50,122.86,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="17,349.01,305.40,156.98,10.91;17,112.66,318.95,35.10,10.91">Overview of touch√© 2022: Argument retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fr√∂be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gurcke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,169.19,318.95,336.80,10.91;17,112.33,332.50,50.36,10.91">Advances in Information Retrieval. 44th European Conference on IR Research (ECIR 2022)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,346.05,393.33,10.91;17,112.66,359.59,393.33,10.91;17,112.28,373.14,394.91,10.91;17,112.66,386.69,394.51,10.91;17,112.66,402.68,109.72,7.90" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="17,334.11,346.05,171.87,10.91;17,112.66,359.59,135.41,10.91">Image Retrieval for Arguments Using Stance-Aware Query Expansion</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reichenbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.argmining-1.4</idno>
		<ptr target="https://aclanthology.org/2021.argmining-1.4/.doi:10.18653/v1/2021.argmining-1.4" />
	</analytic>
	<monogr>
		<title level="m" coord="17,431.19,359.59,74.79,10.91;17,112.28,373.14,390.76,10.91">8th Workshop on Argument Mining (ArgMining 2021) at EMNLP, Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Al-Khatib</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Stede</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="36" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,413.79,394.53,10.91;17,112.30,427.34,394.97,10.91;17,112.66,440.89,393.33,10.91;17,112.66,454.44,264.84,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="17,267.50,427.34,217.89,10.91">Building an argument search engine for the web</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Al</forename><surname>Khatib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Puschmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dorsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Morari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w17-5106</idno>
	</analytic>
	<monogr>
		<title level="m" coord="17,112.66,440.89,393.33,10.91;17,112.66,454.44,46.58,10.91">Proceedings of the 4th Workshop on Argument Mining, Association for Computational Linguistics</title>
		<meeting>the 4th Workshop on Argument Mining, Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="49" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,467.99,393.33,10.91;17,112.39,481.54,393.88,10.91;17,112.66,495.09,76.13,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="17,221.71,467.99,284.28,10.91;17,112.39,481.54,125.67,10.91">Elasticsearch: An advanced and quick search technique to handle voluminous data, Compusoft</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Divya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,245.90,481.54,260.36,10.91">An international journal of advanced computer technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="171" to="175" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,508.64,393.33,10.91;17,112.66,522.18,393.33,10.91;17,112.33,535.73,68.33,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="17,272.86,508.64,233.13,10.91;17,112.66,522.18,47.82,10.91">A detailed analysis of optical character recognition technology</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">A</forename><surname>Hamad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">A Y A</forename><surname>Mehmet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,170.79,522.18,335.20,10.91">International Journal of Applied Mathematics Electronics and Computers</title>
		<imprint>
			<biblScope unit="page" from="244" to="249" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,549.28,393.33,10.91;17,112.66,562.83,393.33,10.91;17,112.66,576.38,242.18,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="17,281.43,549.28,224.56,10.91;17,112.66,562.83,95.22,10.91">Rosetta: Large scale system for text detection and recognition in images</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Borisyuk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sivakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,216.69,562.83,289.29,10.91;17,112.66,576.38,181.27,10.91">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="71" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,589.93,393.33,10.91;17,112.66,603.48,394.92,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="17,264.19,589.93,241.80,10.91;17,112.66,603.48,111.59,10.91">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,233.07,603.48,229.80,10.91">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,617.03,393.33,10.91;17,112.66,630.58,255.07,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="17,164.85,617.03,301.24,10.91">Memes in a digital world: Reconciling with a conceptual troublemaker</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Shifman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="17,474.00,617.03,31.99,10.91;17,112.66,630.58,171.13,10.91">Journal of computer-mediated communication</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="362" to="377" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,644.13,393.32,10.91;18,112.66,86.97,393.33,10.91;18,112.66,100.52,273.05,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="17,287.92,644.13,218.06,10.91;18,112.66,86.97,272.81,10.91">The evolution of political memes: Detecting and characterizing internet memes with multi-modal deep learning</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Beskow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">M</forename><surname>Carley</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2019.102170</idno>
	</analytic>
	<monogr>
		<title level="j" coord="18,393.69,86.97,112.30,10.91;18,112.66,100.52,58.22,10.91">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,114.06,393.33,10.91;18,112.66,127.61,299.40,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="18,302.96,114.06,149.81,10.91">An overview of clustering methods</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>Engelbrecht</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Salman</surname></persName>
		</author>
		<idno type="DOI">10.3233/IDA-2007-11602</idno>
	</analytic>
	<monogr>
		<title level="j" coord="18,461.02,114.06,44.97,10.91;18,112.66,127.61,62.24,10.91">Intelligent Data Analysis</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="583" to="605" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,141.16,393.60,10.91;18,112.66,154.71,393.98,10.91;18,112.41,168.26,220.72,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="18,275.94,141.16,230.32,10.91;18,112.66,154.71,35.10,10.91">Clustering of image data using k-means and fuzzy k-means</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K I</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Arora</surname></persName>
		</author>
		<idno type="DOI">10.14569/IJACSA.2014.050724</idno>
	</analytic>
	<monogr>
		<title level="j" coord="18,156.70,154.71,312.80,10.91">International Journal of Advanced Computer Science and Applications</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="160" to="163" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,181.81,393.33,10.91;18,112.66,195.36,251.71,10.91" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="18,246.14,181.81,259.84,10.91;18,112.66,195.36,49.16,10.91">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/pdf/1409.1556v6" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,208.91,394.53,10.91;18,112.28,222.46,393.70,10.91;18,112.66,236.01,394.51,10.91;18,112.66,252.00,104.78,7.90" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="18,327.85,222.46,178.14,10.91;18,112.66,236.01,40.99,10.91">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j" coord="18,162.12,236.01,187.49,10.91">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,263.11,393.33,10.91;18,112.66,276.66,390.34,10.91" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="18,163.87,263.11,244.36,10.91">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<idno type="DOI">10.1023/B:VISI.0000029664.99615.94</idno>
	</analytic>
	<monogr>
		<title level="j" coord="18,415.21,263.11,90.78,10.91;18,112.66,276.66,88.02,10.91">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,290.20,393.33,10.91;18,112.66,303.75,394.53,10.91;18,112.66,317.30,96.37,10.91" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="18,361.94,290.20,144.04,10.91;18,112.66,303.75,54.16,10.91">Visual categorization with bags of keypoints</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,190.34,303.75,233.83,10.91">Workshop on statistical learning in computer vision</title>
		<meeting><address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,330.85,394.62,10.91;18,112.66,344.40,394.52,10.91;18,112.66,357.95,223.47,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="18,157.15,330.85,329.12,10.91">Image collector iii: a web image-gathering system with bag-of-keypoints</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yanai</surname></persName>
		</author>
		<idno type="DOI">10.1145/1242572.1242816</idno>
	</analytic>
	<monogr>
		<title level="m" coord="18,112.66,344.40,308.59,10.91">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1295" to="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,371.50,393.33,10.91;18,112.66,385.05,393.69,10.91;18,111.79,398.60,258.84,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="18,264.58,371.50,241.41,10.91;18,112.66,385.05,70.35,10.91">Clustering billions of images with large scale nearest neighbor search</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Rowley</surname></persName>
		</author>
		<idno type="DOI">10.1109/WACV.2007.18</idno>
	</analytic>
	<monogr>
		<title level="m" coord="18,229.47,385.05,276.88,10.91;18,111.79,398.60,14.98,10.91">IEEE Workshop on Applications of Computer Vision (WAC V &apos;07)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="28" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,412.15,394.53,10.91;18,112.66,425.70,22.69,10.91" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">√Ö</forename><surname>Nielsen</surname></persName>
		</author>
		<title level="m" coord="18,172.05,412.15,330.54,10.91">A new ANEW: Evaluation of a word list for sentiment analysis in microblogs</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,439.25,393.33,10.91;18,112.66,452.79,382.78,10.91" xml:id="b21">
	<monogr>
		<title level="m" type="main" coord="18,352.90,439.25,153.09,10.91;18,112.66,452.79,181.08,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,466.34,394.62,10.91;18,112.66,479.89,150.33,10.91" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/pdf/1905.05583v3" />
		<title level="m" coord="18,258.65,466.34,197.31,10.91">How to fine-tune bert for text classification?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,493.44,393.33,10.91;18,112.66,506.99,250.42,10.91" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="18,228.64,493.44,277.34,10.91;18,112.66,506.99,122.59,10.91">Natural language processing with Python: analyzing text with the natural language toolkit</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>O&apos;Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,520.54,393.54,10.91;18,112.66,534.09,159.51,10.91" xml:id="b24">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Alani</surname></persName>
		</author>
		<title level="m" coord="18,299.17,520.54,207.02,10.91;18,112.66,534.09,127.59,10.91">On stopwords, filtering and data sparsity for sentiment analysis of twitter</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,547.64,393.33,10.91;18,112.66,561.19,281.80,10.91" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="18,159.38,547.64,180.50,10.91">An overview of the tesseract ocr engine</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="18,350.05,547.64,155.94,10.91;18,112.66,561.19,219.94,10.91">Ninth international conference on document analysis and recognition</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>ICDAR 2007)</note>
</biblStruct>

<biblStruct coords="18,112.66,574.74,393.32,10.91;18,112.66,588.29,393.33,10.91;18,112.66,601.84,397.48,10.91;18,112.36,617.83,127.03,7.90" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="18,377.36,574.74,128.62,10.91;18,112.66,588.29,318.92,10.91">Meme opinion categorization by using optical character recognition (ocr) and na√Øve bayes algorithm</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Amalia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Haisar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Gunawan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">B</forename><surname>Nasution</surname></persName>
		</author>
		<idno type="DOI">10.1109/iac.2018.8780410</idno>
	</analytic>
	<monogr>
		<title level="m" coord="18,480.66,588.29,25.33,10.91;18,112.66,601.84,271.31,10.91">Third International Conference on Informatics and Computing (ICIC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,628.93,394.52,10.91;18,112.48,642.48,199.00,10.91" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="18,193.17,628.93,309.44,10.91">Research on k-value selection method of k-means clustering algorithm</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.3390/j2020016</idno>
	</analytic>
	<monogr>
		<title level="j" coord="18,112.48,642.48,3.51,10.91">J</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="226" to="235" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,656.03,395.17,10.91;18,112.66,669.58,395.01,10.91;19,112.66,86.97,174.74,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="18,221.40,656.03,149.01,10.91">Bert: a sentiment analysis odyssey</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Alaparthi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mishra</surname></persName>
		</author>
		<idno type="DOI">10.1057/s41270-021-00109-8</idno>
		<ptr target="https://link.springer.com/article/10.1057/s41270-021-00109-8.doi:10.1057/s41270-021-00109-8" />
	</analytic>
	<monogr>
		<title level="j" coord="18,378.96,656.03,128.88,10.91;18,112.66,669.58,12.06,10.91">Journal of Marketing Analytics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="118" to="126" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,100.52,393.33,10.91;19,112.66,114.06,375.81,10.91" xml:id="b29">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">L</forename><surname>Gwet</surname></persName>
		</author>
		<title level="m" coord="19,168.32,100.52,337.67,10.91;19,112.66,114.06,227.76,10.91">Handbook of Inter-Rater Reliability, 4th Edition: The Definitive Guide to Measuring The Extent of Agreement Among Raters</title>
		<imprint>
			<publisher>Advanced Analytics, LLC</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="19,112.66,127.61,394.53,10.91;19,112.66,141.16,393.33,10.91;19,112.66,154.71,394.51,10.91;19,112.66,170.70,123.08,7.90" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="19,327.46,127.61,175.13,10.91">TIRA Integrated Research Architecture</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-22948-1_5</idno>
	</analytic>
	<monogr>
		<title level="m" coord="19,240.99,141.16,264.99,10.91;19,112.66,154.71,123.97,10.91">Information Retrieval Evaluation in a Changing World, The Information Retrieval Series</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
