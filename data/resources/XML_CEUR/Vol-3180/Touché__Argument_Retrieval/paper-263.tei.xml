<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.78,84.74,223.83,15.42;1,89.29,106.66,370.32,15.42;1,89.29,128.58,96.15,15.43;1,89.29,150.91,324.97,11.96">Touch√© -Task 1 -Team Korg: Finding pairs of argumentative sentences using embeddings Notebook for the Touch√© Lab on Argument Retrieval at CLEF 2022</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,176.82,48.41,11.96"><forename type="first">Cuong</forename><surname>Vo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Leipzig University</orgName>
								<address>
									<addrLine>Augustusplatz 10</addrLine>
									<postCode>04109</postCode>
									<settlement>Leipzig</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,165.35,176.82,69.62,11.96"><forename type="first">Florian</forename><surname>Reiner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Leipzig University</orgName>
								<address>
									<addrLine>Augustusplatz 10</addrLine>
									<postCode>04109</postCode>
									<settlement>Leipzig</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,247.61,176.82,106.66,11.96"><forename type="first">Immanuel</forename><surname>Von Detten</surname></persName>
							<email>i.vondetten@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Leipzig University</orgName>
								<address>
									<addrLine>Augustusplatz 10</addrLine>
									<postCode>04109</postCode>
									<settlement>Leipzig</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,385.27,176.82,61.81,11.96"><forename type="first">Fabian</forename><surname>St√∂hr</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Leipzig University</orgName>
								<address>
									<addrLine>Augustusplatz 10</addrLine>
									<postCode>04109</postCode>
									<settlement>Leipzig</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Text Mining and Retrieval Group TEMIR</orgName>
								<orgName type="institution">Leipzig University</orgName>
								<address>
									<settlement>Leipzig</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.78,84.74,223.83,15.42;1,89.29,106.66,370.32,15.42;1,89.29,128.58,96.15,15.43;1,89.29,150.91,324.97,11.96">Touch√© -Task 1 -Team Korg: Finding pairs of argumentative sentences using embeddings Notebook for the Touch√© Lab on Argument Retrieval at CLEF 2022</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">F782AA6F03A572608CF6CCB3D34DB7C2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information retrieval</term>
					<term>Argument retrieval</term>
					<term>Touch√© Task 1</term>
					<term>CLEF 2022</term>
					<term>Semantic search</term>
					<term>Doc2Vec</term>
					<term>Sentence BERT</term>
					<term>GPT-2</term>
					<term>Text generation</term>
					<term>Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This notebook outlines the experiments and results for Task 1 of the Touch√© Lab on Argument Retrieval at CLEF 2022 by Team Korg. ElasticSearch serves as our baseline to index the args.me corpus, extended by preprocessing steps of filtering, stemming, a custom stop-word list and WordNet-based synonyms to enrich documents. We approach the problem of finding coherent pairs of argumentative sentences by using and comparing two embedding methods, namely Doc2Vec and Sentence BERT for semantic search. In our first iteration, we use a custom-trained model for Doc2Vec and the out-of-the-box functionality of SBERT for semantic search. To refine the retrieval of meaningful sentence pairs, we incorporate the text generation functionality of GPT-2 to generate prompts as an input for the sentence embeddings. After evaluating those approaches with the Normalized Discounted Cumulative Gain and using an annotated dataset of Touch√© 2021, we identify Doc2Vec without text generation and a revised algorithm to match sentence pairs as our best performing approach for retrieval of argumentative sentences.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Today, an ever-increasing pace of news coverage can be observed, while opinions regarding controversial topics are becoming more polarized. It is important to be exposed to different views in order to form your own opinion. While it is possible to search the World Wide Web for virtually every topic, it can be quite difficult to find arguments with relevant sources, which portray different point of views. Understanding the reasoning behind two opposing points of views can be facilitated by information retrieval systems. Wachsmuth et al. <ref type="bibr" coords="1,469.91,545.96,12.85,10.91" target="#b0">[1]</ref> built an argument search engine which relies on a openly accessible index of nearly 300k web scraped arguments from different debate portals. The Touch√© Lab revolves around information retrieval for the arguments of the args.me corpus <ref type="bibr" coords="1,317.42,586.61,11.58,10.91" target="#b1">[2]</ref>. This years Touch√© Task 1 <ref type="bibr" coords="1,456.19,586.61,11.58,10.91" target="#b2">[3]</ref>, held at the annual CLEF conference <ref type="bibr" coords="2,221.12,86.97,11.58,10.91">[4]</ref>, is about retrieving a pro and a con argument from the corpus and forming strong argumentative sentence pairs for each of them. Our team name is "Korg".</p><p>In this paper we outline the pipeline for our argument retrieval system and the two different models of sentence embeddings we use to fulfill the task. Our argument retrieval model is based on the outcome of the last year's Touch√© 2021 <ref type="bibr" coords="2,321.97,154.71,11.36,10.91" target="#b3">[5]</ref>, more precisely on the results of Team Elrond. Following their approach, we use ElasticSearch for our index and retrieval with the DirichletLM similarity, described in Section 3.1 <ref type="bibr" coords="2,302.65,181.81,11.50,10.91" target="#b4">[6]</ref>. In order to find a pair of fitting sentences for the retrieved result, we compare two semantic search approaches. In Section 4.2 we identify Doc2Vec <ref type="bibr" coords="2,130.85,208.91,12.84,10.91" target="#b5">[7]</ref> and Sentence-BERT (SBERT) <ref type="bibr" coords="2,277.23,208.91,12.84,10.91" target="#b6">[8]</ref> as promising models using sentence embedding. We find a relevant sentence and match it with another sentence to form a coherent pair by embedding the sentences and computing the cosine similarity of the two sentences.</p><p>Sentence embeddings are designed to find sentences with the highest similarity, therefore we have to counteract the problem of retrieving sentences with identical meaning. Therefore we develop a second approach in Section 4. <ref type="bibr" coords="2,266.61,290.20,4.17,10.91" target="#b2">3</ref> with the text generation function from GPT-2 <ref type="bibr" coords="2,478.49,290.20,11.40,10.91" target="#b7">[9]</ref>. In this approach, we take the first retrieved sentence and give it to GPT-2 as an opening text (called prompt). Then the model generates a subsequent text output which gets passed to Doc2Vec and SBERT. With that, these models find a similar sentence in their embeddings to the generated output and form a sentence pair with the first sentence. In Section 5 we evaluate our different experiments and decide on which pipeline we use for our submission for this years Touch√© lab.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The system described in this paper is intended to run on the TIRA platform <ref type="bibr" coords="2,421.08,416.58,16.30,10.91" target="#b8">[10,</ref><ref type="bibr" coords="2,440.08,416.58,12.23,10.91" target="#b9">11]</ref>. TIRA is an online platform where researches can upload and evaluate their proposed solutions for shared tasks. Shared tasks are an increasingly popular form of solving open problems in academia. Shared tasks are often conducted in the scope of conferences, especially in the field of natural language processing or machine learning <ref type="foot" coords="2,276.81,469.02,3.71,7.97" target="#foot_0">1</ref> . The organizers usually contribute large datasets which can be used by the teams of researchers to tackle diverse problems. For the shared task of the Touch√© Lab, TIRA ensures that all solutions are running with the same data and produce reproducible results.</p><p>With web search engines it can be harder to get an overview about different opinions on a topic than to get the answer for a clearly stated question <ref type="bibr" coords="2,359.96,538.52,16.41,10.91" target="#b10">[12]</ref>. The field of computational argumentation deals with this and other problems by mining arguments from unstructured text and computational representation of arguments. It is essential for improving search results involving arguments <ref type="bibr" coords="2,182.34,579.17,17.76,10.91" target="#b11">[13]</ref>  <ref type="bibr" coords="2,202.67,579.17,16.08,10.91" target="#b12">[14]</ref>. There are also approaches to create a World Wide Argument Web based on structured arguments in an Argument Interchange Format (AIF) <ref type="bibr" coords="2,411.13,592.72,16.17,10.91" target="#b13">[15]</ref>. In this paper we focus on the results of previous teams participating in the Touch√© Lab, especially of Task 1 of last years shared task <ref type="bibr" coords="2,185.73,619.81,11.30,10.91" target="#b3">[5]</ref>. The task was to retrieve arguments for "controversial questions from a focused collection of debates to support opinion formation on topics of social importance" <ref type="bibr" coords="2,492.63,633.36,11.28,10.91" target="#b3">[5]</ref>.</p><p>This years task is extending on this and asks the participating teams to additionally retrieve a pair of argumentative sentences for positive and negative stances of an argument. Solutions from Touch√© 2021 covered a wide range of approaches and performance for the task. To retrieve arguments, the retrieval model based on DirichletLM proved to perform better than other models <ref type="bibr" coords="3,124.51,141.16,16.41,10.91" target="#b14">[16]</ref>. Multiple teams worked on evaluating the impact of preprocessing by means of query expansion, word stemming and WordNet-based synonyms. The approaches involved different teams choosing semantic search as a method to retrieve relevant results. Preprocessing proved to be a important step to increase relevance and quality of the search results. Semantic search delivered mixed results, which were influenced by the choice of model and training data.</p><p>To compare sentences from arguments for the sentence pair retrieval two technologies are used in this paper: Sentence BERT (SBERT) <ref type="bibr" coords="3,288.68,222.46,12.99,10.91" target="#b6">[8]</ref> and Doc2Vec <ref type="bibr" coords="3,366.23,222.46,11.58,10.91" target="#b5">[7]</ref>. SBERT is a modification of the BERT network, a neural network designed for natural language processing (NLP) tasks like language modeling and next sentence prediction <ref type="bibr" coords="3,325.16,249.56,16.17,10.91" target="#b15">[17]</ref>. SBERT has a better performance on sentence-pair regression tasks and can for example be used to find similar sentence pairs in a huge collection of sentences. Doc2Vec is based on Word2Vec, a neural network with word embeddings in vector space, where vectors are representing words. The embedding is designed to solve problems like calculating the similarity of words. This is usually done by measuring the cosine similarity of the respective vectors <ref type="bibr" coords="3,276.73,317.30,16.43,10.91" target="#b16">[18,</ref><ref type="bibr" coords="3,295.88,317.30,12.32,10.91" target="#b17">19]</ref>. Doc2Vec extends the vector representation from words to documents of arbitrary size.</p><p>The generative pre-trained transformer (GPT-2) is a language model developed by OpenAI<ref type="foot" coords="3,89.29,356.20,3.71,7.97" target="#foot_1">2</ref> which uses a deep neural network to perform several text-related tasks like translation or text summarization <ref type="bibr" coords="3,178.93,371.50,11.47,10.91" target="#b7">[9]</ref>. In this paper GPT-2 is used to generate follow-up sentences based on sentences from arguments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodological Approach</head><p>In the following section, the methods we use will be described briefly. To provide context, we introduce our pipeline and present the concept of semantic search and sentence embeddings. We rely on these concepts to match sentences which should represent an argument in its entirety and should form a coherent pair at the same time. Then, we will cover the technologies we use in detail: ElasticSearch, Doc2Vec, SBERT and GPT-2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Indexing</head><p>We use ElasticSearch to create an index of all arguments from the args.me corpus. In order to improve the out-of-the-box functionality of ElasticSearch we implement a preprocessing pipeline before building the index. For optimal results, we follow the approach from group Elrond from Touch√© 2021. They performed well in relevance and quality scores, and were placed among the best in both categories. They used a combination of several preprocessing steps, which are: filters, namely Asciifolding and Lowercase, and stemming with the Krovetz algorithm <ref type="bibr" coords="3,136.14,628.89,11.41,10.91" target="#b3">[5,</ref><ref type="bibr" coords="3,150.28,628.89,12.36,10.91" target="#b18">20]</ref>. Then we add an additional step and remove stop words with a custom stop word list. As the last step in our pipeline we follow the approach of Team Elrond again and enrich our documents with WordNet-based synonyms <ref type="bibr" coords="4,333.34,100.52,16.43,10.91" target="#b19">[21,</ref><ref type="bibr" coords="4,352.49,100.52,7.57,10.91" target="#b3">5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Retrieval</head><p>To retrieve arguments based on the given query, we also use ElasticSearch. We use the LM Dirichlet similarity to score matching documents because previous contributions from Touch√© which used ElasticSearch reported best results with LM Dirichlet similarity <ref type="bibr" coords="4,428.35,177.34,16.48,10.91" target="#b20">[22,</ref><ref type="bibr" coords="4,447.56,177.34,7.49,10.91" target="#b3">5,</ref><ref type="bibr" coords="4,457.78,177.34,12.36,10.91" target="#b21">23]</ref>. Figure <ref type="figure" coords="4,89.04,190.89,5.17,10.91">1</ref> provides an overview of the pipeline of our retrieval system up to this point: The args.me corpus serves as a base for indexing and we search for relevant arguments using ElasticSearch. We use the top results as a starting point to find sentences which form a coherent pair. For this, we experiment with different methods which we introduce in the following sections.</p><p>Figure <ref type="figure" coords="4,120.66,454.42,3.82,10.91">1</ref>: The setup of the retrieval system to retrieve relevant arguments up to a point where an initial sentence can be used to find a pair of argumentative sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Semantic Search</head><p>Semantic search describes the process of finding meaning in a text. This meaning could refer to different parts of the search process, like understanding the query, the data or representing knowledge in a suitable way for retrieval <ref type="bibr" coords="4,270.32,544.79,16.09,10.91" target="#b22">[24]</ref>. So instead of the more traditional search engines which try to find documents based on lexical matches, semantic search can also find synonyms <ref type="bibr" coords="4,89.29,571.89,16.25,10.91" target="#b23">[25]</ref>. The basic idea is that known data is embedded into a vector space, as seen in Figure <ref type="figure" coords="5,468.67,264.46,3.66,10.91" target="#fig_0">2</ref>. A text of arbitrary length is represented in the vector space. In the same way, a search query can be embedded into the same vector space and a vector can be inferred for the new data. It is possible to find the closest entry by calculating the euclidean distance between arbitrary vectors. This entry should then have a high semantic overlap with the query <ref type="bibr" coords="5,375.58,318.66,16.33,10.91" target="#b23">[25]</ref>. We use semantic search to compare the sentences we want to pair with the goal to find the most similar sentences. Next, we describe the two methods we used to retrieve sentence pairs: Doc2Vec and Sentence BERT. The two methods are based on the aforementioned concept of embedding documents of different length, in our case sentences, into a vector space and finding documents which are similar in their semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Doc2Vec</head><p>Doc2Vec is an unsupervised machine learning model that represents documents by a dense vector. It was first introduced in 2014 from Mikolov and Le and builds upon Word2Vec <ref type="bibr" coords="5,477.13,449.28,16.44,10.91" target="#b24">[26,</ref><ref type="bibr" coords="5,496.30,449.28,7.58,10.91" target="#b5">7]</ref>. Word2Vec assumes that words can be embedded into the vector space and the resulting vectors can be used to measure semantic similarity. This understanding of semantic similarity or meaning is based on the bag-of-word model. Doc2Vec aims to overcome the major weaknesses of bag-of-word models: Assuming a sentence can be represented by their respective bag of words implies that the meaning is independent from the word order. Therefore, when using Word2Vec, the word ordering within the respective document is lost. Additionally, the context and semantics of the words are usually ignored and semantic features of sentences like negation or irony or cannot be taken into account <ref type="bibr" coords="5,270.33,557.67,16.14,10.91" target="#b24">[26]</ref>. To retrieve argumentative sentences for positive and negative stances, it is needed to represent this context. Using Word2Vec leads to a scenario where different sentences could have the same representation as long as those sentences contain the same words.</p><p>To solve this issue and take the context of the sentence as well as the word order into account, Mikilov and Le add a new vector to a document of variable length. The document vectors serve as information about the context of a word when using the embedding. It is combined with the respective word vectors of the document by averaging or concatenating. The Paragraph id, as seen in Figure <ref type="figure" coords="6,168.21,86.97,3.81,10.91" target="#fig_1">3</ref>, serves as a representation of the context for the paragraph or document, respectively. This document vector D is then used for the training of the word vectors W and holds the document representation. The authors report improvement for information retrieval and classification over other methods based on word embeddings and bigram embeddings alone. We choose Doc2Vec to retrieve coherent sentence pairs because the meaning of documents of variable length, in our case sentences, is represented and takes the sentence as a whole into account instead of every word, independent from the context. Therefore the similarity score produced by Doc2Vec promises to result in sentences which are similar in meaning rather than semantic similarity only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Sentence BERT</head><p>BERT is a language representation model that was first introduced in 2018. For creating this language model, BERT uses deep bidirectional representations from unlabeled text. Bidirectional indicates that the model predicts a word based on what words it precedes and follows. With this approach, BERT managed to achieve success on several state-of-the-art tasks. This includes tasks like question answering and language inference <ref type="bibr" coords="6,330.10,534.09,11.43,10.91" target="#b6">[8]</ref>.</p><p>For semantic similarity search, on the other hand, BERT is not so well suited. This is due to BERTs architecture, which comes with a high computational overhead. To overcome this bottleneck, Sentence-BERT (SBERT) was introduced in 2019 <ref type="bibr" coords="6,361.97,574.74,16.36,10.91" target="#b23">[25]</ref>. SBERT is a modification of the existing pretrained BERT network, which allows us "to derive semantically meaningful sentence embeddings" <ref type="bibr" coords="6,187.93,601.84,16.09,10.91" target="#b23">[25]</ref>. This is achieved by adding a pooling operation to the output of BERT and results in a fixed sized sentence embedding. Furthermore, siamese and triplet networks are created to fine-tune BERT and ensure that the produced embeddings are "semantically meaningful and can be compared with cosine-similarity" <ref type="bibr" coords="6,344.48,642.48,16.43,10.91" target="#b25">[27,</ref><ref type="bibr" coords="6,363.63,642.48,12.32,10.91" target="#b23">25]</ref>.</p><p>SBERT is easily accessible over a python framework, which is based on PyTorch and Transformers <ref type="bibr" coords="6,126.06,669.58,16.30,10.91" target="#b26">[28,</ref><ref type="bibr" coords="6,145.07,669.58,12.23,10.91" target="#b27">29]</ref>. The framework allows the use of different pretrained models to train sentence embeddings on over 100 languages <ref type="bibr" coords="7,250.95,86.97,16.42,10.91" target="#b23">[25]</ref>. The model we use (all-MiniLM-L12-v2) is based on Microsoft's MiniLM and finetuned on more than 1 billion sentence pairs <ref type="bibr" coords="7,413.27,100.52,16.44,10.91" target="#b28">[30,</ref><ref type="bibr" coords="7,432.43,100.52,12.33,10.91" target="#b23">25]</ref>. The authors describe it as a general purpose model. This was a reason for us to not fine-tune the model any further for the training of our sentence embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">GPT-2</head><p>GPT-2 is the abbreviation for Generative Pre-trained Transformer 2 which is an unsupervised transformer language model created by OpenAI in 2019. It is most used to translate texts, answer questions, summarize passages and to generate text output <ref type="bibr" coords="7,389.37,204.44,16.26,10.91" target="#b29">[31]</ref>. OpenAI web scraped 45 million outbound links from the social media platform Reddit to gather training data for the GPT-2 model. In order to assess the quality of a shared URL they used links which had at least 3 upvotes from the community. The final corpus consists of slightly over 8 million documents for a total of 40 GB of text <ref type="bibr" coords="7,194.27,258.64,11.58,10.91" target="#b7">[9]</ref>. The architecture of GPT-2 is based on Transformer <ref type="bibr" coords="7,446.53,258.64,16.41,10.91" target="#b27">[29]</ref>. OpenAI released four different model sizes of GPT-2 for free use which are compared in Table 1 <ref type="bibr" coords="7,479.51,272.19,11.43,10.91" target="#b7">[9]</ref>. We use GTP-2 to generate follow-up sentences for the initially retrieved sentences from the corpus with the goal of finding more coherent sentence pairs. This leads to the pipeline displayed in Figure <ref type="figure" coords="7,177.37,441.43,3.76,10.91" target="#fig_2">4</ref>, where we use the generated sentence from GTP-2 as input to retrieve a similar sentence with Doc2Vec and SBERT. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In the following sections we describe our approach in more detail and explain our experiments. We outline the pipeline for our retrieval system, as well as the setup and preprocessing steps. This includes a detailed description of the data we rely on and our steps to create an index. ElasticSearch serves as a baseline for indexing and retrieval of arguments. Then we describe our process of developing our pipeline with Doc2Vec, SBERT and text generation with GPT-2 from a prototype to a more refined system for retrieving argumentative sentences. In the next section we evaluate the results of the different approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup: Dataset, Indexing and Argument Retrieval</head><p>For our experiments, we used the args.me corpus <ref type="bibr" coords="8,304.53,242.30,11.28,10.91" target="#b1">[2]</ref>. This corpus consists of 387 606 arguments.</p><p>Each argument consists of an id, conclusion, premises (modeled as stance and text), context and sentences (modeled as sourceTitle, sourceId, nextArgumentInSourceId, sourceUrl, discussionTitle, previousArgumentInSourceId, acquisitionTime). The arguments were crawled from four different platforms, namely: Debatewise, IDebate.org, Debatepeia and Debate.org. As described in Section 3.1 and 3.2, we use ElasticSearch for indexing and argument retrieval.</p><p>We work with the configuration used by Team Elrond from the previous Touch√© task 1 <ref type="bibr" coords="8,492.22,323.60,11.58,10.91" target="#b3">[5]</ref>.</p><p>With this setup we can query ElasticSearch to retrieve relevant arguments. Next, we present our approaches to pair fitting sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Approach 1: Semantic Search using Doc2Vec and a matching algorithm</head><p>With this approach, the conclusion and each sentence of a retrieved argument is embedded by Doc2Vec. The Doc2Vec model was trained on the corpus of args.me <ref type="bibr" coords="8,382.82,413.97,11.28,10.91" target="#b1">[2]</ref>. We choose one sentence as a document in the context of Doc2Vec and train the model on all sentences of the corpus. We preprocess the sentences with two filters: First, we remove all sentences with less than three words, because they carry very little to no meaning. Then, we remove all sentences containing a hyperlink, because in most cases these sentences refer to a source to support their argument, but carry no meaning in themselves. For matching two sentences, we first experiment with a naive approach to look for the most similar sentence to the conclusion in the whole corpus. The conclusion is inferred as a vector in our Doc2Vec model and the most similar sentence from the corpus is located by computing the cosine similarity of the inferred vector. The quality of the matches is varying immensely and the two sentences only supported each other in a few cases, because the sentences from the whole corpus are retrieved and are often seemingly similar, but covering very different topics. Therefore we only look for matching sentences within the sentences of an argument.</p><p>To get the most meaningful sentences and to ensure the sentences are forming a coherent pair, we develop the following matching algorithm: All sentences of an argument are matched with each other and the cosine similarity of the respective vectors is calculated. Then, we calculate the cosine similarity for each sentence to the conclusion. Here, our reasoning to compare each sentence to the conclusion is the following: We assume the conclusion carries the summarized meaning of the argument, therefore we want to retrieve sentences which are most similar to the conclusion and carry the most meaning or the most important part of the argument, respectively. In a last step, we average the distance between the two sentences and the distance of each of the two sentences to the conclusion. Then the sentence pair with the highest averaged cosine similarity forms our top argumentative sentence pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Approach 2: Text-Generation</head><p>As an alternative approach we use GPT-2's text generation functionality to find a second sentence. We use the sentence from the results of ElasticSearch as input for GPT-2. It generates a subsequent text output. This output is used as input for semantic search in our Doc2Vec and SBERT models. In the end the most similar sentence to the generated output will be matched with the initial sentence and form a sentence pair. The idea is that GPT-2 can generate a sentence which is a coherent follow-up to the first sentence and that our two models can find the most similar sentence to the generated one.</p><p>Just using the first argument as an opening text (referred to as prompt) leads to mixed quality of the generated texts. In order to stabilize the quality we follow the approach of Akiki and Potthast from the Touch√© Lab 2020 <ref type="bibr" coords="9,243.21,299.28,16.08,10.91" target="#b21">[23]</ref>. They used the text generation functionality for a query expansion to achieve better retrieval results. By embedding the query in an argumentative structure, as seen in Table <ref type="table" coords="9,207.21,326.38,3.72,10.91">2</ref>, they steered the language model into a output which resembles a opinion more closely than a simple statement <ref type="bibr" coords="9,293.49,339.93,16.19,10.91" target="#b30">[32]</ref>. To represent arguments for the same topic, but with opposing stances, they created positive, negative and neutral prompts as shown in Table <ref type="table" coords="9,116.23,367.03,3.80,10.91">2</ref>. This stabilizes the text generation and leads to results which carry a stronger pro or con sentiment. Using the same prompts as Akiki and Potthast, we insert the conclusion from the respective argument into the outlined argumentative structure seen in Table <ref type="table" coords="9,447.39,394.13,5.05,10.91">2</ref> and use the modified prompt as input for GPT-2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Prompts for different stances to get a variety of argumentative output. The pattern of a question, the argument and an affirmative or dissenting beginning of a sentence lead to texts which closer to opinions in an arguments than factual statements.</p><p>Further the choice of the decoding method of GTP-2, which decodes the representation from the model into text, and their respective parameters are important for the outcome of the text output. These methods decide how incoherent, repetitive or generic the generated text is by examining which token could follow another token <ref type="bibr" coords="10,314.74,127.61,16.08,10.91" target="#b31">[33]</ref>. For our purpose we used the following three sampling methods for decoding as they provided the best output:</p><p>Temperature Sampling With this approach, a probability distribution is shaped through temperature <ref type="bibr" coords="10,148.28,183.47,16.41,10.91" target="#b32">[34]</ref>. The parameter ùë° ‚àà [0, 1) steers the distribution towards high probability tokens or low probability tokens. A low value improves the generation quality but decreases the diversity of the output. A high value regularizes the generation by making the model less certain of its top choices <ref type="bibr" coords="10,200.77,224.12,16.43,10.91" target="#b30">[32,</ref><ref type="bibr" coords="10,219.93,224.12,12.32,10.91" target="#b33">35]</ref>. We used a temperature of 1.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Top-K Sampling</head><p>In this sampling method, the neural language model distribution gets truncated to a set of size ùëò of most likely tokens. These tokens will then be filtered and the probability mass is redistributed among those ùëò next tokens <ref type="bibr" coords="10,358.62,279.98,16.43,10.91" target="#b33">[35,</ref><ref type="bibr" coords="10,377.77,279.98,12.32,10.91" target="#b34">36]</ref>. We chose a ùëò of 75.</p><p>Nucleus Sampling Nucleus sampling is a stochastic decoding method. Like top-ùëò sampling, this method is also truncating the language model distribution. It chooses from the smallest possible set of words whose cumulative probability exceeds the probability ùëù. The rest of the tokens get discarded afterwards. It is also called top-ùëù sampling <ref type="bibr" coords="10,373.38,349.38,16.20,10.91" target="#b33">[35]</ref>. We set the probability to ùëù = 0, 6.</p><p>With these 3 decoding strategies and the six prompts we generate 18 text outputs with a length of 75 tokens for each passed argument. In order to figure out which generated output would fit best to the passed argument, we compute the cosine similarity for each sentence with the first argument. Some cosine similarity scores are quite high because GPT-2 was just repeating the passed argument in it's output (Table <ref type="table" coords="10,318.31,445.62,3.55,10.91">3</ref>). Therefore we set a threshold to remove sentences which were too similar to the prompt. Among the remaining generated sentences, the best one is passed to Doc2Vec and BERT for semantic search.</p><p>With this approach we encounter the problem that we always have a conclusion as part of the sentence pair and that some conclusions are quite short, therefore carrying little meaning. To counteract this, we implement the following step in our pipeline:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Doc2vec</head><p>The results of ElasticSearch, the most relevant arguments to a query, are passed on to Doc2Vec. For each sentence of the argument, GPT-2 generates a subsequent text output. Afterwards we compute the cosine similarity of each text output with each sentence of the respective argument and use the most similar sentence pair.</p><p>SBERT The conclusion of the best argument retrieved by ElasticSearch is used as input for SBERT. The language model embeds the conclusion and retrieves the five sentences with the highest semantic similarity. For each of these sentences, GPT-2 generates a subsequent text output. These text outputs are again passed on to SBERT which searches similar sentences. The</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated Output</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Score</head><p>What do you think? We need more sex education in schools. No because that's what the kids do. We need more sex education in schools. Do you agree with the idea that we should have sex education in schools? No. We should have sex education in schools. Do you agree with the idea that we should have sex education in schools? 0.8109 What do you think? We need more sex education in schools. I don't know what to say. I think that sex education is just not enough. If you don't get it right, it's not going to work. I think that if we are really concerned about the sex education system, we need to have a national sex education program, where all ... 0.7838 What do you think? We need more sex education in schools. Not sure ive seen any evidence that sex education is helping to reduce sexual violence. 0.7396</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 3</head><p>This table displays an example of the top three out of 18 sentences ordered by the cosine similarity to the query "sex education in schools". The expanded query is italic and everything following is generated by GPT-2.</p><p>result of SBERT are then matched with the first argument and the most similar sentence pair form the desired argumentative sentence pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation And Discussion</head><p>We use a a two-step approach for our own evaluation of the results of our retrieval system. In the first step we evaluate the results of the argument retrieval and in a second step we evaluate the sentence pairs generated from the results of the argument retrieval. This ensures that the sentence pairs are generated out of a well evaluated baseline of argument retrieval. We use the mean Normalized Discounted Cumulative Gain (NDCG) to measure the quality for both steps over several topics <ref type="bibr" coords="11,197.46,491.37,16.09,10.91" target="#b35">[37]</ref>. This measure is also used to compare the different approaches for the Touch√© 2021 tasks for argument retrieval <ref type="bibr" coords="11,290.10,504.92,11.38,10.91" target="#b3">[5]</ref>. So we are also able to compare our results of the argument retrieval to the results of the previous results of Touch√© 2021. We also evaluate the system in the context of this years Touch√© Lab. The results from the TIRA platform will be discussed at the end of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Argument retrieval evaluation</head><p>We use a dataset with graded results for 50 example topics for the evaluation of the results from the argument retrieval. The dataset was also used for the relevance judgments for the Touch√© 2021 task 1 <ref type="bibr" coords="11,177.73,622.39,16.41,10.91" target="#b36">[38]</ref>. The entries in this dataset contain a topic id, the argument id and a grade which indicates how relevant a argument is for a topic. The grades can have the values -2 non-argument (spam) and from 0 (not relevant) to 3 (highly relevant).</p><p>In Table <ref type="table" coords="12,138.44,86.97,5.00,10.91">4</ref> the results for different approaches on argument retrieval are displayed. The baseline retrieval uses the fields conclusion, premises.text and sentences.sent_text from the argument corpus with the weight of one for each field and LMDirichlet as retrieval model <ref type="bibr" coords="12,454.29,114.06,11.58,10.91" target="#b4">[6]</ref>. For the optimized baseline approach the weights of the fields which are used for retrieval are optimized (in a range from zero to three) with a bayesian optimizer <ref type="bibr" coords="12,336.80,141.16,17.84,10.91" target="#b37">[39]</ref>. The measure of the optimization is the Mean NDCG value as described before. The expanded index approach is the one which is described in Section 3.1. In the optimized expanded index approach the weights for the search fields are also optimized with a bayesian optimizer and in the heavily optimized expanded approach the optimization is made with more iterations but as described in Table <ref type="table" coords="12,451.65,195.36,5.05,10.91">4</ref> the NDCG values are staying the same.</p><p>Compared to the Touch√© 2021 task 1 approaches with the best relevance scores (Elrond with a NDCG@5 value of 0.720 and Pippin Took with a NDCG@5 value of 0.705) the optimized expanded index approach (The expanded index approach from section 3.1 with optimized weigths of the search fields) scores slightly better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Mean NDCG Mean NDCG@5 Baseline 0.8385 0.6754 Baseline, optimized 0.8559 0.7116 Expanded Index 0.8506 0.7166 Expanded Index, optimized 0.8624 0.7417 Expanded Index, heavily optimized 0.8624 0.7417</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4</head><p>The Mean NDCG and NDCG@5 values for different approaches on argument retrieval</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Sentence pair evaluation</head><p>For the evaluation of the retrieved sentence pairs the evaluation data has to be created manually. Therefore a dataset similar to the dataset for the argument retrieval evaluation is created based on the retrieved sentence pairs for a topic. Then the results are reviewed by hand and are graded by the relevance to the topic and how good the sentences fit together in an argumentative way.</p><p>To keep the effort within an acceptable range the evaluation is made for ten top results of each of ten topics. As shown in Table <ref type="table" coords="12,181.76,537.07,4.97,10.91">5</ref> the Doc2Vec method without text generation from GPT-2 returns the best results for the sentence pair retrieval. The baseline approach uses random pairs of sentences of the first ten results. It is striking that the Bert approach performs even worse than the random pairs approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 5</head><p>The Mean NDCG and NDCG@5 values for different approaches of sentence pair retrieval</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Final evaluation on TIRA</head><p>In the previous section, the results of the teams own evaluation were described. The retrieval of sentence pairs is also evaluated in the context of Touch√© 2022 <ref type="bibr" coords="13,357.56,245.69,11.28,10.91" target="#b2">[3]</ref>. For this, we are provided with the TIRA platform <ref type="bibr" coords="13,175.78,259.24,16.41,10.91" target="#b9">[11]</ref>. Researchers can submit their solution for shared tasks and evaluate the results on TIRA. We choose the configuration which performed best in our own evaluation, that is Doc2Vec without GTP-2 text generation. The evaluation results for our submitted run, named korg9000, can be seen in Table <ref type="table" coords="13,258.32,299.89,3.74,10.91">6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 6</head><p>The evaluation results for the run on the TIRA platform by category</p><p>Our retrieval system performed poorly both in comparison to our own evaluation and to the results of the other participants of the shared task. The Mean NDCG@5 for relevance is 0.252, compared to the best performing run from Team Porthos with a Mean NDCG@5 of 0.742. The Mean NDCG@5 for quality is 0.453. The best performing team in this category is Daario Naharis with a Mean NDCG@5 of 0.913. For the coherence evaluation, our run scores a Mean NDCG@5 of 0.168, while the best performing team, again Daario Naharis, has a score of 0.458.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Discussion of the final results</head><p>The approach of our experiments outlined in this paper is to compare different types of semantic search. Within our own evaluation, we were able to compare the retrieved sentence pairs of Doc2Vec and SBERT. The results were mixed in both cases. This can be attributed to the differences between the two methods: While it was easy to train the Doc2Vec model ourselves, the model was not designed to work with short sentences, but with paragraphs in mind. For SBERT, it was outside of the scope of this notebook to customize the training, which could have led to the poor results. We use the same pipeline to generate the input for the semantic search, but we could have benefited from a step-wise evaluation of each step of the pipeline. Furthermore more runs on the TIRA platform with different configurations would have been helpful to test out our approaches. Then the strengths and weaknesses of the respective approaches could have been better analysed and combined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>To solve Touch√© task 1, we developed a system to find pairs of argumentative sentences for positive and negative stances towards an arguement. We worked with the args.me corpus and two different approaches: Doc2Vec and Sentence BERT, and combining both of them with GPT-2, finally reembedding the resulting sentences. For the retrieval of arguments we use ElasticSearch. In the end we compare and evaluate our results with the NDCG and Mean NDCG.</p><p>We show the differences in how the sentences are paired have a great impact on the overall results of the retrieval system. The Doc2Vec approach delivers acceptable results after reworking the sentence pairing algorithm to only use sentences from the respective argument. Furthermore using text generation to improve the input for the sentence embedding models does not provide better results by default. On the contrary, the results for Doc2Vec are better without the additional step of generating a more elaborate prompt. The quality of the sentence pairs depends on the quality of the generated sentences from GPT-2. Using GPT-2 in this context needs more fine-tuning to reduce the randomness in the generated texts. This could be a starting point for further improvements as it is possible to use GPT-2 to generate more than one sentence and check if one of those sentences fits better. This is contrasted by the negative impact on performance by the sentence generation, which slows down the entire pipeline. Doc2Vec without additional text generation delivers the best performance in the context of our own evaluation.</p><p>Unfortunately, Sentence BERT performs really poorly in our experiments. The SBERT approach in combination with the text generation using GPT-2 has a significant lower Mean NDCG/Mean NDCG@5 score in the evaluation than the baseline approach with random sentence pairs. The poor results could stem from not further fine-tuning SBERT or not choosing the right model for this task. The algorithm to match the two desired sentences needs to be revised to produce better results. There is more room for further improvements of our pipeline, like other steps in preprocessing, using part of speech tags or trying to find a subject object relationship between argumentative sentence pairs to get a better understanding of the context of a sentence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,90.97,211.86,414.53,10.91;5,142.44,225.40,351.85,10.91;5,212.73,238.95,211.26,10.91;5,231.64,84.19,132.00,116.40"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A schematic illustration of a vector space in two dimensions. The query of a search, marked as the orange dot, is embedded and relevant documents are closer than irrelevant documents. Figure adapted from [25]</figDesc><graphic coords="5,231.64,84.19,132.00,116.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,92.90,286.64,409.48,10.91;6,148.84,300.19,339.03,10.91;6,195.92,313.74,244.88,10.91;6,141.49,121.18,312.30,154.20"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Doc2Vec's framework for learning paragraph vectors. The Paragraph id represents the context of a paragraph and is used as additional information to calculate embeddings for single words. Figure adapted from [26]</figDesc><graphic coords="6,141.49,121.18,312.30,154.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,104.43,639.77,386.42,10.91;7,295.59,653.32,45.55,10.91;7,142.49,476.23,310.32,152.28"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Continuation of the pipeline including text generation to find a fitting second argument.</figDesc><graphic coords="7,142.49,476.23,310.32,152.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="4,136.07,255.24,323.15,187.91"><head></head><label></label><figDesc></figDesc><graphic coords="4,136.07,255.24,323.15,187.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,88.98,298.53,349.08,97.06"><head>Table 1 Architecture</head><label>1</label><figDesc></figDesc><table coords="7,194.42,298.53,205.89,66.92"><row><cell>Name</cell><cell cols="3">Parameters Layers ùëë ùëöùëúùëëùëíùëô</cell></row><row><cell>SMALL</cell><cell>117M</cell><cell>12</cell><cell>768</cell></row><row><cell>MEDIUM</cell><cell>345M</cell><cell>24</cell><cell>1024</cell></row><row><cell>LARGE</cell><cell>762M</cell><cell>36</cell><cell>1280</cell></row><row><cell>EXTRA LARGE</cell><cell>1542M</cell><cell>48</cell><cell>1600</cell></row></table><note coords="7,142.48,386.72,295.58,8.87"><p><p>hyperparameters for the publicly available four model sizes of GPT-2</p><ref type="bibr" coords="7,426.24,386.72,11.83,8.87" target="#b7">[9]</ref> </p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,660.07,398.44,8.97;2,88.96,671.03,308.78,8.97"><p>for examples of shared tasks on international conferences, see https://www.statmt.org/wmt21/, https://pan. webis.de/clef19/pan19-web/ and https://alt.qcri.org/semeval2019/index.php?id=tasks</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,108.93,671.00,71.49,8.97"><p>https://openai.com/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Thanks to <rs type="person">Theresa Elstner</rs> for the extensive feedback on the draft of this notebook and to the team of Temir https://temir.org/people.html for supporting us with the upload of our software on the Tira https://tira.io/ platform to evaluate our retrieval system.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="15,112.66,111.28,394.53,10.91;15,112.30,124.83,394.98,10.91;15,112.66,138.38,394.53,10.91;15,112.66,151.93,393.33,10.91;15,112.66,165.48,395.01,10.91;15,112.66,179.03,147.65,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="15,264.52,124.83,221.57,10.91">Building an Argument Search Engine for the Web</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Al-Khatib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Puschmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dorsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Morari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W17-5106" />
	</analytic>
	<monogr>
		<title level="m" coord="15,243.48,151.93,262.50,10.91;15,112.66,165.48,229.05,10.91">4th Workshop on Argument Mining (ArgMining 2017) at EMNLP, Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Ashley</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Green</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Habernal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Litman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Petasis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Reed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Slonim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Walker</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="49" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,192.57,393.53,10.91;15,112.28,206.12,393.71,10.91;15,112.66,219.67,393.73,10.91;15,112.34,233.22,286.03,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,415.08,192.57,91.11,10.91;15,112.28,206.12,167.13,10.91">Data Acquisition for Argument Search: The args.me corpus</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-30179-8_4</idno>
	</analytic>
	<monogr>
		<title level="m" coord="15,112.66,219.67,241.68,10.91">German Conference on Artificial Intelligence (KI 2019)</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Benzm√ºller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Stuckenschmidt</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="48" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,246.77,395.17,10.91;15,112.66,260.32,395.17,10.91;15,112.66,273.87,395.01,10.91;15,112.41,287.42,393.57,10.91;15,112.66,300.97,339.15,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,361.25,260.32,146.58,10.91;15,112.66,273.87,62.60,10.91">Overview of Touch√© 2022: Argument Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fr√∂be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gurcke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,197.77,273.87,309.90,10.91;15,112.41,287.42,309.42,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction. 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<title level="s" coord="15,429.80,287.42,76.18,10.91;15,112.66,300.97,78.83,10.91">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="15,112.66,328.07,394.52,10.91;15,112.66,341.62,393.33,10.91;15,112.66,355.17,394.52,10.91;15,112.66,368.71,393.33,10.91;15,112.66,382.26,393.33,10.91;15,112.41,395.81,395.26,10.91;15,112.41,409.36,397.73,10.91;15,112.36,425.35,158.69,7.90" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,336.55,341.62,169.43,10.91;15,112.66,355.17,38.69,10.91">Overview of Touch√© 2021: Argument Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fr√∂be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-85251-1_28</idno>
		<ptr target="https://link.springer.com/chapter/10.1007/978-3-030-85251-1_28.doi:10.1007/978-3-030-85251-1\_28" />
	</analytic>
	<monogr>
		<title level="m" coord="15,238.75,368.71,267.23,10.91;15,112.66,382.26,328.83,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction. 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="15,150.93,396.83,143.04,9.72">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Candan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="volume">12880</biblScope>
			<biblScope unit="page" from="450" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,436.46,393.33,10.91;15,112.66,450.01,393.33,10.91;15,112.66,463.56,394.53,10.91;15,112.28,477.11,395.00,10.91;15,112.31,490.66,290.29,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,210.88,436.46,295.11,10.91;15,112.66,450.01,128.68,10.91">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<idno type="DOI">10.1145/383952.384019</idno>
		<ptr target="https://doi.org/10.1145/383952.384019.doi:10.1145/383952.384019" />
	</analytic>
	<monogr>
		<title level="m" coord="15,269.12,450.01,236.87,10.91;15,112.66,463.56,390.58,10.91">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;01</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;01<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,504.21,393.32,10.91;15,112.39,517.76,176.99,10.91" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m" coord="15,297.98,504.21,208.00,10.91;15,112.39,517.76,52.97,10.91">Efficient estimation of word representations in vector space</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,531.30,393.33,10.91;15,112.66,544.85,311.37,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="15,326.58,531.30,179.40,10.91;15,112.66,544.85,181.08,10.91">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,558.40,393.33,10.91;15,112.66,571.95,170.84,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="15,407.43,558.40,98.55,10.91;15,112.66,571.95,141.16,10.91">Language models are unsupervised multitask learners</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,585.50,393.32,10.91;15,112.66,599.05,393.33,10.91;15,112.66,612.60,353.89,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="15,296.50,585.50,209.49,10.91;15,112.66,599.05,145.74,10.91">Tira: Configuring, executing, and disseminating information retrieval experiments</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hoppe</surname></persName>
		</author>
		<idno type="DOI">10.1109/DEXA.2012.55</idno>
	</analytic>
	<monogr>
		<title level="m" coord="15,281.47,599.05,224.52,10.91;15,112.66,612.60,125.69,10.91">2012 23rd International Workshop on Database and Expert Systems Applications</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="151" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,626.15,394.53,10.91;15,112.66,639.70,393.33,10.91;15,112.66,653.25,394.51,10.91;15,112.66,669.24,123.08,7.90" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="15,327.46,626.15,175.13,10.91">TIRA Integrated Research Architecture</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-22948-1_5</idno>
	</analytic>
	<monogr>
		<title level="m" coord="15,240.99,639.70,264.99,10.91;15,112.66,653.25,123.97,10.91">Information Retrieval Evaluation in a Changing World, The Information Retrieval Series</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,86.97,393.33,10.91;16,112.28,100.52,394.91,10.91;16,112.28,114.06,395.00,10.91;16,112.66,127.61,337.56,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="16,159.06,86.97,216.57,10.91">Web-based open-domain information extraction</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pasca</surname></persName>
		</author>
		<idno type="DOI">10.1145/2063576.2064034</idno>
		<ptr target="https://doi.org/10.1145/2063576.2064034.doi:10.1145/2063576.2064034" />
	</analytic>
	<monogr>
		<title level="m" coord="16,399.09,86.97,106.90,10.91;16,112.28,100.52,391.00,10.91">Proceedings of the 20th ACM International Conference on Information and Knowledge Management, CIKM &apos;11</title>
		<meeting>the 20th ACM International Conference on Information and Knowledge Management, CIKM &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2605" to="2606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,141.16,393.33,10.91;16,112.66,154.71,394.62,10.91;16,112.66,168.26,394.53,10.91;16,112.28,181.81,395.00,10.91;16,112.66,195.36,295.83,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="16,463.74,141.16,42.25,10.91;16,112.66,154.71,369.82,10.91">Show me your evidence -an automatic method for context dependent evidence detection</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dankin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1050</idno>
		<ptr target="https://aclanthology.org/D15-1050.doi:10.18653/v1/D15-1050" />
	</analytic>
	<monogr>
		<title level="m" coord="16,112.66,168.26,394.53,10.91;16,112.28,181.81,193.31,10.91">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="440" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,208.91,395.17,10.91;16,112.66,222.46,394.04,10.91;16,112.66,236.01,395.17,10.91;16,112.66,249.56,156.62,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="16,259.01,208.91,179.05,10.91">Argumentation in artificial intelligence</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bench-Capon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">E</forename><surname>Dunne</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artint.2007.05.001</idno>
		<ptr target="argu-mentation" />
	</analytic>
	<monogr>
		<title level="m" coord="16,172.64,249.56,92.47,10.91">Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="page" from="619" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,263.11,394.53,10.91;16,112.28,276.66,394.42,10.91;16,112.66,290.20,397.48,10.91;16,112.66,303.75,199.84,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="16,253.17,263.11,248.78,10.91">Laying the foundations for a world wide argument web</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Rahwan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Zablith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Reed</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artint.2007.04.015</idno>
		<ptr target="https://doi.org/10.1016/j.artint.2007.04.015" />
	</analytic>
	<monogr>
		<title level="j" coord="16,112.28,276.66,95.98,10.91">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="page" from="897" to="921" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>argumentation in Artificial Intelligence</note>
</biblStruct>

<biblStruct coords="16,112.66,317.30,394.53,10.91;16,112.66,330.85,395.17,10.91;16,112.66,344.40,394.53,10.91;16,112.66,357.95,395.01,10.91;16,112.66,371.50,362.24,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="16,159.87,330.85,208.97,10.91">Argument search: Assessing argument relevance</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Euchner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Heilenk√∂tter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Weidmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331327</idno>
		<ptr target="https://doi.org/10.1145/3331184.3331327.doi:10.1145/3331184.3331327" />
	</analytic>
	<monogr>
		<title level="m" coord="16,390.14,330.85,117.69,10.91;16,112.66,344.40,394.53,10.91;16,112.66,357.95,36.22,10.91">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR&apos;19</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR&apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1117" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,385.05,393.33,10.91;16,112.66,398.60,395.01,10.91;16,112.66,412.15,187.21,10.91" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="16,319.43,385.05,186.56,10.91;16,112.66,398.60,180.57,10.91">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,425.70,393.32,10.91;16,112.39,439.25,176.99,10.91" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m" coord="16,297.98,425.70,208.00,10.91;16,112.39,439.25,52.97,10.91">Efficient estimation of word representations in vector space</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,452.79,393.33,10.91;16,112.66,466.34,295.17,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.4546</idno>
		<title level="m" coord="16,346.41,452.79,159.58,10.91;16,112.66,466.34,171.09,10.91">Distributed representations of words and phrases and their compositionality</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,479.89,393.33,10.91;16,112.28,493.44,393.70,10.91;16,112.66,506.99,395.01,10.91;16,112.41,520.54,381.76,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="16,168.07,479.89,204.17,10.91">Viewing morphology as an inference process</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Krovetz</surname></persName>
		</author>
		<idno type="DOI">10.1145/160688.160718</idno>
		<ptr target="https://doi.org/10.1145/160688.160718.doi:10.1145/160688.160718" />
	</analytic>
	<monogr>
		<title level="m" coord="16,397.98,479.89,108.00,10.91;16,112.28,493.44,393.70,10.91;16,112.66,506.99,85.10,10.91">Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;93</title>
		<meeting>the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;93<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="191" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,534.09,393.33,10.91;16,112.66,547.64,225.93,10.91" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="16,199.15,534.09,306.83,10.91;16,112.66,547.64,69.00,10.91">WordNet: An Electronic Lexical Database, Language, Speech, and Communication</title>
		<editor>C. Fellbaum</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,561.19,393.33,10.91;16,112.66,574.74,394.03,10.91;16,112.66,588.29,233.99,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="16,204.73,561.19,301.26,10.91;16,112.66,574.74,111.46,10.91">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<idno type="DOI">10.1145/3130348.3130377</idno>
		<ptr target="https://doi.org/10.1145/3130348.3130377.doi:10.1145/3130348.3130377" />
	</analytic>
	<monogr>
		<title level="j" coord="16,232.08,574.74,58.07,10.91">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="268" to="276" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,601.84,394.52,10.91;16,112.66,615.39,393.33,10.91;16,112.66,628.93,394.53,10.91;16,112.66,642.48,394.53,10.91;16,112.66,656.03,393.33,10.91;16,112.33,669.58,395.50,10.91;17,112.66,86.97,394.04,10.91;17,112.66,100.52,296.30,10.91" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="16,336.55,615.39,169.43,10.91;16,112.66,628.93,38.77,10.91">Overview of Touch√© 2020: Argument Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fr√∂be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58219-7_26</idno>
		<ptr target="https://link.springer.com/chapter/10.1007/978-3-030-58219-7_26.doi:10.1007/978-3-030-58219-7\_26" />
	</analytic>
	<monogr>
		<title level="m" coord="16,339.32,642.48,167.87,10.91;16,112.66,656.03,393.33,10.91;16,112.33,669.58,27.86,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction. 11th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="16,252.48,670.60,152.18,9.72">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Arampatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Lioma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>N√©v√©ol</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Hei-delberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">12260</biblScope>
			<biblScope unit="page" from="384" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,114.06,394.53,10.91;17,112.66,127.61,395.01,10.91;17,112.66,141.16,236.52,10.91" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="17,291.83,114.06,210.91,10.91">Semantic search on text and knowledge bases</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Buchhold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Haussmann</surname></persName>
		</author>
		<idno type="DOI">10.1561/1500000032</idno>
		<ptr target="http://dx.doi.org/10.1561/1500000032.doi:10.1561/1500000032" />
	</analytic>
	<monogr>
		<title level="j" coord="17,112.66,127.61,225.89,10.91">Foundations and Trends¬Æ in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="119" to="271" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,154.71,394.53,10.91;17,112.66,168.26,122.77,10.91" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m" coord="17,219.42,154.71,283.17,10.91">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,181.81,395.01,10.91;17,112.66,197.80,91.42,7.90" xml:id="b24">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4053</idno>
		<title level="m" coord="17,214.74,181.81,259.87,10.91">Distributed representations of sentences and documents</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,208.91,393.33,10.91;17,112.66,222.46,393.33,10.91;17,112.33,236.01,397.81,10.91;17,112.66,252.00,73.62,7.90" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="17,283.76,208.91,222.22,10.91;17,112.66,222.46,63.89,10.91">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7298682</idno>
		<idno>doi:</idno>
		<ptr target="10.1109/cvpr.2015.7298682" />
	</analytic>
	<monogr>
		<title level="m" coord="17,213.64,222.46,292.35,10.91;17,112.33,236.01,33.52,10.91">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,263.11,394.53,10.91;17,112.66,276.66,394.52,10.91;17,112.66,290.20,394.53,10.91;17,112.66,303.75,393.32,10.91;17,112.66,317.30,394.03,10.91;17,112.66,330.85,385.97,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="17,371.36,290.20,135.83,10.91;17,112.66,303.75,175.42,10.91">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="s" coord="17,310.67,303.75,195.31,10.91;17,112.66,317.30,36.87,10.91">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
			<date type="published" when="2019">2019</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,344.40,395.17,10.91;17,112.66,357.95,273.50,10.91" xml:id="b27">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m" coord="17,148.16,357.95,107.76,10.91">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,371.50,393.33,10.91;17,112.66,385.05,394.03,10.91;17,112.66,398.60,238.64,10.91" xml:id="b28">
	<monogr>
		<title level="m" type="main" coord="17,337.13,371.50,168.86,10.91;17,112.66,385.05,259.39,10.91">Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2002.10957</idno>
		<ptr target="https://arxiv.org/abs/2002.10957.doi:10.48550/ARXIV.2002.10957" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,412.15,394.53,10.91;17,112.66,425.70,122.77,10.91" xml:id="b29">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Patil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05477</idno>
		<title level="m" coord="17,191.63,412.15,310.78,10.91">Unsupervised paraphrase generation using pre-trained language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,439.25,394.53,10.91;17,112.66,452.79,393.33,10.91;17,112.66,466.34,266.73,10.91" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="17,210.10,439.25,214.20,10.91">Exploring Argument Retrieval with Transformers</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/" />
	</analytic>
	<monogr>
		<title level="m" coord="17,281.51,452.79,224.48,10.91;17,112.66,466.34,18.22,10.91">Working Notes Papers of the CLEF 2020 Evaluation Labs</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>N√©v√©ol</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2696. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,479.89,394.52,10.91;17,112.66,493.44,394.53,10.91;17,112.66,506.99,292.48,10.91" xml:id="b31">
	<monogr>
		<title level="m" type="main" coord="17,157.87,479.89,349.31,10.91;17,112.66,493.44,185.58,10.91">Hands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools, and techniques to build intelligent systems</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>G√©ron</surname></persName>
		</author>
		<ptr target="https://katalog.ub.uni-leipzig.de/Record/0-1640048871" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Beijing</pubPlace>
		</imprint>
	</monogr>
	<note>first edition, fifth release ed., O&apos;Reilly</note>
</biblStruct>

<biblStruct coords="17,112.66,520.54,394.53,10.91;17,112.66,534.09,394.04,10.91;17,112.66,547.64,393.68,10.91" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="17,303.19,520.54,199.13,10.91">A learning algorithm for boltzmann machines</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Ackley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0364-0213(85)80012-4</idno>
		<ptr target="https://doi.org/10.1016/S0364-0213(85)80012-4" />
	</analytic>
	<monogr>
		<title level="j" coord="17,112.66,534.09,79.43,10.91">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="147" to="169" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,561.19,394.53,10.91;17,112.66,574.74,122.77,10.91" xml:id="b33">
	<monogr>
		<title level="m" type="main" coord="17,315.06,561.19,187.54,10.91">The curious case of neural text degeneration</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09751</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,588.29,395.01,10.91;17,112.66,604.28,97.35,7.90" xml:id="b34">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04833</idno>
		<title level="m" coord="17,288.80,588.29,180.99,10.91">Hierarchical neural story generation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,615.39,395.01,10.91;17,112.66,628.93,394.51,10.91;17,112.66,644.93,79.55,7.90" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="17,228.21,615.39,217.43,10.91">Cumulated gain-based evaluation of ir techniques</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>J√§rvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kek√§l√§inen</surname></persName>
		</author>
		<idno type="DOI">10.1145/582415.582418</idno>
		<ptr target="https://doi.org/10.1145/582415.582418.doi:10.1145/582415.582418" />
	</analytic>
	<monogr>
		<title level="j" coord="17,454.10,615.39,53.57,10.91;17,112.66,628.93,39.69,10.91">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,112.66,656.03,394.04,10.91;17,112.66,669.58,231.92,10.91" xml:id="b36">
	<monogr>
		<title level="m" type="main" coord="17,161.35,656.03,166.39,10.91">Touch√© 2021 relevance judgements</title>
		<author>
			<persName coords=""><surname>Webis</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>De</surname></persName>
		</author>
		<ptr target="https://webis.de/events/touche-22/shared-task-1.html" />
		<imprint>
			<date type="published" when="2021">2021. 2022-02-27</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="18,112.66,86.97,394.53,10.91;18,112.66,100.52,395.01,10.91;18,112.66,114.06,187.11,10.91" xml:id="b37">
	<monogr>
		<title level="m" type="main" coord="18,161.50,86.97,214.97,10.91">On Bayesian Methods for Seeking the Extremum</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Moƒçkus</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-662-38527-2_55</idno>
		<ptr target="https://doi.org/10.1007/978-3-662-38527-2_55.doi:10.1007/978-3-662-38527-2_55" />
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="400" to="404" />
			<pubPlace>Berlin Heidelberg, Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
