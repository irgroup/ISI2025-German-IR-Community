<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,400.70,15.42;1,89.29,106.66,111.42,15.42">Using BERT to retrieve relevant and argumentative sentence pairs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,87.56,11.96"><forename type="first">Nils</forename><surname>Wenzlitschke</surname></persName>
							<email>wenzlitschke@studserv.uni-leipzig.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Leipzig University</orgName>
								<address>
									<postCode>04109</postCode>
									<settlement>Leipzig</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,189.50,134.97,47.03,11.96"><forename type="first">Pia</forename><surname>SÃ¼lzle</surname></persName>
							<email>suelzle@studserv.uni-leipzig.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Leipzig University</orgName>
								<address>
									<postCode>04109</postCode>
									<settlement>Leipzig</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,400.70,15.42;1,89.29,106.66,111.42,15.42">Using BERT to retrieve relevant and argumentative sentence pairs</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">8682FB714A5EBCDF12BA4648678F838B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Argument Retrieval</term>
					<term>Natural Language Processing</term>
					<term>BERT</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the age of the Internet, it should not be a problem to find enough information on a controversial topic within a short time to be able to form a well-founded opinion. However, finding arguments from different points of view on a particular topic is still difficult. Therefore, the TouchÃ© Shared Tasks of this and the last years are concerned with argument retrieval, that is to find relevant and qualitative arguments. In this paper, we present our results for this year's TouchÃ© Task 1. We test and combine various state-of-the-art argument retrieval methods and natural language processing to retrieve relevant sentence pairs on controversial topics. Evaluating the various combinations of our methods, we compare the different approaches. We find that using DirichletLM as retrieval model yields longer sentences than using BM25 and that pairing single sentences with the Next Sentence Prediction of BERT works better than with the sentence similarity of SBERT. We evaluate the possible combinations using precision@10 and nDCG@10. Our best retrieval system achieves a precision@10 of 0.67 and a nDCG@10 of 0.73 using sentence classification in preprocessing, DirichletLM, querying the arguments with Boolean queries using noun chunking, pairing the sentences via Next Sentence Prediction and re-ranking using a BERT-base method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>While facts can be looked up in seconds in modern search engines like Google<ref type="foot" coords="1,450.61,443.58,3.71,7.97" target="#foot_0">1</ref> , Bing <ref type="foot" coords="1,481.79,443.58,3.71,7.97" target="#foot_1">2</ref> and DuckDuckGo <ref type="foot" coords="1,149.35,457.13,3.71,7.97" target="#foot_2">3</ref> , these search engines often fail to provide arguments on either side of a controversial issue. This makes it difficult to form a well-founded opinion on complex ambiguous issues. Thus, to learn about arguments from both sides of a controversial issue, one must resort to something other than the typical search engines mentioned above. Following discussions on such topics can give one the opportunity to get an overview of arguments from both sides of an issue. Such discussions often take place on social media. However, social media often do not provide a good basis for an opinion-forming process, as people quickly get caught up in a bubble that exclusively reproduces opinions that already exist in their heads <ref type="bibr" coords="1,462.85,553.73,11.59,10.91" target="#b0">[1]</ref>. With controversial topics, however, it is usually essential to look at all sides of the issue in order to form an well-founded opinion. This is why argument search engines which are presenting different views on a controversial topic are becoming increasingly important.</p><p>The TouchÃ© @ CLEF: Argument Retrieval 2022 <ref type="bibr" coords="2,309.46,114.06,13.00,10.91" target="#b1">[2]</ref> Shared Task 1: "Argument Retrieval for Controversial Questions" calls scientists for new approaches on argument search. The goal of this task is to implement an information retrieval system that returns summarized arguments on controversial topics. The task specifies the search results to be pairs of sentence IDs taken from a preprocessed version of the args.me corpus <ref type="bibr" coords="2,315.36,168.26,11.42,10.91" target="#b2">[3]</ref>. The three dimensions of quality which should be focused on are: "(1) each sentence in the pair must be argumentative (. . . ), <ref type="bibr" coords="2,476.71,181.81,11.81,10.91" target="#b1">(2)</ref> the sentence pair must form a coherent text (. . . ), and (3) the sentence pair constitutes a summary of a single argument (. . . )." <ref type="bibr" coords="2,199.40,208.91,11.43,10.91" target="#b1">[2]</ref>. We focus our efforts on using different state-of-the-art argument retrieval and natural language processing methods. The retrieval system consists of different modules that together form the holistic pipeline, which are combined in different ways for the evaluations to find the retrieval system that yields the best results in the end.</p><p>In this paper we will discuss related work, focusing on works from which we drew the inspiration and approaches we used for our retrieval system in Section 2. In Section 3 we will go into more detail about our methodological approach and present and explain the individual optional components of our retrieval system. After that, we will discuss the evaluation and elaborate on our results in Section 4. In Section 5, we conclude to what extent the individual modules of our retrieval system impact the relevance and present the combination of our methods that achieved the best results in the evaluation. After that, in Section 5.1, we will also point out future directions in which our retrieval system could be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This work draws on existing information retrieval methods, argument mining and natural language processing. The following sections introduce the args.me corpus, methods of argument classification, ranking, and sentence pairing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">args.me Corpus</head><p>Our retrieval system is based on an index from a preprocessed version of the args.me corpus provided by the shared task organizers. The original version of this dataset was introduced in 2019 by Ajjour et al. <ref type="bibr" coords="2,180.92,520.50,11.32,10.91" target="#b2">[3]</ref>. It was created by crawling four selected online debate portals, namely (1) debatewise.org, (2) idebate.org, (3) debatepedia.org and (4) debate.org. The dataset consists of 387,606 arguments.</p><p>All of the crawled portals are structured similarly. They consist of user created forum-threads; each thread discusses a controversial topic. In all of the four portals, when contributing their point of view to a thread, users must choose a stance for it. Arguments in the preprocessed dataset are modeled as a triple of (1) a conclusion which mostly is the title of the discussion in the portal, (2) a premise which is a comment of a user on the related conclusion and (3) a pro or con stance towards the conclusion which is chosen by the respective user <ref type="bibr" coords="2,438.18,628.89,11.58,10.91" target="#b3">[4]</ref>. The stance of a premise is assumed to correspond to the stance the replying user took towards the topic. Additionally, each argument is given a unique ID. The arguments are organized in rows of topics with a unique topic ID, a conclusion and a list of premises with stance annotations.</p><p>The difference between the provided preprocessed version of the args.me corpus and the original version is that the preprocessed version contains the premises split up into sentences in an additional column. Since the arguments for the task should consist of two single sentences each, the preprocessed version of the args.me corpus thus provides a better basis for our work. Each sentence in the preprocessed corpus has a unique sentence ID it can be referenced by.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Ranking</head><p>Potthast et al. <ref type="bibr" coords="3,152.35,190.89,12.70,10.91" target="#b4">[5]</ref> compared the four standard retrieval models BM25, TF-IDF, DPH, and Dirich-letLM. The conclusion of their work is that DPH and DirichletLM are relatively similar in terms of relevance. However, TF-IDF and BM25 performed worse in the comparison. Even though Potthast et al. <ref type="bibr" coords="3,151.47,231.54,11.55,10.91" target="#b4">[5]</ref> found that BM25 is not as good as Dirichlet, we use BM25 for comparison. Gienapp <ref type="bibr" coords="3,129.83,245.09,12.90,10.91" target="#b5">[6]</ref> also uses DirichletLM as a retrieval model for textual relevance in Quality-aware Argument Retrieval with Topical Clustering, referring to the frequent use of this model in the context of TouchÃ© Shared Task 2020. The most frequently used retrieval model was DirichletLM followed by BM25 <ref type="bibr" coords="3,173.66,285.73,12.93,10.91" target="#b6">[7]</ref> in the submissions for last year's TouchÃ© Task. For comparison we use BM25 in addition to DirichletLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Argument Classification</head><p>As described in Section 1, one of the quality dimensions of the task is that each sentence of a retrieved sentence pair must be argumentative. To approach the quality dimension of argumentativeness, we found two promising methods in the literature <ref type="bibr" coords="3,404.13,376.11,11.36,10.91" target="#b7">[8,</ref><ref type="bibr" coords="3,418.21,376.11,7.57,10.91" target="#b8">9]</ref>. Gienapp et al. <ref type="bibr" coords="3,156.94,389.66,12.99,10.91" target="#b7">[8]</ref> used a support vector machine (SVM) to classify given texts passages into arguments and non-arguments. This SVM was trained on the Webis-ArgQuality-20 corpus <ref type="bibr" coords="3,89.29,416.76,11.58,10.91" target="#b7">[8]</ref>. Regardless of the premise's topic, a binary decision is made whether this text span is an argument or not. Gienapp et al. <ref type="bibr" coords="3,251.20,430.31,13.00,10.91" target="#b7">[8]</ref> also trains a support vector regression model, which determines the quality of a text span classified as an argument. Because the text quality is not one of the quality dimensions in this year's TouchÃ© task, we decide to use only the SVM for classifying text passages into arguments for our retrieval model.</p><p>Another method of classifying arguments was presented by Reimers et al. <ref type="bibr" coords="3,434.57,484.50,11.50,10.91" target="#b8">[9]</ref>. Contrary to the methods above, they worked on sentence level argument classification. Also, their approach takes a topic into account, to which the sentence can either be an argument for, an argument against or not argumentative. For this, they fine-tuned multiple language models such as ELMo and BERT to the task of argument classification.</p><p>The best performing models they present are BERT-base ğ‘¡ğ‘œğ‘ğ‘–ğ‘ and BERT-large ğ‘¡ğ‘œğ‘ğ‘–ğ‘ . They are fine-tuned versions of the BERT-base and BERT-large models, respectively. The input is a topic and a sentence. The model classifies the sentence either as an argument for, an argument against or not argumentative. The model was trained on the UKP Sentential Argument Mining Corpus, which annotated 25,492 sentences over eight controversial topics. We used their pretrained BERT-base ğ‘¡ğ‘œğ‘ğ‘–ğ‘ , which we refer to as ACL ğµğ¸ğ‘…ğ‘‡ in our experiments. 2), the indexing (Section 3.3), the retrieval (Section 3.4), and the sentence-matching (Section 3.5.2), which yield the argument pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Sentence Pairing</head><p>An essential step for accomplishing the TouchÃ© task is matching the retrieved sentences since quality dimensions (2) and (3) of the task (Section 1) refer to pairing sentences. We use two methods in order to pair sentences. The first method we use pairs sentences by similarity. First, we transform the sentences into sentence embeddings. For that, we use Sentence-BERT (SBERT) by Reimers and Gurevych <ref type="bibr" coords="4,209.94,450.50,16.42,10.91" target="#b9">[10]</ref>. SBERT is an extension of BERT, which, by utilizing Siamese and triplet network structures, reduces the processing time for the transformation of sentences into meaningful sentence embeddings. Second, we match the sentences calculating the cosine similarity between these sentence embeddings. This forms our initial pairing approach. Our second method also uses BERT, a transformer that is intended to capture the relationship between sentences. For this purpose, BERT was trained with a Next Sentence Prediction (NSP) task. This means that BERT was given labeled training data consisting of two sentences, where consecutive sentence pairs got the label IsNext and random sentence pairs got the label NotNext <ref type="bibr" coords="4,89.29,558.89,16.39,10.91" target="#b10">[11]</ref>. Since BERT was trained with exactly such a task, it was apparent to us to use BERT for exactly this purpose. Therefore, our second approach utilizes BERT to find the best match for a given sentence using NSP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodological Approach</head><p>In this Section, we describe all methods we tested to retrieve topic relevant and qualitative sentence pairs from arguments. These methods compose optional components in our retrieval system. First, we describe the necessary preprocessing and indexing steps of our retrieval system. Then, we explain our approaches to retrieve topic relevant sentences using different query structures, noun chunking, and re-ranking via Query-Based Argument Classification. Lastly we detail on out approach to pair sentences using sentence similarity and NSP.</p><p>The retrieval system consists of optional preprocessing steps as described in Section 3.2, indexing of the sentences with Elasticsearch<ref type="foot" coords="5,288.28,152.96,3.71,7.97" target="#foot_3">4</ref> (Section 3.3), retrieval of sentences with one or more methods described in Section 3.4 and one of the two sentence pairing methods described in Section 3.5. An abstract overview of the individual modules of the retrieval system can be seen in Figure <ref type="figure" coords="5,154.68,195.36,3.74,10.91" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Corpus</head><p>From the provided preprocessed corpus args.me described in Section 2.1, the single sentences were used for our retrieval system. We use single sentences, assuming that the sentence that best completes or matches a retrieved sentence does not necessarily have to come from the same source argument. Therefore, our retrieval steps are performed on these single sentences. Consequently, we do not further include the arguments from which the sentences originate in the search, except for argument classification based on the Webis-ArgQuality-20 corpus described in Section 3.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Preprocessing</head><p>Initially, we remove duplicates of single sentences on an exact text match to ensure the variety of search results and to avoid identical arguments. In an exploratory analysis of the corpus, we found many sentences that we were confident would never produce a satisfying result, because (1) they were not proper, that is well-formed, sentences at all (e.g. a URL, a reference, grammatically incorrect) or (2) they were not argumentative in any way (e.g. off-topic, spam, filler sentences, sentences just not making a point towards the topic), and thus not fulfilling the quality dimension, that every retrieved sentence should be argumentative. We experimented with ways to automatically detect and remove such content from the corpus through natural language sentence classification, and two argument classification approaches, which we explain in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Natural language sentence classification</head><p>To remove the not proper language sentences, we established a preprocessing step to categorize our given premises and conclusions into proper sentences and not proper sentences. We started from the simple heuristic that a natural language sentence always contains a verb. Thus, we check the premises and conclusions for the presence of a verb. To do so, we tagged each of these premises and conclusions using NLTK's POS tagger <ref type="bibr" coords="5,353.96,601.58,16.42,10.91" target="#b11">[12]</ref>. Then, we checked the list of POS tags of a sentence for the presence of a verb tag by creating a list of all verb tags of the POS tagger and compared the tag list of the respective sentence with this list. We declared premises or conclusions for which no word was tagged with a verb tag from the reference list as non-sentences and not processed further in the retrieval system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Webis-ArgQuality-20 Argument Classifier</head><p>In order to remove sentences from the corpus that are not argumentative, we trained an SVM to classify arguments. This is based on the approach by Gienapp et al. <ref type="bibr" coords="6,401.78,163.39,12.79,10.91" target="#b7">[8]</ref> described in 2.3. We trained the SVM using the Webis-ArgQuality-20 corpus <ref type="bibr" coords="6,337.21,176.94,11.37,10.91" target="#b7">[8]</ref>. One problem in implementing the SVM is that the underlying Webis-ArgQuality-20 corpus consists of whole argument phrases. These are coherent sentences that together make up an argument, rather than single sentences. Therefore, we used the argument phrases from the preprocessed args.me corpus. Thus, the SVM classifies the argument phrases whether they are arguments or not. Then, the complete passages classified as no argument are removed from the corpus and no longer considered in further processing. Thus, it is assumed that passages that the SVM has classified as no arguments do not contain relevant or argumentative sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Classifying Arguments with BERT</head><p>The second method of removing non-argumentative sentences is based on ACL ğµğ¸ğ‘…ğ‘‡ , which classifies sentences as argumentative or not argumentative with respect to a given input topic, as described in Section 2.3. The defining principle is that the sentences are not classified in isolation, but in the context of a topic.</p><p>We labelled all sentences of our dataset with respect to it's corresponding conclusion. As topic information, we provided the model with the corresponding conclusion of each sentence. Sentences classified as noArgument were removed from the corpus. All sentences classified as Argument_for or Argument_against were kept. The reasoning is that sentences, that are not argumentative with respect to their corresponding conclusions, have little chance to be strong arguments in any context. The stance information was not used for our retrieval system because the classified stance in this case refers only to the underlying conclusion of the respective sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Indexing</head><p>We use Elasticsearch to index the premises and conclusions in the preprocessed args.me corpus. Each document in our index consists of the sentence ID associated with the args.me record, the text of the sentence, and its corresponding conclusion. In addition, using Elasticsearch's inherent analyzer pipeline, different filters are applied to each document. These filters are stemming, stop word removal, and text lower casing. The filters are applied on both the sentence and conclusion text fields. Depending on the retrieval system composition, these documents are then ranked using either DirichletLM or BM25. We determine the parameter ğœ‡ for DirichletLM retrieval by determining the average length of the individual sentences in the corpus. ğœ‡ is 116 after the preprocessing step of sentence classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Retrieval</head><p>After indexing the documents using Elasticsearch, the retrieval is done using the Elasticsearch Search API. For this purpose, we call the Elasticsearch Search API with different query requests. The query requests consist of different query components depending on the retrieval system composition. We distinguish between a simple, naive baseline approach that understands the entered search term as a composition of individual terms and a more refined approach considering Boolean queries. The refined approach considers terms in the text field of the document sentence and terms in the conclusion field of the index, as described in 3.4.1. Thus, sentences with one of the search terms in the sentence and in the corresponding conclusion field are given particular significance. For this second approach, we have developed an additional feature as will be described in Section 3.4.2. This feature uses natural language processing to extract specific terms from the search query and give them particular weight. In Section 3.4.3, we describe our re-ranking approach. Thus, we use the ACL ğµğ¸ğ‘…ğ‘‡ . After the single sentences have been retrieved using one of the two query methods, they are evaluated with ACL ğµğ¸ğ‘…ğ‘‡ for their argumentativeness concerning the query and, if so, ranked further up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">Query</head><p>The queries are full text queries, which we direct to the endpoint of the Elasticsearch index in Elasticsearch's query Domain Specific Language (DSL). These full text queries allow searching the text of a specific document field (sentence or conclusion) in our index. We process the query string with the same analyzer pipeline used to index the Elasticsearch fields, described in Section 3.3. For ranking the sentences, we use either BM25 or DirichletLM.</p><p>We use different full text queries of the query DSL. As a first approach we use a simple match query. The match query searches by default on one document field and separates the query string into individual terms, then combined with a logical or operator. The second -more elaborate -approach uses the Boolean query match_bool_prefix of the Query DSL Language. This allows for queries that consist of several subqueries. With each subquery that matches a document, the calculated score for this document increases concerning the root-query. Different fields can also be taken into account using this query type. For example, we use this query type to search the sentence and conclusion fields simultaneously. Thus, documents in the index that match query terms in both, the sentence field and the conclusion field, are ranked higher. In addition to viewing the query string as a series of individual terms linked with an or operator, we add another subquery that connects the query terms with an and operator. Thus, documents in the index with the exact wording are additionally boosted. Since not all parts of a search query have the same significance for the search results and only in very few cases the entire query string occurs in the text we have refined the second query approach. We extract certain groups of terms of the query string to search for them using the match_phrase_query. The match_phrase_query scores exact matches much higher when the search query occurs in the same way in the text of the searched field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2.">Noun Chunking</head><p>Compound terms, phrases composed of more than one noun, can have a different meaning than the individual words that compose them, such as sex education <ref type="bibr" coords="8,363.28,121.08,16.09,10.91" target="#b12">[13]</ref>. Therefore, these compound terms should be given special consideration in the query. In evaluating our initial experiments, we found that too little value was placed on compound terms. This is because each word of the query was evaluated and searched as a single term in our basic query process. To address this problem, we tried to find a way to consider the compound terms and the nouns in the query and boost them. For this task, we deploy a method that we call noun chunking. For this, we use the Noun Chunker from spaCy <ref type="foot" coords="8,228.68,200.63,3.71,7.97" target="#foot_4">5</ref> . Noun chunks can be considered as "base noun phrases" and consist of a noun and descriptive word that relates to the noun. These descriptive word can be any word specifying the base noun. We pass the query to spaCy which returns a list of noun chunks. These noun chunks are then appended to the Boolean query in match_phrase_query and thus considered separately again when retrieving results in the query process. It is also possible to give the match_phrase_query an extra boost to highlight these noun phrases even more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3.">Re-Ranking: Query-Based Argument Classification</head><p>We want our retrieval system to place strongly argumentative sentences on high ranks. When doing the first evaluations, we noticed that the most argumentative sentences would not always be the highest ranking. To combat this, we once again utilized ACL ğµğ¸ğ‘…ğ‘‡ described in Section 2.3. The classifier takes a sentence and a topic as input, which we hand over from the list of sentences retrieved by Elasticsearch. In contrast to how we use ACL ğµğ¸ğ‘…ğ‘‡ in preprocessing, where we use the conclusion as a topic, we now use the query. This approach considers the properties of a sentence (argumentative or not) depending on the particular input and does not assume that a sentence is inherently argumentative. Sentences classified as an argument for or an argument against were boosted, so they would rank first before the sentences classified as no argument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Sentence Pairing</head><p>Sentence pairing describes the task of matching two relevant sentences for a specific query. We tried different approaches to decide whether two sentences match. The first approach pairs the sentences based on their cosine similarity. In the second approach, the probability that a second sentence follows the first sentence is calculated. The determined value is maximized in both cases, and the corresponding sentence is assigned its optimal match. We use the number of sentence pairs that should appear in the output to determine the number of top sentences. Top sentences are the single sentences relevant to the query which form the first sentence of each retrieved sentence pair. The remaining sentences to compare are the retrieved sentences without the top sentences. Pairing is performed between these top sentences and the pool of the remaining sentences. The number of sentences to be compared with each top sentence is many times larger than the desired number of sentence pairs in the output. Once a sentence is matched with a top sentence, it is removed from the pool of the match candidates, so it cannot appear for twice. Matching is performed in descending order so that the entire pool of match candidates is available for the first sentence, and this pool becomes successively smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1.">Sentence Similarity</head><p>We match sentences by following the intuition that if a retrieved sentence is relevant and argumentative to the associated query and similar to a second sentence, the latter will complement the first sentence and strengthen the argument. Based on the approach described in Section 2.4, we first transform the sentences into sentence embeddings. These sentence embeddings of the top sentences are now matched against the sentence embeddings of the remaining match candidates in terms of cosine similarity. Each top sentence is then assigned the sentence for which the cosine similarity is the highest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2.">Next Sentence Prediction</head><p>As an alternative sentence pairing method to sentence similarity, we experimented with Next Sentence Prediction (NSP) which we explained in more detail in Section 2.4. Thus, for each sentence that our retrieval system returns, we compute the probability that a specific other sentence follows it. This approach follows the idea that a sentence that is relevant and argumentative by our retrieval system concerning the query is likely to be followed by a sentence that (1) syntactically goes along with the structure of the previous sentence and (2) strengthens and completes the mentioned argument by the first sentence. In descending order, we calculate for each sentence the probability computed with every other sentence that has not been used in any combination so far. Then, we assign each sentence to the sentence that is most likely to follow it.</p><p>To implement this, we use the pre-trained bert-base-uncased model from Hugging Face's Transformer Library <ref type="bibr" coords="9,156.21,456.61,16.09,10.91" target="#b13">[14]</ref>, which is based on the work of Devlin et al. <ref type="bibr" coords="9,365.39,456.61,16.08,10.91" target="#b10">[11]</ref>. First, the model transforms both sentences into sentence embeddings using the pre-trained BERT tokenizer. Then the model predicts how likely the second sentence is to follow the first sentence. The pair of sentences that achieves the highest score among all possible combinations of first and second sentences is the best match. Using the NSP model, the desired number of sentence pairs is formed from the top sentences and the less highly ranked sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>To evaluate the methods described above, we applied the retrieval system to ten test queries with varying components enabled, resulting in 8 different combinations. For each query, we retrieved ten results, which we then annotated rating each result with a score out of (-2, 0, 1, 2, 3). The heuristics we used to assign scores during the evaluation can be seen in Table <ref type="table" coords="9,500.04,623.63,3.81,10.91">1</ref>. From the annotated results, we calculated the precision@10 and nDCG@10 for each evaluated combination of components. Precision@10 was calculated interpreting ratings of -2 and 0 as not relevant and 1, 2 and 3 as relevant results. score label -2 spam 0 fulfills some criteria but not relevant to the query 1 relevant to the query and fulfills criteria (1) 2 relevant to the query and fulfills criteria ( <ref type="formula" coords="10,380.10,143.25,3.54,8.87">1</ref>) and (2) 3 relevant to the query and fulfills criteria (1), ( <ref type="formula" coords="10,387.75,155.20,3.54,8.87">2</ref>) and ( <ref type="formula" coords="10,418.78,155.20,3.54,8.87">3</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>For the score a pair of sentences could receive in our evaluation, there were certain criteria that we used as a guideline for assigning the scores. Quality dimensions specified by the task were included in the scoring:</p><p>(1): each sentence in the pair is argumentative (2): the sentence pair forms a coherent text (3): the sentence pair constitutes a summary of a single argument preprocessing retrieval query re-ranking pairing precision@10 nDCG@10 The evaluation results are measured for the individual retrieval system compositions in precision@10 and nDCG@10. SCL refers to the sentence classifier used in preprocessing (Section 3.2.1). ACL ğµğ¸ğ‘…ğ‘‡ refers to both, the preprocessing and the re-ranking step (Section 3.2.3, Section 3.4.3). ACL ğ‘†ğ‘‰ ğ‘€ is the argument classification approach described in (Section 3.2.2). DLM stands for DirichletLM, NC is noun chunking (Section 3.4.2), and sim refers to sentence similarity (Section 3.5.1). The best results are shown in bold.</p><p>The combinations to evaluate were chosen exploratively. First, we evaluated the retrieval system using BM25 as a ranking function, results shown in Table <ref type="table" coords="10,388.94,535.10,3.81,10.91" target="#tab_0">2</ref>. Here, we compared the performance of our two sentence pairing methods. NSP outperformed sentence similarity in precision@10 and nDCG@10, which is why all following retrieval systens we evaluated utilize NSP.</p><p>The highest precision@10 and nDCG@10 we can report with BM25 is 0.54 and 0.69 respectively using NSP as the sentence-pairing method and ACL ğµğ¸ğ‘…ğ‘‡ for re-ranking.</p><p>Our second round of evaluation were all done with NSP because of the promising results in the first evaluation round as shown in Table <ref type="table" coords="10,288.94,629.95,3.75,10.91" target="#tab_0">2</ref>. DirichletLM as a ranking function was chosen since it has proven to work well with argument retrieval in previous work, as described in Section 2.2. We found that the results were worse than our best performing retrieval system with BM25. DirichletLM, NSP and the sentence classifier (Section 3.2.1) yield a precision@10 of only 0.36. Also, we found that the preprocessing with the SVM did not change the result at all. Results improved with ACL ğµğ¸ğ‘…ğ‘‡ in preprocessing and improved again with the query based ACL ğµğ¸ğ‘…ğ‘‡ enabled. Finally, our best performing retrieval system with DirichletLM and NSP, even beating our best retrieval system setup with BM25, utilized the sentence classifier in preprocessing, the Boolean query using noun chunking for querying, and the ACL ğµğ¸ğ‘…ğ‘‡ for re-ranking, reaching a precision@10 of 0.67. Lastly, we examined the sentence lengths of all approaches and sorted them according to whether they were performed with BM25 or DirichletLM. The average sentence length of the approaches shown in Table <ref type="table" coords="11,213.52,208.91,5.09,10.91" target="#tab_0">2</ref> performed with BM25 as the ranking method is 165.15, whereas the approaches using DirichletLM have a much longer average sentence length of 298.19.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In the first part of this section, we will go through the individual optional methods of our retrieval system and take a closer look at their respective influence of each of them. In doing so, we will also discuss the shortcomings of the approaches. In the second part, we address possible further developments of our approaches that could improve our retrieval system in (Section 5.1).</p><p>Preprocessing can be divided into sentence classification and argument classification. We can say that our sentence classifier had an influence in the sense that non-sentences such as links appeared less frequent when going through the results checking a sentence for the presence of a verb. No significant change in the result set was observed after applying argument classification using the SVM. A possible explanation for this would be that the argument passages classified as non-arguments did not find their way into the final retrieved result set, so filtering them out beforehand did not further influence the result. The same reasoning applies to the ACL ğµğ¸ğ‘…ğ‘‡ , which could not make any significant improvement in the preprocessing step, as can be seen in Table <ref type="table" coords="11,115.79,457.22,3.74,10.91" target="#tab_0">2</ref>.</p><p>Our approaches can be divided into two parts for indexing based on the underlying retrieval model. On the one hand, BM25 was used as the retrieval model that provided the best results until the last evaluation run of the DirichletLM retrieval model where we use the noun chunking approach. On the other hand, we obtained the best results using DirichletLM as the retrieval model, which will be described in more detail later. Moreover, we noted that DirichletLM produced nearly twice as long sentence pairs as BM25 for all the performed experiments. In some cases, we retrieved sentences that should have been divided into several single sentences, which they were not due to missing punctuation marks.</p><p>We also used a number of different approaches for retrieval. In our first experiments, we used a simple query where the words of the entered topic are understood as single terms. Using the Boolean query improved the resulting precision@10 value slightly. In the Boolean query, both the sentence and the conclusion field of the documents were included in the ranking. This led to additional results that did not include terms from the actual query in the sentence field, but still related to the query due to a matching conclusion. Thus, the field of match candidates for the pairing methods was expanded, resulting in more heterogeneous sentence-pair combinations. In addition, our noun chunking approach was also used in these Boolean queries, which increased the precision@10 value by boosting individual terms of the query. Besides the query, another crucial part of the retrieval is the ranking. ACL ğµğ¸ğ‘…ğ‘‡ is used again in the retrieval step to re-rank the queried results. The retrieved results classified by ACL ğµğ¸ğ‘…ğ‘‡ as argumentative concerning the associated query string were boosted and re-ranked further up the results. As a re-ranking component in the retrieval system, ACL ğµğ¸ğ‘…ğ‘‡ has a more significant impact on the results than the preprocessing step.</p><p>For sentence pairing, we tried two different approaches. The worse approach of the two was the sentence similarity approach. There were problems with the approach because the matched sentences were too similar. Too similar sentences often consist of the same words. The second sentence merely repeats the first and does not logically continue it. Therefore, we did not consider this approach further after our first round of evaluation and instead focused on the second approach. This approach yields better results regarding sentence pairing since NSP is not about finding similar sentences but about how likely it is that a sentence follows a given sentence.</p><p>Finally, after evaluating different combinations of the different modules of our retrieval system, we were able to identify the retrieval system that gave the best results. This retrieval system uses only the sentence classifier since this was the only classifier in the preprocessing step, producing noticeable evaluation differences. In addition, the retrieval step uses Boolean queries with noun chunking and ACL ğµğ¸ğ‘…ğ‘‡ for re-ranking. We used the retrieval model DirichletLM, and NSP was used as the sentence pairing method. As seen in the last row of Table <ref type="table" coords="12,478.06,624.31,3.81,10.91" target="#tab_0">2</ref>, this combination obtained the best results. With a precision@10 value of 0.64 and an nDCG@10 of 0.74, our retrieval system does not yet function optimally, but offers room for improvement due to its modular structure. The application of certain modules led to an improvement of precision@k or nDCG@k. For example, noun chunking improved the precision@10 value, while the ACL ğµğ¸ğ‘…ğ‘‡ improved the nDCG@10 in the re-ranking step. Our final retrieval system can be seen in Figure <ref type="figure" coords="13,186.07,114.06,3.74,10.91" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Future Directions</head><p>In order to further develop our retrieval system, various modules could be refined, potentially yielding better results. For example, the use of the ACL ğµğ¸ğ‘…ğ‘‡ produced better results in the re-ranking process, but we did not use any fine-tuning. It would be possible to train the classifier on our evaluation data to improve the results and the re-ranking with the ACL ğµğ¸ğ‘…ğ‘‡ . After we have done evaluation runs, there would be data with which the classifier could be trained on the topics given by TouchÃ©, in addition to the eight topics on which the classifier was originally trained.</p><p>A further improvement to re-rank the results would be possible based on ACL ğµğ¸ğ‘…ğ‘‡ . In the current approach, the sentences classified as arguments by ACL ğµğ¸ğ‘…ğ‘‡ are pushed to the beginning of the retrieved sentences. To refine this re-ranking, it would be possible to count the classification towards the retrieval score by a boost and not just pushing the classified arguments to the beginning.</p><p>When evaluating the results, we noticed that the sentence pairs sometimes contradict each other despite their respective good argumentativeness or relevance. The given stances from the dataset cannot be used due to our approach, which considers the sentences independently of their respective original argument passage. One way to address this problem would be to use, for example, sentiment analysis or similar methods to ensure that the sentences are matched only with sentences that represent the same point of view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>The aim of the present research was to implement a retrieval system to obtain relevant and argumentative sentence pairs from the args.me corpus and evaluate its performance. In the preprocessing step, we have removed duplicates and sentences that do not add value to the final result. The latter include non-proper language sentences, which were identified based on POS heuristics, and non-argumentative sentences, which were identified using two different argument classifiers ACL ğµğ¸ğ‘…ğ‘‡ and ACL ğ‘†ğ‘‰ ğ‘€ based on approaches by Gienapp <ref type="bibr" coords="13,492.99,520.50,13.00,10.91" target="#b5">[6]</ref> and Reimers et al. <ref type="bibr" coords="13,171.22,534.05,11.38,10.91" target="#b8">[9]</ref>. For indexing and retrieving the premises and conclusions, Elasticsearch with either DirichletLM or BM25 was used. In addition to a simple query where the search terms are understood as individual terms, we also used Boolean queries to retrieve the relevant arguments. Within the Boolean queries our noun chunking approach allowed certain terms to be weighted. The ACL ğµğ¸ğ‘…ğ‘‡ based on Reimers et al. <ref type="bibr" coords="13,317.52,588.25,12.68,10.91" target="#b8">[9]</ref> was used again after retrieval to re-rank the sentences based on their argumentativeness. In the last step, the obtained top sentences were matched with the remaining retrieved sentences based on two different approaches. In the first approach, the sentences were matched based on their cosine similarity using SBERT <ref type="bibr" coords="13,487.35,628.89,16.25,10.91" target="#b9">[10]</ref>. In the second approach, the sentences were matched using BERT and NSP <ref type="bibr" coords="13,422.01,642.44,16.25,10.91" target="#b10">[11]</ref>. Nine different combinations of these intermediate steps were evaluated using precision@10 and nDCG@10. The best combination with a precision@10 of 0.67 and an nDCG@10 of 0.73 used our sentence classifier and ACL ğµğ¸ğ‘…ğ‘‡ in preprocessing, DirichletLM for indexing, the Boolean query with Noun Chunking, and ACL ğµğ¸ğ‘…ğ‘‡ again for re-ranking.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,89.29,314.22,416.70,8.93;4,88.99,326.23,416.99,8.87;4,89.29,338.18,272.74,8.87;4,89.29,84.19,416.70,222.61"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Abstract overview of our retrieval system. It consists of the preprocessed args.me corpus (Section 3.1), preprocessing (Section 3.2), the indexing (Section 3.3), the retrieval (Section 3.4), and the sentence-matching (Section 3.5.2), which yield the argument pairs.</figDesc><graphic coords="4,89.29,84.19,416.70,222.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="12,89.29,267.90,416.70,8.93;12,89.29,279.91,417.79,8.87;12,89.29,291.87,416.70,8.87;12,89.29,303.82,416.69,9.38;12,89.29,315.78,191.77,8.87;12,89.29,84.19,416.68,176.29"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Final retrieval system, which yielded the results. It consists of the preprocessed args.me corpus (Section 3.1), the sentence classification (Section 3.2.1), the indexing by DirichletLM (Section 3.3), the retrieval by Boolean queries (Section 3.4.1) extended by the noun chunking (Section 3.4.2) and the subsequent re-ranking by ACL ğµğ¸ğ‘…ğ‘‡ (Section 3.4.3). Finally, the NSP method (Section 3.5.2) performs the pairing, which outputs the argument pairs.</figDesc><graphic coords="12,89.29,84.19,416.68,176.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="10,88.99,295.15,384.24,130.21"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="10,95.34,295.15,377.89,110.50"><row><cell></cell><cell>BM25</cell><cell>simple</cell><cell>sim</cell><cell>0.34</cell><cell>0.55</cell></row><row><cell></cell><cell>BM25</cell><cell>simple</cell><cell>NSP</cell><cell>0.47</cell><cell>0.65</cell></row><row><cell></cell><cell>BM25</cell><cell>simple</cell><cell>ACL ğµğ¸ğ‘…ğ‘‡ NSP</cell><cell>0.54</cell><cell>0.69</cell></row><row><cell>SCL</cell><cell>DLM</cell><cell>simple</cell><cell>NSP</cell><cell>0.36</cell><cell>0.62</cell></row><row><cell>SCL + ACL ğ‘†ğ‘‰ ğ‘€</cell><cell>DLM</cell><cell>simple</cell><cell>NSP</cell><cell>0.36</cell><cell>0.62</cell></row><row><cell cols="2">SCL + ACL ğµğ¸ğ‘…ğ‘‡ DLM</cell><cell>simple</cell><cell>NSP</cell><cell>0.39</cell><cell>0.57</cell></row><row><cell cols="2">SCL + ACL ğµğ¸ğ‘…ğ‘‡ DLM</cell><cell>simple</cell><cell>ACL ğµğ¸ğ‘…ğ‘‡ NSP</cell><cell>0.42</cell><cell>0.62</cell></row><row><cell>SCL</cell><cell>DLM</cell><cell>Boolean</cell><cell>NSP</cell><cell>0.48</cell><cell>0.59</cell></row><row><cell>SCL</cell><cell>DLM</cell><cell cols="2">Boolean + NC ACL ğµğ¸ğ‘…ğ‘‡ NSP</cell><cell>0.67</cell><cell>0.73</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,108.93,649.09,90.12,8.97"><p>https://www.google.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,108.93,660.05,81.96,8.97"><p>https://www.bing.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="1,108.93,671.00,88.33,8.97"><p>https://duckduckgo.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,108.93,670.99,81.76,8.97"><p>https://www.elastic.co</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="8,108.93,671.02,55.65,8.97"><p>https://spacy.io</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="14,112.66,159.14,382.47,10.91" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Quattrociocchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Scala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Sunstein</surname></persName>
		</author>
		<title level="m" coord="14,307.69,159.14,122.26,10.91">Echo chambers on facebook</title>
		<imprint>
			<publisher>SSRN</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,172.69,395.17,10.91;14,112.66,186.24,395.17,10.91;14,112.66,199.79,395.01,10.91;14,112.41,213.34,393.57,10.91;14,112.66,226.89,339.15,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,361.25,186.24,146.58,10.91;14,112.66,199.79,62.60,10.91">Overview of TouchÃ© 2022: Argument Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>FrÃ¶be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gurcke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,197.77,199.79,309.90,10.91;14,112.41,213.34,309.42,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction. 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<title level="s" coord="14,429.80,213.34,76.18,10.91;14,112.66,226.89,78.83,10.91">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="14,112.66,240.44,393.53,10.91;14,112.28,253.99,393.71,10.91;14,112.66,267.54,393.73,10.91;14,112.34,281.08,286.03,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,415.08,240.44,91.11,10.91;14,112.28,253.99,167.13,10.91">Data Acquisition for Argument Search: The args.me corpus</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-30179-8_4</idno>
	</analytic>
	<monogr>
		<title level="m" coord="14,112.66,267.54,241.68,10.91">German Conference on Artificial Intelligence (KI 2019)</title>
		<editor>
			<persName><forename type="first">C</forename><surname>BenzmÃ¼ller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Stuckenschmidt</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="48" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,294.63,394.53,10.91;14,112.30,308.18,394.98,10.91;14,112.66,321.73,394.53,10.91;14,112.66,335.28,393.33,10.91;14,112.66,348.83,303.16,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,264.52,308.18,221.57,10.91">Building an Argument Search Engine for the Web</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Al-Khatib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Puschmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dorsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Morari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,243.48,335.28,262.50,10.91;14,112.66,348.83,225.77,10.91">4th Workshop on Argument Mining (ArgMining 2017) at EMNLP, Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Ashley</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Green</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Habernal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Litman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Petasis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Reed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Slonim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Walker</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="49" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,362.38,394.53,10.91;14,112.66,375.93,364.25,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="14,162.46,375.93,215.34,10.91">Argument search: Assessing argument relevance</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Euchner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>HeilenkÃ¶tter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Weidmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1117" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,389.48,394.53,10.91;14,112.66,403.03,393.58,10.91;14,112.66,416.58,393.53,10.91;14,112.66,430.13,395.01,10.91;14,112.66,443.67,48.96,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,170.72,389.48,258.44,10.91">Quality-aware argument retrieval with topical clustering</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,310.85,403.03,195.40,10.91;14,112.66,416.58,239.57,10.91">Proceedings of the Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="14,275.43,430.13,148.42,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<meeting>the Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum<address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">September 21st -to -24th, 2021. 2936. 2021</date>
			<biblScope unit="page" from="2366" to="2373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,457.22,394.52,10.91;14,112.66,470.77,393.33,10.91;14,112.66,484.32,141.62,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,336.55,470.77,92.68,10.91">Overview of TouchÃ©</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>FrÃ¶be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,459.33,470.77,46.66,10.91;14,112.66,484.32,37.40,10.91">Argument retrieval</title>
		<imprint>
			<biblScope unit="page" from="2258" to="2284" />
			<date type="published" when="2021">2021. 2936. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,497.87,393.33,10.91;14,112.66,511.42,393.33,10.91;14,112.33,524.97,79.22,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,312.62,497.87,193.37,10.91;14,112.66,511.42,31.90,10.91">Efficient Pairwise Annotation of Argument Quality</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,168.25,511.42,337.74,10.91;14,112.33,524.97,48.20,10.91">The 58th annual meeting of the Association for Computational Linguistics (ACL), ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,538.52,393.33,10.91;14,112.66,552.07,306.62,10.91" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="14,425.21,538.52,80.77,10.91;14,112.66,552.07,276.12,10.91">Classification and clustering of arguments with contextualized word embeddings</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Daxenberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Stab</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,565.62,394.53,10.91;14,112.66,579.17,395.17,10.91;14,112.66,592.72,253.55,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,219.74,565.62,282.85,10.91">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,127.42,579.17,380.41,10.91;14,112.66,592.72,30.43,10.91">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,606.27,393.33,10.91;14,112.66,619.81,315.99,10.91" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="14,319.43,606.27,186.56,10.91;14,112.66,619.81,181.08,10.91">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>CoRR abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,633.36,393.60,10.91;14,112.66,646.91,394.53,10.91;14,112.66,660.46,340.45,10.91" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,199.81,633.36,165.57,10.91">NLTK: The natural language toolkit</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/P04-3031" />
	</analytic>
	<monogr>
		<title level="m" coord="14,395.31,633.36,110.95,10.91;14,112.66,646.91,390.38,10.91">Proceedings of the ACL Interactive Poster and Demonstration Sessions, Association for Computational Linguistics</title>
		<meeting>the ACL Interactive Poster and Demonstration Sessions, Association for Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="214" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,86.97,393.32,10.91;15,112.66,100.52,273.87,10.91" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,252.92,86.97,253.07,10.91;15,112.66,100.52,88.98,10.91">Implementing and evaluating phrasal query suggestions for proximity search</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Feuer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Savev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,210.00,100.52,92.60,10.91">Information Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="711" to="723" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,114.06,394.53,10.91;15,112.66,127.61,395.01,10.91;15,112.66,141.16,393.32,10.91;15,112.66,154.71,393.33,10.91;15,112.66,168.26,393.33,10.91;15,112.66,181.81,123.97,10.91" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="15,337.49,141.16,168.49,10.91;15,112.66,154.71,90.39,10.91">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,227.47,154.71,278.52,10.91;15,112.66,168.26,245.24,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
