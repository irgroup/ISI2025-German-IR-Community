<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,416.63,15.42;1,89.29,106.66,217.62,15.42;1,89.29,129.00,324.97,11.96">SEUPD@CLEF: Team Gamora on Argument Retrieval for Controversial Questions Notebook for the Touch√© Lab on Argument Retrieval at CLEF 2022</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,154.90,92.48,11.96"><forename type="first">Alessandro</forename><surname>Benetti</surname></persName>
							<email>alessandro.benetti.1@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,194.00,154.90,86.47,11.96"><forename type="first">Michele</forename><surname>De Togni</surname></persName>
							<email>michele.detogni@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,293.12,154.90,66.81,11.96"><forename type="first">Giovanni</forename><surname>Foti</surname></persName>
							<email>giovanni.foti@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,372.57,154.90,64.76,11.96"><forename type="first">Ralton</forename><surname>Lacini</surname></persName>
							<email>ralton.lacini@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,88.87,168.85,86.22,11.96"><forename type="first">Andrea</forename><surname>Matteazzi</surname></persName>
							<email>andrea.matteazzi.2@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,187.74,168.85,83.47,11.96"><forename type="first">Enrico</forename><surname>Sgarbossa</surname></persName>
							<email>enrico.sgarbossa.1@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,302.21,168.85,60.31,11.96"><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
							<email>ferro@dei.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,416.63,15.42;1,89.29,106.66,217.62,15.42;1,89.29,129.00,324.97,11.96">SEUPD@CLEF: Team Gamora on Argument Retrieval for Controversial Questions Notebook for the Touch√© Lab on Argument Retrieval at CLEF 2022</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">6537A0A95902E823E59096221F08BC64</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information retrieval</term>
					<term>Controversial questions</term>
					<term>Argument retrieval</term>
					<term>Query reduction</term>
					<term>Query boost</term>
					<term>Argument quality</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper is the report of the work we have done for Argument Retrieval CLEF 2022 Touch√© Task 1 as Gamora team. Argument Retrieval CLEF 2022 Touch√© Task 1 focuses on the problem of retrieving and ranking relevant pairs of sentences from a collection of arguments for a given controversial questions. After an analysis of the structure and the possibilities of manipulating document data, we concentrated our work on query management, applying methods of query expansion, reduction, query boost to conclude with satisfactory results with the last two solutions listed. We came up with two systems, one based on a two-stage operation, double index and double search, and one based on sentence pair ranking based on its argumentative quality, assessed by a machine learning model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this paper we describe our solution, as team Gamora, for the CLEF 2022 Touch√© <ref type="bibr" coords="1,469.03,424.41,13.00,10.91" target="#b0">[1]</ref> Task 1 <ref type="bibr" coords="1,96.93,437.96,11.58,10.91" target="#b1">[2]</ref>. According to Touch√© Task 1, our goal is to realize an information retrieval system that can support people searching arguments to be used in conversations. We developed a Java based system that takes as input a list of more than 360.000 documents from args.me corpus dataset <ref type="bibr" coords="1,124.26,478.61,12.99,10.91" target="#b2">[3]</ref> and it returns a ranked list with the best pairs of sentences. As required, the two sentences in this pair may come from two different arguments and they are both related to the right topic in order to have a global vision of pros and cons or some information about a certain discussion point. We decided to develop simultaneously two different solutions, in order to achieve different results and compare them. In the following we refer to these two as S1 and S2. All the differences between them are described in Section 3.</p><p>The paper is organized as follows: Section 3 describes our approach; Section 4 explains our experimental setup; Section 5 discusses our main findings; finally, Section 7 draws some conclusions and outlooks for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The task of retrieving arguments for controversial questions involves three main stages: the retrieval of suitable arguments, the augmentation of these results via query expansion and a reranking step of the retrieved arguments. For the retrieval of the arguments existing frameworks employ statistical language models such as Dirichlet LM or ranking functions like BM25 <ref type="bibr" coords="2,322.69,226.89,11.31,10.91" target="#b3">[4]</ref>. For the augmentation of the retrieved results, query expansion components in past experiments employ WordNet based systems <ref type="bibr" coords="2,493.13,240.44,12.85,10.91" target="#b4">[5]</ref> and neural models such as BERT <ref type="bibr" coords="2,237.53,253.99,12.84,10.91" target="#b5">[6]</ref> alike. For the ranking and reranking of the arguments it has been proven effective to leverage both the relevance and the quality of an argument with respect to the query <ref type="bibr" coords="2,386.69,281.08,11.32,10.91" target="#b3">[4]</ref>. The Webis-ArgQuality-20 Corpus <ref type="bibr" coords="2,138.77,294.63,13.00,10.91" target="#b6">[7]</ref> contains a collection of arguments from the argsme corpus, each with scores for its rhetorical, logical and dialectical quality along with a combined score for these three components. Gienapp <ref type="bibr" coords="2,189.77,321.73,12.85,10.91" target="#b7">[8]</ref> used a Support Vector Regressor trained on this dataset's combined quality score to assess the quality of an argument and used this measure along with topical clustering to rerank the documents. In a similar fashion, Green et al. <ref type="bibr" coords="2,384.27,348.83,11.37,10.91" target="#b5">[6]</ref> used BERT-based models trained on the Webis-ArgQuality-20 Corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Our system is composed by 3 sections: the first one takes care of doing the parse of all the documents, the second section create the index file and there is the third part, that is the search part. Both the solutions S1 and S2 have that structure but they differ in the methods that have been used. For each section, we are going to discuss the differences between the two approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Parsing</head><p>Considering the fact that we have a single file .csv that collects all documents, we want to avoid to load in ram the entire file to parse it. The solution was to use a iterator to read line by line and parse it.</p><p>When parsing documents, what we realised is that the premises field contains a number of sentences taken from the source-text field belonging to the context. These, in turn, are then subdivided by the full stop to create a vector of sentences and the sentence contained within the conclusion field is also added to the latter. Moreover, compared to last year, this year we have included in the dataset the previously mentioned context in which there are a series of data such as the title of the topic, all the source code, the publication date, the source, the author and more. Some of these data could be used to refine the search, for example based on the goodness of the source/author or comparing the date of publication and the date on which the query is made, but it is a deepening that for complexity and time we have not pursued.</p><p>We manually inspected the corpus to gather information about its structure. Documents are sourced from four different debate discussion portals: "idebate", "debate.org", "debatepedia", "canadian-parliament". The structure of the documents from these sources is the same for the id, conclusion, premises, sentences fields but differ in the subfields of the field context, where, among other differences, canadian-parliament sourced documents don't provide a "discussionTitle" subfield but provide a "topic" subfield instead. Also, out of the 365'308 documents provided in the corpus, 998 ids of these documents are shared with other documents, with some ids getting repeated in up to 8 documents. This results in a total of 2'214 documents with non-unique ids. Since our task is to retrieve a pair of sentences, we also analyzed the sentences of the corpus. In total, the corpus contains 6'123'792 sentences across all the documents. Since as noted above, some of the documents are non-unique, we counted the sentences with unique document id and unique sentence text bringing the total number of sentences to 5'337'409, meaning that around 13% of the sentences in the corpus are either duplicates or from a duplicate document..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Indexing</head><p>After parsing the documents, we have created a class ArgsParsed in which we have inserted a series of functions that return values in string form for the various main fields that we have decided to index. To the Lucene document we decided to add:</p><p>‚Ä¢ the id of the document ‚Ä¢ the discussionTitle, if present, otherwise the topic ‚Ä¢ the text of the sentence for each sentences / premises text ‚Ä¢ the conclusion ‚Ä¢ the stance In the S1 branch we decided to index the text of sentences concatenated because it is already filtered of some non-useful words and in the S2 directly the premises field.</p><p>In S1 we used a custom Analyzer to which we have added the standard tokenizer, a token filter of the KStem type and another lower case. On the other hand, in S2 we experimented with different combinations of filters, thanks to the fact that we created a system that allows us to run it several times and update the parameters at each repetition. In the end, the best results in terms of nDCG were achieved using stopwords filter, length filter, kstemmer and the standard tokenizer.</p><p>We provide two tables with the results of trec_eval. The Table <ref type="table" coords="3,372.86,574.75,4.97,10.91" target="#tab_1">1</ref> refer to the relevance results, while the Table <ref type="table" coords="3,160.09,588.29,5.07,10.91" target="#tab_0">2</ref> refers to the quality results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Searching</head><p>At this stage, as with document indexing, we parsed the file containing the queries. The document contained 50 questions, each with four tags: num, topic, description and narrative. We only considered the identifier and the title (topic) to carry out the search.</p><p>At this point we added the queries to Lucene's BooleanQueryBuilder, carried out the search and obtained a first run but since it was the first it became the yardstick for our subsequent attempts, listed later in a dedicated section Results and Discussion, Section 5.</p><p>The file that Lucene returns from the run, however, does not conform to the task requested by Touch√© because we have a file with the results of the best documents associated with the queries ordered by score, but what we need to obtain is a file to which, for each query, the two best argumentative sentences are associated and these two can also belong to different documents, as long as they do not contradict each other. In order to better understand the scenario of argumentative sentences, we consulted the article about Identifying Argumentative Questions in Web Search Logs <ref type="bibr" coords="4,220.46,213.89,11.43,10.91" target="#b8">[9]</ref>.</p><p>Here, we again divide the two approaches: in S1 we decided to create a new index from the sentences alone based on the results of the first run and then perform a new search on the latter, while in S2 we applied the second idea, i.e., again based on the results of the first run, create all possible combinations of pairs of sentences and then perform a relevance analysis and return the highest scoring pairs in the correct format, thus avoiding a new index and a subsequent search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">S1 solution</head><p>Limiting the maximum number of documents to be retrieved for each topic, we considered a sample of sentences from all these documents to create the second index, discarding sentences that are too short assuming that a sentence, in order to be argumentative, must contain at least a minimum number of words. We also decided to discard those sentences that are too long because we noticed that very long sentences, i.e. that are composed of more than 10000 characters, always contains strange characters such as base64 images or binary strings. As a first attempt, this second search was carried out by considering the topic title without any modification in order to verify its correct functioning and obtain a starting point to compare future alternative solutions, the first of which was based on query expansion made by WordNet. By manually analyzing some sentences obtained for the various topics, it didn't work well neither adding 1-2 synonyms to the topic title nor replacing original words with synonyms, probably because the quality of the synonyms found by WordNet was often poor as it caused the topic to drift. We then also tried a trained online model (explained in a separate section 3.4) to get more relevant synonyms but again no sign of major improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Heuristics</head><p>Query reduction So, we tried another solution, based on a manual query reduction in order to detect words that we wanted to exclude from the query (e.g. articles, adverbs), keeping only those 2-6 main terms of the query title. This version was born out of an attempt to give weight to certain terms in the query in order to make them more important.</p><p>Query boosting This feature consists in splitting the original topic in the list of its words and for each word create a TermQuery. This is later wrapped in a BoostQuery object in which we apply a boost, consisting in a numerical value. Every BoostQuery object is then added to a BooleanQuery in order to build the final query. This technique is used in combination with Query decorators, explained below, in order to give more weight to original topic words, rather than the decorators. The results seemed satisfactory, we read a tens of sentences for each topic in order to be confident we were heading in the right direction.</p><p>Query decorators This feature consists in attaching a number of keywords, called decorators, to the original query. An example of such used keywords is: consequence, conclusion, reason, proof, fact, argument, therefore, show, result etc. These, when attached to the original query, will increase the chance that the retrieved sentence will be more argumentative.</p><p>An example of how a query changed in these steps is given below:</p><p>1. Should teachers get tenure? 2. teachers tenure 3. teachers*4 tenure*4 consequence conclusion reason proof evidence therefore show Once again, we improved on the previous results, obtaining sentences of a reasonable length, inherent to the topic and in agreement with each other.</p><p>For this kind of task attempts were made with both LMDirichlet similarity and BM25 similarity and only with the last one we obtain logically correlated and relevant sentences. With LMDirichlet many of the sentences with high scores were completely out of context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">S2 solution</head><p>In this approach we decided to choose the best sentence pairs using an argument quality regression model that evaluates the argumentative quality of each sentence obtained as concatenation of the sentences in the pair. The sentence pairs to be evaluated are gathered starting from the best documents retrieved in the run file produced during a Lucene-based search on for a list of queries (or topics as in our case). For each topic, the ids of the best documents are gathered, and for each document the sentences with the best quality are kept in a list shared between documents for the same topic. Then all possible combinations of rank two between sentences in this list are computed and the concatenation of the two sentences is evaluated according to the quality regression model. The final score of a pair is obtained as weighted average between the predicted quality of the composed sentence and the mean of the score of the two documents the original sentences belong to.</p><p>To evaluate the quality of the sentences we trained a Support Vector Regressor model on the "Webis Argument Quality Corpus 2020 (Webis-ArgQuality-20)" dataset <ref type="bibr" coords="5,407.10,587.79,12.86,10.91" target="#b6">[7]</ref> in a similar way to Gienapp <ref type="bibr" coords="5,124.52,601.34,15.10,10.91" target="#b7">[8]</ref> and Bundesmann et al. <ref type="bibr" coords="5,244.89,601.34,15.28,10.91" target="#b9">[10]</ref>. We created two models that differ between them only in the vectorizer model. One uses a TFIDF Vectorizer which considers only frequency of tokens, the second one is an encoder based on the SBERT architecture <ref type="bibr" coords="5,362.99,628.44,17.75,10.91" target="#b10">[11]</ref> which encodes also relevant semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Model for query expansion</head><p>In order to improve the search, we have tried to enhance the queries through the expansion of important keywords with synonyms. In particular, in order to find the more important terms in the queries, we have exploited Part of Speech (PoS) <ref type="bibr" coords="6,350.66,134.63,17.76,10.91" target="#b11">[12]</ref> for selecting just the keywords corresponding to meaningful tags. Then, once the more meaningful terms have been extracted, for each of them we have found the top two closest words to add to the query, in such a way to avoid the addition of derived words with respect to the corresponding word on the query. The model used to perform such a task is GloVE <ref type="bibr" coords="6,284.95,188.83,16.18,10.91" target="#b12">[13]</ref>, which is an unsupervised learning algorithm for obtaining vector representations for words. Its training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. In particular, we found a family of pretrained models from <ref type="bibr" coords="6,196.90,243.03,17.91,10.91" target="#b13">[14]</ref> and after some experiments, it turned out that the best model for our documents collection was "glove-twitter-200", which is based on 2B tweets, 27B tokens, 1.2M vocab, all uncased. The interesting feature of this family of pretrained model is the implementation of the method "most_similar", which returns, given a word, the top ten closest words in vector space sense with a score of closeness for each of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>In order to ensure compatibility and repeatability of the experiments, the used paradigm was the Cranfield one. This approach is based on three main components:</p><p>‚Ä¢ Documents collection: which contains all the documents to index (the information to retrieve); ‚Ä¢ Topics: which are the surrogates for the user query; ‚Ä¢ Relevance Judgment: this is written in the qres.txt file and contains the ground-truth of the ranked lists obtained querying for the topics on the documents collection;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Documents collection</head><p>The documents collection was provided by CLEF and contains 365408 documents about various topics. The collection is an 9 GB file in csv format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topics</head><p>Also the topic file was provided by CLEF and it contains 50 different topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relevance Judgment</head><p>In order to test the different analyzers performance, the qrels were used, so the output ranked lists were computed also from the 50 topics. Then for the submission the 2021 topics list has been used.</p><p>For both solutions S1 and S2, our work is based on the following experimental setups:</p><p>‚Ä¢ Used collections: Apache Maven, Lucene;</p><p>‚Ä¢ Java JDK version 18;</p><p>‚Ä¢ Version control system: git;</p><p>‚Ä¢ Repository: https://bitbucket.org/upd-dei-stud-prj/seupd2122-javacafe/src/master ‚Ä¢ During the develop and the experimentation we have used our own computer and in the end we have run our code using Tira;</p><p>We provide three different results for S1 solution by manually enabling/disabling certain features, resulting in three different runs.</p><p>The first one, called "StandardDoubleIndex" consists in a standard Java/Lucene pipeline carried out by first searching the topic title in the first index and then on the second index, as described here. 3.3.1</p><p>The second pipeline, called "HeuristicsDoubleIndex" follows the same path as the first one, but the following heuristics are applied:</p><p>‚Ä¢ Query reduction; ‚Ä¢ Query decoration; ‚Ä¢ Query boosting;</p><p>As a third attempt, called "HeuristicsOnlyQueryReductionDoubleIndex" only the query reduction technique is applied.</p><p>In our second solution S2 we created a class used for creation and searching on the index. The input parameter for the indexer and searcher objects are given as command line arguments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Discussion</head><p>In this section we provide graphical and numerical results about the experiments we conducted during the project development. The two different solutions S1 and S2 had in common the first run, which is the one on the documents indexing, so the following graph is very similar for both.</p><p>In the following graph 1 we report the values of the interpolated precision against recall analysis of the three different method we have used which differ by their similarity: kstemstopengpos-bm25, kstemstopengpos-lmdir and kstemstopengpos-multi. The graph shows that Lucene's Multi-Similarity gave us better result, in particular comparing to BM25Similarity. Indeed, it is possible to see a bigger Area Under the Curve (AUC) for the curve corresponding to MultiSimilarity and this translates in a better trade-off between precision and recall. Such trade-off can be better seen in Table <ref type="table" coords="7,150.44,543.74,5.07,10.91" target="#tab_1">1</ref> since the F-score is the harmonic mean between precision and recall. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison about set measures and rank based</head><p>In the following configuration, we used the stopword filter in the analyzer. The following tables report the results of trec_eval. The Table <ref type="table" coords="8,281.09,361.65,5.17,10.91" target="#tab_1">1</ref> refer to the relevance results, while the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Statistical Analysis</head><p>In this section we conduct a study of the performances of our systems based on the 5 runs we submitted to CLEF <ref type="bibr" coords="9,191.51,358.95,12.95,10.91" target="#b0">[1]</ref> (Table <ref type="table" coords="9,237.66,358.95,3.62,10.91" target="#tab_4">3</ref>). The specific settings of each run are indicated in Table <ref type="table" coords="9,500.08,358.95,3.79,10.91" target="#tab_5">4</ref>. Furthermore each of these runs also use a lowercase filter and the Krovetz Stemmer (KStem) in the Analyzer and also skip repeated sentences and sentences too long or too short. Considering the boxplots of the runs based on Average Precision (AP) Figure <ref type="figure" coords="9,371.52,399.59,9.86,10.91" target="#fig_1">2a</ref> and Normalized Discounted Cumulative Gain at 10 (nDCG@10) Figure <ref type="figure" coords="9,279.23,413.14,8.52,10.91" target="#fig_1">2b</ref>, it is possible to see how the medians are similar among all the runs. To study this phenomenon an analysis of variance (ANOVA) test with a significance of 0.05 was conducted on the nDCG@10 and Average Precision performances of the runs. Table <ref type="table" coords="9,140.82,453.79,9.87,10.91" target="#tab_4">3a</ref> and Table <ref type="table" coords="9,198.59,453.79,10.25,10.91" target="#tab_4">3b</ref> show that the test fails to reject the null hypothesis, hence the runs are not statistically different under the significance level we considered. Figure <ref type="figure" coords="9,434.40,467.34,9.86,10.91" target="#fig_3">3a</ref> and Figure <ref type="figure" coords="9,495.74,467.34,10.24,10.91" target="#fig_3">3b</ref> highlight how runs from the two different solutions S1 and S2 compare between them, with runs from the same solutions being similar between them and runs from different solutions being more dissimilar between them, particularly when looking at nDCG@10 performance. To confirm this analysis and also whether the runs are really different in terms of mean performance, so if the null hypothesis ùêª 0 : ùúá ùë• = ùúá ùë¶ for runs x and y holds, we conducted Student's t test for some runs groups in Table <ref type="table" coords="9,185.15,548.63,3.80,10.91" target="#tab_6">5</ref>. From the p-values it turns out that ùêª 0 is always failed to reject since the p-values are always higher than 0.05, hence the means of these runs are not statistically different.</p><p>Another interesting results visible from the boxplots is that the order of run's performance is approximately the same, with higher mean performance in S1 compared to S2. Performance per topic of our system is highlighted in Figure <ref type="figure" coords="9,285.10,616.38,5.10,10.91">4</ref> which shows the nDCG@10 for each one of the 50 topics.        </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Discussion</head><p>Since the task of our system is, given a query, to retrieve a pair of sentences which have to be as much related as possible each other and with the query (e.g. they do not contradict each other), the goal is to find a way to evaluate the quality of two sentences as they would come from the same argumentation. Indeed, our idea is to integrate a machine learning model in order to be able to re-rank sentence pairs retrieved by a language model. To achieve this goal, datasets such as Webis-ArgQuality-20 <ref type="bibr" coords="12,254.24,199.99,12.69,10.91" target="#b6">[7]</ref> have been successfully used as training set for sentence quality evaluation models <ref type="bibr" coords="12,210.11,213.54,16.42,10.91" target="#b9">[10]</ref>. However datasets of this size limit both the performance of a model and the structure a model itself can have since more complicated models that could achieve better efficiency need more data. Thus, bigger and more diverse datasets on sentence argumentativeness and quality could lead to more expressive and precise models alike, along with the employment or creation of ad-hoc neural or natural language processing models for this task. The diversity of the task led us not to focus too much on the results of previous years, so particular manipulations on the documents were not of interest to us, but instead we turned to operate on the queries, thus creating a kind of pseudo-relevance feedback system in the S1 approach. Staying with the latter, we notice that results are visibly improving during various attempt, but the system has one significant downside: speed. The fact that we have to create a second index immediately after performing the first search means that we cannot prepare it beforehand because we need the query, so creating the index and then performing a second search after the first one slows down the procedure considerably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Conclusion and Future Work</head><p>In conclusion, we developed two different systems for this task, a system more focused on retrieving relevant sentences and a second system that searches for sentences pair of high argumentative quality. According to relevance judgements the first solution (S1) works better overall but the strategies employed by the second solution (S2) are close or better in performance when looking at quality and coherence judgements respectively. For this reason one step forward for our system could be merging the two strategies we propose. While the solution S1 provides a more refined set of sentences to choose from, it employs a naive strategy for the scoring of the couples it creates. In this context, the more thoughtful ranking of S2 that also tries to leverage the sentence pair quality as a sentence of its own could greatly boost the system's overall performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,89.29,277.98,325.56,8.93"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Plot of the Interpolated precision against recall of the three methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,89.29,430.48,305.54,8.93;10,310.14,241.20,195.84,158.78"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Boxplots of the 5 runs in decreasing order of mean performance.</figDesc><graphic coords="10,310.14,241.20,195.84,158.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="11,143.99,344.50,86.16,9.96;11,376.78,344.50,57.28,9.96"><head></head><label></label><figDesc>(a) Average Precision (b) nDCG@10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="11,89.29,368.12,181.89,8.93;11,153.07,392.99,289.14,233.09"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Analysis of Variance of the 5 runs.</figDesc><graphic coords="11,153.07,392.99,289.14,233.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="11,89.29,638.64,153.69,9.43"><head>Figure 4 : 3 7</head><label>43</label><figDesc>Figure 4: nDCG@10 per Topic of S1 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,89.29,361.65,416.70,248.46"><head>Table 2</head><label>2</label><figDesc>refers to the quality results. Note that Multi value under Similarity column is the Lucene's MultiSimilarity, that we built by combining BM25 and LMDirichlet similarities.</figDesc><table coords="8,169.98,422.32,255.32,187.79"><row><cell>Run ID</cell><cell>F-Score</cell><cell>Precision</cell><cell>Recall</cell><cell>nDCG@5</cell><cell>Similarity</cell><cell>Len. Filter</cell><cell>Eng. Filter</cell><cell>Stemmer</cell></row><row><cell>porterstop</cell><cell>0.4152</cell><cell>0.3704</cell><cell>0.5502</cell><cell>0.6542</cell><cell>Multi</cell><cell></cell><cell></cell><cell>Porter</cell></row><row><cell>kstemstopengpos</cell><cell>0.4094</cell><cell>0.3656</cell><cell>0.5426</cell><cell>0.6737</cell><cell>Multi</cell><cell></cell><cell>O</cell><cell>KStem</cell></row><row><cell>kstemstop</cell><cell>0.4087</cell><cell>0.3648</cell><cell>0.5421</cell><cell>0.6733</cell><cell>Multi</cell><cell></cell><cell></cell><cell>KStem</cell></row><row><cell>kstemlenstopengpos</cell><cell>0.4065</cell><cell>0.3624</cell><cell>0.5397</cell><cell>0.6744</cell><cell>Multi</cell><cell>O</cell><cell>O</cell><cell>KStem</cell></row><row><cell>kstemstoplen</cell><cell>0.4061</cell><cell>0.3620</cell><cell>0.5394</cell><cell>0.6702</cell><cell>Multi</cell><cell>O</cell><cell></cell><cell>KStem</cell></row><row><cell>stop</cell><cell>0.4009</cell><cell>0.3604</cell><cell>0.5294</cell><cell>0.6403</cell><cell>Multi</cell><cell></cell><cell></cell><cell></cell></row><row><cell>stopengpos</cell><cell>0.3982</cell><cell>0.3580</cell><cell>0.5257</cell><cell>0.6444</cell><cell>Multi</cell><cell></cell><cell>O</cell><cell></cell></row><row><cell>stoplen</cell><cell>0.3969</cell><cell>0.3572</cell><cell>0.5234</cell><cell>0.6398</cell><cell>Multi</cell><cell>O</cell><cell></cell><cell></cell></row><row><cell>kstemstopengpos</cell><cell>0.3832</cell><cell>0.3400</cell><cell>0.5136</cell><cell>0.5898</cell><cell>BM25</cell><cell></cell><cell>O</cell><cell>KStem</cell></row><row><cell>kstemlenstopengpos</cell><cell>0.3827</cell><cell>0.3396</cell><cell>0.5130</cell><cell>0.5900</cell><cell>BM25</cell><cell>O</cell><cell>O</cell><cell>KStem</cell></row><row><cell>kstemstop</cell><cell>0.3827</cell><cell>0.3396</cell><cell>0.5130</cell><cell>0.5864</cell><cell>BM25</cell><cell></cell><cell></cell><cell>KStem</cell></row><row><cell>kstemstopengpos</cell><cell>0.3819</cell><cell>0.3424</cell><cell>0.5076</cell><cell>0.6439</cell><cell>LMDir</cell><cell></cell><cell>O</cell><cell>KStem</cell></row><row><cell>kstemstop</cell><cell>0.3815</cell><cell>0.3420</cell><cell>0.5072</cell><cell>0.6446</cell><cell>LMDir</cell><cell></cell><cell></cell><cell>KStem</cell></row><row><cell>kstemstoplen</cell><cell>0.3806</cell><cell>0.3376</cell><cell>0.5108</cell><cell>0.5925</cell><cell>BM25</cell><cell>O</cell><cell></cell><cell>KStem</cell></row><row><cell>porterstop</cell><cell>0.3804</cell><cell>0.3368</cell><cell>0.5077</cell><cell>0.5699</cell><cell>BM25</cell><cell></cell><cell></cell><cell>Porter</cell></row><row><cell>porterstop</cell><cell>0.3782</cell><cell>0.3400</cell><cell>0.4960</cell><cell>0.6105</cell><cell>LMDir</cell><cell></cell><cell></cell><cell>Porter</cell></row><row><cell>kstemstoplen</cell><cell>0.3771</cell><cell>0.3384</cell><cell>0.5004</cell><cell>0.6406</cell><cell>LMDir</cell><cell>O</cell><cell></cell><cell>KStem</cell></row><row><cell>stop</cell><cell>0.3758</cell><cell>0.3384</cell><cell>0.4995</cell><cell>0.6146</cell><cell>LMDir</cell><cell></cell><cell></cell><cell></cell></row><row><cell>stop</cell><cell>0.3752</cell><cell>0.3344</cell><cell>0.5006</cell><cell>0.5877</cell><cell>BM25</cell><cell></cell><cell></cell><cell></cell></row><row><cell>stopengpos</cell><cell>0.3751</cell><cell>0.3380</cell><cell>0.4983</cell><cell>0.6146</cell><cell>LMDir</cell><cell></cell><cell>O</cell><cell></cell></row><row><cell>stopengpos</cell><cell>0.3751</cell><cell>0.3344</cell><cell>0.5004</cell><cell>0.5903</cell><cell>BM25</cell><cell></cell><cell>O</cell><cell></cell></row><row><cell>stoplen</cell><cell>0.3724</cell><cell>0.3364</cell><cell>0.4909</cell><cell>0.6123</cell><cell>LMDir</cell><cell>O</cell><cell></cell><cell></cell></row><row><cell>stoplen</cell><cell>0.3649</cell><cell>0.3248</cell><cell>0.4894</cell><cell>0.5911</cell><cell>BM25</cell><cell>O</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,185.46,622.37,224.02,10.91"><head>Table 1 :</head><label>1</label><figDesc>Relevance table for S2 ordered by F-Score.</figDesc><table coords="9,169.98,86.08,255.32,187.79"><row><cell>Run ID</cell><cell>F-Score</cell><cell>Precision</cell><cell>Recall</cell><cell>nDCG@5</cell><cell>Similarity</cell><cell>Len. Filter</cell><cell>Eng. Filter</cell><cell>Stemmer</cell></row><row><cell>porterstop</cell><cell>0.4315</cell><cell>0.4856</cell><cell>0.3992</cell><cell>0.8092</cell><cell>Multi</cell><cell></cell><cell></cell><cell>Porter</cell></row><row><cell>kstemstopengpos</cell><cell>0.4277</cell><cell>0.4804</cell><cell>0.3962</cell><cell>0.7986</cell><cell>Multi</cell><cell></cell><cell>O</cell><cell>KStem</cell></row><row><cell>kstemlenstopengpos</cell><cell>0.4272</cell><cell>0.4792</cell><cell>0.3962</cell><cell>0.7957</cell><cell>Multi</cell><cell>O</cell><cell>O</cell><cell>KStem</cell></row><row><cell>kstemstop</cell><cell>0.427</cell><cell>0.4796</cell><cell>0.3956</cell><cell>0.8006</cell><cell>Multi</cell><cell></cell><cell></cell><cell>KStem</cell></row><row><cell>kstemstoplen</cell><cell>0.4268</cell><cell>0.4788</cell><cell>0.3957</cell><cell>0.7915</cell><cell>Multi</cell><cell>O</cell><cell></cell><cell>KStem</cell></row><row><cell>stop</cell><cell>0.4198</cell><cell>0.4728</cell><cell>0.3881</cell><cell>0.8052</cell><cell>Multi</cell><cell></cell><cell></cell><cell></cell></row><row><cell>stopengpos</cell><cell>0.4178</cell><cell>0.4704</cell><cell>0.3864</cell><cell>0.8043</cell><cell>Multi</cell><cell></cell><cell>O</cell><cell></cell></row><row><cell>stoplen</cell><cell>0.4175</cell><cell>0.4704</cell><cell>0.3859</cell><cell>0.8028</cell><cell>Multi</cell><cell>O</cell><cell></cell><cell></cell></row><row><cell>kstemstop</cell><cell>0.4098</cell><cell>0.4612</cell><cell>0.3791</cell><cell>0.7963</cell><cell>LMDir</cell><cell></cell><cell></cell><cell>KStem</cell></row><row><cell>kstemstopengpos</cell><cell>0.4094</cell><cell>0.4608</cell><cell>0.3787</cell><cell>0.7979</cell><cell>LMDir</cell><cell></cell><cell>O</cell><cell>KStem</cell></row><row><cell>kstemstoplen</cell><cell>0.4026</cell><cell>0.4528</cell><cell>0.3727</cell><cell>0.7869</cell><cell>LMDir</cell><cell>O</cell><cell></cell><cell>KStem</cell></row><row><cell>stop</cell><cell>0.4025</cell><cell>0.4544</cell><cell>0.3717</cell><cell>0.7942</cell><cell>LMDir</cell><cell></cell><cell></cell><cell></cell></row><row><cell>stopengpos</cell><cell>0.4022</cell><cell>0.454</cell><cell>0.3713</cell><cell>0.794</cell><cell>LMDir</cell><cell></cell><cell>O</cell><cell></cell></row><row><cell>porterstop</cell><cell>0.4017</cell><cell>0.4528</cell><cell>0.3712</cell><cell>0.8056</cell><cell>LMDir</cell><cell></cell><cell></cell><cell>Porter</cell></row><row><cell>stoplen</cell><cell>0.4003</cell><cell>0.452</cell><cell>0.3696</cell><cell>0.7889</cell><cell>LMDir</cell><cell>O</cell><cell></cell><cell></cell></row><row><cell>stop</cell><cell>0.3876</cell><cell>0.4336</cell><cell>0.3605</cell><cell>0.7164</cell><cell>BM25</cell><cell></cell><cell></cell><cell></cell></row><row><cell>stopengpos</cell><cell>0.3874</cell><cell>0.4332</cell><cell>0.3603</cell><cell>0.7189</cell><cell>BM25</cell><cell></cell><cell>O</cell><cell></cell></row><row><cell>kstemstop</cell><cell>0.3871</cell><cell>0.4316</cell><cell>0.3607</cell><cell>0.7125</cell><cell>BM25</cell><cell></cell><cell></cell><cell>KStem</cell></row><row><cell>kstemstopengpos</cell><cell>0.387</cell><cell>0.4316</cell><cell>0.3605</cell><cell>0.7153</cell><cell>BM25</cell><cell></cell><cell>O</cell><cell>KStem</cell></row><row><cell>kstemlenstopengpos</cell><cell>0.3848</cell><cell>0.4288</cell><cell>0.3586</cell><cell>0.7012</cell><cell>BM25</cell><cell>O</cell><cell>O</cell><cell>KStem</cell></row><row><cell>porterstop</cell><cell>0.3847</cell><cell>0.43</cell><cell>0.3578</cell><cell>0.7141</cell><cell>BM25</cell><cell></cell><cell></cell><cell>Porter</cell></row><row><cell>kstemstoplen</cell><cell>0.3826</cell><cell>0.4264</cell><cell>0.3566</cell><cell>0.7016</cell><cell>BM25</cell><cell>O</cell><cell></cell><cell>KStem</cell></row><row><cell>stoplen</cell><cell>0.379</cell><cell>0.4232</cell><cell>0.353</cell><cell>0.7117</cell><cell>BM25</cell><cell>O</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,191.21,286.13,212.54,10.91"><head>Table 2 :</head><label>2</label><figDesc>Quality table for S2 ordered by F-Score.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,134.99,138.49,326.78,66.87"><head>Table 3 :</head><label>3</label><figDesc>Run Tags.</figDesc><table coords="10,134.99,163.49,326.78,41.87"><row><cell>Run tag</cell><cell>Sol.</cell><cell>Sim.</cell><cell>Analyzer filters</cell><cell></cell><cell>Notes</cell></row><row><cell>S1 1</cell><cell>S1</cell><cell>BM25</cell><cell></cell><cell></cell><cell>All heuristics (query reduction decorators boosting)</cell></row><row><cell>S1 2</cell><cell>S1</cell><cell>BM25</cell><cell></cell><cell></cell><cell>S1 baseline (only java-lucene pipeline no heuristics)</cell></row><row><cell>S1 3</cell><cell>S1</cell><cell>BM25</cell><cell></cell><cell></cell><cell>Only query reduction heuristic</cell></row><row><cell>S2 1</cell><cell>S2</cell><cell>Multi</cell><cell>Stopword filter</cell><cell>English possessive filter</cell><cell>TF-IDF vectorizer for ML reranker</cell></row><row><cell>S2 2</cell><cell>S2</cell><cell>Multi</cell><cell>Stopword filter</cell><cell>English possessive filter</cell><cell>SBERT encoder for ML reranker</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,251.73,217.00,91.48,10.91"><head>Table 4 :</head><label>4</label><figDesc>Run Details.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="10,160.52,524.00,275.60,92.20"><head>Table 5 :</head><label>5</label><figDesc>Students t test under ùêª 0 among pairs of runs x and y. ùêª 0 : ùúá ùë• = ùúá ùë¶ , ùêª 1 : ùúá ùë• Ã∏ = ùúá ùë¶</figDesc><table coords="10,172.92,563.45,249.44,52.75"><row><cell>Source</cell><cell>SS</cell><cell>df</cell><cell>MS</cell><cell>F</cell><cell>Prob&gt;F</cell></row><row><cell cols="2">Columns 0.0029</cell><cell>4</cell><cell cols="3">7.2455e-04 2.0385 0.0896</cell></row><row><cell>Error</cell><cell cols="3">0.0871 245 3.5544e-04</cell><cell></cell><cell></cell></row><row><cell>Total</cell><cell cols="2">0.0900 249</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="10,256.11,629.20,82.73,10.91"><head>Table 6 :</head><label>6</label><figDesc>Anova AP</figDesc><table coords="11,182.28,85.89,230.73,52.75"><row><cell>Source</cell><cell>SS</cell><cell>df</cell><cell>MS</cell><cell>F</cell><cell>Prob&gt;F</cell></row><row><cell cols="2">Columns 0.2271</cell><cell>4</cell><cell cols="3">0.0568 1.7565 0.1383</cell></row><row><cell>Error</cell><cell cols="3">7.9184 245 0.0323</cell><cell></cell><cell></cell></row><row><cell>Total</cell><cell cols="2">8.1455 249</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="11,238.85,151.64,117.24,10.91"><head>Table 7 :</head><label>7</label><figDesc>Anova nDCG@10</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="13,112.66,111.28,395.17,10.91;13,112.66,124.83,395.17,10.91;13,112.66,138.38,395.01,10.91;13,112.41,151.93,393.57,10.91;13,112.66,165.48,339.15,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,361.25,124.83,146.58,10.91;13,112.66,138.38,62.60,10.91">Overview of Touch√© 2022: Argument Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fr√∂be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gurcke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,197.77,138.38,309.90,10.91;13,112.41,151.93,309.42,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction. 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<title level="s" coord="13,429.80,151.93,76.18,10.91;13,112.66,165.48,78.83,10.91">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="13,112.66,179.03,394.53,10.91;13,112.33,192.57,394.36,10.91;13,112.66,206.12,160.31,10.91" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fr√∂be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gurcke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<ptr target="https://webis.de/events/touche-22/shared-task-1.html" />
		<title level="m" coord="13,112.33,192.57,281.06,10.91">Touch√© Task 1: Argument Retrieval for Controversial Questions</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,219.67,393.53,10.91;13,112.28,233.22,393.71,10.91;13,112.66,246.77,393.73,10.91;13,112.34,260.32,286.03,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,415.08,219.67,91.11,10.91;13,112.28,233.22,167.13,10.91">Data Acquisition for Argument Search: The args.me corpus</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-30179-8_4</idno>
	</analytic>
	<monogr>
		<title level="m" coord="13,112.66,246.77,241.68,10.91">German Conference on Artificial Intelligence (KI 2019)</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Benzm√ºller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Stuckenschmidt</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="48" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,273.87,394.52,10.91;13,112.66,287.42,393.33,10.91;13,112.66,300.97,379.30,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="13,338.25,287.42,167.73,10.91;13,112.66,300.97,35.82,10.91">Overview of touch√© 2021: Argument retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fr√∂be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/#paper-205" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2258" to="2284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,314.52,393.33,10.91;13,112.66,328.07,394.61,10.91;13,112.66,341.62,179.78,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="13,295.58,314.52,210.41,10.91;13,112.66,328.07,218.66,10.91">A search engine system for touch√© argument retrieval task to answer controversial questions</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Raimondi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Alessio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Levorato</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/#paper-217" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2423" to="2440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,355.17,393.53,10.91;13,112.66,368.71,394.03,10.91;13,112.66,382.26,52.20,10.91" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="13,269.29,355.17,236.90,10.91;13,112.66,368.71,84.42,10.91">Exploring bert synonyms and quality prediction for argument retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Moroldo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Valente</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/#paper-213" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2374" to="2388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,395.81,395.17,10.91;13,112.28,409.36,397.86,10.91;13,112.66,425.35,43.94,7.90" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.3780049</idno>
		<ptr target="https://doi.org/10.5281/zenodo.3780049.doi:10.5281/zenodo.3780049" />
		<title level="m" coord="13,305.96,395.81,201.87,10.91;13,112.28,409.36,59.38,10.91">Webis Argument Quality Corpus 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Webis-ArgQuality-20</note>
</biblStruct>

<biblStruct coords="13,112.66,436.46,395.01,10.91;13,112.66,450.01,256.15,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="13,167.88,436.46,254.16,10.91">Quality-aware argument retrieval with topical clustering</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/#paper-212" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2366" to="2373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,463.56,393.32,10.91;13,112.26,477.11,102.40,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Braslavski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<title level="m" coord="13,331.75,463.56,174.24,10.91;13,112.26,477.11,70.48,10.91">Identifying argumentative questions in web search logs</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,490.66,393.33,10.91;13,112.66,504.21,185.31,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,291.85,490.66,214.13,10.91;13,112.66,504.21,29.66,10.91">Creating an argument search engine for online debates</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bundesmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Christ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Richter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,169.35,504.21,98.72,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,517.76,269.60,10.91" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="13,166.58,517.76,58.80,10.91">Sentence bert</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<ptr target="https://www.sbert.net" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,531.30,394.52,10.91;13,112.66,544.85,219.65,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="13,230.38,531.30,269.34,10.91">Natural language processing with python. o&apos;reilly media inc</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<ptr target="https://www.nltk.org/book/ch05.html" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,558.40,394.53,10.91;13,112.66,571.95,227.14,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D M</forename><surname>Jeffrey Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<ptr target="https://nlp.stanford.edu/pubs/glove.pdf" />
		<title level="m" coord="13,305.91,558.40,196.90,10.91">Glove: Global vectors for word representation</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,585.50,394.04,10.91;13,112.66,599.05,57.60,10.91" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="13,199.64,585.50,55.27,10.91">Gensim-data</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Technologies</surname></persName>
		</author>
		<ptr target="https://github.com/RaRe-Technologies/gensim-data" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,612.60,393.33,10.91;13,112.66,626.15,393.32,10.91;13,112.66,639.70,367.93,10.91" xml:id="b14">
	<analytic>
		<ptr target="http://ceur-ws.org/Vol-2936/" />
	</analytic>
	<monogr>
		<title level="m" coord="13,355.76,612.60,150.22,10.91;13,112.66,626.15,328.23,10.91">Proceedings of the Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum (CLEF 2021)</title>
		<title level="s" coord="13,124.26,639.70,129.93,10.91">CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<meeting>the Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum (CLEF 2021)<address><addrLine>Aachen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2936. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
