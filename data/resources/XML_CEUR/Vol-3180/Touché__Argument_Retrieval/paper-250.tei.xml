<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,417.06,15.42;1,89.29,106.66,190.02,15.42;1,89.29,129.00,324.97,11.96">SEUPD@CLEF: Team Lgtm on Argument Retrieval for Controversial Questions Notebook for the Touch√© Lab on Argument Retrieval at CLEF 2022</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,154.90,78.90,11.96"><forename type="first">Manuel</forename><surname>Barusco</surname></persName>
							<email>manuel.barusco@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,180.84,154.90,94.11,11.96"><forename type="first">Gabriele</forename><forename type="middle">Del</forename><surname>Fiume</surname></persName>
							<email>gabriele.delfiume@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,287.60,154.90,78.65,11.96"><forename type="first">Riccardo</forename><surname>Forzan</surname></persName>
							<email>riccardo.forzan@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,378.89,154.90,111.84,11.96"><forename type="first">Mario</forename><forename type="middle">Giovanni</forename><surname>Peloso</surname></persName>
							<email>mariogiovanni.peloso@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,89.29,168.85,73.67,11.96"><forename type="first">Nicola</forename><surname>Rizzetto</surname></persName>
							<email>nicola.rizzetto.2@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,175.60,168.85,85.17,11.96"><forename type="first">Elham</forename><surname>Soleymani</surname></persName>
							<email>elham.soleymani@studenti.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,291.77,168.85,60.31,11.96"><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
							<email>ferro@dei.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,417.06,15.42;1,89.29,106.66,190.02,15.42;1,89.29,129.00,324.97,11.96">SEUPD@CLEF: Team Lgtm on Argument Retrieval for Controversial Questions Notebook for the Touch√© Lab on Argument Retrieval at CLEF 2022</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">9BF2066D973109F79BDAF0B13B1BA538</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information Retrieval</term>
					<term>Search Engines</term>
					<term>Argument Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This document demonstrates the work of the LGTM Team of the University of Padua's participation in CLEF 2022 Touch√® Task 1, creating a system to retrieve arguments for controversial questions. This system aims to provide a solution to assist users who search for arguments to be used in conversations and retrieves a pair of sentences from a collection of arguments. In the various runs, we used a custom parser for parsing the document collection, a custom indexer, the BM25 Similarity, the DataMuse API for query expansion and boosting in Searcher. We also tried to improve the system by using different stoplists, stemmers and apply re-ranking based on sentiment analysis and readability of the document.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>During the last decade, continuous optimization of information retrieval effectiveness has driven search engines to new quality levels. Most people are satisfied most of the time, and web search has become a standard and often preferred source of information finding.</p><p>Nevertheless, in recent years, a principal driver of innovation has been the World Wide Web, unleashing publication at the scale of tens of millions of content creators. This explosion of published information would be questionable if the information could not be found, annotated, and analyzed so that each user can quickly find relevant and comprehensive information for their needs. <ref type="bibr" coords="1,144.40,497.34,12.84,10.91" target="#b0">[1]</ref> Moreover, One of the most critical parts of these search engines that can be improved is argument retrieval for controversial questions. To contribute to enhancing different solutions for this problem, we decided to participate in the Touch√© 2022 Lab proposed by CLEF <ref type="bibr" coords="1,468.26,537.99,14.96,10.91" target="#b1">[2]</ref>. We chose task 1 1 among three different tasks in the Touch√© lab to work with. This project is conducted as a student project in the Search Engines course in the academic year 2021-2022 at the Computer Engineering master's degree at University of Padua. This task aims to retrieve and rank sentences that carry significant points pertinent to the controversial topic.</p><p>The dataset used in this project is a pre-processed version of args.me dataset <ref type="bibr" coords="2,453.77,141.16,11.58,10.91" target="#b2">[3]</ref>. For the complete development, we decided to use the downloadable version of the dataset 2 . Additionally, To solve this task, we used Apache Lucene 3 , a Java library providing powerful indexing and search features and advanced analysis and tokenization capabilities.</p><p>The organization of this paper contains these sections: Section 2 indicates the previously related work, Section 3 illustrates our approach and method to show a new solution; Section 4 denotes our experimental setup; Section 5 defines our solutions and results we had; ultimately, Section 6 implies the conclusions and outlooks for future work that might be possible to develop the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Various researchers have developed the solution for the problem related to argument retrieval previously. Moreover, our initial step in developing an Argument retrieval system was the review of the previous year's projects <ref type="bibr" coords="2,265.47,335.28,13.00,10.91">[4]</ref> in the Search Engines course participating in the Touch√© Lab <ref type="bibr" coords="2,144.07,348.83,13.00,10.91" target="#b3">[5]</ref> and the code examples presented in the course, which contain a basic parser, indexer, and searcher. Some approaches and tools were used in these cases, for instance: using the Lucene implementation of BM25 and LMDirichlet Model to rank the relevancy of each document, and changing the weight method in every field in documents. Furthermore, using synonyms from different sources in query expansion to improve the result and using the OpenNLP Machine learning toolkit to boost the tokenization procedure.</p><p>While, many of these approaches lead to an acceptable and usable system, some of them did not have a great functionality. In this section we would love to explain about some of the previous ideas that helped us to have a better vision of the ways we can improve an information retrieval system. We divided these approaches in two categories. The first category is the frequently used methods which are the common methods that have been used on these projects although they have some drawbacks as well as their benefits. Additionally, Some unique approaches, that were not common in every study and the systems creators were innovative with a newborn methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Frequent methods</head><p>These tools and approaches have been used in most of the studies from last year <ref type="bibr" coords="2,440.22,588.25,11.28,10.91">[4]</ref>. Some tools and approaches that were common in most of the projects are: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Unique methods</head><p>Some approaches and tools are unique in these cases, for instance: Changing the weight method in every field in documents. Furthermore, using the OpenAI GPT-2 algorithm, the OpenNLP Machine learning toolkit to boost the tokenization procedure, and the score obtained by sentiment analysis on the documents to re-rank them. However, not all of these tools resulted in an spectacular system. Although some of them did not have an remarkable result, they thought us to try or not to try new tools and ways to create a new system. That is the reason that we listed them here. A list of these exclusive tools and a detailed description of them is shown below:</p><p>‚Ä¢ EnglishMinimalStemmer<ref type="foot" coords="3,225.44,448.68,3.71,7.97" target="#foot_0">4</ref> : an updated version of the Porter stemmer in the analyzer. <ref type="bibr" coords="3,488.49,450.44,17.49,10.91" target="#b11">[13]</ref> ‚Ä¢ LingPipe<ref type="foot" coords="3,156.91,463.44,3.71,7.97" target="#foot_1">5</ref> : a tool kit for processing text using computational linguistics, for instance, finding the names of people, organizations, or locations in the news or Suggesting correct spellings of queries. <ref type="bibr" coords="3,205.18,492.30,17.54,10.91" target="#b12">[14]</ref> ‚Ä¢ The Apache OpenNLP library<ref type="foot" coords="3,249.13,505.30,3.71,7.97" target="#foot_2">6</ref> : a machine learning-based toolkit for processing natural language text which supports the most common NLP tasks, such as tokenization, sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing, and coreference resolution. These tasks are usually required to build more advanced text processing services. OpenNLP also includes maximum entropy and perceptron-based machine learning. According to previous studies, it should be the best in accuracy terms, but it was very slow to index. <ref type="bibr" coords="3,247.95,588.35,17.94,10.91" target="#b13">[15]</ref> ‚Ä¢ OpenAI GPT-2 model: a Machine Learning algorithm that generates synthetic text samples from arbitrary input. It is used in query expansion, giving the topic title input to develop a complete phrase with new words that could help the searching part. The output of GPT-2 was not as good as expected. <ref type="bibr" coords="4,278.79,100.52,17.91,10.91" target="#b14">[16]</ref> ‚Ä¢ Sentiment analysis: Re-ranking regards the score obtained by performing sentiment analysis on the documents using VADER (Valence Aware Dictionary and Sentiment Reasoner), a lexicon and rule-based sentiment analysis tool specifically attuned to sentiments expressed in social media. <ref type="bibr" coords="4,230.98,156.02,18.94,10.91" target="#b15">[17]</ref> [18]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Our Approach</head><p>Our work evolved from the Hello Tipster <ref type="bibr" coords="4,278.23,205.73,18.07,10.91" target="#b17">[19]</ref> repository and several methods used by other studies as mentioned above that had marvelous results. Additionally, some of the ways of other studies lead to not satisfying results, so we learnt from them not to use those methods to optimize our approach. We tried to develop the project to showcase our approach to a new information retrieval system. Several classes, methods, and filters are added to improve performance in indexing time, parsing, and searching. The details of the form of our solution are explained in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>We create our system starting from the standard pipeline of every Information Retrieval system: we develop the main parts for the parse, index, and search phases separately. Numerous methods and tools such as different filters, similarities, APIs, and search optimization techniques are used in this project to optimize the indexing, parsing, and searching of the documents. Other studies inspired us after we saw their precious results. So we used some of their primary, frequent, and unique tools. Some of these tools are:</p><p>‚Ä¢ LengthFilter <ref type="foot" coords="4,171.39,450.49,3.71,7.97" target="#foot_3">7</ref> and EnglishPossessiveFilter<ref type="foot" coords="4,303.57,450.49,3.71,7.97" target="#foot_4">8</ref> in the analyzer ‚Ä¢ Sentiment analysis in re-ranking <ref type="bibr" coords="4,264.93,467.10,17.91,10.91" target="#b16">[18]</ref> ‚Ä¢ BM25 and LMDirichletSimilarity similarities <ref type="bibr" coords="4,316.84,481.96,12.84,10.91">[4]</ref> After trying different tools and solutions in every part of the project, we tried to develop some methods that were not used in previous years' studies to improve the Information retrieval procedure. So to examine if these methods and tools are influential and they can enhance the pace of the system or lead us to an optimized system, we used these particular tools:</p><p>‚Ä¢ Jackson library to process the collection <ref type="bibr" coords="4,297.21,573.28,17.91,10.91" target="#b18">[20]</ref> ‚Ä¢ Rapid Automatic Keyword Extraction (RAKE) to improve the query expansion process <ref type="bibr" coords="4,116.56,601.69,17.91,10.91" target="#b19">[21]</ref> ‚Ä¢ DataMuse API for searching adequate synonyms in query expansion <ref type="bibr" coords="4,424.79,616.55,17.91,10.91" target="#b20">[22]</ref> ‚Ä¢ Re-rank techniques based on the sentiment and readability analysis <ref type="bibr" coords="5,419.67,86.97,17.91,10.91" target="#b21">[23]</ref> Moreover, the sections below are indicated to demonstrate a better understanding of the workflow of this system. These sections provide every package in our source code with its objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Parsing, Pre-Processing, and Indexing</head><p>We developed a custom parser for parsing the collection itself for the parsing phase of the system pipeline. With this particular parser, we managed to parse the collection in the most meaningful way for our purpose. For this phase, two libraries are used:</p><p>‚Ä¢ Jackson library: Jackson is a suite of data-processing tools for Java (and the JVM platform), including the flagship streaming JSON parser/generator library, matching data-binding library (POJOs to and from JSON), and additional data format modules to process data encoded in CSV, XML, and many other formats. We used this library for parsing some specific fields of the documents in the collection, which were encoded in a JSON format.</p><p>[20] ‚Ä¢ Apache CSV library <ref type="foot" coords="5,207.94,319.81,3.71,7.97" target="#foot_5">9</ref> for parsing the main CSV structure of the collection file.</p><p>Our system stores different information independently and with a proper weight in fields for creating Lucene Documents <ref type="foot" coords="5,211.26,355.87,7.41,7.97" target="#foot_6">10</ref> . We decided to extract from every document in the collection the following fields: Then, after the extraction of these fields, we decided to index only the ID, CONCLUSION, DISCUSSION TITLE, SOURCE TITLE, and SOURCE TEXT fields. In our project, we indexed the collection documents by using Lucene default fields, and in order to give a better organization for our scope, we added some custom index fields:</p><p>‚Ä¢ Bodyfield: A custom field for indexing the body of a document by storing the tokens, the term vectors, documents, frequencies, positions and offsets of the field. The whole content of the body is not stored.</p><p>‚Ä¢ IdKeyField: A custom field for indexing the ID of a sentence or a document, or every field containing key values such as acquisition time, stance, etc. This field is not tokenized, and it is stored. ‚Ä¢ BodyCorrelatedField: A custom field for indexing the additional information of the body field such as conclusion, stance, discussion title, source title, etc. The indexing options of this field are the same as the body field, but the field is stored in this case.</p><p>We decided to adopt the ClassicTokenizer<ref type="foot" coords="6,292.16,179.82,7.41,7.97" target="#foot_7">11</ref> provided by Apache Lucene. It is a grammarbased tokenizer that is good for most European-language documents. This tokenizer splits the text field into tokens, treating white space and punctuation as delimiters. These delimiters are discarded with the following expectations:</p><p>‚Ä¢ Periods (dots) that are not followed by white space are kept as part of the token.</p><p>‚Ä¢ Words are split at hyphens unless there is a number in the word, in which case the token is not split and the numbers and hyphen(s) are preserved. ‚Ä¢ Recognizes Internet domain names and email addresses and preserves them as a single token. <ref type="bibr" coords="6,147.47,301.64,17.91,10.91" target="#b22">[24]</ref> Beyond this tokenizer, we decided to use the LowerCaseFilter <ref type="foot" coords="6,380.79,322.40,7.41,7.97" target="#foot_8">12</ref> , in order to normalize all tokens to lower case. This also allows terms of the query to match with terms in the documents written, for example in upper case. The next filter used is the EnglishPossessiveFilter<ref type="foot" coords="6,477.20,349.50,7.41,7.97" target="#foot_9">13</ref> that removes possessives (trailing 's) from words. The last filter we used is LengthFilter <ref type="foot" coords="6,450.24,363.05,7.41,7.97" target="#foot_10">14</ref> . This filter keeps tokens with a length between 3 and 20 characters, removing the others. Previous studies showed that this filter worked well in this collection. In the next section we talk about the usage of different stoplists and stemmers for system performance improvement. All the tests were done in a simplified version of the system where the search wasn't boosted and without any query expansion or re-rank technique. This was done in order to see the impact of that solution (stoplists or stemmer application) on the system. The similarity used was BM25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Tested Stoplists</head><p>Stopword filtering is a common step in preprocessing text because it removes lots of not informative words. We tested different stoplists in the analyzer, and we realized that stoplists have a considerable impact on the system performances, so we tried different lists, as reported in Tab. 1 and we decided to keep the stoplist solution that gives us the best performance measures. We saw that this list gave good performances to previous years' projects <ref type="bibr" coords="7,441.40,307.99,16.25,10.91" target="#b16">[18]</ref>. ‚Ä¢ GoogleStop: simple English stoplist used by Google. It contains 174 words.</p><p>‚Ä¢ Simple English stopwords list downloaded from ranks.hl 18 . It contains 174 words.</p><p>The results in Tab. 1 show that the two better solutions are: NO STOP LIST and CoreNLP stoplists. But, after testing these two solutions in the final system, we saw that the no stop list solution is better. Since we do not have any index storage limitation, not applying any stoplist is not a problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Tested Stemmers</head><p>After setting the best stoplist solution, we tried different stemmers. Stemming is the reduction of a word into its base form, called stem. In particular, we tried three different stemmers synthesized in Tab. 2. The stemmers can generalize the search process, but they can also worsen it, so we decided to test different stemmers and see which one gives us the best performance. The different stemmers used are:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stemmer</head><p>‚Ä¢ EnglishMinimalStemmer <ref type="foot" coords="8,232.50,107.73,7.41,7.97" target="#foot_11">19</ref> : Stemmer that simply stems plural English words to their singular form. ‚Ä¢ Krovetz Stemmer 20 : Hybrid algorithmic-dictionary stemmer that produces words. ‚Ä¢ Porter Stemmerr 21 : Stemmer that eliminates the longest suffix possible, working by steps and trying to delete each suffix every time until it reaches the base form for generating stems.</p><p>As we can see from Tab. 2, the score obtained decreases by using stemmers, probably due to the limitations of stemmers used. So we decided not to use any stemmer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Searcher</head><p>After setting up a basic function for search that simply uses a boolean query to check for matches in documents, we tried two different similarities and we tried to improve performances using various approaches: query boosting, query expansion and re-ranking. Our system produces essentially two ranks: one rank that contains the most relevant documents to the query and one rank that contains the most relevant sentences pairs. The second rank is obtained from the first one by analyzing the sentences contained in every document in the first rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Similarity decision</head><p>We tested the BM25 Similarity 22 and the Dirichlet Similarity 23 on the simplified version of the system, enhanced with no stop lists and no stemmers, which were the best solutions obtained from the previous tests. We tested this system with the two similarities, gaining the following results. After these results, we decided to use the BM25 similarity for our project. We did this test at the end of the project development, based on the results of the next sections about Query Boosting, Query Expansion and Re-ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Query Boosting</head><p>Since documents have more than one field to search in, it is possible to assign different weights to each field at query time. In this way, a term found in a field with a higher weight will also have a higher impact on the final score of the document. As already explained in the section 3.1, we decided to index four main fields: Conclusion, Source Text, Discussion Title, and Source Title. So we decided to do query boosting with different weights on these fields and determine the best weight set to apply. We noticed that Source Text is the most informative field; instead the conclusion, source title, and discussion title are often composed by a few terms, and very rarely these are relevant to the search. So, according to these considerations, the best score would be obtained by assigning a higher weight to the source text and a lower one to other fields, as we can also see from Tab. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source Text Conclusion Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Query expansion</head><p>In order to increase the recall of the system, an idea was to implement query expansion. To do that we used:</p><p>‚Ä¢ Rapid Automatic Keyword Extraction (RAKE) algorithm <ref type="bibr" coords="9,369.12,492.98,17.91,10.91" target="#b23">[25]</ref> ‚Ä¢ Datamuse API <ref type="bibr" coords="9,183.35,507.89,17.91,10.91" target="#b20">[22]</ref> The rake algorithm used is based on the implementation found in the Github repository. <ref type="bibr" coords="9,488.01,530.40,17.97,10.91" target="#b19">[21]</ref> This algorithm is used to recognize the most significant token of a query. Then synonyms of this token are retrieved using Datamuse API, this API returns for a given word a list of synonyms and a score for each one of them.</p><p>This process aims to generate queries that belong to topics similar to the original query. The results of both the original query and the generated ones are then mixed in a collection. If duplicates are found while merging the results list of the queries, the document with the highest score will be kept.</p><p>In the process of query expansion, we tried different approaches, and tuned some parameters: for example we considered as valid synonyms all the words which score is above half of the best synonym. Using this approach we were able to generate more than two hundred queries starting from the fifty we had originally. The runs in which we used this query expansion were worst (in terms of precision and recall) than the ones without query expansion. The best results we experimentally obtained were the one using only one synonym for the most significant token found using RAKE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Re-Ranking</head><p>In order to increase our precision and our nDCG@5 we tried to re-rank documents. We tried to order the documents based on two metrics:</p><p>‚Ä¢ Sentiment score of the document ‚Ä¢ Readability of the document An idea we had was trying to increase the scores of the document and sentiment was similar to the one calculated on the query. To do so, we used the VADER library <ref type="foot" coords="10,418.73,262.22,7.41,7.97" target="#foot_12">24</ref> : we calculate the sentiment of the query, then we boost the score of documents whose conclusion field has a similar value compared to the one of the query. To do so, we use a Gaussian distribution, and the score is boosted according to the interval in which the score of the document is. Experimentally we realized that the conclusion fields and the query fields are really short, so the boosting was not as significant as expected.</p><p>A similar story can be said for the readability score. Using the Flesch-Kincaid <ref type="bibr" coords="10,454.82,345.27,18.06,10.91" target="#b24">[26]</ref> metric, indeed the implementation found on GitHub <ref type="bibr" coords="10,296.07,358.82,18.07,10.91" target="#b21">[23]</ref> we ordered the documents based on their readability score. We tried to use it in the re-ranking since calculating the score of readability while indexing was very heavy computationally speaking. We decided to re-rank documents based on the readability computed on the conclusion field of the document. As occurred with the sentiment analysis before, we have that the score computed on the conclusion field is too unstable and not that significant due to the fact that the conclusions are extremely condensed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>Touch√® offers us the possibilities to access the args.me corpus via the API of args.me search engine <ref type="foot" coords="10,119.88,496.99,7.41,7.97" target="#foot_13">25</ref> or downloading the file containing all the documents. We decided to download the entire corpus of the pre-processed dataset, that is available on the Touche Task 1 website <ref type="foot" coords="10,484.54,510.54,7.41,7.97" target="#foot_14">26</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Document</head><p>The updated version of the args.me corpus contains 365.408 arguments crawled from some debate portals such as debatewise.org, idebate.org, debatepedia.org, and debate.org. <ref type="bibr" coords="10,467.63,596.14,13.45,10.91" target="#b2">[3]</ref> Each argument is identified by an ID, and it is constituted by a conclusion and one or more premises. Every argument is also associated with a stance (PRO or CON), some key sentences of the argument text, and also some information about the context like the source URL, the title of the discussion, and many others.</p><p>The collection can be found here: https://files.webis.de/data-in-progress/data-research/arguana/touche/touche22/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Topics</head><p>For testing and tuning the system we used the 2021 Topics (only the title). Available at: https://webis.de/events/touche-22/topics-task-1-only-titles-2021.zip For the runs submission, we used the 2022 topics available at: https://files.webis.de/corpora/corpora-webis/corpus-touche-task1-22/topics.xml</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Relevance judgments</head><p>For testing and tuning the system we used the relevance judgments of the previous year, available at: https://webis.de/events/touche-22/touche-task1-51-100-relevance.qrels</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Measures</head><p>We used three different evaluation measures to assess the quality of our system and of our different solutions. These measures were all calculated by the trec_eval<ref type="foot" coords="11,413.76,368.40,7.41,7.97" target="#foot_15">27</ref> executable, which was executed with the various test runs of the system:</p><p>‚Ä¢ Normalized Discounted Cumulative Gain (nDCG) score with an evaluation depth of 5 (nDCG@5). In particular, we used the implementation provided by the trec_eval library to measure the performance of our IR system. The nDCG is the result of the equations Eq.1 and Eq.2 below. Parameter b indicates the patience of the user in scanning the result lists, and usually it is a value of 2 for an impatient user, or 10 for a patient user. Since the result is not bounded in [0,1], it is necessary to normalize the score dividing nDCG by the Ideal Discounted Cumulated Gain (iDCG), provided by Touch√© Lab, as can be seen in Eq. 2</p><formula xml:id="formula_0" coords="11,222.35,522.87,283.63,33.58">ùê∑ùê∂ùê∫@5 = 5 ‚àëÔ∏Å ùëõ=1 ùëüùëíùëôùëíùë£ùëéùëõùëêùëí ùëõ ùëöùëéùë•(1 + log ùëè (ùëõ + 1))<label>(1)</label></formula><p>ùëõùê∑ùê∂ùê∫@5 = ùê∑ùê∂ùê∫@5 ùëñùê∑ùê∂ùê∫@5</p><p>‚Ä¢ Precision at position 5 (P_5), which indicates the percentage of how many relevant documents are in the first 5 positions:</p><formula xml:id="formula_2" coords="11,273.77,627.49,232.21,33.58">ùëÉ (5) = 1 5 5 ‚àëÔ∏Å ùëõ=1 ùëü ùëõ<label>(3)</label></formula><p>In this case r is a binary relevance of the document (r=1 document relevant, r=0 document not relevant) ‚Ä¢ Recall, for evaluating how many relevant documents were retrieved by the system in the whole rank: ùëÖ = |relevant documents retrieved for that query| |relevant documents for that query| (4)</p><p>|.| indicates the cardinality of the set, so the Recall is the ratio between the cardinality of the set of relevant documents retrieved for that query and the cardinality of the set of relevant documents for that query in the collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Repository</head><p>It is possible to find the source code of the project in the link below: https://bitbucket.org/upd-dei-stud-prj/seupd2122-lgtm/src/master/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Hardware components</head><p>The devices used for parsing, indexing, searching and testing are: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results</head><p>In the final phase of the project we tried to set the system in the best configuration by combining the various solutions that we developed in the search part. We obtained the following results: The score achieved both by using synonyms to all tokens returned by rake and using synonyms only to the main token with the query expansion is lower than expected. This is probably caused by the fact that the expanded queries generated are sometimes satisfactory and sometimes unsatisfactory. So, many times they transform the original query to a worse one. Furthermore, by using more synonyms, we create a lot of noisy queries that degrade the results.</p><p>The sentiment analysis performed only on the conclusion field leads to a weak solution, the same as the readability analysis. This is probably caused by the fact that these two scores have to be calculated on the source text of the document. We couldn't do this test because the sentiment and readability re-ranking based on source text requires a lot of time in the search process. Finally, the BM25 Similarity seems to work better than the LMDirichlet, unless it seems the last one worked better in the previous year's studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Statistical Analysis</head><p>In this section, several statistical analyses about the evaluation of the performance of the runs of the system are provided. In particular, in Section 5.2.4 we analyze the performance of the system when using our query expansion technique because this technique was the less useful one and it did not give us the results that we expected. We decided to analyze the behavior of the system with all possible tunable parameters of our technique and by using both the BM25 Similarity and also the LMDirichlet Similarity in order to see if the previous results are confirmed. In this analysis we used the Task 1 2022 Relevance Judgments because we think the query expansion techniques have the main purpose of retrieving more relevant results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Box Plot</head><p>Exploratory data analysis involves the use of statistical techniques to identify patterns that may be hidden in a group of numbers. One of these techniques is the "box plot," which is used to visually summarize and compare groups of data. The box plot uses the median, the approximate quarterlies , and the lowest and highest data points to convey the level, spread, and symmetry of a distribution of data values. It can also be easily refined to identify outlier data values and can be easily constructed by hand. We apply box plots to tabular data from two recently published articles to show how readers can use box plots to improve the interpretation of data in complex tables. The box plot, like other visual methods, is more than a substitute for a table: It is a tool that can improve our reasoning about quantitative information. We recommend that the box plot be used more frequently. <ref type="bibr" coords="14,222.80,312.43,17.91,10.91" target="#b8">[10]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Student's t test</head><p>It is used to test whether the mean difference between two groups is statistically significant. The null hypothesis stated that both means are statistically equal, whereas the alternative hypothesis stated that both means are not statistically equal, i.e., they are statistically different. T-test has three types, i.e., one sample t-test, independent samples t-test, and paired samples t-test. <ref type="bibr" coords="14,484.90,402.41,17.91,10.91" target="#b9">[11]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">ANOVA</head><p>ANOVA is used to compare the means among three or more groups. In ANOVA, the first gets a common P value. A significant P value of ANOVA test indicates for at least one pair, between which the mean difference was statistically significant. To identify that significant pair(s), posthoc test (multiple comparisons) is used. In ANOVA test, when at least one covariate (continuous variable) is adjusted to remove the confounding effect from the result called ANCOVA. ANOVA test (F test) is called "Analysis of Variance" rather than "Analysis of Means" because inferences about means are made by analyzing variance. <ref type="bibr" coords="14,291.87,533.03,17.87,10.91" target="#b9">[11]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4.">Statistical Analysis on Query expansion with synonyms provided by DataMuse API</head><p>In Fig. <ref type="figure" coords="14,120.81,595.91,3.79,10.91">1</ref>, we have the boxplots of the runs obtained by using BM25 and our query expansion technique. As the first step, we have generated a lot of runs by combining all the possible values that our technique parameters could have. The tunable parameter of our system are:</p><p>‚Ä¢ AllTokens: parameter that indicates if we have to generate synonyms for all the query tokens (AllTokens = true) or only for the main token (AllToken = false).</p><p>‚Ä¢ MaxSynonims: maximum number of synonyms to retrieve (MaxSynonims = 1,2,5,7,10,100) for the given token. ‚Ä¢ Threshold: score threshold that must be satisfied by the returned synonyms. It is expressed as a fraction of the synonym score over the best synonym score. (Threshold is in the interval [0.55 , 1]).</p><p>We obtained a lot of runs by varying all the possible parameter values and, then we clustered all the obtained results into some clusters because many runs were totally equal. This is caused by the fact that in many cases, DataMuse API returns only a few synonyms, so by setting a given threshold, if we return 5 or more synonyms we always obtain the same results. Still, we will return later on this problem. For every cluster, we then keep one of the runs of that cluster as a "representative" for our analyzes. In Fig. <ref type="figure" coords="15,282.55,231.50,3.85,10.91">1</ref>, we have the boxplots of 4 different runs obtained by the previous "clustering":</p><p>‚Ä¢ the first one (A) is obtained by generating 5 synonyms for only the main query token where the synonyms satisfy the threshold of 0.55. ‚Ä¢ the second one (B) is obtained by generating 10 synonyms for all the query tokens where the synonyms satisfy the threshold of 0.55. ‚Ä¢ the third one (C) is obtained by generating 2 synonyms for all the query tokens where the synonyms satisfy the threshold of 0.55. ‚Ä¢ the fourth one (D) is obtained by generating 5 synonyms for all the query tokens where the synonyms satisfy the threshold of 0.55.</p><p>Looking at the boxplots, we notice that the runs do not seem to have a normal distribution. Indeed, the runs are not balanced due to IQR located in the lower portion of the graph. Even if the boxplots do not seem distributed as a normal distribution, since we have 50 topics for all the runs, for the Central Limit Theorem, we can consider ANOVA robust to violations of normality. Hence, we perform the ANOVA Test (Tab. 6) on the runs and obtained a p-value of 0.706525. Since the p-value is greater than 0.05, we accept H0 (the null hypothesis that all the means are equal), meaning that all the runs are quite equal and come from the same distribution. In fact, by performing a multiple pairwise comparison (Tukey's HSD Test) (Tab. 7), we can notice that the runs are pretty equal due to a p-value always very big and near to 1. We decided to go deeper in this analysis to see why we did not get significantly different runs. This is caused by DataMuse API directly because this API does not return a lot of synonims for every input token. DataMuse returns only the most important tokens, that are not so many, so our subsequent score threshold mechanism is not so effective and setting all the different parameters not lead to very different solutions. For example, if the API returns only 2 synonims, each one with half of the best score, for a token, all the configurations that return 2 or more synonims for a token by selecting 0.5 * bestSynonimScore threshold, will always return the same results.  We decided to repeat the previous analysis by using the LMDirichlet Similarity to see if the results that obtained in the previous sections are corrected. In Fig. <ref type="figure" coords="16,387.65,646.52,4.00,10.91">2</ref>, we have the boxplots of the runs obtained by using LMDirichlet and our query expansions technique. As for BM25, we generated all the possible runs obtained with all the possible parameter values that could be set and, then we clustered them in 2 main groups that are represented by the configurations:</p><p>‚Ä¢ LMDirichlet with AllToken = false, MaxSynonims = 5, Threshold = 0.55 (A) ‚Ä¢ LMDirichlet with AllToken = true, MaxSynonims = 5, Threshold = 0.55 (B) By looking at the plots, we can notice that the runs seem to have a strange distribution and present some outliers. This Similarity tends to have a very unstable and not so good behaviour. As before, we performed the ANOVA Test (Tab. 8) on the runs, and we obtained a p-value of 0.881112. Since the p-value is much higher than 0.05, we accept H0 (the null hypothesis that all the means are equal), meaning that all the runs are quite equal and come from the same distribution. In fact, by performing a pairwise comparison (Tukey's HSD Test) (Tab. 7), we can notice that the runs are quite equal due to a p-value very big and near to 1. Also, in this case, we have similar runs for the same reason we reported in the previous section with the BM25 Similarity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5.">Best Runs</head><p>We compared the best runs obtained using BM25 and synonyms with the best run obtained using LMDirichlet with synonyms (comparing the median value of the boxplots) and the run obtained by the best configuration of our system (only BM25 Similarity without Query Expansion).</p><p>Looking at the boxplots, the runs look very different, and the median of the BM25 with no Query Expansion is higher than the others.  We then performed ANOVA to compare the three runs, and since the p-value obtained is below 0.05, we rejected H0 (the null hypothesis that the two means are equal), so the runs can be considered significantly different. We then performed a T Student Test to compare the runs. As we can see from Table <ref type="table" coords="19,203.34,324.82,8.21,10.91">11</ref>, the runs obtained with BM25 are different from the runs obtained with LMDirichlet. Since the median value is always higher in the runs with BM25 with respect to the runs with LMDirichlet, we decided to confirm the use of this Similarity in our system. Moreover, the BM25 similarity solutions with no query expansion and with query expansion are equal (p value greater than 0.05), so since the use of query expansion with synonyms does not lead to a highly significant difference in terms of system effectiveness (as we can see ) it is better to avoid overhead by not including our query expansion technique. Hence, we can confirm that synonyms do not work well and do not increase the performance as we stated in the previous sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Failure Analysis</head><p>We perform a failure analysis on the final runs returned by the best configuration of our system that, as stated in the previous sections, is composed of BM25 Similarity with No Query Expansion and only Query Boosting. We decided to perform the failure analysis based on the three judgments about the Relevance, Quality, and Coherence of the retrieved sentences. In the following graphs, we can see the nDCG@5 measures obtained for every query by our system by using the three different judgment criteria.</p><p>As we can see from the plots, the system performs quite well in retrieving relevant and quality results but struggles a lot when retrieving coherent pair of sentences. In particular, we decided to analyze deeper some topics where the systems return a rank with nDCG@5 equal to zero. An example of these topics is topics number 4,6,17,21,22. We decided to analyze these topics and the ranks returned by our system with these topics, and we see three main problems:</p><p>‚Ä¢ The topics title (the query text) contains some main keywords that are not correctly recognized and processed by our system. For example, topic number 4, "Is a college education worth it?" induces our system to search sentences about the education field and not the college education that is the main concept of the query. So the documents found by the system are not relevant for the query. ‚Ä¢ The topics with a lot of stop words and words connected by a hyphen are poorly processed by our system. ‚Ä¢ Our sentences retrieve mechanism is too simple and must be improved. This is the main cause of the poor performances on the Coherence Judgments. In fact, our system retrieves the pair of sentences from the top ranks documents by assuming that these sentences are relevant and, in some ways, connected without analyzing any correlation between the sentences. This mechanism sometimes works and sometimes does not. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Work</head><p>Over the last decade, search engine quality has improved dramatically due to the uninterrupted optimization of information retrieval effectiveness. Furthermore, Argument retrieval for controversial questions is one of the fields that can significantly impact the latest information retrieval systems. Moreover, to be involved in the improvement of this area, we participated in Touch√© 2022 Lab task 1 proposed by CLEF <ref type="bibr" coords="21,237.96,386.85,15.78,10.91" target="#b1">[2]</ref> which emphasizes retrieving and ranking sentences that carry consequential points related to the controversial topic. We implemented our IR system [27] to retrieve the most relevant sentence pairs to the given queries provided in the Touch√© shared task. We tried to test different solutions in terms of stoplists, stemmers, and similarities. In addition, we created the system based on the best results and we do statistical analysis in order to see if our ideas gives us better performances.</p><p>We also showed how essential it is to give the right weight to the different parts of a document, since a lot of information can be useless during the search.</p><p>Nevertheless, there are some aspects that can be improved to reach better performances. For example, in query expansion, it can be helpful to use some NLP tools that can rewrite one query to another one that is related to the first one in a meaningful way. Furthermore a pseduo relevance feedback technique can also be added in the query expansion phase.</p><p>Another improvement can be made by using a better formula to re-rank the documents or using a different score instead of the one formulated with sentiment and readability analysis. For example, with a machine learning approach, it would be possible to train a model to assign a quality score to each topic and then use this value to re-rank the top retrieved documents. Furthermore, a more clever sentences retriever could be developed by analyzing with NLP Tools the content of the sentences.</p><p>As a final observation, we presented our approach to the problem, and we think that in the future always better solutions will be presented, especially with the help of machine learning and high computational platforms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,107.28,393.69,126.53,10.91;5,107.28,408.59,206.09,10.91;5,107.28,423.50,231.49,10.91;5,107.28,438.40,246.94,10.91;5,107.28,453.31,273.62,10.91;5,107.28,468.21,197.68,10.91;5,107.28,483.11,114.91,10.91;5,107.28,498.02,154.43,10.91;5,107.28,512.92,194.33,10.91"><head>‚Ä¢</head><label></label><figDesc>ID: the id of the document ‚Ä¢ Conclusion: the conclusion of the document ‚Ä¢ Stance: the stance of the document (PRO or CON) ‚Ä¢ AcquisitionTime: time of acquisition of the argument ‚Ä¢ discussionTitle: title of the discussion in the debate website ‚Ä¢ sourceTitle: title of the source debate page ‚Ä¢ url: url of the argument ‚Ä¢ sourceText: text of the argument ‚Ä¢ sentences: key sentences of the argument</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="15,221.44,613.40,8.90,10.91;15,251.29,613.40,87.21,10.91;15,364.22,613.40,5.29,10.91;15,397.98,613.40,32.42,10.91;15,169.78,629.73,22.36,10.91;15,219.62,629.73,12.55,10.91;15,249.19,629.73,183.96,10.91;15,162.13,646.07,175.60,10.91;15,365.10,646.07,3.52,10.91;15,412.44,646.07,3.52,10.91;15,185.59,669.58,224.09,10.91"><head>6 :</head><label>6</label><figDesc>ANOVA of runs using BM25 and synonyms</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="16,188.00,457.45,220.82,8.87;16,130.96,84.19,333.36,358.88"><head>Figure 1 :Tab 7 :</head><label>17</label><figDesc>Figure 1: Boxplots of runs using BM25 and synonyms.</figDesc><graphic coords="16,130.96,84.19,333.36,358.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="17,175.16,642.12,246.49,8.87;17,172.63,291.49,250.02,336.24"><head>Figure 2 :Tab 9 :</head><label>29</label><figDesc>Figure 2: Boxplots of runs using LMDirichlet and synonyms.</figDesc><graphic coords="17,172.63,291.49,250.02,336.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="18,89.29,636.23,416.69,8.87;18,203.27,648.18,190.27,8.87;18,172.63,328.71,250.02,293.13"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Boxplots of runs using the Best BM25 and Dirichlet with synonyms and the best configuration of our system that use only a BM25 Similarity.</figDesc><graphic coords="18,172.63,328.71,250.02,293.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="20,142.70,414.31,309.89,8.87;20,130.96,227.30,333.36,172.62"><head>Figure 4 :Figure 5 :Figure 6 :</head><label>456</label><figDesc>Figure 4: Failure analysis with Relevance Judgments on best run with BM25</figDesc><graphic coords="20,130.96,227.30,333.36,172.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="21,130.96,84.19,333.36,169.23"><head></head><label></label><figDesc></figDesc><graphic coords="21,130.96,84.19,333.36,169.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,107.28,86.97,398.92,207.88"><head></head><label></label><figDesc>‚Ä¢ BM25, LMDirichlet, and Boolean model: The usage of the Lucene implementation of these model for Similarity.[7] ‚Ä¢ PorterStemFilter and LovinsStemmer: Testing a variety range of Stemmers for instance KStemFilter, PorterStemFilter, and LovinsStemmer. ‚Ä¢ Trec eval: A standard tool used by the TREC community for evaluating an ad hoc retrieval run, given the results file and a standard set of judged results.[8] ‚Ä¢ Synonyms extraction from WordNet: Queries expanded by synonyms extracted from WordNet, an extensive lexical database of English nouns, verbs, adjectives, and adverbs, are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept.[9] ‚Ä¢ The Box Plot: It is used to visually summarize and compare groups of data. [10] ‚Ä¢ Student's t-test: It is one of the most popular statistical techniques used to test whether the mean difference between two groups is statistically significant. [11] ‚Ä¢ Analysis of Variance (ANOVA): To test if survey or experiment results are useful or compare more than two groups simultaneously to determine whether a relationship exists between them.[12]</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,100.20,85.89,407.46,219.46"><head></head><label></label><figDesc>CoreNLP 15 : stoplist used in CoreNLP java library. This list contains 275 tokens. ‚Ä¢ CountWordsFree 16 : a list of stop words that are frequently used in English. It contains 851 words. ‚Ä¢ EBSCO 17 : list of 24 words used in EBSCOhost medical databases MEDLINE and CINAHL.</figDesc><table coords="7,100.20,85.89,313.16,176.26"><row><cell>StopList</cell><cell>nDCG@5</cell><cell>P_5</cell><cell>RECALL</cell></row><row><cell>CoreNLP</cell><cell>0.4240</cell><cell>0.3519</cell><cell>0.8507</cell></row><row><cell>CountWordsFree</cell><cell>0.4240</cell><cell>0.3482</cell><cell>0.8496</cell></row><row><cell>EBSCO</cell><cell>0.4200</cell><cell>0.3522</cell><cell>0.8496</cell></row><row><cell>GoogleStop</cell><cell>0.4240</cell><cell>0.3519</cell><cell>0.8500</cell></row><row><cell>Ranks</cell><cell>0.4080</cell><cell>0.3362</cell><cell>0.8499</cell></row><row><cell>NO STOP</cell><cell>0.4200</cell><cell>0.3532</cell><cell>0.8524</cell></row><row><cell cols="4">Tab. 1: System performances with different stoplists</cell></row><row><cell>The different stop lists used are:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>‚Ä¢</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="13,100.20,85.89,406.06,355.56"><head></head><label></label><figDesc>NO QE", to indicate that we are not using Query Expansion, "QE in all token", to indicate that we are performing query expansion in all the tokens returned by RAKE, "QE only main token", to indicate that we are performing Query Expansion only on the main token returned by RAKE.‚Ä¢ "NO RE-R" to indicate that we are not using Re-Ranking, "RE-R Sent", to indicate that we are performing re-ranking based on sentiment analysis on the conclusion field, "RE-R Read", to indicate that we are performing re-ranking based on the readability of the conclusion field.After this final test, we conclude that the best configuration for our system is:</figDesc><table coords="13,100.20,85.89,405.78,355.56"><row><cell>Configuration</cell><cell cols="3">nDCG@5</cell><cell>P 5</cell><cell>RECALL</cell></row><row><cell>Search with NO BOOST, NO QE, NO RE-R</cell><cell></cell><cell cols="2">0.3621</cell><cell>0.4320</cell><cell>0.7867</cell></row><row><cell>Search with BOOST, NO QE, NO RE-R</cell><cell></cell><cell cols="2">0.4428</cell><cell>0.5120</cell><cell>0.8436</cell></row><row><cell cols="2">Search with BOOST, QE in all token, NO RE-R</cell><cell cols="2">0.3961</cell><cell>0.4640</cell><cell>0.7527</cell></row><row><cell cols="2">Search with BOOST, QE only main token, NO RE-R</cell><cell cols="2">0.4294</cell><cell>0.5000</cell><cell>0.7925</cell></row><row><cell cols="2">Search with BOOST, QE only main token, RE-R Sent</cell><cell cols="2">0.2383</cell><cell>0.2760</cell><cell>0.6304</cell></row><row><cell cols="2">Search with BOOST, QE only main token, RE-R Read</cell><cell cols="2">0.1416</cell><cell>0.1800</cell><cell>0.4684</cell></row><row><cell cols="5">Tab. 5: System performances with different search solutions</cell></row><row><cell>In the previous table we use the following terms:</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">‚Ä¢ "NO BOOST", "BOOST": to indicate if we are using query boosting (with the best weight</cell></row><row><cell>set).</cell><cell></cell><cell></cell><cell></cell></row><row><cell>‚Ä¢ "Configuration</cell><cell cols="2">nDCG@5</cell><cell>P 5</cell><cell>RECALL</cell></row><row><cell>Search with BOOST, NO QE, NO RE-R</cell><cell>0.4428</cell><cell></cell><cell>0.5120</cell><cell>0.8436</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="3,108.93,638.16,335.82,8.97;3,89.29,649.12,109.50,8.97"><p>https://lucene.apache.org/core/4_10_2/analyzers-common/org/apache/lucene/analysis/en/ EnglishMinimalStemmer.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="3,108.93,660.08,120.46,8.97"><p>http://www.alias-i.com/lingpipe/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="3,108.93,671.04,253.04,8.97"><p>https://opennlp.apache.org/docs/1.9.4/manual/opennlp.html#opennlp</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="4,108.93,638.16,374.95,8.97;4,89.29,649.12,63.62,8.97"><p>https://lucene.apache.org/core/7_0_1/analyzers-common/org/apache/lucene/analysis/miscellaneous/ LengthFilter.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4" coords="4,108.93,660.08,331.57,8.97;4,89.29,671.04,103.52,8.97"><p>https://lucene.apache.org/core/7_3_1/analyzers-common/org/apache/lucene/analysis/en/ EnglishPossessiveFilter.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5" coords="5,108.93,660.06,187.57,8.97"><p>https://commons.apache.org/proper/commons-csv/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6" coords="5,108.93,671.02,323.19,8.97"><p>https://lucene.apache.org/core/9_1_0/core/org/apache/lucene/document/Document.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_7" coords="6,108.93,594.25,354.88,8.97;6,89.29,605.21,80.41,8.97"><p>https://lucene.apache.org/core/6_5_0/analyzers-common/org/apache/lucene/analysis/standard/ ClassicTokenizer.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_8" coords="6,108.93,616.17,398.36,8.97;6,89.29,627.13,17.11,8.97"><p>https://lucene.apache.org/core/8_8_1/analyzers-common/org/apache/lucene/analysis/core/LowerCaseFilter. html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_9" coords="6,108.93,638.08,331.57,8.97;6,89.29,649.04,103.52,8.97"><p>https://lucene.apache.org/core/7_3_1/analyzers-common/org/apache/lucene/analysis/en/ EnglishPossessiveFilter.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_10" coords="6,108.93,660.00,374.95,8.97;6,89.29,670.96,63.62,8.97"><p>https://lucene.apache.org/core/8_8_1/analyzers-common/org/apache/lucene/analysis/miscellaneous/ LengthFilter.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_11" coords="8,108.93,605.26,335.82,8.97;8,89.29,616.22,114.39,8.97"><p>https://lucene.apache.org/core/4_10_2/analyzers-common/org/apache/lucene/analysis/en/ EnglishMinimalStemFilter.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24" xml:id="foot_12" coords="10,108.93,649.11,157.07,8.97"><p>https://github.com/cjhutto/vaderSentiment</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="25" xml:id="foot_13" coords="10,108.93,660.07,122.39,8.97"><p>https://www.args.me/api-en.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="26" xml:id="foot_14" coords="10,108.93,671.03,216.27,8.97"><p>https://webis.de/events/touche-22/shared-task-1.html#data</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="27" xml:id="foot_15" coords="11,108.93,671.04,110.54,8.97"><p>https://trec.nist.gov/trec_eval/</p></note>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>//lucene.apache.org/ 20 https://lucene.apache.org/core/5_3_1/analyzers-common/org/apache/lucene/analysis/en/KStemFilter.html 21 https://lucene.apache.org/core/8_0_0/analyzers-common/org/apache/lucene/analysis/en/PorterStemFilter. html 22 https://lucene.apache.org/core/7_0_1/core/org/apache/lucene/search/similarities/BM25Similarity.html 23 https://lucene.apache.org/core/8_0_0/core/org/apache/lucene/search/similarities/LMDirichletSimilarity.html</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="22,112.66,111.28,395.17,10.91;22,112.66,124.83,394.04,10.91;22,112.66,138.38,146.75,10.91" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sch√ºtze</surname></persName>
		</author>
		<ptr target="http://nlp.stanford.edu/IR-book/information-retrieval-book.html" />
		<title level="m" coord="22,304.83,111.28,170.13,10.91">Introduction to Information Retrieval</title>
		<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,151.93,276.16,10.91" xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Touche</forename><surname>Clef</surname></persName>
		</author>
		<ptr target="https://webis.de/events/touche-21/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,165.48,393.53,10.91;22,112.28,179.03,393.71,10.91;22,112.66,192.57,393.73,10.91;22,112.34,206.12,286.03,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="22,415.08,165.48,91.11,10.91;22,112.28,179.03,167.13,10.91">Data Acquisition for Argument Search: The args.me corpus</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-30179-8_4</idno>
	</analytic>
	<monogr>
		<title level="m" coord="22,112.66,192.57,241.68,10.91">German Conference on Artificial Intelligence (KI 2019)</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Benzm√ºller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Stuckenschmidt</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="48" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,260.32,394.52,10.91;22,112.66,273.87,393.33,10.91;22,112.66,287.42,393.33,10.91;22,112.66,300.97,393.53,10.91;22,112.66,314.52,291.42,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="22,336.55,273.87,169.43,10.91;22,112.66,287.42,37.55,10.91">Overview of Touch√© 2022: Argument Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fr√∂be</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,172.83,287.42,333.15,10.91;22,112.66,300.97,268.87,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction. 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<title level="s" coord="22,388.23,300.97,117.96,10.91;22,112.66,314.52,31.10,10.91">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="22,112.66,328.07,395.01,10.91;22,112.66,341.62,167.53,10.91" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="22,194.27,328.07,67.36,10.91">Apache Lucene</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Foundation</surname></persName>
		</author>
		<ptr target="https://lucene.apache.org/core/corenews.html#apache-lucenetm-910-available" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,355.17,394.04,10.91;22,112.41,368.71,316.55,10.91" xml:id="b5">
	<monogr>
		<ptr target="https://lucene.apache.org/core/7_0_1/core/org/apache/lucene/search/similarities/Similarity.html" />
		<title level="m" coord="22,246.96,355.17,115.43,10.91">Apache lucene similarities</title>
		<imprint>
			<publisher>Apache Software Foundation</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,382.26,375.37,10.91" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Community</surname></persName>
		</author>
		<ptr target="https://github.com/usnistgov/trec_eval#readme" />
		<title level="m" coord="22,179.23,382.26,39.00,10.91">Trec eval</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,395.81,395.01,10.91;22,112.66,409.36,47.61,10.91" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="22,212.86,395.81,158.77,10.91">Princeton university &apos;about wordnet</title>
		<ptr target="https://wordnet.princeton.edu/" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>Princeton University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,422.91,301.08,10.91;22,437.74,422.91,70.09,10.91;22,112.66,436.46,394.04,10.91;22,112.66,450.01,157.86,10.91;22,320.17,450.01,187.49,10.91;22,112.66,466.00,417.77,7.90;22,112.66,477.11,68.91,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="22,112.66,422.91,296.73,10.91">The box plot: A simple visual method to interpret data</title>
		<idno type="DOI">10.7326/0003-4819-110-11-916</idno>
		<ptr target="https://www.acpjournals.org/doi/pdf/10.7326/0003-4819-110-11-916,pMID:2719423" />
	</analytic>
	<monogr>
		<title level="j" coord="22,437.74,422.91,70.09,10.91;22,112.66,436.46,79.96,10.91">Annals of Internal Medicine</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="916" to="921" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,490.66,394.53,10.91;22,112.66,504.21,397.48,10.91;22,112.36,520.20,109.22,7.90" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="22,366.71,490.66,140.48,10.91;22,112.66,504.21,155.58,10.91">Application of student&apos;s t-test, analysis of variance, and covariance</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pandey</surname></persName>
		</author>
		<idno type="DOI">10.4103/aca.ACA_94_19</idno>
	</analytic>
	<monogr>
		<title level="j" coord="22,276.72,504.21,134.70,10.91">Annals of Cardiac Anaesthesia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">407</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,531.30,394.61,10.91;22,112.31,544.85,379.53,10.91" xml:id="b10">
	<monogr>
		<ptr target="https://en.wikipedia.org/w/index.php?title=Analysis_of_variance&amp;oldid=1085080872" />
		<title level="m" coord="22,112.66,531.30,361.55,10.91">Wikipedia contributors, Analysis of variance -Wikipedia, the free encyclopedia</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,558.40,394.03,10.91;22,112.66,571.95,394.65,10.91;22,112.66,585.50,22.69,10.91" xml:id="b11">
	<monogr>
		<ptr target="https://bitbucket.org/upd-dei-stud-prj/seupd2021-btg/src/master/HMW2_BTG/ARQMath_Task_1__BetterThanG_solution.pdf" />
		<title level="m" coord="22,112.66,558.40,211.40,10.91">btg team, Arqmath task 1: Betterthang solution</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,112.66,599.05,394.04,10.91;22,112.66,612.60,254.99,10.91" xml:id="b12">
	<monogr>
		<ptr target="https://bitbucket.org/upd-dei-stud-prj/seupd2021-dtx/src/master/Datex_Report_HW2.pdf" />
		<title level="m" coord="22,112.66,599.05,214.02,10.91">Step approach to information retrieval</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>dtx team</note>
</biblStruct>

<biblStruct coords="22,112.66,626.15,394.03,10.91;22,112.66,639.70,243.78,10.91" xml:id="b13">
	<monogr>
		<ptr target="https://bitbucket.org/upd-dei-stud-prj/seupd2021-fpga/src/master/report_2/report2.pdf" />
		<title level="m" coord="22,162.07,626.15,164.28,10.91">Blade solution for argument retrieval</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>fpga team</note>
</biblStruct>

<biblStruct coords="22,112.66,653.25,393.33,10.91;22,112.66,666.80,394.90,10.91" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Cassetta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Piva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Vicentini</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/#paper-210" />
		<title level="m" coord="22,268.87,653.25,237.11,10.91;22,112.66,666.80,88.80,10.91">Document retrieval task on controversial topic with re-ranking approach</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2331" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,86.97,393.33,10.91;23,112.66,100.52,393.33,10.91;23,112.66,114.06,158.66,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="23,206.21,86.97,299.77,10.91;23,112.66,100.52,71.87,10.91">Vader: A parsimonious rule-based model for sentiment analysis of social media text</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hutto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,206.53,100.52,299.46,10.91;23,112.66,114.06,24.53,10.91">Proceedings of the international AAAI conference on web and social media</title>
		<meeting>the international AAAI conference on web and social media</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="216" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,127.61,393.32,10.91;23,112.66,141.16,332.86,10.91" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="23,313.84,127.61,192.14,10.91;23,112.66,141.16,26.59,10.91">Development of an ir system for argument search</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Alecci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Baldo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Martinelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ziroldo</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/#paper-208" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2302" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,154.71,394.04,10.91;23,112.66,168.26,94.58,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<ptr target="https://bitbucket.org/frrncl/se-unipd/src/master/hello-tipster/" />
		<title level="m" coord="23,154.90,154.71,107.45,10.91">Hello Tipster repository</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,181.81,394.86,10.91" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jackson</forename><surname>Contributers</surname></persName>
		</author>
		<ptr target="https://github.com/FasterXML/jackson" />
		<title level="m" coord="23,210.45,181.81,91.62,10.91">Jackson project home</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,195.36,395.01,10.91" xml:id="b19">
	<monogr>
		<ptr target="https://github.com/aneesha/RAKE" />
		<title level="m" coord="23,112.66,195.36,106.32,10.91">Theory and Applications</title>
		<meeting><address><addrLine>Rake</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley Sons</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,208.91,289.67,10.91" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="23,181.84,208.91,41.69,10.91">Datamuse</title>
		<ptr target="https://www.datamuse.com/api/" />
	</analytic>
	<monogr>
		<title level="m" coord="23,112.66,208.91,61.68,10.91">Datamuse API</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,222.46,394.52,10.91;23,112.66,236.01,22.69,10.91" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><surname>Whelk</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Io</surname></persName>
		</author>
		<ptr target="https://github.com/whelk-io/flesch-kincaid" />
		<title level="m" coord="23,155.32,222.46,153.69,10.91">Flesch-kincaid java implementation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,249.56,82.13,10.91;23,210.16,249.56,110.46,10.91;23,335.98,249.56,170.71,10.91;23,112.66,263.11,363.22,10.91;23,112.66,276.66,46.67,10.91" xml:id="b22">
	<monogr>
		<ptr target="https://lucene.apache.org/core/6_6_0/analyzers-common/org/apache/lucene/analysis/standard/ClassicTokenizer.html" />
		<title level="m" coord="23,112.66,249.56,77.13,10.91;23,210.16,249.56,106.12,10.91">Class classictokenizer</title>
		<imprint>
			<date type="published" when="2000">2000-2017</date>
		</imprint>
	</monogr>
	<note>Apache Lucene</note>
</biblStruct>

<biblStruct coords="23,112.66,290.20,393.32,10.91;23,112.66,303.75,296.77,10.91" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Cowley</surname></persName>
		</author>
		<idno type="DOI">10.1002/9780470689646.ch1</idno>
		<title level="m" coord="23,295.42,290.20,210.56,10.91;23,112.66,303.75,47.87,10.91">Automatic Keyword Extraction from Individual Documents</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,112.66,317.30,395.17,10.91;23,112.66,330.85,393.86,10.91;23,112.66,344.40,162.35,10.91" xml:id="b24">
	<monogr>
		<ptr target="https://en.wikipedia.org/w/index.php?title=Flesch%E2%80%93Kincaid_readability_tests&amp;oldid=1082199491" />
		<title level="m" coord="23,112.66,317.30,395.17,10.91;23,112.66,330.85,22.17,10.91">Wikipedia contributors, Flesch-kincaid readability tests -Wikipedia, the free encyclopedia</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
