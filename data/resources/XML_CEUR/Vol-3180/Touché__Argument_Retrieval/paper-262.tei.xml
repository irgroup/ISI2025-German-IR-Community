<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,88.78,84.74,409.95,15.42;1,88.69,106.66,200.06,15.42;1,89.29,129.00,324.97,11.96">The Pearl Retriever: Two-Stage Retrieval for Pairs of Argumentative Sentences Notebook for the Touché Lab on Argument Retrieval at CLEF 2022</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,154.90,88.97,11.96"><forename type="first">Sebastian</forename><surname>Schmidt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Leipzig University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,190.91,154.90,60.65,11.96"><forename type="first">Jonas</forename><surname>Probst</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Leipzig University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,264.20,154.90,68.78,11.96"><forename type="first">Bianca</forename><surname>Bartelt</surname></persName>
							<email>bianca.bartelt@studserv.uni-leipzig.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Leipzig University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,363.97,154.90,76.87,11.96"><forename type="first">Alexander</forename><surname>Hinz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Leipzig University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,88.78,84.74,409.95,15.42;1,88.69,106.66,200.06,15.42;1,89.29,129.00,324.97,11.96">The Pearl Retriever: Two-Stage Retrieval for Pairs of Argumentative Sentences Notebook for the Touché Lab on Argument Retrieval at CLEF 2022</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">485895C2652405486FDE27AAD01F9D91</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Argument retrieval</term>
					<term>Controversial questions</term>
					<term>ArgRank</term>
					<term>Sentence matching</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the retrieval approach submitted by Team Pearl to the first Touché shared task at CLEF 2022 <ref type="bibr" coords="1,172.06,232.06,9.41,8.97" target="#b0">[1]</ref>. The model combines two retrieval pipelines to obtain pairs of argumentative sentences for a given query that relates to a controversial topic. The first pipeline uses a Dirichlet model to identify relevant arguments while the second applies a DPH model to retrieve relevant sentences. Both sentences and arguments are filtered using pre-calculated scores of argumentative quality and only sentences that belong to one of the remaining arguments are presented as results. We experimented with reranking retrieved arguments using an adapted version of the ArgRank proposed by Wachsmuth et al. <ref type="bibr" coords="1,208.32,297.81,10.54,8.97" target="#b1">[2]</ref> but did not find our implementation to improve retrieval performance beyond chance effects. Furthermore, we evaluated different approaches of matching sentences to form coherent pairs and found the naive approach of choosing partners from the immediate neighbourhood of a sentence in its parent argument to outperform more sophisticated solutions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Popular web search engines are a central access point to the vast range of available information on the Web. Despite that position, they currently do not address the challenges around retrieving argumentative texts like the assessment of argumentative quality <ref type="bibr" coords="1,390.47,459.48,11.58,10.91" target="#b2">[3]</ref>. With our submission to the third Touché lab, we aim to contribute to the lab's goal of trying to close this research gap by exploring how to identify pairs of sentences that represent arguments relevant to a user's opinion formation process on a controversial topic. A relevant argument is one that both discusses the topic at hand as well as fulfills quality criteria like logical coherence. A pair of sentences can be considered representative of such an argument if it contains a central take-away from the argument, is itself relevant to the topic, and is both argumentative and coherent. We aim to address these challenges with our retrieval model that consists of two separate retrieval pipelines and quality assessments, ArgRank-based reranking <ref type="bibr" coords="1,432.15,567.88,11.28,10.91" target="#b1">[2]</ref>, and sentence matching that aims for high coherence and quality in the resulting pair. 1  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the following, we refer to previous work that provided the inspiration and basis for our retrieval approach. First, references are made to argument structure, then to argument relevance, and finally to argument and sentence quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Argument Structure</head><p>The two central units of retrieval for the context of this paper are arguments and sentences taken from these arguments. As our approach was inspired by the framework for argument search introduced by Wachsmuth et al. <ref type="bibr" coords="2,269.64,215.20,11.58,10.91" target="#b3">[4]</ref>, this paper uses the argument model put forth in their paper. This model builds on the common argument structure of conclusion and premises, with the former being the main claim of an argument and the latter sentences that discuss the argument's topic to arrive at the conclusion <ref type="bibr" coords="2,286.82,255.85,11.43,10.91" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Argument Relevance</head><p>The retrieval of argumentative sentences is a classic example of information retrieval. According to Stein et al. <ref type="bibr" coords="2,156.90,319.13,11.58,10.91" target="#b5">[6]</ref>, a retrieval task consists of responding to an information need using an information resource like a collection of documents. The goal of this task is to identify a subset of documents from the information resource that the user considers relevant to their information need. This is achieved by retrieval models which aim to rank the available documents according to the probability that they are relevant to a given information need stated in the form of a query. While there is a wide range of general retrieval models available, no specific model was developed for the task of argument retrieval so far. Given this research gap, Potthast et al. <ref type="bibr" coords="2,437.41,413.97,12.69,10.91" target="#b6">[7]</ref> assessed the performance of four popular retrieval models in this field . Specifically, they applied each model to the task of argument retrieval on a set of 20 controversial issues, represented by a neutral and a biased query per issue, and evaluated the models' performance on the relevance of the results to the query as well as three measures of argument quality (rhetorical, logical, and dialectical quality). The results showed that the DPH <ref type="bibr" coords="2,278.35,481.72,12.78,10.91" target="#b7">[8]</ref> and Dirichlet <ref type="bibr" coords="2,354.63,481.72,12.78,10.91" target="#b8">[9]</ref> model both clearly outperform BM25 <ref type="bibr" coords="2,111.04,495.27,21.75,10.91" target="#b9">[10]</ref> and TF-IDF <ref type="bibr" coords="2,188.89,495.27,16.22,10.91" target="#b10">[11]</ref>, with DPH achieving the best performance on the relevance metric and Dirichlet demonstrating lower variance than DPH as well achieving a higher score on two of the quality metrics. Building on these findings, we applied both models to the retrieval of argumentative sentence pairs. The Dirichlet model is a language model approach, a subcategory of retrieval models that develop document-specific language models and rank documents based on the probability that their respective language model generated the query <ref type="bibr" coords="2,326.07,576.56,11.40,10.91" target="#b5">[6]</ref>. This relevance calculation is derived from Bayes' theorem applied to the conditional probability for a document 𝑑 𝑖 given query 𝑞:</p><formula xml:id="formula_0" coords="2,241.78,613.57,264.20,24.43">𝑃 (𝑑 𝑖 |𝑞) = 𝑃 (𝑞|𝑑 𝑖 )𝑃 (𝑑 𝑖 ) 𝑃 (𝑞)<label>(1)</label></formula><p>Given that the probability of a query 𝑃 (𝑞) is the same for all documents, it is omitted from the equation. Furthermore, the probability distribution for all documents is assumed to be uniform and 𝑃 (𝑑 𝑖 ) is therefore discarded as well as it wont affect the ranking between documents. This uniformity assumption can however it be dropped in favor of query-independent relevance judgements that estimate the document-specific probabilities 𝑃 (𝑑 𝑖 ). One way to achieve this was suggested by Wachsmuth et al. <ref type="bibr" coords="3,250.18,154.71,12.86,10.91" target="#b1">[2]</ref> with their reinterpretation of the PageRank algorithm <ref type="bibr" coords="3,89.29,168.26,17.76,10.91" target="#b11">[12]</ref> for the application of argument search. The authors distinguish between the local and global relevance of an argument and its parts, with the former assessing if its premises are relevant to the conclusion of the argument and the latter evaluating the contribution of an argument as a whole to resolving a debate <ref type="bibr" coords="3,234.58,208.91,34.04,10.91">[13][14]</ref>. The "ArgRank" estimates the global relevance of an argument based on other arguments that use its conclusion as a premise, thereby assuming that the relevance of a conclusion to a given discussion is reflected by how many other arguments in that discussion refer to it. Similar to PageRank, the ArgRank is calculated as the weighted sum of a ground relevance (left term in equation 2) and a recursive relevance (right term) that rewards both a high rank as well as a low number of outgoing links of referring documents.</p><formula xml:id="formula_1" coords="3,196.31,300.11,309.67,29.52">𝑝 ˆ(𝑐 𝑖 ) = (1 -𝛼) * 𝑝(𝑑) * |𝐷| |𝐴| + 𝛼 * ∑︁ 𝑗=1 𝑝 ˆ(𝑐 𝑗 ) |𝑃 𝑗 |<label>(2)</label></formula><p>• 𝑝 ˆ(𝑐 𝑖 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ArgRank of argument 𝑖 • 𝛼</head><p>Weighting factor between 0 and 1 </p><formula xml:id="formula_2" coords="3,164.95,582.18,341.04,26.48">𝐵(|𝑑 𝑖 |, 𝑓 𝑖𝑗 , 𝑃 (𝑡 𝑗 )) = (︂ |𝑑 𝑖 | 𝑓 𝑖𝑗 )︂ * 𝑃 (𝑡 𝑗 ) 𝑓 𝑖𝑗 * (1 -𝑃 (𝑡 𝑗 )) |𝑑 𝑖 |-𝑓 𝑖𝑗<label>(3)</label></formula><p>Hence, the model calculates the probability of observing 𝑓 𝑖𝑗 occurrences of 𝑡 𝑗 in 𝑑 𝑖 given (i) the probability 𝑃 (𝑡 𝑗 ) of observing the term in the underlying population and (ii) the number of terms |𝑑 𝑖 | of the document 𝑑 𝑖 . In order to turn an unlikely observation into a high information value, the resulting probability is transformed using the binary logarithm multiplied by minus one:</p><formula xml:id="formula_3" coords="4,204.75,100.52,301.23,12.35">𝐼𝑛𝑓 (𝑓 𝑖𝑓 ||𝑑 𝑖 ) = -log 2 [𝐵(|𝑑 𝑖 |, 𝑓 𝑖𝑗 , 𝑃 (𝑡 𝑗 ))]<label>(4)</label></formula><p>This equation for the information value is then used to estimate the relevance of a document 𝑑 𝑖 based on the terms 𝑡 𝑗 of the query 𝑞.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Argument and Sentence Quality</head><p>Argument Quality has been thoroughly discussed by Wachsmuth et al., who identified three main dimensions <ref type="bibr" coords="4,170.66,210.92,16.41,10.91" target="#b14">[15]</ref>. First, logical quality assesses if the argument is well structured and correctly builds on premises to form its conclusion. Second, rhetorical quality represents how well-written and persuasive the argument is in the given context. And finally, dialectical quality evaluates how important the argument is for resolving the current discussion.</p><p>Based on these quality dimensions, Gienapp et al, introduced the Webis Argument Quality Corpus 2020 <ref type="bibr" coords="4,146.25,278.66,16.09,10.91" target="#b15">[16]</ref>, which contains 1,610 text spans in total, of which 339 were annotated as nonarguments while the remaining 1,271 arguments were annotated in the given quality dimensions on a scale between -4 (spam) and 3 (high quality). In addition to that, these arguments also received a combined quality score. The scores were calculated using pairwise annotations by crowd workers, which has shown to yield results of higher quality in comparison with traditional rating methods in which the argument quality is assessed directly.</p><p>For the Touché Task 2021, Team Yeagerists <ref type="bibr" coords="4,283.31,373.51,17.95,10.91" target="#b16">[17]</ref> used this dataset to train a quality estimation model based on BERT <ref type="bibr" coords="4,186.87,387.05,17.75,10.91" target="#b17">[18]</ref> to refine their argument ranking function. Their best model achieves an 𝑅 2 -Score of 0.7439 and a MSE of 0.7280 on the test set. The model architecture is based on the base model from Gretz et al. <ref type="bibr" coords="4,384.51,414.15,16.08,10.91" target="#b18">[19]</ref>, who compare different versions of BERT models to predict sentence quality on the IBM-Rank-30k dataset, which was also introduced in <ref type="bibr" coords="4,170.52,441.25,16.09,10.91" target="#b18">[19]</ref>. It consists of over 30,000 sentences which were evaluated for quality on a scale between 0 and 1. Their base model is the pre-trained BERT model without fine-tuning to which a fully-connected hidden layer with 100 neurons is added, feeding their outputs to a sigmoid activation layer to produce a single output. The base model achieves a Pearson correlation of 0.48 and a Spearman Correlation of 0.43 on the test set. This is a significant improvement over the method based on GloVe embeddings and even more so over the simple baseline that only considers argument length.</p><p>In our work, we used both models for predicting argument and sentence quality, respectively, and implemented them using the Huggingface library and Pytorch, leaving the model architectures unchanged. For the sentence model, we achieved Pearson correlation of 0.489 and a Spearman Correlation of 0.436, replicating the quality scores of Gretz et al. <ref type="bibr" coords="4,374.64,576.74,16.16,10.91" target="#b18">[19]</ref>. For the argument model, we achieved an R2-score of 0.7, which is worse than the 0.74 achieved by Team Yeagerists <ref type="bibr" coords="4,89.29,603.84,16.16,10.91" target="#b16">[17]</ref>. Since we replicate their architecture exactly, this difference can likely be attributed to the random split in training, validation and test set. The results are still comparable, so the model is used as is. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Retrieval Model</head><p>Our retrieval model was developed in iterative steps that are described below. After preprocessing the dataset, we developed a vertical prototype to explore the task and identify potential for improvement. These learnings were then used to develop the refined prototype.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>The underlying dataset is taken from the args.me corpus <ref type="bibr" coords="5,348.07,460.79,16.41,10.91" target="#b19">[20]</ref>, collected for the development of argument search engines. It consists of roughly 360,000 arguments crawled from debating platforms such as idebate.org, debatepedia.org, debatewise.org and debate.org. In addition to the argument text grouped into conclusion and premises, each argument has a set of meta data including the title of the discussion it was stated in and the stance of the argument to that discussion. In order to perform retrieval on individual sentences, the premises and conclusions of the args.me corpus were provided as an additional document collection.</p><p>After an initial exploration of the argument and sentence collection, we implemented an idea developed by Gienapp <ref type="bibr" coords="5,183.19,569.18,19.83,10.91" target="#b20">[21]</ref> for Touché 2021 to clean the dataset of non-argumentative documents.</p><p>In their approach, the authors trained a Support Vector Machine to predict if a given text is argumentative based on the "Is Argument?"-label in the Webis Argument Quality Corpus 2020. We used the same approach and achieved the same F1-score of 0.88, using 10-fold crossvalidation. The 63,019 arguments that were removed from the corpus have an average quality of -1.76 according to our argument quality prediction model. This shows a strong agreement between the non-argument-label given in the Webis Argument Quality Corpus 2020 and the quality scores our model predicts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Vertical Prototype</head><p>The first step in constructing the retrieval approach was to develop a vertical prototype in order to both explore our initial ideas as well as create an evaluation baseline against which to compare the effect of any subsequent changes to the approach. This vertical prototype consists of a combination of two parallel retrieval pipelines, visualized in Figure <ref type="figure" coords="6,416.72,148.18,3.81,10.91" target="#fig_0">1</ref>. The first pipeline uses Dirichlet to retrieve relevant arguments, while the second directly retrieves sentences with DPH. Both retrieval models are implemented using PyTerrier, the Python API for Terrier <ref type="bibr" coords="6,89.29,188.83,16.32,10.91" target="#b21">[22]</ref>. After obtaining a ranked set of retrieved arguments with Dirichlet, these arguments are filtered based on the pre-computed quality scores from the argument quality estimation model described in chapter 2.3. The remaining arguments are then used to filter the set of sentences retrieved by the DPH model. This is done to ensure that the remaining sentences are not only deemed relevant by the DPH model, but are also taken exclusively from qualitatively good arguments that are relevant to the query. In the last step, the retrieved sentences are matched with the conclusion of their respective argument to create the final sentence pairs. These pairs are then ranked according to the relevance score of the sentence retriever. The performance of this initial approach was evaluated on a set of 35 queries taken from the controversial topics used in the Touché Task 2021 <ref type="bibr" coords="6,324.37,310.77,16.41,10.91" target="#b22">[23]</ref>. For each of these queries, the ten most relevant sentence pairs were obtained and evaluated across three metrics. <ref type="foot" coords="6,438.67,322.57,3.71,7.97" target="#foot_0">2</ref> In addition to providing a baseline for the evaluation of changes to the retrieval process, this also yielded two important insights on issues with the existing approach.</p><p>The first discovery of the baseline evaluation was that the reason the DPH model was chosen for sentence retrieval, namely its strong focus on documents that contain relatively rare terms of the query, is also one of its weaknesses. On the one hand, the model is susceptible to homonyms and terms being used in a different context than that of the query. For instance, the query "Should Insider Trading Be Allowed?" yielded sentences debating whether or not the attacks on September 11, 2001 were an "inside job". This behavior could be one explanation of the relatively high score variance that Potthast et al. found for DPH in comparison with Dirichlet-based retrieval <ref type="bibr" coords="6,130.26,459.81,11.59,10.91" target="#b6">[7]</ref>. On the other hand, the model assigns a high relevance to sentences even if the terms are only used as part of a URL or other types of sources. This behavior negatively affects the task-specific retrieval performance as some highly ranked results were not argumentative premises but a list of sources for the respective argument.</p><p>The second discovery was that matching all retrieved sentences with the conclusion of their parent argument led to large quality variations in the results. This circumstance is a consequence of the way that the args.me corpus was created. As the majority of arguments crawled from the debate platforms did not have a dedicated "conclusion"-field, this missing data was imputed with the title of the discussion that the argument was stated in. Given that most arguments on these platforms take a stance (PRO/CON) to a discussion title, these imputed conclusions occasionally reflect a position opposite to that of the argument they belong to. Hence, in some instances, using the conclusions as partners for the retrieved sentences resulted in incoherent sentence pairs and a failure to correctly represent the parent argument. The final retrieval approach of the refined prototype.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Refined Prototype</head><p>The features of the refined prototype shown in Figure <ref type="figure" coords="7,342.74,324.33,5.17,10.91" target="#fig_1">2</ref> were largely chosen based on the insights gained in the evaluation of the vertical prototype. However, some initial changes were made before addressing the issues outlined above. Firstly, we introduced a quality filter for sentences, which was trained on the IBM-Rank-30k dataset <ref type="bibr" coords="7,361.53,364.98,16.41,10.91" target="#b18">[19]</ref>, as described in chapter 2.3. Secondly, we added a query expansion to address the problem of term mismatch by expanding the original query with new, related terms. Specifically, we applied the Bo1 query expansion as implemented in the PyTerrier platform. Bo1 is a "Divergence From Randomness"-weighting model based on the Bose-Einstein statistics <ref type="bibr" coords="7,281.17,419.18,17.81,10.91" target="#b23">[24]</ref> and research on query expansion has shown it to be effective in finding additional terms for a search query <ref type="bibr" coords="7,357.69,432.73,33.87,10.91">[25][26]</ref>. The Bo1 model achieves this goal by extracting terms from the top ranked documents obtained for the original query, weighting them based on their informativeness and adding the highest weighted terms to the original query.</p><p>After adding the sentence quality filter and query expansion, the identified weaknesses of the vertical prototype were addressed. The effect of these changes was evaluated on a reduced set of ten topics from the Touché Task 2021 <ref type="bibr" coords="7,265.01,514.02,16.09,10.91" target="#b22">[23]</ref>, containing the five best and five worst performing topics for the vertical prototype. For each of these topics, the ten most relevant sentence pairs were used to calculate the nDCG@10, with the vertical prototype achieving a score of 0.4977.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DPH's Weaknesses</head><p>The first measure aimed to reduce DPH's susceptibility to query terms used as part of URLs inside arguments. Therefore, the argument as well as the sentences collection were preprocessed by replacing all instances of URLs with placeholders in the form of "[URL]" and calculating a new index. As a consequence, only those sentences that contain query terms in the "regular" text body are rewarded by DPH. Afterwards, to reduce the bias that is introduced by terms being used in different contexts than that of the query, the calculation of the final relevance score was adapted. While the relevance scoring of the vertical prototype was based entirely on the DPH model, the refined prototype also incorporates the relevance scores of the sentences' parent arguments as obtained by the Dirichlet model. The idea behind this solution was to use the lower variance of Dirichlet to "stabilize" the retrieval results and reward sentences from arguments with a high estimated relevance to the given query. After the adaptation, the retrieval results were initially ranked by their argument relevance first and sentence relevance second (i.e. within the same argument). This solution, however, comes with its own drawback as the model returns all retrieved sentences from the most relevant argument before moving on to those from the second most relevant argument and so forth. As a consequence, a sentence with the highest relevance according to the DPH model will only be returned as the most relevant result if it is also part of the argument with the highest relevance according to the Dirichlet model. Therefore, as the final change to the refined prototype, the total score was calculated based on a weighted sum between the argument and the sentence score. The weighting factor was calculated using the generalized reduced gradient method as implemented by the Solver add-in for Microsoft Excel to find the weighting that would have resulted in an optimal ranking of existing retrieval results. As this step was applied at the end of model development, the optimal weighting factors were identified for the two best performing approaches: The "blocklist"-model and the "ArgRank"-model that uses all features of the former and re-ranks results using ArgRank.</p><p>The identified weighting factor for sentence relevance was 0.38 in case of the ArgRank-model and 0.51 for the blocklist-model with the argument relevance being assigned the remaining weight. A more detailed description of the two approaches can be found in the corresponding sections below. The optimally weighted scores resulted in nDCG-values of 0.7332 for the blocklist-model and 0.7352 for the ArgRank-model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence Matching</head><p>As the pairing of a sentence with its conclusion proved ineffective, we experimented with three different approaches to identify a partner for a given sentence. The first approach consisted of using Next Sentence Prediction (NSP) <ref type="bibr" coords="8,276.13,475.02,18.06,10.91" target="#b17">[18]</ref> based on BERT-encodings. In more detail, the sentences that remain after filtering by retrieved arguments and sentence quality are used as a set of pairing candidates 𝑅. The sentences in this set are then processed in order of estimated relevance with the most relevant sentence 𝑠 𝑖 being combined with all other available sentences.</p><p>After identifying the partner 𝑠 𝑗 that is most likely to follow 𝑠 𝑖 , both sentences are removed from 𝑅 to avoid redundant results and the process continues with the next most relevant sentence. This matching procedure did not improve retrieval performance over that of the vertical prototype as it resulted in a relatively low nDCG of 0.4255. In our manual evaluation we found that the NSP tends to value sentences higher which contain the same or similar words as the first sentence. In many cases, this results in the second sentence only repeating the content of the first sentence, and thereby not providing new information.</p><p>In order to increase diversity in the sentence pairs, the second approach was inspired by the Maximal Marginal Relevance (MMR) that Carbonell and Goldstein <ref type="bibr" coords="8,378.29,637.62,17.76,10.91" target="#b26">[27]</ref> introduced as a result set diversification method. Instead of selecting retrieval results by weighting between relevance to the query and similarity to the already existing retrieval set, our approach forms sentence pairs by weighting between NSP-score and cosine similarity of the BERT-encodings. This approach also works iteratively over the set of pairing candidates 𝑅 in order of relevance and determines the best partner for a sentence 𝑠 𝑖 by computing a score for each possible candidate 𝑠 𝑗 in 𝑅.</p><p>max</p><formula xml:id="formula_4" coords="9,185.32,141.16,320.67,18.35">𝑠 𝑗 𝜖𝑅∖{𝑠 𝑖 } [𝜆 * 𝑠𝑖𝑚 1 (𝑠 𝑖 , 𝑠 𝑗 ) -(1 -𝜆) * 𝑠𝑖𝑚 2 (𝑠 𝑖 , 𝑠 𝑗 )]<label>(5)</label></formula><p>We implemented this approach from scratch as follows: The score is calculated using the linear combination of 𝑠𝑖𝑚 1 (normalized NSP) and 𝑠𝑖𝑚 2 (cosine similarity between 𝑠 𝑖 and 𝑠 𝑗 ). Again, the sentence 𝑠 𝑗 with the highest score is designated as the partner for 𝑠 𝑖 and both are removed from the set of pairing candidates 𝑅. When 𝑠𝑖𝑚 1 and 𝑠𝑖𝑚 2 were equally weighted (𝜆 = 0.5), we obtained a very low nDCG of 0.2801. Setting 𝜆 equal to 1 returns the Next Sentence Prediction mentioned above. Given these results, we concluded that punishing cosine similarity negatively affects retrieval performance and did not explore other levels for 𝜆.</p><p>The final method for sentence matching we explored, was the naive approach of choosing a partner from a sentence's "neighbourhood" in its parent argument. For a retrieved sentence 𝑠 𝑖 , its partner is chosen to be either the preceding sentence 𝑠 𝑖-1 or the following one 𝑠 𝑖+1 . The choice depends on which of the two candidates produces the higher quality score when matched with 𝑠 𝑖 . We calculated the quality between adjacent sentences using the same model that we had previously used for individual sentence quality. While the scores and pairs are pre-calculated in this approach, the set of retrieved sentences 𝑅 is still processed iteratively and any used sentence removed from it to avoid duplicates. Among the three approaches applied to sentence matching, this one clearly outperformed the other two with a nDCG of 0.6593 and was thus chosen for our refined prototype.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blocklist</head><p>A comparably small change that proved effective was the inclusion of a blocklist for certain sentence parts. During multiple manual evaluation sessions, we noticed a recurring pattern of sentences in the retrieval results that either did not contain arguments or even cited positions of an opposing stance. Those sentences often contained specific statements like "my opponent claims... ", "PRO claims.. " or "I accept this debate", which are commonly used on debate platforms. The addition of a blocklist that filters out sentences containing these phrases led to an nDCG of 0.6914 and thus showed clear improvement over the previous value of 0.6593 for neighbour matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reranking Using ArgRank</head><p>As described in chapter 2.2, the probability calculation of the Dirichlet retrieval model can be extended to also include a query-independent document probability in the form of an ArgRank. The first step towards calculating this probability is to construct a directed argument graph with edges 𝑒 = (𝑑 𝑗 , 𝑑 𝑖 ) denoting that argument 𝑗 uses the conclusion 𝑐 𝑖 of argument 𝑖 as one of its premises 𝑝 𝑗𝑘 (The subscript 𝑘 refers to the position of the premise in 𝑗). In order to find these edges, reuses of 𝑐 𝑖 need to be identified by searching for semantically equivalent premises in the sentence collection. While determining semantic equivalence continues to be a difficult challenge <ref type="bibr" coords="10,133.87,86.97,11.33,10.91" target="#b1">[2]</ref>, transformers have proven very capable at encoding semantic information of text passages. The specific model we chose for this task is the MPNet proposed by Song et al. <ref type="bibr" coords="10,487.35,100.52,16.25,10.91" target="#b27">[28]</ref>.</p><p>Both the conclusion 𝑐 𝑖 and all premises 𝑝 𝑗𝑘 from a set of candidates 𝑃 𝑐 are encoded using this sentence transformer. The semantic similarity between 𝑐 𝑖 and every 𝑝 𝑗𝑘 ∈ 𝑃 𝑐 is then calculated as the cosine similarity between the encodings. If the cosine similarity between 𝑝 𝑗𝑘 and 𝑐 𝑖 is above a threshold of 0.7, the edge (𝑑 𝑗 , 𝑑 𝑖 ) is added to the argument graph together with the specific similarity score 𝑠𝑖𝑚(𝑐 𝑖 , 𝑝 𝑗𝑘 ) and the number of premises |𝑃 𝑗 |. The search space for the set of candidate premises 𝑃 𝑐 is restricted to those arguments that both were stated in the same discussion as well as have the same stance towards the discussion's topic as argument 𝑖. This more conservative approach was chosen to increase the probability that a semantically similar sentence constitutes a reuse of 𝑐 𝑖 as it is stated in support of an "allied" argument in the same discussion. Furthermore, given that discussion titles were used to impute missing conclusions in the collection, the search for edges of the graph was only conducted for arguments with a conclusion that is different from the corresponding discussion title.</p><p>After constructing the argument graph, the next step consisted in calculating the ArgRank.</p><p>Here, we again made some alterations to the approach suggested by Wachsmuth et al. <ref type="bibr" coords="10,467.03,303.75,11.28,10.91" target="#b1">[2]</ref>. First, as all arguments in the collection were obtained from debate platforms, the relevance of their parent documents (i.e., the web pages they were taken from) is assumed to be equal for all arguments. Hence, the term reflecting the ground relevance is normalized using |𝐴|, the number of arguments in the collection. Second, we experimented with using the cosine similarity of an edge in the argument graph as a weighting factor of the recursive relevance. This was done to evaluate how punishing lower semantic similarity scores would affect retrieval performance. These changes lead to the following, adapted version of the ArgRank from equation 2 where 𝑠𝑖𝑚(𝑐 𝑖 , 𝑝 𝑗𝑘 ) constitutes an optional use of the similarity score as a multiplicand:</p><formula xml:id="formula_5" coords="10,179.52,435.61,326.46,29.52">𝑝 ˆ(𝑐 𝑖 ) = (1 -𝛼) * 1 |𝐴| + 𝛼 * ∑︁ 𝑗=1 𝑝 ˆ(𝑐 𝑗 ) |𝑃 𝑗 | * 𝑠𝑖𝑚(𝑐 𝑖 , 𝑝 𝑗𝑘 )<label>(6)</label></formula><p>As the similarity scores were saved for all edges of the argument graph, different versions of the graph were constructed based on a minimum required similarity. Each version of the graph was then combined with different values of the weighting factor 𝛼 and one of two versions of the recursive relevance (similarity weighted or not) to conduct a grid search across the combinations. While the two best combinations (minimum similarity = 0.75 [0.80], 𝛼 = 0.3 [0.4], both with unweighted recursive relevance) managed to achieve a slightly higher overall nDCG@10 than the blocklist-model (0.6944 [0.6924]), this does not permit the conclusion that ArgRank improved the retrieval performance of our model. Firstly, the reranking through ArgRank was applied on top of the blocklist-model and thus benefits from all previous approaches. Secondly, the improvement in nDCG resulted not from an increase in the relevance metric but from one in the "argument representation" metric by retrieving two new sentence pairs while the remaining 98 results stayed the same as for the blocklist model. Hence, the reranking did not improve the metric it aimed for and the observed improvement can only be attributed to chance. 0.4977 (0,02971) 0.3997 (0,18709) 0.3966 (0,06420) 0.6967 (0,02848)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Metric-specific and overall mean nDCG@10 with variance in parentheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>The evaluation of our different approaches is based on the three following criteria with scores taken from {-2, 0, 1, 2, 3}: Argumentativeness, Sentence Coherence and Argument Representation.</p><p>While the values between one and three were given for increasing quality in the respective metric, the meaning of both zero and -2 depended on the category. In order of the metrics above, a zero was used for (i) non-argumentative sentences, (ii) unrelated sentence pairs, and (iii) representation of the argument by only one sentence. The negative score was assigned to pairs that were (i) irrelevant to the query, (ii) contradicting themselves, and (iii) contradicting their parent argument. Before calculating the nDCG-scores, a value of two was added to all evaluations to shift the range to non-negative values. In all evaluation steps, the ten highest ranked results per query were evaluated by two persons at a time and the final score per sentence pair and metric calculated as the average of those two evaluations. As discussed in chapter 3.2, the vertical prototype was evaluated on a larger set of 35 queries to get a broader overview. The query set for all subsequent approaches was then restricted to the five best performing and worst performing queries of the vertical prototype. Hence, the values reported for the three metrics in Figure <ref type="figure" coords="12,299.16,114.06,5.12,10.91" target="#fig_2">3</ref> are the average nDCG@10-scores across ten queries and two evaluators. The overall score is finally calculated as the average of the three nDCG-scores for each individual metric.</p><p>As visible in the graph, the introduction of a blocklist was able to improve performance beyond the neighbour-matching both in terms of argumentativeness as well as argument representation. Furthermore, the addition of ArgRank-based reranking only leads to slight improvements over the blocklist in the argument representation metric. Finally, using a weighted sum of the two relevance scores instead of ranking results by argument relevance first and sentence relevance second led to improvements both when using the ArgRank (Weighted AR vs. ArgRank) and when not using it (Weighted Blocklist vs. Blocklist).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>A few things became evident throughout the different stages of model development. Firstly, the DPH model's strength in identifying terms with a high information value for documents can turn into a weakness under certain circumstances. As shown in chapter 3.2, the model is susceptible to homonyms and does not regard the context a term is placed in. While we were able to partially address this issue with changes to the document collection and relevance scoring, a central weakness in the scope of this paper remains: Out of two documents with an equal number of occurrences 𝑓 𝑖𝑗 for term 𝑗, DPH will reward the one with a lower number of terms |𝑑 𝑖 |. Given that the relative importance of |𝑑 𝑖 | in equation 3 increases as documents have fewer terms, this effect becomes stronger for short documents such as sentences and potentially leads to biased results. Unfortunately, Dirichlet does not appear to be well-suited for sentence retrieval either, because the model's confidence in a document-specific language model decreases with document length. In order to clarify this assumption, a potential avenue for future research is to evaluate how well Dirichlet performs on the retrieval of argumentative sentences in comparison with DPH. Secondly, we found naive solutions such as the introduction of a blocklist or "neighbour"matching to have considerable positive impact on retrieval performance. While the success of our specific blocklist can be attributed to the origins of the document collection being debate platforms, we are confident that a similar solution is likely to also perform well on a broader corpus of argumentative sentences as recapping an opponents argument happens not exclusively on these platforms. The finding that matching sentences with their immediate neighbours in parent arguments outperforms more sophisticated approaches can likely be explained by the capabilities of current language systems. While transformers like BERT prove very effective at encoding semantic information, they appear to not yet be on par with human debaters in the task of creating argumentatively sound pairs of sentences. Finally, we found our version of the ArgRank to not lead to noticeable changes in retrieval performance despite slight improvements to the nDCG. This outcome can be attributed to the way the argument graph was constructed. Restricting the search for edges to unique conclusions and reuses in the same discussion with the same stance led to a sparse graph with only 44,250 edges for cosine similarity &gt; 0.7. As a consequence, a maximum of only 10,806 of the total 302,388 arguments is rewarded by a higher ArgRank. Hence, to get a better understanding of the impact of ArgRank, we suggest that future research increases the search space for reuses of conclusions. A potential way to do this with the args.me-corpus is to find candidate arguments not only in the same discussion as argument 𝑖 but in a set of retrieval results obtained by using the discussion title of 𝑖 as a query. This approach, while likely to yield more "reuse candidates", introduces further uncertainty. On the one hand, potential reuses need to be more carefully evaluated to ensure that a candidate argument does in fact discuss the same topic as argument 𝑖.</p><p>On the other hand, it needs to be determined if a candidate argument has the same stance as 𝑖 or an opposing one, a task that continues to be difficult to solve in a domain-agnostic, automated way <ref type="bibr" coords="13,110.37,236.01,16.25,10.91" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>The retrieval model we proposed in our paper is a first step towards addressing the challenge of presenting short overviews of arguments on a controversial topic without omitting considerations like argumentative quality and logical coherence. The combination of two retrieval pipelines helps in retrieving sentence pairs that are not only deemed relevant by themselves but also originate from a set of relevant arguments. By applying two stages of quality filters, we further refine the retrieval results and remove arguments and sentences that are not of sufficient argumentative quality. Finally, by evaluating different matching approaches, we were able to increase logical coherence and argument representation of retrieved sentence pairs beyond the baseline of matching with an argument's conclusion. Taken together, the stages of our retrieval model can offer key messages of arguments relevant to a user's information need. These sentence pairs are, however, by themselves insufficient for the opinion formation process and should only be used in combination with links to their sources to allow users to get a better understanding of the parent arguments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,89.29,312.32,416.69,8.93;5,89.29,324.32,92.33,8.87;5,89.29,84.19,416.70,220.43"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An overview of the vertical prototype, consisting of the two parallel pipelines for argument and sentence retrieval.</figDesc><graphic coords="5,89.29,84.19,416.70,220.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,89.29,266.16,258.26,8.93;7,89.29,84.19,416.69,169.41"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2:The final retrieval approach of the refined prototype.</figDesc><graphic coords="7,89.29,84.19,416.69,169.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="11,89.29,294.78,287.48,8.93;11,89.29,84.19,416.69,198.02"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Mean nDCG@10 scores and variance per evaluation metric.</figDesc><graphic coords="11,89.29,84.19,416.69,198.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.77,369.80,418.90,200.56"><head></head><label></label><figDesc>𝑖 being relevant to the query 𝑞<ref type="bibr" coords="3,216.68,478.16,11.28,10.91" target="#b5">[6]</ref>. The model's central assumption can be summarized as follows: If a given term 𝑡 𝑗 of the query 𝑞 is both relatively rare in the overall document collection 𝐷 and relatively common in a specific document 𝑑 𝑖 , then 𝑡 𝑗 has a high information content for 𝑑 𝑖 . While this relation generally follows a hypergeometric distribution, it can be reduced to the following binomial distribution for large document collections and comparably short documents where |𝑑 𝑖 | is the number of terms in 𝑑 𝑖 , 𝑓 𝑖𝑗 is the number of occurrences of 𝑡 𝑗 in 𝑑 𝑖 and 𝑃 (𝑡 𝑗 ) is its frequency in 𝐷:</figDesc><table /><note coords="3,107.28,369.80,28.94,10.91;3,161.81,369.80,148.19,11.36;3,313.23,369.80,50.68,10.91;3,107.28,384.58,24.68,10.91;3,161.81,384.58,171.68,10.91;3,107.28,399.36,23.53,10.91;3,161.81,399.36,221.04,10.91;3,107.28,414.14,32.36,11.36;3,161.81,414.14,171.64,11.36;3,336.68,414.14,54.89,10.91;3,107.28,428.92,26.73,11.36;3,161.81,428.92,154.96,10.91;3,88.96,451.06,417.03,10.91;3,89.29,464.61,413.31,10.91"><p><p>• 𝑝(𝑑) PageRank of the web page that 𝑐 𝑖 is stated on • |𝐷| Number of web pages in the collection • |𝐴| Number of arguments on all web pages combined • 𝑝 ˆ(𝑐 𝑗 ) ArgRank of an argument 𝑗 that uses 𝑐 𝑖 as a premise • |𝑃 𝑗 | Number of premises of argument 𝑗</p>The DPH model, finally, belongs to the family of probabilistic models that treat relevance as a binary event (1=relevant and 0=irrelevant) and try to estimate the probability of a document 𝑑</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="6,108.93,670.98,285.85,8.97"><p>A more detailed description of the evaluation process is provided in chapter 4.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="13,112.66,515.85,395.17,10.91;13,112.66,529.40,395.17,10.91;13,112.66,542.95,395.01,10.91;13,112.41,556.50,393.57,10.91;13,112.66,570.05,339.15,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,361.25,529.40,146.58,10.91;13,112.66,542.95,62.60,10.91">Overview of Touché 2022: Argument Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gurcke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,197.77,542.95,309.90,10.91;13,112.41,556.50,309.42,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction. 13th International Conference of the CLEF Association (CLEF 2022)</title>
		<title level="s" coord="13,429.80,556.50,76.18,10.91;13,112.66,570.05,78.83,10.91">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="13,112.66,583.60,394.53,10.91;13,112.28,597.15,393.70,10.91;13,112.66,610.69,394.52,10.91;13,112.66,624.24,289.53,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,274.35,583.60,156.85,10.91">PageRank&quot; for Argument Relevance</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/E17-1105" />
	</analytic>
	<monogr>
		<title level="m" coord="13,234.24,597.15,271.74,10.91;13,112.66,610.69,193.33,10.91">15th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2017)</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1116" to="1126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,637.79,395.17,10.91;13,112.66,651.34,395.17,10.91;13,112.66,664.89,394.53,10.91;14,112.30,86.97,395.53,10.91;14,112.66,100.52,393.73,10.91;14,112.34,114.06,48.46,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,361.25,651.34,146.58,10.91;13,112.66,664.89,63.77,10.91">Overview of Touché 2022: Argument Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gurcke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,180.11,86.97,327.72,10.91;14,112.66,100.52,81.28,10.91">Advances in Information Retrieval. 44th European Conference on IR Research (ECIR 2022)</title>
		<title level="s" coord="14,200.97,100.52,154.23,10.91">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Hagen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Verberne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Seifert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Balog</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Nørvåg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Setty</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,127.61,394.53,10.91;14,112.30,141.16,394.98,10.91;14,112.66,154.71,394.53,10.91;14,112.66,168.26,393.33,10.91;14,112.66,181.81,395.01,10.91;14,112.66,195.36,147.65,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,264.52,141.16,221.57,10.91">Building an Argument Search Engine for the Web</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Al-Khatib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Puschmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dorsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Morari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Bevendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W17-5106" />
	</analytic>
	<monogr>
		<title level="m" coord="14,243.48,168.26,262.50,10.91;14,112.66,181.81,229.05,10.91">4th Workshop on Argument Mining (ArgMining 2017) at EMNLP, Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Ashley</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Green</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Habernal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Litman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Petasis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Reed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Slonim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Walker</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="49" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,208.91,394.53,10.91;14,112.28,222.46,141.20,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,253.17,208.91,248.78,10.91">Laying the foundations for a world wide argument web</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Rahwan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Zablith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,112.28,222.46,52.19,10.91">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="page" from="897" to="921" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,236.01,393.33,10.91;14,112.66,249.56,394.52,10.91;14,112.66,263.11,274.36,10.91" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,248.86,236.01,70.03,10.91">Retrieval Models</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Anderka</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4614-7163-9_117-1</idno>
	</analytic>
	<monogr>
		<title level="m" coord="14,447.79,236.01,58.20,10.91;14,112.66,249.56,216.25,10.91">Encyclopedia of Social Network Analysis and Mining (ESNAM)</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Alhajj</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Rokne</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,276.66,394.53,10.91;14,112.66,290.20,393.33,10.91;14,112.28,303.75,394.91,10.91;14,112.28,317.30,397.86,10.91;14,112.66,333.29,43.94,7.90" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,166.11,290.20,228.32,10.91">Argument Search: Assessing Argument Relevance</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Euchner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Heilenkötter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Weidmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331327</idno>
		<ptr target="http://doi.acm.org/10.1145/3331184.3331327.doi:10.1145/3331184.3331327" />
	</analytic>
	<monogr>
		<title level="m" coord="14,421.40,290.20,84.59,10.91;14,112.28,303.75,390.50,10.91">42nd International ACM Conference on Research and Development in Information Retrieval (SIGIR 2019)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,344.40,380.98,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,159.18,344.40,261.34,10.91">Frequentist and bayesian approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,443.05,344.40,20.13,10.91">ECIR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,357.95,393.33,10.91;14,112.66,371.50,393.33,10.91;14,112.66,385.05,394.53,10.91;14,112.28,398.60,395.00,10.91;14,112.31,412.15,290.29,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="14,210.88,357.95,295.11,10.91;14,112.66,371.50,128.68,10.91">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<idno type="DOI">10.1145/383952.384019</idno>
		<ptr target="https://doi.org/10.1145/383952.384019.doi:10.1145/383952.384019" />
	</analytic>
	<monogr>
		<title level="m" coord="14,269.12,371.50,236.87,10.91;14,112.66,385.05,390.58,10.91">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;01</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;01<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,425.70,394.61,10.91;14,112.33,439.25,53.86,10.91" xml:id="b9">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<title level="m" coord="14,423.89,425.70,64.18,10.91">Okapi at trec-3</title>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,452.79,393.33,10.91;14,112.66,466.34,395.01,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="14,194.86,452.79,311.13,10.91;14,112.66,466.34,35.10,10.91">A statistical interpretation of term specificity and its application in retrieval</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Spärck</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.1108/eb026526</idno>
		<ptr target="https://doi.org/10.1108/eb026526" />
	</analytic>
	<monogr>
		<title level="j" coord="14,154.60,466.34,112.90,10.91">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="11" to="21" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,479.89,393.33,10.91;14,112.66,493.44,395.01,10.91;14,112.66,506.99,284.90,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="14,311.38,479.89,194.60,10.91;14,112.66,493.44,73.05,10.91">The PageRank Citation Ranking: Bringing Order to the Web</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<idno>1999-66</idno>
		<ptr target="http://ilpubs.stanford.edu:8090/422/,previousnumber=SIDL-WP-1999-0120" />
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>Stanford InfoLab</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="14,112.66,520.54,393.33,10.91;14,112.66,534.09,394.03,10.91;14,112.66,547.64,115.19,10.91" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="14,199.38,520.54,306.60,10.91;14,112.66,534.09,24.17,10.91">Logical Self-defense, Key titles in rhetoric, argumentation, and debate series</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Blair</surname></persName>
		</author>
		<ptr target="https://books.google.de/books?id=ojNbr4vYooQC" />
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>International Debate Education Association</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,561.19,393.60,10.91;14,112.66,574.74,393.54,10.91;14,112.66,588.29,392.35,10.91" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="14,190.36,561.19,315.90,10.91;14,112.66,574.74,241.90,10.91">Reasonableness and Effectiveness in Argumentative Discourse: Fifty Contributions to the Development of Pragma-Dialectics</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Van Eemeren</surname></persName>
		</author>
		<ptr target="https://books.google.de/books?id=b1h1CgAAQBAJ" />
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,601.84,394.53,10.91;14,112.66,615.39,394.53,10.91;14,112.28,628.93,393.70,10.91;14,112.66,642.48,394.04,10.91;14,112.66,656.03,91.79,10.91" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="14,112.66,615.39,318.75,10.91">Computational Argumentation Quality Assessment in Natural Language</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Naderi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Bilu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">A</forename><surname>Thijm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/E17-1017" />
	</analytic>
	<monogr>
		<title level="m" coord="14,234.24,628.93,271.74,10.91;14,112.66,642.48,194.48,10.91">15th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2017)</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="176" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,112.66,669.58,393.33,10.91;15,112.66,86.97,393.33,10.91;15,112.66,100.52,394.62,10.91;15,112.66,114.06,386.47,10.91" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="14,315.65,669.58,190.33,10.91;15,112.66,86.97,28.28,10.91">Efficient pairwise annotation of argument quality</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.511</idno>
		<ptr target="https://aclanthology.org/2020.acl-main.511.doi:10.18653/v1/2020.acl-main.511" />
	</analytic>
	<monogr>
		<title level="m" coord="15,163.32,86.97,342.67,10.91;15,112.66,100.52,238.14,10.91">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5772" to="5781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,127.61,393.53,10.91;15,112.66,141.16,394.03,10.91;15,112.66,154.71,52.20,10.91" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="15,269.29,127.61,236.90,10.91;15,112.66,141.16,84.42,10.91">Exploring bert synonyms and quality prediction for argument retrieval</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Moroldo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Valente</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/#paper-213" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2374" to="2388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,168.26,393.33,10.91;15,112.66,181.81,311.37,10.91" xml:id="b17">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m" coord="15,326.58,168.26,179.40,10.91;15,112.66,181.81,181.08,10.91">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,195.36,394.53,10.91;15,112.28,208.91,393.70,10.91;15,112.66,222.46,365.41,10.91" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="15,112.28,208.91,357.38,10.91">A large-scale dataset for argument quality ranking: Construction and analysis</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gretz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Cohen-Karlik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Toledo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lahav</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Aharonov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1911.11408" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,236.01,393.53,10.91;15,112.28,249.56,393.71,10.91;15,112.66,263.11,393.73,10.91;15,112.34,276.66,286.03,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="15,415.08,236.01,91.11,10.91;15,112.28,249.56,167.13,10.91">Data Acquisition for Argument Search: The args.me corpus</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kiesel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-30179-8_4</idno>
	</analytic>
	<monogr>
		<title level="m" coord="15,112.66,263.11,241.68,10.91">German Conference on Artificial Intelligence (KI 2019)</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Benzmüller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Stuckenschmidt</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="48" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,290.20,395.01,10.91;15,112.66,303.75,256.15,10.91" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="15,167.88,290.20,254.16,10.91">Quality-aware argument retrieval with topical clustering</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2936/#paper-212" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2366" to="2373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,317.30,395.16,10.91;15,112.66,330.85,255.10,10.91" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<title level="m" coord="15,406.53,317.30,101.29,10.91;15,112.66,330.85,223.18,10.91">Terrier: A high performance and scalable information retrieval platform</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,344.40,394.52,10.91;15,112.66,357.95,393.33,10.91;15,112.66,371.50,394.52,10.91;15,112.66,385.05,393.33,10.91;15,112.66,398.60,393.33,10.91;15,112.41,412.15,395.26,10.91;15,112.41,425.70,397.73,10.91;15,112.36,441.69,158.69,7.90" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="15,336.55,357.95,169.43,10.91;15,112.66,371.50,38.69,10.91">Overview of Touché 2021: Argument Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gienapp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fröbe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beloucif</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ajjour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-85251-1_28</idno>
		<ptr target="https://link.springer.com/chapter/10.1007/978-3-030-85251-1_28.doi:10.1007/978-3-030-85251-1\_28" />
	</analytic>
	<monogr>
		<title level="m" coord="15,238.75,385.05,267.23,10.91;15,112.66,398.60,328.83,10.91">Experimental IR Meets Multilinguality, Multimodality, and Interaction. 12th International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="15,150.93,413.16,143.04,9.72">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Candan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="volume">12880</biblScope>
			<biblScope unit="page" from="450" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,452.79,395.17,10.91;15,112.66,466.34,48.32,10.91" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="15,158.62,452.79,349.20,10.91;15,112.66,466.34,17.36,10.91">Probability models for information retrieval based on divergence from randomness</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,479.89,394.52,10.91;15,112.66,493.44,116.84,10.91" xml:id="b24">
	<monogr>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Datta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1303.0667</idno>
		<title level="m" coord="15,229.22,479.89,273.67,10.91">Query expansion using term distribution and term association</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,506.99,394.53,10.91;15,112.66,520.54,22.69,10.91" xml:id="b25">
	<monogr>
		<title level="m" type="main" coord="15,202.24,506.99,300.27,10.91">Bose einstein 1 and bose einstein 2 model for optimal query expansion</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,534.09,393.33,10.91;15,112.66,547.64,393.33,10.91;15,112.28,561.19,393.71,10.91;15,111.79,574.74,395.49,10.91;15,112.66,588.29,315.54,10.91" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="15,238.10,534.09,267.89,10.91;15,112.66,547.64,166.84,10.91">The use of mmr, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="DOI">10.1145/290941.291025</idno>
		<ptr target="https://doi.org/10.1145/290941.291025.doi:10.1145/290941.291025" />
	</analytic>
	<monogr>
		<title level="m" coord="15,303.36,547.64,202.62,10.91;15,112.28,561.19,393.71,10.91;15,111.79,574.74,11.83,10.91">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;98</title>
		<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;98<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,601.84,393.53,10.91;15,112.66,615.39,235.57,10.91" xml:id="b27">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.09297</idno>
		<title level="m" coord="15,293.60,601.84,212.59,10.91;15,112.66,615.39,105.29,10.91">Mpnet: Masked and permuted pre-training for language understanding</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,112.66,628.93,393.54,10.91;15,112.33,642.48,393.65,10.91;15,112.66,656.03,393.33,10.91;15,112.66,669.58,392.95,10.91" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="15,399.30,628.93,106.90,10.91;15,112.33,642.48,214.25,10.91">On Classifying whether Two Texts are on the Same Side of an Argument</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Körner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wiedemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">D</forename><surname>Hakimi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Heyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.emnlp-main.795/" />
	</analytic>
	<monogr>
		<title level="m" coord="15,350.66,642.48,155.32,10.91;15,112.66,656.03,393.33,10.91;15,112.66,669.58,46.58,10.91">The 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP 2021), Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10130" to="10138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,112.66,86.97,393.33,10.91;16,112.66,100.52,393.32,10.91;16,112.66,114.06,367.93,10.91" xml:id="b29">
	<analytic>
		<ptr target="http://ceur-ws.org/Vol-2936/" />
	</analytic>
	<monogr>
		<title level="m" coord="16,355.76,86.97,150.22,10.91;16,112.66,100.52,328.23,10.91">Proceedings of the Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum (CLEF 2021)</title>
		<title level="s" coord="16,124.26,114.06,129.93,10.91">CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Maistro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Piroi</surname></persName>
		</editor>
		<meeting>the Working Notes of CLEF 2021 -Conference and Labs of the Evaluation Forum (CLEF 2021)<address><addrLine>Aachen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2936. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
