<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,397.18,15.42;1,89.29,106.66,207.62,15.42;1,296.91,103.62,5.85,10.48">Exploring Transformers for Multilingual Historical Named Entity Recognition ⋆</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,88.87,134.97,54.12,11.96"><forename type="first">Anja</forename><surname>Ryser</surname></persName>
							<email>anja.ryser@uzh.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<addrLine>Rämistrasse 21</addrLine>
									<postCode>8006</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,161.16,134.97,99.58,11.96"><forename type="first">Quynh</forename><forename type="middle">Anh</forename><surname>Nguyen</surname></persName>
							<email>quynhanh.nguyen@uzh.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<addrLine>Rämistrasse 21</addrLine>
									<postCode>8006</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Milan</orgName>
								<address>
									<addrLine>Via Festa del Perdono, 7</addrLine>
									<postCode>20122</postCode>
									<settlement>Milano</settlement>
									<region>MI</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,284.18,134.97,91.86,11.96"><forename type="first">Niclas</forename><surname>Bodenmann</surname></persName>
							<email>niclaslinus.bodenmann@uzh.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<addrLine>Rämistrasse 21</addrLine>
									<postCode>8006</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,412.55,134.97,73.56,11.96"><forename type="first">Shih-Yun</forename><surname>Chen</surname></persName>
							<email>shih-yun.chen@uzh.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<addrLine>Rämistrasse 21</addrLine>
									<postCode>8006</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Zurich University of Applied Sciences</orgName>
								<address>
									<addrLine>Gertrudstrasse 15</addrLine>
									<postCode>8401</postCode>
									<settlement>Winterthur</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,397.18,15.42;1,89.29,106.66,207.62,15.42;1,296.91,103.62,5.85,10.48">Exploring Transformers for Multilingual Historical Named Entity Recognition ⋆</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">890C8644593D16D14B4A502CFB2C5E7F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Named Entity Recognition</term>
					<term>Historical Newspaper</term>
					<term>HIPE2022</term>
					<term>Transfer Learning</term>
					<term>Transformers</term>
					<term>Multilingual Models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper explores the performance of out-of-the-box transformers language models for historical Named Entity Recognition (NER). Within the HIPE2022 (Identifying Historical People, Places, and other Entities) shared task, we participated in the NER-COARSE task of the Multilingual Newspaper Challenge (MNC). Three main approaches are experimented with: ensembling techniques on multiple fine-tuned models, using multilingual pretrained models, and relabeling the entity tags from the IOB-segmentation to a simplified version. By ensembling predictions from different system outputs, we outperformed the baseline model in the majority of cases. Moreover, through post-submission experiments, we found that using multilingual models did not yield better results compared to monolingual models. Furthermore, the relabeling experiment on the Newseye French dataset showed that merging entity labels and inferring the IOB segmentation in postprocessing increases precision but lowers recall. Last but not least, soft-label ensembling experiments on the same dataset enhanced precision, recall and thus F1-scores compared to hard-label ensembling by at least one percentage point.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Named Entity Recognition (NER) on historical newspaper text is a task with many pitfalls. Differences in language, its use, and the world it refers to, as well as technical artifacts, make models that perform well in contemporary texts significantly worse in historical texts. With our contribution to the HIPE2022 Shared Task, we explore the performance of transformersarchitectures pretrained on historical and contemporary data available via HuggingFace <ref type="bibr" coords="1,492.22,514.48,11.58,10.91" target="#b0">[1]</ref>. We combine these models with task-specific knowledge in pre-and postprocessing, and in post-submission experiments, we further investigate the performance of only predicting on categories (without using IOB encoding), soft-label ensembling, and multilingual language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Transformers <ref type="bibr" coords="2,150.38,111.28,12.69,10.91" target="#b1">[2]</ref> has rapidly become the dominant architecture for natural language processing, surpassing alternative neural models such as convolutional and recurrent neural networks in performance for tasks in both natural language understanding and natural language generation <ref type="bibr" coords="2,89.29,151.93,11.46,10.91" target="#b2">[3]</ref>. The Transformers architecture is particularly conducive to pretrain on large text corpora, leading to major gains in accuracy on downstream tasks <ref type="bibr" coords="2,330.70,165.48,12.37,10.91" target="#b2">[3]</ref>. As a result, the release of pretrained contextualised word embeddings such as BERT <ref type="bibr" coords="2,300.19,179.03,12.77,10.91" target="#b3">[4]</ref> pushed further the upper bound of modern NER performances <ref type="bibr" coords="2,175.78,192.57,12.84,10.91" target="#b4">[5]</ref> and established state-of-the-art results for modern NER <ref type="bibr" coords="2,440.95,192.57,12.84,10.91" target="#b4">[5]</ref>  <ref type="bibr" coords="2,456.51,192.57,11.43,10.91" target="#b5">[6]</ref>.</p><p>In the HIPE2020 Shared Task, several top solutions were developed based on pretrained language model embeddings with transformers-based architectures. Ghannay et al. <ref type="bibr" coords="2,436.93,219.67,12.68,10.91" target="#b6">[7]</ref> achieved the second-best result for French with an 81% F1-score in the strict scenario by using CamemBERT <ref type="bibr" coords="2,89.29,246.77,11.28,10.91" target="#b7">[8]</ref>, a multi-layer bidirectional transformer similar to RoBERTa <ref type="bibr" coords="2,364.21,246.77,11.28,10.91" target="#b8">[9]</ref>, together with a CRF decoder. Todorov et al. <ref type="bibr" coords="2,151.87,260.32,17.75,10.91" target="#b9">[10]</ref> implemented an architecture made of a modular embedding layer which was combined by newly trained and pre-trained embeddings, and a task-specific Bi-LSTM-CRF layer to handle NERC on coarse and fine-grained tags. They conclude that character-level embeddings, BERT, and a document-level data split are the most important factors in improving NER results. Besides, the experiment also shows that pretrained language models can be beneficial for NERC on low-resource historical corpora. Provatorova et al. <ref type="bibr" coords="2,338.34,328.07,18.07,10.91" target="#b10">[11]</ref> fine-tuned two pretrained BERT models <ref type="bibr" coords="2,123.79,341.62,16.25,10.91" target="#b11">[12]</ref>, including bert-base-cased for English and bert-base-multilingual-cased for French and German. In order to enhance the robustness of the approach, a majority voting ensemble of 5 fine-tuned model instances was implemented per language. Their models achieved F1-scores of 68%, 52% and 47% for French, German and English respectively. Section 4.2 describes how we employed multiple fine-tuned models, exploited ensembling techniques, and applied relabeling entities method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task and Datasets</head><p>We worked on coarse NER in digitized historical newspapers across different label sets and languages. More detailed information on this task can be found on the HIPE2022 website.</p><p>NER on historical newspapers poses its own unique challenges; non-standard language with old lexicon and syntax, errors from digitization such as errors in layout and optical character recognition (OCR) and the lack of resources for training make this task challenging <ref type="bibr" coords="2,462.59,522.18,11.43,10.91" target="#b4">[5]</ref>.</p><p>We used a part of the data provided by the organizers of this task, namely 5 datasets of historical newspapers in English, German, French, Swedish and Finnish spanning from the 18th to the 20th century. The data contains newspapers digitized through different European cultural heritage projects. While most of the data were published before HIPE2022, some unpublished parts of the datasets were used as test-sets. Each dataset is annotated following different annotation guidelines and contains NER-tags and NEL-links to Wikidata. All datasets were provided in the HIPE-format <ref type="bibr" coords="2,238.32,617.03,16.09,10.91" target="#b12">[13]</ref>. Table <ref type="table" coords="2,287.27,617.03,4.97,10.91" target="#tab_0">1</ref> presents an overview of the historical newspaper datasets of HIPE2022 used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Resources</head><p>We train our models on Google Colab with GPU enabled. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Preprocessing</head><p>We use a simple approach to preprocess the data. Lines with erroneous characters, empty lines, and lines containing metadata were removed while reading the tabulator-separated values (TSV) files. 'Nan'-values were filled with empty strings to keep the data structure intact.</p><p>Tokens are split into sentences using the EndOfSentence-tag provided in the data. The data is tokenized using the corresponding transformers model's tokenizer without any additional fine-tuning on our data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training</head><p>Models We employed a variety of models and pretrained weights for all different datasets. We distinguish between models that have been pretrained on historical data (historical language models, HLM) and on contemporary data. The HLMs mainly come from a single source, the Bavarian State Library <ref type="bibr" coords="3,191.30,426.33,16.17,10.91" target="#b13">[14]</ref>. We expect that the HLMs have already learned to deal with errors stemming from OCR, as these errors are prevalent in most historical datasets.</p><p>We mainly use BERT-based models <ref type="bibr" coords="3,266.49,453.43,16.56,10.91" target="#b11">[12,</ref><ref type="bibr" coords="3,286.61,453.43,14.11,10.91" target="#b14">15]</ref> but also experimented with XLNet <ref type="bibr" coords="3,467.61,453.43,18.06,10.91" target="#b15">[16]</ref> and ELECTRA <ref type="bibr" coords="3,136.44,466.98,16.08,10.91" target="#b16">[17]</ref>. See Table <ref type="table" coords="3,203.29,466.98,4.97,10.91">7</ref> for a full description of which pretrained models have been used. In the submitted run 1, all models listed were used for the ensembling. In run 2, the results of the single best model were submitted (marked in bold in the table). The models are instantiated with standard token classification heads from HuggingFace. <ref type="bibr" coords="3,358.43,507.63,42.16,10.91">[18, p. 98]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Parameters</head><p>We fine-tune the models with the parameters coming from the pretrained models; were not these set, the default values of HuggingFace TrainingArguments have been used. All sentences were padded or truncated to a maximum length of 100 tokens. Because of the number of models we set out to deploy, we do not run a hyperparameter search. An initial experiment with label weights did not improve per-performance, and we returned to the defaults. We trained all models for three epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation metrics</head><p>The evaluation metrics used for NER tasks in HIPE2022 are Precision, Recall, and F1 score on macro and micro levels. The same evaluation metrics are used to assess our systems. F1-macro scores are computed on the document level and F1-micro scores on the entity-type level. Precisely, macro measures the average of the corresponding micro scores across all the documents, accounting for variance in document length but not for class imbalances.</p><p>Additionally, the model's performance was also measured in strict and fuzzy. In the strict scenario, a mention was only counted as correct when the exact gold-standard boundaries were met, whereas in the fuzzy evaluation, only a part of the mention needed to be recognized correctly. Because of this, in the strict measurement, predicting wrong boundaries leads to severe punishment, i.e., a mention is recognized, but one boundary is set wrong, leading to the whole entity being counted as False <ref type="bibr" coords="4,250.79,195.36,16.25,10.91" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Inference</head><p>Single Models The single models we employ are initialized with a token classification head provided by HuggingFace. This is a linear mapping from the last encoder state to the output layer, where for every token, there are as many logits as labels in the dataset.</p><p>Because the gold labels are on whole words, while the models operate on subwords, we need a non-trivial mapping regime. In preprocessing, the label of the whole word is propagated down to all subwords. All subword logits belonging to a single word are summed up during inference, and the label with the highest score is chosen for the whole word. Ács et al. <ref type="bibr" coords="4,446.49,326.38,18.27,10.91" target="#b18">[19]</ref> evaluate different pooling strategies for subword aggregation. While they tend towards neural solutions such as an additional LSTM over the subword logits, they mention how the pooling strategy has a lower influence on NER (as opposed to morphological tasks such as POS-Tagging). Still, more advanced subword pooling strategies remain to be explored.</p><p>Ensembling On one dataset, predictions of all models were gathered, and the final label was chosen through a hard-ensembling method. The final prediction was the label with the most votes. In a tie between different labels, entity labels were favored, and between different entity labels, the choice was randomized.</p><p>Postprocessing We employed only one postprocessing rule for the shared task submission: If a token gets a label prediction starting with an I (inside) but is not preceded by an I or a B (beginning), it is changed to a B. Erroneously, we did not consider the label class. This was remedied in the post-submission experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Post-Submission Experiments</head><p>This section introduces the three approaches we experimented with after the submission. We focused on Newseye French for the monolingual and all Newseye languages for multilingual approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivation</head><p>We tried to improve the submitted results. For better comparability of the different approaches of the post-submission experiments and due to time constraints, we decided to focus on one dataset and one language for the monolingual experiments. We saw the most potential for improvement in the Newseye French dataset. Therefore, we used all available languages in the Newseye dataset for the multilingual approach. Our goal was to beat the baseline provided by the task organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Soft-Label Ensembling</head><p>For the submission, we employed hard-label ensembling ('voting'). In this post-submission experiment, we evaluate the performance of soft-label ensembling on the Newseye French dataset. To infer the final label for a token, we average the probabilities (softmax logits) for the whole tokens of the individual models.</p><p>We follow Ju et al. <ref type="bibr" coords="5,190.63,217.99,16.41,10.91" target="#b18">[19]</ref>, who argue for averaging the softmaxed logits because different models' logits might differ in magnitude. This is expected in our case, as the models use their own subword tokenization and, therefore, might sum over a different amount of subwords for the logits of the whole word.</p><p>The same models are used for the submission, presented in Table <ref type="table" coords="5,392.12,272.19,3.74,10.91">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Multilingual Models</head><p>As shown in previous work, the performance of NLP-tasks can benefit from leveraging crosslingual transfer learning and using multilingual models, which leads to more training data for a single model <ref type="bibr" coords="5,148.55,349.01,16.25,10.91" target="#b19">[20]</ref>.</p><p>To test this, we used the same methods as described in chapter 4 with different multilingual BERT models (for further details, see Table <ref type="table" coords="5,274.06,376.11,4.08,10.91">7</ref>) and Newseye data in all four available languages. In addition, we tested the single best model and the hard-label ensemble as described in paragraph 'Ensembling' in section 4. <ref type="bibr" coords="5,202.80,403.21,3.74,10.91" target="#b3">4</ref>.</p><p>The first trained model received the input sorted after language, which we assume could lead to catastrophic forgetting of the languages first seen. To avoid this, the sentences were shuffled before being fed in batches to the model during fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Relabeling</head><p>Through the error analysis in section 7.2, we noticed that one of the most occurring errors is Right classification and wrong segmentation, i.e., the model predicts I-LOC while the ground truth is B-LOC. We assume this is because the models are fine-tuned on the whole label set where B-tags and I-tags are handled as two different labels.</p><p>We chose the Newseye dataset in French to train the model used for this approach. All nine classes of the dataset, <ref type="figure" coords="5,187.85,562.34,319.33,9.72;5,88.12,575.89,62.60,9.72">['O', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-PER', 'I-PER', 'B-HumanProd',  'I-HumanProd'</ref>] are relabeled into five entity labels which are ['O', 'ORG', 'LOC', 'PER', 'Human-Prod']. After that, the text and corresponding new label of each word are used as the input of the training pipeline. The pipeline results in predictions with new labels that were reencoded. The IOB-tagging is reconstructed in the postprocessing. Table <ref type="table" coords="5,375.60,615.52,10.35,10.91" target="#tab_0">13</ref> shows more details of the results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>To keep the results section concise, we focused on the micro-strict F1 score. From all measurements provided by the task organizers, micro-strict is the most punishing, resulting in lower scores. For more detailed results, see Tables 8 to 13 in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Submission</head><p>Table <ref type="table" coords="6,116.55,441.91,5.17,10.91" target="#tab_1">2</ref> and 3 show the F1-score over all labels for both submitted systems. 'avg' shows the average overall languages in each dataset. The best run for each language and averaged over all available languages between the two runs are marked in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Post-Submission Experiments</head><p>Soft-Label Ensembling All scores benefit from switching from hard-label to soft-label ensembling by at least one percentage point from an average F1 score of 0.7 to 0.8 (see Table <ref type="table" coords="6,497.11,532.28,3.52,10.91" target="#tab_2">4</ref>). However, our best run on the test set from the submission was not the (hard-label) ensembled model but the single model with the best scores on the validation set. With regards to Micro-F1 strict and fuzzy, the soft-label ensembled model is on par with the best individual model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multilingual models</head><p>For this section, the test sets of the different languages were labeled and evaluated separately. The results shown in Table <ref type="table" coords="6,327.48,615.24,5.07,10.91" target="#tab_3">5</ref> are then averaged over all languages. 'submission ensembling' and 'submission best model' contain the results we handed in for submission, trained and ensembled monolingually and averaged over all languages. 'best model multilingual' is the best out of the five models used for ensembling. 'ensemble multilingual' is a multilingually trained system with five different results which were then run through the ensembling process. Results of the two runs of our submission show that ensembling improved the performance over all languages and performed better than the single models. In the multilingual experiments, the best model performs slightly better than the ensembling and beats the monolingual best model. Overall, the monolingual ensembling yielded the best results. We assume the multilingual ensembling results could be improved by excluding or replacing the worst performing model used in ensembling. Table <ref type="table" coords="7,207.67,452.74,10.15,10.91" target="#tab_9">12</ref> in the Appendix shows more detailed results.</p><p>Relabeling Table <ref type="table" coords="7,178.51,481.49,9.94,10.91" target="#tab_0">13</ref> shows the comparison between applying and not applying the relabeling method. The relabeling approach generally improves precision scores by around 1 to 2 percentage points.</p><p>The result shows notable changes in models performances regarding precision and recall scores. While Precision improves, recall scores slightly decrease compared to the performance of model without relabeling. As a consequence, F1-scores remain similar in both conditions.</p><p>Besides, the relabeling approach has also contributed to the marginal enhancement of model 4, i.e., the pretrained and fine-tuned French Europeana ELECTRA model. What stands out in Table <ref type="table" coords="7,115.70,589.89,10.10,10.91" target="#tab_0">13</ref> is the difference between model 4 using the relabeling method and model 4 not using relabeling method. All considered metrics uniformly rise around 0.3-3% with relabeling. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>Table <ref type="table" coords="8,116.46,236.36,5.17,10.91" target="#tab_1">2</ref> and 3 show that the systems fine-tuned on the German Newseye and Sonar corpora perform worse than those fine-tuned on the German HIPE2020 dataset. This could be because the used datasets differ significantly in size; Sonar and News-eye are much smaller than HIPE2020, so the lousy performance could come from overfitting. We also looked at each dataset's bestand worst-performing label and reported them with their F1-score in Tables <ref type="table" coords="8,429.84,290.56,27.84,10.91" target="#tab_8">8 to 11</ref>.</p><p>The evaluation metrics (Tables <ref type="table" coords="8,241.87,304.11,28.83,10.91" target="#tab_8">8 to 11</ref>) reveal that both the best model and the ensembled models better recognize the LOC, PER, and TIME labels across all datasets and measurements. While these labels dominate the corresponding datasets, hard-label ensembling ('voting') reflects the preference to reassign the label Os to these tokens. In contrast, the ORG and other minor category labels are generally worse handled, as a system is less likely to predict them, and in many cases, voting overrode these labels in favor of a more frequent predicted label. For example, the gold standard for a label is ORG. One model predicts the correct, infrequent label, the other 3 predict the more likely O. Due to majority voting, the correct guess is overruled and the incorrect label is chosen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Comparison of Contemporary and Historical BERT</head><p>Table <ref type="table" coords="8,117.10,462.23,5.17,10.91" target="#tab_4">6</ref> demonstrates the Micro-F1 results for HIPE2020 German between the BERT-based model trained on historical data and the BERT-based model trained on contemporary data. The HIPE2020 data are from historical newspapers between the 19th and 20th centuries, as do the training data of the BERT-based HLM. As a result, the BERT-based model trained on historical data outperforms the BERT-based model trained on contemporary data on the overall result (column ALL in Table <ref type="table" coords="8,215.75,529.97,3.55,10.91" target="#tab_4">6</ref>). The outcome is as expected because the pre-trained data cover historical data requirements in the historical NER task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Error Analysis</head><p>To clarify the errors in our models, we conduct error analysis on the models trained on the Newseye dataset. We compare the NER results of the hard-label ensembling models on the Newseye French test-set (as the best-performing model) and the Newseye German test-set (as the worst-performing model) to the gold standard data. The five major errors discovered are as follows:</p><p>•    The most common errors appear to follow a pattern. They usually occur at the incorrect entity recognition of the beginning token. To wrong segmentation errors, our model occasionally labels just the entities of the names without their titles because it fails to recognize the beginning tokens of location or individual titles such as 'café' or 'v' '.'. Subword tokenization also raises segmentation errors particularly in the French NER task. The model typically performs NER and labels the tokens without the negation symbol '¬'.</p><p>With an incorrectly labeled entity classification to the beginning token, the remaining sequence of tokens follows the incorrect classification. We see this error randomly happen in sentences. Our model tends to start the NE with a non-location or non-personal noun, a punctuation, or an article. After assigning the beginning token, the following tokens will adopt the same incorrect classification. The following examples explain the two most common errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Right classification, wrong segmentation</head><p>The French gold standard data label 'café' 'Mollard' as 'B-LOC' 'I-LOC', whereas our model labels 'café' 'Mollard' as 'O' 'I-LOC'. Without labeling 'café', our model assigns 'Mollard' as a beginning token.</p><p>In the German gold standard data, the entities of the German family name 'v' '. ' 'Plener' are 'B-PER' 'I-PER' 'I-PER'. They exist several times in the dataset, but not all have been correctly recognized. Although our model adequately recognized the first two occurrences of the three tokens, it does not consistently learn the entity pattern. Thus, there are also errors such as just 'Plener' as 'B-PER' or 'v' as 'B-PER'.</p><p>Subword tokenization, for instance, 'Rau¬' 'court', is challenging for our model to perform exactly NER. Our model predicts the entities as 'O' 'B-LOC' instead of 'B-LOC' 'I-LOC' as in the French gold standard data. Other examples include 'Ro¬' 'mans' (gold NER as 'I-PER' 'I-PER'), 'Pierre¬' 'Vaast' (gold NER as 'I-LOC' 'I-LOC') or 'AI¬' 'bert' (gold NER as 'I-PER' 'I-PER') which our model does not recognize all tokens containing '¬' and assigns 'O' to them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wrong classification, wrong segmentation</head><p>With an inanimate French noun like 'matinée' ('morning' in French), our model recognizes its entity classification as 'B-PER', which should not be labelled. The incorrect classification leads the following tokens '. ' '-' 'Le' 'président' 'du' 'Conseil' 'a' 'reçu' ('. -the president of the council has received' in French) all in 'I-PER' where only 'Conseil' should be recognized as 'B-ORG'.</p><p>Our model recognizes all possible tokens including punctuation to perform NER. For instance, only 'professeur' 'Vaquez' have entities of 'B-PER' 'I-PER' from the token sequence: '", "' 'nièce' 'du' 'professeur' 'Vaquez' '. ' (', niece of professor Vaquez. ' in French). However, our NER model begins with the entity 'B-LOC' by '", "' and the following tokens are 'I-LOC'.</p><p>Based on these identified errors, we are encouraged to propose relabeling post-submission experiments to improve the NERC task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Post-Submission Experiments</head><p>Soft-Labeling With improvements across all scores, soft-label ensembling is preferable to hard-label ensembling. It seems that the additional information embedded in soft-labeling benefits the system. However, to be able to make more profound statements extending to other datasets, more experiments would be needed.</p><p>Multilingual Models Multilingual approaches did not improve the performance of our system. However, their performance is comparable to our monolingual approaches. The best multilingually trained model performed better than the average best single monolingual model.</p><p>However, it seems that the performance of multilingual ensemble predictions could still be improved through a better selection of multilingual models or leveraging newer models such as XLM-R <ref type="bibr" coords="11,137.38,127.61,16.42,10.91" target="#b19">[20]</ref>. In addition, it is striking that all models performed poorly in German; more analyses should be done to investigate further reasons for this and improve the system.</p><p>Relabeling Relabeling improves Precision scores and deteriorates Recall scores. This means that our systems tend to return very few but precise NE predictions. Relabeling would be suited best for scenarios where precision is more important than recall, and False positives should be avoided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Future Work</head><p>The existing system's performance could be optimized by using early stopping instead of training for a fixed number of epochs and applying grid-search on hyper-parameters.</p><p>We investigated the influence of frozen and unfrozen embeddings anecdotally, revealing that employing frozen word embeddings for NER tasks slightly improved results. However, due to time constraints, we could not implement our system with frozen embeddings, which would assumedly improve the overall results, especially for the relatively small training sets we used.</p><p>Because of the workflow in our experiment, each dataset and languages use different pretrained models. Using the same models for one language across datasets and using multilingual models across languages would make the system more uniform and easier to improve as a whole.</p><p>Our ensembling approach could benefit from using a more careful selection of the single models, and replacing the worst model would probably improve the overall performance. This could particularly help the approach described in the post-submission experiment on multilingual models, where all models performed poorly in German. More analyses should be done to explain this bad performance and to improve it.</p><p>Experiments with other frameworks, such as AdapterHub <ref type="bibr" coords="11,352.54,472.43,17.76,10.91" target="#b20">[21]</ref> or other different architectures could improve performance. Other newer models, such as RoBERTa <ref type="bibr" coords="11,384.48,485.98,16.09,10.91" target="#b14">[15]</ref>, XLM-R <ref type="bibr" coords="11,440.79,485.98,16.08,10.91" target="#b19">[20]</ref>, or models trained on historical newspapers, could also help to improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusion</head><p>In this paper, we reported on the performance of different language models for Named Entity Recognition (NER) in historical newspapers. One of the main challenges in this domain is digitization artifacts, a problem we address by fine-tuning models which have already been pretrained on noisy historical data. Furthermore, we experiment with ensembling, multilingual models, and label simplification.</p><p>In a case study for all languages of the HIPE-CLEF 2022 Newseye dataset, we found that models that have been trained over all languages did not improve the scores compared to monolingual models. In a second case study for the Newseye French dataset, we found that solely predicting entity categories and inferring the IOB encoding in postprocessing did not help to improve F1-measures but shifted the scores to higher precision and a lower recall. On the same dataset, soft-label ensembling substantially improved all scores compared to hard-label ensembling.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="9,116.56,86.97,299.07,10.91;9,107.28,101.87,313.34,10.91;9,107.28,116.77,398.70,10.91;9,116.56,130.32,242.83,10.91;9,107.28,145.23,398.71,10.91;9,116.56,158.78,64.04,10.91;9,107.28,173.68,398.70,10.91;9,116.56,188.24,9.69,9.72"><head></head><label></label><figDesc>Right classification, wrong segmentation: e.g. B-PER v.s. I-PER. • Wrong classification, right segmentation: e.g. B-LOC v.s. B-PER. • Wrong classification, wrong segmentation: The models predicted different NEs compared to the annotated data, e.g., B-LOC v.s. I-PER. • Complete false positive: All tokens of a predicted entity are labeled with O in the gold-standard. • Complete false negative: All Tokens of an entity in the gold-standard are predicted as O.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,100.20,212.74,405.79,10.91;9,89.29,226.28,418.02,10.91;9,88.87,239.83,333.42,10.91;9,203.88,262.57,187.51,141.64"><head>Figure 1</head><label>1</label><figDesc>Figure 1 and 2 visualize the results of the error analysis. Besides the Complete false positive and Complete false negative errors, the two most frequent errors are Right classification, wrong segmentation and Wrong classification, wrong segmentation.</figDesc><graphic coords="9,203.88,262.57,187.51,141.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,89.29,416.77,396.72,8.93"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: NER performance of the hard-label ensembling model on the Newseye French test-set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,89.29,608.11,413.88,8.93;9,203.88,455.58,187.51,139.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: NER performance of the hard-label ensembling model using the Newseye German test-set.</figDesc><graphic coords="9,203.88,455.58,187.51,139.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,88.99,90.49,314.48,105.74"><head>Table 1</head><label>1</label><figDesc>Description of datasets contained in the HIPE2022-data</figDesc><table coords="3,191.80,122.10,211.67,74.12"><row><cell>dataset</cell><cell cols="2">languages comments</cell></row><row><cell>HIPE2020</cell><cell>de, en, fr</cell><cell>19-20C</cell></row><row><cell>Newseye</cell><cell cols="2">de, fi, fr, sv 19-20 C</cell></row><row><cell cols="2">Topres19th en</cell><cell>19C, only location types</cell></row><row><cell>Sonar</cell><cell>de</cell><cell>19-20C</cell></row><row><cell>Letemps</cell><cell>fr</cell><cell>19-20C, unpublished</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,88.99,90.49,333.45,223.57"><head>Table 2</head><label>2</label><figDesc>F1-scores of Micro-strict evaluation of submitted ensembling system (Run 1)</figDesc><table coords="6,88.99,122.10,333.45,191.96"><row><cell>dataset</cell><cell>en</cell><cell>fr</cell><cell>de</cell><cell>sv</cell><cell>fi</cell><cell>avg.</cell></row><row><cell>HIPE2020</cell><cell cols="4">0.513 0.678 0.725 -</cell><cell>-</cell><cell>0.639</cell></row><row><cell>Newseye</cell><cell>-</cell><cell cols="5">0.648 0.395 0.643 0.567 0.563</cell></row><row><cell cols="3">Topres19th 0.787 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.787</cell></row><row><cell>Sonar</cell><cell>-</cell><cell>-</cell><cell cols="2">0.490 -</cell><cell>-</cell><cell>0.49</cell></row><row><cell>Letemps</cell><cell>-</cell><cell cols="2">0.644 -</cell><cell>-</cell><cell>-</cell><cell>0.644</cell></row><row><cell>Table 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">F1-scores of Micro-strict evaluation of submitted best single model (Run 2)</cell><cell></cell></row><row><cell>dataset</cell><cell>en</cell><cell>fr</cell><cell>de</cell><cell>sv</cell><cell>fi</cell><cell>avg.</cell></row><row><cell>HIPE2020</cell><cell>x</cell><cell cols="3">0.696 0.695 -</cell><cell>-</cell><cell>0.696</cell></row><row><cell>Newseye</cell><cell>-</cell><cell cols="5">0.656 0.408 0.636 0.556 0.564</cell></row><row><cell cols="3">Topres19th 0.781 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.781</cell></row><row><cell>Sonar</cell><cell>-</cell><cell>-</cell><cell cols="2">0.477 -</cell><cell>-</cell><cell>0.477</cell></row><row><cell>Letemps</cell><cell>-</cell><cell cols="2">0.622 -</cell><cell>-</cell><cell>-</cell><cell>0.622</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,88.99,90.49,416.99,103.29"><head>Table 4</head><label>4</label><figDesc>Scores for all labels on Newseye French. 'Best Model (Run 2)' are the predictions of the best individual model with the best validation scores. The other two rows are different ensembling strategies over all models.</figDesc><table coords="7,94.20,142.76,406.88,51.02"><row><cell></cell><cell>Micro-P</cell><cell>Micro-R</cell><cell>Micro-F1</cell><cell>Macro-P</cell><cell cols="2">Macro-R</cell><cell>Macro-F1</cell></row><row><cell></cell><cell cols="6">strict fuzzy strict fuzzy strict fuzzy strict fuzzy strict fuzzy strict fuzzy</cell></row><row><cell cols="7">Hard-Label (Run 1) 0.673 0.801 0.625 0.744 0.648 0.772 0.659 0.814 0.614 0.762 0.630 0.779</cell></row><row><cell cols="7">Best Model (Run 2) 0.655 0.785 0.657 0.787 0.656 0.786 0.630 0.775 0.623 0.777 0.621 0.766</cell></row><row><cell>Soft-Label</cell><cell cols="5">0.685 0.818 0.636 0.758 0.659 0.787 0.677 0.829 0.63</cell><cell>0.771 0.649 0.793</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,88.99,262.32,416.99,103.91"><head>Table 5</head><label>5</label><figDesc>Micro-strict scores averaged over all newseye testsets (de, fr, fi, sv) for experiments with multilingual models</figDesc><table coords="7,190.75,303.67,211.28,62.56"><row><cell>System</cell><cell cols="3">Precision Recall F1</cell></row><row><cell cols="2">submission ensembling 0.70</cell><cell>0.65</cell><cell>0.67</cell></row><row><cell>submission best model</cell><cell>0.54</cell><cell>0.52</cell><cell>0.56</cell></row><row><cell cols="2">best model multilingual 0.62</cell><cell>0.55</cell><cell>0.58</cell></row><row><cell>ensemble multilingual</cell><cell>0.63</cell><cell>0.53</cell><cell>0.57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,88.99,90.49,416.99,85.32"><head>Table 6</head><label>6</label><figDesc>Comparison of Micro-F1 for HIPE2020 German between the BERT-base LM trained on historical data and the BERT-base model LM on contemporary data.</figDesc><table coords="8,94.36,133.19,406.55,42.62"><row><cell></cell><cell>ALL</cell><cell>LOC</cell><cell>ORG</cell><cell>PERS</cell><cell>PROD</cell><cell>TIME</cell></row><row><cell></cell><cell cols="6">strict fuzzy strict fuzzy strict fuzzy strict fuzzy strict fuzzy strict fuzzy</cell></row><row><cell>Historical</cell><cell cols="6">0.702 0.805 0.814 0.870 0.441 0.572 0.690 0.841 0.356 0.525 0.596 0.808</cell></row><row><cell cols="7">Contemporary 0.657 0.778 0.773 0.839 0.454 0.535 0.574 0.778 0.418 0.636 0.630 0.804</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="16,52.70,102.98,489.87,256.97"><head>Table 8</head><label>8</label><figDesc>Macro-fuzzy evaluation of submitted systems</figDesc><table coords="16,52.70,133.70,489.87,226.26"><row><cell>System</cell><cell>Dataset</cell><cell>Eval</cell><cell cols="4">Precision all Recall all F1 all-labels Best category</cell><cell>Worst category</cell></row><row><cell cols="2">Best Model HIPE2020_de</cell><cell cols="2">macro-fuzzy 0.789</cell><cell>0.828</cell><cell>0.796</cell><cell>TIME (0.9)</cell><cell>ORG (0.484)</cell></row><row><cell cols="2">Best Model HIPE2020_fr</cell><cell cols="2">macro-fuzzy 0.865</cell><cell>0.831</cell><cell>0.836</cell><cell>TIME (0.942)</cell><cell>ORG (0.587)</cell></row><row><cell cols="2">Best Model Letemps_fr</cell><cell cols="2">macro-fuzzy 0.52</cell><cell>0.712</cell><cell>0.752</cell><cell>PERS (0.845)</cell><cell>ORG (0.256)</cell></row><row><cell cols="2">Best Model Newseye_de</cell><cell cols="2">macro-fuzzy 0.392</cell><cell>0.502</cell><cell>0.547</cell><cell>PER (0.591)</cell><cell>HUMANPROD (0.0)</cell></row><row><cell cols="2">Best Model Newseye_fi</cell><cell cols="2">macro-fuzzy 0.765</cell><cell>0.626</cell><cell>0.668</cell><cell>HUMANPROD (0.863) ORG (0.57)</cell></row><row><cell cols="2">Best Model Newseye_fr</cell><cell cols="2">macro-fuzzy 0.775</cell><cell>0.777</cell><cell>0.766</cell><cell>PER (0.82)</cell><cell>ORG (0.606)</cell></row><row><cell cols="2">Best Model Newseye_sv</cell><cell cols="2">macro-fuzzy 0.738</cell><cell>0.72</cell><cell>0.735</cell><cell>LOC (0.821)</cell><cell>ORG (0.428)</cell></row><row><cell cols="2">Best Model Sonar_de</cell><cell cols="2">macro-fuzzy 0.617</cell><cell>0.667</cell><cell>0.633</cell><cell>LOC (0.711)</cell><cell>ORG (0.451)</cell></row><row><cell cols="4">Best Model Topres19th_en macro-fuzzy 0.813</cell><cell>0.86</cell><cell>0.824</cell><cell>LOC (0.873)</cell><cell>BUILDING (0.643)</cell></row><row><cell>AVERAGE</cell><cell></cell><cell></cell><cell>0.62</cell><cell>0.63</cell><cell>0.64</cell></row><row><cell cols="2">Ensembled HIPE2020_de</cell><cell cols="2">macro-fuzzy 0.819</cell><cell>0.826</cell><cell>0.808</cell><cell>TIME (0.89)</cell><cell>ORG (0.501)</cell></row><row><cell cols="2">Ensembled HIPE2020_en</cell><cell cols="2">macro-fuzzy 0.724</cell><cell>0.656</cell><cell>0.689</cell><cell>TIME (1.0)</cell><cell>PROD (0.0)</cell></row><row><cell cols="2">Ensembled HIPE2020_fr</cell><cell cols="2">macro-fuzzy 0.858</cell><cell>0.816</cell><cell>0.826</cell><cell>TIME (0.951)</cell><cell>ORG (0.609)</cell></row><row><cell cols="2">Ensembled Letemps_fr</cell><cell cols="2">macro-fuzzy 0.545</cell><cell>0.712</cell><cell>0.763</cell><cell>LOC (0.847)</cell><cell>ORG (0.239)</cell></row><row><cell cols="2">Ensembled Newseye_de</cell><cell cols="2">macro-fuzzy 0.403</cell><cell>0.469</cell><cell>0.538</cell><cell>LOC (0.585)</cell><cell>HUMANPROD (0.167)</cell></row><row><cell cols="2">Ensembled Newseye_fi</cell><cell cols="2">macro-fuzzy 0.796</cell><cell>0.581</cell><cell>0.703</cell><cell>HUMANPROD (0.795) ORG (0.593)</cell></row><row><cell cols="2">Ensembled Newseye_fr</cell><cell cols="2">macro-fuzzy 0.814</cell><cell>0.762</cell><cell>0.779</cell><cell>PER (0.811)</cell><cell>ORG (0.621)</cell></row><row><cell cols="2">Ensembled Newseye_sv</cell><cell cols="2">macro-fuzzy 0.765</cell><cell>0.722</cell><cell>0.747</cell><cell>HUMANPROD (0.861) ORG (0.417)</cell></row><row><cell cols="2">Ensembled Sonar_de</cell><cell cols="2">macro-fuzzy 0.663</cell><cell>0.672</cell><cell>0.654</cell><cell>LOC (0.758)</cell><cell>ORG (0.432)</cell></row><row><cell cols="4">Ensembled Topres19th_en macro-fuzzy 0.881</cell><cell>0.823</cell><cell>0.841</cell><cell>LOC (0.889)</cell><cell>BUILDING (0.642)</cell></row><row><cell>AVERAGE</cell><cell></cell><cell></cell><cell>0.70</cell><cell>0.68</cell><cell>0.69</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="16,52.71,403.62,489.86,257.25"><head>Table 9</head><label>9</label><figDesc>Macro-strict evaluation of submitted systems</figDesc><table coords="16,52.71,434.34,489.86,226.52"><row><cell>System</cell><cell>Dataset</cell><cell>Eval</cell><cell cols="4">Precision all Recall all F1 all-labels Best category</cell><cell>Worst category</cell></row><row><cell cols="2">Best Model HIPE2020_de</cell><cell cols="2">macro-strict 0.671</cell><cell>0.693</cell><cell>0.672</cell><cell>LOC (0.805)</cell><cell>ORG (0.384)</cell></row><row><cell cols="2">Best Model HIPE2020_fr</cell><cell cols="2">macro-strict 0.764</cell><cell>0.735</cell><cell>0.74</cell><cell>LOC (0.772)</cell><cell>ORG (0.499)</cell></row><row><cell cols="2">Best Model Letemps_fr</cell><cell cols="2">macro-strict 0.448</cell><cell>0.625</cell><cell>0.659</cell><cell>LOC (0.754)</cell><cell>ORG (0.114)</cell></row><row><cell cols="2">Best Model Newseye_de</cell><cell cols="2">macro-strict 0.302</cell><cell>0.386</cell><cell>0.421</cell><cell>LOC (0.464)</cell><cell>HUMANPROD (0.0)</cell></row><row><cell cols="2">Best Model Newseye_fi</cell><cell cols="2">macro-strict 0.682</cell><cell>0.561</cell><cell>0.596</cell><cell>PER (0.733)</cell><cell>ORG (0.481)</cell></row><row><cell cols="2">Best Model Newseye_fr</cell><cell cols="2">macro-strict 0.63</cell><cell>0.623</cell><cell>0.621</cell><cell>HUMANPROD (0.699) ORG (0.419)</cell></row><row><cell cols="2">Best Model Newseye_sv</cell><cell cols="2">macro-strict 0.611</cell><cell>0.583</cell><cell>0.602</cell><cell>HUMANPROD (0.758) ORG (0.338)</cell></row><row><cell cols="2">Best Model Sonar_de</cell><cell cols="2">macro-strict 0.46</cell><cell>0.5</cell><cell>0.474</cell><cell>LOC (0.649)</cell><cell>ORG (0.241)</cell></row><row><cell cols="4">Best Model Topres19th_en macro-strict 0.77</cell><cell>0.812</cell><cell>0.779</cell><cell>LOC (0.824)</cell><cell>BUILDING (0.527)</cell></row><row><cell>AVERAGE</cell><cell></cell><cell></cell><cell>0.62</cell><cell>0.63</cell><cell>0.64</cell></row><row><cell cols="2">Ensembled HIPE2020_de</cell><cell cols="2">macro-strict 0.686</cell><cell>0.679</cell><cell>0.67</cell><cell>LOC (0.815)</cell><cell>ORG (0.411)</cell></row><row><cell cols="2">Ensembled HIPE2020_en</cell><cell cols="2">macro-strict 0.553</cell><cell>0.494</cell><cell>0.523</cell><cell>TIME (0.718)</cell><cell>PROD (0.0)</cell></row><row><cell cols="2">Ensembled HIPE2020_fr</cell><cell cols="2">macro-strict 0.741</cell><cell>0.7</cell><cell>0.712</cell><cell>LOC (0.712)</cell><cell>ORG (0.403)</cell></row><row><cell cols="2">Ensembled Letemps_fr</cell><cell cols="2">macro-strict 0.48</cell><cell>0.636</cell><cell>0.681</cell><cell>LOC (0.776)</cell><cell>ORG (0.095)</cell></row><row><cell cols="2">Ensembled Newseye_de</cell><cell cols="2">macro-strict 0.316</cell><cell>0.364</cell><cell>0.419</cell><cell>LOC (0.494)</cell><cell>HUMANPROD (0.167)</cell></row><row><cell cols="2">Ensembled Newseye_fi</cell><cell cols="2">macro-strict 0.666</cell><cell>0.484</cell><cell>0.585</cell><cell>LOC (0.655)</cell><cell>ORG (0.477)</cell></row><row><cell cols="2">Ensembled Newseye_fr</cell><cell cols="2">macro-strict 0.659</cell><cell>0.614</cell><cell>0.63</cell><cell>HUMANPROD (0.766) ORG (0.471)</cell></row><row><cell cols="2">Ensembled Newseye_sv</cell><cell cols="2">macro-strict 0.654</cell><cell>0.608</cell><cell>0.634</cell><cell>HUMANPROD (0.739) ORG (0.306)</cell></row><row><cell cols="2">Ensembled Sonar_de</cell><cell cols="2">macro-strict 0.512</cell><cell>0.514</cell><cell>0.503</cell><cell>LOC (0.695)</cell><cell>ORG (0.23)</cell></row><row><cell cols="4">Ensembled Topres19th_en macro-strict 0.839</cell><cell>0.785</cell><cell>0.802</cell><cell>LOC (0.86)</cell><cell>BUILDING (0.554)</cell></row><row><cell>AVERAGE</cell><cell></cell><cell></cell><cell>0.69</cell><cell>0.68</cell><cell>0.69</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="17,52.73,102.58,489.81,257.83"><head>Table 10</head><label>10</label><figDesc>Micro-fuzzy evaluation of submitted systems</figDesc><table coords="17,52.73,133.31,489.81,227.10"><row><cell>System</cell><cell>Dataset</cell><cell>Eval</cell><cell cols="4">Precision all Recall all F1 all-labels Best category</cell><cell>Worst category</cell></row><row><cell cols="2">Best Model HIPE2020_de</cell><cell cols="2">micro-fuzzy 0.783</cell><cell>0.826</cell><cell>0.804</cell><cell>PERS (0.874)</cell><cell>ORG (0.545)</cell></row><row><cell cols="2">Best Model hipe2020_fr</cell><cell cols="2">micro-fuzzy 0.825</cell><cell>0.776</cell><cell>0.8</cell><cell>PERS (0.848)</cell><cell>PROD (0.596)</cell></row><row><cell cols="2">Best Model Letemps_fr</cell><cell cols="2">micro-fuzzy 0.61</cell><cell>0.771</cell><cell>0.681</cell><cell>LOC (0.734)</cell><cell>ORG (0.208)</cell></row><row><cell cols="2">Best Model Newseye_de</cell><cell cols="2">micro-fuzzy 0.48</cell><cell>0.512</cell><cell>0.495</cell><cell>LOC (0.541)</cell><cell>HUMANPROD (0.0)</cell></row><row><cell cols="2">Best Model Newseye_fi</cell><cell cols="2">micro-fuzzy 0.681</cell><cell>0.603</cell><cell>0.64</cell><cell>HUMANPROD (0.732) ORG (0.478)</cell></row><row><cell cols="2">Best Model Newseye_fr</cell><cell cols="2">micro-fuzzy 0.785</cell><cell>0.787</cell><cell>0.786</cell><cell>PER (0.849)</cell><cell>HUMANPROD (0.579)</cell></row><row><cell cols="2">Best Model Newseye_sv</cell><cell cols="2">micro-fuzzy 0.786</cell><cell>0.704</cell><cell>0.742</cell><cell>LOC (0.799)</cell><cell>ORG (0.457)</cell></row><row><cell cols="2">Best Model Sonar_de</cell><cell cols="2">micro-fuzzy 0.625</cell><cell>0.718</cell><cell>0.668</cell><cell>LOC (0.765)</cell><cell>ORG (0.468)</cell></row><row><cell cols="4">Best Model Topres19th_en micro-fuzzy 0.807</cell><cell>0.851</cell><cell>0.829</cell><cell>LOC (0.872)</cell><cell>STREET (0.661)</cell></row><row><cell>AVERAGE</cell><cell></cell><cell></cell><cell>0.61</cell><cell>0.63</cell><cell>0.65</cell></row><row><cell cols="2">Ensembled HIPE2020_de</cell><cell cols="2">micro-fuzzy 0.812</cell><cell>0.833</cell><cell>0.822</cell><cell>LOC (0.866)</cell><cell>PROD (0.574)</cell></row><row><cell cols="2">Ensembled HIPE2020_en</cell><cell cols="2">micro-fuzzy 0.726</cell><cell>0.661</cell><cell>0.692</cell><cell>TIME (0.909)</cell><cell>PROD (0.0)</cell></row><row><cell cols="2">Ensembled HIPE2020_fr</cell><cell cols="2">micro-fuzzy 0.824</cell><cell>0.773</cell><cell>0.798</cell><cell>TIME (0.847)</cell><cell>ORG (0.555)</cell></row><row><cell cols="2">Ensembled Letemps_fr</cell><cell cols="2">micro-fuzzy 0.642</cell><cell>0.773</cell><cell>0.701</cell><cell>LOC (0.7)</cell><cell>ORG (0.178)</cell></row><row><cell cols="2">Ensembled Newseye_de</cell><cell cols="2">micro-fuzzy 0.481</cell><cell>0.478</cell><cell>0.479</cell><cell>LOC (0.551)</cell><cell>HUMANPROD (0.08)</cell></row><row><cell cols="2">Ensembled Newseye_fi</cell><cell cols="2">micro-fuzzy 0.73</cell><cell>0.619</cell><cell>0.67</cell><cell>PER (0.706)</cell><cell>ORG (0.495)</cell></row><row><cell cols="2">Ensembled Newseye_fr</cell><cell cols="2">micro-fuzzy 0.801</cell><cell>0.744</cell><cell>0.772</cell><cell>PER (0.839)</cell><cell>ORG (0.58)</cell></row><row><cell cols="2">Ensembled Newseye_sv</cell><cell cols="2">micro-fuzzy 0.797</cell><cell>0.702</cell><cell>0.746</cell><cell>LOC (0.801)</cell><cell>ORG (0.442)</cell></row><row><cell cols="2">Ensembled Sonar_de</cell><cell cols="2">micro-fuzzy 0.641</cell><cell>0.696</cell><cell>0.667</cell><cell>LOC (0.78)</cell><cell>ORG (0.443)</cell></row><row><cell cols="4">Ensembled Topres19th_en micro-fuzzy 0.869</cell><cell>0.81</cell><cell>0.838</cell><cell>LOC (0.88)</cell><cell>BUILDING (0.659)</cell></row><row><cell>AVERAGE</cell><cell></cell><cell></cell><cell>0.70</cell><cell>0.67</cell><cell>0.68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="17,52.75,403.27,487.64,258.97"><head>Table 11</head><label>11</label><figDesc>Micro-strict evaluation of submitted systems</figDesc><table coords="17,52.75,434.03,487.64,228.21"><row><cell>System</cell><cell>Dataset</cell><cell>Eval</cell><cell cols="4">Precision all Recall all F1 all-labels Best category</cell><cell>Worst category</cell></row><row><cell cols="2">Best Model HIPE2020_de</cell><cell cols="2">micro-strict 0.677</cell><cell>0.714</cell><cell>0.695</cell><cell>LOC (0.794)</cell><cell>ORG (0.411)</cell></row><row><cell cols="2">Best Model HIPE2020_fr</cell><cell cols="2">micro-strict 0.718</cell><cell>0.675</cell><cell>0.696</cell><cell>LOC (0.748)</cell><cell>PROD (0.519)</cell></row><row><cell cols="2">Best Model Letemps_fr</cell><cell cols="2">micro-strict 0.557</cell><cell>0.704</cell><cell>0.622</cell><cell>LOC (0.692)</cell><cell>ORG (0.12)</cell></row><row><cell cols="2">Best Model Newseye_de</cell><cell cols="2">micro-strict 0.395</cell><cell>0.421</cell><cell>0.408</cell><cell>LOC (0.479)</cell><cell>HUMANPROD (0.0)</cell></row><row><cell cols="2">Best Model newseye_fi</cell><cell cols="2">micro-strict 0.592</cell><cell>0.524</cell><cell>0.556</cell><cell>HUMANPROD (0.683) ORG (0.407)</cell></row><row><cell cols="2">Best Model Newseye_fr</cell><cell cols="2">micro-strict 0.655</cell><cell>0.657</cell><cell>0.656</cell><cell>PER (0.709)</cell><cell>ORG (0.441)</cell></row><row><cell cols="2">Best Model Newseye_sv</cell><cell cols="2">micro-strict 0.673</cell><cell>0.603</cell><cell>0.636</cell><cell>HUMANPROD (0.75)</cell><cell>ORG (0.343)</cell></row><row><cell cols="2">Best Model Sonar_de</cell><cell cols="2">micro-strict 0.447</cell><cell>0.513</cell><cell>0.477</cell><cell>LOC (0.685)</cell><cell>ORG (0.293)</cell></row><row><cell cols="4">Best Model Topres19th_en micro-strict 0.761</cell><cell>0.802</cell><cell>0.781</cell><cell>LOC (0.833)</cell><cell>BUILDING (0.564)</cell></row><row><cell>AVERAGE</cell><cell></cell><cell></cell><cell>0.64</cell><cell>0.66</cell><cell>0.67</cell></row><row><cell cols="2">Ensembled HIPE2020_de</cell><cell cols="2">micro-strict 0.716</cell><cell>0.735</cell><cell>0.725</cell><cell>LOC (0.82)</cell><cell>PROD (0.452)</cell></row><row><cell cols="2">Ensembled HIPE2020_en</cell><cell cols="2">micro-strict 0.538</cell><cell>0.49</cell><cell>0.513</cell><cell>LOC (0.607)</cell><cell>PROD (0.0)</cell></row><row><cell cols="2">Ensembled HIPE2020_fr</cell><cell cols="2">micro-strict 0.7</cell><cell>0.657</cell><cell>0.678</cell><cell>LOC (0.761)</cell><cell>PROD (0.421)</cell></row><row><cell cols="2">Ensembled Letemps_fr</cell><cell cols="2">micro-strict 0.589</cell><cell>0.71</cell><cell>0.644</cell><cell>LOC (0.715)</cell><cell>ORG (0.089)</cell></row><row><cell cols="2">Ensembled Newseye_de</cell><cell cols="2">micro-strict 0.396</cell><cell>0.394</cell><cell>0.395</cell><cell>LOC (0.485)</cell><cell>HUMANPROD (0.08)</cell></row><row><cell cols="2">Ensembled Newseye_fi</cell><cell cols="2">micro-strict 0.618</cell><cell>0.524</cell><cell>0.567</cell><cell>HUMANPROD (0.615) ORG (0.385)</cell></row><row><cell cols="2">Ensembled Newseye_fr</cell><cell cols="2">micro-strict 0.673</cell><cell>0.625</cell><cell>0.648</cell><cell>PER (0.712)</cell><cell>ORG (0.455)</cell></row><row><cell cols="2">Ensembled Newseye_sv</cell><cell cols="2">micro-strict 0.686</cell><cell>0.604</cell><cell>0.643</cell><cell>LOC (0.716)</cell><cell>ORG (0.288)</cell></row><row><cell cols="2">Ensembled Sonar_de</cell><cell cols="2">micro-strict 0.47</cell><cell>0.511</cell><cell>0.49</cell><cell>LOC (0.709)</cell><cell>ORG (0.268)</cell></row><row><cell cols="4">Ensembled Topres19th_en micro-strict 0.816</cell><cell>0.76</cell><cell>0.787</cell><cell>LOC (0.84)</cell><cell>BUILDING (0.551)</cell></row><row><cell>AVERAGE</cell><cell></cell><cell></cell><cell>0.69</cell><cell>0.66</cell><cell>0.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="18,88.99,90.49,418.09,646.20"><head>Table 12</head><label>12</label><figDesc>Evaluation multilingual experiments. Model1: dbmdz/bert-base-historic-multilingual-cased, Model2: setu4993/LaBSE, Model3: bert-base-multilingual-cased, Model4: bert-base-multilingual-uncased, Model5: distilbert-base-multilingual-cased</figDesc><table coords="18,116.42,146.01,359.94,590.68"><row><cell>System</cell><cell>Dataset</cell><cell>Eval</cell><cell cols="3">Precision all Recall all F1 all-labels</cell></row><row><cell cols="4">sub_Best Model Newseye_de micro-strict 0.395</cell><cell>0.421</cell><cell>0.408</cell></row><row><cell cols="2">sub_Best Model Newseye_fi</cell><cell cols="2">micro-strict 0.592</cell><cell>0.524</cell><cell>0.556</cell></row><row><cell cols="2">sub_Best Model Newseye_fr</cell><cell cols="2">micro-strict 0.655</cell><cell>0.657</cell><cell>0.656</cell></row><row><cell cols="4">sub_Best Model Newseye_sv micro-strict 0.673</cell><cell>0.603</cell><cell>0.636</cell></row><row><cell>AVERAGE</cell><cell></cell><cell></cell><cell>0.54</cell><cell>0.52</cell><cell>0.56</cell></row><row><cell cols="4">sub_Ensembled Newseye_de micro-strict 0.396</cell><cell>0.394</cell><cell>0.395</cell></row><row><cell cols="2">sub_Ensembled Newseye_fi</cell><cell cols="2">micro-strict 0.618</cell><cell>0.524</cell><cell>0.567</cell></row><row><cell cols="2">sub_Ensembled Newseye_fr</cell><cell cols="2">micro-strict 0.673</cell><cell>0.625</cell><cell>0.648</cell></row><row><cell cols="4">sub_Ensembled Newseye_sv micro-strict 0.686</cell><cell>0.604</cell><cell>0.643</cell></row><row><cell>AVERAGE</cell><cell></cell><cell></cell><cell>0.70</cell><cell>0.65</cell><cell>0.67</cell></row><row><cell>set_random</cell><cell cols="3">Newseye_de micro-strict 0.006</cell><cell>0.022</cell><cell>0.009</cell></row><row><cell>set_random</cell><cell>Newseye_fi</cell><cell cols="2">micro-strict 0.005</cell><cell>0.01</cell><cell>0.007</cell></row><row><cell>set_random</cell><cell>Newseye_fr</cell><cell cols="2">micro-strict 0.005</cell><cell>0.013</cell><cell>0.008</cell></row><row><cell>set_random</cell><cell cols="3">Newseye_sv micro-strict 0.01</cell><cell>0.023</cell><cell>0.013</cell></row><row><cell>AVERAGE</cell><cell></cell><cell></cell><cell>0.01</cell><cell>0.02</cell><cell>0.01</cell></row><row><cell>Model1</cell><cell cols="3">Newseye_de micro-strict 0.407</cell><cell>0.399</cell><cell>0.403</cell></row><row><cell>Model1</cell><cell>Newseye_fi</cell><cell cols="2">micro-strict 0.671</cell><cell>0.564</cell><cell>0.613</cell></row><row><cell>Model1</cell><cell>Newseye_fr</cell><cell cols="2">micro-strict 0.653</cell><cell>0.616</cell><cell>0.634</cell></row><row><cell>Model1</cell><cell cols="3">Newseye_sv micro-strict 0.729</cell><cell>0.627</cell><cell>0.674</cell></row><row><cell>AVERAGE</cell><cell></cell><cell></cell><cell>0.62</cell><cell>0.55</cell><cell>0.58</cell></row><row><cell>Model2</cell><cell cols="3">Newseye_de micro-strict 0.406</cell><cell>0.416</cell><cell>0.411</cell></row><row><cell>Model2</cell><cell>Newseye_fi</cell><cell cols="2">micro-strict 0.624</cell><cell>0.514</cell><cell>0.563</cell></row><row><cell>Model2</cell><cell>Newseye_fr</cell><cell cols="2">micro-strict 0.65</cell><cell>0.607</cell><cell>0.628</cell></row><row><cell>Model2</cell><cell cols="3">Newseye_sv micro-strict 0.693</cell><cell>0.599</cell><cell>0.643</cell></row><row><cell>AVERAGE</cell><cell></cell><cell></cell><cell>0.59</cell><cell>0.53</cell><cell>0.56</cell></row><row><cell>Model3</cell><cell cols="3">Newseye_de micro-strict 0.407</cell><cell>0.44</cell><cell>0.423</cell></row><row><cell>Model3</cell><cell>Newseye_fi</cell><cell cols="2">micro-strict 0.586</cell><cell>0.462</cell><cell>0.517</cell></row><row><cell>Model3</cell><cell>Newseye_fr</cell><cell cols="2">micro-strict 0.648</cell><cell>0.585</cell><cell>0.615</cell></row><row><cell>Model3</cell><cell cols="3">Newseye_sv micro-strict 0.643</cell><cell>0.548</cell><cell>0.592</cell></row><row><cell>AVERAGE</cell><cell></cell><cell></cell><cell>0.57</cell><cell>0.51</cell><cell>0.54</cell></row><row><cell>Model4</cell><cell cols="3">Newseye_de micro-strict 0.405</cell><cell>0.428</cell><cell>0.416</cell></row><row><cell>Model4</cell><cell>Newseye_fi</cell><cell cols="2">micro-strict 0.563</cell><cell>0.438</cell><cell>0.493</cell></row><row><cell>Model4</cell><cell>Newseye_fr</cell><cell cols="2">micro-strict 0.626</cell><cell>0.587</cell><cell>0.606</cell></row><row><cell>Model4</cell><cell cols="3">Newseye_sv micro-strict 0.637</cell><cell>0.53</cell><cell>0.579</cell></row><row><cell>AVERAGE</cell><cell></cell><cell></cell><cell>0.56</cell><cell>0.50</cell><cell>0.52</cell></row><row><cell>Model5</cell><cell cols="3">Newseye_de micro-strict 0.232</cell><cell>0.157</cell><cell>0.188</cell></row><row><cell>Model5</cell><cell>Newseye_fi</cell><cell cols="2">micro-strict 0.266</cell><cell>0.113</cell><cell>0.159</cell></row><row><cell>Model5</cell><cell>Newseye_fr</cell><cell cols="2">micro-strict 0.245</cell><cell>0.239</cell><cell>0.242</cell></row><row><cell>Model5</cell><cell cols="3">Newseye_sv micro-strict 0.356</cell><cell>0.182</cell><cell>0.241</cell></row><row><cell>AVERAGE</cell><cell></cell><cell></cell><cell>0.27</cell><cell>0.17</cell><cell>0.21</cell></row><row><cell>Ensembling</cell><cell cols="3">Newseye_de micro-strict 0.434</cell><cell>0.408</cell><cell>0.421</cell></row><row><cell>Ensembling</cell><cell>Newseye_fi</cell><cell cols="2">micro-strict 0.649</cell><cell>0.502</cell><cell>0.566</cell></row><row><cell>Ensembling</cell><cell>Newseye_fr</cell><cell cols="2">micro-strict 0.691</cell><cell>0.608</cell><cell>0.647</cell></row><row><cell>Ensembling</cell><cell cols="3">Newseye_sv micro-strict 0.742</cell><cell>0.596</cell><cell>0.661</cell></row><row><cell>AVERAGE</cell><cell></cell><cell></cell><cell>0.629</cell><cell>0.5285</cell><cell>0.57375</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Thanks to <rs type="person">Simon Clematide</rs> and <rs type="person">Andrianos Michail</rs> for lecturing the courses "Machine Learning for NLP 1 and 2" and for mentoring our group during development.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Online Resources</head><p>• Repository for this paper, • HIPE2022 datasets, • HIPE2022 evaluation module, • Google Colab. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,112.66,244.87,394.53,10.91;12,112.66,258.42,395.01,10.91;12,112.66,271.96,395.17,10.91;12,112.66,285.51,394.61,10.91;12,112.66,299.06,393.58,10.91;12,112.66,312.61,74.84,10.91" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="12,335.99,271.96,171.84,10.91;12,112.66,285.51,157.67,10.91">HuggingFace&apos;s Transformers: State-ofthe-art Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1910.03771</idno>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<idno>arXiv:1910.03771 [cs</idno>
		<ptr target="http://arxiv.org/abs/1910.03771.doi:10.48550/arXiv.1910.03771" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>type: article</note>
</biblStruct>

<biblStruct coords="12,112.66,326.16,395.17,10.91;12,112.66,339.71,273.50,10.91" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m" coord="12,148.16,339.71,107.76,10.91">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,353.26,394.53,10.91;12,112.66,366.81,394.53,10.91;12,112.66,380.36,395.17,10.91;12,112.66,393.91,393.33,10.91;12,112.66,407.46,393.33,10.91;12,112.66,421.01,395.00,10.91;12,112.66,434.55,197.48,10.91" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,311.38,380.36,196.45,10.91;12,112.66,393.91,77.28,10.91">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">Le</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-demos.6.doi:10.18653/v1/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m" coord="12,219.91,393.91,286.07,10.91;12,112.66,407.46,393.33,10.91;12,112.66,421.01,47.14,10.91">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,448.10,393.33,10.91;12,112.66,461.65,395.01,10.91;12,112.66,475.20,167.31,10.91" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="12,352.90,448.10,153.09,10.91;12,112.66,461.65,186.90,10.91">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1810.04805</idno>
		<ptr target="https://arxiv.org/abs/1810.04805.doi:10.48550/ARXIV.1810.04805" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,488.75,393.61,10.91;12,112.66,502.30,393.98,10.91;12,112.66,515.85,53.29,10.91" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ehrmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hamdi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Linhares Pontes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Romanello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<idno>arXiv-2109</idno>
		<title level="m" coord="12,443.66,488.75,62.60,10.91;12,112.66,502.30,290.76,10.91">Named entity recognition and classification on historical documents: A survey</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,529.40,393.32,10.91;12,112.33,542.95,397.81,10.91;12,112.66,558.94,73.62,7.90" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,229.59,529.40,246.59,10.91">A survey on deep learning for named entity recognition</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2020.2981314</idno>
	</analytic>
	<monogr>
		<title level="j" coord="12,484.58,529.40,21.40,10.91;12,112.33,542.95,226.70,10.91">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="50" to="70" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,570.05,393.61,10.91;12,112.66,583.60,395.00,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,277.61,570.05,228.67,10.91;12,112.66,583.60,138.28,10.91">Experiments from limsi at the french named entity recognition coarse-grained task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ghannay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Grouin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lavergne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,273.51,583.60,106.27,10.91">Proc of CLEF 2020 LNCS</title>
		<meeting>of CLEF 2020 LNCS<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,112.66,597.15,394.52,10.91;12,112.66,610.69,393.33,10.91;12,112.66,624.24,393.33,10.91;12,112.66,637.79,395.01,10.91;12,112.66,651.34,191.55,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,153.65,610.69,193.04,10.91">CamemBERT: a tasty French language model</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Ortiz Suárez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Romary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">É</forename><surname>De La Clergerie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sagot</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.645</idno>
		<ptr target="https://aclanthology.org/2020.acl-main.645.doi:10.18653/v1/2020.acl-main.645" />
	</analytic>
	<monogr>
		<title level="m" coord="12,369.22,610.69,136.77,10.91;12,112.66,624.24,393.33,10.91;12,112.66,637.79,46.07,10.91">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7203" to="7219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,86.97,395.17,10.91;13,112.66,100.52,393.32,10.91;13,112.33,114.06,296.49,10.91" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="1907.11692" />
		<title level="m" coord="13,140.43,100.52,261.00,10.91">Roberta: A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,127.61,393.33,10.91;13,112.66,141.16,395.01,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,233.41,127.61,272.57,10.91;13,112.66,141.16,31.65,10.91">Transfer learning for named entity recognition in historical corpora</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Colavizza</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_168.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="13,166.36,141.16,96.08,10.91">CLEF (Working Notes)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,154.71,393.61,10.91;13,112.66,168.26,394.62,10.91;13,112.66,181.81,393.33,10.91;13,110.82,195.36,397.01,10.91;13,112.66,208.91,394.61,10.91;13,112.31,222.46,172.79,10.91" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,444.24,154.71,62.02,10.91;13,112.66,168.26,372.05,10.91">Named entity recognition and linking on historical newspapers: Uva.ilps &amp; rel at clef hipe 2020</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Provatorova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vakulenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Dercksen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Van Hulst</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2696/paper_209.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="13,370.77,181.81,135.22,10.91;13,110.82,195.36,219.37,10.91">Working Notes of CLEF 2020 -Conference and Labs of the Evaluation Forum</title>
		<title level="s" coord="13,229.32,208.91,156.19,10.91">CEUR Workshop Proceedings, CEUR</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</editor>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">0001. September 22-25, 2020. 2696. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,236.01,395.17,10.91;13,112.66,249.56,394.53,10.91;13,112.66,263.11,394.52,10.91;13,112.66,276.66,152.81,10.91" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="13,337.26,236.01,170.57,10.91;13,112.66,249.56,224.68,10.91">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1810.04805</idno>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<idno>arXiv:1810.04805 [cs</idno>
		<ptr target="http://arxiv.org/abs/1810.04805.doi:10.48550/arXiv.1810.04805" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>type: article</note>
</biblStruct>

<biblStruct coords="13,112.66,290.20,393.33,10.91;13,112.66,303.75,297.17,10.91" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ehrmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Romanello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clematide</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.6045662</idno>
		<ptr target="https://doi.org/10.5281/zenodo.6045662" />
		<title level="m" coord="13,349.26,290.20,156.73,10.91;13,112.66,303.75,64.50,10.91">Hipe 2022 shared task participation guidelines v1.0</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,317.30,395.01,10.91;13,112.66,330.85,45.75,10.91" xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Staatsbibliothek</surname></persName>
		</author>
		<ptr target="https://huggingface.co/dbmdz" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
		<respStmt>
			<orgName>Bayerische Staatsbibliothek</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,344.40,394.53,10.91;13,112.30,357.95,393.68,10.91;13,112.66,371.50,394.51,10.91;13,112.66,385.05,252.89,10.91" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="13,172.21,357.95,281.35,10.91">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1907.11692</idno>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<idno>arXiv:1907.11692 [cs</idno>
		<ptr target="http://arxiv.org/abs/1907.11692.doi:10.48550/arXiv.1907.11692" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>type: article</note>
</biblStruct>

<biblStruct coords="13,112.66,398.60,395.17,10.91;13,112.66,412.15,394.52,10.91;13,112.66,425.70,394.52,10.91;13,112.66,439.25,152.81,10.91" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="13,400.11,398.60,107.72,10.91;13,112.66,412.15,232.60,10.91">XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1906.08237</idno>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<idno>arXiv:1906.08237 [cs</idno>
		<ptr target="http://arxiv.org/abs/1906.08237.doi:10.48550/arXiv.1906.08237" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>type: article</note>
</biblStruct>

<biblStruct coords="13,112.66,452.79,395.17,10.91;13,112.66,466.34,394.61,10.91;13,112.66,479.89,393.32,10.91;13,112.66,493.44,74.84,10.91" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="13,311.81,452.79,196.02,10.91;13,112.66,466.34,156.63,10.91">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2003.10555</idno>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<idno>arXiv:2003.10555 [cs</idno>
		<ptr target="http://arxiv.org/abs/2003.10555.doi:10.48550/arXiv.2003.10555" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>type: article</note>
</biblStruct>

<biblStruct coords="13,112.66,506.99,394.53,10.91;13,112.66,520.54,22.69,10.91" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="13,256.86,506.99,206.40,10.91">Natural Language Processing with Transformers</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Werra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>O&apos;Reilly</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,534.09,393.33,10.91;13,112.66,547.64,395.01,10.91;13,112.66,561.19,191.05,10.91" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,238.58,534.09,267.41,10.91;13,112.66,547.64,236.81,10.91">The relative performance of ensemble methods with deep convolutional neural networks for image classification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bibaut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Laan</surname></persName>
		</author>
		<idno type="DOI">10.1080/02664763.2018.1441383</idno>
	</analytic>
	<monogr>
		<title level="j" coord="13,357.32,547.64,118.95,10.91">Journal of applied statistics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,574.74,394.53,10.91;13,112.66,588.29,393.33,10.91;13,112.66,601.84,393.32,10.91;13,112.66,615.39,144.26,10.91" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="13,271.58,588.29,234.41,10.91;13,112.66,601.84,20.10,10.91">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">É</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,155.39,601.84,350.59,10.91;13,112.66,615.39,46.58,10.91">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,112.66,628.93,394.61,10.91;13,112.28,642.48,395.39,10.91;13,112.66,656.03,187.21,10.91" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="13,452.77,628.93,54.50,10.91;13,112.28,642.48,175.38,10.91">Adapterhub: A framework for adapting transformers</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rücklé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Poth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Vulic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="2007.07779" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
