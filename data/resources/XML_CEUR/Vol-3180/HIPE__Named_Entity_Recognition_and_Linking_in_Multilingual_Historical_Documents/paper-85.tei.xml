<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,89.29,84.74,367.84,15.42;1,89.29,106.66,276.36,15.42">Entity Linking in Multilingual Newspapers and Classical Commentaries with BERT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,89.29,134.97,59.36,11.96"><forename type="first">Kai</forename><surname>Labusch</surname></persName>
							<email>kai.labusch@sbb.spk-berlin.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Staatsbibliothek zu Berlin -Preußischer Kulturbesitz</orgName>
								<address>
									<postCode>10785</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,161.29,134.97,97.70,11.96"><forename type="first">Clemens</forename><surname>Neudecker</surname></persName>
							<email>clemens.neudecker@sbb.spk-berlin.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Staatsbibliothek zu Berlin -Preußischer Kulturbesitz</orgName>
								<address>
									<postCode>10785</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,89.29,84.74,367.84,15.42;1,89.29,106.66,276.36,15.42">Entity Linking in Multilingual Newspapers and Classical Commentaries with BERT</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">66569C93A231D32325F085AC8F112FD5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>entity-linking</term>
					<term>BERT</term>
					<term>historical multi-lingual newspaper</term>
					<term>classical commentaries</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Building on our BERT-based entity recognition and three stage entity linking (EL) system [1] that we evaluated in the CLEF HIPE 2020 challenge [2], we focused in the CLEF HIPE 2022 challenge [3] on the entity linking part by participation in the EL-only tasks. We submitted results for the multilingual newspaper challenge (MNC), the multilingual classical commentary challenge (MCC), and the global adaptation challenge (GAC). This working note presents the most important modifications of the entity linking system in comparison to the HIPE 2020 approach and the additional results that have been obtained on the ajmc, hipe2020, newseye, topres19th, and sonar datasets for German, French, and English. The results show that our entity linking approach can be applied to a broad range of text categories and qualities without heavy adaptation and reveals qualitative differences of the impact of hyperparameters on our system that need further investigation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The Berlin State Library (Staatsbibliothek zu Berlin -Preußischer Kulturbesitz, SBB) is one of the largest research libraries in Germany. The SBB continuously expands its digital collections, which at the time of writing comprise of approx. 200,000 digitized works from roughly 1500 to 1950. A key component in providing online access to the digitized collections of SBB is a keyword search, that is based on the full text derived by application of Optical Character Recognition (OCR) to the scanned document pages.</p><p>Thanks to recent advances in OCR technology for historical documents due to deep learning <ref type="bibr" coords="1,89.29,496.34,11.23,10.91" target="#b3">[4,</ref><ref type="bibr" coords="1,102.69,496.34,7.49,10.91" target="#b4">5]</ref>, the quality of the OCR results has now reached a level where Natural Language Processing (NLP) techniques like Named Entity Recognition (NER) and Entity Linking (EL) can be used to further enrich the unstructured texts and thereby enable new ways of searching and analyzing the digitized content.</p><p>In the QURATOR research project <ref type="bibr" coords="1,250.44,550.54,11.27,10.91" target="#b5">[6]</ref>, the SBB had the opportunity to investigate the suitability of state-of-the-art NER <ref type="bibr" coords="1,206.80,564.09,12.69,10.91" target="#b6">[7]</ref> and EL <ref type="bibr" coords="1,255.41,564.09,12.69,10.91" target="#b0">[1]</ref> methods for digitized historical documents, which led to the SBB's participation in the CLEF HIPE 2020 Shared Task organized by Ehrmann et al. <ref type="bibr" coords="1,492.53,577.64,11.35,10.91" target="#b1">[2]</ref>. Since then, the EL system has been further improved and evaluated <ref type="bibr" coords="1,396.40,591.19,11.55,10.91" target="#b7">[8]</ref>, with the CLEF HIPE 2022 EL-only Task providing a welcome opportunity to revisit the evaluation of the EL system and determine what performance improvements the changes have brought in comparison with the previous results and those from other research groups, such as L3i, who obtained the best results for EL-only in CLEF HIPE 2020 <ref type="bibr" coords="2,263.95,114.06,12.90,10.91" target="#b8">[9]</ref> and were the only other participant in the EL-only task for CLEF HIPE 2022 <ref type="bibr" coords="2,205.07,127.61,11.58,10.91" target="#b1">[2]</ref>. The EL-only task focuses on entity linking while the required entity recognition information is provided based on ground-truth as additional input to the system <ref type="bibr" coords="2,122.26,154.71,16.09,10.91" target="#b9">[10]</ref>. This approach enables an independent evaluation of the EL system by avoiding the additional variation that is introduced due to named entity recognition differences or errors.</p><p>Also, with SBB being involved in multiple research projects, where different requirements and annotation standards for NER/EL are being used, the introduction of a new set of challenges related to the adaptation of systems for multilingual and multi-domain input data with differing annotation depth and quality in the training data, fits particularly well with the real world scenarios encountered at SBB.</p><p>Accordingly, SBB participated in the EL-only task where we submitted results for all three challenges, the Multilingual Newspaper Challenge (MNC), Multilingual Classical Commentaries Challenge (MCC) and Global Adaptation Challenge (GAC). The paper is structured as follows: Section 2 provides a brief description of our EL method, followed by a short explanation of the experimental setup in Section 3, results achieved in our CLEF HIPE 2022 participation are analyzed in more detail in Section 4 and Section 5 concludes the paper with a summary and outlook to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Entity-Linking</head><p>The experiments have been performed using an improved version of the EL system<ref type="foot" coords="2,463.11,387.72,3.71,7.97" target="#foot_0">1</ref> that has previously been evaluated in the CLEF HIPE 2020 Shared Task <ref type="bibr" coords="2,364.77,403.03,11.24,10.91" target="#b0">[1,</ref><ref type="bibr" coords="2,378.73,403.03,7.49,10.91" target="#b1">2]</ref>. Here, we only give a brief overview of that system and discuss the most relevant additions and modifications with respect to our CLEF HIPE 2020 participation. For a more detailed description of our EL system, we refer to our CLEF HIPE 2020 working notes <ref type="bibr" coords="2,261.84,443.67,11.43,10.91" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Wikidata Knowledge Base</head><p>In the CLEF HIPE 2020 EL tasks, we learned that our knowledge base that was derived by means of the category structure of Wikipedia provided an insufficient coverage of the entities that are mentioned in the CLEF HIPE 2020 EL test data.</p><p>In order to construct a more comprehensive knowledge base, we introduced a knowledge base that is derived from Wikidata and Wikipedia. Wikidata is used in order to find persons, locations and organisations that have a corresponding Wikipedia article by means of a set of SPARQL-queries<ref type="foot" coords="2,164.73,572.94,3.71,7.97" target="#foot_1">2</ref> . Though the current knowledge base is limited to persons, locations, and organizations due to the SPARQL approach it is adaptable and can be augmented with additional entity types by addition of corresponding SPARQL-queries.</p><p>For all those relevant entities that have been found within Wikidata, all the sentences of any Wikipedia article where those entities have been linked by a Wikipedia author are extracted and made accessible in a database for use in the evaluation step. See Section 4 of <ref type="bibr" coords="3,419.77,86.97,12.68,10.91" target="#b0">[1]</ref> for a description of the database. Sentences from this database where the candidate match has been linked by a Wikipedia author are compared against sentences of the target text where the entity in question of is mentioned (see also Section 4 of <ref type="bibr" coords="3,256.40,127.61,11.09,10.91" target="#b0">[1]</ref>).</p><p>Our EL system can only identify those entities that have a corresponding Wikipedia page since it performs a number of text comparisons for each candidate match out of the knowledge base, i.e., it needs at least one reference in a Wikipedia article to compare that reference and its context against the occurrence in the target text.</p><p>Wikidata provides lots of additional information about the entities. For instance, we include date-of-birth and date-of-inception for persons resp. organisations in the knowledge base and utilize this information to exclude entities from the linking process based on the date of publication. Our EL system accepts a time constraint as input so that persons or organisations that according to Wikidata did not exist at that point in time are not considered in the entity linking process. During the challenge, we evaluated the "hipe2022:date" field and provided that date as time constraint to the EL system.</p><p>After the CLEF HIPE 2022 test data had been published, we computed the percentage of Wikidata-IDs in the test data that actually can be found in the knowledge base of our system. Table <ref type="table" coords="3,115.82,317.30,5.08,10.91" target="#tab_0">1</ref> shows the coverage per dataset and language. The coverage defines an upper limit for the performance of the EL system. It is in our case close to or even above 90% for all datasets except ajmc which is due to the fact that currently certain types of entities, e.g., work of art, are not contained in the knowledge base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Lookup of Candidates</head><p>The first step of EL is the identification of entries of the knowledge base that possibly match with the text passage that has been tagged either as a person, location or organisation. The passage of text in question is termed target surface. Those entries that are selected from the knowledge base as possible matches are denoted as candidates and further processed in the second and third stages of the EL process. The maximum number of candidates considered is an important hyperparameter and denoted as 𝑚𝑎𝑥 𝑐𝑎𝑛𝑑 in the following.</p><p>Our candidate selection uses an approximate nearest neighbour index where word embeddings of parts of the Wikipedia page titles of the entities of the knowledge base are stored. The parts of the page titles are obtained, first by application of regular expressions that replace or remove characters such as whitespace, punctuation and separator characters, and second by splitting the lower-cased result along remaining word boundaries, i.e., " ", "-", and "_". We plan to improve this procedure by utilization of the name variants of the entities provided by Wikidata instead of Wikipedia page titles.</p><p>Then, by use of word embeddings of parts of the target surface that are derived by the same algorithm, related entities can be determined in an approximate nearest neighbour search within the word embedding space. Another important hyperparameter is the cut-off-distance of the nearest neighbour search that is denoted with Δ 𝑙 . See also Section 3 of <ref type="bibr" coords="3,407.20,624.46,11.43,10.91" target="#b0">[1]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Evaluation of Candidates</head><p>For each candidate that has been selected in the first step, a number of text comparisons is performed. From the sentence database (see Section 2.1), for each candidate the sentences of Wikipedia where it is referenced are retrieved and pairwise compared with the sentence that surrounds the target surface. The comparison is done by means of a BERT model that has been purpose trained on pairs of Wikipedia sentences to answer the binary question: "Do these two sentences refer to the same entity or not?". The outcome of the second stage is a set of matching probabilities of the sentence pairs of each candidate. See also Section 4 of <ref type="bibr" coords="4,418.08,397.32,11.43,10.91" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Ranking of Candidates</head><p>In the third step for each candidate, a feature vector is computed that contains statistical descriptors, i.e., minimum, maximum, standard deviation, median, and 0.1, . . . , 0.9 quantiles of the sentence pair matching probabilities from the second step, together with additional features from the lookup step, such as same statistical descriptors of the word embedding similarities within the embedding space. The per-candidate feature vectors are fed into a random forest model that outputs the overall probability that a particular candidate is actually the correct match for the target surface. The final output consists of all candidates sorted according to this matching probability. Δ 𝑟 , the ranking threshold, is another important hyperparameter. Candidates that have a matching probability below that cut-off probability will be discarded. See also Section 5 of <ref type="bibr" coords="4,448.07,568.99,11.43,10.91" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Submission</head><p>The goal of our submission was to evaluate our EL system "as is" in order to determine the offthe-shelf performance for a variety of text categories and qualities. Since the ranking ML-models (random forest) that are in use at SBB have been trained on the CLEF HIPE 2020 EL ground truth <ref type="bibr" coords="4,89.29,668.27,16.21,10.91" target="#b10">[11]</ref>, they are the only component that had to be re-trained to ensure that only permitted data has been used for training. The BERT models that are used in the entity lookup and evaluation steps have been trained only on the digitized collections of the SBB and on Wikipedia sentences. The entries of the lookup index have been retrieved from Wikidata and Wikipedia as explained before. Therefore, we used those parts without any re-training or modification.</p><p>The ranking ML-models have been separately re-trained per language (de, fr, en) on the joined training sets of the ajmc, hipe2020, newseye, topres19th, and sonar datasets. The information about the dataset membership of a sample is not provided to the ranking models. The dev subsets have not been used for training.</p><p>For the submission two different configurations have been used:</p><p>• Run 1: maximum number of candidates (𝑚𝑎𝑥 𝑐𝑎𝑛𝑑 ) = 25; lookup threshold (Δ 𝑙 ) = 0.05; ranking threshold (Δ 𝑟 ) = 0.2 • Run 2: maximum number of candidates (𝑚𝑎𝑥 𝑐𝑎𝑛𝑑 ) = 50; lookup threshold (Δ 𝑙 ) = 0.13; ranking threshold (Δ 𝑟 ) = 0.2</p><p>In order to evaluate the impact of maximum number of candidates, lookup threshold and ranking threshold, we conducted additional experiments for both configurations, where we tried different ranking thresholds, e.g., Δ 𝑟 = 0.05, 0.1, . . . , 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>All the plots are based on the NEL-LIT-micro-fuzzy-eval-TIME-ALL-LED-ALL-metric implemented in the CLEF HIPE 2022 scorer<ref type="foot" coords="5,260.03,372.02,3.71,7.97" target="#foot_2">3</ref> . We do not repeat figures for other metrics, since the findings that are discussed in the following can also be qualitatively observed for metrics like macro-doc-strict, macro-doc-fuzzy or micro-doc-strict 4,5 .</p><p>Figure <ref type="figure" coords="5,130.93,414.42,5.01,10.91" target="#fig_0">1</ref> shows the 𝐹 1 -score, precision and recall obtained on the dev subsets of the datasets versus the corresponding measurements obtained on the test sets. For each combination of language and dataset, i.e., ajmc, hipe2020, newseye, topres19th and sonar, different parametrizations of the EL system have been evaluated (see Section 3). Pairs of measurements located below the diagonal indicate that performance on the dev subset is better than performance on the test subset, whereas pairs of measurements located above the diagonal mean that performance on the dev subset is worse than performance on the corresponding test subset.</p><p>The results show that the performance on the dev subsets is actually a suitable indicator for the performance on the test set. Hence parameter optimization on the dev-sets is feasible in order to optimize performance on the test sets. Results on the dev subset have a tendency to overestimate the performance. Comparison of precision and recall figures shows, that the cause is overestimation of both recall and precision and that it is most noticeable in case of the newseye and sonar datasets. Only in case of the French ajmc dataset the results on the test subset are clearly better than those on the dev subset.</p><p>The intra-dataset and inter-dataset variation of performance differs among the datasets and languages. Intra-dataset variation of results is higher for German than for French or English. For French and English, the measurements show distinct per-dataset clusters, while the measurements for German are much more spread out even within a particular dataset. We obtain both the best and the worst results for English, where performance depends mainly on the dataset.</p><p>Figure <ref type="figure" coords="6,132.88,582.64,5.17,10.91">2</ref> depicts the results of the best performing hyperparameters on the dev subsets versus their corresponding measurements on the test subsets for each combination of dataset and language. Again, dev subset best-performance is a feasible indicator of test subset bestperformance. The best-performance results are roughly symmetrically distributed around the diagonal. Table <ref type="table" coords="6,159.82,636.83,5.07,10.91" target="#tab_1">2</ref> lists the parameter combinations that have best dev subset performance, the corresponding test subset performance and for comparison the best-performing CLEF HIPE 2022 submission for each combination of dataset and language. All the dev subset optimized Evaluation: NEL-LIT-micro-fuzzy-eval-TIME-ALL-LED-ALL Figure <ref type="figure" coords="7,121.35,469.95,3.86,8.93">2</ref>: Observed 𝐹 1 score, precision and recall on the dev sets versus the corresponding measurements on the test set for the best combination of 𝑚𝑎𝑥 𝑐𝑎𝑛𝑑 , Δ 𝑙 , and Δ 𝑟 per dataset and language that have been evaluated. Color encodes the dataset whereas the shape of marker encodes the language.</p><p>parameter combinations slightly outperform our best CLEF HIPE 2022 submission which were obtained with deliberately fixed off-the-shelf parameters. Overall comparison of these results with the coverage of the knowledge base in Table <ref type="table" coords="7,315.32,558.56,5.16,10.91" target="#tab_0">1</ref> leads us to the conclusion, that although the performance is weakly correlated to the coverage, it does not provide an explanation for the performance differences observed between different datasets and languages. From Table <ref type="table" coords="7,153.75,599.21,5.14,10.91" target="#tab_1">2</ref> it can be seen, that a higher number of candidates for evaluation 𝑚𝑎𝑥 𝑐𝑎𝑛𝑑 in combination with a larger lookup threshold Δ 𝑙 does not necessarily lead to best performance. Rather, it depends on the dataset and language if a higher number of evaluation candidates is beneficial for performance.</p><p>Figure <ref type="figure" coords="7,130.58,653.40,4.97,10.91" target="#fig_2">3</ref> shows the impact of variation of the ranking threshold Δ 𝑟 on precision versus recall and its dependence on the number of evaluation candidates 𝑚𝑎𝑥 𝑐𝑎𝑛𝑑 and lookup threshold Δ 𝑙 . For each combination of dataset, language, number of evaluation candidates and lookup threshold, one obtains a receiver-operator-characteristics-like trajectory of measurements by variation of the ranking threshold Δ 𝑟 = 0.05, . . . , 0.5. The lower the ranking threshold, the better recall gets while precision decreases. Only in German, except for ajmc, it is clearly beneficial to use a lower number of evaluation candidates and a lower lookup threshold in high-recall and low-to-medium-precision regimes. In high-precision and low-recall regimes, a high number of evaluation candidates and a larger lookup threshold seems to be the better choice.</p><p>For French and English, we do not observe significant differences on the ajmc, newseye, and topres19th datasets, while for both languages on the hipe2020 dataset, an improvement can be obtained by use of a higher number of evaluation candidates and a larger lookup threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>For an EL system that is meant to be applied to library collections of digitized text materials from a broad variety of domains, origins and qualities, covering a time span of over 400 years, it is first and foremost required to be robust and able to deliver a solid baseline performanceeven when not optimized for a particular type of material, language or time frame. Furthermore, use cases for NER and EL at SBB derive from varying contexts such as retrieval, historical social network analysis or subject indexing, which have distinct requirements and application contexts each, demanding a system that can also easily be tailored to more specific use cases.</p><p>Our experiments performed in the CLEF HIPE 2022 challenges indicate that our EL system actually provides a robust baseline performance for a variety of text categories, qualities and origins, though the overall performance can still be greatly improved.</p><p>Comparison of our results for dev subsets versus test subsets shows consistent outcomes for all the datasets.  much better coverage of relevant Wikidata-IDs in the test data. However there is still some variation in the coverage, and this variation cannot explain the performance differences between datasets. Furthermore, the results for different datasets and languages show non-trivial qualitative impact of different parametrizations of our EL system. These findings need further investigation, since a simple adaptation of the systems parameters does not offer a consistent improvement over all datasets and languages, and the systems response to a change in the hyperparameters is not identical for all the datasets.</p><p>In summary, the second edition of the CLEF HIPE Shared Task has again provided us with many useful insights into our system performance on diverse datasets, and we look forward to investigate the potential for further performance improvements according to these findings. In future work we aim to integrate visual embeddings into a multi-modal system (cf. <ref type="bibr" coords="10,483.35,168.26,15.89,10.91" target="#b11">[12]</ref>), and we will also further exploit prior known constraints (such as e.g. date of publication or birth/death dates of authors) for the purpose of EL.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,89.29,466.71,416.69,9.65;6,89.29,478.66,418.23,9.65;6,89.29,490.89,321.03,8.87"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1:Observed 𝐹 1 score, precision and recall on the dev subsets versus the corresponding measurements on the test subsets for all combinations of 𝑚𝑎𝑥 𝑐𝑎𝑛𝑑 , Δ 𝑙 , and Δ 𝑟 that have been evaluated. Color encodes the dataset whereas the shape of marker encodes the language.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,89.29,550.78,418.23,9.65;9,89.29,562.74,416.69,9.65;9,88.13,574.69,117.13,9.65"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Influence of 𝑚𝑎𝑥 𝑐𝑎𝑛𝑑 , Δ 𝑙 , and Δ 𝑟 on precision and recall on the test sets in the challenge. Color encodes the dataset whereas the marker shape encodes (𝑚𝑎𝑥 𝑐𝑎𝑛𝑑 = 25, Δ 𝑙 = 0.05) versus (𝑚𝑎𝑥 𝑐𝑎𝑛𝑑 = 50, Δ 𝑙 = 0.13).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,88.99,89.95,416.99,178.82"><head>Table 1</head><label>1</label><figDesc>Percentage of Wikidata-IDs of the test-data that are actually contained in our knowledge base that has been derived from Wikidata and Wikipedia.</figDesc><table coords="4,234.77,89.95,125.74,134.70"><row><cell>dataset</cell><cell cols="2">lang coverage</cell></row><row><cell>ajmc</cell><cell>de</cell><cell>0.57</cell></row><row><cell>newseye</cell><cell>de</cell><cell>0.88</cell></row><row><cell>hipe2020</cell><cell>de</cell><cell>0.86</cell></row><row><cell>sonar</cell><cell>de</cell><cell>0.93</cell></row><row><cell>ajmc</cell><cell>fr</cell><cell>0.53</cell></row><row><cell>newseye</cell><cell>fr</cell><cell>0.93</cell></row><row><cell>hipe2020</cell><cell>fr</cell><cell>0.90</cell></row><row><cell>ajmc</cell><cell>en</cell><cell>0.54</cell></row><row><cell>hipe2020</cell><cell>en</cell><cell>0.93</cell></row><row><cell cols="2">topres19th en</cell><cell>0.99</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,88.99,89.95,416.99,203.53"><head>Table 2</head><label>2</label><figDesc>𝑙 𝑚𝑎𝑥 𝑐𝑎𝑛𝑑 Δ 𝑟 𝐹 1 𝑑𝑒𝑣 𝐹 1𝑡𝑒𝑠𝑡 𝐹 1 𝑠𝑢𝑏 Δ 𝑟 𝑑Δ 𝑙 𝑚𝑎𝑥 𝑐𝑎𝑛𝑑 Best test performance in terms of 𝐹 1 -score by hyperparameter selection based on dev set performance versus best parameter configuration of our original CLEF HIPE 2022 submission. The evaluation metric is NEL-LIT-micro-fuzzy-eval-TIME-ALL-LED-ALL-@1.</figDesc><table coords="8,103.91,89.95,368.51,147.44"><row><cell></cell><cell></cell><cell cols="3">Selection based on dev set</cell><cell>HIPE 2022 Submission</cell></row><row><cell cols="3">dataset lang Δ ajmc de 0.13 50</cell><cell cols="2">0.35 0.538 0.518</cell><cell>0.503 0.2 0.13 50</cell></row><row><cell>ajmc</cell><cell>en</cell><cell>0.13 50</cell><cell cols="2">0.35 0.422 0.390</cell><cell>0.381 0.2 0.13 50</cell></row><row><cell>ajmc</cell><cell>fr</cell><cell>0.13 50</cell><cell cols="2">0.15 0.393 0.473</cell><cell>0.470 0.2 0.05 25</cell></row><row><cell>hipe2020</cell><cell>de</cell><cell>0.05 25</cell><cell>0.4</cell><cell>0.526 0.515</cell><cell>0.506 0.2 0.05 25</cell></row><row><cell>hipe2020</cell><cell>en</cell><cell>0.13 50</cell><cell cols="2">0.25 0.373 0.405</cell><cell>0.393 0.2 0.13 50</cell></row><row><cell>hipe2020</cell><cell>fr</cell><cell>0.13 50</cell><cell>0.4</cell><cell>0.613 0.602</cell><cell>0.596 0.2 0.13 50</cell></row><row><cell>newseye</cell><cell>de</cell><cell>0.05 25</cell><cell cols="2">0.35 0.432 0.450</cell><cell>0.444 0.2 0.05 25</cell></row><row><cell>newseye</cell><cell>fr</cell><cell>0.13 50</cell><cell cols="2">0.45 0.520 0.446</cell><cell>0.430 0.2 0.13 50</cell></row><row><cell>sonar</cell><cell>de</cell><cell>0.05 25</cell><cell>0.3</cell><cell>0.587 0.525</cell><cell>0.517 0.2 0.05 25</cell></row><row><cell cols="2">topres19th en</cell><cell>0.13 50</cell><cell cols="2">0.35 0.693 0.658</cell><cell>0.651 0.2 0.13 50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,162.50,663.33,343.49,10.91"><head></head><label></label><figDesc>Our improved Wikidata-based construction of the knowledge base provides</figDesc><table coords="9,84.55,122.54,421.72,277.23"><row><cell>Recall</cell><cell cols="2">0.4 0.275 0.300 0.325 0.350 0.375 0.400 0.425 0.450 0.30</cell><cell>0.6 de: @1</cell><cell>0.8</cell><cell>0.30 0.35 0.40 0.45 0.50</cell><cell>0.6 de: @3</cell><cell>0.8</cell><cell>0.30 0.35 0.40 0.45 0.50</cell><cell>0.6 de: @5</cell><cell>0.8</cell><cell>ajmc max cand =25 l =0.05 ajmc max cand =50 l =0.13 hipe2020 max cand =25 l =0.05 hipe2020 max cand =50 l =0.13 newseye max cand =25 l =0.05 newseye max cand =50 l =0.13 topres19th max cand =25 l =0.05 topres19th max cand =50 l =0.13 sonar max cand =25 l =0.05 sonar max cand =50 l =0.13</cell></row><row><cell></cell><cell>0.25</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,108.93,660.06,330.88,8.97"><p>https://github.com/qurator-spk/sbb_ned/tree/79f3739dcb3b68ade798ab95c177f4bfb641ae52</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,108.93,671.02,363.61,8.97"><p>https://github.com/qurator-spk/sbb_tools/tree/d954888d10a80096f7be4d5e5b202ba782479300/sparql</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,108.93,638.16,152.74,8.97"><p>https://github.com/hipe-eval/HIPE-scorer</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,108.93,649.12,375.82,8.97;5,89.29,660.08,62.16,8.97"><p>https://github.com/qurator-spk/sbb_ned/blob/3feacdd60807df2ce45f8d0430f04974cfc79919/notebook/ HIPE-2022.ipynb</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,108.93,671.04,165.36,8.97"><p>https://github.com/hipe-eval/HIPE-2022-eval</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,112.66,253.99,395.17,10.91;10,112.66,267.54,394.53,10.91;10,112.39,281.08,204.64,10.91" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,232.29,253.99,275.54,10.91;10,112.66,267.54,97.01,10.91">Named Entity Disambiguation and Linking on Historic Newspaper OCR with BERT</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Labusch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Neudecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,238.91,267.54,263.87,10.91">Conference and Labs of the Evaluation Forum (CLEF 2020)</title>
		<title level="s" coord="10,173.53,281.08,113.40,10.91">CEUR-WS Working Notes</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2696</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,294.63,394.62,10.91;10,112.66,308.18,393.33,10.91;10,112.66,321.73,394.53,10.91;10,112.66,335.28,80.57,10.91" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,369.83,294.63,137.44,10.91;10,112.66,308.18,303.15,10.91">Overview of CLEF HIPE 2020: Named Entity Recognition and Linking on Historical Newspapers</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ehrmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Romanello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Flückiger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clematide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,446.71,308.18,59.28,10.91;10,112.66,321.73,346.66,10.91">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="288" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,348.83,393.32,10.91;10,112.33,362.38,394.94,10.91;10,112.28,375.93,393.70,10.91;10,112.66,389.48,394.53,10.91;10,112.66,403.03,397.48,10.91;10,112.36,419.02,152.76,7.90" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,353.42,348.83,152.57,10.91;10,112.33,362.38,373.98,10.91">Introducing the HIPE 2022 Shared Task: Named Entity Recognition and Linking in Multilingual Historical Documents</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ehrmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Romanello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clematide</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-99739-7_44</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-99739-7_44.doi:10.1007/978-3-030-99739-7_44" />
	</analytic>
	<monogr>
		<title level="m" coord="10,112.28,375.93,393.70,10.91;10,112.66,389.48,18.17,10.91">Advances in Information Retrieval: 44th European Conference on IR Research, ECIR 2022</title>
		<meeting><address><addrLine>Stavanger, Norway; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2022">April 10-14, 2022. 2022</date>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,430.13,393.33,10.91;10,112.66,443.67,393.33,10.91;10,112.66,457.22,268.39,10.91" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,350.46,430.13,155.53,10.91;10,112.66,443.67,192.91,10.91">High-performance OCR for printed English and Fraktur using LSTM networks</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ul-Hasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Al-Azawi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,332.29,443.67,173.70,10.91;10,112.66,457.22,154.02,10.91">2013 12th international conference on document analysis and recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="683" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,470.77,393.33,10.91;10,112.66,484.32,393.57,10.91;10,112.33,497.87,29.19,10.91" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,246.39,470.77,259.60,10.91;10,112.66,484.32,236.38,10.91">Calamari -A High-Performance Tensorflow-based Deep Learning Package for Optical Character Recognition</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Reul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Puppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,358.86,484.32,134.01,10.91">Digital Humanities Quarterly</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,511.42,394.52,10.91;10,112.28,524.97,395.56,10.91;10,112.66,538.52,394.53,10.91;10,112.28,552.07,394.91,10.91;10,112.66,565.62,394.53,10.91;10,112.66,579.17,393.33,10.91;10,112.66,592.72,395.01,10.91;10,112.66,608.71,97.35,7.90" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,284.13,579.17,221.86,10.91;10,112.66,592.72,82.07,10.91">QURATOR: Innovative Technologies for Content and Data Curation</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rehm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bourgonje</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hegele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Kintzel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ostendorff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zaczynska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Räuchle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Rauenbusch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Rutenburg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wild</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Seva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Quantz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Böttger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fricke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Thomsen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Paschke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Qundus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Karam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Weichhardt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fillies</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Neudecker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Labusch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rezanezhad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zellhöfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Siewert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bunk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Pintscher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Aleynikova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Heine</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<ptr target="2004.12195" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,112.66,619.81,395.17,10.91;10,112.66,633.36,393.33,10.91;10,112.66,646.91,393.33,10.91;11,112.66,86.97,395.01,10.91;11,112.66,100.52,357.98,10.91" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,286.14,619.81,221.69,10.91;10,112.66,633.36,112.72,10.91">BERT for Named Entity Recognition in Contemporary and Historic German</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Labusch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Neudecker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zellhöfer</surname></persName>
		</author>
		<ptr target="https://corpora.linguistik.uni-erlangen.de/data/konvens/proceedings/papers/KONVENS2019_paper_4.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,249.48,633.36,256.51,10.91;10,112.66,646.91,46.87,10.91;10,244.03,646.91,261.96,10.91;11,112.66,86.97,104.01,10.91">Long Papers, German Society for Computational Linguistics &amp; Language Technology</title>
		<meeting><address><addrLine>KONVENS; Erlangen, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Proceedings of the 15th Conference on Natural Language Processing</note>
</biblStruct>

<biblStruct coords="11,112.66,114.06,394.52,10.91;11,112.66,127.61,395.17,10.91;11,112.66,141.16,393.53,10.91;11,112.66,154.71,172.03,10.91" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,112.66,127.61,395.17,10.91;11,112.66,141.16,320.21,10.91">Named Entity Linking mit Wikidata und GND-Das Potenzial handkuratierter und strukturierter Datenquellen für die semantische Anreicherung von Volltexten</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Menzel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schnaitter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zinck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Petras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Neudecker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Labusch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Leitner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rehm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,441.14,141.16,65.05,10.91;11,112.66,154.71,88.09,10.91">Qualität in der Inhaltserschließung</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="229" to="257" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,168.26,394.53,10.91;11,112.66,181.81,394.62,10.91;11,112.66,195.36,393.33,10.91;11,112.14,208.91,137.88,10.91" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,112.66,181.81,373.01,10.91">Robust named entity recognition and linking on historical multilingual documents</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Boros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">L</forename><surname>Pontes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Cabrera-Diego</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hamdi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Sidère</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,112.66,195.36,273.24,10.91">Conference and Labs of the Evaluation Forum (CLEF 2020)</title>
		<title level="s" coord="11,459.51,195.36,46.48,10.91;11,112.14,208.91,65.11,10.91">CEUR-WS Working Notes</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2696</biblScope>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,222.46,395.17,10.91;11,112.66,236.01,394.61,10.91;11,112.28,249.56,394.91,10.91;11,112.28,263.11,394.91,10.91;11,112.66,276.66,393.33,10.91;11,112.66,290.20,394.53,10.91;11,112.66,303.75,22.69,10.91" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,424.19,222.46,83.64,10.91;11,112.66,236.01,373.61,10.91">Overview of HIPE-2022: Named Entity Recognition and Linking in Multilingual Historical Documents</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ehrmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Romanello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Najem-Meyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clematide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,339.03,263.11,168.16,10.91;11,112.66,276.66,393.33,10.91;11,112.66,290.20,125.83,10.91">Proceedings of the Thirteenth International Conference of the CLEF Association (CLEF</title>
		<title level="s" coord="11,269.88,290.20,184.66,10.91">Lecture Notes in Computer Science (LNCS</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Barrón-Cedeño</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Da San Martino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Degli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Esposti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sebastiani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Macdonald</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Pasi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hanbury</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Potthast</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Faggioli</surname></persName>
		</editor>
		<editor>
			<persName><surname>Ferro</surname></persName>
		</editor>
		<meeting>the Thirteenth International Conference of the CLEF Association (CLEF</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note>Experimental IR Meets Multilinguality, Multimodality, and Interaction</note>
</biblStruct>

<biblStruct coords="11,112.66,317.30,395.17,10.91;11,112.66,330.85,348.36,10.91" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Maud</forename><surname>Ehrmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matteo</forename><surname>Romanello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Flückiger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simon</forename><surname>Clematide</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.6046853</idno>
		<title level="m" coord="11,480.33,317.30,27.50,10.91;11,112.66,330.85,207.17,10.91">CLEF-HIPE-2020 Shared Task Named Entity Datasets</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,112.66,344.40,393.71,10.91;11,112.66,357.95,394.53,10.91;11,112.66,371.50,173.79,10.91" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06347</idno>
		<title level="m" coord="11,433.79,344.40,72.57,10.91;11,112.66,357.95,389.72,10.91">WikiDiverse: A Multimodal Entity Linking Dataset with Diversified Contextual Topics and Entity Types</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
