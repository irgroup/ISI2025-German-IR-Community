<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,148.67,150.71,312.95,14.08;1,211.38,170.80,172.70,10.67">Mixture of Experts Authorship Attribution Notebook for PAN at CLEF 2012</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,240.22,207.67,54.09,9.11"><forename type="first">Michael</forename><surname>Ryan</surname></persName>
							<email>mryan@jgaap.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Evaluating Variations in Language Laboratory</orgName>
								<orgName type="institution">Duquesne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,301.55,207.67,64.88,9.11"><forename type="first">John</forename><surname>Noecker</surname><genName>Jr</genName></persName>
							<email>jnoecker@jgaap.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Evaluating Variations in Language Laboratory</orgName>
								<orgName type="institution">Duquesne University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,148.67,150.71,312.95,14.08;1,211.38,170.80,172.70,10.67">Mixture of Experts Authorship Attribution Notebook for PAN at CLEF 2012</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2FB191D755160FA85E821C6C81937533</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For problems A, B, C, D, I, and J we used three Authorship Attribution techniques; a distance based nearest neighbor, a svm, and method that used a distanced based NN approach to classify sections of a document and classifying based on who wrote majority of the document. These three techniques were then considered experts and each given a vote to determine the author of each document. For problems E and F we clustered paragraphs based on named entity use and then preformed authorship attribution on the nonclustered paragraphs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We will describe two different techniques we applied to generate solutions for the PAN 2012 Author Identification task, specifically the Authorship Attribution subtask. For our analysis, we used a number of our current best performing techniques on each problem, then combined them according to a weighted voting system. For problems E and F, we clustered paragraphs based on shared named entities, then used the technique from problems A, B, C, D, I and J to attribute previously unattributed paragraphs to the appropriate author 2 Materials and Methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Corpus</head><p>We used the PAN 2012 Authorship Attribution corpus, a subtask of the Author Identification task. The corpus is available, in several parts, from the PAN 2012 website, currently hosted at pan.webis.de. It consists of 8 different problem sets. Problems A, C and I are standard, closed-class authorship identification tasks. Problems B, D and J are open-class identification tasks drawn from the same three groups of candidate authors.</p><p>Problems E and F were authorship diarization tasks, wherein an unknown number of candidate authors' writings were commingled, by paragraph, into a single document. The task was then to identify how many authors were present in the sample, and to which author each paragraph belonged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Authorship Attribution</head><p>The authorship attribution task included problems A, B, C, D, I and J. As previously mentioned, A, C and I were closed-class problems, while B, D and J were the corresponding open-class problems, with the training data drawn from the same candidate author set. We used the Java Graphical Authorship Attribution Program (JGAAP) for the analysis. JGAAP follows a multi-step paradigm for closed-class authorship attribution, as follows:</p><p>1. Canonicizers -Preprocessing is done to the documents, for instance to remove punctuation or remove letter case distinctions. 2. Events -Documents are broken down into features, such as characters or words. 3. Analysis -Various nearest-neighbor distance-based analysis or classification methods (e.g. SVMs) can be run on the data, with JGAAP returning the most likely candidate author for each document based on the result.</p><p>Typically we combined the results of the following three methods:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Centroid L1</head><p>This method used no canonicizers and used character 3-gram events. The character 3-grams use a sliding window where each event is a group of 3 adjacent characters. For the analysis stage, we used a centroid-based author model with a nearest-neighbor L 1 distance classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Weka SMO</head><p>This method also used no canonicizers and character 3-gram events.</p><p>Here, we used a Weka sequential minimal optimization trained support vector machine classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Repeated Microdocument Analysis</head><p>For this method, we used Normalize Whitespace and Unify Case canonicizers, with character 11-grams as events. Each document was split into chunks that were roughly 3,000 characters in size, both for training and testing documents. The model was generated using a centroid-based intersection distance on the split training data. Each test document split was then classified, and the overall document was assigned authorship by a "voting" system. For openclass problems, an answer of "none of the above" was given if there was no clear majority author.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Authorship Diarization</head><p>For problems E and F, a different approach was used, which we will describe as "authorship diarization". For these problems, each document contained samples of writing by an unknown number of different authors, where each paragraph came from a single author. The goal here was to group the paragraphs by their author</p><p>For this task, we clustered paragraphs based on their shared named entity usage. In this way, we identified a first set of candidate authors (For instance, we might generate a set of documents with characters named "Bob" and "Sue", another set with characters named "Zebulon" and Xyzyyz" and a third set with "John" and "Michael"). Documents without shared named entity usage were analyzed later.</p><p>After the possible candidate authors had been identified, the repeated microdocument analysis technique from 2.2.3 was used both to consolidate authors and to assign previously unassigned paragraphs to their appropriate author (e.g. perhaps Authors 1 and 3 are actually the same author but from different novels -in this step we would consolidate both into a single author). Instead of using 3,000 character chunks, however, we kept the paragraphs whole.</p><p>This step was done repeatedly, training on the entire identified corpus and testing on all yet-unidentified paragraphs. At each step, the paragraph identified with the highest confidence was then tagged and added to the training step for the next iteration. This process continued until all paragraphs were identified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Table <ref type="table" coords="3,150.59,515.59,3.73,9.11" target="#tab_0">1</ref>, below, gives the total number of correctly identified documents, the number of total documents, and the overall accuracy for each problem.</p><p>The official PAN results give two scores, an "overall percentage" and a "documents correct" score. The overall percentage is given as the mean accuracy across the problem sets. The documents correct score is the total number of documents correctly identified in all of the problems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Observe that the overall by-document accuracy was 88.8%. This was the highest document accuracy in the PAN 2012 results. The mean accuracy was 83.2%, which places the results third in the mean accuracy portion of the competition.</p><p>One particularly interesting thing to consider is that our system is not tuned for open-class attribution problems. A stopgap solution was put together for PAN 2012, but this was the first time we have attempted to apply these techniques to an openclass problem. Removing the "none of the above" problems from the set results in a document accuracy of 91.6% and a mean accuracy of 88.5%.</p><p>We thus propose that the most important future work here is to better adapt the methodology to work on open-class problems. We have such a system in the works, based on a larger number of "experts" in the analysis voting, and hope to have refined it for the next iteration of the PAN competition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,124.80,584.71,345.77,103.67"><head>Table 1 :</head><label>1</label><figDesc>Table 1 thus gives the documents correct score. Results</figDesc><table coords="3,160.08,642.86,280.69,45.52"><row><cell>Problem</cell><cell>Number Correct</cell><cell cols="2">Total Number Accuracy</cell></row><row><cell>A</cell><cell>6</cell><cell>6</cell><cell>100%</cell></row><row><cell>B</cell><cell>7</cell><cell>10</cell><cell>70%</cell></row><row><cell>C</cell><cell>7</cell><cell>8</cell><cell>87.5%</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
