<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,147.48,149.97,311.77,14.67;1,134.16,167.37,327.00,14.67;1,191.40,184.77,212.50,14.67;1,211.32,204.95,172.53,11.03">Educated guesses and equality judgements: using search engines and pairwise match for external plagiarism detection Notebook for PAN at CLEF 2012</title>
				<funder ref="#_7bqUQ4X">
					<orgName type="full">JISC</orgName>
				</funder>
				<funder>
					<orgName type="full">EPSRC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,224.64,242.15,43.29,9.02"><forename type="first">Lee</forename><surname>Gillam</surname></persName>
							<email>l.gillam@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,274.80,242.15,53.99,9.02"><forename type="first">Neil</forename><surname>Newbold</surname></persName>
							<email>n.newbold@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,336.24,242.15,45.82,9.02"><forename type="first">Neil</forename><surname>Cooke</surname></persName>
							<email>n.cooke@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,147.48,149.97,311.77,14.67;1,134.16,167.37,327.00,14.67;1,191.40,184.77,212.50,14.67;1,211.32,204.95,172.53,11.03">Educated guesses and equality judgements: using search engines and pairwise match for external plagiarism detection Notebook for PAN at CLEF 2012</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E92A61B4D8211C9EFADD3FD9DA830702</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the approaches taken to the two subtasks of Candidate Document Retrieval and Detailed Comparison, in the Plagiarism Detection track at PAN 12. For the first of these, we describe how we used a combination of frequency and a variation of a contrastive corpus measure to select keywords with which to make queries to the ChatNoir search system; for the second, we provide an overview of how we re-used software that had previously featured in PAN 11. We comment specifically on how effective both approaches were, and what steps we might take to improve if the competition remains substantially similar next time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The PAN activity first appeared in 2007 as an International Workshop on Plagiarism Analysis, Authorship Identification, and Near-Duplicate Detection, and subsequently evolved its name to Uncovering Plagiarism, Authorship, and Social Software Misuse<ref type="foot" coords="1,193.08,501.91,3.00,5.44" target="#foot_0">1</ref> . The first competitive activity in PAN occurred in 2009, separated into an external task that involved checking document content against a collection, and an intrinsic component apparently looking at writing style changes within a document and which seems to have migrated into the authorship task. External detection is consistent with what many would typically think of as plagiarism detection -finding sources that match parts of the content of a particular documentand it is this task with which we are largely concerned in this paper.</p><p>The external detection part of the plagiarism task remained relatively consistent from 2009 to 2011, treating a collection of (a few) tens of thousands of documents in almost equal quantities of suspicious documents that may contain plagiarized material and source documents from whence the this material may have been taken. It was useful first to construct some kind of index of the set of source documents, and then to use this to respond to queries generated from the suspicious documents. For example, the n-gram based approach of Grozea and Popescu 2011 would seem to suggest an inverted index keyed on n-gram<ref type="foot" coords="2,255.48,149.23,3.00,5.44" target="#foot_1">2</ref> , with initial result ranking according to the number of matches in each source document and a threshold above which further analysis is undertaken. Clearly, for such n-gram based approaches, the size of the index will depend on the value for N and the extent of overlapping; the speed of match will depend on how many n-grams are selected from the suspicious document. With the number of overlapping n-grams that could be created from just one document, and subsequent analytical steps, it is easy to understand why various researchers would want to make use of high performance clusters to undertake such tasks.</p><p>In contrast to these previous iterations of PAN, in 2012 the plagiarism detection tasks seem to be encouraging a search-engine-first keyword-based approach, with subsequent checking. Here, the subsequent checking could be undertaken interactively, in reducing the quantity being checked once a "hit" is obtained, or by constructing a sub-index of all the retrieved material or, as seems to be implied, by pairwise checking <ref type="bibr" coords="2,201.84,299.03,124.17,9.02" target="#b1">(Elsayed, Lin and Oard, 2008)</ref>. Part of the rationale for this shift seems to be the difficulty that previous participants have had in processing the relatively small (GB) collections of data, which would make scaling to larger (TB) collections quite onerous. There is already a multitude of online resources that claim to detect plagiarism and are built above common search engines. But such systems seem to operate best when extended phrases are used. Of course, extended phrases come at additional cost to the search engine provider creating a tension between accuracy and utility. This also presents a very different problem: where previously an exhaustive match could be made within the entire collection, exhaustive match now depends on the ability to make the search engine return a set of results from its index that are useful for this purpose. The simplest way to cater for an exhaustive match is to make more queries until the gain achieved is suitably diminished. The costs of undertaking such a task are therefore devolved into the costs of search, and each query-response-retrieval will take some time, and the cost of match with this resulting subset (and subsequent cycles of these two as required). But if the former fails (recall=0), the latter is not possible.</p><p>In this paper, we outline the approach taken at the University of Surrey to these two tasks of Candidate Document Retrieval and Detailed Comparison in the Plagiarism Detection track at PAN 12. In section 2, we describe how we use a combination of frequency and a contrastive corpus measure to select keywords with which to make queries to the ChatNoir search system and discuss the results obtained, which show high values for recall offering good scope for the match phase. In section 3, we provide an overview of how we re-used software that had previously featured in PAN 11 and comment on several simple optimizations that would have reduced quite significantly the time taken for our comparisons. We decided against optimization as this would have moved us away from our goal of developing a scalable indexing system. Section 4 concludes the paper with considerations for future work, including how we envisage processing the entire ClueWeb collection for full-document search and the initial steps we have made towards this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Candidate Document Retrieval</head><p>Candidate Document Retrieval involves creating a set of queries for a text that might be useful in retrieving other texts from a search engine that offer matches to that text. The extent to which an individual text retrieved in such offers a match to the original can only be known through subsequent processing. In this case, the search engine is Chatnoir, developed by the Webis Group at Bauhaus-Universität Weimar, which indexes the ClueWeb09 collected in January and February 2009 and comprising of some 1,040,809,705 web pages in 10 languages (25 TB of uncompressed data) <ref type="foot" coords="3,204.48,257.59,3.00,5.44" target="#foot_2">3</ref> .</p><p>In formulating our approach, we explored the extent to which several extant text analysis components of the System Quirk toolset could offer something for such a task. We considered how to make use of n-grams (here we might include termbearing, but also collocation patterns and concordances), frequency analysis, contrastive corpus analysis, and indicative text summarization, all of which are variously offered through the applications Ferret, ColloQator, KonTEXT and Summit.</p><p>Our initial efforts suggested little gain from indicative summarization, although this will be worth exploring again now that our approach produces a reasonable return. And since we were unable to identify how phrases could be used with Chatnoir, this seemed to enforce an approach based entirely on locating keywords. Furthermore, initial tests with Chatnoir showed some unexpected outcomes. Consider, for example, the text with ClueWeb ID 255104308; this is the first response to a query comprising the two words flushmate and gpf, and contains four instances of the first word and two of the second in about 500 words. The whole text is part of a product catalogue with numerous outbound links and just one contiguous paragraph of text that contains neither of these terms. The second result contains 36 instances (30 and 6 respectively) in about 600 words. Moreover, the first term appears sooner in the second document than in the first. The ratios and positions seem unusual, suggesting either that word count might be being produced after removing data such as prices (formed of numbers and punctuation), or there is an unclear interplay between the ranking function (BM25) and the term proximity approach, about which it is not possible to find details of how bucket sizes are produced for Chatnoir. Having observed this, and given the likely passage-based formulation of the document set, it was considered that extracting terms at document-level would be doubly unlikely to obtain good results, and so subsequent efforts would work on smaller fragments to see if proximity could be exploited.</p><p>Core to our approach is enhanced weirdness (ew, eqn.1), obtained by squaring the relative frequency in our scaled weirdness equation (e.g. <ref type="bibr" coords="3,393.60,591.35,77.10,9.02;3,124.80,602.87,58.17,9.02" target="#b2">Gillam, Tariq and Ahmad, 2005)</ref>. Scaled weirdness has been used variously as a contrast between relative frequencies in general and specialist language to flag terms; here its purpose is to generate sets of search terms which have a lower likelihood of appearing in general text and therefore would be expected to occur in fewer documents in an index.</p><p>2 ) 1 (</p><formula xml:id="formula_0" coords="4,237.24,155.51,233.48,34.84">SL GL SL GL N f f N ew + = (1)</formula><p>where f SL is the frequency of a word in the (split) text, f GL is its frequency in the 100m tokens of the British National Corpus (BNC), and N SL and N GL are the token counts of the (split) text and the BNC respectively. This is used in the approach briefly outlined below:</p><p>For each suspicious text, T:</p><p>1. Split to sub-texts S by number of lines l.</p><p>2. For each sub-text in S, generate queries Q by: a. Rank by ew. b. Select the top 10 terms, and re-rank by frequency c. top frequency-ranked word paired with the next m words 3. Retrieve texts for each query in Q.</p><p>Consider the first text in the test collection (004 -Table <ref type="table" coords="4,373.68,349.43,4.12,9.02" target="#tab_0">1</ref>) without line splitting applied, and ranked by ew to obtain the top 10 terms (2a, above) . The table demonstrates how this promotes certain terms that are both highly frequent and unusual (toilet and toilets), which by weirdness values alone would not feature in this table; a frequency of 4 is sufficient for caulk to feature, showing the bias towards weirdness. Re-ranking list by frequency would select toilet for pairing with the others, and the first query of toilet and toilets (2c) to use to retrieve texts (3). Our competition run used l=25 (lines), m=4. Creating the queries takes just 4 minutes for the entire set of 32 test texts. Processing is readily automated via a set of (Linux) shell scripts that make use of split and KonTEXT, and formulate the queries to Chatnoir, obtain responses in JSON, process the JSON to obtain the required LongID, retrieve the texts, and pass them to the pairwise matching. For this last part, we use our own pairwise comparison approach, outlined in the next section, to select resources to submit for evaluation.</p><p>For suspicious-text-004 from the test collection, we craft 90 queries which retrieve some 729 files. 21 of these files contain matches of various sizes, and with a degree of duplication (see Table <ref type="table" coords="5,216.24,195.59,3.63,9.02" target="#tab_1">2</ref>). We report all matches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>are reported by start position in each text (offset) and length of detection (length), with prefixes indicating whether this was in the suspicious (su_) or source document (so_).</head><p>The suspicious file is 37472 characters. Of note in these results: 1. Overlap and duplication can be significant 2. Large overlap between first segment of 82916556 and the result for 82916557.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Large undetected segment from 11915 to 17202</head><p>Using this approach, we achieved the highest values for recall (0.5567) of downloaded and retrieved sources amongst the competitors when including nearduplicates(see Table <ref type="table" coords="5,209.28,520.91,3.63,9.02" target="#tab_3">3</ref>), though a near-duplicate is as yet undefined.  However, these results are not necessarily a reliable indication of performance -a second (unreported) attempt was made to see whether a variation to l and m might improve performance; quantities of downloads likely reflects the extra work done here, and is also likely to be a factor in what otherwise appears to be an underreporting of sources that constrained our precision at 0.2493 (against a possible 0.2775).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Detailed Comparison</head><p>In <ref type="bibr" coords="6,147.84,177.35,77.47,9.02" target="#b0">Cooke et al (2011)</ref> we described various aspects of our system as used for the external plagiarism detection task, which we stated could process the entire PAN11 collection within relatively short timescales, and which was still able to produce a reasonable degree of matching performance (4th place, with PlagDet=0. <ref type="bibr" coords="6,433.83,211.79,36.89,9.02;6,124.80,223.31,271.25,9.02">2467329, Recall=0.1500480, Precision=0.7106536, Granularity=1.0058894)</ref>. We also stated that we were unable to disclose too many details about the approach due to a patent application that was in progress. The patent was since filed in the US (US13/307,428, filed 30th November 2011), but we are waiting for the review of that filing before disclosing the simple method used at its core.</p><p>What we can state at this time is that we do not: 1. remove stopwords per se since we consider them to be an important part of the signature of the text (our approach to one part of the authorship attribution task builds out our consideration of this importance, albeit in a rather different way). 2. use methods of encryption or hashing in order to create same-length keys for the data 3. break the text into large numbers of short character-based or word-based ngrams. Indeed, we consider that such approaches have a relatively high computational cost which rapidly become prohibitive when dealing with large volumes of data (e.g. if we were attempting to deal directly with the ClueWeb09 data).</p><p>Our approach uses the same parameters and values as for PAN11. Parameters that we could tune were:</p><p>• Minimum detection run length (RR) -to remove segments less than 50 words • Maximum Stitch distance (SS) -to address granularity in joining segments • Minimum cosine score (CS) -to verify segment similarity. We kept the value of these parameters consistent with those used in PAN11 for comparison purposes, and since our previous parameter sweeps had not demonstrated much by way of gain across a range of values. The values used were: RR=50 (minimum suggested length of plagiarism); SS=900, CS=0.75.</p><p>Our approach to translated texts merely made use of a post run adjustment by character ratio of the source to the translation via a shell script run subsequent to matching to modify the character positions in the XML results.</p><p>The software was constructed in a relatively ad hoc manner previously, using a combination of shell scripts, Python and C++ code. We only put effort into forming this into batch programs, which leaves a large number of inherent overheads in the interfacing of components -e.g. launching a shell to launch Python code that in turn loads in a shared object file and coverts calls from Python to C++ for its operation, and then runs other separate components, for example, for our stitching approach and cosine matching -each of which involves another intermediary file-based communication. Added to this processing cost, our first pass search usually derives matches from large collections and builds up an index from this and the cosine matching reopens those files implicated in order to undertake verification. So where this is a match, the files are being processed twice. Comparing pairs in the training corpus took, on average, 7.8 seconds but are reported at 9.4 seconds for the test corpus on an apparently more capable system. Since many plagiarism detection systems in previous years had reported relatively slow processing of large collections, we did not perceive a need to optimize our code for speed although clearly there is plenty of scope for this and we would expect at least half the processing time to be necessary. We also make no attempt to use threads or multiple cores to achieve better throughput.</p><p>The software was provided, under licence, to the organizers for evaluation purposes. The Zip file containing the program occupies around 240kb, and requires python 2.7.1. It was built for a 64-bit Ubuntu platform and appears to have been usable by the organizers without requiring modifications to the build.</p><p>Performance results from the training corpus are shown below (Table <ref type="table" coords="7,426.48,287.51,3.59,9.02" target="#tab_4">4</ref>). We did not produce results for '05_translation', as this was being handled differently in the test phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test</head><p>Plagdet Score Our competition results were largely as expected. We achieved the highest precision, and 5 th best granularity, but low recall (Table <ref type="table" coords="7,349.56,461.75,3.63,9.02" target="#tab_5">5</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recall Precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detailed Comparison Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rank</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>In contrast to these previous iterations of PAN, the 2012 external plagiarism detection tasks seem to be encouraging a search-engine-first keyword-based approach, with subsequent checking. The ability to undertake match, then, depends on the educated guesses made of suitable queries that will impel the search engine to offer up the right documents. It is not possible to recover from bad guesses, only to keep guessing in the hope that something will be found. Whilst the approach to crafting the guesses can be made systematic, obtaining results depends on the extent of pollution/noise contained by the search engine -i.e. the number of results that would be produced ahead of the results sought in each case. It is quite possible, also, that constraints within the search system or the implementation itself would prevent a specific text being returned for a particular query. In addition, it could be quite possible to produce good pairwise match results in relatively short time without really performing pairwise match -e.g. using a bag of words or n-gram approach when sentences are within a few words length of each other, but as previous PANs have shown, not being able to readily scale such an approach.</p><p>Our results have shown that we have a decent strategy for educated guesses, but that our pairwise matching suffers under obfuscation. We could readily reduce the number of queries required by dropping the need to query for segments already covered by results; on the other hand, we should look to formulate more queries for an unmatched segment. For Pairwise matching, and for our approach in general, we need to begin handling obfuscation. However, such approaches are not really in our preferred direction of travel, which is towards full-document (private) search. And having obtained ClueWeb09 dataset, and formed an approach for this which we believe will readily scale, hope to be able to report on this at the next PAN.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,136.20,447.30,329.00,181.79"><head>Table 1 : Enhanced weirdness applied to suspicious document 004 in the test collection</head><label>1</label><figDesc></figDesc><table coords="4,191.76,447.30,211.86,164.77"><row><cell>Term</cell><cell>fSL</cell><cell>fSL / NSL</cell><cell>w</cell><cell>Ew</cell></row><row><cell>gpf</cell><cell>9</cell><cell>0.001403</cell><cell>140401.2</cell><cell>196.9161</cell></row><row><cell>flushmate</cell><cell>5</cell><cell>0.000779</cell><cell>78000.65</cell><cell>60.77657</cell></row><row><cell>toilet</cell><cell>161</cell><cell>0.02509</cell><cell>1590.64</cell><cell>39.90853</cell></row><row><cell>caulk</cell><cell>4</cell><cell>0.000623</cell><cell>62400.52</cell><cell>38.897</cell></row><row><cell>actuator</cell><cell>8</cell><cell>0.001247</cell><cell>31200.26</cell><cell>38.897</cell></row><row><cell>flange</cell><cell>20</cell><cell>0.003117</cell><cell>10064.6</cell><cell>31.36855</cell></row><row><cell>shims</cell><cell>6</cell><cell>0.000935</cell><cell>31200.26</cell><cell>29.17275</cell></row><row><cell>toilets</cell><cell>64</cell><cell>0.009974</cell><cell>2021.069</cell><cell>20.15715</cell></row><row><cell>inducer</cell><cell>8</cell><cell>0.001247</cell><cell>13866.78</cell><cell>17.28756</cell></row><row><cell>composting</cell><cell>13</cell><cell>0.002026</cell><cell>7511.173</cell><cell>15.21665</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,123.12,218.66,350.49,192.75"><head>Table 2 : Detections applied to suspicious document 004 in the test collection. Results</head><label>2</label><figDesc></figDesc><table coords="5,123.12,218.66,350.49,175.76"><row><cell></cell><cell>su_offset</cell><cell>su_length</cell><cell>so_offset</cell><cell>so_length</cell><cell>Notes on duplicates</cell></row><row><cell>82916556</cell><cell>315</cell><cell>5665</cell><cell>1149</cell><cell>4923</cell><cell>Contained:</cell></row><row><cell></cell><cell>12584</cell><cell>1412</cell><cell>7725</cell><cell>1367</cell><cell>340124840 (622+1326);</cell></row><row><cell></cell><cell>36534</cell><cell>330</cell><cell>6583</cell><cell>340</cell><cell>82916586 (1868+337)</cell></row><row><cell>82916557</cell><cell>3711</cell><cell>8204</cell><cell>10</cell><cell cols="2">6193 Significant overlap: 655702818,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>926517445, 1037219213,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1082516754, 1234439206,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1283114388 (all 4924+3705);</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>512814224, 1337033767</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(4933+3641)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Contained:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>456806343 (4933+3135)</cell></row><row><cell>32718446</cell><cell>17202</cell><cell>1040</cell><cell>632</cell><cell>1499</cell><cell></cell></row><row><cell>811900</cell><cell>19134</cell><cell>7285</cell><cell>3034</cell><cell cols="2">7233 Exact:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>811901, 811902</cell></row><row><cell>102839362</cell><cell>30070</cell><cell>6472</cell><cell>10</cell><cell cols="2">8732 Contained:</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>476400740, 601432982</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(30301+4339)</cell></row><row><cell>74735759</cell><cell>37035</cell><cell>431</cell><cell>5752</cell><cell>438</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,124.80,579.47,345.82,18.42"><head>Table 3 : Precision and recall values for our approach; 0.5567 was the highest recall value achieved in the task..</head><label>3</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,124.80,334.25,345.90,107.77"><head>Table 4 : Performance results for the training corpus. Note that we have yet to fully address the problem of obfuscation, hence low values in recall.</head><label>4</label><figDesc></figDesc><table coords="7,415.32,334.25,51.54,8.96"><row><cell>Granularity</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,124.80,498.62,345.78,74.19"><head>Table 5 : Performance results for the training corpus. Note that we have yet to fully address the problem of obfuscation, hence low values in recall.</head><label>5</label><figDesc></figDesc><table coords="7,147.24,498.62,301.48,45.85"><row><cell></cell><cell>PlagDet</cell><cell>Precision</cell><cell>Recall</cell><cell>Granularity</cell><cell>Runtime* [Seconds/Pair]</cell></row><row><cell>9</cell><cell cols="3">0.3088109 0.8984268 0.1903951</cell><cell>1.0243572</cell><cell>9.4009198</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,132.24,686.09,217.65,8.15"><p>Presumably the N of PAN now comes from the conjunction.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,130.20,665.45,340.42,8.15;2,133.32,675.89,337.50,8.15;2,133.32,686.21,248.73,8.15"><p>However, their paper does not readily mention the value of N used for their competitive effort (the associated presentation suggests N=2,3 .. 16). Without such information it's not readily possible to estimate the size of their index or challenge in building it.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,130.08,686.09,240.00,8.15"><p>See: http://lemurproject.org/clueweb09.php/[accessed, 14/8/2012]   </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors gratefully acknowledge the prior contributions of <rs type="person">Peter Wrobel</rs> and <rs type="person">Henry Cooke</rs> to the formulation of the codebase used for this task. This work has been supported in part by the <rs type="funder">EPSRC</rs> and <rs type="funder">JISC</rs> (<rs type="grantNumber">EP/I034408/1</rs>), and we are very grateful to <rs type="institution">Amazon Web Services (AWS)</rs> for providing a supporting grant for this research and for competition use of both EC2 and EBS services.</p><p>We also gratefully acknowledge the invaluable efforts of the PAN12 organizers in crafting and managing the tasks.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7bqUQ4X">
					<idno type="grant-number">EP/I034408/1</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,136.20,556.55,334.50,9.02;8,124.80,568.07,298.77,9.02" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,441.84,556.55,28.86,9.02;8,124.80,568.07,162.69,9.02">A high performance plagiarism detection system</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gillam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Wrobel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Al-Obaidli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,294.36,568.07,124.55,9.02">Proc. of the 3rd PAN workshop</title>
		<meeting>of the 3rd PAN workshop</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,136.20,579.47,334.42,9.02;8,124.80,590.99,345.79,9.02;8,124.80,602.51,346.03,9.02;8,124.80,614.03,15.06,9.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,314.16,579.47,156.46,9.02;8,124.80,590.99,117.30,9.02">Pairwise document similarity in large collections with MapReduce</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,252.60,590.99,218.00,9.02;8,124.80,602.51,309.95,9.02">Proc. 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers</title>
		<meeting>46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="265" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,136.20,625.55,334.39,9.02;8,124.80,637.07,345.71,9.02;8,124.80,648.47,125.58,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,319.32,625.55,151.27,9.02;8,124.80,637.07,35.71,9.02">Terminology and the Construction of Ontology</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gillam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tariq</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Ahmad</surname></persName>
		</author>
		<idno>ISSN 1569-9994</idno>
	</analytic>
	<monogr>
		<title level="j" coord="8,168.24,637.07,52.26,9.02">Terminology</title>
		<idno type="ISSN">0929-9971</idno>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="81" />
			<date type="published" when="2005">2005</date>
			<publisher>John Benjamins Publishing Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
