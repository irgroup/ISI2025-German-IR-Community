<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,138.69,115.90,337.98,12.90;1,282.96,133.83,49.44,12.90;1,223.43,153.68,168.50,10.75;1,149.09,209.41,162.63,7.77">An Overview of the Traditional Authorship Attribution Subtask Notebook for PAN at CLEF 2012</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,281.88,190.08,51.60,8.64"><forename type="first">Patrick</forename><surname>Juola</surname></persName>
							<email>juola@mathcs.duq.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Associates</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,138.69,115.90,337.98,12.90;1,282.96,133.83,49.44,12.90;1,223.43,153.68,168.50,10.75;1,149.09,209.41,162.63,7.77">An Overview of the Traditional Authorship Attribution Subtask Notebook for PAN at CLEF 2012</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0A85D4570A5251F581EE7C9EC4B27D12</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the Traditional Authorship Attribution subtask of the PAN/CLEF 2012 workshop. As a followup to our subtask at PAN/CLEF 2011 (Amsterdam), we established a new corpus for analysis for 2012 (Rome). The new corpus differed in several ways from the previous subtask:</p><p>-Both the number and size of documents were decreased -The documents were taken from a different genre (fiction, represented by the Feedbooks.com site) -The documents were no longer marked up extensively -A new sub-sub-task was added : Authorship clustering. In this new problem (related to Òintrinsic plagiarismÓ) participants were given a text of mixed authorship and asked to determine which paragraphs came from which authors. The resulting corpus consisted of eight problems, including three closed-class authorship attribution problems, three open-class (the set of correct answers included Ònone of the aboveÓ), and two clustering problems. Twenty-five teams participated in this subtask from many different parts of the world. Detailed results are available on the Web at pan.webis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Background</head><p>Although traditionally authorship studies are done on the basis of close reading for stylistic detail, "nontraditional" or statistical authorship attribution has been around long enough <ref type="bibr" coords="1,185.83,524.93,11.32,8.64" target="#b5">[6,</ref><ref type="bibr" coords="1,197.15,524.93,7.55,8.64" target="#b0">1,</ref><ref type="bibr" coords="1,204.70,524.93,7.55,8.64" target="#b3">4,</ref><ref type="bibr" coords="1,212.24,524.93,7.55,8.64" target="#b4">5,</ref><ref type="bibr" coords="1,219.79,524.93,7.55,8.64" target="#b6">7]</ref> to have developed into a traditional research problem of its own, especially in comparison to new tasks such as sexual predator identification <ref type="bibr" coords="1,447.62,536.89,10.58,8.64" target="#b1">[2]</ref>. The task is well-understood (given a document, determine who wrote it) although amenable to many variations (given a document, determine a profile of the author; given a document pair, determine whether they were written by the same author; given a document, determine which parts of it were written my any specific person) and the motivation is clear. Applications for this technology include not only plagiarism detection but also historical inquiry, journalism, and legal dispute resolution (forensics). TREC-style competitive analyses of authorship methods using a standardized corpus have been around since at least 2004 <ref type="bibr" coords="1,210.58,632.53,10.58,8.64" target="#b2">[3]</ref>.</p><p>This competition follows on the heels of a previous subtask at the PAN 2011 conference, but differs from that competition in several ways:</p><p>1.1 Both the number and size of documents were decreased.</p><p>In last year's competition, the corpus consisted of several thousand relatively small documents, with distractor sets consisting of hundreds of authors. This was considered to create impracticalities for many participants, especially those that relied upon machineaided instead of fully automatic analysis. We have instead focused on a smaller group of larger documents, perhaps more typical of the type of cases usually analyzed by "traditional" close reading,.</p><p>1.2 The documents were taken from a different genre.</p><p>Last year's corpus was taken from the Enron email corpus; this years instead was collected from the free fiction collection published by Feedbooks.com, including both classic fiction that is now out-of-copyright as well as (fiction, represented by the Feedbooks.com site). This of course introduces the standard issue of analysis-by-Google, but that's a very difficult problem to avoid short of generating content to order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.3</head><p>The documents were no longer marked up extensively.</p><p>As no one made particular use of any markup last year, the documents were simply released as text documents.</p><p>1.4 A new sub-sub-task was added : Authorship clustering.</p><p>In the most major change from last year, we created a new style of problem related to what has in prior competitions been called "intrinsic plagiarism." In this new problem participants were given a text of mixed authorship and asked to determine which paragraphs came from which authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Problems and Corpus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Traditional authorship attribution</head><p>There were six problems of straightforward authorship attribution, presented as three pairs, representing the closed-and open-class versions of the attribution problem, respectively. In a closed-class attribution problem, a document is presented along with a set of sample authors, and the analyst or computer is asked to determine which of the sample authors wrote that document as a forced-choice scenario. In the open-class version, by contrast, "none of the above" is an acceptable answer and some of the documents to be analyzed were, in fact, written by someone other than the set of sample authors.</p><p>Problems A and B both used the same training set : two samples (each, six samples in total) by each of three authors A, B, and C. All samples were between 1800 and 6060 words. The test set for problem A consisted of six samples (two by each author); the test set for problem B consisted of a different set of six samples (two by each author) as well as four "none of the above" samples for a total of ten.</p><p>Problems C and D similarly used a shared training set, but had a larger number of authors (and hence documents). The training set had 8 authors (again, two samples per author) but were generally larger, ranging up to about 13000 words. The test set for problem C contained one sample for each training author (hence 8 documents); the test set for problem D contained one sample per in-class author, plus nine out-of-class ("none of the above") for a total of test documents.</p><p>Problems G and H were disregarded due to security issues (the test data was inadvertently released along with the training data) and replaced by problem I and J. These problems again shared a common training set, but were of novel (or at least novella) length, ranging from about 40,000 words up to about 170,000. There were 14 authors represented (the most of any task in this collection). Test data for problem I consisted of fourteen additional novels, one per candidate author; test data for problem J contained sixteen additional novels, one per candidate author plus two out-of-class novels.</p><p>The number of documents per problem approximately matches perceived difficulty of the problems; more distractor authors are more difficult, and of course open-class is more difficult than closed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Authorship clustering</head><p>Problems E and F focused on the clustering problem; as such, no "training" data is actually needed since the point of clustering is to learn group authors based only on document-internal evidence. However, "sample" data was made available for both problems E and F to illustrate the format used.</p><p>Problem E contained intermixed paragraphs (in random order) from several different documents by different authors (one document per author; problem E1 contained two authors, E2 contained three, and E3 contained 4. Problem F by comparison contained four documents, three of which contained a single intrusive passage of several consecutive paragraphs and the final one was singly authored. All documents were segmented by paragraphs and all authorship changes occurred at paragraph boundaries. No attempt was make to control for subject or authorial voice, making this task easier than many other related plagiarism corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Grading</head><p>Normal information retrieval measures such as precision, recall, and F-score are not really applicable to multiple (non-binary) categorization environments. Instead, documents were graded on a simple percentage-correct basis; i.e. a submission that correctly categorized four of the six documents in problem A would score 4/6 or 67% on that problem. Similarly, each paragraph in problems E/F was treated as a separate "document" and evaluated as correct or incorrect in its assignment.</p><p>Problem E was a little more complicated; each cluster identified by the participant was matched to an existing correct partition, and the number of paragraphs contained in both partitions were counted as "correct." If the number of partitions identified was incorrect, some partitions would be unmatched. Because matching can be done in many ways, we used the matching that generated the highest overall score. For example, if all the odd numbered paragraphs were by author A, and all of the even numbers were by author B, and a participant submitted two clusters, one containing paragraphs 1-15 and another containing 16-30, there would be two possible ways of matching : odd-low (and even-high) or odd-high (and even-low). Matching odd-low generates 8 matches (paragraphs 1,3,5,7,9,11,13,15) as does even-high, for a total of 16/30 correct. Matching the other way generates 7 correct paragraphs, hence the participant would have scored 16/30 or 53% correct, the higher score.</p><p>Overall grading of a corpus like this can be slightly controversial because different approaches can yield different results on different problems. Like a decathlon (or in this case an octathlon), how does one combine different scores to an overall measure? SInce the key point of a competition such as this one is not to award medals but to encourage exploration of the field, we have taken a simple, agnostic approach to scoring and present two separate scores. The first "overall" score is the average of the individual percentages correct on all eight problems. The second "documents correct" score is the percentage of documents correctly analyzed. This second approach thus weights larger (more documents) problems more heavily, but we expect (correctly, as it turns out) that good methods will score well across all problems.</p><p>Details of the corpus are presented as table <ref type="table" coords="4,322.90,323.58,3.74,8.64" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Participants and results</head><p>Twenty-five submissions were received from twelve teams. Results are summarized in table <ref type="table" coords="4,156.44,584.19,3.74,8.64" target="#tab_1">2</ref>. A full breakdown of results including per-problem results can be obtained from the PAN website (pan.webis.de). Some participants received low score due to partial submissions. For example, the Brooke submission participated in only problems E and F (authorship clustering) and did quite well on problem F in particular (41/90 for problem E, 68/80 for problem F), but did not participate in any traditional authorship problems and hence scored 0 on those. consider having documents "written to order" in order either to prevent cheating-by-Google or to ensure tighter control over the documents (especially for clustering/intrinsic plagiarism context? -Can/should we find a way for "traditional" language scholars to participate and compare their hand analyses with the results of the computer runs? -What other types of (sub)tasks should be attached to the competitive study of authorship attribution? Examples of potential problems might include authorship profiling (e.g. was this document written in New York, London, or California? Was it written by a man or a woman?), document dating, and so forth. -How should future competitions be judged?</p><p>Despite the necessarily incomplete nature of any fixed-corpus study of authorship, PAN/CLEF 2012 has produced a valuable set of results and basis for discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head><p>We want to thank all the PAN 2012 and CLEF 2012 organisers for their hard work and support, as well as the participants of the competition for their patience and suggestions. This material is based upon work supported by the National Science Foundation under Grant No. OCI-1032683. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. This of course applies to the organizers and participants as well. More tersely put, the errors mine, the thanks theirs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,136.16,358.16,246.94,128.62"><head>Table 1 .</head><label>1</label><figDesc>Corpus construction summarized</figDesc><table coords="4,136.16,358.16,105.03,117.76"><row><cell cols="3">Task Training docs Test docs</cell></row><row><cell>A</cell><cell>6</cell><cell>6</cell></row><row><cell>B</cell><cell></cell><cell>10</cell></row><row><cell>C</cell><cell>16</cell><cell>8</cell></row><row><cell>D</cell><cell></cell><cell>17</cell></row><row><cell>E</cell><cell>n/a</cell><cell>90</cell></row><row><cell>F</cell><cell>n/a</cell><cell>80</cell></row><row><cell>G</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell>H</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell>I</cell><cell>28</cell><cell>14</cell></row><row><cell>J</cell><cell></cell><cell>16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,136.16,239.01,237.36,293.01"><head>Table 2 .</head><label>2</label><figDesc>Summary results of subtaskAs predicted, both scores yielded the same overall set of "winners", albeit in a different order. The top three participants were identical for both scores, as presented in table3.</figDesc><table coords="5,136.16,239.01,175.02,282.14"><row><cell>TEAM</cell><cell cols="2">Overall score Documents correct</cell></row><row><cell>Vilarino 1</cell><cell cols="2">50.45839169 59.75103734</cell></row><row><cell>Vilarino 2</cell><cell cols="2">62.13264472 63.07053942</cell></row><row><cell>de Graaff 1</cell><cell cols="2">57.54989496 21.99170124</cell></row><row><cell>de Graaff 2</cell><cell cols="2">39.47610294 15.76763485</cell></row><row><cell>de Graaff 3</cell><cell cols="2">2.941176471 1.659751037</cell></row><row><cell cols="3">Brainsignals 86.37429972 81.32780083</cell></row><row><cell>Ruseti</cell><cell cols="2">57.40239846 22.82157676</cell></row><row><cell cols="3">CLLE-ERSS 1 70.8092612 77.593361</cell></row><row><cell cols="3">CLLE-ERSS 2 59.12931839 68.87966805</cell></row><row><cell cols="3">CLLE-ERSS 3 64.70967554 64.3153527</cell></row><row><cell cols="3">CLEE-ERSS 4 67.66208567 73.02904564</cell></row><row><cell>Lip6 1</cell><cell cols="2">59.76759454 21.99170124</cell></row><row><cell>Lip6 2</cell><cell cols="2">54.40782563 20.33195021</cell></row><row><cell>Lip6 3</cell><cell cols="2">52.67069328 19.50207469</cell></row><row><cell cols="3">Bar-Ilan Univ 83.40321545 81.74273859</cell></row><row><cell>Sapkota</cell><cell cols="2">58.35346639 21.57676349</cell></row><row><cell>EVL Lab</cell><cell cols="2">81.67221055 87.96680498</cell></row><row><cell>Surrey</cell><cell cols="2">53.98663632 75.5186722</cell></row><row><cell>Zech terms</cell><cell cols="2">43.17664566 15.76763485</cell></row><row><cell>Zech stylo</cell><cell cols="2">22.91404062 8.713692946</cell></row><row><cell>Zech stats</cell><cell cols="2">30.11379552 11.2033195</cell></row><row><cell>Brooke</cell><cell cols="2">16.31944444 45.22821577</cell></row><row><cell>Zech I-2</cell><cell>17.96875</cell><cell>50.62240664</cell></row><row><cell>Zech I-3</cell><cell>17.03125</cell><cell>48.13278008</cell></row><row><cell>Zech I-4</cell><cell cols="2">16.47569444 46.47302905</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,134.77,217.02,345.83,190.90"><head>Table 3 .</head><label>3</label><figDesc>Highest scoring participants 5 Future work Assuming that future PAN participation in the CLEF experimental framework is desired (an assumption the author supports), there remain several further issues to explore. -Both this and the previous PAN competition have focused exclusively on English documents. Should future competitions include non-English languages, and if so, which, and in what representation? -Similarly, what genre(s) should be represented, and what sources could be used to get documents in those genres? What size of problems should be done, including both number of authors and number and size of training/test documents? -Should we</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.13,142.87,334.17,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,197.52,142.87,78.73,7.77">Authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">I</forename><surname>Holmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,281.78,142.87,111.33,7.77">Computers and the Humanities</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="87" to="106" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,153.83,307.21,7.77;7,146.47,164.79,326.88,7.77;7,146.47,175.75,273.13,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,233.16,153.83,212.17,7.77;7,146.47,164.79,85.89,7.77">Overview of the international sexual predator identification competition at pan-2012</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Inches</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,431.25,164.79,42.10,7.77;7,146.47,175.75,198.69,7.77">CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Womser-Hacker</surname></persName>
		</editor>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,186.71,317.49,7.77;7,146.47,197.67,334.12,7.77;7,146.47,208.63,300.07,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,180.84,186.71,149.98,7.77">Ad-hoc authorship attribution competition</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Juola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,349.03,186.71,106.59,7.77;7,146.47,197.67,334.12,7.77;7,146.47,208.63,182.57,7.77">Proc. 2004 Joint International Conference of the Association for Literary and Linguistic Computing and the Association for Computers and the Humanities (ALLC/ACH 2004)</title>
		<meeting>2004 Joint International Conference of the Association for Literary and Linguistic Computing and the Association for Computers and the Humanities (ALLC/ACH 2004)<address><addrLine>Göteborg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-06">June 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,219.59,319.33,7.77;7,146.47,230.55,23.90,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,180.84,219.59,78.73,7.77">Authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Juola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,265.10,219.59,175.18,7.77">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,241.50,317.87,7.77;7,146.47,252.46,333.80,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,279.08,241.50,173.63,7.77">Computational methods in authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Schler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,146.47,252.46,263.58,7.77">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="26" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,263.42,324.86,7.77;7,146.47,274.38,148.70,7.77" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Q</forename><surname>Morton</surname></persName>
		</author>
		<title level="m" coord="7,200.01,263.42,262.98,7.77;7,146.47,274.38,38.78,7.77">Literary Detection: How to Prove Authorship and Fraud in Literature and Documents</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Scribner&apos;s</publisher>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.13,285.34,339.86,7.77;7,146.47,296.30,253.62,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,202.76,285.34,182.01,7.77">A survey of modern authorship attribution methods</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,391.09,285.34,86.90,7.77;7,146.47,296.30,174.43,7.77">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="538" to="556" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
