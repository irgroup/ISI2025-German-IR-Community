<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,154.69,115.90,305.98,12.90;1,158.77,133.83,297.81,12.90;1,223.43,153.68,168.50,10.75">Vote/Veto Classification, Ensemble Clustering and Sequence Classification for Author Identification Notebook for PAN at CLEF 2012</title>
				<funder>
					<orgName type="full">Austrian Ministry of Economics and Labor</orgName>
				</funder>
				<funder ref="#_4RdQrGU">
					<orgName type="full">(Marie Curie)</orgName>
				</funder>
				<funder ref="#_wbdSuA2">
					<orgName type="full">European Commission</orgName>
				</funder>
				<funder>
					<orgName type="full">Austrian Research Promotion Agency FFG</orgName>
				</funder>
				<funder>
					<orgName type="full">State of Styria</orgName>
				</funder>
				<funder ref="#_KXXj5se">
					<orgName type="full">Austrian Ministry of Transport, Innovation and Technology</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,198.93,190.08,50.94,8.64"><forename type="first">Roman</forename><surname>Kern</surname></persName>
							<email>rkern@tugraz.at</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Knowledge Management</orgName>
								<orgName type="institution">Graz University of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Know-Center GmbH Graz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.65,190.08,60.50,8.64"><forename type="first">Stefan</forename><surname>Klampfl</surname></persName>
							<email>sklampfl@know-center.at</email>
							<affiliation key="aff1">
								<orgName type="institution">Know-Center GmbH Graz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,352.48,190.08,59.48,8.64"><forename type="first">Mario</forename><surname>Zechner</surname></persName>
							<email>mzechner@know-center.at</email>
							<affiliation key="aff1">
								<orgName type="institution">Know-Center GmbH Graz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,154.69,115.90,305.98,12.90;1,158.77,133.83,297.81,12.90;1,223.43,153.68,168.50,10.75">Vote/Veto Classification, Ensemble Clustering and Sequence Classification for Author Identification Notebook for PAN at CLEF 2012</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A36990CF414E414F14516730638EB356</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Author Identification task for PAN 2012 consisted of three different sub-tasks: traditional authorship attribution, authorship clustering and sexual predator identification. We developed three machine learning approaches for these tasks. For the two authorship related tasks we created various sets of feature spaces, where individual differences in writing styles are assumed to surface in just a subset of these spaces. The challenge there was to combine these feature spaces to enable the machine learning algorithms to detect these differences across multiple feature spaces. In the case of authorship attribution we combined the results of multiple base classifiers by following a supervised vote/veto meta classifier approach. For the intrinsic plagiarism/authorship clustering subtask we used an unsupervised ensemble clustering approach in order to combine information from several feature spaces. In the sexual predator identification task we applied a supervised sequence classification approach to uncover temporal patterns within chat conversations by categorizing not only the offending messages, but also the reactions to these offending messages.</p><p>A vs B (328)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the following we provide a detailed description of our approaches to solve the three subtasks of the Author Identification track of PAN 2012. This lab report is structured as follows: In section 2 we present our supervised vote/veto classification approach to solving the traditional authorship attribution subtask. These meta classifiers collect information from various heterogeneous feature spaces, a method we extended in section 3 for the unsupervised authorship clustering task by employing an ensemble clustering approach. Finally, in section 4 we describe a sequence classification approach for identifying offending messages by potential sexual predators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Vote/Veto Classification for supervised authorship attribution</head><p>The task of supervised authorship attribution is to assign a previously unseen text to an author. For this author there may already exist a set of reference text documents. In this case the problem is labelled closed class. If the author of the text is possibly not in the set of known authors, the problem is labelled as open class. For the open class problem we simply added texts from different data sets to the training data set and labelled all new authors as "other".</p><p>For the PAN 2012 competition we re-applied our system from the previous year <ref type="bibr" coords="2,134.77,179.09,11.62,8.64" target="#b4">[5]</ref> with only minor modifications. This allowed to compare the two different data sets, instead of comparing algorithmic approaches. In 2011 the test corpora had consisted of emails and the task had been to identify the original sender of these mails. Due to specific characteristics found in such texts, our system contained features specifically engineered for such a scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-Classifiers &amp; Feature Sets</head><p>We decided to use techniques from the field of supervised machine learning as a base for the authorship attribution task. In order to utilise a classification algorithm one has to transform the input data into a representation suitable for such an algorithm. Therefore the input data needed to be transformed into features, organised within feature spaces. Thereby these features were further organised in feature sets, which combined multiple features into coherent sets of similar features.</p><p>We defined ten different feature spaces (for a more detailed description please refer to the original paper <ref type="bibr" coords="2,220.17,333.65,10.62,8.64" target="#b4">[5]</ref>): Basic Statistics, Token Statistics, Grammar Statistics, Stop-Word Terms, Pronoun Terms, Slang Terms, Intro-Outro Terms, Bigram Terms, Unigram Terms, and Terms. Some of these feature spaces encode statistical properties of the text, others are sensitive to the topic. Stop-words and pronouns are expected to serve a function within the sentence, but not to be specific to certain topics. Neither are slang terms, intro-outro terms, or grammar, which rather reflect the writing style of the author. On the other hand, terms, unigrams, and bigrams should be indicative for specific topics.</p><p>As most of these feature sets were not compatible with each other, the individual features could not simply be combined to build a single feature space. Therefore we developed a meta-classifier, which took the output of individual base classifiers to reach a final classification decision. Each of these base classifiers operated in a single feature space.</p><p>During the training phase the performance of each base classifiers was assessed using a ten-fold cross-validation approach. The precision and recall for each class was recorded, where each author in the training set was represented as a single class. If the precision in the test phase exceeded a given threshold t p for a class, the base classifier became eligible to vote for this class. If the recall exceeded another threshold t r for a class, the base classifier could veto against this class. In the training phase the meta-classifier was just responsible to record the individual performance of the base classifiers for each class.</p><p>In the classification phase, where a previously unseen text document needed to be assigned to one of the authors (or an author not being present in the training data set for the open-class problem), the role of the meta-classifier was to combine the results of the base-classifiers. One of the base classifiers was treated differently than the others, its output was taken directly without taking into account its performance in the training phase, and the individual probabilities for the classes were taken as initialisation. For all the others, their a posteriori classification results were taken into consideration, as well as their assessed training performance. If a base classifier C assigned a probability p C i to a specific class i it was compared to another set of two thresholds. In the case that the base classifier might vote for this class and the probability exceeded the threshold p p , the probability was multipled by w p and added to the class probability. When the base classifier was eligible for a veto and the probability was smaller than p r , then the product of w r (1 -p C i ) was deducted from the class probability. Thus the final score for each author was then the combination of the individual classification results of the base classifiers.</p><p>Configurations The modular nature of our system allowed us to assess the influence of the different feature spaces on the overall performance. We were especially interested on the influence of the content based feature spaces in relation to the pure statistical feature spaces. Generally, statistical features can be considered to be less dependent on the actual topic or domain of the text than content based features. Thus features like terms will rather allow to detect a change in topic instead of actually identifying individual writing styles.</p><p>We defined three different configurations for our system, each of them is a combination of base-classifiers:</p><p>terms In this configuration all feature spaces were combined and the results of the terms feature space was an initialisation for the author scores. One would expect that this configuration should work best in cases where changes in authorship are directly coupled to changes in topic and each author used different content words. stylo The second configuration was a combination of statical features and term features, which should carry little semantics. All three statistical feature spaces were used in combination with the stop-word and the pronoun feature spaces. The token statistics feature space was used as initialisation for the author scores. stats The final configuration consisted only of the statistical based feature spaces, again using the token statistics for initialisation. This configuration was expected to provide the worst performance in cases where terms are indicative for authorship. In cases where different authors produce texts which are topically related this configuration should not be affected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of the Data Sets</head><p>To compare the data sets from the PAN 2011 with the PAN 2012 workshops we conducted a feature analysis on the individual base classifiers. The list of features for each feature space was ranked according to their information gain.</p><p>The results are shown in Table <ref type="table" coords="3,258.78,536.71,3.74,8.64" target="#tab_0">1</ref>.</p><p>In the case of the basic statistics feature space the differences between the two data sets are obvious. For the authorship of emails the layout appears to have played an important role, while for the PAN 2012 data set the length of the text was the most relevant factor. For the token statistics feature space the most discriminative features appear to be more similar. A closer look reveals that the histogram of token lengths between the two data sets appear to be different, at least in regard to their ability to distinguish between individual authors. The grammar statistics between the two data set varied by great degree, especially with regard to the depth of the sentence parse tree, which had been a good indicator for the PAN 2011 data set, but only ranked as the 11th most discriminant feature for the PAN 2012 data set. Performance on the Test Data Set The official numbers from the organisers allowed us to compare the performance of our system for the three configurations (see Table <ref type="table" coords="4,469.80,436.06,3.60,8.64" target="#tab_1">2</ref>). The configuration which performed best is the one which incorporated the terms feature spaces. This can be seen as an indicator that the test set contained authors with disjunct topics, thus there was little overlap in their content words. The performance of the stylo configuration is expected to lie between the terms and stats configuration. The most plausible reason why this configuration performed worst is that for such a scenario the thresholds and weighting factors were not properly optimised. This can be seen as an indicator that for authorship attribution the domain does has an influence and should be taken into account when tuning an algorithm for optimal performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Ensemble clustering for unsupervised authorship identification</head><p>In the intrinsic plagiarism/authorship clustering task of the author identification track we were given a number of texts, each of which was written by at least two different authors. The task was to recover the author of individual paragraphs in an unsupervised manner, yielding a clustering or partition of the paragraphs. (For simplicity each text was segmented into paragraphs such that each paragraph was written by exactly one author.)</p><p>Table <ref type="table" coords="5,201.06,115.83,3.36,8.06">3</ref>. Stylometric features and used in the authorship clustering task.</p><p>feature name description alpha-chars-ratio the fraction of total characters in the paragraph which are letters digit-chars-ratio the fraction of total characters in the paragraph which are digits upper-chars-ratio the fraction of total characters in the paragraph which are upper-case white-chars-ratio the fraction of total characters in the paragraph which are whitespace characters type-token-ratio ratio between the size of the vocabulary (i.e., the number of different words) and the total number of words <ref type="bibr" coords="5,306.16,192.36,11.62,6.05" target="#b12">[13]</ref>  To solve these unsupervised authorship attribution problems we followed the hypothesis that individual differences in the writing style of different authors may emerge only in non-trivial combinations of heterogeneous features. We therefore extended the idea in the previous section of combining information from multiple feature spaces to the more challenging unsupervised case, and employed an ensemble clustering approach. Ensemble clustering (also known as consensus clustering or clustering aggregation) deals with the problem of finding a single clustering that agrees as much as possible with a given set of input partitions of the same data, see, e.g., <ref type="bibr" coords="5,422.27,388.93,15.18,8.64" target="#b10">[11,</ref><ref type="bibr" coords="5,437.44,388.93,7.59,8.64" target="#b3">4]</ref>. For the problem at hand, we found an ensemble clustering approach a suitable choice, since it is able to collect information spread over very heterogeneous feature spaces, whose features are not directly comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature spaces</head><p>In order to extract valuable information from a given input text, we preprocessed the raw paragraphs with an NLP pipeline that performed POS tagging and token normalization. In the next step we extracted features from these individual annotated paragraphs of the documents, yielding one instance per paragraph in each feature space. In particular, the feature spaces we considered were as follows:</p><p>1. frequencies of individual characters (a-z), including digits (0-9) and punctuations (e.g., ",", ".", "?", etc.), 2. frequencies of character bigrams within tokens (e.g., the normalized token "word" consists of the bigrams "wo", "or", "rd"), 3. frequencies of character trigrams within tokens (e.g., "wor", "ord"), 4. frequencies of stopwords and pronouns (e.g., "they", "for", "until"), 5. frequencies of stem-suffices, i.e., endings of words that were removed in a stemming procedure <ref type="bibr" coords="5,217.00,592.31,11.62,8.64" target="#b5">[6]</ref> (e.g., "ible", "ized", "ness"), 6. a variety of stylometric features, as in Table <ref type="table" coords="5,328.56,603.66,3.74,8.64">3</ref>, 7. a number of basic statistical features, as in <ref type="bibr" coords="5,331.93,615.01,11.62,8.64" target="#b4">[5]</ref> and in the supervised authorship attribution subtask desribed in section 2.</p><p>These feature spaces were chosen to reflect the style of the author, rather than the topic, which typically does not change within a plagiarized document. Hence we did not consider the frequency of unigrams or other terms carrying semantic information as features. Similar features have been used in previous studies on authorship attribution and intrinsic plagiarism detection, see, e.g., <ref type="bibr" coords="6,327.19,233.97,15.84,8.64" target="#b14">[15,</ref><ref type="bibr" coords="6,343.03,233.97,7.92,8.64" target="#b8">9,</ref><ref type="bibr" coords="6,350.95,233.97,11.88,8.64" target="#b13">14,</ref><ref type="bibr" coords="6,362.82,233.97,11.88,8.64" target="#b9">10]</ref>. While each feature space grouped together similar features, the feature spaces themselves were quite different from each other. Some feature spaces measured frequencies of characters or character groups, others measured statistical properties. Since many of these features are not directly comparable, we used an ensemble clustering approach to combine information distributed over these feature spaces.</p><p>Ensemble clustering For each of the feature spaces 1-7 we used the standard k-means clustering with k-means++ seed selection <ref type="bibr" coords="6,301.08,331.45,11.62,8.64" target="#b1">[2]</ref> algorithm to obtain individual clusterings.</p><p>Depending on the feature space we also used different similarity functions to measure the distance between two instances/paragraphs: for the frequency based feature spaces 1-5 we employed cosine similarity and for the statistical feature spaces 6 and 7 we chose the standard Euclidean distance. In the latter case we additionally scaled individual features to zero mean and unit variance before feeding them into the clustering algorithm. In order to merge these individual clusterings obtained in each feature space into a final single clustering we then used the median partition approach presented in <ref type="bibr" coords="6,134.77,427.09,15.27,8.64" target="#b11">[12]</ref>. This approach performs another k-means clustering in a meta-space spanned by the individual clusterings, while not using any information from the underlying feature spaces. In this meta-space, two instances have a large distance to each other if they are mostly assigned to different clusters by the individual partitions, and a small distance if the majority of individual clusterings puts them into the same cluster. This clustering aggregation method also allowed the relative weighting of the individual clusterings, enabling us to enforce feature spaces with "good" clusterings and weaken the influence of feature spaces where no discriminative clusterings with respect to the author could be obtained. In order to determine the best weights we performed an exhaustive search: we varied each of the 7 weights from a minimum value of 1 to a maximum value of 5 in steps of size 1. The performance measure we optimized was the average classification accuracy over 10 trial runs of this clustering pipeline on labelled training data for which we used documents from the traditional authorship training corpus (see next subsection for details). We arrived at the weights shown in Table <ref type="table" coords="6,448.63,582.87,3.74,8.64" target="#tab_3">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The training corpus for the intrinsic plagiarism/authorship attribution subtask consisted of just two files: the first file for the mixed task consisted of 6 paragraphs where even and odd paragraphs belonged to two different authors, and the second file for the intrusive task spanned 17 paragraphs where most paragraphs were written by one author, except for two particular consecutive paragraphs belonging to a different (intrusive) author. We found these provided datasets too limited for a reliable evaluation of our methods, and furthermore they were also quite unrealistic for a plagiarism scenario: the combined original documents varied considerably in topic and type (e.g., political theory vs children's fiction). For the development and evaluation of our ensemble clustering approach we therefore used different datasets. More precisely, we generated artificial input data by randomly mixing all paragraphs from two prespecified authors from the traditional authorship attribution dataset with 8 authors ("Problem C").</p><p>Table <ref type="table" coords="7,174.51,206.70,4.98,8.64">5</ref> shows the results of our algorithm on these artificial data. After clustering with k = 2 we assigned the found clusters to the known labels (authors) by minimizing the total number of misclassifications. This assignment problem was solved by the wellknown Hungarian Algorithm, where the cost of labelling a cluster C i with a label L j was the number of samples of class L j outside cluster C i <ref type="bibr" coords="7,364.12,254.52,10.58,8.64" target="#b0">[1]</ref>. We then interpreted clustering performance as classification accuracy, i.e., the percentage of correctly classified instances. The number of instances/paragraphs for each author pair ranged from 328 to 830, and for particular pairs performance values up to 80% were achieved. The average performance over all datasets was 67.15%; this average performance measure was also the optimization objective for the weights in Table <ref type="table" coords="7,355.87,314.29,3.74,8.64" target="#tab_3">4</ref>. These accuracy values might seem not particularly high, but one has to keep in mind that many paragraphs consisted of just a few words, e.g., in a dialogue, and are therefore hard to separate. Table <ref type="table" coords="7,456.89,338.20,4.98,8.64">5</ref> also compares the performance values for selected author pairs obtained by single clusterings in individual feature spaces with the performance of the combined clustering. It can be seen that the combined performance is greater than any performance obtained in a single feature space, demonstrating the power of the clustering ensemble approach. For the sake of completeness we also report the performance on the provided training files in Table <ref type="table" coords="7,187.97,409.94,3.74,8.64" target="#tab_5">6</ref>. In the mixed task there was one misclassified paragraph, whereas in the intrusive task the algorithm correctly recalled both paragraphs from the other author, but incorrectly identified four other paragraphs from the main author.</p><p>We would also like to report the performance of our ensemble clustering method on the provided test corpus. The test corpus consisted of 3 texts of 30 paragraphs each for the mixed task, and 4 texts of 20 paragraphs each for the intrinsic task. In the mixed task each text was written by 2-4 authors; however, this twist in the final challenge assignment violated the original problem description, which had asked to return exactly two clusters. We therefore had limited time to develop methods for estimating also the correct number of clusters, and therefore submitted separate runs with k = 2, k = 3, and k = 4. Among the approaches we tried was to optimize a clustering objective across different k, such as the residual sum of squares or the stability across multiple runs with perturbed input. However, the results are typically biased towards small k, and due to the high-dimensionality of the feature spaces the results were numerically not very stable. As far as the intrusive task is concerned, we did not employ an additional method for finding a consecutive range of paragraphs by the intrusive author, we only returned the result of the clustering for three different runs.</p><p>According to the results published on the PAN website we achieved performances of 70%, 70%, and 65.56% for the runs in the mixed task with k = 2, k = 3, and k = 4, respectively. Among the 14 runs submitted, these runs ranked 6th and 11th. The three runs of the intrusive task yielded performances of 73.25% and 66.25% (twice), 66.10% B vs C ( <ref type="formula" coords="8,248.56,218.26,9.06,6.05">576</ref>  corresponding to rank 10 and 11. These are reasonable results given that we did not check for a consecutive range. If one only counts the best run per group, we rank 6th of 8 for the mixed task and 7th of 8 for the intrusive task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Sequence classification for sexual predator identification</head><p>To identify sexual predators within chats we transformed the problem into a sequence classification task. Each chat was seen as a sequence of individual messages, and each message was represented as a single instance to be classified. In the first pass the classification of messages was done independently from the other messages present within a chat conversation. An important aspect of this classification is that the a posteriori probabilities need to be present together with alternative classification results, again normal An ordinary message not being written by a sexual predator. This should be the most common class of messages. predator A message written by a predator, which is not seen as offending. We did not expect the classification algorithm to be reliably able to distinguish between normal messages and predator messages. offending The first offending message from an predator within a chat conversation.</p><p>After an offending message only the remaining two labels can occur. reaction All messages written by normal chat participants after an offending message were labelled as reaction. This is based on the intuition that in some circumstances the reactions to an offending message is easier to detect than the original messages by the predator. post-offending Any messages which follow an offending message by the predator are labelled as post-offending. This is motivated by the assumption that the behaviour of the predator might change once an offending message has been posted.</p><p>Two exemplary chats should illustrate the labelling sequence for the messages. In the first example a sexual predator takes part in a chat conversation, but behaves like an normal chat participant (see Figure <ref type="figure" coords="9,278.23,560.54,7.61,8.64" target="#fig_0">1a</ref>). Ideally the classification algorithm would still be able to detect the individual messages by the predator. In such a scenario it would be sufficient to identify only one of the predator messages, as this decision could then be propagated to the other messages from the same author as a post-processing step.</p><p>In the second example the chat does not only contain messages from a predator, but at least one of the messages can be attributed to be offending (see Figure <ref type="figure" coords="9,428.08,620.57,7.89,8.64" target="#fig_0">1b</ref>). Here the first of such messages is labelled as offending. All consecutive messages by the same author are marked as post-offending. The remaining messages in the chat are labelled as reaction. A single detected reaction would allow to attribute the other participant in a chat (assuming there are only two participants) as a predator. Alternatively such a case could also be filed for later manual inspection.</p><p>Processing the data set Our proposed approach operated on the level of individual messages and therefore one of the prerequisites was a training corpus containing chats and messages labelled according to our message classes. Unfortunately the training data set supplied by the organisers only contained the information whether an author is a known sexual predator or not. In order to test our approach we had to develop a data set on our own. Therefore we manually annotated the supplied training data set to follow our message classes. Due to time and resource constraints we were not able to annotate all the training data, but just a small subset of it. In the first iteration we created a development data set for the initial tests and creating the sequence labelling classification system.</p><p>To boost our productivity in annotating the data, we developed a web-based system to to ease the process of identifying offending messages. Unfortunately we did not have an expert on sexual predator behaviour at our disposal therefore a single member of our team spent a day to annotate a subset of the training data set. The result of this effort was used to train the classifiers for the submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification Algorithm &amp; Feature Sets</head><p>There are a number of classification algorithms which do support the integration of sequence information. Examples for this type of algorithms are Hidden Markov Models and Conditional Random Fields. An alternative approach is to use a base classification algorithm, which does not provide native support of sequence information, but to postprocess the output of the base classifiers. We followed the second approach and used the Apache OpenNLP<ref type="foot" coords="10,391.35,407.14,3.49,6.05" target="#foot_0">3</ref> project, which is an open-source implementation of the Maximum Entropy <ref type="bibr" coords="10,360.30,420.76,11.62,8.64" target="#b2">[3]</ref> classification algorithm in combination with a Beam Search strategy <ref type="bibr" coords="10,303.40,432.72,10.58,8.64" target="#b7">[8]</ref>.</p><p>We also used the OpenNLP library to tokenise the message into separate tokens. We did not apply any stop-word processing or further analysis of the text. The tokens were just processed with the Double Metaphone algorithm <ref type="bibr" coords="10,374.06,468.80,10.58,8.64" target="#b6">[7]</ref>. Based on these tokens we built the features for the classification algorithms. For each message we added the individual tokens not only of the message itself, but also all the terms from all the other messages by the same author.</p><p>To incorporate the sequence information we added the classification result of the four preceding messages as features. We also added additional binary features: i) isIni-tialAuthor, if the author of the message initiated the chat, ii) isLastAuthor, if the author has posted the final message, iii) isMostVerboseAuthor, for the messages from the author with the most messages, iv) isFewerAuthors, if the message caused other authors to stop writing, and v) hasTermFromPrevious, if the messages shares at least a single term from the directly preceding message.</p><p>Once the classification result has been processed by OpenNLP we apply a set of simple post-processing rules: i) if there are multiple messages classified as offending, we only keep the label for the first one, all others are labelled as post-offending; ii) if there are messages classified as post-offending, but none as offending, we assign the message that directly precedes the first post-offending message as offending; iii) if there is at least a single message classified as reaction and a single message classified as predator, the predator message which precedes the reaction is assigned the label offending.</p><p>Results on the Development and Test Data Sets We used the development data set to asses the initial performance of our system. Then we split the already small data set into a training part and into a test part, where we made sure that the test did not contain any authors which are used for training. We report the performance figures for our system based on the results on the test part of our development data set (see Table <ref type="table" coords="11,158.57,399.34,3.60,8.64" target="#tab_6">7</ref>). The test set contained a total of three predators, where our classification system together with the postprocessing rule achieved a precision of 0.667 and a recall of 1.</p><p>Due to the limited size of the development data set these figures cannot be regarded as conclusive. Nevertheless one can conclude from the measured performance that the indirect identification via the reaction and the post-offending behaviour appear to be easier to detect that the offending message itself. To train our system for the official submitted runs, we assembled a training data set out of our manually assigned chat conversations. We took all conversations which contained a message identified as offending, added all chats from the same predator to the training set. We then added about the same amount of chats, which did not contain a participant marked as sexual predator (about 600 chats). The results are shown in Table <ref type="table" coords="11,159.14,531.76,3.74,8.64" target="#tab_6">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We presented our systems for three tasks of the PAN 2012 challenge: We employed supervised vote/veto ensemble classification for the classical authorship attribution, unsupervised ensemble clustering for intrinsic plagiarism detection and sequence classification for the identification of sexual predators within chat logs. The source code for our systems can be used under the AGPL license and is available at https://knowminer.knowcenter.tugraz.at/svn/opensource/projects/pan2012.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="9,134.77,221.86,345.83,8.12;9,134.77,233.17,345.82,7.77;9,134.77,244.13,345.82,7.77;9,134.77,255.09,339.62,7.77"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. (a) Classes of the individual messages for a single chat conversation, which contains a sexual predator, but no message can be considered to be offending. (b) Messages and their class labels for a single chat conversation, which contains a sexual predator, but this time a message has been identified as offending. All messages which succeed this message are labelled differently.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,115.83,345.82,180.12"><head>Table 1 .</head><label>1</label><figDesc>Comparison of the most discriminative features for the basic statistics feature space, the token statistics feature space, and the grammar statistics feature space of the PAN 2011 and PAN 2012 data set. Average depth of the sentence parse tree Likelihood of dependency type nsubj 3 Likelihood of the phrase FRAG Likelihood of phrase NP 4 Likelihood of dependency type appos Likelihood of dependency type conj 5 Number of phrase types Likelihood of dependency type possessive</figDesc><table coords="4,134.77,154.97,336.02,117.07"><row><cell>basic statistics:</cell><cell></cell><cell></cell><cell>token statistics:</cell></row><row><cell>PAN 2011</cell><cell>PAN 2012</cell><cell></cell><cell>PAN 2011</cell><cell>PAN 2012</cell></row><row><cell cols="3">1 Paragraph to lines ratio Number of characters</cell><cell>1 Likelihood of proper nouns</cell><cell>Number of tokens</cell></row><row><cell>2 Text to lines ratio</cell><cell cols="2">Number of words</cell><cell>2 Number of tokens</cell><cell>Likelihood of proper nouns</cell></row><row><cell>3 Number of lines</cell><cell>Number of lines</cell><cell></cell><cell>3 Average token length</cell><cell>Average verb length</cell></row><row><cell>4 Empty lines ratio</cell><cell cols="2">Number of stop-words</cell><cell cols="2">4 Likelihood of infrequent word groups Average token length</cell></row><row><cell cols="3">5 Number of paragraphs Number of tokens</cell><cell>5 Likelihood of tokens of length 9</cell><cell>Likelihood of pronouns</cell></row><row><cell>grammar statistics:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PAN 2011</cell><cell></cell><cell cols="2">PAN 2012</cell></row><row><cell cols="2">1 Number of phrases per sentence</cell><cell cols="2">Likelihood of dependency type poss</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,134.77,313.13,345.83,69.63"><head>Table 2 .</head><label>2</label><figDesc>Comparison of the performance of our system (classification accuracy in %) for the three configurations explained in the text.</figDesc><table coords="4,171.75,347.52,255.72,35.23"><row><cell>Configuration</cell><cell>A</cell><cell>B</cell><cell>C</cell><cell>D</cell><cell>I</cell><cell>J</cell></row><row><cell>terms</cell><cell>83.3</cell><cell>50</cell><cell>62.5</cell><cell>35.3</cell><cell>64.3</cell><cell>50</cell></row><row><cell>stylo</cell><cell>33.3</cell><cell>40</cell><cell>25</cell><cell>11.8</cell><cell>35.8</cell><cell>37.5</cell></row><row><cell>stats</cell><cell>66.7</cell><cell>40</cell><cell>25</cell><cell>23.5</cell><cell>35.8</cell><cell>50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,134.77,115.83,345.82,53.99"><head>Table 4 .</head><label>4</label><figDesc>Relative weighting of the individual feature spaces obtained by an exhaustive search in the space {1, 2, 3, 4, 5} 7 .</figDesc><table coords="6,142.29,150.52,324.67,19.29"><row><cell>feature space</cell><cell cols="2">characters bigrams</cell><cell>trigrams</cell><cell cols="4">stopwords stem suffix stylometry basic stats</cell></row><row><cell>weight</cell><cell>5</cell><cell>5</cell><cell>4</cell><cell>5</cell><cell>2</cell><cell>4</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,134.77,389.31,345.83,92.12"><head>Table 6 .</head><label>6</label><figDesc>Performance of our ensemble clustering approach on the provided training corpus. Shown are the confusion matrices for both the mixed and intrusive task: C1 and C2 denote the obtained clusters, and E1/E2 (F1/F2) are the labels/authors of the mixed (intrusive) task. The classification accuracy was 83.33% for the mixed task and 76.47% for the intrusive task.</figDesc><table coords="8,213.01,442.04,189.34,39.39"><row><cell cols="2">mixed (83.33%)</cell><cell cols="2">intrusive (76.47%)</cell></row><row><cell cols="2">C1 C2 Total</cell><cell cols="2">C1 C2 Total</cell></row><row><cell>E1 3 0</cell><cell>3</cell><cell>F1 0 2</cell><cell>2</cell></row><row><cell>E2 1 2</cell><cell>3</cell><cell cols="2">F2 11 4 15</cell></row><row><cell>Total 4 2</cell><cell>6</cell><cell cols="2">Total 11 6 17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="11,134.77,115.83,345.83,118.07"><head>Table 7 .</head><label>7</label><figDesc>Sexual predator identification performance on the development data set and the test data set. The performance for the indirect identification of an offending message is (post-offending, reaction) is far better than the direct identification (offending).</figDesc><table coords="11,149.27,156.82,297.67,77.07"><row><cell cols="2">development data set:</cell><cell></cell><cell></cell><cell>test data set:</cell></row><row><cell>Class</cell><cell cols="3">Count Precision Recall</cell><cell></cell></row><row><cell>normal predator</cell><cell>3,117 29</cell><cell cols="2">0.955 0.995 0.3 0.103</cell><cell>Task</cell><cell>Precision Recall</cell></row><row><cell>offending post-offending reaction</cell><cell>52 216 275</cell><cell cols="2">0 0.871 0.847 0 0.959 0.764</cell><cell>Identify predators Identify predator line</cell><cell>0.1476 0.6920 0.0855 0.2074</cell></row><row><cell>Identify predators</cell><cell>2</cell><cell>0.667</cell><cell>1</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="10,144.73,657.08,93.98,7.77"><p>http://opennlp.apache.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has been funded by the <rs type="funder">European Commission</rs> as part of the <rs type="programName">TEAM IAPP</rs> project (grant no. <rs type="grantNumber">251514</rs>) within the <rs type="programName">FP7 People Programme</rs> <rs type="funder">(Marie Curie)</rs>. The <rs type="projectName">Know-Center</rs> is funded within the <rs type="programName">Austrian COMET Program</rs> under the auspices of the <rs type="funder">Austrian Ministry of Transport, Innovation and Technology</rs>, the <rs type="funder">Austrian Ministry of Economics and Labor</rs> and by the <rs type="funder">State of Styria</rs>. COMET is managed by the <rs type="funder">Austrian Research Promotion Agency FFG</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_wbdSuA2">
					<idno type="grant-number">251514</idno>
					<orgName type="program" subtype="full">TEAM IAPP</orgName>
				</org>
				<org type="funded-project" xml:id="_4RdQrGU">
					<orgName type="project" subtype="full">Know-Center</orgName>
					<orgName type="program" subtype="full">FP7 People Programme</orgName>
				</org>
				<org type="funding" xml:id="_KXXj5se">
					<orgName type="program" subtype="full">Austrian COMET Program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table <ref type="table" coords="8,158.35,115.83,3.36,8.06">5</ref>. Performance evaluation of our ensemble clustering approach on artificial training data. We selected two authors from the traditional authorship attribution dataset with 8 authors ("Problem C") and reported clustering performance as classification accuracy. The shown accuracy is the average over 10 runs; the numbers in brackets denote the number of total instances/paragraphs for the particular author pair. For the bold entries the performances obtained in individual feature spaces are expanded below. These numbers demonstrate that combining feature spaces achieves a better performance than any clustering in a single feature space alone. authors perf. authors perf. authors perf. authors perf.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.61,255.28,304.90,7.77;12,150.95,266.24,328.04,7.77;12,150.95,277.20,258.38,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,380.52,255.28,66.99,7.77;12,150.95,266.24,191.01,7.77">A semi-supervised cluster-and-label approach for utterance classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Albalate</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Suchindranath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Suendermann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Minker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,359.68,266.24,119.32,7.77;12,150.95,277.20,190.12,7.77">Workshop Proceedings of the 6th International Conference on Intelligent Environments</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,287.91,336.14,7.77;12,150.95,298.87,303.48,7.77;12,150.95,309.83,66.49,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,252.47,287.91,176.32,7.77">k-means ++ : The Advantages of Careful Seeding</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,434.92,287.91,43.83,7.77;12,150.95,298.87,258.65,7.77">Proceedings of the eighteenth annual ACMSIAM symposium on Discrete algorithms</title>
		<meeting>the eighteenth annual ACMSIAM symposium on Discrete algorithms</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2007</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1027" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,320.55,291.86,7.77;12,150.95,331.51,155.17,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,200.49,320.55,230.25,7.77">A Maximum Entropy Approach to Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,150.95,331.51,95.40,7.77">Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,342.22,310.08,7.77;12,150.95,353.18,183.02,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,287.43,342.22,80.30,7.77">Clustering aggregation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Tsaparas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,373.67,342.22,79.02,7.77;12,150.95,353.18,118.78,7.77">ACM Transactions on Knowledge Discovery from Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,363.90,333.46,7.77;12,150.95,374.86,297.86,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,328.05,363.90,148.02,7.77;12,150.95,374.86,46.72,7.77">Vote/Veto Meta-Classifier for Authorship Identification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Seifert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zechner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Granitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,225.17,374.86,197.50,7.77">3nd International Competition on Plagiarism Detection</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,385.57,337.98,7.77;12,150.95,396.53,316.88,7.77;12,150.95,407.49,246.06,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,325.36,385.57,155.23,7.77;12,150.95,396.53,316.88,7.77">External and Intrinsic Plagiarism Detection using a Cross-Lingual Retrieval and Segmentation System Lab Report for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Muhr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zechner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Granitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,173.37,407.49,197.50,7.77">2nd International Competition on Plagiarism Detection</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,418.21,332.31,7.77;12,150.95,429.16,72.47,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,192.80,418.21,142.96,7.77">The double metaphone search algorithm</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Philips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,341.72,418.21,133.21,7.77">CC PLUS PLUS USERS JOURNAL</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="38" to="43" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,439.88,325.54,7.77;12,150.95,450.84,69.24,7.77" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="12,212.71,439.88,251.71,7.77">Maximum Entropy Models for Natural Langual Ambiguity Resolution</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ratnaparkhi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="12,142.61,461.55,331.76,7.77;12,150.95,472.51,311.72,7.77;12,150.95,483.47,23.90,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,207.24,461.55,218.14,7.77">Intrinsic plagiarism detection using character n-gram profiles</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,443.07,461.55,31.30,7.77;12,150.95,472.51,269.60,7.77">3rd PAN Workshop: Uncovering Plagiarism, Authorship and Social Software Misuse</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="38" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,494.19,337.58,7.77;12,150.95,505.15,127.53,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,284.58,494.19,99.90,7.77">Intrinsic plagiarism analysis</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,390.18,494.19,89.64,7.77;12,150.95,505.15,38.63,7.77">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="82" />
			<date type="published" when="2010-01">Jan 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,515.86,326.19,7.77;12,150.95,526.82,289.12,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,228.41,515.86,240.01,7.77;12,150.95,526.82,64.97,7.77">Cluster ensembles -a knowledge reuse framework for combining multiple partitions</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,221.45,526.82,139.44,7.77">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="583" to="617" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,537.54,322.12,7.77;12,150.95,548.50,243.29,7.77" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,277.03,537.54,137.67,7.77">Combining Multiple Weak Clusterings</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Topchy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Punch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,420.53,537.54,43.83,7.77;12,150.95,548.50,170.07,7.77">Proceedings IEEE International Conference on Data Mining</title>
		<meeting>IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="331" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,559.21,331.15,7.77;12,150.95,570.17,243.95,7.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,241.40,559.21,231.99,7.77;12,150.95,570.17,39.16,7.77">How variable may a constant be? Measures of lexical richness in perspective</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Tweedie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Baayen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,208.11,570.17,109.61,7.77">Computers and the Humanities</title>
		<imprint>
			<biblScope unit="page" from="323" to="352" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,580.89,335.88,7.77;12,150.95,591.85,293.66,7.77;12,150.95,602.80,317.71,7.77;12,150.95,613.76,101.36,7.77" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,325.70,580.89,152.41,7.77;12,150.95,591.85,92.47,7.77">External and intrinsic plagiarism detection using vector space models</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zechner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Muhr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Granitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,261.89,591.85,182.73,7.77;12,150.95,602.80,317.71,7.77;12,150.95,613.76,33.39,7.77">PAN09 -3rd Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse and 1st International Competition on Plagiarism Detection</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="47" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.24,624.48,313.72,7.77;12,150.95,635.44,311.17,7.77;12,150.95,646.40,236.07,7.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,291.17,624.48,164.79,7.77;12,150.95,635.44,255.72,7.77">A Framework for Authorship Identification of Online Messages: Writing-Style Features and Classification Techniques</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,412.82,635.44,49.31,7.77;12,150.95,646.40,152.39,7.77">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="378" to="393" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
