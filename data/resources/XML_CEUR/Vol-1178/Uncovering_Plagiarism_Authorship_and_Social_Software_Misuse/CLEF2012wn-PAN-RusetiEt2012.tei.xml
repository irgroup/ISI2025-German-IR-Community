<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.04,149.97,332.71,14.68;1,220.08,167.37,155.15,14.68;1,211.44,187.55,172.47,8.51">Authorship Identification Using a Reduced Set of Linguistic Features Notebook for PAN at CLEF 2012</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,242.52,224.80,52.00,8.96"><forename type="first">Stefan</forename><surname>Ruseti</surname></persName>
							<email>stefan_ruseti@yahoo.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University Politehnica of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,301.08,224.80,63.10,8.96"><forename type="first">Traian</forename><surname>Rebedea</surname></persName>
							<email>traian.rebedea@cs.pub.ro</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University Politehnica of Bucharest</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.04,149.97,332.71,14.68;1,220.08,167.37,155.15,14.68;1,211.44,187.55,172.47,8.51">Authorship Identification Using a Reduced Set of Linguistic Features Notebook for PAN at CLEF 2012</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0DF286B74148042D8B04C8E43473AAB4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The proposed solution for authorship attribution combines a couple of the most important features identified in previous research in this domain with classification algorithms in order to detect the correct author. We consider that the most relevant aspect of our work is the small number of linguistic features and the use of the same framework to solve both the open and the closed class authorship problem, by only changing the classification algorithm. This approach obtained an overall 77% accuracy with regard to the total number of correctly classified documents.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The problem of authorship identification or attribution of text documents has been widely studied in the last decades, especially in the last 20 years, but the solutions are not mature enough to consider the problem solved. Nowadays, the Web offers very large amounts of texts to be used as corpora for authorship identification, but it also provides many different types of discourse that may be analyzed: from narratives and e-mails to online conversations and social network updates. It is obvious that each type of discourse should be treated independently; nevertheless even the problem of identifying the author of large narrative texts is far from being closed.</p><p>Authorship attribution may be divided into two different subtasks: determining the most descriptive features of the texts under consideration, then applying a classification algorithm in order to detect the most probable author <ref type="bibr" coords="1,400.80,565.24,10.69,8.96" target="#b0">[1]</ref>. The methods for conducting the classification stage range from principal component analysis and cluster analysis to support vector machines (SVMs) and neural networks.</p><p>The proposed solution has started from examining the most important features and most powerful classification algorithms developed for PAN 2011 <ref type="bibr" coords="1,398.04,611.32,10.60,8.96" target="#b1">[2]</ref>. Of course, as the discourse type has changed from e-mails to short narratives, the feature set also had to be changed as described in the next section. An extensive set of features used for previous works in authorship attribution has also been presented in <ref type="bibr" coords="1,415.32,645.76,10.69,8.96" target="#b2">[3]</ref>. The most successful team in PAN 2011 has used a broad set of features, including several specific to e-mail conversations, and a maximum entropy classifier <ref type="bibr" coords="1,418.56,668.80,10.69,8.96" target="#b3">[4]</ref>. Another approach used a semi-supervised approach based just on character trigrams and SVM in order to determine documents in the test corpus that had the highest probability of being written by one of the authors. These documents were then added to the training set and the classifiers were retrained <ref type="bibr" coords="2,273.72,172.60,10.69,8.96" target="#b4">[5]</ref>. A last interesting approach that offered good results was a voting approach that used several classifiers that might elect of veto directly the author of a document. The results of all the classifiers were combined <ref type="bibr" coords="2,453.84,195.64,10.69,8.96" target="#b5">[6]</ref>.</p><p>The following section describes the small feature set chosen for solving the proposed problem, while section 3 briefly presents the choice of the classifier. Then, we describe the results that were obtained and how the answers for the open problem were derived and wrap up with some conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Reduced Set of Linguistic Features</head><p>In the multitude of approaches to author identification over the last years, a large list of features was used, ranging from character and lexical to the semantic layer. Of course, some of them need to be problem specific <ref type="bibr" coords="2,338.76,329.92,10.69,8.96" target="#b2">[3]</ref>. From all these features, we extracted a reduced set that proved to be suitable for the type of discourse in the PAN 2012 corpus. Thus, features related to the layout of the text or spelling errors were irrelevant because the corpus was composed of short narratives (or novels), which are usually written correctly and most times are also edited.</p><p>The remaining features used to describe the texts and to solve the classification problem are:</p><p>• Character trigrams -the most common 100 trigrams from the training corpus were selected and then the distribution of each text has been computed. • POS bigrams and trigrams -the most common 50 bigrams and 100 trigrams were selected. The POS tagging was realized using the RITa POS Tagger 1 . However, as most other taggers, it returned very specific POS tags that did not offer enough generality needed to extract each author's style. For example, nnps was for proper noun plural, but only the first letter from these tags was considered sufficient and descriptive for the author's writing style. • Suffixes -the most common 32 English suffixes were counted in each text. The percentage of suffixed words from all words was recorded as well. To check if a word has a certain suffix, we first checked to see if the word was composed using a suffix by using a stemmer. After this test, we only checked if the word ended with a suffix from the list. This approach is not 100% correct, but it had a very small error rate that did not influence the classification. • Word length -word lengths from 1 to 15 were counted, any word longer than 15 characters was considered in the 15 category. These features should capture the author's vocabulary richness. • Syntactic complexity and structure -we used the Stanford parser 2 to create the parsing tree for each sentence and to extract the syntactic dependencies. The average sentence length, the average and the maximum tree depth, the average and the maximum distance between the elements of a dependency were recorded. Each dependency type was also counted to try to represent the author's predisposition for certain syntactic structures. • Percentage of direct speech -some authors may tend to use more dialogue in their texts than others, so also took this under consideration. Sometimes, this feature can be irrelevant because the type of the text can also determine the percentage of dialogue. In the evaluation stage, this feature increased the overall accuracy, so we decided to use it.</p><p>Each feature was normalized so that the lengths of the texts do not interfere with the results. Because there were only 2 training texts for each author, we split each one into smaller pieces. The cross-validation for only 2 examples would have been very irrelevant, because only one text would remain as a training example, so no generalization could be made. For the sets A and C 5kB pieces were used, and for set I 50kB. This produced 100-200 training examples for each author, so a better generalization could be made by the classifier. The splitting took into account sentences, so it would not interfere with the syntactic features. Also, the last slice could not be smaller than half of the average slice size in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Classification Task</head><p>For classification we used a SVM implementation available in WEKA, the Sequential Minimal Optimization (SMO) algorithm. The test documents were also split into pieces of the same size as the training data, and the most common result was used as the output of the classifier each document.</p><p>For the open class problem, we used the same classifier, but with a logistic regression model for the output because we needed a more exact probability estimation for each author in the training set. If the classifier offered an expected value over 0.75 for an author for a text on average (the text was also split into pieces of different sizes), the classifier outputs that class; otherwise the answer is "other".</p><p>We have also tried a Naive Bayes classifier, but the results were not as good as for SVM when using cross-validation on the training set. However, it offered very close results, so it can also be a viable classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>In order to determine the reduced feature set presented in section 2, different combinations of features have been evaluated and we have selected the ones that had the best results in cross-validation. Different split sizes were used for texts as well.</p><p>The experimental validation concentrated only on the closed attribution problem. In the 10-fold cross-validation, the results were very good: • 100% -set A (using 5kB and 10kB slices) • 96.6% -set C (using 5kB slices)</p><p>• 99.5% -set I (using 20kB and 50kB slices) However, these results are not very relevant, because the training examples are from the same document, so one expects many linguistic similarities between them. It was clear that the results on the test corpus will be significantly under these levels. However, the described approach turned out to yield good results on the PAN 2012 test corpora as well, both for the closed and open problems: As expected, the results are not as good as for cross-validation, but the depreciation was not very steep. Thus, our solution obtained an overall document accuracy of 77%, ranking 3rd in the author attribution competition and an average accuracy over all the 6 test sets of 76%, ranking 7th, at a very close distance from the previous 4 places.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Using only a reduced set of linguistic features has proven to offer good results for the author identification task. These results might have improved by adding more application specific features. Moreover, spiting the training texts proved to be a good solution for training, evaluation and scoring the test documents. The last conclusion is that using logistic regression over the solution designed for the closed class problem provided competitive results for the open class problem as well.</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,130.08,675.69,284.80,8.10"><p>http://www.rednoise.org/rita/documentation/ripostagger_class_ripostagger.htm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,130.08,686.13,174.10,8.10"><p>http://nlp.stanford.edu/software/lex-parser.shtml</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,128.20,538.05,281.05,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,171.84,538.05,79.10,8.10">Authorship attribution</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Juola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,256.44,538.05,87.57,8.10">Found. Trends Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="233" to="334" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,128.20,548.37,342.49,8.10;4,136.20,558.69,334.62,8.10;4,136.20,569.01,87.12,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,222.60,548.37,248.10,8.10;4,136.20,558.69,48.17,8.10">Overview of the International Authorship Identification Competition at PAN-2011</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Juola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,382.44,558.69,44.94,8.10">CLEF 2011</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<imprint/>
	</monogr>
	<note>Notebook Papers/Labs/Workshop</note>
</biblStruct>

<biblStruct coords="4,128.20,579.45,342.52,8.10;4,136.20,589.77,101.58,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,195.24,579.45,190.40,8.10">A survey of modern authorship attribution methods</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,393.24,579.45,77.49,8.10;4,136.20,589.77,31.77,8.10">J. Am. Soc. Inf. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="538" to="556" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,128.20,600.09,342.43,8.10;4,136.20,610.41,334.54,8.10;4,136.20,620.85,172.32,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,361.44,600.09,109.20,8.10;4,136.20,610.41,150.41,8.10">A Multitude of Linguisticallyrich Features for Authorship Attribution</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tanguy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Urieli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Calderone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hathout</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Sajous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,136.20,620.85,42.30,8.10">CLEF 2011</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<imprint/>
	</monogr>
	<note>Notebook Papers/Labs/Workshop</note>
</biblStruct>

<biblStruct coords="4,128.20,631.17,342.54,8.10;4,136.20,641.49,326.63,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,245.16,631.17,206.60,8.10">Author Identification Using Semi-supervised Learning</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Kourtis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,290.64,641.49,42.18,8.10">CLEF 2011</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<imprint/>
	</monogr>
	<note>Notebook Papers/Labs/Workshop</note>
</biblStruct>

<biblStruct coords="4,128.20,651.81,342.49,8.10;4,136.20,662.25,334.62,8.10;4,136.20,672.57,87.12,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,318.12,651.81,152.58,8.10;4,136.20,662.25,47.46,8.10">Vote/Veto Meta-Classifier for Authorship Identification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Seifert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zechner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Granitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,381.96,662.25,45.18,8.10">CLEF 2011</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<imprint/>
	</monogr>
	<note>Notebook Papers/Labs/Workshop</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
