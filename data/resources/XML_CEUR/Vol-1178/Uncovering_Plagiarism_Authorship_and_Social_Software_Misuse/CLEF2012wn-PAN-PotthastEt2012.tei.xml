<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,155.71,115.90,303.93,12.90;1,243.73,133.83,127.89,12.90">Overview of the 4th International Competition on Plagiarism Detection</title>
				<funder ref="#_RcRRjvm">
					<orgName type="full">ERCIM</orgName>
				</funder>
				<funder ref="#_3XGe53A">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_C3Mqycm #_29MeXzB #_r7TbSkK">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,138.68,171.88,60.37,8.64"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Web Technology &amp; Information Systems Bauhaus</orgName>
								<orgName type="institution">Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,209.85,171.88,44.61,8.64"><forename type="first">Tim</forename><surname>Gollub</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Web Technology &amp; Information Systems Bauhaus</orgName>
								<orgName type="institution">Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.72,171.88,61.11,8.64"><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Web Technology &amp; Information Systems Bauhaus</orgName>
								<orgName type="institution">Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,338.54,171.88,55.36,8.64"><forename type="first">Jan</forename><surname>Graßegger</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Web Technology &amp; Information Systems Bauhaus</orgName>
								<orgName type="institution">Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,405.25,171.88,62.97,8.64"><forename type="first">Johannes</forename><surname>Kiesel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Web Technology &amp; Information Systems Bauhaus</orgName>
								<orgName type="institution">Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,135.45,183.83,75.32,8.64"><forename type="first">Maximilian</forename><surname>Michel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Web Technology &amp; Information Systems Bauhaus</orgName>
								<orgName type="institution">Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,222.11,183.83,65.61,8.64"><forename type="first">Arnd</forename><surname>Oberländer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Web Technology &amp; Information Systems Bauhaus</orgName>
								<orgName type="institution">Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,298.94,183.83,67.92,8.64"><forename type="first">Martin</forename><surname>Tippmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Web Technology &amp; Information Systems Bauhaus</orgName>
								<orgName type="institution">Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,378.60,183.83,92.27,8.64"><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Departament de Llenguatges i Sistemes Informàtics</orgName>
								<orgName type="institution">Universitat Politècnica de Catalunya</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,214.50,195.79,45.74,8.64"><forename type="first">Parth</forename><surname>Gupta</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Natural Language Engineering Lab</orgName>
								<orgName type="institution">ELiRF Universitat Politècnica de València</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,271.68,195.79,47.41,8.64"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Natural Language Engineering Lab</orgName>
								<orgName type="institution">ELiRF Universitat Politècnica de València</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,347.40,195.79,48.99,8.64"><forename type="first">Benno</forename><surname>Stein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Web Technology &amp; Information Systems Bauhaus</orgName>
								<orgName type="institution">Universität Weimar</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,155.71,115.90,303.93,12.90;1,243.73,133.83,127.89,12.90">Overview of the 4th International Competition on Plagiarism Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2BAF2C109D8E1939E450D2CA89C704BA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper overviews 15 plagiarism detectors that have been evaluated within the fourth international competition on plagiarism detection at PAN'12. We report on their performances for two sub-tasks of external plagiarism detection: candidate document retrieval and detailed document comparison. Furthermore, we introduce the PAN plagiarism corpus 2012, the TIRA experimentation platform, and the ChatNoir search engine for the ClueWeb. They add scale and realism to the evaluation as well as new means of measuring performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>To plagiarize means to reuse someone else's work while pretending it to be one's own. Text plagiarism is perhaps one of the oldest forms of plagiarism, which, to this day, remains difficult to be identified in practice. Therefore, a lot of research has been conducted to detect plagiarism automatically. However, much of this research lacks proper evaluation, rendering it irreproducible at times and mostly incomparable across papers <ref type="bibr" coords="1,154.83,477.11,15.27,8.64" target="#b26">[24]</ref>. In order to alleviate these issues, we have been organizing annual competitions on plagiarism detection since 2009 <ref type="bibr" coords="1,306.34,489.06,15.77,8.64" target="#b24">[22,</ref><ref type="bibr" coords="1,325.07,489.06,12.45,8.64" target="#b25">23,</ref><ref type="bibr" coords="1,340.47,489.06,11.83,8.64" target="#b27">25]</ref>. For the purpose of these competitions, we developed the first standardized evaluation framework for plagiarism detection, which has been deployed and revised in the past three competitions, in which a total of 32 teams of researchers took part, 9 of whom more than once.</p><p>Ideally, an evaluation framework accurately emulates the real world around a given computational task in a controlled laboratory environment. But actually, every evaluation framework models the real world to some extent only, while resting upon certain design choices which affect the generalizability of evaluation results to practice. This is also true for our evaluation framework, which has a number of shortcomings rendering it less realistic, sometimes leading its users to impractical algorithm design. In this year's fourth edition of the plagiarism detection competition, we venture off the beaten track in order to further push the limits of evaluating plagiarism detectors. Our goal is to create a more realistic evaluation framework in the course of the coming years. In this paper, we describe our new framework and overview the 15 approaches submitted to this year's competition.</p><p>Figure <ref type="figure" coords="2,225.78,256.00,3.36,8.06" target="#fig_3">1</ref>. Generic retrieval process to detect plagiarism <ref type="bibr" coords="2,399.13,256.35,13.74,7.77" target="#b33">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Plagiarism Detection and its Step-wise Evaluation</head><p>Figure <ref type="figure" coords="2,162.71,320.17,4.98,8.64" target="#fig_3">1</ref> shows a generic retrieval process to detect plagiarism in a given suspicious document d plg , when also given a (very large) document collection D of potential source documents. This process is also referred to as external plagiarism detection since plagiarism in d plg is detected by searching for text passages in D that are highly similar to text passages in d plg . 1 The process is divided into three basic steps, which are typically implemented in most plagiarism detectors. First, candidate retrieval, which identifies a small set of candidate documents D src ⊆ D that are likely sources for plagiarism regarding d plg . Second, detailed comparison, where each candidate document d src ∈ D src is compared to d plg , extracting all passages of text that are highly similar. Third, knowledge-based post-processing, where the extracted passage pairs are cleaned, filtered, and possibly visualized for later presentation.</p><p>In the previous plagiarism detection competitions, we evaluated external plagiarism detection as a whole, handing out large corpora of suspicious documents and source documents. But instead of following the outlined steps, many resorted to comparing all suspicious documents exhaustively to the available source documents. The reason for this was that the number of source documents in our corpus was still too small to justify serious attempts at candidate retrieval. In a realistic setting, however, the source collection is no less than the entire web, which renders exhaustive comparisons infeasible. We hence decided to depart from a one-fits-all approach to evaluation, and instead to evaluate plagiarism detectors step-wise. Our focus is on the candidate retrieval task and the detailed comparison task, we devised new evaluation frameworks for each of them. In the following two sections, we detail the evaluations of both tasks. 1 Another approach to detect plagiarism is called intrinsic plagiarism detection, where detectors are given only a suspicious document and supposed to identify text passages in them which deviate in their style from the remainder of the document. In this year's competition, we focus on external plagiarism detection, while intrinsic plagiarism detection has been evaluated in a related sub-task within PAN's authorship identification task <ref type="bibr" coords="2,360.32,634.70,13.74,7.77" target="#b17">[15]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Candidate Retrieval Evaluation</head><p>The web has become one of the most common sources for plagiarism and text reuse. Presumably, humans follow the three steps outlined in Figure <ref type="figure" coords="3,387.97,303.22,4.98,8.64" target="#fig_0">2</ref> when reusing or plagiarizing text from the web: starting with a search for appropriate sources on a given topic, text is copied from them, and afterwards possibly modified to the extent that the author believes it may not be easily detected anymore. Unsurprisingly, manufacturers of commercial plagiarism detectors as well as researchers working on this subject frequently claim their systems to be searching the web or at least be scalable to its size. However, there is hardly any evidence to substantiate these claims. Neither have commercial plagiarism detectors been found to reliably identify plagiarism from the web, <ref type="foot" coords="3,476.61,385.23,3.49,6.05" target="#foot_0">2</ref>nor have many researchers presented convincing evaluations of their prototypes <ref type="bibr" coords="3,451.00,398.86,15.27,8.64" target="#b26">[24]</ref>. In this year's competition, we address this issue for the first time by developing a corpus specifically suited for candidate retrieval from the web.</p><p>Related Work. The previous PAN corpora <ref type="foot" coords="3,306.87,450.98,3.49,6.05" target="#foot_1">3</ref> marked a major step forward in evaluating plagiarism detectors <ref type="bibr" coords="3,217.42,464.61,15.27,8.64" target="#b26">[24]</ref>. But these corpora have a number of shortcomings that render them less realistic compared to real plagiarism from the web:</p><p>-All plagiarism cases were generated by randomly selecting text passages from source documents and inserting them at random positions in another host document. This way, the "plagiarized" passages do not match the topic of the host document. -The majority of the plagiarized passages were modified to emulate plagiarist behavior. However, the strategies applied are, again, basically random (i.e., shuffling, replacing, inserting, or deleting words randomly). An effort was made to avoid non-readable text, yet none of it bears any semantics. -The corpus documents are parts of books from the Project Gutenberg. <ref type="foot" coords="3,438.46,578.51,3.49,6.05" target="#foot_2">4</ref> Many of these books are rather old, whereas today the web is the predominant source for plagiarists.</p><p>With respect to the second issue, about 4000 plagiarized passages were rewritten manually via crowdsourcing on Amazon's Mechanical Turk. Though the obtained results are of a high quality, an analysis of topic drift in a suspicious document still reveals a plagiarized passage more easily than actually searching its original source <ref type="bibr" coords="4,434.42,155.18,15.27,8.64" target="#b27">[25]</ref>. While neither of these issues is entirely detrimental to evaluation, it becomes clear that there are limits to constructing plagiarism corpora automatically.</p><p>Besides the PAN corpora, there is only one other that explicitly comprises plagiarized text. The Clough09 corpus consists of 57 short answers to one of 5 computer science questions which were plagiarized from a given Wikipedia article <ref type="bibr" coords="4,425.19,214.95,10.58,8.64" target="#b3">[2]</ref>. While the text was genuinely written by a small number of volunteer students, the choice of topics is very narrow, and text lengths range only from 200 to 300 words, which is hardly more than 2-3 paragraphs. Also, the sources to plagiarize were given up front so that there is no data on retrieving them.</p><p>A research field closely related to plagiarism detection is that of text reuse. Text reuse and plagiarism are in fact two of a kind <ref type="bibr" coords="4,313.36,286.69,15.49,8.64" target="#b23">[21]</ref>: text reuse may appear in the forms of citations, boilerplate text, translations, paraphrases, and summaries, whereas all of them may be considered plagiarism under certain circumstances. Text reuse is more general than plagiarism but the latter has received a lot more interest in terms of publications. For the study of text reuse, there is the Meter corpus, which comprises 445 cases of text reuse among 1716 news articles <ref type="bibr" coords="4,284.89,346.46,10.58,8.64" target="#b4">[3]</ref>. The text reuse cases found in the corpus are realistic for the news domain, however, they have not been created by the reuse process outlined in Figure <ref type="figure" coords="4,208.66,370.37,3.74,8.64" target="#fig_0">2</ref>.</p><p>Contributions. Altogether, we lack a realistic, large-scale evaluation corpus for candidate retrieval from the web. For this year's competition, we hence decided to construct a new corpus comprising long, manually written documents. The corpus construction, for the first time, emulates the entire process of plagiarizing or reusing text shown in Figure <ref type="figure" coords="4,162.98,444.38,3.74,8.64" target="#fig_0">2</ref>, both at scale and in a controlled environment. The corpus comprises a number of features that set it apart from previous ones: (1) the topics of each plagiarized document in the corpus are derived from the topics of the TREC Web Track, and sources have been retrieved from the ClueWeb corpus. <ref type="bibr" coords="4,327.39,480.24,11.62,8.64" target="#b3">(2)</ref> The search for sources is logged, including click-through and browsing data. (3) A fine-grained edit history has been recorded for each plagiarized document. (4) A total of 300 plagiarized documents were produced, most of them 5000 words long, while ensuring diversity via crowdsourcing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Corpus Construction Tools: TREC Topics, ClueWeb, ChatNoir, Web Editor</head><p>This section details the ingredients that went into the development of our new corpus and the constraints that were to be met. Generally, when constructing a new corpus, a good starting point is a clear understanding of a-priori required resources. With regard to our corpus, we distinguish three categories: data, technologies, and human resources. For each of these, a number of constraints as well as desirable properties and characteristics were identified.</p><p>Data. Two pieces of data were required for corpus construction: a set of topics about which documents were to be written, and a corpus of web pages to be used as sources for plagiarism. Coming up with a variety of topics to write about is certainly not a big problem. However, we specifically sought not to reinvent the wheel, and when looking for sources of topics, TREC can hardly go unnoticed. After reviewing the topics used for the TREC Web Track, <ref type="foot" coords="5,236.63,153.51,3.49,6.05" target="#foot_3">5</ref> we found them amenable for our purpose. Hence, our topics are derived from TREC topics, rephrasing them so that one is asked to write a text instead of searching for relevant web pages. Yet, writing a text on a given topic may still include the task of searching for relevant web pages. For example, below is a quote of topic 001 of the TREC Web Track 2009:</p><p>Query: obama family tree Description: Find information on President Barack Obama's family history, including genealogy, national origins, places and dates of birth, etc.</p><p>Sub-topic 1: Find the TIME magazine photo essay "Barack Obama's Family Tree." Sub-topic 2: Where did Barack Obama's parents and grandparents come from? Sub-topic 3: Find biographical information on Barack Obama's mother.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This topic has been rephrased as follows:</head><p>Obama's family. Write about President Barack Obama's family history, including genealogy, national origins, places and dates of birth, etc. Where did Barack Obama's parents and grandparents come from? Also include a brief biography of Obama's mother.</p><p>All except one sub-topic could be preserved, whereas sub-topic 1 was considered too specific to be of real use, especially since a photo essay does not contain a lot of text. Two groups of topics are among the TREC Web track topics, namely faceted and ambiguous ones. The former were easier to be translated into essay topics, whereas for the latter, we typically chose one of the available ambiguities.</p><p>With regard to the corpus of web pages used as sources for plagiarism, one of the top requirements was a huge size in order to foreclose exhaustive comparisons of suspicious documents to all the documents in the corpus. The corpus should also be as representative as possible compared to the entire web. To date, one of the most representative web crawls available to researchers is the ClueWeb corpus. <ref type="foot" coords="5,426.95,508.67,3.49,6.05" target="#foot_4">6</ref> The corpus consists of more than 1 billion documents from 10 languages which amount to 25 terabytes of data. It has become a widely accepted resource, being used to evaluate the retrieval performance of search engines in the course of the TREC Web Track 2009-2011. Other publicly available corpora include the DOTGOV crawl and the WT10G crawl, which were previously used in TREC, as well as the crawl released by the Commoncrawl initiative, which is 5 times larger than the ClueWeb corpus. <ref type="foot" coords="5,413.11,580.40,3.49,6.05" target="#foot_5">7</ref> However, since the ClueWeb corpus is still the de-facto standard evaluation corpus, we decided to stick with it. Consequently, we have limited our choice of topics to those used within the TREC Web Tracks 2009-2011.</p><p>Technologies. Three pieces of technology were required for corpus construction: a search engine for the aforementioned web corpus, a web service that serves web pages from the web corpus on demand, and a text editor with which the plagiarized documents were to be written. Unfortunately, neither of these technologies could be obtained off the shelf, since we required full access to them in order to closely track our human subjects during corpus construction to make measurements and collect data.</p><p>For the sake of realism, we expected the search engine used for corpus construction to resemble commercial ones as close as possible, so that our human subjects behave naturally and similar to "real plagiarists." This requires not only a state-of-the-art web retrieval model, but also an intuitive interface as well as fast retrieval. Until now, there has been no search engine publicly available to researchers that combines all of these properties. However, our work on this corpus construction project coincided with the development of a new research search engine for the ClueWeb at our site: ChatNoir <ref type="bibr" coords="6,461.51,262.77,15.27,8.64" target="#b28">[26]</ref>.</p><p>The ChatNoir search engine is based on the classic BM25F retrieval model <ref type="bibr" coords="6,461.51,274.73,15.27,8.64" target="#b29">[27]</ref>, using the anchor text list provided by the University of Twente <ref type="bibr" coords="6,398.12,286.69,15.27,8.64" target="#b15">[13]</ref>, the PageRanks provided by the Carnegie Mellon University, <ref type="foot" coords="6,316.22,296.97,3.49,6.05" target="#foot_6">8</ref> and the spam rank list provided by the University of Waterloo <ref type="bibr" coords="6,229.74,310.60,10.58,8.64">[4]</ref>. ChatNoir also incorporates an approximate proximity feature with variable-width buckets as described by Elsayed et al. <ref type="bibr" coords="6,382.13,322.55,10.79,8.64" target="#b6">[5]</ref>: the text body of each document is divided into 64 buckets such that neighboring buckets have a half-bucket overlap. For each keyword, not the exact position is stored in a 1-gram index but its occurrence in the individual buckets is indicated via a bit flag. Hence, for each keyword in a document, a 64-bit vector stores whether it occurs in one of the 64 buckets. This retrieval model is of course not as mature as those of commercial search engines; yet it combines some of the most widely accepted approaches in information retrieval. In addition to the retrieval model, ChatNoir also implements two search facets relevant to those who plagiarize from the web: text readability scoring, and long text search. The former facet, similar to that offered by Google, scores the readability of the text found on every web page using the well-known Flesh-Kincaid grade level formula. It estimates the number of years one should have visited school in order to understand a given text. This number is then mapped onto the three fuzzy categories "simple," "intermediate," and "expert." The long text search facet filters search results which do not contain at least one continuous paragraph of text longer than a threshold of 300 words. The facets can be combined with each other. ChatNoir indexes 1-grams, 2-grams, and 3-grams, and the implementation of the underlying inverted index has been optimized to guarantee fast response times. It runs on a cluster of 10 standard quad-core PCs and 2 eight-core rack servers. Short to medium length queries are answered between 1 and 5 seconds. The web interface of ChatNoir resembles that of commercial search engines in terms of search result presentation and usability. <ref type="foot" coords="6,337.21,559.98,3.49,6.05" target="#foot_7">9</ref>When a user of our search engine is presented with her search results and then clicks on one of them, she is not redirected into the real web but the ClueWeb instead. Although the ClueWeb provides the original URLs from which the web pages have been obtained, many are dead or have been updated since the corpus was crawled. Moreover, in order to create traceable plagiarism cases, the plagiarized texts should come from a web page which is available in the ClueWeb. We hence have set up a web service that serves web pages from the ClueWeb on demand: when accessing a web page, it is pre-processed before being shipped, removing all kinds of automatic referrers, and replacing all links that point into the real web with links onto their corresponding web page inside the ClueWeb. This way, the ClueWeb may be browsed as if surfing the real web, whereas it becomes possible to precisely track a user's movements. The ClueWeb itself is stored in the HDFS of our 40 node Hadoop cluster, and each web page is fetched directly from there with latencies around 200ms.</p><p>Next to the search engine and the ClueWeb access service, the third technology required for corpus construction must strike as a simple one: a basic text editor. However, in order to properly trace how a plagiarism case was constructed, the human subjects who plagiarized on our behalf must be minutely tracked while modifying a piece of text copied from a web page that has been retrieved with our search engine. Furthermore, the editor to be used should allow for remote work since we did not know in advance who was going to plagiarize for us. Looking at the available alternatives (Microsoft Word, OpenOffice, Google Docs, Zoho, rich text editors, etc.) we decided to go with the simplest option of web-based rich text editors. They offer maximal flexibility while being simple to use: using a well-known web toolkit, a web application has been developed that features a rich text editor alongside a set of instructions. The editor provides an autosave feature that sends the current state of a text to our servers, every time the user stops typing for more than 300ms. On the server side, each new revision of a given text is stored into a Git repository. This way, a detailed revision history is recorded which tracks the edits made to a text in much finer detail than, for instance, those of Wikipedia articles. Moreover, the editor enables its users to track the sources of text copied into it by allowing for it to be colored, and each color to be linked to the web page the text came from (cf. Figure <ref type="figure" coords="7,223.08,418.19,3.74,8.64" target="#fig_2">3</ref>, right column). This manual effort of immediate annotation was required from our human subjects in order to ease post-processing and quality control.</p><p>Human Resources. Having a working, controlled web search environment at hand, an open question was who to pick as volunteer plagiarists. Short of real ones, we employ people who act as plagiarists. Since the target language of the corpus is English, we require them to be fluent English speakers and preferably have some writing experience. We assume that (semi-)professional writers are not only faster but also have at their disposal more diverse ways of modifying a text so as to vary the outcome. We have made use of crowdsourcing platforms to hire writers. Crowdsourcing has quickly become one of the cornerstones of constructing evaluation corpora. This is especially true for paid crowdsourcing via Amazon's Mechanical Turk <ref type="bibr" coords="7,323.95,555.68,10.58,8.64" target="#b2">[1]</ref>. This platform is frequently used by researchers to annotate and collect data. On the upside, it offers a flexible interface, a large workforce, and very low costs per unit. Many report on collecting thousands of annotations for just a couple of dollars. On the downside, however, scammers constantly submit fake results in order to get paid without actually working, so that quality control is one of the main obstacles to using Mechanical Turk. Moreover, the workforce has very diverse skills and knowledge so that task design and simplicity of task description have severe impact on result quality. Since our task is hardly simple enough for  Mechanical Turk, we resort to oDesk instead. <ref type="foot" coords="8,319.47,345.53,6.97,6.05" target="#foot_8">10</ref> At oDesk, wages are higher, whereas scamming is much lower because of more advanced rating features for workers and employers. Finally, at our site, two post docs worked part time to supervise the oDesk workers and check their results, whereas the technologies were kept running by three undergrad students.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Corpus Construction</head><p>To construct our corpus, we hired 18 (self-proclaimed) professional writers at oDesk, coming from various countries and backgrounds. A job advertisement was posted at oDesk to which more than half of the workforce replied, whereas the other authors were sought out manually. Employment criteria included a reasonable application to the job ad, a reasonable number of previously billed hours in other writing appointments, and, for budgetary reasons, an hourly rate below 25 US$. Nevertheless, a few writers above that rate were hired to ensure diversity. The hourly rates ranged from 3.33 US$ to 34 US$ with an average of 12.43 US$. Before hiring writers at oDesk, we recruited 10 colleagues and friends to test the environment by producing a plagiarized document each, so that a total of 28 different writers were involved in corpus construction.</p><p>Plagiarized documents were produced in two batches of 150 documents each, so that each of the aforementioned TREC topics has been written about twice. No writer was allowed to pick the same topic twice. There was a key difference between the first and the second batch which relates to the retrieval model employed for searching source documents in the ClueWeb. Given the fact that the ChatNoir search engine implements only a single, basic retrieval model, our setup bears the risk of a systematic error if writers use only our search engine to search for source documents on a given topic (e.g., implementation errors, poor retrieval performance, etc.). Hence, we decided to circumvent this risk by reusing TREC qrels (documents retrieved by participants of the TREC Web Tracks and judged for relevance by the TREC organizers) which achieved at least the relevance level 2 of 3 on a given topic. For each topic in the first batch, writers were asked to plagiarize from as many of the topic-specific qrels as they wished, without using ChatNoir. In the second batch, no source documents were given and writers were asked to use ChatNoir to retrieve sources matching a given topic. This way, the plagiarism in the documents from the first batch stems from source documents that were retrieved by the union of all retrieval models that took part in the TREC Web Tracks, whereas the second batch is based on ChatNoir's retrieval model only.</p><p>Each writer worked on one topic at a time while being allowed to choose from the remaining topics. They were asked to write documents of at least 5000 words length, which sometimes was not possible due to lack of source material. Besides plagiarized text passages, they were also asked to genuinely write text of their own. In this connection, we observed that, once plagiarism is allowed, writers became reluctant to write genuine text. The number of possible sources was limited to up to 30 for the first batch and increased to up to 75 during the second batch. Throughout corpus construction, a number of measurements were made so that substantial meta data can be provided in addition to each plagiarized document (Figure <ref type="figure" coords="9,321.26,334.51,4.98,8.64" target="#fig_2">3</ref> shows an example of this data):</p><p>-Plagiarized document -Set of source documents plagiarized.</p><p>-Annotations of passages plagiarized from the sources.</p><p>-Log of queries posed by the writer while writing the text.</p><p>-Search results for each query.</p><p>-Click-through data for each query.</p><p>-Browsing data of links clicked within ClueWeb documents.</p><p>-Edit history of the document.</p><p>Finally, each plagiarized document, along with its meta data, is post-processed to correct errors and to convert it into a machine-readable format that suits the needs of evaluating plagiarism detectors. The post-processing step includes double-checking whether the source documents are all contained in the ClueWeb, and whether the annotation of text passages to source documents is correct. Moreover, because of the rather crude HTML code that is generated by today's rich text editors, each plagiarized document must be manually annotated in order to mark the beginning and end of each plagiarized text passage. At the time of writing, post-processing the plagiarized documents constructed for our corpus was still a work in progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluation with the ChatNoir Search Engine</head><p>Since our new corpus uses the ClueWeb as the collection of source documents, it is this collection that must be input to a plagiarism detector implementing the retrieval process shown in Figure <ref type="figure" coords="9,201.93,627.41,3.74,8.64" target="#fig_3">1</ref>. Given the prize and size of the ClueWeb, however, we could hardly expect participants to first buy a license and then index the ClueWeb in the three months of the competition's training phase. In the 2009 plagiarism detection competition, our first plagiarism corpus of more than 40 000 text documents already posed a challenge to participants, so that handling the 0.5 billion English web pages of the ClueWeb would have been an insurmountable obstacle to most prospective participants. It is hence that ChatNoir served two purposes in our evaluation: its user interface was used by our human plagiarists to search for source documents, and its API was used by participants to develop retrieval algorithms that rediscover the sources of a suspicious document. This is a realistic approach, since it is likely that real plagiarists use the same search engine as commercial plagiarism detectors.</p><p>Evaluation Corpus. The competition was divided into two phases, a training phase and a test phase. For both phases, a subset of our corpus was released. The training corpus consisted of 8 plagiarized documents including annotations that indicated which text passage was plagiarized from which ClueWeb document, whereas the test corpus (consisting of 32 documents) naturally did not contain such annotations. Note that the evaluation corpora used in this year's candidate retrieval task did not include all plagiarized documents created, but only a subset of them. There are two reasons for this: at the time of release of the training and test corpora, not all documents were finished, and post-processing the raw documents obtained from the oDesk writers was too timeconsuming so that no further documents could be added in time for the release.</p><p>Performance Measures. The performance of a candidate retrieval algorithm depends on what is its desired behavior. The candidate retrieval step of plagiarism detection is supposed to filter the collection of potential source documents and to output only the candidate documents that are likely sources for plagiarism with respect to a given suspicious document. Ideally, a candidate retrieval algorithm would retrieve exactly the documents used as sources by a plagiarist, and none other. This way, the subsequent steps in plagiarism detection would have to deal only with documents that are worthwhile processing. Furthermore, when considering that the document collection to be searched by a candidate retrieval algorithm may be the entire web, it becomes clear that existing retrieval models, search engines, and search infrastructures should be reused for this task. Building one's own search engine is out of bounds for many, whereas access to existing search engines is usually not free of charge (both in terms of cost per query and retrieval time). Therefore, an ideal candidate retrieval algorithm also minimizes the amount of queries posed to a search engine in order to retrieve candidate documents. With these considerations in mind, we measure candidate retrieval performance for each suspicious document in the test corpus using the following five scores: Measures 1-3 capture a candidate retrieval algorithm's overall behavior and measures 4-5 assess the time to first detection. Note in this connection, that neither of these measures captures the quality of extracting plagiarized passages from suspicious documents since this is supposed to happen within the detailed comparison of candidate documents with a suspicious document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Survey of Retrieval Approaches</head><p>Five of the 15 participants submitted runs for the candidate retrieval task, four of whom also submitted a notebook describing their approach. An analysis of these notebooks reveals a number of building blocks that were commonly used to build candidate retrieval algorithms: (1) chunking, (2) keyphrase extraction, (3) query formulation, (4) search control, and (5) download filtering. In what follows, we describe them in detail.</p><p>Chunking. Given a suspicious document, it is divided into (possibly overlapping) passages of text. Each chunk of text is then processed individually. Rationale for chunking the suspicious document is to evenly distribute "attention" over a suspicious document so that algorithms employed in subsequent steps are less susceptible to unexpected characteristics of the suspicious document. The chunking strategies employed by the participants are 25-line chunking <ref type="bibr" coords="11,242.02,270.84,10.58,8.64" target="#b8">[7]</ref>, 4-sentence chunking <ref type="bibr" coords="11,342.37,270.84,15.27,8.64" target="#b16">[14]</ref>, paragraph chunking <ref type="bibr" coords="11,444.94,270.84,15.27,8.64" target="#b19">[17]</ref>, and no chunking <ref type="bibr" coords="11,188.17,282.79,15.27,8.64" target="#b34">[32]</ref>. Neither of the participants mention the extent to which the chunks produced by their strategies overlap.</p><p>Keyphrase Extraction. Given a chunk (or the entire suspicious document), keyphrases are extracted from it in order to formulate queries with them. Rationale for keyphrase extraction is to select only those phrases (or words) which maximize the chance of retrieving source documents matching the suspicious document. Keyphrase extraction may also serve as a means to limit the amount of queries formulated, thus reducing the overall costs of using a search engine. This step is perhaps the most important one of a candidate retrieval algorithm since the decisions made here directly affect overall performance: the fewer keywords are extracted, the better the choice must be or recall is irrevocably lost.</p><p>Since ChatNoir, during this year's test phase, unfortunately still lacked phrasal search, participants resorted to extracting keywords. Kong et al. <ref type="bibr" coords="11,400.23,441.24,16.60,8.64" target="#b19">[17]</ref> rank a chunk's words by their tf • idf values, where a word's document frequency is presumably obtained from a large external document collection, and then extract the top 10 most discriminative words. Jayapal <ref type="bibr" coords="11,242.95,477.11,16.60,8.64" target="#b16">[14]</ref> uses the first 10 words of a chunk whose part-of-speech are either noun, pronoun, verb, or adjective. Gillam et al. <ref type="bibr" coords="11,359.58,489.06,11.62,8.64" target="#b8">[7]</ref> rank a chunk's words using a so-called "enhanced weirdness" score which measures the likelihood of a word not appearing in general text, then re-rank the top 10 words by their frequency, and return the top most frequent term plus the 4 words succeeding it. The keyphrase extractor of Suchomel et al. <ref type="bibr" coords="11,197.60,536.89,16.60,8.64" target="#b34">[32]</ref> goes beyond the aforementioned ones in that it employs not one but three strategies at the same time: (1) a chunk's words are ranked by their tf •idf values, where a word's document frequency is obtained from an English web corpus, and then extract the top most discriminative words that exceed a threshold. (2) The suspicious document is chunked in 45-word chunks (40 word overlap), then chunks are picked whose vocabulary richness is significantly higher than that of neighboring chunks, from each of which the first sentence longer than 8 words (excluding stop words) is selected and the first 6 words (excluding stop words) are extracted. (3) Headings in the suspicious document are identified and extracted (excluding stop words).</p><p>A key insight from reviewing the participants approaches is that Suchomel et al.'s strategy of combining different ideas to extract keyphrases is probably superior to the one-fits-all approach of the other participants. This way, just as with chunking, the risk of algorithm error is further diminished and it becomes possible to exploit different sources of information that complement each other. A minor criticism is that only custom-made keyphrase extractors were used, and hardly any reference was made to the extensive literature on the subject. While it may be necessary to tailor keyphrase extraction methods to the task of candidate retrieval, existing work should not be entirely neglected.</p><p>Query Formulation. Given sets of keywords extracted from chunks, queries are formulated which are tailored to the API of the search engine used. Rationale for this is to adhere to restrictions imposed by the search engine and to exploit search features that go beyond basic keyword search. The maximum number of search terms enforced by ChatNoir is 10 keywords per query. All participants except Jayapal <ref type="bibr" coords="12,423.37,266.47,15.27,8.64" target="#b16">[14]</ref>, who formulated 10-word queries, formulated 5-to-6-word queries, using the word ranking of their respective keyword extractor. The reason for limiting query length was to avoid overspecific queries. Interestingly, the participants formulate non-overlapping queries (i.e., they do not use the same keyword in more than one query), in contrast to previous candidate retrieval strategies in the literature <ref type="bibr" coords="12,312.71,326.25,15.27,8.64" target="#b14">[12]</ref>. Also note that non of the participants made use of the search facets offered by ChatNoir, namely the facet to search for web pages of at least 300 words of text, and the facet to filter search results by readability.</p><p>Search Control. Given a set of queries, the search controller schedules their submission to the search engine and directs the download of search results. Rationale for this is to dynamically adjust the search based on the results of each query, which may include dropping queries, reformulating existing ones, or formulating new ones based on the relevance feedback obtained from search results. Only Suchomel et al. <ref type="bibr" coords="12,435.23,425.59,16.60,8.64" target="#b34">[32]</ref> implemented a search controller, whereas all other participants simply submitted all queries to the search engine, downloading all, or only the top n results. Suchomel et al.'s search controller schedules queries dependent on the keyphrase extractor which extracted their words: the order of precedence corresponds to the order in which they have been explained above. Then, for each search result obtained from submitting one open query, it is checked whether its snippet is contained in the suspicious document. If so, the document is downloaded and subject to detailed comparison to the suspicious document. In case, a plagiarized passage is discovered, all queries whose keywords originate from that portion of the suspicious document are discarded from the list of open queries. No attempts are made at reformulating existing queries or formulating new ones based on the documents downloaded. Download Filtering. Given a set of downloaded documents, a download filter removes all documents that are probably not worthwhile being compared in detail with the suspicious document. Rationale for this is to further reduce the set of candidates and to save invocations of the subsequent detailed comparison step. All except one participant skipped this step and proceeded to detailed comparison directly. Jayapal <ref type="bibr" coords="12,423.42,632.53,16.60,8.64" target="#b16">[14]</ref> computes the 5-gram Jaccard similarity of each downloaded document to the suspicious document and discards documents that do not exceed a similarity threshold. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Evaluation Results</head><p>Table <ref type="table" coords="13,160.16,361.84,4.98,8.64" target="#tab_0">1</ref> shows averaged performances of the five candidate retrieval algorithms over the 32 plagiarized documents that formed the test corpus. While computing the performances, we encountered a problem that required (semi-)automatic post-processing of the submitted runs, namely that many web pages in the ClueWeb are near-duplicates of one another. Although the plagiarized passages in the corpus documents were lifted from exactly one web page, it is very well possible that a candidate retrieval algorithm retrieves a different page with the same contents. In such cases, the candidate algorithm is not in error and hence its performance should not be discounted. To alleviate this issue we compared all documents retrieved by the candidate retrieval algorithms in turn to each of the 362 sources for plagiarism in the test corpus in order to identify near-duplicate web pages. Here, two web pages are considered near-duplicates if their cosine similarity under a tf -weighted 1-gram vector space model is higher than 0.85 and higher than 0.7 under a tf -weighted 3-gram vector space model. A number of spot checks were made to ensure a low rate of false positives, however, since some plagiarized passages in our corpus are rather short, there is the possibility that some of the documents retrieved were falsely considered not being duplicates of the actual source documents. The top half of Table <ref type="table" coords="13,270.55,553.12,4.98,8.64" target="#tab_0">1</ref> shows performances when treating near-duplicates as positive source documents, and the bottom half shows performances when counting only detections of actual source documents. The differences are profound, indicating that most candidate retrieval algorithms mainly retrieved near-duplicates of the plagiarized documents' actual sources from the ClueWeb. The column group "Reported Sources" shows precision and recall of the sets of candidate documents that have been submitted as runs by the participants, whereas the performances shown in all other columns and column groups are based on access log data recorded at our site. To facilitate these measurements, each participant's candidate retrieval algorithm was assigned a unique access token and an exclusive access time period to ChatNoir in order to process the test corpus. All other access methods were disabled during the test phase. Furthermore, participants were asked to indicate during access which plagiarized document of the test corpus is currently processed by their algorithm. This enabled us to record detailed information about queries submitted, search results obtained, and downloads made.</p><p>The three column groups "Retrieved Sources," "Downloaded Sources," and "Reported Sources" measure performance at specific points of executing a candidate retrieval algorithm: after all queries have been submitted, after selected search results have been downloaded, and after the final decision is made as to which downloaded documents are considered candidates. Hence, the set of retrieved documents equals or subsumes the set of downloaded ones, which in turn equals or subsumes the set of reported documents. It can be seen that noteworthy precision is only achieved regarding reported sources, whereas recall is at times more than double as high on downloaded and retrieved sources compared to reported ones. This indicates that all candidate retrieval algorithms trade a lot of recall for precision during download filtering. The best performing approach regarding reported sources is that of Jayapal <ref type="bibr" coords="14,403.57,310.60,15.27,8.64" target="#b16">[14]</ref>, while coming in second to Gillam et al. <ref type="bibr" coords="14,237.43,322.55,11.62,8.64" target="#b8">[7]</ref> regarding downloaded and retrieved sources.</p><p>The three remaining column groups indicate the total workload and the time to first detection of each candidate retrieval algorithm, given in absolute terms of queries submitted and downloads made. The column "No Detection" indicates for how many plagiarized documents no true positive detection was made, which shows very low values in the top half of the table. Otherwise, the average number of queries and downloads ranges from tens to thousands. The best performing approach by a long margin is that of Suchomel et al. <ref type="bibr" coords="14,242.36,406.24,15.27,8.64" target="#b34">[32]</ref>. Here, their search controller pays off, while presumably the combination of three keyword extractors maintains high precision and recall regarding reported sources: despite their low numbers of queries and downloads, a remarkably competitive precision and recall is achieved. Clearly, this approach is the most cost-effective one. The approach of Jayapal, however, requires only six times as many queries overall and less than twice as many downloads.</p><p>Discussion. The obtained results validate our setup: the search engine apparently retrieves web pages that are useful for both humans and machines. Especially, it is possible for a machine plagiarism detector to trace the steps of a human plagiarist. That said, there is still a lot of room for improvement concerning the tools used within our new evaluation framework and the participants' candidate retrieval approaches. With regard to our evaluation framework, ChatNoir is still a single point of failure, and improvements to its retrieval model directly affect participants' performances. Moreover, the performance measures used above require further research: from the evaluation results, it can already be concluded that candidate retrieval is a recall-oriented task, whereas precision mainly becomes a measure of runtime of subsequent steps. Finally, the participants' approaches are still somewhat immature (as can be expected when introducing a new evaluation framework <ref type="bibr" coords="14,254.95,627.41,14.94,8.64" target="#b24">[22]</ref>). Their detection quality is rather low, which limits the overall detection performance of a web plagiarism detector. Also, the research presented is often not well-founded in the literature.</p><p>After a set of candidate source documents has been retrieved for a suspicious document, the follow-up task of an external plagiarism detection pipeline is to analyze in detail whether the suspicious document in fact contains plagiarized passages from these sources (cf. Figure <ref type="figure" coords="15,212.12,178.42,3.60,8.64" target="#fig_3">1</ref>), and to extract them with high accuracy. The major difference of this year's detailed comparison evaluation compared to previous years is that we asked participants to submit their detection software instead of just detection results on a given data set, which has a couple of advantages:</p><p>-Runtime Analysis. Software submissions of detailed comparison approaches, allows for measuring and comparing runtime characteristics, since, for commercial plagiarism detectors, efficiency is just as important as detection effectiveness.</p><p>-Real Plagiarism Cases. Real plagiarism cases are the ultimate resource to evaluate plagiarism detectors. In previous competitions, privacy concerns prevented us from using them in our evaluation, since these cases would have to be released to the participants. Having the software at our site, this year, a small set of real plagiarism cases was incorporated into the test collection for the first time.</p><p>-Continuous Evaluation. In the course of the previous competitions, the employed corpora have changed considerably (improving their quality in each iteration).</p><p>Hence, it has become difficult to compare detection performances across years and thus, it is hard to tell to which extent the approaches have improved over time. For instance, in 2011, the best overall detection performance was, in absolute terms, below that of 2010, whereas the evaluation corpus used in 2011 was a lot more difficult than before. With software submissions, it is now possible to continuously compare approaches from multiple years whenever new collections are released, given the consent of the respective authors.</p><p>This year, eleven teams submitted software, which is comparable to last year's number of participants. Software submissions are hence no big obstacle to participants and should be pursued further in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation with the Experimentation Platform TIRA</head><p>Besides the aforementioned advantages of software submissions, there are also disadvantages, for instance, the non-negligible amount of extra work on the organizer's side. One possibility to reduce the complexity of the evaluation task is to constrain participants to a certain operating system and programming language. However, due to the diverse coding preferences of software developers and the fact that a mismatch of our constraints to their preferences would foreclose reusing existing prior work, it is futile to find a platform that satisfies all participants. We hence decided to allow for detailed comparison software of various kinds, and use the experimentation platform TIRA <ref type="bibr" coords="15,464.00,608.62,16.60,8.64" target="#b11">[10]</ref> to keep the organizational overhead moderate. This way, we kept the criteria to be met by participants' software at a minimum: it must be executable on either a Windows or a Linux platform, it must accept two input parameters (the location of the suspicious document and that of a candidate document) and return detection results in a pre-specified XML format, and it must be accompanied by a comprehensive installation manual. Furthermore, TIRA provides a set of features that facilitate our work <ref type="bibr" coords="16,396.71,131.27,10.79,8.64" target="#b10">[9]</ref>:</p><p>-Experiments as a Service. TIRA creates a web service for every deployed program. The web service can be accessed remotely using a web browser. It allows for the convenient execution of a program with individual parameter settings as well as for the retrieval of already computed experiment results.</p><p>-System Independence. Shipped as a Java program, TIRA can be instantiated on both Windows and Linux platforms. Deployed programs are executed from a command shell of the underlying operating system, rendering program execution independent of a specific programming language or middleware.</p><p>-Peer To Peer Networking. The possibility to join TIRA instances on different machines to form a TIRA network allows to control programs running on different operating systems from a single web page.</p><p>-Multivalued Configurations. TIRA provides an intuitive mechanism for specifying multiple values for program parameters. This way, a large series of program runs can be initiated by a single run configuration.</p><p>-Distributed Execution. The scheduling mechanism of TIRA supports the efficient execution of programs in different threads and across multiple machines.</p><p>-Result Retrieval. TIRA provides means for storing, indexing, and retrieving experiment results.</p><p>TIRA was employed in the training phase as well as the test phase of this year's competition. In the training phase, participants were given the possibility to upload detection results which they obtained on the released training corpus. As a response, TIRA returned the performance values for the submission. Since the performance values have been publicly visible, our idea was to use TIRA as an online leader board for the early phase of the competition. However, the service has not been adopted until the last week before the run submission deadline <ref type="bibr" coords="16,313.01,468.50,10.58,8.64" target="#b9">[8]</ref>. For the test phase, a Windows 7 and an Ubuntu 12.04 virtual machine was set up, each running on a computer with 70 GB RAM and two quad-core Intel Xeon E552 CPUs, and TIRA as well as all program submissions were installed. For one submission, we experienced unsolvable runtime problems and, in mutual agreement with the authors, omitted this submission from the evaluation. The two virtual machines were accessible from the local network, and linked to a third TIRA instance which served as the control instance. From this control instance the evaluation of all submissions on all of the sub-corpora of the test corpus was managed. Computing the results for all submissions took two days.</p><p>Evaluation Corpus. As an evaluation corpus, we employed the corpus construction process used in previous competitions. Based on the books of Project Gutenberg, we extracted passages from one subset of the books (source documents), obfuscated them, and inserted them as plagiarized passages into a different subset of books (suspicious documents). For the obfuscation process, the four strategies No Obfuscation, Artificial (Low), Artificial (High), and Manual Paraphrasing were used. In addition, the eval- uation corpus contains documents without any plagiarized passages. A more detailed description of the obfuscation strategies can be found in <ref type="bibr" coords="17,360.13,316.59,15.27,8.64" target="#b23">[21]</ref>. Similar to previous years, we also constructed a set of cross-language plagiarism cases. This time, however, we did not apply Google Translate as it was observed that such a construction strategy is biased towards detailed comparison approaches which themselves use Google Translate to detect cross-language plagiarism. Instead, we completely revised the cross-language sub-corpus and created the cases based on the multilingual Europarl corpus <ref type="bibr" coords="17,231.99,388.32,15.27,8.64" target="#b18">[16]</ref>. Starting from a non-English source document in the corpus, we first eliminated all paragraphs not coming from the document's main speaker, selected a passage to be plagiarized, extracted the corresponding passage from the English version of the source document, and inserted it into a Gutenberg book. Another improvement over previous years is that document similarity was considered when choosing a suspicious document for a given source document. Especially pairs of suspicious and source document pairs without plagiarized passages or unobfuscated ones, we intended to make the detection task more challenging and realistic by selecting books with a similar vocabulary. Similarities were computed under a tf -weighted vector space model, employing stop word removal, and the cosine similarity metric. The average similarities for each of the sub-corpora and their respective sizes are given in Table <ref type="table" coords="17,160.03,519.82,3.74,8.64" target="#tab_1">2</ref>. Finally, the 33 real plagiarism cases used this year were obtained by manually collecting them from the Internet. Each case consists of passages of no more than 75-150 words, and they were found not to be embedded in host documents. We hence resorted to embedding them, automatically, into topically related host documents sampled from Wikipedia. Of 10 000 Wikipedia articles, for each plagiarism case, two different host documents were chosen which are similar to the plagiarized passage under a tf • idf -weighted vector space model and the cosine similarity. Then, the plagiarism case's plagiarized and source passages were inserted at randomly selected positions of the two documents, each in one of them.</p><p>Performance Measures. To assess the performance of the submitted detailed comparison approaches, we employed the performance measures used in previous competitions.</p><p>For this paper to be self-contained, we summarize the definition found in <ref type="bibr" coords="18,425.05,119.31,15.49,8.64" target="#b26">[24]</ref>: let S denote the set of plagiarism cases in the corpus, and let R denote the set of detections reported by a plagiarism detector for the suspicious documents. To simplify notation, a plagiarism case s = s plg , d plg , s src , d src , s ∈ S, is represented as a set s of references to the characters of d plg and d src , specifying the passages s plg and s src . Likewise, a plagiarism detection r ∈ R is represented as r. Based on this notation, precision and recall of R under S can be measured as follows:</p><formula xml:id="formula_0" coords="18,145.38,208.99,324.59,27.47">prec(S, R) = 1 |R| r∈R | s∈S (s r)| |r| , r ec(S, R) = 1 |S| s∈S | r∈R (s r)| |s| ,</formula><p>where s r = s ∩ r if r detects s, ∅ otherwise.</p><p>Observe that neither precision nor recall account for the fact that plagiarism detectors sometimes report overlapping or multiple detections for a single plagiarism case. This is undesirable, and to address this deficit also a detector's granularity is quantified as follows:</p><formula xml:id="formula_1" coords="18,245.48,321.43,124.39,27.42">gran(S, R) = 1 |S R | s∈S R |R s |,</formula><p>where S R ⊆ S are cases detected by detections in R, and R s ⊆ R are detections of s; i.e., S R = {s | s ∈ S ∧ ∃r ∈ R : r detects s} and R s = {r | r ∈ R ∧ r detects s}. Note further that the above three measures alone do not allow for a unique ranking among detection approaches. Therefore, the measures are combined into a single overall score as follows:</p><p>plagdet(S, R) = F 1 log 2 (1 + gran(S, R))</p><p>,</p><p>where F 1 is the equally weighted harmonic mean of precision and recall.</p><p>Analysis of the plagdet Score. The plagdet score has been criticized to put too much weight on granularity instead of precision and recall, thus ranking plagiarism detectors differently than a human would. We stress that plagdet was not meant to be the single yardstick of evaluation and that-just as with the F α -Measure-one should always look at individual performance measures to judge an algorithm's performance. Though a number of experiments were conducted to verify the score, we did not continue research in this direction, until now. Inspired by the experiments of Grozea and Popescu <ref type="bibr" coords="18,447.75,542.90,15.27,8.64" target="#b13">[11]</ref>, we now close this gap and shed further light on how plagdet ranks plagiarism detectors. Grozea and Popescu have identified a situation in which plagdet ranks two hypothetical plagiarism detectors in a possibly counterintuitive way. The situation involves one detector A with the performance characteristic prec A = 1, r ec A = 0.5, gran A = 1 and another detector B with the characteristic 1, 1, 2 . <ref type="foot" coords="18,438.01,601.01,6.97,6.05" target="#foot_9">11</ref> In plain terms, given a single plagiarism case, detector A would detect half of it in one piece, whereas detector B would detect the entire plagiarism case in two pieces. The plagdet score of detector A is 0.67, and that of detector B is 0.63, so that A is ranked higher than B. It is debatable which of the two plagiarism detectors humans prefer in general, the one that detects a part of each plagiarism case in one piece, or the one that detects entire plagiarism cases in many pieces. Incidentally, the introduction of the granularity measure forced developers of plagiarism detectors for the first time to actually care about how often a single plagiarism case is detected. But for argument's sake, we follow the reasoning of Grozea and Popescu that this ranking is counterintuitive; they then generalize from the above example and claim that plagdet ranks two plagiarism detectors A and B always counterintuitive if prec A = prec B , r ec A &lt; r ec B , gran A = 1, and gran B &gt; 1. <ref type="foot" coords="19,203.26,225.24,6.97,6.05" target="#foot_10">12</ref> Again, in plain terms, their claim is that, under plagdet, a plagiarism detector with perfect granularity is always counterintuitively ranked higher than one with higher recall and less than perfect granularity, when keeping precision fixed. To further substantiate their claim, Grozea and Popescu present at plot in which they attempt to visualize plagdet's alleged area of counterintuitivity in precision-recall space: for this, detector A is fixed at perfect granularity gran A = 1, varying precision and recall. Detector B is fixed at perfect recall r ec B = 1, a granularity gran B = 2, varying precision in accordance with the above condition prec A = prec B . Observe that this setting fulfills the above conditions for counterintuitivity. Next, both detectors' plagdet scores for pairs of precision and recall are computed, and their differences plagdet A -plagdet B are visualized at the respective points in precision-recall space by means of color. The left contour plot of Figure <ref type="figure" coords="19,334.21,358.42,4.98,8.64" target="#fig_4">4</ref> illustrates this procedure. <ref type="foot" coords="19,441.93,356.75,6.97,6.05" target="#foot_11">13</ref> Grozea and Popescu then state that the area above the 0-contour is plagdet's area of counterintuitivity (i.e., the plagdet of detector A is better than that of detector B). Moreover, it is claimed that, because of the area above the 0-contour filling more than 50% of the precision-recall space, the alleged problem is severe, presuming the probability of reaching a given point in precision-recall space is evenly distributed.</p><p>In what follows, we pick up the analysis where Grozea and Popescu left off and show that the problem is in fact not that severe. To begin with, observe that the entire left half of the precision-recall space, where precision is below 0.5, is not interesting in practice. This area corresponds to plagiarism detectors for which more than half of the text reported as plagiarism actually is not plagiarized. To be considered useful, a detector should achieve precision at least above 0.5. The ranking of poor performing detectors below that is unimportant. Further note that, when the recall of detector A approaches the perfect recall of detector B, their ranking becomes less and less counterintuitive until, at perhaps r ec A = 0.8, humans would start preferring detector A over B for A's perfect granularity and despite its less than perfect recall. Say, the higher the recall, the more important other performance measures become. These considerations significantly reduce the size of the critical area in the precision-recall space, as shown left in Figure <ref type="figure" coords="19,190.07,573.61,3.74,8.64" target="#fig_4">4</ref>. However, the critical area can be reduced even further when adjusting detector B to be more realistic. When choosing r ec B = 0.7 which is the best recall achieved this year, and gran B = 1.1, which is this year's average granularity (exclud-  ing the detector of Jayapal <ref type="bibr" coords="20,240.77,400.32,14.94,8.64" target="#b16">[14]</ref>), the resulting plagdet differences of detectors A and B are less pronounced (see right plot in Figure <ref type="figure" coords="20,311.74,412.28,3.60,8.64" target="#fig_4">4</ref>). Still, a precision below 0.5 remains uninteresting, and a recall r ec A &gt; 0.7 violates the above condition that r ec A &lt; r ec B . Thus, the critical area becomes much smaller than before, and even more so when taking into account that, when r ec A approaches r ec B , Grozea and Popescu's claim that rankings within the critical area are always counterintuitive turns out to be false. Moreover, the actual plagdet differences between detectors A and B are much smaller than 0.1, so that rankings of more than one detector may only be affected locally, but not globally. Altogether, we conclude that the problem of possible counterintuitive rankings of plagiarism detectors under plagdet is not as severe as previously thought. Hence, we believe that introducing a new hyper-parameter to counter this issue-as is proposed by Grozea and Popescu-is not of primary concern, and would only add confusion. In general, however, there is still a lot of room to improve existing performance measures and to invent new ones. In the future, we will again take a closer look in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Survey of Detection Approaches</head><p>Eleven of the 15 participants submitted softwares for the detailed comparison task, ten of whom also submitted at notebook describing their approach. An analysis of these notebooks reveals a number of building blocks that were commonly used to build detailed comparison algorithms: (1) seeding, (2) match merging, and (3) extraction filtering. In what follows, we describe them in detail.</p><p>Seeding. Given a suspicious document and a source document, matches (also called "seeds") between the two documents are identified using some seed heuristic. Seed heuristics either identify exact matches or create matches by changing the underlying texts in a domain-specific or linguistically motivated way. Rationale for this is to pinpoint substrings that altogether make up for the perceived similarity between suspicious and source document. By coming up with as many reasonable seeds as possible, the subsequent step of "growing" them into aligned passages of text becomes a lot easier.</p><p>A number of seed heuristics have been applied by this year's participants: Kong et al. <ref type="bibr" coords="21,157.40,214.95,16.60,8.64" target="#b19">[17]</ref> use sentences from the suspicious and source document whose surrounding passages have a similarity above some threshold and whose sentence-overlap is above another threshold. Suchomel et al. <ref type="bibr" coords="21,271.23,238.86,16.60,8.64" target="#b34">[32]</ref> use sorted word 5-grams and unsorted stop word 8-grams (the latter having been introduced in <ref type="bibr" coords="21,314.74,250.82,14.94,8.64" target="#b32">[30]</ref>). Grozea and Popescu <ref type="bibr" coords="21,422.23,250.82,16.60,8.64" target="#b13">[11]</ref> and Oberreuter al. <ref type="bibr" coords="21,182.34,262.77,16.60,8.64" target="#b21">[19]</ref> use char 16-grams and char 18-grams. Rodríguez Torrejón and Martín Ramos <ref type="bibr" coords="21,165.38,274.73,16.60,8.64" target="#b30">[28]</ref> use sorted word 3-grams and sorted word 1-skip-3-grams. Palkovskii and Belov <ref type="bibr" coords="21,161.07,286.69,16.60,8.64" target="#b22">[20]</ref> use word 3-grams, Küppers and Conrad <ref type="bibr" coords="21,342.48,286.69,16.60,8.64" target="#b20">[18]</ref> use non-overlapping 250 char chunks whose word-based similarity under Dice's coefficient is above some threshold, Sánchez-Vega et al. <ref type="bibr" coords="21,216.91,310.60,16.60,8.64" target="#b31">[29]</ref> use single words, and Jayapal <ref type="bibr" coords="21,359.67,310.60,16.60,8.64" target="#b16">[14]</ref> uses char n-grams where n ranges from 3 to 15. Before computing seeds, some participants choose to collapse whitespace, reduce cases, remove stop words, and stem the remaining words, if applicable to their respective seed heuristics. However, the idea of synonym normalization used in previous years appears to have been forgotten or discarded.</p><p>Match Merging. Given seed matches identified between a suspicious document and a source document, they are merged into aligned text passages of maximal length between the two documents which are then reported as plagiarism detections. Rationale for merging seed matches is to determine whether a document contains plagiarized passages at all rather than just seeds matching by chance, and to identify a plagiarized passage as a whole rather than only its fragments.</p><p>Most of the participants' match merging heuristics are rule-based, merging seeds into aligned passages if they are adjacent in both suspicious and source document and the size of the gap between them is below some threshold. The exact rule depends on the seeds used, and instead of using just one rule, many participants develop sets of constraints that have to be fulfilled by aligned passages in order to be reported as plagiarism detections. Since the rules are usually highly involved with their respective setup, we exemplify only one rule set here in order to give an idea of what they may look like: Suchomel et al. <ref type="bibr" coords="21,200.26,543.72,16.60,8.64" target="#b34">[32]</ref> employ a 2-step merge heuristic, where in the first step, adjacent seed matches that are no more than 4000 chars apart are merged. The resulting passages from the first step are then merged again, considering pairs of adjacent passages in turn, and checking if the gap between them contains at least four seeds so that there is at least one seed per 10 000 chars of gap length between them. To be merged, adjacent passages further have to fulfill the constraints that their gap is smaller than 30 000 chars, that their combined size is bigger than twice the gap size, and that the ratio of seeds per chars of the adjacent passages does not drop by a factor of more than three in the potentially merged passage. The only participants who go beyond rule-based merging are Grozea and Popescu <ref type="bibr" coords="21,219.67,651.32,15.27,8.64" target="#b13">[11]</ref>, who combine rules with randomization, and Palkovskii and Belov <ref type="bibr" coords="21,160.91,663.27,15.27,8.64" target="#b22">[20]</ref>, who employ clustering algorithms for unsupervised merging.</p><p>Passage Filtering. Given a set of aligned passages, a passage filter removes all aligned passages that do not meet certain criteria. Rationale for this is mainly to deal with overlapping passages and to discard extremely short passages. Kong et al. <ref type="bibr" coords="22,432.59,143.22,16.60,8.64" target="#b19">[17]</ref> discard passages whose word overlap under a modified Jaccard coefficient is below a threshold. Suchomel et al. <ref type="bibr" coords="22,199.99,167.13,16.60,8.64" target="#b34">[32]</ref> discard overlapping passages that are shorter than 300 chars, and keep only the passages longer than 300 chars. Oberreuter et al. <ref type="bibr" coords="22,394.06,179.09,16.60,8.64" target="#b21">[19]</ref> discard passages than 120 chars and Palkovskii and Belov <ref type="bibr" coords="22,340.25,191.04,16.60,8.64" target="#b22">[20]</ref> discard passages shorter than 190 chars. Gillam et al. <ref type="bibr" coords="22,232.12,203.00,11.62,8.64" target="#b8">[7]</ref> discard passages shorter than 50 words that have less than 0.75 cosine similarity under a vector space model. The other participants do not apply passage filtering.</p><p>Remarks. Since seven of this year's participants have taken part in previous competitions as well, many of them have simply reused their earlier solutions, some repeatedly. Others have simply used existing algorithms like greedy string tiling and BLAST out of the box. While there is no problem with doing so, this indicates that participants are maybe at a loss about how to further improve their approaches or devise new ones. That said, the winning approach of Kong et al. <ref type="bibr" coords="22,306.37,316.57,16.60,8.64" target="#b19">[17]</ref> is entirely new to the competition, but unfortunately no details are disclosed about it because of a patent pending.</p><p>The detailed comparison of documents for plagiarism detection is closely related to sequence alignment in bioinformatics, of which the terminology used above is borrowed. Oberreuter et al. <ref type="bibr" coords="22,232.36,364.39,16.60,8.64" target="#b21">[19]</ref> are the first to explore this connection by applying one of the algorithms from the well-known BLAST family. Moreover, a number of new ideas could be observed:</p><p>-Sánchez-Vega et al. <ref type="bibr" coords="22,234.43,408.23,16.60,8.64" target="#b31">[29]</ref> apply scored seeding and merge seed matches based on their scores instead of just their proximity. This idea is also relates to sequence alignment algorithms. -Palkovskii and Belov <ref type="bibr" coords="22,237.47,444.09,16.60,8.64" target="#b22">[20]</ref> and Jayapal <ref type="bibr" coords="22,304.39,444.09,16.60,8.64" target="#b16">[14]</ref> try to adjust their approaches differently based on the situation at hand (i.e., based on how strong a plagiarism case is obfuscated). Although the two approaches, in their current state, are not that successful, this idea points into a promising new direction for tailoring detailed comparison algorithms to certain situations instead of developing a one-fits-all approach. -Suchomel et al. <ref type="bibr" coords="22,214.41,503.87,16.60,8.64" target="#b34">[32]</ref> and Palkovskii and Belov <ref type="bibr" coords="22,335.06,503.87,16.60,8.64" target="#b22">[20]</ref> are the first to employ more than one seed heuristic at the same time. This shows that combining multiple sources of information may further help to devise better detailed comparison algorithms. -Suchomel et al. <ref type="bibr" coords="22,219.81,539.74,16.60,8.64" target="#b34">[32]</ref> and Palkovskii and Belov <ref type="bibr" coords="22,349.48,539.74,16.60,8.64" target="#b22">[20]</ref> merge seed matches not in one but two steps. This iterative merging of seeds allows for using different merge heuristics at different levels of abstraction, thus reducing the risk of making errors. Again, a one-fits-all approach is probably more difficult to develop and maintain.</p><p>Finally, regarding the detection of cross-language plagiarism, most participants simply resorted to using the translated version of a document obtained from Google Translate which was provided as an additional parameter in the test phase. Some disregarded non-English cases altogether, while only Rodríguez Torrejón and Martín Ramos <ref type="bibr" coords="22,464.00,631.39,16.60,8.64" target="#b30">[28]</ref> continued to develop their own dictionary-based solution. To sum up the fourth international competition on plagiarism detection at PAN'12, we have introduced a number of novelties: a new evaluation framework that approaches plagiarism detection step-wise instead of as a whole, and that allows for software submissions instead of just result submissions. We are the first to consider the heretofore neglected, yet important scenario of plagiarism detection from the web at a representative scale. Furthermore, we introduce new tools to evaluate plagiarism detectors, namely the ChatNoir search engine, and the TIRA experimentation platform. Both allow for assessing a plagiarism detector's performance based on new measures, such as runtime and web retrieval performance. We have constructed a new, large plagiarism corpus based on crowdsourcing that consists of entirely manually written plagiarism cases.</p><p>Our corpus construction pipeline emulates the entire process of plagiarizing text from the web in a controlled environment. The data collected allows for a first time glimpse over the shoulders of plagiarists. Finally, we have used real plagiarism cases to evaluate detection performance, which was made possible by our new evaluation tools that do not require test data to be handed out to participants. Otherwise, the use of real plagiarism for evaluation would cause legal and ethical problems.</p><p>We have demonstrated that a step-wise evaluation of plagiarism detectors is the way forward, whereas the two plagiarism detection subtasks candidate retrieval and detailed comparison pose new challenges for evaluation. In the coming competitions, we plan on improving our evaluation framework in order to reach a new stable point at which evaluations within the framework can be run smoothly out of the box. In particular, we will encourage software submissions not only for detailed comparison but also for candidate retrieval, again using the TIRA experimentation platform to facilitate this goal. Our vision is to implement a fully automatic, web-based plagiarism detection evaluator, available to all researchers in this field.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,200.31,223.11,214.73,8.12"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The basic steps of plagiarizing from the web [21].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,134.77,291.99,345.81,8.12;8,134.77,303.30,165.97,7.77;8,229.13,201.10,69.25,56.44"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Corpus example (topic 058): plagiarized passages can be traced back to their source web pages and the queries that retrieved them.</figDesc><graphic coords="8,229.13,201.10,69.25,56.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="10,139.25,559.23,131.44,8.64;10,139.25,570.38,153.59,8.64;10,139.25,581.54,313.31,8.64;10,139.25,592.69,232.72,8.64;10,139.25,603.85,272.07,8.64"><head>1 .</head><label>1</label><figDesc>Number of queries submitted. 2. Number of web pages downloaded. 3. Precision and recall of web pages downloaded regarding the actual sources. 4. Number of queries until the first actual source is found. 5. Number of downloads until the first actual source is downloaded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="20,134.77,312.24,345.83,8.12;20,134.77,323.26,319.40,8.06;20,454.16,327.55,5.52,5.24;20,463.38,323.26,17.21,7.86;20,134.77,334.22,62.71,7.86;20,197.47,338.50,5.54,5.24;20,207.14,334.22,53.59,8.06;20,260.73,338.50,5.52,5.24;20,270.10,334.22,28.48,7.86;20,298.57,334.51,182.01,9.24;20,134.77,345.18,19.89,7.86;20,154.65,349.46,5.52,5.24;20,163.98,345.18,89.72,7.86;20,253.70,349.46,5.54,5.24;20,263.33,345.18,60.66,8.06;20,323.99,349.46,5.52,5.24;20,333.32,345.18,28.43,7.86;20,361.75,345.47,118.84,9.24;20,134.77,356.14,327.41,8.06"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Contour plots of the differences of the plagdet scores of two plagiarism detectors A and B in the precision-recall space. The left plot shows the situation when setting gran A = 1, r ecB = 1, gran B = 2, and prec A = prec B . The right plot shows the situation when setting gran A = 1, r ecB = 0.7, gran B = 1.1, and prec A = prec B . The line markings delineate a critical area in which detectors A and B might be ranked counterintuitively under plagdet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="13,134.77,115.83,345.81,195.34"><head>Table 1 .</head><label>1</label><figDesc>Performances on the candidate retrieval subtask. Values are averaged over the 32 suspicious documents from the test corpus. The top half of the table shows performances when interpreting near-duplicates of the actual source documents as true positives; the bottom half of the table shows performances without considering near-duplicates true positives.</figDesc><table coords="13,135.81,168.84,343.75,142.33"><row><cell></cell><cell cols="2">Total</cell><cell cols="2">Time to</cell><cell>No</cell><cell>Reported</cell><cell>Downloaded</cell><cell>Retrieved</cell></row><row><cell>Team</cell><cell cols="2">Workload</cell><cell cols="2">1st Detection</cell><cell>Detection</cell><cell>Sources</cell><cell>Sources</cell><cell>Sources</cell></row><row><cell></cell><cell cols="4">Queries Downloads Queries Downloads</cell><cell></cell><cell cols="2">Precision Recall Precision Recall Precision Recall</cell></row><row><cell>Gillam</cell><cell>63.44</cell><cell>527.41</cell><cell>4.47</cell><cell>25.88</cell><cell>1</cell><cell cols="2">0.6266 0.2493 0.0182 0.5567 0.0182 0.5567</cell></row><row><cell>Jayapal</cell><cell>67.06</cell><cell>173.47</cell><cell>8.78</cell><cell>13.50</cell><cell>1</cell><cell cols="2">0.6582 0.2775 0.0709 0.4342 0.0698 0.4342</cell></row><row><cell>Kong</cell><cell>551.06</cell><cell cols="2">326.66 80.59</cell><cell>27.47</cell><cell>2</cell><cell cols="2">0.5720 0.2351 0.0178 0.3742 0.0141 0.3788</cell></row><row><cell>Palkovskii</cell><cell>63.13</cell><cell cols="2">1026.72 27.28</cell><cell>318.94</cell><cell>6</cell><cell cols="2">0.4349 0.1203 0.0025 0.2133 0.0024 0.2133</cell></row><row><cell>Suchomel</cell><cell>12.56</cell><cell>95.41</cell><cell>1.53</cell><cell>6.28</cell><cell>2</cell><cell cols="2">0.5177 0.2087 0.0813 0.3513 0.0094 0.4519</cell></row><row><cell>Gillam</cell><cell>63.44</cell><cell cols="2">527.41 52.38</cell><cell>445.25</cell><cell>22</cell><cell cols="2">0.0310 0.0414 0.0016 0.0526 0.0019 0.0526</cell></row><row><cell>Jayapal</cell><cell>67.06</cell><cell cols="2">173.47 39.00</cell><cell>115.13</cell><cell>16</cell><cell cols="2">0.0328 0.0394 0.0079 0.0994 0.0108 0.0994</cell></row><row><cell>Kong</cell><cell>551.06</cell><cell cols="2">326.66 440.59</cell><cell>274.06</cell><cell>21</cell><cell cols="2">0.0280 0.0458 0.0019 0.0391 0.0015 0.0435</cell></row><row><cell>Palkovskii</cell><cell>63.13</cell><cell cols="2">1026.72 54.88</cell><cell>881.34</cell><cell>25</cell><cell cols="2">0.0246 0.0286 0.0002 0.0286 0.0002 0.0364</cell></row><row><cell>Suchomel</cell><cell>12.56</cell><cell cols="2">95.41 11.16</cell><cell>93.72</cell><cell>30</cell><cell cols="2">0.0208 0.0124 0.0007 0.0124 0.0003 0.0208</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="17,165.06,115.83,285.23,158.52"><head>Table 2 .</head><label>2</label><figDesc>Corpus statistics of the PAN 2012 detailed comparison test corpus.</figDesc><table coords="17,165.06,140.92,285.23,133.43"><row><cell cols="2">Evaluation Corpus Statistics</cell><cell></cell></row><row><cell>Sub-Corpus</cell><cell>Number of Cases</cell><cell>Avg. Cosine Similarity</cell></row><row><cell>Real Cases</cell><cell>33</cell><cell>0.161</cell></row><row><cell>Simulated</cell><cell>500</cell><cell>0.364</cell></row><row><cell>Translation ({de, es} → en)</cell><cell>500</cell><cell>0.018</cell></row><row><cell>Artificial (High)</cell><cell>500</cell><cell>0.392</cell></row><row><cell>Artificial (Low)</cell><cell>500</cell><cell>0.455</cell></row><row><cell>No Obfuscation</cell><cell>500</cell><cell>0.560</cell></row><row><cell>No Plagiarism</cell><cell>500</cell><cell>0.431</cell></row><row><cell>Overall / Averaged</cell><cell>3033</cell><cell>0.369</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="23,143.47,115.83,328.42,44.29"><head>Table 3 .</head><label>3</label><figDesc>Implementation details of the detailed comparison submissions, sorted by runtime.</figDesc><table coords="23,154.70,140.92,302.46,19.19"><row><cell>Team</cell><cell cols="4">Submission Operating Programming Average Runtime</cell></row><row><cell></cell><cell>Size [MB]</cell><cell>System</cell><cell>Language</cell><cell>[sec/comparison]</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,144.73,623.72,168.75,7.77"><p>http://plagiat.htw-berlin.de/software-en/2010-2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="3,144.73,634.87,136.36,7.77"><p>http://www.webis.de/research/corpora</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="3,144.73,646.02,92.23,7.77"><p>http://www.gutenberg.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="5,144.73,634.95,108.21,7.77"><p>http://trec.nist.gov/tracks.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="5,144.73,646.10,138.08,7.77"><p>http://lemurproject.org/clueweb09.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="5,144.73,657.09,86.62,7.77"><p>http://commoncrawl.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6" coords="6,144.73,641.06,272.29,7.77"><p>http://boston.lti.cs.cmu.edu/clueweb09/wiki/tiki-index.php?page=PageRank</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7" coords="6,144.73,652.21,84.18,7.77"><p>http://chatnoir.webis.de</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8" coords="8,144.73,655.75,81.11,7.77"><p>http://www.odesk.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9" coords="18,144.73,645.84,335.86,8.06;18,144.73,657.09,327.11,7.77"><p>In Grozea and Popescu's example, B's performance characteristic is first defined as 1, 1, 3 , but later the granularity is changed to 2. For consistency, we set granularity to 2 right away.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_10" coords="19,144.73,617.14,190.76,8.06;19,335.49,621.43,5.52,5.24;19,345.57,617.14,41.38,7.86;19,386.96,617.43,93.63,9.24;19,144.73,628.39,189.23,7.77"><p>Grozea and  Popescu add another condition plagdet A &gt; plagdet B , which follows directly from the above conditions and can hence be omitted.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_11" coords="19,144.73,639.54,335.85,7.77;19,144.73,650.21,161.52,8.06"><p>The plot of Grozea and Popescu shows only the contour labeled 0, say, the line at which the ranking of A and B under plagdet switches.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank the participants of <rs type="institution">PAN</rs> for their dedicated work, and for being patient with our ever changing idea of how plagiarism detectors should be evaluated. This work was partly funded by the <rs type="programName">EC WIQ-EI</rs> project (project no. <rs type="grantNumber">269180</rs>) within the <rs type="programName">FP7 People Program</rs>, by the <rs type="projectName">MICINN Text-Enterprise</rs> (<rs type="grantNumber">TIN2009-13391-C04-03</rs>) research project, and by the <rs type="funder">ERCIM</rs> "<rs type="programName">Alain Bensoussan" Fellowship Programme</rs> (funded from the <rs type="funder">European Union</rs> <rs type="programName">Seventh Framework Programme</rs> <rs type="grantNumber">FP7/2007-2013</rs> under grant agreement number <rs type="grantNumber">246016</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_C3Mqycm">
					<idno type="grant-number">269180</idno>
					<orgName type="program" subtype="full">EC WIQ-EI</orgName>
				</org>
				<org type="funded-project" xml:id="_RcRRjvm">
					<idno type="grant-number">TIN2009-13391-C04-03</idno>
					<orgName type="project" subtype="full">MICINN Text-Enterprise</orgName>
					<orgName type="program" subtype="full">FP7 People Program</orgName>
				</org>
				<org type="funding" xml:id="_3XGe53A">
					<orgName type="program" subtype="full">Alain Bensoussan&quot; Fellowship Programme</orgName>
				</org>
				<org type="funding" xml:id="_29MeXzB">
					<idno type="grant-number">FP7/2007-2013</idno>
					<orgName type="program" subtype="full">Seventh Framework Programme</orgName>
				</org>
				<org type="funding" xml:id="_r7TbSkK">
					<idno type="grant-number">246016</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation Results</head><p>The softwares submitted to the detailed comparison subtask vary widely with respect to their sizes, employed programming languages, and runtime performances (cf. Table 3 for details). Regarding runtime, the submission of Rodríguez Torrejón and Martín Ramos <ref type="bibr" coords="23,165.04,393.42,16.60,8.64" target="#b30">[28]</ref> is outstanding: it achieves 0.19 seconds per comparison, which is an order of magnitude faster than all other submissions. The two fastest approaches are both implemented in C/C++. Note that we always tried to run a submission on a Linux system and resorted to Windows only if necessary. Table <ref type="table" coords="23,174.15,441.24,4.98,8.64">4</ref> shows the detection performances of the ten evaluated detailed comparison approaches. Besides the overall performance on the complete evaluation corpus (first value in brackets), also the performance on each sub-corpus is given (remaining values in brackets). Note that for the sub-corpus without plagiarism, no performance values can be stated due to the lack of true positives. However, false positive detections for this sub-corpus influenced the overall performance of course. The winner of this year's detailed comparison task is the approach by Kong et al. <ref type="bibr" coords="23,353.88,512.97,15.27,8.64" target="#b19">[17]</ref>, which achieves the highest plagdet score on the complete corpus, the real cases, the simulated paraphrase cases, as well as on the cross language cases. The best performing approach on the artificial paraphrase cases (both high and low) was submitted by Oberreuter et al. <ref type="bibr" coords="23,404.25,548.84,15.27,8.64" target="#b21">[19]</ref>, whose overall performance suffered from a bad performance on the cross-language cases. Suchomel et al. <ref type="bibr" coords="23,157.30,572.75,15.27,8.64" target="#b34">[32]</ref>, who submitted the second best approach, performed best on the plagiarism cases without any obfuscation. Regarding real plagiarism cases, the ranking from the second place onwards would change; here, Rodríguez Torrejón and Martín Ramos <ref type="bibr" coords="23,464.00,596.66,16.60,8.64" target="#b30">[28]</ref> achieve the best recall. Most approaches perform better on real plagiarism cases compared to their overall performance. However, these results must be taken with a grain of salt, since the statistical mass of 33 real cases is too small to claim good performance in detecting real plagiarism. Nevertheless, the inclusion of real cases adds more realism to our evaluation, and we strive to scale up their number in the future.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="24,192.35,632.89,4.03,6.72;24,188.67,581.39,7.77,49.30;24,188.67,570.22,7.77,8.97;24,188.67,557.06,7.77,10.96;24,188.67,526.47,7.77,28.39;24,188.67,482.43,7.77,41.84;24,188.67,451.08,7.77,29.14;24,188.67,436.56,7.77,12.32;24,188.67,417.93,7.77,16.43;24,188.67,370.42,7.77,45.31;24,188.67,343.07,7.77,25.15;24,188.67,329.91,7.77,10.96;24,188.67,296.19,7.77,31.51;24,188.67,275.07,7.77,18.92;24,188.67,266.88,7.77,5.98;24,188.67,243.76,7.77,20.92;24,188.67,234.58,7.77,6.98;24,188.67,221.42,7.77,10.96;24,188.67,205.26,7.77,13.95;24,188.67,184.63,7.77,18.43;24,188.67,161.01,7.77,21.42;24,188.67,147.84,7.77,10.96;24,188.67,115.85,7.77,29.79;24,199.63,652.47,7.77,10.45;24,199.63,633.06,7.77,16.43;24,199.63,610.40,7.77,19.67;24,199.63,593.47,7.77,13.94;24,199.63,554.13,7.77,36.36;24,199.63,527.24,7.77,23.90;24,199.63,517.28,7.77,6.98;24,199.63,497.87,7.77,16.43;24,199.63,468.58,7.77,26.30;24,199.63,448.68,7.77,16.92;24,199.63,438.72,7.77,6.98;24,199.63,424.78,7.77,10.96;24,199.63,378.97,7.77,42.82;24,199.63,359.55,7.77,16.43;24,199.63,333.40,7.77,23.16;24,199.63,294.05,7.77,36.37;24,199.63,246.14,7.77,44.93;24,199.63,196.90,7.77,46.26;24,199.63,161.53,7.77,32.38;24,199.63,115.86,7.77,42.69;24,210.59,642.75,7.77,20.17;24,210.59,608.13,7.77,32.38;24,210.59,563.20,7.77,42.69;24,210.59,543.09,7.77,17.87;24,210.59,527.90,7.77,12.95;24,210.59,514.71,7.77,10.96;24,210.59,466.40,7.77,46.07;24,210.59,450.21,7.77,13.94;24,210.59,433.53,7.77,14.44;24,210.59,412.37,7.77,18.92;24,210.59,403.15,7.77,6.98;24,210.59,384.48,7.77,16.43;24,210.59,355.34,7.77,26.90;24,210.59,347.12,7.77,5.98;24,210.59,301.79,7.77,43.09;24,235.39,655.61,8.06,20.93;24,235.39,650.88,8.06,2.49;24,235.39,627.55,8.06,21.09;24,235.39,533.86,8.06,30.38;24,235.39,406.51,8.06,35.20;24,235.39,287.21,8.06,23.90;24,235.39,151.06,8.06,46.33;24,255.76,551.78,7.77,2.99;24,255.76,522.03,7.77,26.75;24,255.76,516.81,7.77,2.24;24,255.76,491.40,7.77,16.43;24,255.76,466.00,7.77,23.16;24,255.76,418.43,7.77,38.61;24,255.76,366.70,7.77,42.77;24,255.76,325.35,7.77,32.38;24,255.76,302.94,7.77,20.17;24,255.76,261.60,7.77,32.38;24,255.76,241.49,7.77,17.87;24,255.76,221.56,7.77,10.96;24,255.76,172.51,7.77,46.81;24,278.39,673.05,6.05,3.49;24,278.39,654.32,6.05,15.25;24,278.39,605.21,6.05,2.32;24,278.12,590.02,6.27,12.20;24,278.39,585.29,6.05,1.74;24,278.12,569.60,6.27,13.95;24,278.12,553.91,6.27,13.95;24,278.12,538.22,6.27,13.95;24,278.39,522.53,6.05,13.95;24,278.39,506.84,6.05,13.95;24,278.39,490.56,6.05,14.53;24,278.39,480.27,6.05,2.32;24,278.39,465.08,6.05,12.20;24,278.39,460.34,6.05,1.74;24,278.39,444.65,6.05,13.95;24,278.39,428.96,6.05,13.95;24,278.39,413.27,6.05,13.95;24,278.39,397.58,6.05,13.95;24,278.39,381.89,6.05,13.95;24,278.39,365.62,6.05,14.53;24,278.39,355.32,6.05,2.32;24,278.12,340.14,6.27,12.20;24,278.39,335.40,6.05,1.74;24,278.39,319.71,6.05,13.95;24,278.12,304.02,6.27,13.95;24,278.12,288.33,6.27,13.95;24,278.12,272.64,6.27,13.95;24,278.39,256.95,6.05,13.95;24,278.39,240.68,6.05,14.53;24,278.39,230.38,6.05,2.32;24,278.39,215.19,6.05,12.20;24,278.39,210.46,6.05,1.74;24,278.39,194.77,6.05,13.95;24,278.39,179.07,6.05,13.95;24,278.39,163.38,6.05,13.95;24,278.39,147.69,6.05,13.95;24,278.39,132.00,6.05,13.95;24,278.39,115.73,6.05,14.53;24,295.07,673.05,6.05,3.49;24,295.07,641.67,6.05,27.90;24,295.07,605.21,6.05,2.32;24,295.07,590.02,6.05,12.20;24,295.07,585.29,6.05,1.74;24,295.07,569.60,6.05,13.95;24,295.07,553.90,6.05,13.95;24,295.07,538.21,6.05,13.95;24,295.07,522.52,6.05,13.95;24,295.07,506.83,6.05,13.95;24,294.80,490.56,6.27,14.53;24,295.07,480.27,6.05,2.32;24,295.07,465.08,6.05,12.20;24,295.07,460.34,6.05,1.74;24,295.07,444.65,6.05,13.95;24,295.07,428.96,6.05,13.95;24,295.07,413.27,6.05,13.95;24,295.07,397.58,6.05,13.95;24,295.07,381.89,6.05,13.95;24,295.07,365.62,6.05,14.53;24,295.07,355.32,6.05,2.32;24,295.07,340.14,6.05,12.20;24,295.07,335.40,6.05,1.74;24,295.07,319.71,6.05,13.95;24,295.07,304.02,6.05,13.95;24,295.07,288.33,6.05,13.95;24,295.07,272.63,6.05,13.95;24,295.07,256.94,6.05,13.95;24,295.07,240.67,6.05,14.53;24,295.07,230.39,6.05,2.32;24,295.07,215.19,6.05,12.20;24,295.07,210.46,6.05,1.74;24,295.07,194.77,6.05,13.95;24,295.07,179.07,6.05,13.95;24,295.07,163.38,6.05,13.95;24,295.07,147.69,6.05,13.95;24,295.07,132.00,6.05,13.95;24,295.07,115.73,6.05,14.53;24,311.76,673.05,6.05,3.49;24,311.76,649.43,6.05,20.13;24,311.76,605.21,6.05,2.32;24,311.76,590.02,6.05,12.20;24,311.76,585.29,6.05,1.74;24,311.76,569.60,6.05,13.95;24,311.76,553.90,6.05,13.95;24,311.76,538.21,6.05,13.95;24,311.76,522.52,6.05,13.95;24,311.76,506.83,6.05,13.95;24,311.76,490.56,6.05,14.53;24,311.76,480.27,6.05,2.32;24,311.76,465.08,6.05,12.20;24,311.76,460.34,6.05,1.74;24,311.76,444.65,6.05,13.95;24,311.76,428.96,6.05,13.95;24,311.76,413.27,6.05,13.95;24,311.76,397.58,6.05,13.95;24,311.76,381.89,6.05,13.95;24,311.76,365.62,6.05,14.53;24,311.76,355.32,6.05,2.32;24,311.76,340.14,6.05,12.20;24,311.76,335.40,6.05,1.74;24,311.76,319.71,6.05,13.95;24,311.76,304.02,6.05,13.95;24,311.76,288.33,6.05,13.95;24,311.76,272.63,6.05,13.95;24,311.76,256.94,6.05,13.95;24,311.76,240.67,6.05,14.53;24,311.76,230.39,6.05,2.32;24,311.76,215.19,6.05,12.20;24,311.76,210.46,6.05,1.74;24,311.76,194.77,6.05,13.95;24,311.76,179.07,6.05,13.95;24,311.76,163.38,6.05,13.95;24,311.76,147.69,6.05,13.95;24,311.76,132.00,6.05,13.95;24,311.76,115.73,6.05,14.53;24,328.44,673.05,6.05,3.49;24,328.44,641.11,6.05,30.20;24,328.44,605.21,6.05,2.32;24,328.44,590.02,6.05,12.20;24,328.44,585.29,6.05,1.74;24,328.44,569.60,6.05,13.95;24,328.44,553.90,6.05,13.95;24,328.44,538.21,6.05,13.95;24,328.17,522.53,6.27,13.95;24,328.17,506.83,6.27,13.95;24,328.44,490.56,6.05,14.53;24,328.44,480.27,6.05,2.32;24,328.44,465.08,6.05,12.20;24,328.44,460.34,6.05,1.74;24,328.44,444.65,6.05,13.95;24,328.44,428.96,6.05,13.95;24,328.17,413.27,6.27,13.95;24,328.17,397.58,6.27,13.95;24,328.17,381.89,6.27,13.95;24,328.44,365.62,6.05,14.53;24,328.44,355.33,6.05,2.32;24,328.44,340.14,6.05,12.20;24,328.44,335.40,6.05,1.74;24,328.44,319.71,6.05,13.95;24,328.44,304.02,6.05,13.95;24,328.44,288.33,6.05,13.95;24,328.44,272.63,6.05,13.95;24,328.17,256.95,6.27,13.95;24,328.17,240.68,6.27,14.53;24,328.44,230.38,6.05,2.32;24,328.44,215.19,6.05,12.20;24,328.44,210.46,6.05,1.74;24,328.44,194.77,6.05,13.95;24,328.44,179.07,6.05,13.95;24,328.44,163.38,6.05,13.95;24,328.44,147.69,6.05,13.95;24,328.44,132.00,6.05,13.95;24,328.44,115.73,6.05,14.53;24,345.12,673.05,6.05,3.49;24,345.12,640.51,6.05,29.05;24,345.12,615.50,6.05,23.84;24,345.12,605.21,6.05,2.32;24,345.12,590.02,6.05,12.20;24,345.12,585.29,6.05,1.74;24,345.12,569.60,6.05,13.95;24,345.12,553.90,6.05,13.95;24,345.12,538.21,6.05,13.95;24,345.12,522.52,6.05,13.95;24,345.12,506.83,6.05,13.95;24,345.12,490.56,6.05,14.53;24,345.12,480.27,6.05,2.32;24,345.12,465.08,6.05,12.20;24,345.12,460.34,6.05,1.74;24,345.12,444.65,6.05,13.95;24,345.12,428.96,6.05,13.95;24,345.12,413.27,6.05,13.95;24,345.12,397.58,6.05,13.95;24,345.12,381.89,6.05,13.95;24,345.12,365.62,6.05,14.53;24,345.12,355.32,6.05,2.32;24,345.12,340.14,6.05,12.20;24,345.12,335.40,6.05,1.74;24,344.85,319.71,6.27,13.95;24,345.12,304.02,6.05,13.95;24,345.12,288.33,6.05,13.95;24,345.12,272.64,6.05,13.95;24,345.12,256.95,6.05,13.95;24,345.12,240.68,6.05,14.53;24,345.12,230.38,6.05,2.32;24,345.12,215.19,6.05,12.20;24,345.12,210.46,6.05,1.74;24,345.12,194.77,6.05,13.95;24,345.12,179.07,6.05,13.95;24,345.12,163.38,6.05,13.95;24,345.12,147.69,6.05,13.95;24,345.12,132.00,6.05,13.95;24,345.12,115.73,6.05,14.53;24,361.81,673.05,6.05,3.49;24,361.81,642.14,6.05,29.17;24,361.81,605.21,6.05,2.32;24,361.81,590.02,6.05,12.20;24,361.81,585.29,6.05,1.74;24,361.81,569.60,6.05,13.95;24,361.81,553.90,6.05,13.95;24,361.81,538.21,6.05,13.95;24,361.81,522.52,6.05,13.95;24,361.81,506.83,6.05,13.95;24,361.81,490.56,6.05,14.53;24,361.81,480.27,6.05,2.32;24,361.81,465.08,6.05,12.20;24,361.81,460.34,6.05,1.74;24,361.81,444.65,6.05,13.95;24,361.81,428.96,6.05,13.95;24,361.81,413.27,6.05,13.95;24,361.81,397.58,6.05,13.95;24,361.81,381.89,6.05,13.95;24,361.81,365.62,6.05,14.53;24,361.81,355.32,6.05,2.32;24,361.81,340.14,6.05,12.20;24,361.81,335.40,6.05,1.74;24,361.81,319.71,6.05,13.95;24,361.81,304.02,6.05,13.95;24,361.81,288.33,6.05,13.95;24,361.81,272.63,6.05,13.95;24,361.81,256.94,6.05,13.95;24,361.81,240.67,6.05,14.53;24,361.81,230.39,6.05,2.32;24,361.81,215.19,6.05,12.20;24,361.81,210.46,6.05,1.74;24,361.81,194.77,6.05,13.95;24,361.81,179.07,6.05,13.95;24,361.81,163.38,6.05,13.95;24,361.81,147.69,6.05,13.95;24,361.81,132.00,6.05,13.95;24,361.81,115.73,6.05,14.53;24,378.49,673.05,6.05,3.49;24,378.49,645.94,6.05,23.63;24,378.49,605.21,6.05,2.32;24,378.49,590.02,6.05,12.20;24,378.49,585.29,6.05,1.74;24,378.49,569.60,6.05,13.95;24,378.49,553.90,6.05,13.95;24,378.49,538.21,6.05,13.95;24,378.49,522.52,6.05,13.95;24,378.49,506.83,6.05,13.95;24,378.49,490.56,6.05,14.53;24,378.49,480.27,6.05,2.32;24,378.49,465.08,6.05,12.20;24,378.49,460.34,6.05,1.74;24,378.49,444.65,6.05,13.95;24,378.22,428.97,6.27,13.95;24,378.49,413.27,6.05,13.95;24,378.49,397.58,6.05,13.95;24,378.49,381.89,6.05,13.95;24,378.49,365.62,6.05,14.53;24,378.49,355.33,6.05,2.32;24,378.49,340.14,6.05,12.20;24,378.49,335.40,6.05,1.74;24,378.49,319.71,6.05,13.95;24,378.49,304.02,6.05,13.95;24,378.49,288.33,6.05,13.95;24,378.49,272.63,6.05,13.95;24,378.49,256.94,6.05,13.95;24,378.49,240.67,6.05,14.53;24,378.49,230.39,6.05,2.32;24,378.49,215.19,6.05,12.20;24,378.49,210.46,6.05,1.74;24,378.49,194.77,6.05,13.95;24,378.49,179.07,6.05,13.95;24,378.49,163.38,6.05,13.95;24,378.49,147.69,6.05,13.95;24,378.49,132.00,6.05,13.95;24,378.49,115.73,6.05,14.53" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="24,206.80,196.90,0.60,46.26;24,199.63,161.53,7.77,32.38;24,199.63,115.86,7.77,42.69;24,210.59,642.75,7.77,20.17;24,210.59,608.13,7.77,32.38;24,210.59,563.20,7.77,42.69;24,210.59,543.09,7.77,17.87;24,210.59,527.90,7.77,12.95;24,210.59,514.71,7.77,10.96;24,210.59,466.40,7.77,46.07;24,210.59,450.21,7.77,13.94;24,210.59,433.53,7.77,14.44;24,210.59,412.37,7.77,18.92;24,210.59,403.15,7.77,6.98;24,210.59,384.48,7.77,16.43;24,210.59,355.34,7.77,26.90;24,210.59,347.12,7.77,5.98;24,210.59,301.79,7.13,43.09">Artificial Paraphrases High, Artificial Paraphrases Low, and No Obfuscation. The best score in each column is highlighted</title>
	</analytic>
	<monogr>
		<title level="j" coord="24,235.39,655.61,8.06,20.93;24,235.39,650.88,8.06,2.49;24,235.39,627.55,8.06,21.09;24,235.39,533.86,8.06,30.38;24,235.39,406.51,8.06,35.20;24,235.39,287.21,8.06,23.90;24,235.39,151.06,8.06,46.33;24,255.76,551.78,7.77,2.99;24,255.76,522.03,7.77,26.75;24,255.76,516.81,7.77,2.24;24,255.76,491.40,7.77,16.43;24,255.76,466.00,7.77,23.16;24,255.76,418.43,7.77,38.61;24,255.76,366.70,7.77,42.77;24,255.76,325.35,7.77,32.38;24,255.76,302.94,7.77,20.17;24,255.76,261.60,7.77,32.38;24,255.76,241.49,7.77,17.87;24,255.76,221.56,7.77,10.96;24,255.76,172.51,7.13,46.81">Rank / Team PlagDet Precision Recall Granularity [ Overall , Real Cases, Simulated, Translation, Artificial High, Artificial Low, No Obfuscation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">756</biblScope>
			<biblScope unit="page">257</biblScope>
			<date type="published" when="2000">900. 1.00, 1.00, 1.00, 1.02, 1.04, 1.00. 1.00, 1.00, 1.00, 1.00, 1.00, 1.00. 03 , 1.00, 1.00, 1.00, 1.10, 1.13, 1.00. 1.00. 1.00, 1.00, 1.00, 1.00, 1.02, 1.00</date>
		</imprint>
	</monogr>
	<note>Performances on the detailed comparison subtask. For each performance metric, the achieved score is stated in the first entry inside the brackets for each team. The remaining entries in each bracket refer to the sub-corpora Real Cases, Simulated Paraphrases, Translations. 1.01 ,. 1.00 ,. 1.00 ,. 5 Rodríguez Torrejón [ .625</note>
</biblStruct>

<biblStruct coords="24,395.17,673.05,6.05,3.49;24,395.17,630.21,6.05,39.36;24,395.17,605.21,6.05,2.32;24,395.17,590.02,6.05,12.20;24,395.17,585.29,6.05,1.74;24,395.17,569.60,6.05,13.95;24,395.17,553.90,6.05,13.95;24,395.17,538.21,6.05,13.95;24,395.17,522.52,6.05,13.95;24,395.17,506.83,6.05,13.95;24,395.17,490.56,6.05,14.53;24,395.17,480.27,6.05,2.32;24,395.17,465.08,6.05,12.20;24,395.17,460.34,6.05,1.74;24,395.17,444.65,6.05,13.95;24,395.17,428.96,6.05,13.95;24,395.17,413.27,6.05,13.95;24,395.17,397.58,6.05,13.95;24,395.17,381.89,6.05,13.95;24,395.17,365.62,6.05,14.53;24,395.17,355.32,6.05,2.32;24,395.17,340.14,6.05,12.20;24,395.17,335.40,6.05,1.74;24,395.17,319.71,6.05,13.95;24,395.17,304.02,6.05,13.95;24,395.17,288.33,6.05,13.95;24,395.17,272.63,6.05,13.95;24,395.17,256.94,6.05,13.95;24,395.17,240.67,6.05,14.53;24,395.17,230.39,6.05,2.32;24,395.17,215.19,6.05,12.20;24,395.17,210.46,6.05,1.74;24,395.17,194.77,6.05,13.95;24,395.17,179.07,6.05,13.95;24,395.17,163.38,6.05,13.95;24,395.17,147.69,6.05,13.95;24,395.17,132.00,6.05,13.95;24,395.17,115.73,6.05,14.53;24,411.93,673.05,6.05,3.49;24,411.93,650.19,6.05,19.37;24,411.93,605.21,6.05,2.32;24,411.93,590.02,6.05,12.20;24,411.93,585.29,6.05,1.74;24,411.93,569.60,6.05,13.95;24,411.93,553.90,6.05,13.95;24,411.93,538.21,6.05,13.95;24,411.93,522.52,6.05,13.95;24,411.93,506.83,6.05,13.95;24,411.93,490.56,6.05,14.53;24,411.93,480.27,6.05,2.32;24,411.66,465.08,6.27,12.20;24,411.93,460.35,6.05,1.74;24,411.66,444.66,6.27,13.95;24,411.93,428.96,6.05,13.95;24,411.93,413.27,6.05,13.95;24,411.93,397.58,6.05,13.95;24,411.93,381.89,6.05,13.95;24,411.93,365.62,6.05,14.53;24,411.93,355.33,6.05,2.32;24,411.93,340.14,6.05,12.20;24,411.93,335.40,6.05,1.74;24,411.93,319.71,6.05,13.95;24,411.93,304.02,6.05,13.95;24,411.93,288.33,6.05,13.95;24,411.93,272.63,6.05,13.95;24,411.93,256.94,6.05,13.95;24,411.93,240.67,6.05,14.53;24,411.93,230.39,6.05,2.32;24,411.93,215.19,6.05,12.20;24,411.93,210.46,6.05,1.74;24,411.93,194.77,6.05,13.95;24,411.93,179.07,6.05,13.95;24,411.93,163.38,6.05,13.95;24,411.93,147.69,6.05,13.95;24,411.93,132.00,6.05,13.95;24,411.93,115.73,6.05,14.53;24,428.62,669.57,6.05,6.97;24,428.62,645.17,6.05,20.92;24,428.62,605.21,6.05,2.32;24,428.62,590.02,6.05,12.20;24,428.62,585.29,6.05,1.74;24,428.62,569.60,6.05,13.95;24,428.62,553.90,6.05,13.95;24,428.62,538.21,6.05,13.95;24,428.62,522.52,6.05,13.95;24,428.62,506.83,6.05,13.95;24,428.62,490.56,6.05,14.53;24,428.62,480.27,6.05,2.32;24,428.62,465.08,6.05,12.20;24,428.62,460.34,6.05,1.74;24,428.62,444.65,6.05,13.95;24,428.62,428.96,6.05,13.95;24,428.62,413.27,6.05,13.95;24,428.62,397.58,6.05,13.95;24,428.62,381.89,6.05,13.95;24,428.34,365.62,6.27,14.53;24,428.62,355.33,6.05,2.32;24,428.62,340.14,6.05,12.20;24,428.62,335.40,6.05,1.74;24,428.62,319.71,6.05,13.95;24,428.62,304.02,6.05,13.95;24,428.62,288.33,6.05,13.95;24,428.62,272.63,6.05,13.95;24,428.62,256.94,6.05,13.95;24,428.62,240.67,6.05,14.53;24,428.62,230.39,6.05,2.32;24,428.62,215.19,6.05,12.20;24,428.62,210.46,6.05,1.74;24,428.62,194.77,6.05,13.95;24,428.62,179.07,6.05,13.95;24,428.62,163.38,6.05,13.95;24,428.62,147.69,6.05,13.95;24,428.62,132.00,6.05,13.95;24,428.62,115.73,4.84,14.53;25,134.77,581.81,66.92,10.75" xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j" coord="24,395.17,630.21,6.05,39.36">Sánchez-Vega</title>
		<imprint>
			<biblScope unit="volume">309</biblScope>
			<biblScope unit="page">537</biblScope>
			<date type="published" when="2003">1.00, 1.71, 1.71, 1.36, 1.81, 1.22. 000. 009. 1.00, .915. 000. 000. 1.00, 1.00, 1.00, 1.88, 1.20, 1.00. 000. 000. 000. 2.23, 3.10, 1.00, 2.07, 3.58, 14.3</date>
		</imprint>
	</monogr>
	<note>1.02 ,</note>
</biblStruct>

<biblStruct coords="25,156.34,607.30,304.63,8.82;25,156.34,619.44,22.42,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="25,295.81,607.48,58.98,8.64">AI gets a Brain</title>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luis</forename><forename type="middle">Felipe</forename><surname>Cabrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="25,362.47,607.30,23.75,8.59">Queue</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="24" to="29" />
			<date type="published" when="2006-05">May 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,156.34,631.39,305.39,8.64;25,156.34,643.17,307.27,8.82;25,156.34,655.30,116.23,8.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="25,295.82,631.39,165.91,8.64;25,156.34,643.35,32.68,8.64">Developing a Corpus of Plagiarised Short Answers</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Stevenson</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10579-009-9112-1</idno>
	</analytic>
	<monogr>
		<title level="j" coord="25,197.28,643.17,76.79,8.59">Lang. Resour. Eval</title>
		<idno type="ISSN">1574-020X</idno>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="24" />
			<date type="published" when="2011-03">March 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,156.34,119.31,313.56,8.64;26,156.34,131.09,283.73,8.82;26,156.34,143.04,276.92,8.82;26,156.34,155.18,311.15,8.64;26,156.34,167.13,109.87,8.64" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="26,433.37,119.31,36.53,8.64;26,156.34,131.27,91.55,8.64">METER: MEasuring TExt Reuse</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yorick</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Wilks</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073110</idno>
	</analytic>
	<monogr>
		<title level="m" coord="26,266.74,131.09,173.34,8.59;26,156.34,143.04,207.95,8.82">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02</title>
		<title level="s" coord="26,276.27,155.18,168.24,8.64">Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,156.34,179.09,318.00,8.64;26,156.34,190.86,295.76,8.82;26,156.34,202.82,127.97,8.82" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="26,423.40,179.09,50.94,8.64;26,156.34,191.04,240.80,8.64">Efficient and effective spam filtering and re-ranking for large web datasets</title>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="26,404.50,190.86,47.60,8.59;26,156.34,202.82,34.98,8.59">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="441" to="465" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,156.34,214.95,316.86,8.64;26,156.34,226.91,286.88,8.64;26,156.34,238.68,292.95,8.59;26,156.34,250.64,305.68,8.59;26,156.34,262.77,97.41,8.64" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="26,363.08,214.95,110.13,8.64;26,156.34,226.91,271.48,8.64">When close enough is good enough: approximate positional indexes for efficient ranked retrieval</title>
		<author>
			<persName coords=""><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="26,156.34,238.68,292.95,8.59;26,156.34,250.64,102.13,8.59">Proceedings of the 20th ACM Conference on Information and Knowledge Management, CIKM 2011</title>
		<meeting>the 20th ACM Conference on Information and Knowledge Management, CIKM 2011<address><addrLine>Glasgow, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">October 24-28, 2011. 2011</date>
			<biblScope unit="page" from="1993" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,156.34,274.55,322.36,8.82;26,156.34,286.51,324.25,8.59;26,156.34,298.46,178.47,8.82;26,156.34,310.60,222.73,8.64" xml:id="b7">
	<monogr>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<title level="m" coord="26,431.93,274.55,46.78,8.59;26,156.34,286.51,221.01,8.59">CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers</title>
		<editor>
			<persName><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jussi</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christa</forename><surname>Womser-Hacker</surname></persName>
		</editor>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09-20">17-20 September. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,156.34,322.55,304.85,8.64;26,156.34,334.51,317.24,8.64;26,156.34,346.46,290.57,8.64;26,156.34,358.42,104.03,8.64;26,156.34,370.37,222.73,8.64" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="26,335.85,322.55,125.34,8.64;26,156.34,334.51,317.24,8.64;26,156.34,346.46,160.13,8.64">Educated Guesses and Equality Judgements: Using Search Engines and Pairwise Match for External Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Lee</forename><surname>Gillam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neil</forename><surname>Newbold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neil</forename><surname>Cooke</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,156.34,382.33,321.11,8.64;26,156.34,394.28,318.52,8.64;26,156.34,406.24,323.26,8.64;26,156.34,418.01,319.59,8.82;26,156.34,429.97,127.02,8.82" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="26,347.95,382.33,129.50,8.64;26,156.34,394.28,196.57,8.64">First Experiences with TIRA for Reproducible Evaluation in Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="26,213.95,418.01,261.99,8.59;26,156.34,429.97,11.83,8.59">SIGIR 12 Workshop on Open Source Information Retrieval (OSIR 12)</title>
		<editor>
			<persName><forename type="first">Andrew</forename><surname>Trotman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iadh</forename><surname>Clarke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">Shane</forename><surname>Ounis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marc-Allen</forename><surname>Culpepper</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shlomo</forename><surname>Cartright</surname></persName>
		</editor>
		<editor>
			<persName><surname>Geva</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2012-08">August 2012</date>
			<biblScope unit="page" from="52" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,156.34,442.10,316.26,8.64;26,156.34,454.06,299.71,8.64;26,156.34,465.83,286.10,8.82;26,156.34,477.79,308.04,8.59;26,156.34,489.74,270.08,8.82;26,156.34,501.88,279.33,8.64" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="26,347.95,442.10,124.65,8.64;26,156.34,454.06,266.33,8.64">Ousting Ivory Tower Research: Towards a Web Framework for Providing Experiments as a Service</title>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Burrows</surname></persName>
		</author>
		<idno type="DOI">10.1145/2348283.2348501</idno>
		<ptr target="http://dx.doi.org/10.1145/2348283.2348501" />
	</analytic>
	<monogr>
		<title level="m" coord="26,156.34,477.79,308.04,8.59;26,156.34,489.74,81.40,8.59">International ACM Conference on Research and Development in Information Retrieval (SIGIR 12)</title>
		<editor>
			<persName><forename type="first">Bill</forename><surname>Hersh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yoelle</forename><surname>Maarek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012-08">August 2012</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1125" to="1126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,156.34,513.83,279.88,8.64;26,156.34,525.79,316.76,8.64" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="26,410.21,513.83,26.01,8.64;26,156.34,525.79,312.36,8.64">TIRA: Configuring, Executing, and Disseminating Information Retrieval Experiments</title>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dennis</forename><surname>Hoppe</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="26,156.34,537.74,297.93,8.64;26,156.34,549.52,324.26,8.82;26,156.34,561.47,155.79,8.82" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="26,188.37,549.52,255.85,8.59">9th International Workshop on Text-based Information Retrieval</title>
		<editor>A Min Tjoa, Stephen Liddle, Klaus-Dieter Schewe, and Xiaofang Zhou</editor>
		<imprint>
			<date type="published" when="2012-09">September 2012</date>
		</imprint>
	</monogr>
	<note>TIR 12) at DEXA (to appear</note>
</biblStruct>

<biblStruct coords="26,156.34,573.61,307.79,8.64;26,156.34,585.56,284.02,8.64;26,156.34,597.52,104.03,8.64;26,156.34,609.47,222.73,8.64" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="26,308.52,573.61,155.61,8.64;26,156.34,585.56,174.66,8.64">Encoplot -Tuned for High Recall (also proposing a new plagiarism detection score)</title>
		<author>
			<persName coords=""><forename type="first">Cristian</forename><surname>Grozea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marius</forename><surname>Popescu</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<editor>Forner et al.</editor>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,156.34,621.43,320.22,8.64;26,156.34,633.21,308.63,8.82;26,156.34,645.16,287.90,8.82;27,156.34,119.13,223.18,8.82;27,156.34,131.27,197.97,8.64" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="26,294.14,621.43,182.42,8.64;26,156.34,633.39,83.85,8.64">Candidate Document Retrieval for Web-Scale Text Reuse Detection</title>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-24583-1_35</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-642-24583-1_35" />
	</analytic>
	<monogr>
		<title level="m" coord="26,258.69,633.21,206.28,8.59;26,156.34,645.16,150.59,8.59">18th International Symposium on String Processing and Information Retrieval (SPIRE 11)</title>
		<title level="s" coord="26,378.75,645.16,65.50,8.59;27,156.34,119.13,71.15,8.59">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">7024</biblScope>
			<biblScope unit="page" from="356" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,156.34,143.15,318.83,8.64;27,156.34,155.10,305.55,8.64" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="27,304.94,143.15,170.23,8.64;27,156.34,155.10,46.79,8.64">MIREX: MapReduce information retrieval experiments</title>
		<author>
			<persName coords=""><forename type="first">Djoerd</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Claudia</forename><surname>Hauff</surname></persName>
		</author>
		<idno>TR-CTIT-10-15</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>University of Twente</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="27,156.34,166.98,323.44,8.64;27,156.34,178.93,317.99,8.64;27,156.34,190.89,146.91,8.64;27,156.34,202.84,222.73,8.64" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="27,243.21,166.98,236.57,8.64;27,156.34,178.93,230.43,8.64">Similarity Overlap Metric and Greedy String Tiling at PAN 2012: Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Arun</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jayapal</forename></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,156.34,214.72,324.26,8.64;27,156.34,226.68,197.68,8.64;27,156.34,238.63,222.73,8.64" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="27,213.72,214.72,250.95,8.64">An Overview of the Traditional Authorship Attribution Subtask</title>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Juola</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<editor>Forner et al.</editor>
		<imprint>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,156.34,250.51,318.21,8.64;27,156.34,262.28,324.04,8.82;27,156.34,274.42,186.04,8.64;27,156.34,286.37,179.40,8.64" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="27,219.92,250.51,250.67,8.64">Europarl: A Parallel Corpus for Statistical Machine Translation</title>
		<author>
			<persName coords=""><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<ptr target="http://mt-archive.info/MTS-2005-Koehn.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="27,167.13,262.28,253.56,8.59">Conference Proceedings: the tenth Machine Translation Summit</title>
		<meeting><address><addrLine>Phuket, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>AAMT</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,156.34,298.25,304.68,8.64;27,156.34,310.21,324.26,8.64;27,156.34,322.16,309.40,8.64;27,156.34,334.12,129.22,8.64;27,156.34,346.07,222.73,8.64" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="27,178.89,310.21,301.71,8.64;27,156.34,322.16,205.25,8.64">Approaches for Candidate Document Retrieval and Detailed Comparison of Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Leilei</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haoliang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cuixia</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Suhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yong</forename><surname>Han</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,156.34,357.95,290.44,8.64;27,156.34,369.90,290.57,8.64;27,156.34,381.86,104.03,8.64;27,156.34,393.81,222.73,8.64" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="27,299.57,357.95,147.21,8.64;27,156.34,369.90,160.13,8.64">A Set-Based Approach to Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Robin</forename><surname>Küppers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Conrad</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,156.34,405.69,304.86,8.64;27,156.34,417.65,296.39,8.64;27,156.34,429.60,322.13,8.64;27,156.34,441.56,284.99,8.64;27,156.34,453.51,262.37,8.64" xml:id="b21">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Gabriel</forename><surname>Oberreuter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Carrillo-Cisneros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Isaac</forename><forename type="middle">D</forename><surname>Scherson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Juan</forename><forename type="middle">D</forename><surname>Velásquez</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<title level="m" coord="27,202.81,417.65,249.92,8.64;27,156.34,429.60,37.10,8.64">Submission to the 4th International Competition on Plagiarism Detection</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint/>
		<respStmt>
			<orgName>University of Chile, Chile, and the University of California</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="27,156.34,465.39,288.80,8.64;27,156.34,477.34,299.75,8.64;27,156.34,489.30,307.19,8.64;27,156.34,501.25,168.77,8.64;27,156.34,513.21,222.73,8.64" xml:id="b22">
	<monogr>
		<title level="m" type="main" coord="27,297.64,465.39,147.50,8.64;27,156.34,477.34,299.75,8.64;27,156.34,489.30,241.49,8.64">Applying Specific Clusterization and Fingerprint Density Distribution with Genetic Algorithm Overall Tuning in External Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Yurii</forename><surname>Palkovskii</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexei</forename><surname>Belov</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,156.34,524.91,298.53,8.82;27,156.34,537.04,185.72,8.64" xml:id="b23">
	<monogr>
		<title level="m" type="main" coord="27,224.14,524.91,171.68,8.59">Technologies for Reusing Text from the Web</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-12">December 2011</date>
		</imprint>
		<respStmt>
			<orgName>Bauhaus-Universität Weimar</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Dissertation</note>
</biblStruct>

<biblStruct coords="27,156.34,548.92,322.79,8.64;27,156.34,560.87,315.90,8.64;27,156.34,572.83,316.00,8.64;27,156.34,584.60,324.25,8.82;27,156.34,596.56,315.69,8.82;27,156.34,608.69,130.34,8.64" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="27,186.78,560.87,281.34,8.64">Overview of the 1st International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Eiselt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-502" />
	</analytic>
	<monogr>
		<title level="m" coord="27,219.42,584.60,261.18,8.59;27,156.34,596.56,131.22,8.59">SEPLN 09 Workshop on Uncovering Plagiarism, Authorship, and Social Software Misuse (PAN 09)</title>
		<editor>
			<persName><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Moshe</forename><surname>Koppel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009-09">September 2009</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,156.34,620.57,322.79,8.64;27,156.34,632.53,319.21,8.64;27,156.34,644.30,308.66,8.82;27,156.34,656.26,260.81,8.82;28,156.34,119.31,104.03,8.64;28,156.34,131.27,222.73,8.64" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="27,186.78,632.53,284.65,8.64">Overview of the 2nd International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Eiselt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="27,426.82,644.30,38.19,8.59;27,156.34,656.26,159.93,8.59">Notebook Papers of CLEF 10 Labs and Workshops</title>
		<editor>
			<persName><forename type="first">Martin</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emanuele</forename><surname>Pianta</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010-09">September 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,156.34,143.22,304.53,8.64;28,156.34,155.18,309.30,8.64;28,156.34,166.95,312.57,8.82;28,156.34,178.91,295.48,8.82;28,156.34,191.04,172.14,8.64" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="28,448.70,143.22,12.17,8.64;28,156.34,155.18,189.81,8.64">An Evaluation Framework for Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="28,247.32,166.95,221.60,8.59;28,156.34,178.91,54.17,8.59">International Conference on Computational Linguistics (COLING 10)</title>
		<editor>
			<persName><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</editor>
		<meeting><address><addrLine>Stroudsburg, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-08">August 2010</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="997" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,156.34,203.00,322.79,8.64;28,156.34,214.95,317.55,8.64;28,156.34,226.73,324.25,8.82;28,156.34,238.68,303.76,8.82;28,156.34,250.82,245.15,8.64" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="28,186.78,214.95,282.99,8.64">Overview of the 3rd International Competition on Plagiarism Detection</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Eiselt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
	</analytic>
	<monogr>
		<title level="m" coord="28,402.52,226.73,78.07,8.59;28,156.34,238.68,119.87,8.59">Notebook Papers of CLEF 11 Labs and Workshops</title>
		<editor>
			<persName><forename type="first">Vivien</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2011-09">September 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,156.34,262.77,297.76,8.64;28,156.34,274.73,320.66,8.64;28,156.34,286.69,314.37,8.64;28,156.34,298.46,291.58,8.82;28,156.34,310.42,302.85,8.82;28,156.34,322.55,182.07,8.64;28,156.34,334.51,177.23,8.64" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="28,353.04,274.73,123.96,8.64;28,156.34,286.69,90.76,8.64">ChatNoir: A Search Engine for the ClueWeb09 Corpus</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Graßegger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maximilian</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Tippmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clement</forename><surname>Welsch</surname></persName>
		</author>
		<idno type="DOI">10.1145/2348283.2348429</idno>
		<ptr target="http://dx.doi.org/10.1145/2348283.2348429" />
	</analytic>
	<monogr>
		<title level="m" coord="28,255.13,298.46,192.79,8.59;28,156.34,310.42,196.64,8.59">International ACM Conference on Research and Development in Information Retrieval (SIGIR 12)</title>
		<editor>
			<persName><forename type="first">Bill</forename><surname>Hersh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yoelle</forename><surname>Maarek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012-08">August 2012</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1004" to="1004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,156.34,346.46,306.63,8.64;28,156.34,358.24,311.25,8.82;28,156.34,370.19,284.64,8.59;28,156.34,382.15,262.70,8.82" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="28,406.78,346.46,56.19,8.64;28,156.34,358.42,146.13,8.64">Simple BM25 extension to multiple weighted fields</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="28,320.29,358.24,147.30,8.59;28,156.34,370.19,279.71,8.59">Proceedings of the 2004 ACM CIKM International Conference on Information and Knowledge Management</title>
		<meeting>the 2004 ACM CIKM International Conference on Information and Knowledge Management<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">November 8-13, 2004. 2004</date>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,156.34,394.28,285.99,8.64;28,156.34,406.24,323.21,8.64;28,156.34,418.19,261.32,8.64;28,156.34,430.15,222.73,8.64" xml:id="b30">
	<monogr>
		<title level="m" type="main" coord="28,408.58,394.28,33.75,8.64;28,156.34,406.24,323.21,8.64;28,156.34,418.19,24.36,8.64">Detailed Comparison Module In CoReMo 1.9 Plagiarism Detector-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Diego</forename><forename type="middle">A</forename><surname>Rodríguez Torrejón</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Manuel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martín</forename><surname>Ramos</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,156.34,442.10,318.59,8.64;28,156.34,454.06,319.23,8.64;28,156.34,466.01,271.00,8.64;28,156.34,477.97,222.73,8.64" xml:id="b31">
	<monogr>
		<title level="m" type="main" coord="28,156.34,454.06,319.23,8.64;28,156.34,466.01,34.04,8.64">Optimized Fuzzy Text Alignment for Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Sánchez-Vega</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manuel</forename><surname>Montes Y Gómez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luis</forename><surname>Villaseñor-Pineda</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,156.34,489.74,318.26,8.82;28,156.34,501.88,266.61,8.64" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="28,248.49,489.74,186.44,8.82">Plagiarism Detection Using Stopword n-Grams</title>
		<author>
			<persName coords=""><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.21630</idno>
		<ptr target="http://dx.doi.org/10.1002/asi.21630" />
	</analytic>
	<monogr>
		<title level="j" coord="28,443.19,489.74,26.93,8.59">JASIST</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2512" to="2527" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,156.34,513.83,282.63,8.64;28,156.34,525.79,303.24,8.64;28,156.34,537.56,309.92,8.82;28,156.34,549.52,317.57,8.59;28,156.34,561.65,307.62,8.64;28,156.34,573.61,177.23,8.64" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="28,385.72,513.83,53.25,8.64;28,156.34,525.79,134.70,8.64">Strategies for Retrieving Plagiarized Documents</title>
		<author>
			<persName coords=""><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sven</forename><surname>Meyer Zu Eißen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<idno type="DOI">10.1145/1277741.1277928</idno>
		<ptr target="http://dx.doi.org/10.1145/1277741.1277928" />
	</analytic>
	<monogr>
		<title level="m" coord="28,370.25,537.56,96.01,8.59;28,156.34,549.52,313.63,8.59">30th International ACM Conference on Research and Development in Information Retrieval (SIGIR 07)</title>
		<editor>
			<persName><forename type="first">Charles</forename><surname>Clarke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Norbert</forename><surname>Fuhr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Noriko</forename><surname>Kando</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wessel</forename><surname>Kraaij</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Arjen</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007-07">July 2007</date>
			<biblScope unit="page" from="825" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,156.34,585.56,324.26,8.64;28,156.34,597.52,261.22,8.64;28,156.34,609.47,290.57,8.64;28,156.34,621.43,104.03,8.64;28,156.34,633.39,222.73,8.64" xml:id="b34">
	<monogr>
		<title level="m" type="main" coord="28,376.72,585.56,103.88,8.64;28,156.34,597.52,261.22,8.64;28,156.34,609.47,160.13,8.64">Three Way Search Engine Queries with Multi-feature Document Comparison for Plagiarism Detection-Notebook for PAN at CLEF</title>
		<author>
			<persName coords=""><forename type="first">Šimon</forename><surname>Suchomel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Kasprzak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michal</forename><surname>Brandejs</surname></persName>
		</author>
		<ptr target="http://www.clef-initiative.eu/publication/working-notes" />
		<editor>Forner et al.</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
