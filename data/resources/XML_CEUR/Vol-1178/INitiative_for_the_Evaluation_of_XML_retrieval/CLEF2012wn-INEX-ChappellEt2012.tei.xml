<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,136.42,115.96,342.52,12.62;1,287.90,134.79,39.56,12.62">Overview of the INEX 2012 Relevance Feedback Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,223.20,173.16,79.98,8.74"><forename type="first">Timothy</forename><surname>Chappell</surname></persName>
							<email>timothy.chappell@qut.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Queensland University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,330.33,173.16,57.36,8.74"><forename type="first">Shlomo</forename><surname>Geva</surname></persName>
							<email>s.geva@qut.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">Queensland University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,136.42,115.96,342.52,12.62;1,287.90,134.79,39.56,12.62">Overview of the INEX 2012 Relevance Feedback Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">659773824FC0C098D6F0AEEF9B38F8D5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The INEX 2012 Relevance Feedback track provided participating organisations with an evaluation system designed to simulate the user of a search engine. Participants provided their own search systems designed to interface with the evaluation platform and receive live feedback from the simulated user showing which parts, if any, of the current document were considered by the user to be relevant. This version of the track was run in a very different manner compared to the INEX 2011 and 2010 versions of the Relevance Feedback track in an attempt to increase participation and strengthen the quality of the evaluations. While the former goal was not met, the new format of the track allowed the entire Wikipedia collection[5] to be used, as opposed to the small subsets used in 2010 and 2011. We present the evaluation methodology, its implementation, and experimental results obtained for thirteen submissions from two participating organisations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper presents an overview of the INEX 2012 Relevance Feedback track. The track was designed to facilitate the development of search engine modules that incorporate focused relevance feedback. The INEX Wikipedia Collection <ref type="bibr" coords="1,464.30,517.81,12.22,8.74" target="#b4">[5]</ref>, a 50.7GB collection of 2,666,190 Wikipedia articles in XML format was used as the data collection for the track. The search topics and assessments used were collected for the INEX 2009 and 2010 Ad Hoc tracks <ref type="bibr" coords="1,362.47,555.47,11.51,8.74" target="#b7">[8]</ref> <ref type="bibr" coords="1,373.99,555.47,11.51,8.74" target="#b0">[1]</ref>.</p><p>Organisations participated by supplying executables that would communicate with a supplied evaluation platform through standard operating system I/O pipes. The evaluation platform would provide the search topics and, for each document provided to it by the search module, reply with relevant passages. The search module can then make use of this information to rerank the remaining documents as necessary.</p><p>After each topic has been searched, the evaluation platform uploads the document IDs returned by the search module for each topic in the form of a trec eval <ref type="bibr" coords="2,152.60,118.99,13.16,8.74" target="#b1">[2]</ref>-compatible submission. The submission is evaluated on a remote server against relevance assessments for the topics and the results are sent back to the evaluation platform.</p><p>The evaluation platform had two modes, training and evaluation, with a different set of topics for each. Training mode would run the module over a smaller set of 10 topics and, while the submission would still be uploaded to the remote server, the results would be returned to the user but not recorded as a submission. In contrast, evaluation mode uses a larger set of 50 topics and records all runs submitted. Hence, users can provide submissions to the track simply by executing evaluation runs with the platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Focused Feedback</head><p>This track covers the use of focused feedback, a relevance feedback model wherein users specify segments of the document (usually through some form of selection or highlighting tool) considered relevant to the search topic. This allows users to give more flexible feedback when only portions of the current document are relevant to their search.</p><p>More information about focused feedback is available in <ref type="bibr" coords="2,396.73,349.90,9.96,8.74" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>Submissions to the Relevance Feedback track are evaluated from the perspective of a user searching for information on a number of topics. The user reads each document returned by the search system and highlights sections that are relevant to the current topic. If the document returned is not relevant at all, the user simply skips this document and asks for a new one. Hence, the search system has an opportunity to rerank the unseen documents at every step along the way, taking into account new information about what the user is searching for. However, as the user's search experience is the ultimate indicator of search performance, documents the user has already seen are considered frozen and can not be reranked after they have been presented.</p><p>To simulate the user, relevance judgments from the Ad Hoc tracks from INEX 2009 <ref type="bibr" coords="2,158.31,543.14,10.52,8.74" target="#b7">[8]</ref> and 2010 <ref type="bibr" coords="2,215.64,543.14,10.52,8.74" target="#b0">[1]</ref> are used to provide information about which segments of documents are relevant to which topics. These assessments consist of offsetlength pairs, each indicating that the specified segment in the given document is relevant to the given topic. The evaluation platform uses these assessments, returning the segments that match the given document. The relevance feedback modules can then rerank the remaining documents in the collection with information from this and from previous feedback to produce more relevant results for the remaining documents to be presented to the user.</p><p>At the end of a run, the evaluation platform compiles the documents, in the order they were presented, into a trec eval -compatible submission file, which is uploaded to a remote server where the evaluation is performed. The results are then returned to the user. This serves to keep the relevance judgments secret; though only to an extent as the relevance judgments for the Ad Hoc track are publicly available and it is trivial to convert them to TREC format.</p><p>In the 2010 and 2011 versions of the Relevance Feedback track, the evaluation platform would provide the relevance feedback plugin with the offset and length of each segment of relevant text. This was changed in 2012 to reduce the need for the entire, uncompressed collection to be available to the relevance feedback plugin. Instead, the direct text from the documents, stripped of XML tags, was passed to the relevance feedback plugin. This made it more practical to create Relevance Feedback submissions without a copy of the uncompressed Wikipedia collection or a copy of the archive that makes random access within the collection feasible. As the default form the Wikipedia collection is distributed in (.tar.bz2) is not suitable for random access, it is difficult and time-consuming to extract individual documents as they are required. This step will also make it more feasible to create Relevance Feedback tasks based on other large collections, such as ClueWeb09 <ref type="bibr" coords="3,212.77,319.84,14.43,8.74" target="#b2">[3]</ref>.</p><p>The topics used for this collection were the topics for the INEX 2009 <ref type="bibr" coords="3,450.94,332.39,10.52,8.74" target="#b7">[8]</ref> and 2010 <ref type="bibr" coords="3,159.04,344.95,10.52,8.74" target="#b0">[1]</ref> Ad Hoc tracks. After stripping out the topics that had no relevance judgments attached, the first ten were used as the training set. Out of the remaining topics, every second topic was used to make up the evaluation set until all fifty slots were filled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>Track participants were tasked with creating relevance feedback modules that would interface with the provided evaluation platform and respond with results in answer to queries. With each result, the evaluation platform would respond with relevant passages from each document and the relevance feedback module would have the opportunity to rerank the remaining results in that topic to deliver better results.</p><p>In past iterations of the track, these relevance feedback modules were implemented as dynamic plugins written in Java. These plugins were provided by the track participants as submissions. This approach, while effective at preventing approaches like tuning to specific topics, came with a number of drawbacks. It restricted the implementation environment to Java. In addition, because it would not be feasible for the users to submit their own index of the collection (which can be hundreds of megabytes large) or index the Wikipedia collection in its entirety at the time of evaluation, only subsets of the Wikipedia collection were, making it more difficult to gather realistic performance information.</p><p>In the 2011 iteration of the Relevance Feedback track, the same system was used; however, participants were also provided with a Java plugin capable of interfacing with generic platform-dependent executables over pipes. This made it possible to implement relevance feedback modules in more languages but brought with it issues of compatibility as the resulting module had to be evaluated on a specific operating system and hardware architecture.</p><p>The 2012 iteration made an attempt to rectify these issues, keeping the pipe communication aspect from the binary interface plugin but otherwise heavily changing the way the track was run. In the 2012 Relevance Feedback track, participants create a relevance feedback module in whatever language they choose and the only restriction is that the module run on their own hardware. The evaluation platform was rewritten for the new task and would communicate directly with the relevance feedback module over pipes and make submissions to a remote server, set up specifically for the track. Making a submission with the new system was as simple as running the evaluation platform (in the correct mode.)</p><p>Separate training and evaluation modes were included to allow participants to test their relevance feedback modules with the code without needing to make a submission. Every evaluation submission was recorded by the server to ensure that, while participants could still tune their code to the evaluation topics, all the results of doing so would be recorded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Submission format</head><p>When the track was first opened, the evaluation platform was made available from the INEX website. On supplying a valid path to the relevance feedback module, the evaluation platform would open up an I/O pipe to the module and begin working through the selected topic set.</p><p>Choosing the Evaluation Run mode would run the module through the 50 topic Evaluation Mode set.</p><p>Participating organisations created relevance feedback module executables that adhered to the following specifications, as described in the documentation provided with the evaluation platform:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Relevance Feedback module interface protocol</head><p>The evaluation platform and the module communicate using a pipe, a standard feature of all modern operating systems. Hence, any programming language capable of creating an executable that can read from standard input and write to standard output would be suitable for creating a relevance feedback module for the task.</p><p>Each message from the evaluation platform or the relevance feedback module will be in the form of a single line of text ending in a linefeed character. The meaning of the line of text will be derived from the context in which it is submitted.</p><p>The evaluation platform communicates first, providing a topic line. This line will either contain the text of the topic or the text EOF, signalling to the module that the evaluation is over and it may exit. The module will respond with a document line. This line will contain either a document ID or the text EOF, signalling to the evaluation platform that the module has finished presenting documents for the current topic and is ready to move on to the next topic. If a document ID is presented, the evaluation platform will respond with feedback.</p><p>Feedback will be provided in the form of a line with a number indicating the number of passages of relevant text found in the document. If that number was 0, the document was not relevant and the module should provide the next document ID. Otherwise, the evaluation platform will immediately follow up the number with that many passages of feedback text, each on a single line. After all the lines of feedback have been sent, the module is expected to respond with another document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Relevance Feedback module interface format</head><p>The topic line supplied by the evaluation platform will be in ASCII text, stripped of characters outside the 32-127 range. The line will be no more than 127 characters long, including the linefeed.</p><p>The document ID line returned by the module should contain a number in ASCII text, corresponding to the document ID within the Wikipedia collection of the document to return.</p><p>The 'lines of feedback' line returned by the evaluation platform in response to a document ID line will be a number in ASCII text containing the number of segments of relevant text in the document. The feedback will then be followed by lines of text, one for each segment of feedback. The line will be no more than 1048575 characters long, including the linefeed. This, too, will be in ASCII text, stripped of characters outside the 32-127 range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Submissions</head><p>Two groups made a total of 15 submissions to the INEX 2012 Relevance Feedback track, up from four submissions from two groups in 2011. This may be partly due to the new format making it easier to make many submissions as the need for each submission to be packaged into its own Java archive and uploaded was no longer present.</p><p>Queensland University of Technology made five submissions using an experimental relevance feedback mode in TopSig <ref type="bibr" coords="6,323.04,341.45,13.04,8.74" target="#b3">[4]</ref>. This was originally planned to be the relevance run for the INEX 2012 Relevance Feedback track but due to time constraints this was not possible. The TopSig runs, apart from the baseline run which did not make use of feedback at all, simply used the feedback text as a new query and reranked the remaining documents found by the initial query each time. More information about the signature approach used by TopSig can be found in <ref type="bibr" coords="6,187.90,416.77,9.96,8.74" target="#b6">[7]</ref>.</p><p>The baseline TOPSIG run consisted of an untuned 1024-bit signature search without using collection statistics or relevance feedback returning 100 documents per topic. Subsequent TOPSIG runs incorporated the simple feedback system described earlier. TOPSIG-RF1 reranked the remaining documents not yet presented to the user by using the last line of feedback presented as a new search query. TOPSIG-RF2 kept the same approach but increased the number of documents returned to 1000. TOPSIG-RF3 increased the signature size to 2048 bits and TOPSIG-RF4 changed the feedback approach to use all of the feedback presented instead of the last line. As this is the first experiment performed with using active relevance feedback for signature searching in TopSig, preliminary results are only experimental.</p><p>The Universidad Autónoma Metropolitana made 10 submissions using Indri <ref type="bibr" coords="6,146.13,580.80,11.36,8.74" target="#b8">[9]</ref> as a base and employing a Markov random field to rerank results with relevance feedback. The BASE-IND run consists of a run with Indri without incorporating relevance feedback while the MF and LF runs consist of the results when adding the 20 most frequent and least frequent terms respectively from the feedback to the query. The RRMRF runs are also based on Indri but employ the Markov random field for reranking. The 100D, 300D and 1000D runs are the results from returning 100, 300 and 1000 documents respectively per topic. The L values represent the lambda parameter within the reranking approach. More details are available in the Universidad Autónoma Metropolitana's track paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation</head><p>Two sets of topics were made available, not directly to participants but through the evaluation platform. The training set used the first 10 topics from the INEX 2009 Ad Hoc track while the evaluation set used 50 topics chosen from every 2nd topic from the INEX 2009 and 2010 Ad Hoc tracks, excluding the topics used for the training set. Topics without associated relevance judgments were removed from the set beforehand.</p><p>All of the submissions were run through trec eval <ref type="bibr" coords="7,369.44,257.87,10.52,8.74" target="#b1">[2]</ref> using default settings. The results of each run were also presented to the submitter immediately after submission.</p><p>Trec eval reports results using a variety of different metrics, including interpolated recall-precision, average precision, exact precision and R-precision. Recall-precision reports the precision (the fraction of relevant documents returned out of the documents returned so far) at varying points of recall (after a given portion of the relevant documents have been returned.) R-precision is calculated as the precision (number of relevant documents) after R documents have been seen, where R is the number of relevant documents in the collection. Average precision is calculated from the sum of the precision at each recall point (a point where a certain fraction of the documents in the collection have been seen) divided by the number of recall points.</p><p>Unlike in the previous incarnations of the relevance feedback track, the evaluation platform did not come with the option of producing no-feedback runs. However, both participating organisations created runs that did not utilise feedback, showing where feedback has improved the results of these runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparisons</head><p>The following tables show the results of each submission in terms of average precision and R-precision.</p><p>The charts compare groups of submissions by exact precision. The y axis shows the proportion of relevant documents retrieved and the x axis shows the total number of documents retrieved. Figure <ref type="figure" coords="7,331.53,559.94,4.98,8.74">2</ref> shows a comparison of the exact precision of each of the UAM runs submitted. Figure <ref type="figure" coords="7,381.91,572.49,4.98,8.74" target="#fig_1">3</ref> shows a comparison of each QUT run, while figure <ref type="figure" coords="7,273.87,585.05,4.98,8.74" target="#fig_2">4</ref> gives a comparison of the best feedback and non-feedback runs from each group.  <ref type="table" coords="8,208.73,339.61,4.13,7.89">1</ref>. Average precision and R-precision for submitted runs Submission @5 @10 @15 @20 @30 @100 @200 @500 @1000 </p><formula xml:id="formula_0" coords="9,409.93,202.89,68.43,75.54">-INDQE-20tMF BASE-INDQE-20tLF BASE-INDQE-20tMFandLF RRMRF-100D-L03 RRMRF-100D-L05 RRMRF-300D-L03 RRMRF-300D-L05 RRMRF-1000D-L03 RRMRF-1000D-L05</formula><p>Fig. <ref type="figure" coords="9,157.73,335.61,4.13,7.89">2</ref>. Exact precision of submissions by the Universidad Autónoma Metropolitana @5 @10 @15 @20 @30 @100 @200 @500 @1000 0 @10 @15 @20 @30 @100 @200 @500 @1000 0 0.1 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented the Relevance Feedback track at INEX 2012.</p><p>It is difficult to compare results between different incarnations of the track. While the results are far worse in 2012 from an objective perspective, the large changes in the way the tracks were run between the two years can account for this. In the 2010 and 2011 versions of the Relevance Feedback track, only subsets of the Wikipedia collection were used and these subsets heavily favoured relevant documents. As the burden of finding the results has shifted more to the search systems in the 2012 version of the track the overall results have also declined. The search systems presented at the INEX 2012 Relevance Feedback track are not necessarily worse than those presented in 2011.</p><p>While the number of submissions has increased since INEX 2011, the number of participants has not. Lowering the barriers to entry have not resulted in the increased interest in the Relevance Feedback track that was expected. Part of this may be due to the lack of a strong reference submission. In the INEX 2010 and 2011 iterations of the Relevance Feedback track, a relevance feedback module with complete was provided to participants in advance, to be used as a base for other submissions if desired. No equivalent was provided for the INEX 2012 Relevance Feedback track which may have discouraged participation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,158.86,655.03,297.64,7.89;4,203.93,442.67,207.50,187.08"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Evaluation Platform for the INEX 2012 Relevance Feedback track</figDesc><graphic coords="4,203.93,442.67,207.50,187.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,140.00,613.13,335.36,7.89"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Exact precision of submissions by the Queensland University of Technology</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="10,134.77,293.71,345.82,7.89;10,134.77,305.25,104.02,7.86"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Exact precision comparison: best non-RF and best RF submissions from each participating organisation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,136.16,435.53,356.47,190.00"><head>Table 2 .</head><label>2</label><figDesc>Exact precision of submitted runs</figDesc><table coords="8,136.16,435.53,356.47,168.96"><row><cell>BASE-IND</cell><cell cols="2">0.456 0.41 0.36 0.327 0.3007 0.1844 0.1166 0.0557 0.0292</cell></row><row><cell>BASE-INDQE-20tMF</cell><cell cols="2">0.396 0.33 0.2907 0.264 0.228 0.1232 0.0789 0.0406 0.0239</cell></row><row><cell>BASE-INDQE-20tLF</cell><cell cols="2">0.308 0.198 0.148 0.125 0.0967 0.0412 0.0241 0.0114 0.0063</cell></row><row><cell cols="3">BASE-INDQE-20tMFandLF 0.392 0.304 0.2653 0.237 0.204 0.1136 0.0741 0.0396 0.0225</cell></row><row><cell>RRMRF-100D-L03</cell><cell cols="2">0.448 0.406 0.3733 0.348 0.3107 0.1846 0.0923 0.0369 0.0185</cell></row><row><cell>RRMRF-100D-L05</cell><cell cols="2">0.452 0.404 0.372 0.351 0.312 0.1846 0.0923 0.0369 0.0185</cell></row><row><cell>RRMRF-300D-L03</cell><cell cols="2">0.452 0.398 0.3587 0.337 0.3027 0.1876 0.117 0.0512 0.0256</cell></row><row><cell>RRMRF-300D-L05</cell><cell>0.46 0.42 0.368 0.349 0.3</cell><cell>0.1708 0.1157 0.0512 0.0256</cell></row><row><cell>RRMRF-1000D-L03</cell><cell cols="2">0.456 0.41 0.36 0.328 0.3007 0.1848 0.1166 0.0557 0.0292</cell></row><row><cell>RRMRF-1000D-L05</cell><cell cols="2">0.456 0.41 0.36 0.328 0.3007 0.1848 0.1166 0.0557 0.0292</cell></row><row><cell>TOPSIG</cell><cell cols="2">0.448 0.42 0.3827 0.366 0.332 0.232 0.116 0.0464 0.0232</cell></row><row><cell>TOPSIG-RF1</cell><cell cols="2">0.496 0.44 0.4067 0.384 0.3593 0.232 0.116 0.0464 0.0232</cell></row><row><cell>TOPSIG-RF2</cell><cell cols="2">0.524 0.46 0.4173 0.398 0.3747 0.242 0.1733 0.0933 0.0569</cell></row><row><cell>TOPSIG-RF3</cell><cell cols="2">0.56 0.504 0.4813 0.465 0.4187 0.2614 0.1906 0.1049 0.0623</cell></row><row><cell>TOPSIG-RF4</cell><cell cols="2">0.568 0.52 0.4773 0.459 0.42 0.2656 0.1923 0.1054 0.0623</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p>We would like to thank all the participating organisations for their contributions and hard work.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="11,138.35,143.14,342.24,7.86;11,146.91,154.64,333.68,7.86;11,146.91,166.15,45.06,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,432.81,143.14,47.78,7.86;11,146.91,154.64,117.26,7.86">Overview of the INEX 2010 Ad Hoc track</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Arvola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schenkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Trotman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vainio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,272.19,154.64,179.40,7.86">Comparative Evaluation of Focused Retrieval</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.35,177.66,211.07,7.86" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<title level="m" coord="11,196.96,177.66,124.18,7.86">trec eval IR evaluation package</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.35,189.16,342.24,7.86;11,146.91,200.67,60.86,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="11,315.00,189.16,78.00,7.86">Clueweb09 data set</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-01">Jan, 2009</date>
		</imprint>
	</monogr>
	<note>boston. lti. cs. cmu. edu</note>
</biblStruct>

<biblStruct coords="11,138.35,212.18,342.24,7.86;11,146.91,223.68,118.72,7.86" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chappell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Geva</surname></persName>
		</author>
		<ptr target="http://www.topsig.org/" />
		<title level="m" coord="11,250.46,212.18,226.11,7.86">TopSig: Topological signature indexing and search engine</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.35,235.19,342.24,7.86;11,146.91,246.70,20.99,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,304.68,235.19,111.44,7.86">The Wikipedia XML Corpus</title>
		<author>
			<persName coords=""><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,423.14,235.19,52.63,7.86">SIGIR Forum</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.35,258.20,342.24,7.86;11,146.91,269.71,102.50,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,260.04,258.20,155.00,7.86">Focused relevance feedback evaluation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chappell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,425.69,258.20,54.90,7.86;11,146.91,269.71,42.79,7.86">Simulation of Interaction</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.35,281.22,342.24,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="11,257.80,281.22,195.11,7.86">Topsig: Topology preserving document signatures</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Vries</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.35,292.73,342.25,7.86;11,146.91,304.23,333.68,7.86;11,146.91,315.74,20.99,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,442.93,292.73,37.66,7.86;11,146.91,304.23,132.97,7.86">Overview of the INEX 2009 Ad Hoc track</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lethonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schenkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Thom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Trotman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,290.47,304.23,134.99,7.86">Focused Retrieval and Evaluation</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="4" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.35,327.25,342.24,7.86;11,146.91,338.75,333.68,7.86;11,146.91,350.26,117.46,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,359.07,327.25,121.52,7.86;11,146.91,338.75,135.34,7.86">Indri: A language model-based search engine for complex queries</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,303.45,338.75,177.15,7.86;11,146.91,350.26,89.37,7.86">Proceedings of the International Conference on Intelligent Analysis</title>
		<meeting>the International Conference on Intelligent Analysis</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
