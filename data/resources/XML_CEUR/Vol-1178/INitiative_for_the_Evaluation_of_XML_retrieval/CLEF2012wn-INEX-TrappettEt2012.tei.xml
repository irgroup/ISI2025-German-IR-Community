<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,148.28,113.97,318.80,15.78;1,288.25,131.90,38.85,15.78">Overview of the INEX 2012 Snippet Retrieval Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,145.03,163.42,81.09,19.92"><forename type="first">Matthew</forename><surname>Trappett</surname></persName>
							<email>matthew.trappett@qut.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,236.67,163.42,57.34,19.92"><forename type="first">Shlomo</forename><surname>Geva</surname></persName>
							<email>s.geva@qut.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Queensland University of Technology</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,304.57,163.42,75.31,19.92"><forename type="first">Andrew</forename><surname>Trotman</surname></persName>
							<email>andrew@cs.otago.ac.nz</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Otago</orgName>
								<address>
									<settlement>Dunedin</settlement>
									<region>New Zealand</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,390.44,163.42,53.29,19.92"><forename type="first">Falk</forename><surname>Scholer</surname></persName>
							<email>falk.scholer@rmit.edu.au</email>
							<affiliation key="aff2">
								<orgName type="institution">RMIT University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.97,175.38,70.94,19.92"><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
							<email>mark.sanderson@rmit.edu.au</email>
							<affiliation key="aff2">
								<orgName type="institution">RMIT University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,148.28,113.97,318.80,15.78;1,288.25,131.90,38.85,15.78">Overview of the INEX 2012 Snippet Retrieval Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">317ECDE18085EE9164F90BA5794C3DED</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper gives an overview of the INEX 2012 Snippet Retrieval Track. The goal of the Snippet Retrieval Track is to provide a common forum for the evaluation of the eectiveness of snippets, and to investigate how best to generate snippets for search results, which should provide the user with sucient information to determine whether the underlying document is relevant. We discuss the setup of the track, details of the assessment and evaluation, and initial participation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Queries performed on search engines typically return far more results than a user could ever hope to look at. While one way of dealing with this problem is to attempt to place the most relevant results rst, no system is perfect, and irrelevant results are often still returned. To help with this problem, a short text snippet is commonly provided to help the user decide whether or not the result is relevant.</p><p>The goal of snippet generation is to provide sucient information to allow the user to determine the relevance of each document, without needing to view the document itself, allowing the user to quickly nd what they are looking for.</p><p>The goal of the INEX Snippet Retrieval track is to provide a common forum for the evaluation of the eectiveness of snippets, and to investigate how best to generate informative snippets for search results.</p><p>This year is the second year in which the INEX Snippet Retrieval track has run. In response to feedback from the rst year, search topics have been made more specic, and document-based assessment has been introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Snippet Retrieval Track</head><p>In this section, we briey summarise the snippet retrieval task, the submission format, the assessment method, and the measures used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task</head><p>The task is to return a ranked list of documents for the requested topic to the user, and with each document, a corresponding text snippet describing the document. This text snippet should attempt to convey the relevance of the underlying document, without the user needing view the document itself.</p><p>Each run must return 20 documents per topic, with a maximum of 180 characters per snippet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Test Collection</head><p>The Snippet Retrieval Track uses the INEX Wikipedia collection introduced in 2009 an XML version of the English Wikipedia, based on a dump taken on 8 October 2008, and semantically annotated as described by Schenkel et al. <ref type="bibr" coords="2,467.31,264.00,9.96,19.92" target="#b0">[1]</ref>. This corpus contains 2,666,190 documents.</p><p>This year there are 35 topics in total. The majority of these topics (25 of 35) have been created specically for this track, with the goal being to create topics requesting more specic information than is likely to be found in the rst few paragraphs of a document. The remaining 10 topics have been reused from the INEX 2010 Ad Hoc Track <ref type="bibr" coords="2,251.13,335.73,9.96,19.92" target="#b1">[2]</ref>.</p><p>Each topic contains a short content only (CO) query, a phrase title, a one line description of the search request, and a narrative with a detailed explanation of the information need, the context and motivation of the information need, and a description of what makes a document relevant or not.</p><p>For those participants who wished to generate snippets only, and not use their own search engine, a reference run was generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Submission Format</head><p>An XML format was chosen for the submission format, due to its human readability, its nesting ability (as information was needed at three hierarchical levels submission-level, topic-level, and snippet-level), and because the number of existing tools for handling XML made for quick and easy development of assessment and evaluation.</p><p>The submission format is dened by the DTD given in Figure <ref type="figure" coords="2,420.73,516.86,3.87,19.92">1</ref>. The following is a brief description of the DTD elds. Each submission must contain the following:</p><p>participant-id: The participant number of the submitting institution. run-id: A run ID, which must be unique across all submissions sent from a single participating organisation. description: a brief description of the approach used. Every run should contain the results for each topic, conforming to the following: topic: contains a ranked list of snippets, ordered by decreasing level of relevance of the underlying document. <ref type="figure" coords="3,211.17,301.95,4.13,5.89">1</ref>. DTD for Snippet Retrieval Track run submissions topic-id: The ID number of the topic. snippet: A snippet representing a document. doc-id: The ID number of the underlying document. rsv: The retrieval status value (RSV) or score that generated the ranking.</p><formula xml:id="formula_0" coords="3,134.77,115.00,258.84,192.84">&lt;!ELEMENT inex-snippet-submission (description,topic+)&gt; &lt;!ATTLIST inex-snippet-submission participant-id CDATA #REQUIRED run-id CDATA #REQUIRED &gt; &lt;!ELEMENT description (#PCDATA)&gt; &lt;!ELEMENT topic (snippet+)&gt; &lt;!ATTLIST topic topic-id CDATA #REQUIRED &gt; &lt;!ELEMENT snippet (#PCDATA)&gt; &lt;!ATTLIST snippet doc-id CDATA #REQUIRED rsv CDATA #REQUIRED &gt; Fig.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Assessment</head><p>To determine the eectiveness of the returned snippets at their goal of allowing a user to determine the relevance of the underlying document, manual assessment will be used. In response to feedback from the previous year, both snippetbased and document-based assessment will be used. The documents will rst be assessed for relevance based on the snippets alone, as the goal is to determine the snippet's ability to provide sucient information about the document. The documents will then be assessed for relevance based on the full document text, with evaluation based on comparing these two sets of assessments.</p><p>Each topic within a submission will be assigned an assessor. The assessor, after reading the details of the topic, read through the 20 returned snippets, and judge which of the underlying documents seem relevant based on the snippets. The assessor will then be presented the full text of each document, and determine whether or not the document was actually relevant.</p><p>To avoid bias introduced by assessing the same topic more than once in a short period of time, and to ensure that each submission is assessed by the same assessors, the runs will be shued in such a way that each assessment package contains one run from each topic, and one topic from each submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Evaluation Measures</head><p>Submissions are evaluated by comparing the snippet-based relevance judgements with the document-based relevance judgements, which are treated as a ground truth. This section gives a brief summary of the specic metrics used. In all cases, the metrics are averaged over all topics.</p><p>We are interested in how eective the snippets were at providing the user with sucient information to determine the relevance of the underlying document, which means we are interested in how well the user was able to correctly determine the relevance of each document. The simplest metric is the mean precision accuracy (MPA) the percentage of results that the assessor correctly assessed, averaged over all topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPA = TP + TN TP + FP + FN + TN</head><p>(1) Due to the fact that most topics have a much higher percentage of irrelevant documents than relevant, MPA will weight relevant results much higher than irrelevant results for instance, assessing everything as irrelevant will score much higher than assessing everything as relevant.</p><p>MPA can be considered the raw agreement between two assessors one who assessed the actual documents (i.e. the ground truth relevance judgements), and one who assessed the snippets. Because the relative size of the two groups (relevant documents, and irrelevant documents) can skew this result, it is also useful to look at positive agreement and negative agreement to see the eects of these two groups.</p><p>Positive agreement (PA) is the conditional probability that, given one of the assessors judges a document as relevant, the other will also do so. This is also equivalent to the F 1 score.</p><formula xml:id="formula_1" coords="4,254.25,420.00,105.67,22.31">PA = 2 • TP 2 • TP + FP + FN</formula><p>(2) Likewise, negative agreement (NA) is the conditional probability that, given one of the assessors judges a document as irrelevant, the other will also do so.</p><formula xml:id="formula_2" coords="4,253.14,483.81,107.88,22.31">NA = 2 • TN 2 • TN + FP + FN</formula><p>(3) Mean normalised prediction accuracy (MNPA) calculates the rates for relevant and irrelevant documents separately, and averages the results, to avoid relevant results being weighted higher than irrelevant results. MNPA = 0.5 TP TP + FN + 0.5 TN TN + FP (4) This can also be thought of as the arithmetic mean of recall and negative recall. These two metrics are interesting themselves, and so are also reported separately. Recall is the percentage of relevant documents that are correctly assessed. (6) The primary evaluation metric, which is used to rank the submissions, is the geometric mean of recall and negative recall (GM). A high value of GM requires a high value in recall and negative recall i.e. the snippets must help the user to accurately predict both relevant and irrelevant documents. If a submission has high recall but zero negative recall (e.g. in the case that everything is judged relevant), GM will be zero. Likewise, if a submission has high negative recall but zero recall (e.g. in the case that everything is judged irrelevant), GM will be zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GM = TP TP + FN</head><p>• TN TN + FP (7) Participation in the track has been split into two rounds, the rst of which has had a compressed schedule. As of this writing, submissions for round 1 have closed, with submissions received from three participating organisations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper gave an overview of the INEX 2012 Snippet Retrieval track. The goal of the track is to provide a common forum for the evaluation of the eectiveness of snippets. The paper has discussed the setup of the track, the assessment method and evaluation metrics, as well as initial participation in the track.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,266.42,653.44,37.22,8.74;4,320.69,646.70,13.98,8.74;4,307.61,660.27,40.12,8.74;4,467.86,645.30,12.73,19.92;5,149.71,110.86,330.88,19.92;5,134.77,122.81,65.82,19.92;5,272.37,157.77,25.32,8.74;5,314.39,151.03,14.67,8.74;5,301.66,164.60,40.12,8.74"><head></head><label></label><figDesc>recall (NR) is the percentage of irrelevant documents that are correctly assessed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.77,325.15,306.24,110.06"><head>Table 1 .</head><label>1</label><figDesc>Participation in Round 1 of the Snippet Retrieval Track</figDesc><table coords="5,134.77,325.15,254.41,110.06"><row><cell>3 Participation</cell></row><row><cell>ID Institute</cell></row><row><cell>20 Queensland University of Technology</cell></row><row><cell>46 Jadavpur University</cell></row><row><cell>65 University of Minnesota Duluth</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,138.35,634.09,342.24,9.21;5,146.91,645.05,333.68,9.21;5,146.91,656.01,223.30,9.21" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,336.61,634.09,143.98,9.21;5,146.91,645.05,93.27,9.21">YAWN: A semantically annotated Wikipedia XML corpus</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schenkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kasneci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,261.61,645.05,214.88,9.21">12. GI-Fachtagung für Datenbanksysteme in Business</title>
		<meeting><address><addrLine>BTW</addrLine></address></meeting>
		<imprint>
			<publisher>Technologie und Web</publisher>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page">277291</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,118.89,342.24,9.21;6,146.91,129.85,333.68,9.21;6,146.91,140.81,333.68,9.21;6,146.91,151.76,25.59,9.21" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,418.37,118.89,62.22,9.21;6,146.91,129.85,96.81,9.21">Overview of the INEX 2010 ad hoc track</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Arvola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schenkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Trotman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vainio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,428.87,129.85,51.72,9.21;6,146.91,140.81,131.46,9.21">Comparative Evaluation of Focused Retrieval</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Geva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Trotman</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin / Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">132</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
