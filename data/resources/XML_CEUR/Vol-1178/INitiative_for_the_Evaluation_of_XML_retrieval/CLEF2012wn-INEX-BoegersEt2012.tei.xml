<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,160.38,116.50,294.59,12.42">RSLIS at INEX 2012: Social Book Search Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,242.63,153.97,53.52,8.66"><forename type="first">Toine</forename><surname>Bogers</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Royal School of Library and Information Science</orgName>
								<address>
									<addrLine>Birketinget 6</addrLine>
									<postCode>2300</postCode>
									<settlement>Copenhagen</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,317.17,153.97,55.56,8.66"><forename type="first">Birger</forename><surname>Larsen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Royal School of Library and Information Science</orgName>
								<address>
									<addrLine>Birketinget 6</addrLine>
									<postCode>2300</postCode>
									<settlement>Copenhagen</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,160.38,116.50,294.59,12.42">RSLIS at INEX 2012: Social Book Search Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9FE900B34DD7C50CD10B77E405D7197D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>XML retrieval</term>
					<term>social tagging</term>
					<term>controlled metadata</term>
					<term>book recommendation</term>
					<term>re-ranking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe our participation in the INEX 2012 Social Book Search track. We investigate the contribution of different types of document metadata, both social and controlled, and examine the effectiveness of re-ranking retrieval results using different social features, such as user ratings, tags, and authorship information. We find that the best results are obtained using all available document fields and topic representations. Reranking retrieval results works better on shorter topic representations, where there is less information for the retrieval algorithm to work with; longer topic representations do not benefit from our social re-ranking approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we describe our participation in the INEX 2012 Social Book Search track <ref type="foot" coords="1,155.67,413.16,3.72,6.06" target="#foot_0">1</ref> . Our goals for the Social Book Search task were <ref type="bibr" coords="1,369.63,415.02,13.29,8.66" target="#b0">(1)</ref> to investigate the contribution of additional controlled metadata provided for this year's task; and (2) to examine the effectiveness of using social features for re-ranking the initial contentbased search results. We focus in particular on using techniques from collaborative filtering (CF) to improve our content-based search results.</p><p>The structure of this paper is as follows. We start in Section 2 by describing our methodology: pre-processing the data, which document and topic fields we used for retrieval, and our evaluation. In Section 3, we describe the results of our content-based retrieval runs, including the effect of the additional controlled metadata sources. Section 4 describes our use of social features to re-rank the contentbased search results. Section 5 describes which runs we submitted to INEX, with the results of those runs presented in Section 6. We discuss our results and conclude in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data and Preprocessing</head><p>In our experiments we used the Amazon/LibraryThing collection provided by the organizers of the INEX 2012 Social Book Search track. This collection contains XML representations of 2.8 million books, with the book representation data crawled from both Amazon.com and LibraryThing (LT). The 2012 collection is identical to the collection provided for the 2011 track <ref type="bibr" coords="2,308.91,142.54,13.77,9.56" target="#b0">[1]</ref> in all but two ways: the collection has been expanded with additional library records from the British Library (BL) and the Library of Congress (LoC). Of the 2.8 million books in the collection, 1.15 million have a BL record and 1.25 have a LoC record. Together these two sources cover 1.82 million of the 2.8 million books in the collection.</p><p>We converted the collection's original XML schema into a simplified version to retain only those metadata fields that were most likely to contribute to the successful retrieval of relevant books <ref type="foot" coords="2,292.39,225.32,3.72,6.06" target="#foot_1">2</ref> . After these pre-processing steps, we were left with the following 19 content-bearing XML fields in our collection: &lt;isbn&gt;, &lt;title&gt;, &lt;publisher&gt;, &lt;editorial&gt;, &lt;creator&gt;, &lt;series&gt;, &lt;award&gt;, &lt;character&gt;, &lt;place&gt;, &lt;blurber&gt;, &lt;epigraph&gt;, &lt;firstwords&gt;, &lt;lastwords&gt;, &lt;quotation&gt;, &lt;dewey&gt;, &lt;subject&gt;, &lt;browseNode&gt;, &lt;review&gt;, and &lt;tag&gt;.</p><p>We replaced the numeric Dewey codes in the original &lt;dewey&gt; fields by their proper textual descriptions using the 2003 list of Dewey category descriptions <ref type="foot" coords="2,464.81,297.33,3.72,6.06" target="#foot_2">3</ref> to enrich the controlled metadata assigned to each book. For example, the XML element &lt;dewey&gt;519&lt;/dewey&gt; was replaced by the element &lt;dewey&gt;Probabilities &amp; applied mathematics&lt;/dewey&gt;. The BL and LoC records were provided in MODS format <ref type="foot" coords="2,194.18,345.15,3.72,6.06" target="#foot_3">4</ref> , we mapped this format to the appropriate new XML fields and added them to the book representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Field categories and Indexing</head><p>The 19 selected XML fields in our collection's book representations fall into different categories. Some fields, such as &lt;dewey&gt; and &lt;subject&gt;, are examples of controlled metadata produced by LIS professionals, whereas other fields contains usergenerated metadata, such as &lt;review&gt; and &lt;tag&gt;. Yet other fields contain 'regular' book metadata, such as &lt;title&gt; and &lt;publisher&gt;. Fields such as &lt;quotation&gt; and &lt;firstwords&gt; represent a book's content more directly.</p><p>To examine the influence of these different types of fields, we divided the document fields into five different categories, each corresponding to an index. To examine the contribution of the additional BL/LoC controlled metadata we created two versions of the index containing controlled metadata: one with and one without this additional controlled metadata. In addition, we combined all five groups of relevant fields for an index containing all fields. This all-fields index also comes in two variants: one with and one without the BL/LoC metadata. This resulted in a total of eight indexes:</p><p>All fields For our first index all-doc-fields we simply indexed all of the available XML fields (see the previous section for a complete list). The all-doc-fields-plus index contains all of the original 2011 fields as well as the BL/LoC metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metadata</head><p>In our metadata index, we include all metadata fields that are immutably tied to the book itself and supplied by the publisher: &lt;title&gt;, &lt;publisher&gt;, &lt;editorial&gt;, &lt;creator&gt;, &lt;series&gt;, &lt;award&gt;, &lt;character&gt;, and &lt;place&gt;. Content For lack of access to the actual full-text books, we grouped together all XML fields in the content index that contain some part of the book text: blurbs, epigraphs, the first and last words, and quotations. This corresponded to indexing the fields &lt;blurber&gt;, &lt;epigraph&gt;, &lt;firstwords&gt;, &lt;lastwords&gt;, and &lt;quotation&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Controlled metadata</head><p>In our controlled-metadata index, we include the three controlled metadata fields curated by library professionals harvested from Amazon: &lt;browseNode&gt;, &lt;dewey&gt;, and &lt;subject&gt;. The controlled-metadata-plus index contains the original metadata as well as the BL/LoC metadata. Tags We split the social metadata contained in the document collection into two different types: tags and reviews. For the tags index, we used the tag field, expanding the tag count listed in the original XML. For example, the original XML element &lt;tag count="3"&gt;fantasy&lt;/tag&gt; would be expanded as &lt;tag&gt;fantasy fantasy fantasy&lt;/tag&gt;. This ensures that the most popular tags have a bigger influence on the final query-document matching. Reviews All user reviews belonging to a single book were combined in a single document representation for that book and added to our review index reviews.</p><p>We used the Indri 5.1 retrieval toolkit <ref type="foot" coords="3,304.65,360.88,3.72,6.06" target="#foot_4">5</ref> for indexing and retrieval. We performed stopword filtering on all of our indexes using the SMART stopword list, and preliminary experiments showed that using the Krovetz stemmer resulted in the best performance. Topic representations were processed in the same manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Topics</head><p>As part of the INEX 2012 Social Book Search track three sets of topics were released with requests for book recommendations based on textual description of the user's information need: two training sets and a test set. All topic sets were extracted from the LibraryThing forum. The original training set of 43 topics created for the 2011 Social Book Search track came with unverified relevance judgments, so we only used the test set of 2011 as our training set for 2012. This second training set contains 211 topics with relevance judgments derived from the books recommended on the LibraryThing discussion threads of these 211 topics. We used this training set to optimize our retrieval algorithms in the different runs. The results we report in Sections 3 and 4 were obtained using this training set.</p><p>The test set for 2012 contains 90 additional topics which, combined with the 211 training set topics, were used to rank and compare the different participants' systems at INEX 2012. The results listed in Section 6 were obtained on this combined set of 301 topics. Each topic is represented by several different fields: Title The &lt;title&gt; field contains the title of the forum topic and typically provide a concise description of the information need. Runs that only use the topic title are referred to as title.</p><p>Group The LibraryThing forum is divided into different groups covering different topics. Narrative The first message of each forum topic, typically posted by the topic creator, describes the information need in more detail. This often contains a description of the information need, some background information, and possibly a list of books the topic creator has already read or is not looking for. The narrative typically contains the richest description of the topic. All topic fields We also performed runs with all three fields combined, referred to as all-topic-fields.</p><p>In our experiments with the training and the test set, we restricted ourselves to automatic runs using the following title and the all-topic-fields representations (based on our experiments for INEX 2011 <ref type="bibr" coords="4,311.53,257.41,12.25,9.56" target="#b1">[2]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Experimental setup</head><p>In all our retrieval experiments, we used the language modeling approach with Jelinek-Mercer (JM) smoothing as implemented in the Indri 5.1 toolkit. We preferred JM smoothing over Dirichlet smoothing, because previous work has shown that for longer, more verbose queries JM smoothing outperforms Dirichlet smoothing <ref type="bibr" coords="4,150.55,351.97,12.32,9.56" target="#b3">[3]</ref>, which matches the richer topic descriptions provided in the topic sets.</p><p>For the best possible performance, we optimized the λ parameter, which controls the influence of the collection language model, with higher values giving more influence to the collection language model. We varied λ in steps of 0.1, from 0.0 to 1.0 using the training set of topics. We also examined the value of stop word filtering and stemming and use the SMART stop word list and Krovetz stemming in these cases. This resulted in 44 different possible combinations of these three parameters. For each topic we retrieved up 1000 documents and we used NDCG@10 as our evaluation metric <ref type="bibr" coords="4,238.63,447.61,12.32,9.56" target="#b4">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Content-based Retrieval</head><p>In order to produce a competitive baseline for our experiments with re-ranking based on social features, we conducted a first round of experiments focused on optimizing a standard content-based retrieval approach for each combination of index and topic representations. We found that the best results were always produced with stop word filtering and Krovetz stemming, so all results reported in this paper share these settings. We compared the different index and the different topic representations for a total of 16 different content-based retrieval runs. Table <ref type="table" coords="4,431.43,572.74,5.32,8.66" target="#tab_0">1</ref> shows the best NDCG@10 results for each run on the training set.</p><p>We can see several interesting results in Table <ref type="table" coords="4,341.03,596.65,3.99,8.66" target="#tab_0">1</ref>. First, we see that the best overall content-based run used all topic fields for the training topics, retrieved against the index containing all document fields (all-doc-fields) with an NDCG@10 score of 0.3058. Retrieving on the all-doc-fields index performs best on both topic sets (all-topic-fields and title). The reviews index is a close second with strong performance on both topic sets. When we compare the two topic sets, we see that the all-topic-fields set consistently outperforms the title topic set. These findings are all in line with our 2011 results <ref type="bibr" coords="5,255.89,314.82,12.32,9.56" target="#b1">[2]</ref>.</p><p>Finally, we observe that the content and controlled-metadata indexes result in the worst retrieval performance across all four topic sets. Adding the extra BL/LoC controlled metadata has a positive effect on retrieving over only controlled metadata: the controlled-metadata-plus index outperforms the controlled-metadata on both topic sets. However, the adding this additional BL/LoC metadata to the index containing all document fields (all-doc-fields-plus) actually causes a small but surprising drop in performance. This suggests that for some topics the existing document fields better describe the documents than the information present in the BL/LoC fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Social Re-ranking</head><p>The inclusion of user-generated metadata in the Amazon/LibraryThing collection gives the track participants the opportunity to examine the effectiveness of using social features to re-rank or improve the initial content-based search results. One such a source of social data are the tags assigned by LibraryThing users to the books in the collection. The results in the previous section showed that even when treating these as a simple content-based representation of the collection using our tags index, we can achieve relatively good performance.</p><p>However, there are still many topics for which performance is sub-par, with many possible reasons for this performance gap. One explanation could be differences in document field sparsity, which could cause certain indexes to underperform for particular topics. The well-known vocabulary problem <ref type="bibr" coords="5,388.87,595.97,13.77,9.56" target="#b5">[5]</ref> could be another explanation, resulting in mismatches between synonymous query and document terms. Finally, content-based matches are no guarantee for high-quality recommendations, merely for on-topic recommendations.</p><p>To remedy these problems, we explore the use of social features for re-ranking the content-based search results in this section. We experiment with re-ranking based on book similarities (Section 4.1) as well as a personalized re-ranking approach (Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Book similarity re-ranking</head><p>Similar books that are equally relevant to a user's request for recommendations might appear at wildly different positions in the results list due to differences in term usage between the documents and the topic description. The goal of our reranking approach is to push those relevant documents that did not score well under a content-based approach to a higher position in the ranked results list. To that end we propose calculating a new retrieval score for each book that is a linear combination of (1) the original retrieval score and (2) the combined contributions of all other documents in the results list, weighted by their similarity to the book in question. This means that each of the books j retrieved for a topic contributes a little bit to the final retrieval score of a specific book i, depending on the original retrieval score score org ( j) of book j and its similarity sim(i, j) to book i. More similar books and books retrieved at higher ranks contribute more to book i's new re-ranked score score re-ranked (i); others contribute less. Equation <ref type="formula" coords="6,367.74,323.44,5.32,8.66" target="#formula_0">1</ref>shows how we calculate this score:</p><formula xml:id="formula_0" coords="6,166.72,363.77,313.87,29.90">score re-ranked (i) = α • score org (i) + (1 -α) • n j=1,i = j score org ( j) • sim(i, j)<label>(1)</label></formula><p>Before re-ranking we apply rank normalization on the retrieved results to map the score into the range [0, 1] <ref type="bibr" coords="6,261.43,416.65,12.32,9.56" target="#b6">[6]</ref>. The balance between the original retrieval score score org (i) and the contributions of the other books in the results list is controlled by the α parameter, which takes values in the range [0, 1]. The actual book similarities sim(i, j) can be calculated using different types of social features; we have explored five variants, which are described in more detail below.</p><p>User ratings As mentioned earlier, content-based matches are no guarantee for high-quality book recommendations; they merely indicate a strong term overlap between the topic description and the book descriptions. One way of dealing with this problem is to consider one of the social features in the collection that explicitly capture the quality of a book: user ratings. The reviews in the Amazon/LibraryThing collection contain the Amazon user names of the reviewers as well as their ratings on a five-star scale. We extract and use these ratings to calculate the similarities between the different books.</p><p>For each book in each of our results lists, we construct an vector of book ratings that contains all the ratings for that book from each reviewer in the Amazon/LibraryThing collection. Missing ratings-in case a reviewer did not review that particular book-receive a score of zero. We combine all item rating vectors in an IU ratings matrix where I is the number of books retrieved in all of our results lists combined and U is the number of reviewers in the collection. We normalize the IU ratings to compensate for individual differences in rating behavior <ref type="bibr" coords="6,426.59,655.75,12.32,9.56" target="#b7">[7]</ref>.</p><p>Inspired by item-based collaborative filtering <ref type="bibr" coords="7,341.48,118.62,12.32,9.56" target="#b8">[8]</ref>, we then calculate the cosine similarity between pairs of book vectors (i.e., row vectors). For re-ranking purposes we only need to calculate the book similarities for pairs of books that occur in the same results list. The resulting book similarities are then fed into our re-ranking approach (Eq. 1); we refer to this as IU-similarity.</p><p>Amazon's "similar products" The Amazon/LibraryThing collection already contains information about similar books: each book representation can contain up to ten &lt;similarproduct&gt; fields which contain the ISBN numbers of similar books, as seen on Amazon under the "similar products" section of a book Web page. We also explore the value of these book similarities in our re-ranking approaches, setting the similarity between two books sim(i, j) to 1 if book j is mentioned in the representation of book i (and vice versa), and to 0 otherwise. We refer to this as II-similarity.</p><p>How do these "similar products" stack up against the ratings-based book similarities? This "similar products" data is likely to be a more accurate representation of book similarity based on user ratings as it is calculated over the entire set of user ratings, both with and without reviews <ref type="bibr" coords="7,321.52,326.30,12.32,9.56" target="#b9">[9]</ref>. In contrast , the ratings in our IU matrix only represent the ratings of a subset of reviews and not the ratings made by users with entering an actual review. However, the "similar products" similarities are binary even though the original similarities calculated by Amazon's algorithms were not. Moreover, the "similar products" data is likely to be incomplete. Amazon only shows a random selection of 10 similar books each time a book's Web page is generated. This means that the set of similar books during the original crawling of the Amazon/LibraryThing collection represents just a subset of all similarity pairs. Tags Another source of information for calculating book similarities are the tags assigned to the different books. For this source of book similarities, we construct a IT matrix, analogous to our IU matrix. In the IT matrix, the columns represent the different tags assigned to all the books in our result lists. Each value in IT represents the number of times tag t has been assigned to book i. If a tag was not assigned to a book, that cell receives the value 0. The IT matrix is then row-normalized. We obtain the similarity between two books by calculating the cosine similarity between their two row vectors. We refer to this as IT-similarity.</p><p>Authors Author-book associations represent another way of calculating book similarities: books written by the same author(s) are often similar in style and content. To explore this type of similarity, we construct a IA matrix where the columns represent the authors associated with all the books in our result lists. Values in IA are binary, with a value of 1 if author a wrote book i, and a 0 otherwise. We obtain the similarity between two books by taking the cosine similarity between their vectors. We refer to this as IA-similarity.</p><p>Fusing ratings, tags and authors Instead of picking just one of the aforementioned sources of book similarity, we also experimented with using a combination of user ratings, tags, and authorship for calculating the book similarities. To this end we construct a combined matrix IUTA, which consists of the IU, IT, and IA matrices combined so that each book vectors contains both user ratings, tags, and authorship information. The expectation here is that the different information sources can augment each other's performance. Again, we calculate the similarity between two books by calculating the cosine similarity between their two IUTA row vectors. We refer to this as IUTA-similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Personalized re-ranking</head><p>In addition to the one-size-fits-all approach to re-ranking described in Section 4.1, we also explore a personalized re-ranking approach that takes into account the past preferences of the user who originally created the LT topic requesting book recommendations. The goal is to calculate a new personalized score score personalized (u, i) for a LibraryThing user u and a retrieved book i that pushes i up in the rankings if it is similar to other books read by u in the past. The new personalized score is a linear combination of the original retrieval score score org (i) for book i and the similarity between i and the other books in u's profile. Equation <ref type="formula" coords="8,369.08,324.93,5.32,8.66" target="#formula_1">2</ref>shows how we calculate this personalized score:</p><formula xml:id="formula_1" coords="8,191.33,360.27,289.26,10.81">score personalized (u, i) = α • score org (i) + (1 -α) • sim tag (u, i)<label>(2)</label></formula><p>Again, we control the balance the original retrieval score score org (i) and the similarity with the user's past preferences with the α parameter, which takes values in the range [0, 1]. There are different ways of calculating the similarity sim(u, i) between a user's profile and a book i book similarities: user ratings, tags, authors, or even term overlap between different metadata fields. Tags showed the most promising performance in preliminary experiments, so we construct a tag vector for all tags assigned by the user to books read in the past and calculated the cosine similarity sim tag (u, i) between that vector and the IT row vector corresponding to book i. That way, a book that shares a lot of tags with books read by a user in the past will be seen as more similar. We refer to this as pers-similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training set results</head><p>Table <ref type="table" coords="8,160.24,536.84,5.32,8.66">2</ref> shows the results of the different social re-ranking runs for the optimal α values. We optimized in steps of 0.01. The baseline runs for both topic representations are also included for convenience.</p><p>The results of the social re-ranking approaches are very different for the two topic representations. When using the title field for retrieval, all non-personalized re-ranking methods provide impressive boosts over the baseline. The best-performing re-ranking approach here is II-similarity, which uses Amazon's data about "similar products". With an NDCG@10 of 0.2429 it increase performance over the baseline by 115%. Typically, most weight is given to the original scores with α values ranging from 0.92 to 0.99, although the other retrieved books do seem to offer a small but valuable contribution, given the performance increases.</p><p>Table <ref type="table" coords="9,159.03,116.47,3.74,7.76">2</ref>. Results of the 12 different re-ranking runs using NDCG@10 as evaluation metric.</p><p>The results of the best baseline runs for each topic representation are also included for convenience. Best-performing runs for each topic representation are printed in bold. A possible explanation for the fact that II-similarity outperforms IU-similarity is that the latter similarities are calculated over an incomplete subset of Amazon user ratings; Amazon's "similar products" are likely calculated over all ratings. We can therefore also consider the results using II-similarity as an upper threshold on performance if we had all user ratings in the Amazon/LibraryThing collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs</head><p>Of the three types of similarity calculated directly on the Amazon/LibraryThing collection-IU-similarity, IT-similarity, and IA-similarity-re-ranking using tag overlap seem to provide the best performance with a score of 0.1895. Surprisingly, the combination of the three sources, IUTA-similarity, does not perform better than the individual sources. This is not in line with previous research <ref type="bibr" coords="9,387.11,412.65,17.40,9.56" target="#b10">[10]</ref>. However, when using all available topic fields for retrieval (all-topic-fields), social re-ranking does not help at all with all optimal al pha values being equal to 1.0 (which retains only the original retrieval scores. Apparently, using longer query representations makes it that much easier for the retrieval algorithm to find matching book representations so that there is no room for other types of similarities to improve upon this. This suggests that social re-ranking methods have more merit in situations where user tend to use short queries, e.g., like in Web search engines.</p><p>Personalized re-ranking does not appear to work as well as non-personalized re-ranking. The most likely explanation for this is that LibraryThing topic creators typically ask for targeted recommendations on books they do not know anything about yet and do not have in their catalog yet. However, re-ranking the results lists towards a user's past books biases the results list to a ranking that is in fact more like books they already know about as opposed to new and relevant books.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Submitted runs</head><p>We selected six automatic runs for submission to INEX 6 based on the results of our content-based and social re-ranking runs. Two of these submitted runs were content-based runs, the other four were social re-ranking-based runs. Since the reranking approaches did not benefit using all topic fields, we submitted three reranking runs based on the title and all-doc-fields baseline and one re-ranking run based on the all-topic-fields and all-doc-fields run.</p><p>Run 1 (title.all-doc-fields) This run used the titles of the test topics and ran this against the index containing all available document fields. Run 2 (all-topic-fields.all-doc-fields) This run used all topic fields combined and ran this against the index containing all available document fields. Run 3 (all-topic-fields.pers-similarity.α=0.99) This run applies the personalized re-ranking approach (pers-similarity) to run 2 with α set to 0.99; the value producing the highest NDCG scores yet not equal to 1.0. Run 4 (title.pers-similarity.α=0.65) This run applies the personalized re-ranking approach (pers-similarity) to run 1 with α set to 0.65, which provided the best results for run 1 on the training set. Run 5 (title.II-similarity.α=0.94) This run applies the re-ranking approach based on Amazon's "similar products" information (II-similarity) to run 1 with α set to 0.94, which provided the best results for run 1 on the training set. Run 6 (title.IUTA-similarity.α=0.97) This run applies the re-ranking approach based on the combination of the three information sources (IUTA-similarity) to run 1 with α set to 0.97, which provided the best results for run 1 on the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>The runs submitted to the INEX 2012 Social Book Search track were evaluated using graded relevance judgments. Books suggested by members other than the topic creator are considered relevant suggestions and received the relevance value 1. Books that are added by the topic creator to his/her LibraryThing catalog after creating the topic are considered the best suggestions and receive the relevance value 4. All runs were evaluated using NDCG@10, P@10, MRR, with NDCG@10 as the main metric. Table <ref type="table" coords="10,231.13,502.22,5.32,8.66" target="#tab_1">3</ref> shows the official evaluation results. We see that, unsurprisingly, the best-performing run on all 301 topics was run 2 with an NCDG@10 of 0.1492. Run 2 used all available topic fields and document fields. Again we see that re-ranking does not improve over the baseline when using all available topic fields. When using the title representation, we see the same performance improvements as on the training set. Run 5, for example, improves over the title baseline by 73.0%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion &amp; Conclusions</head><p>On both the training and the test sets the best results were achieved by combining all topic and document fields. This shows continued support for the principle of polyrepresentation <ref type="bibr" coords="11,216.88,257.02,19.09,9.56" target="#b11">[11]</ref> which states that combining cognitively and structurally different representations of the information needs and documents will increase the likelihood of finding relevant documents. Adding extra controlled metadata from BL and LoC did not benefit the retrieval results however.</p><p>We also experimented with different re-ranking approaches where all the books retrieved in a run were able to contribute the final scores of each separate book by weighting those scores by their similarity to the target book. We examined the usefulness of different information sources for calculating these book similarities, such as user ratings, tags, authorship, and Amazon's "similar products" information. We found that all re-ranking approaches are successful when using shorter queries; longer topic representations did not benefit from re-ranking. Although all re-ranking approach improved retrieval results using the title representations as our topics, we found that Amazon's "similar products" information-being based on the complete set of Amazon user ratings-provides the best performance.</p><p>Personalized re-ranking did not work as well as the non-personalized methods, which is likely due its inappropriate for the recommendation task: the goal is not to find books similar to what the user has read in the past, but new books that are unlike the user's past interests.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.77,116.46,345.83,149.99"><head>Table 1 .</head><label>1</label><figDesc>Results of the 16 different content-based retrieval runs on the training set using NDCG@10 as evaluation metric. Best-performing runs for each topic representation are printed in bold.</figDesc><table coords="5,198.55,159.51,213.45,106.94"><row><cell>Document fields</cell><cell>title</cell><cell cols="2">Topic fields all-topic-fields</cell></row><row><cell>metadata</cell><cell cols="2">0.0915</cell><cell>0.2015</cell></row><row><cell>content</cell><cell cols="2">0.0108</cell><cell>0.0115</cell></row><row><cell>controlled-metadata</cell><cell cols="2">0.0406</cell><cell>0.0496</cell></row><row><cell>controlled-metadata-plus</cell><cell cols="2">0.0514</cell><cell>0.0691</cell></row><row><cell>tags</cell><cell cols="2">0.0792</cell><cell>0.2056</cell></row><row><cell>reviews</cell><cell cols="2">0.1041</cell><cell>0.2832</cell></row><row><cell>all-doc-fields</cell><cell cols="2">0.1129</cell><cell>0.3058</cell></row><row><cell>all-doc-fields-plus</cell><cell cols="2">0.1120</cell><cell>0.3029</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,134.77,533.96,345.83,117.12"><head>Table 3 .</head><label>3</label><figDesc>Results of the six submitted runs on the test set, evaluated using all 301 topics with relevance judgments extracted from the LibraryThing forum topics. The best run scores are printed in bold.</figDesc><table coords="10,155.65,577.01,299.83,74.07"><row><cell cols="2">Run # Run description</cell><cell cols="2">NDCG@10 P@10</cell><cell>MRR</cell></row><row><cell>1</cell><cell>title.all-doc-fields</cell><cell>0.0678</cell><cell>0.0583</cell><cell>0.1341</cell></row><row><cell>2</cell><cell>all-topic-fields.all-doc-fields</cell><cell>0.1492</cell><cell cols="2">0.1198 0.3069</cell></row><row><cell>3</cell><cell>all-topic-fields.pers-similarity.α=0.99</cell><cell>0.1488</cell><cell cols="2">0.1198 0.3066</cell></row><row><cell>4</cell><cell>title.pers-similarity.α=0.65</cell><cell>0.0875</cell><cell>0.0719</cell><cell>0.1762</cell></row><row><cell>5</cell><cell>title.II-similarity.α=0.94</cell><cell>0.1173</cell><cell>0.1073</cell><cell>0.2558</cell></row><row><cell>6</cell><cell>title.IUTA-similarity.α=0.97</cell><cell>0.0958</cell><cell>0.0823</cell><cell>0.2392</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,657.03,238.27,8.03"><p>https://inex.mmci.uni-saarland.de/tracks/books/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,634.55,281.35,8.61"><p>Please consult<ref type="bibr" coords="2,200.03,634.55,12.39,8.61" target="#b1">[2]</ref> for more details on this filtering and conversion process.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,144.73,646.07,309.46,8.03"><p>Available at http://www.library.illinois.edu/ugl/about/dewey.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,144.73,657.03,271.77,8.03"><p>See http://www.loc.gov/standards/mods/ for more information.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,144.73,657.03,188.03,8.03"><p>Available at http://www.lemurproject.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="9,144.73,657.08,100.05,7.79"><p>Our participant ID was 54.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,143.14,515.16,337.45,7.79;11,151.71,526.11,328.88,7.79;11,151.71,537.07,100.68,7.79" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,376.27,515.16,104.33,7.79;11,151.71,526.11,108.99,7.79">Overview of the INEX 2011 Book and Social Search Track</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Landoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,279.98,526.11,200.61,7.79;11,151.71,537.07,46.10,7.79">INEX 2011 Workshop pre-proceedings. INEX Working Notes Series</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="11" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,143.14,547.90,337.45,7.79" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="11,304.55,547.90,172.13,7.79">RSLIS at INEX 2011: Social Book Search Track</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bogers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Larsen</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,151.71,558.86,328.88,7.79;11,151.71,569.81,328.88,7.79;11,151.71,580.77,324.34,7.79" xml:id="b2">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="11,311.75,558.86,168.84,7.79;11,151.71,569.81,261.68,7.79">INEX 2011: Proceedings of the 10th International Workshop of the Initiative for the Evaluation of XML Retrieval</title>
		<title level="s" coord="11,151.71,580.77,129.09,7.79">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Geva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Schenkel</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7424</biblScope>
			<biblScope unit="page" from="45" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,143.14,591.60,337.45,7.79;11,151.71,602.56,328.88,7.79;11,151.71,613.51,14.36,7.79" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,234.77,591.60,245.82,7.79;11,151.71,602.56,80.35,7.79">A Study of Smoothing Methods for Language Models Applied to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,241.16,602.56,163.10,7.79">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="214" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,143.14,624.34,337.45,7.79;11,151.71,635.30,227.92,7.79" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,259.60,624.34,193.92,7.79">Cumulated Gain-based Evaluation of IR Techniques</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,462.20,624.34,18.39,7.79;11,151.71,635.30,139.41,7.79">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,143.14,646.12,337.45,7.79;11,151.71,657.08,296.30,7.79" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,344.22,646.12,136.37,7.79;11,151.71,657.08,87.01,7.79">The Vocabulary Problem in Human-System Communication</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,246.51,657.08,108.21,7.79">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="964" to="971" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,143.14,119.95,337.45,7.79;12,151.71,130.91,328.88,7.79;12,151.71,141.87,159.24,7.79" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,257.68,119.95,222.92,7.79;12,151.71,130.91,30.41,7.79">Web Metasearch: Rank vs. Score-based Rank Aggregation Methods</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Renda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Straccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,201.69,130.91,274.50,7.79">SAC &apos;03: Proceedings of the 2003 ACM Symposium on Applied Computing</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="841" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,143.14,152.83,337.45,7.79;12,151.71,163.79,328.88,7.79;12,151.71,174.75,328.88,7.79;12,151.71,185.71,214.56,7.79" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,302.44,152.83,178.16,7.79;12,151.71,163.79,173.14,7.79">Unifying User-based and Item-based Collaborative Filtering Approaches by Similarity Fusion</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Reinders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,347.55,163.79,133.04,7.79;12,151.71,174.75,328.88,7.79;12,151.71,185.71,49.35,7.79">SIGIR &apos;06: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="501" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,143.14,196.67,337.45,7.79;12,151.71,207.62,328.88,7.79;12,151.71,218.58,240.31,7.79" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,322.10,196.67,158.50,7.79;12,151.71,207.62,82.29,7.79">Item-Based Collaborative Filtering Recommendation Algorithms</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,253.68,207.62,226.91,7.79;12,151.71,218.58,73.86,7.79">WWW &apos;01: Proceedings of the 10th International Conference on World Wide Web</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="285" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,143.14,229.54,337.45,7.79;12,151.71,240.50,221.68,7.79" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,267.09,229.54,213.50,7.79;12,151.71,240.50,46.80,7.79">Amazon.com Recommendations: Item-to-Item Collaborative Filtering</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Linden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>York</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,205.27,240.50,94.18,7.79">IEEE Internet Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="80" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.74,251.46,337.85,7.79;12,151.71,262.42,328.88,7.79;12,151.71,273.38,322.45,7.79" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,353.04,251.46,127.55,7.79;12,151.71,262.42,177.65,7.79">Tag-aware Recommender Systems by Fusion of Collaborative Filtering Algorithms</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">H L</forename><surname>Tso-Sutter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">B</forename><surname>Marinho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,352.01,262.42,128.57,7.79;12,151.71,273.38,146.99,7.79">SAC &apos;08: Proceedings of the 2008 ACM symposium on Applied computing</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1995" to="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.74,284.34,337.85,7.79;12,151.71,295.30,259.02,7.79" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,208.09,284.34,272.50,7.79;12,151.71,295.30,79.26,7.79">Cognitive Perspectives of Information Retrieval Interaction: Elements of a Cognitive IR Theory</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ingwersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,238.27,295.30,98.30,7.79">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="50" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
