<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,135.94,115.96,343.47,12.62;1,208.15,133.89,199.07,12.62">DCU</title>
				<funder>
					<orgName type="full">Centre for Next Generation Localisation</orgName>
					<orgName type="abbreviated">CNGL</orgName>
				</funder>
				<funder ref="#_ZaGzHyb">
					<orgName type="full">Science Foundation Ireland</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,174.65,171.83,71.19,8.74"><forename type="first">Debasis</forename><surname>Ganguly</surname></persName>
							<email>dganguly@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution" key="instit1">CNGL</orgName>
								<orgName type="institution" key="instit2">Dublin City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,254.12,171.83,77.92,8.74"><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
							<email>jleveling@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution" key="instit1">CNGL</orgName>
								<orgName type="institution" key="instit2">Dublin City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,359.06,171.83,81.65,8.74"><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
							<email>gjones@computing.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution" key="instit1">CNGL</orgName>
								<orgName type="institution" key="instit2">Dublin City University</orgName>
								<address>
									<settlement>Dublin 9</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,135.94,115.96,343.47,12.62;1,208.15,133.89,199.07,12.62">DCU</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">981C678462B590DD6CB5FB698AF76B46</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For the participation of Dublin City University (DCU) in the INEX-2012 tweet contextualization task, we investigated sentence retrieval methodologies. The task requires providing the context to an ad-hoc real-life tweet. This context is to be constructed from Wikipedia articles. Our approach involves indexing the passages in Wikipedia articles as separate retrievable units, extracting sentences from the top ranked passages, computing the sentence selection score for each such sentence with respect to the query, and then returning the top most similar ones. The simple sentence selection strategy performed quite well in the task. Our best run has ranked first from the readability perspective and ranked eighth as ordered by informativeness out of 33 official runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The tweet contextualization task was first introduced at INEX in 2011. The task requires construction of a short summary so as to explain the context associated with a given tweet. This context information has to be constructed from Wikipedia articles. As an example, for the CNN tweet "RT @CNNLive: View stake-out camera at funeral home where #WhitneyHouston body is expected to arrive in New Jersey. Watch live: http://t.co/nyqT4PUa", the system is expected to provide such expository information as who is Whitney Houston, what is she famous for, how did she die etc.</p><p>The task being different from standard ad-hoc IR poses with its own set of challenges. Firstly, the tweet text is very different from keyword based queries of ad-hoc search or web search. This necessiatates applying pre-processing steps on the tweet texts to get an appropriate query string. For example, the tweet hash-tags do not exist in Wikipedia articles and needs to be appropriately processed to get a useful query term. Secondly, a standard passage retrieval may not be suitable for the task because of the restriction on the length in the reported summary. The text in a passage itself may surpass the length theshold requirement of the summary. It thus makes sense to decompose passages into smaller units, i.e. sentences, and collate them together.</p><p>Previous approaches to INEX-QA have mostly used passage retrieval coupled with a summarizer. Sentence retrieval on the other hand has widely been employed in TREC-QA tasks for both factoid and definition question answering <ref type="bibr" coords="1,467.30,656.12,9.96,8.74" target="#b0">[1]</ref>.</p><p>Sentence retrieval has the potential to perform well for tweet contextualization because sentences being short contain more focussed information than the relatively larger passages which may contain digressory content. Furthermore, the effect of sentence retrieval on tweet contextualization has still been unexplored. This motivated us to apply various sentence retrieval strategies on the tweet contextualization task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>In this section, we describe our system details. After describing the document and query processing, and retrieval, we focus on to our working methodologies for sentence selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document Indexing</head><p>We used a modified version of the SMART<ref type="foot" coords="2,317.70,296.59,3.97,6.12" target="#foot_0">1</ref> system for the experiments at INEX 2012. Each paragraph from the Wikipedia corpus<ref type="foot" coords="2,350.37,308.55,3.97,6.12" target="#foot_1">2</ref> was indexed as a retrievable document unit. The beginning of a passage is marked by the XML tag &lt;p&gt;. This resulted in a total of over 26M passages to retrieve from. Extracted portions of documents, namely text under the &lt;title&gt;, &lt;p&gt;, &lt;h&gt;, &lt;t&gt; tags, were indexed using single terms and a controlled vocabulary (or pre-defined set) of statistical phrases following Salton's blueprint for automatic indexing <ref type="bibr" coords="2,440.76,369.90,9.96,8.74" target="#b1">[2]</ref>. Stopwords that occur in the standard stop-word list included within SMART were removed. Words were stemmed using a variation of the Lovins' stemmer implemented within SMART. Frequently occurring word bi-grams (loosely referred to as phrases) were also used as indexing units. We used the N-gram Statistics Package (NSP)<ref type="foot" coords="2,179.79,428.10,3.97,6.12" target="#foot_2">3</ref> on the English Wikipedia text corpus from INEX 2006 and selected the 100,000 most frequent word bi-grams as the list of candidate phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Query Processing</head><p>The tweet texts were pre-processed to produce queries to retrieve against the indexed collection. The pre-processing steps are described as follows. The URLs from the tweets were removed employing a regular expression based pattern matcher. Medial capital words, i.e. words with inner uppercase letters, were split into separate words e.g. the word "WhitneyHouston" was decomposed into "Whitney" and "Houston". Tweet hash-tags were split up into the prefix # character followed by the word, e.g. "#Whitney" was decomposed into # and "Whitney".</p><p>The following word breaking rules were applied to split hashtags starting with "#" and usernames starting with "@": A break between the last and current character is employed if: i) the last character is lower case and the current character is upper case or digit (e.g. "OccupyWallStreet" -&gt;"Occupy Wall Street"); ii) the last character is upper case and the last character of a valid acronym, the current character is also upper case or a digit (e.g. "CNNNews" -&gt; "CNN News"; iii) the last character and the current character have different case and the resulting word would be longer than 3 characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Retrieval</head><p>The context for each tweet was constructed in two passes as follows. In the first pass, we retrieved N passages using language modelling (LM) <ref type="bibr" coords="3,403.84,249.40,10.52,8.74" target="#b2">[3]</ref> similarity with Jelinek-Mercer smoothing. The smoothing parameter λ was set to 0.6. In the second pass, we score senteneces based on three different methodologies, explained later in details. We then concatenate the top M sentences until the length of the concatenated summary string exceeds the threshold of 500 characters limit. The concatenation step ensures that we do not add duplicate sentences in the summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Sentence Retrieval Methodologies</head><p>Language Modelling Similarity. The most simple sentence scoring technique is that of scoring a sentence S by its LM score computed with respect to the query i.e. the pre-processed tweet text. This is done as shown in Equation <ref type="formula" coords="3,461.32,392.37,3.87,8.74" target="#formula_0">1</ref>.</p><formula xml:id="formula_0" coords="3,226.40,414.18,254.18,20.06">P (S|Q) ∝ q∈Q µP (q|S) + (1 -µ)P (q)<label>(1)</label></formula><p>Note that the smooting parameter µ used in Equation 1 is different from λ which was used for retrieving the passages as discussed in Section 2.3.</p><p>Relevance Model Similarity. The second sentence selection strategy which we use is derived from relevance model (RLM) term scores <ref type="bibr" coords="3,395.60,497.33,9.96,8.74" target="#b3">[4]</ref>. The key idea in RLM-based retrieval is that relevant documents and query terms are assumed to be sampled from an underlying hypothetical model of relevance R pertaining to the information need expressed in the query. In the absence of training data for the relevant set of documents, the only observable variables are the query terms and the top-ranked R pseudo-relevant documents assumed to be generated from the relevance model. Thus, the estimation of the probability of a word w being generated from the relevance model is approximated by the conditional probability of observing w given the observed query terms. Thus higher a word w co-occurs with a query tern q, higher is the likelihood of w to be sampled from the relevance model, i.e. higher is P (w|R). This is shown in Equation <ref type="formula" coords="3,440.52,616.88,3.87,8.74" target="#formula_1">2</ref>.</p><formula xml:id="formula_1" coords="3,237.96,636.58,242.63,30.32">P (w|q i ) ∝ R j=1 P (w|D j )P (q i |D j )<label>(2)</label></formula><p>We can easily extend this notion of relevance model weighting of terms to whole sentences by simply aggregating over the constituent words of a sentence. This is shown in Equation 3 which we use to score every sentence and select the top-scoring ones in the returned summary.</p><formula xml:id="formula_2" coords="4,259.24,174.83,221.35,20.06">P (S|R) = w∈S P (w|R)<label>(3)</label></formula><p>Topical Relevance Model Similarity. This sentence selection score is based on an extended version of relevance model (RLM) similarity. In our extended relevance model, we compute the probabilities P (w|D)s by marginalizing them over a set of latent topics. Firstly, we estimate the topic distribution over the set of top ranked passages retrieved in the initial step by latent Dirichlet allocation (LDA) <ref type="bibr" coords="4,167.77,271.17,9.96,8.74" target="#b4">[5]</ref>. LDA outputs two distribution vectors θ (from document to topic) and φ (from topic to word). Modified smoothed document models are obtained by using these two distributions as shown in Equation <ref type="formula" coords="4,370.33,295.08,3.87,8.74" target="#formula_3">4</ref>, where K is the number of topics used in the LDA estimation.</p><formula xml:id="formula_3" coords="4,230.18,324.35,250.41,30.55">P (w|D) = K k=1 P (w|z k , φ)P (z k |D, θ)<label>(4)</label></formula><p>We then use the topic smoothed document models in the estimation of RLM i.e. we use the definition of P (w|D) as obtained from Equation <ref type="formula" coords="4,398.03,374.18,4.98,8.74" target="#formula_3">4</ref>in 2 to obtain an extended RLM sentence selection methodology which we name topical relevance model (TRLM) similarity.</p><formula xml:id="formula_4" coords="4,195.93,415.96,284.67,30.55">P (w|q i ) ∝ R j=1 K k=1 P (w|z k , φ)P (z k |D j , θ) P (q i |D j ) (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Run Description</head><p>We submitted three official runs (run ids: 185, 186 and 187) for the INEX-2012 Tweet contextualization task. The first pass passage retrieval for each of the three runs is identical and follows the description of Sections 2.1, 2.2 and 2.3. The sentence retrieval strategies of each of these runs is different. Run 185 used simple language modelling (LM) similarity, run 186 used RLM similarity, whereas run 187 used TRLM similarity to score sentences. The number of top documents used for the (T)RLM estimation was set to 20. For TRLM, the additional parameter K, i.e. the number of topics, was set to 5. Our submissions did not use any automatic summarization techniques for sentence selection. We rather relied on pure IR-based approaches to generate the twweet contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>The tweet contexts were evaluated with two measures: a) informativeness, which measures the closeness of the answer string with a golden reference with the help of KL divergence between the two; and b) readability, which measures the syntactic coherence of the text such as whether it has grammatical errors, has unresolved anaphora or is redundant etc <ref type="bibr" coords="5,314.04,369.07,9.96,8.74" target="#b5">[6]</ref>. Table <ref type="table" coords="5,177.56,381.34,4.98,8.74" target="#tab_0">1</ref> reports the official results of our three submitted runs. Along with our runs, the table shows the offcial best run as measured by informativeness and also the run submitted by the organizers as the baseline. Informativeness evaluation involves computation of three metrics: the KL divergence between the golden summary and the returned summary for uni-grams, bi-grams, and bi-grams with two allowable gaps in between <ref type="bibr" coords="5,330.67,441.12,9.96,8.74" target="#b5">[6]</ref>. Note that KL divergence being a distance measure implies that a lower value of this metric is indicative of a better result. The readability metric on the other hand reports the proportion of text which has correct syntax, structure and is relevant in the context. As a result, a higher value of these metrics indicates a better result.</p><p>It can be seen that the most simple sentence retrieval technique using LM similarity fairly well, achieving rank eight, as measured by informativeness. This run in fact achieves the best readability result.</p><p>Our other runs, i.e. the (T)RLM based sentence selection strategies, have not performed well in the official evaluation. The release of official relevance assessments namely the reference summary context for each tweet would enable us to tune the parameters of the two other sentence selection strategies in order to achieve an improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future work</head><p>In our first participation at the INEX Tweet contextualization task, we applied sentence retrieval to construct answer fragments for each tweet. Three different sentence selection methodologies were used: i) language modelling (LM) score, ii) relevance modelling (RLM) scoring of a sentence by accumulating over the RLM scores of its constituent terms, and iii) topical relevance modelling (TRLM) scoring of a sentence by accumulating over the topic smoothed RLM scores of its constituent terms.</p><p>The results confirm that simple IR-based sentence selection techniques can perform fairly well on both the informativeness and the readability metrics, without the application of any complex NLP techniques. The main advantage of the sentence retrieval methodologies is that these are very fast in contrast to computationally intensive NLP methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,154.14,115.91,307.08,198.72"><head>Table 1 .</head><label>1</label><figDesc>Official results for INEX-2012 Tweet contextualization task</figDesc><table coords="5,154.14,138.15,307.08,176.48"><row><cell cols="2">Run Id Run Description</cell><cell>Rank</cell><cell cols="2">Informativeness Metrics</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Uni-gram Bi-gram Skip-gram</cell></row><row><cell>185</cell><cell>LM sentence retrieval</cell><cell>8</cell><cell>0.8265 0.9129</cell><cell>0.9135</cell></row><row><cell>186</cell><cell>RLM sentence retrieval</cell><cell>10</cell><cell>0.8347 0.9210</cell><cell>0.9208</cell></row><row><cell>187</cell><cell cols="2">TRLM sentence retrieval 11</cell><cell>0.8360 0.9235</cell><cell>0.9237</cell></row><row><cell>178</cell><cell>Official best</cell><cell>1</cell><cell>0.7734 0.8616</cell><cell>0.8623</cell></row><row><cell>194</cell><cell>Organizers' baseline</cell><cell>4</cell><cell>0.7864 0.8868</cell><cell>0.8887</cell></row><row><cell cols="2">Run Id Run Description</cell><cell>Rank</cell><cell cols="2">Readability Metrics</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Relevance Syntax Structure</cell></row><row><cell>185</cell><cell>LM sentence retrieval</cell><cell>1</cell><cell>0.7728 0.7452</cell><cell>0.6446</cell></row><row><cell>186</cell><cell>RLM sentence retrieval</cell><cell>5</cell><cell>0.7008 0.6676</cell><cell>0.5636</cell></row><row><cell>187</cell><cell cols="2">TRLM sentence retrieval 14</cell><cell>0.6093 0.5252</cell><cell>0.4847</cell></row><row><cell>194</cell><cell>Organizers' baseline</cell><cell>4</cell><cell>0.6975 0.6342</cell><cell>0.5703</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,624.57,164.76,7.47"><p>ftp://ftp.cs.cornell.edu/pub/smart/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,635.53,324.81,7.47;2,144.73,646.48,28.24,7.47"><p>http://dev.termwatch.es/esj/Term2IR/2012/data/tweetcontext2012corpus. xml.gz</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,144.73,657.44,183.59,9.21"><p>http://www.d.umn.edu/ ~tpederse/nsp.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research is supported by the <rs type="funder">Science Foundation Ireland</rs> (Grant <rs type="grantNumber">07/CE/I1142</rs>) as part of the <rs type="funder">Centre for Next Generation Localisation (CNGL)</rs> project.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ZaGzHyb">
					<idno type="grant-number">07/CE/I1142</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,138.35,349.81,342.24,7.86;6,146.91,360.77,23.04,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="6,219.51,349.81,224.32,7.86">Overview of the TREC 2003 question answering track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="54" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,371.70,342.24,7.89;6,146.91,382.69,48.13,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,196.31,371.73,145.97,7.86">A Blueprint for Automatic Indexing</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,351.82,371.73,80.81,7.86">ACM SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="22" to="38" />
			<date type="published" when="1981">Fall 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,393.65,342.25,7.86;6,146.91,404.61,257.40,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,203.47,393.65,195.24,7.86">Using Language Models for Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>AE Enschede</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Center of Telematics and Information Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="6,138.35,415.56,342.24,7.86;6,146.91,426.52,147.89,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,262.35,415.56,134.62,7.86">Relevance based language models</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">W</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,421.52,415.56,59.07,7.86;6,146.91,426.52,55.48,7.86">Proceedings of the SIGIR &apos;01</title>
		<meeting>the SIGIR &apos;01</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,437.48,342.24,7.86;6,146.91,448.41,94.05,7.89" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,298.66,437.48,106.90,7.86">Latent Dirichlet allocation</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,415.46,437.48,65.13,7.86;6,146.91,448.44,17.08,7.86">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,459.40,342.24,7.86;6,146.91,470.36,333.67,7.86;6,146.91,481.32,333.68,7.86;6,146.91,492.28,103.83,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,392.20,459.40,88.39,7.86;6,146.91,470.36,179.52,7.86">Overview of the INEX 2011 Question Answering Track (QA@INEX)</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Moriceau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,349.45,470.36,131.13,7.86;6,146.91,481.32,235.76,7.86">Pre-proceedings of the INitiative for the Evaluation of XML retrieval workshop (INEX 2011)</title>
		<meeting><address><addrLine>Saarbrcken (Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-12">December 2011</date>
			<biblScope unit="page" from="145" to="153" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
