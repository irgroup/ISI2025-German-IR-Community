<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.45,106.08,314.46,26.29;1,149.60,124.01,316.16,26.29;1,224.19,141.94,166.98,26.29">Testing a Statistical Word Stemmer based on Axality Measurements in INEX 2012 Tweet Contextualization Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,151.65,189.05,132.86,9.96"><forename type="first">Carlos-Francisco</forename><surname>Méndez-Cruz</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">qsvEsnstituto de sngenierí xewD wéxio mendezdiiFunmFmxD sorinopveldgmilFom</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,295.06,189.05,142.04,9.96"><forename type="first">Edmundo-Pavel</forename><surname>Soriano-Morales</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">qsvEsnstituto de sngenierí xewD wéxio mendezdiiFunmFmxD sorinopveldgmilFom</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,257.25,201.01,96.39,9.96"><forename type="first">Alfonso</forename><surname>Medina-Urrea</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">il golegio de wéxio eFgFD wéxio medinudolmexFmx</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.45,106.08,314.46,26.29;1,149.60,124.01,316.16,26.29;1,224.19,141.94,166.98,26.29">Testing a Statistical Word Stemmer based on Axality Measurements in INEX 2012 Tweet Contextualization Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2E52BE4977C6CD366F38A71FFFBC9FF6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>INEX</term>
					<term>Automatic summarization system</term>
					<term>Axality Measurements</term>
					<term>Morphological Segmentation</term>
					<term>Statistical Stemming</term>
					<term>CORTEX</term>
					<term>Tweet Contextualization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>his pper presents n experiment of sttistil word stemE ming sed on 0xlity mesurementsF hese mesurements quntify three hrteristis of lngugeF sn this experiment we tested one strtE egy of stemming with three di'erent sizes of trining dtF he developed stemmer ws used y the utomti summriztion system Cortex to preproess input texts nd produe redle summriesF ell summries were evluted s prt of the sxi PHIP weet gontextuliztion rkF e present the results of evlution nd disussion out our stemming strtegyF</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task proposed in the INEX 2012 Tweet Contextualization Track consists in obtaining some textual context from the English Wikipedia about the subject of a tweet. The nal contextualization of the tweet should take the form of a readable summary of 500 words. An amount of 1133 documents, contextualized tweets with text from Wikipedia from November 2011, were processed in order to obtained summaries. Bibliographic references an empty Wikipedia pages were omitted.</p><p>The evaluation of summaries was done by the INEX organizers taking into account informativeness and readability. The former was obtained using Kullback-Leibler divergence with Dirichlet smoothing by comparing n-gram distributions. The latter was accomplished by the participants in the track; they evaluated the summaries taking into account syntax, anaphoric resolution and redundancy. More details of the system of evaluation and the INEX 2012 Tweet Contextualization Track could be found in <ref type="bibr" coords="1,275.07,655.45,9.96,9.96">[1]</ref>.</p><p>For this track we developed a stemmer based on morphological segmentation. The stemmer was coupled with Cortex, an automatic summarization system, in order to generate the summaries. We tested three sizes of training corpora to determine the best option for statistical stemming for English.</p><p>The organization of this paper is as follows: in Section 2 we review some approaches of morphological segmentation; in Section 3 we present word stemming; in Section 4 we describe the axality measurements; Section 5 presents the stemming strategy; evaluation obtained in INEX track is expose in Section 6 and nally, in Section 7, we briey present our conclusions and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Morphological Segmentation</head><p>The rst work for unsupervised discovery of morphological units of language is due to Zellig Harris <ref type="bibr" coords="2,239.50,282.32,9.96,9.96">[2]</ref>. His method, commonly known as frequent successor, consists in counting dierent letters or symbols before and after a possible morphological boundary. As more dierent symbols, the probability of a true morphological cut increases. This approach shown, among other things, that uncertainty is a well clue for morphological segmentation. Now a day, one of the most utilized methods for unsupervised learning of morphology is based on Minimum Description Length (MDL) approach. This has been developed as a computational system called Linguistica <ref type="bibr" coords="2,429.77,366.06,10.51,9.96">[3,</ref><ref type="bibr" coords="2,441.95,366.06,7.01,9.96">4]</ref>. <ref type="foot" coords="2,452.46,363.96,3.97,7.94" target="#foot_0">1</ref> This method tries to obtain a lexicon of morphs inferred from a corpus. The best lexicon is the one that has the less redundancy, i.e. when the description length of the data is the lowest. Also, this utilizes some combinatorial structures called signatures in order to improve segmentation. This method has been employed for stemming work in <ref type="bibr" coords="2,231.50,425.84,9.96,9.96">[5]</ref>. In that paper the developed stemmer was utilized for an information retrieval task instead of summarization.</p><p>The mission of preprocessing documents for tasks of NLP, such as Question Answering, Information Retrieval or Automatic Text Summarization, in agglutinative languages is more complex. This is due to the fact that agglutinative languages have numerous combinations of morphs rather than a simple prexstem-sux combination. A method of unsupervised morphological segmentation for these kinds of languages is called Morfessor <ref type="bibr" coords="2,339.59,509.58,18.59,9.96">[69]</ref>.<ref type="foot" coords="2,365.53,507.48,3.97,7.94" target="#foot_1">2</ref> This approach uses MDL by Maximum a Posteriori framework. Also, it integrates a morphotactic analysis to represent each word by a Hidden Markov Model (HMM). We are not sure if this method has been used for word stemming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Word stemming</head><p>The majority of NLP systems preprocesses documents in order to decrease the Vector Space Model representation. This is the case of Cortex, which will be explained below. A well-known strategy for that purpose is word stemming, i.e. truncating words by eliminating the inection. Also, it is possible to remove derivational axes.</p><p>The methods most widely used for word stemming are created by means of hand-made rules, like <ref type="bibr" coords="3,246.41,154.22,15.50,9.96">[10,</ref><ref type="bibr" coords="3,263.57,154.22,11.62,9.96">11]</ref>. These kinds of stemmers have been successfully applied for European languages. However, languages with more complex morphology than English, such as agglutinative ones, need unsupervised morphological strategies in order to deal with language complexity.</p><p>In <ref type="bibr" coords="3,162.06,202.07,15.50,9.96">[12]</ref> a review of stemming methods is presented. The variety of stemming approaches includes: distance function to measure an orthographical similarity <ref type="bibr" coords="3,134.77,225.98,14.61,9.96">[13]</ref>, directed graphs <ref type="bibr" coords="3,227.30,225.98,15.50,9.96">[14,</ref><ref type="bibr" coords="3,244.46,225.98,7.01,9.96">5]</ref>, and frequency of n-grams of letters <ref type="bibr" coords="3,415.61,225.98,14.61,9.96">[15]</ref>. Moreover, there are some works about stemming evaluation in information retrieval tasks, for example <ref type="bibr" coords="3,189.02,249.89,15.50,9.96">[16,</ref><ref type="bibr" coords="3,206.18,249.89,11.62,9.96">17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Axality Measurements</head><p>The axality measurements used to morphological segmentation were proposed for Spanish in <ref type="bibr" coords="3,196.67,317.94,15.50,9.96">[18,</ref><ref type="bibr" coords="3,213.82,317.94,11.62,9.96">19]</ref>. These measurements have been also applied to <ref type="bibr" coords="3,433.98,317.94,42.96,9.96">Czech [20]</ref>, and to the Amerindian Languages <ref type="bibr" coords="3,283.41,329.89,112.74,9.96">Chuj and Tarahumara [21]</ref>. This approach lies on the linguistic idea that there is a force between segments of a word (morphs) called axality. If we can quantify this axality, we can expect some peaks where morphological cuts are possible. In next sections we present the way to calculate these measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Entropy</head><p>As we said above, Harris's approach revealed that uncertainty helps to morphological segmentation. This uncertainty could be seen as the Shannon's concept of information content (entropy) <ref type="bibr" coords="3,285.38,451.74,14.61,9.96">[22]</ref>. To calculate the entropy of a possible segmentation, given a i,j ::b i,j as a word segmentation, and B i,j as a set of all segments combined with a i,j , we can used the formula:</p><formula xml:id="formula_0" coords="3,205.87,509.61,274.72,10.32">H (a i,j :: B i,j ) = - p (b k,j ) × log 2 (p (b k,j ))<label>(1)</label></formula><p>where k = 1, 2, 3, . . . |B i,j | and each b k,j ∈ B i,j . For our purpose we tested peaks of entropy from right to left in order to discover suxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Economy Principle</head><p>The Economy Principle could be understood as follows: fewer units at one level of language are combined in order to create a great number of other units at the next level. Taking advantage of this principle, we can dene a stem as a word segment that belong to a big set of relatively infrequent units, and axes as word segments that belong to a small set of frequent ones. In [23] a quantication of this economy was suggested, however, we present a reformulation. Given a word segmentation a i,j ::b i,j , the economy of a segmentation is calculated depending on type of morph hypothesized:</p><formula xml:id="formula_1" coords="4,182.51,157.63,298.09,28.53">K p i,j = 1 - |A i,j | -|A p i,j | |B s i,j | ; K s i,j = 1 - |B i,j | -|B s i,j | |A p i,j |<label>(2)</label></formula><p>where A i,j is the set of segments which alternate with b i,j (a i,j ∈ A i,j ), and B i,j a set of segments which alternate with a i,j (b i,j ∈ B i,j ). Also, let A p i,j be the set of segments which are likely prexes, and B s i,j the set of segments which are likely suxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Numbers of Squares</head><p>Joseph <ref type="bibr" coords="4,167.64,279.50,63.79,9.96">Greenberg [24]</ref> proposed the concept of square when four expressions of language, let say A, B, C, D, are combined to form AC, BC, AD, and BD. Hence, we set c i,j as a number of squares found in segment j of the word i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Stemming Strategy</head><p>The axality of all possible segmentations within a word is estimated by an average of normalized values of the three explained measurements:</p><formula xml:id="formula_2" coords="4,196.99,398.89,283.60,22.31">AF n (s x ) = c x /max c i + k x /max k i + h x /max h i 3 (3)</formula><p>To calculate this axality, a training corpus of raw text is required. In this track we use three dierent sizes of 100k, 200k, and 500k word tokens. With an index of axality calculated for each possible word segment, it is possible to choose a strategy for morphological segmentation; for example <ref type="bibr" coords="4,410.05,464.21,15.50,9.96">[19]</ref> propounded four strategies.</p><p>In this experiment we use a peak-valley strategy for segmentation. Given a set of axality indexes inside a word af k i , let af k i-1 &lt; af k i &gt; af k i+1 be a peak of axality from left to right, where k is the length of the word plus one (the ending of the word). The main disadvantage of this approach is that small peaks are taking into account generating oversegmentation.</p><p>Regarding stemming, we truncate words at most left peak of axality. For a language with scare morphology like English, we can imagine that a most right peak of axality could be sucient for stemming. However, in order to improve Cortex summarization, we decide to strongly conate words by a left-peak strategy. Next section explains CORTEX's approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Cortex Summarizer</head><p>As we mentioned before, Cortex is an automatic text summarizer system. A wide explanation of this summarizer could be found in <ref type="bibr" coords="4,375.80,655.45,28.46,9.96">[2529]</ref>. Here, we briey describe some relevant aspects. First, Cortex represents input documents in Vector Space Model. To do that, the documents should be preprocessed. Actually, we incorporate our stemmer in this step.</p><p>After preprocessing, a frequency matrix γ is generated representing the presence and absence of words (terms) in a sentence:</p><formula xml:id="formula_3" coords="5,200.64,186.56,279.95,54.98">γ =      γ 1 1 γ 1 2 . . . γ 1 i . . . γ 1 M γ 2 1 γ 2 2 . . . γ 2 i . . . γ 2 M . . . . . . . . . . . . . . . . . . γ P 1 γ P 2 . . . γ P i . . . γ P M      , γ µ i ∈ {0, 1, 2, . . . }<label>(4)</label></formula><p>each element γ µ i of this matrix represents the number of occurrences of the wordi in the sentence µ;</p><formula xml:id="formula_4" coords="5,243.33,258.96,172.04,9.96">1 ≤ i ≤ M words, 1 ≤ µ ≤ P sentences.</formula><p>Then, statistical information is extracted from the matrix by calculating some metrics. More information about these metrics could be found in <ref type="bibr" coords="5,451.06,282.87,14.61,9.96">[30]</ref>. A summary of this metrics is oered here; they are based on frequencies, entropy, measures of Hamming and hybrid values.</p><p>1. Frequency measures.</p><p>(a) Term Frequency:</p><formula xml:id="formula_5" coords="5,138.97,333.27,341.62,77.42">F µ = M i=1 γ µ i (b) Interactivity of segments: I µ = M i=1 ξ µ i =0 P j=1 j =µ ξ j i (c) Sum of probability frequencies: ∆ µ = M i=1 p i γ µ i ; p i = word's i probabil- ity 2. Entropy. E µ = - M i=1 ξ µ i =0 p i log 2 p i</formula><p>3. Measures of Hamming. These metrics use a Hamming matrix H, a square matrix M × M :</p><formula xml:id="formula_6" coords="5,216.62,439.70,263.98,30.32">H m n = P j=1 1 if ξ j m = ξ j n 0 elsewhere for m ∈ [2, M ] n ∈ [1, m]<label>(5)</label></formula><p>(a) Hamming distances: </p><formula xml:id="formula_7" coords="5,150.38,479.34,215.74,31.83">Ψ µ = M m=2 ξ µ m =0 m n=1 ξ µ n =0 H m n (b)</formula><formula xml:id="formula_8" coords="5,138.97,557.33,319.23,30.07">Ω µ = M i=1 ψ i γ µ i 4. Titles. θ µ = cos M i=1 γ µ i Title γ µ Title</formula><p>Finally, a decision algorithm combines those metrics to score sentences. Two averages are calculated, λ µ &gt; 0.5, and λ µ &lt; 0.5 (λ µ = 0.5 is ignored):</p><formula xml:id="formula_9" coords="5,175.46,626.61,305.13,40.02">µ α = Γ ν=1 λ ν µ &gt;0.5 λ ν µ -0.5 ; µ β = Γ ν=1 λ ν µ &lt;0.5 0.5 -λ ν µ (6)</formula><p>The next expression is used to calculate the score of each sentence:</p><formula xml:id="formula_10" coords="6,205.27,140.09,205.88,57.64">If µ α &gt; µ β then Λ µ = 0.5 + µ α Γ else Λ µ = 0.5 - µ β Γ</formula><p>Cortex sorts nal sentences by using Λ µ ; µ = 1, • • • , P . Additionally, Cortex let us delimit a compression rate, which was xed at 500 words.</p><p>6 Experiments and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Design of Experiments</head><p>We made use of three sizes of training corpora, 100K, 200K, and 500K word tokens, to test our stemmer. With these sizes we performed the three runs for INEX track. The assigned numbers of runs were 153 (100K), 154 (200K), and 155 (500K). The corpus for evaluation was the 1133 contextualized tweets with text from Wikipedia from November 2011. About training corpora, we selected 24 documents from the same contextualized tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>For informativeness, Cortex, coupled with our stemmer, obtained rank 12, 14, and 15. Average scores of informativeness are shown in Table <ref type="table" coords="6,402.03,438.44,3.87,9.96" target="#tab_0">1</ref>. The best run in this evaluation was run 154 (200K). Those scores were computed by organizers using a Perl script (inexqa-eval.pl); for details about this script check <ref type="bibr" coords="6,284.12,606.81,9.95,9.96">[1]</ref>.</p><p>On the other hand, the best results for readability evaluation were obtained by run 155 (500K), see Table <ref type="table" coords="6,263.67,631.54,3.87,9.96" target="#tab_1">2</ref>. Comparing our results with other runs, run 155 (500K) obtained rank 4 in relevance, rank 6 in syntax, and rank 9 in structure. The worst run in our experiment was the run 153 (100K) in both evaluations. In this paper we reported an experiment using a stemmer based on morphological segmentation. We used axality measurements in order to segment words. This stemmer was coupled with Cortex, an automatic summarization system. We suggested the next stemming strategy: given some peaks of axality of a word, we truncated at most left peak. Also, we tested three training corpus sizes to obtain statistical information for the axality indexes: 100K, 200K, and 500K word tokens. Our two goals were to know if our stemming strategy can produce readable summaries, and if dierent sizes of training corpora can improve the Cortex performance.</p><p>According to results of evaluation, our stemming strategy produces not only readable summaries but also competitive ones. That is, from an average of relevance, syntax, and structure (0.6148), run 155 obtained a rank 7 among 27 runs. What is more, concerning informativeness, run 154 obtained rank 12 among 33 participants.</p><p>Regarding corpus sizes, it is not clear what size is the best for English, between 200K and 500K word tokens. However, it is clear that increasing corpus size is a good strategy because 100K obtained the worst results. Additionally, a greater training corpus gives better position in the ranking, for example, from an average of relevance, syntax, and structure, run 155 (500K) obtained rank 7 and run 153 (100K) obtained rank 15.</p><p>In future experiments we will test dierent strategies for morphological segmentation and stemming. Additionally, we can test dierent stemming approaches, such as Porter's stemmer. SF ikD tFrFD witrD wFD ruiD FuFD trvelinD uFX qeX en e'etive nd e0ient stemming lgorithm for informtion retrievlF egw rnsF snfF ystF 29 @PHIIA TF greutzD wFD vgusD uFX nsupervised hisovery of worphemesF snX roF of the orkshop on worphologil nd honologil verning of egvEHPD hildelphiD sqryxEegv @PHHPA PI!QH UF greutzD wFX nsupervised segmenttion of words using prior distriutions of morph length nd frequenyF sn rinrihsD iFD othD hFD edsFX RIst ennul weeting of the egvD pporoD tpnF @PHHQA PVH!PVU VF greutzD wFD vgusD uFX sndution of imple worphology for righlyEsn)eting vngugesF snX roF of Uth weeting of the egv peil snterest qroup in gomE puttionl honology sqryxEegvF @PHHRA RQ!SI WF greutzD wFD vgusD uFX snduing the worphologil vexion of xturl vnE guge from nnnotted extF snX sntF nd snterdisiplinry gonfF on edptive unowledge epresenttion nd esoning @euHSAF @PHHSA IHT!IIQ IHF vovinsD tFfFX hevelopment of temming elgorithmF wehnil rnsltion nd gomputtionl vinguistis 11 @IWTVA PQ!QI IIF orterD wFpFX en lgorithm for u0x trippingF rogrm 14 @IWVHA IQH!IQU IPF vennonD wFD iereD hFD rryD fFD illetD FX en evlution of some on)tion lgorithms for informtion retrievlF tF of snformtion iene 3 @IWVIA IUU!IVQ IQF wjumderD FD witrD wFD lD hFX fulgrinD rungrin nd gzeh stemming using eF snX roeedings of edvnes in wultilingul nd wultimodl snformtion etrievlD pringerEerlgD ferlin @PHHVA RW!ST IRF fhinD wFD perroD xFD weluiD wFX e proilisti model for stemmer genertionF wehnil rnsltion nd gomputtionl vinguistis 41 @PHHSA IPI!IQU ISF wxmeeD FD wy(eldD tFX ghrter nEgrm tokeniztion for iuropen lnguge text retrievlF snformtion etrievl 7 @PHHRA UQ!WU ITF urovetzD FX iewing worphology s n snferene roessF snX roedings of the ITth egwGsgs gonfereneF @IWWQA IWI!PHP IUF rullD hFeFX temming lgorithms E e se study for detiled evlutionF tournl of the emerin oiety for snformtion iene 47 @IWWTA UH!VR IVF wedinErreD eFX snvestigión untittiv de (jos y lítios del espñol de wéxioF qlutinometrí en el gorpus del ispñol wexino gontemporáneoF hh thesisD il golegio de wéxioD wéxio @PHHQA IWF wedinErreD eFX eutomti hisovery of e0xes y mens of gorpusX e gtlog of pnish e0xesF tournl of untittive vinguistis 7 @PHHHA WU!IIR PHF wedinErreD eFD rlvá¢ ováD tFX eutomti eognition of gzeh herivtionl re(xesF snX roeedings of gsgving PHHSF olume QRHTFD vxgD pringerD ferlinGreidelergGxew ork @PHHSA IVW!IWU PIF wedinErreD eFX e0x hisovery sed on intropy nd ionomy wesurementsF exs vinguistis oiety 10 @PHHVA WW!IIP PPF hnnonD gFD everD FX he wthemtil heory of gommunitionF niverE sity of sllinois ressD rn @IWRWA PQF de uokD tFD fossertD FX sntroduión l lingüísti utomáti en ls lengus románisF qredosD wdrid @IWURA PRF qreenergD tFrFX issys in vinguistisF he nivF of ghigo ressD ghigo @IWSUA PSF orresEworenoD tFwFX ésume utomtique de doumentsF vvoisierD ris @PHIIA PTF orresEworenoD tFwFD ggionD rFD d gunhD sFD ntunD iFD elázquezEworlesD FX ummry ivlution with nd without eferenesF oliits 42 @PHIHA IQ!IW PUF ggionD rFD orresEworenoD tFwFD d gunhD sFD ntunD iFX wultilingul summE riztion evlution without humn modelsF snX PQrd sntF gonfF on gomputtionl vinguistisF gyvsxq 9IHD feijingD ghinD egv @PHIHA IHSW!IHTU PVF orresEworenoD tFwFD elzquezEworlezD FD weunierD tFX gyiD un lgorithme pour l ondenstion utomtique de textesF snX egoF olume PF @PHHSA QTS PWF orresEworenoD tFwFD elzquezEworlesD FD weunierD tFX gondensés de textes pr des méthodes numériquesF teh 2 @PHHPA UPQ!UQR QHF orresEworenoD tFwFD tEyngeD FvFD qgnonD wFD ilEfèzeD wFD fellotD FX eutoE mti ummriztion ystem oupled with uestionEenswering ystem @eeAF go abs/0905.2990 @PHHWA</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,168.64,501.21,139.68,9.96;5,308.32,500.30,4.84,6.12;5,316.43,501.87,7.75,8.74;5,337.46,498.91,7.60,6.12;5,337.46,506.90,12.91,6.12;5,352.52,501.87,4.36,8.74;5,357.34,499.13,4.84,6.12;5,356.88,506.69,2.82,6.12;5,151.48,514.53,238.09,9.96;5,389.85,513.62,4.84,6.12;5,398.68,515.19,7.75,8.74;5,420.43,512.22,7.60,6.12;5,423.32,518.43,12.91,6.12;5,420.43,522.95,8.10,8.13;5,424.06,524.96,15.06,8.00;5,441.27,514.53,39.31,10.32;5,168.64,535.30,47.74,10.32;5,229.66,533.00,5.07,6.12;5,229.66,541.00,14.93,6.12;5,246.75,535.97,4.36,8.74;5,251.57,533.22,4.84,6.12;5,251.11,540.78,2.82,6.12;5,150.38,546.81,135.00,9.96;5,286.18,545.91,4.84,6.12;5,294.29,547.48,16.45,8.74;5,310.75,545.91,4.84,6.12;5,316.09,547.48,7.60,8.74;5,323.96,545.91,4.84,6.12;5,151.48,559.63,230.64,9.96"><head></head><label></label><figDesc>Hamming weight of segments: φ µ = M i=1 ξ µ i (c) Sum of Hamming weight of words per segment: Θ µ = heavy weight: Π µ = φ µ Θ µ (e) Sum of Hamming weights of words by frequency:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,220.51,483.96,174.34,73.22"><head>Table 1 .</head><label>1</label><figDesc>everge sores of informtiveness</figDesc><table coords="6,223.61,517.19,168.14,40.00"><row><cell cols="4">Rank Run Unigrams Bigrams Skip</cell></row><row><cell>12</cell><cell>154</cell><cell>0.8233</cell><cell>0.9254 0.9251</cell></row><row><cell cols="2">IR ISS</cell><cell>HFVPSQ</cell><cell>HFWPVH HFWPUR</cell></row><row><cell cols="2">IS ISQ</cell><cell>HFVPTT</cell><cell>HFWPWI HFWPWH</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,134.77,115.11,248.22,122.91"><head>Table 2 .</head><label>2</label><figDesc>ores of redility</figDesc><table coords="7,134.77,148.34,248.22,89.69"><row><cell cols="4">Run Relevance Syntax Structure</cell></row><row><cell>155</cell><cell>0.6968</cell><cell>0.6161</cell><cell>0.5315</cell></row><row><cell>ISR</cell><cell>HFSQSP</cell><cell>HFSQHS</cell><cell>HFRURV</cell></row><row><cell>ISQ</cell><cell>HFRWVR</cell><cell>HFRSUT</cell><cell>HFQUVR</cell></row><row><cell cols="3">7 Conclusions and Future Work</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,645.05,123.92,9.21"><p>httpXGGlinguistiFuhigoFedu</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,656.01,162.15,9.21"><p>httpXGGwwwFisFhutF(GprojetsGmorphoG</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,251.43,568.19,229.16,9.21;7,151.52,579.15,329.07,9.21;7,151.52,590.11,329.07,9.21;7,151.52,601.07,32.25,9.21;7,139.37,612.08,306.31,9.21;7,139.37,623.08,341.22,9.21;7,151.52,634.04,184.35,9.21;7,139.37,645.05,341.22,9.21;7,151.52,656.01,186.09,9.21" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><surname>Wothed</surname></persName>
		</author>
		<title level="m" coord="7,398.26,568.19,82.33,9.21;7,151.52,579.15,329.07,9.21;7,151.52,590.11,301.08,9.21;7,280.97,656.01,56.64,9.21">tFX yverview of the sxi PHII uestion enswering rk @edsxiAF snX sxi PHII orkshop reeEroeedingsD s ulitionsD rofgut smshD rrükenD qermny</title>
		<imprint/>
	</monogr>
	<note>PHHTA QSQ!QUI</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
