<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,209.19,116.95,196.99,12.62;1,198.16,134.89,219.02,12.62">Overview of the INEX 2012 Tweet Contextualization Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,142.71,172.56,58.42,8.74"><forename type="first">Eric</forename><surname>Sanjuan</surname></persName>
							<email>eric.sanjuan@univ-avignon.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">LIA</orgName>
								<orgName type="institution">Université d&apos;Avignon et des Pays de Vaucluse (France)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,211.69,172.56,87.79,8.74"><forename type="first">Véronique</forename><surname>Moriceau</surname></persName>
							<email>moriceau@limsi.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">LIMSI-CNRS</orgName>
								<orgName type="institution" key="instit2">University Paris-Sud (</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.03,172.56,65.36,8.74"><forename type="first">Xavier</forename><surname>Tannier</surname></persName>
							<email>xtannier@limsi.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">LIMSI-CNRS</orgName>
								<orgName type="institution" key="instit2">University Paris-Sud (</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,385.95,172.56,60.08,8.74"><forename type="first">Patrice</forename><surname>Bellot</surname></persName>
							<email>patrice.bellot@lsis.org</email>
							<affiliation key="aff2">
								<orgName type="department">LSIS</orgName>
								<orgName type="institution">Universit Aix-Marseille (</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.94,184.51,63.01,8.74"><forename type="first">Josiane</forename><surname>Mothe</surname></persName>
							<email>josiane.mothe@irit.fr</email>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">IRIT</orgName>
								<orgName type="institution" key="instit2">Universtité de Toulouse (France)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,209.19,116.95,196.99,12.62;1,198.16,134.89,219.02,12.62">Overview of the INEX 2012 Tweet Contextualization Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FED880F459EC931251A7E70B9469C9E8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Automatic Summarization</term>
					<term>Focused Information Retrieval</term>
					<term>XML</term>
					<term>Twitter</term>
					<term>Wikipedia</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The use case of the Tweet Contextualization task is the following: given a new tweet, participating systems must provide some context about the subject of a tweet, in order to help the reader to understand it. In this task, contextualizing tweets consists in answering questions of the form "what is this tweet about?" which can be answered by several sentences or by an aggregation of texts from different documents of the Wikipedia. Thus, tweet analysis, XML/passage retrieval and automatic summarization are combined in order to get closer to real information needs. This article describes the data sets and topics, the metrics used for the evaluation of the systems submissions, as well as the results that they obtained.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Tweet Contextualization task to be performed by the participating groups of INEX 2012 is contextualizing tweets, i.e. answering questions of the form "what is this tweet about?" using a recent cleaned dump of the Wikipedia. The general process involves:</p><p>-Tweet analysis, -Passage and/or XML element retrieval, -Construction of the context/summary. We regard as relevant passages those that both contain relevant information but also contain as little non-relevant information as possible.</p><p>For evaluation purposes, we require that a summary uses only elements or passages previously extracted from the document collection. The correctness of summaries is established exclusively based on the support passages and documents. The summaries are evaluated according to:</p><p>-Informativeness: the way they overlap with relevant passages, -Readability, assessed by evaluators and participants.</p><p>The paper is organized as follows. Section 2 details the collection of tweets and documents. Section 3 presents the metrics and tools used for evaluation, as well as results obtained by the participants. Finally, section 4 draws some preliminary conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Test data</head><p>Organizers provided a document collection extracted form Wikipedia, as well as 1000 topics made of tweets from several different accounts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Tweets</head><p>About 1000 tweets in English were collected by the track organizers from Twitter R Search API. They were selected among informative accounts (for example, @CNN, @TennisTweets, @PeopleMag, @science...), in order to avoid purely personal tweets that could not be contextualized. Information such as the user name, tags or URLs have been provided. These tweets were available in two formats:</p><p>a full JSON format with all tweet metadata. For example: "created at":"Wed, 15 Feb 2012 23:32:22 +0000", "from user":"FOXBroadcasting", "from user id":16537989, "from user id str":"16537989", "from user name":"FOX Broadcasting", "geo":null, "id":169927058904985600, "id str":"169927058904985600", "iso language code":"en", "metadata":"result type":"recent", "profile image url":"http://a0.twimg.com/profile images/...", "profile image url https":"https://si0.twimg.com/profile images/...", "source":"&amp;lt;a href=&amp;quot;http://www.hootsuite.com...", "text":"Tensions are at an all-time high as the @AmericanIdol Hollywood Round continues, Tonight at 8/7c. #Idol", "to user":null, "to user id":null, "to user id str":null, "to user name":null a two-column text format with only tweet id and tweet text. For example: 169927058904985600 "Tensions are at an all-time high as the @AmericanIdol Hollywood Round continues, Tonight at 8/7c. #Idol" 63 of these tweets were selected manually by organizers. For each of them, we checked that the document collection contained some information related to the topic of the tweet. This means that all 63 tweets had some contextualization material inside the provided collection.</p><p>From the accounts used for extraction of these 63 messages, a number of other tweets were automatically selected, bringing to 1000 the total number of tweets to be contextualized by the participants. This is done to ensure that only fully automatic and robust enough systems could accomplish the task.</p><p>However, only the 63 tweets that had been manually collected and checked have been used for informativeness evaluation; only 18 of them have been used for readability evaluation (due to the complexity of this evaluation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document collection</head><p>The document collection has been built based on a recent dump of the English Wikipedia from November 2011. Since we target a plain XML corpus for an easy extraction of plain text answers, we removed all notes and bibliographic references that are difficult to handle and kept only non empty Wikipedia pages (pages having at least one section).</p><p>Resulting documents are made of a title (title), an abstract (a) and sections (s). Each section has a sub-title (h). Abstract and sections are made of paragraphs (p) and each paragraph can have entities (t) that refer to other Wikipedia pages. Therefore the resulting corpus has this simple DTD:</p><formula xml:id="formula_0" coords="3,153.10,510.89,202.42,117.06">&lt;!ELEMENT xml (page)+&gt; &lt;!ELEMENT page (ID, title, a, s*)&gt; &lt;!ELEMENT ID (#PCDATA)&gt; &lt;!ELEMENT title (#PCDATA)&gt;&lt;!ELEMENT a (p+)&gt; &lt;!ELEMENT s (h, p+)&gt; &lt;!ATTLIST s o CDATA #REQUIRED&gt; &lt;!ELEMENT h (#PCDATA)&gt; &lt;!ELEMENT p (#PCDATA | t)*&gt; &lt;!ATTLIST p o CDATA #REQUIRED&gt; &lt;!ELEMENT t (#PCDATA)&gt; &lt;!ATTLIST t e CDATA #IMPLIED&gt;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For example:</head><p>&lt;?xml version="1.0" encoding="utf-8"?&gt; &lt;page&gt; &lt;ID&gt;2001246&lt;/ID&gt; &lt;title&gt;Alvin Langdon Coburn&lt;/title&gt; &lt;s o="1"&gt; &lt;h&gt;Childhood (1882-1899)&lt;/h&gt; &lt;p o="1"&gt;Coburn was born on June 11, 1882, at 134 East Springfield Street in &lt;t&gt;Boston, Massachusetts&lt;/t&gt;, to a middle-class family. His father, who had established the successful firm of Coburn &amp;amp; Whitman Shirts, died when he was seven. [...] &lt;/p&gt; &lt;p o="2"&gt;In 1890 the family visited his maternal uncles in Los Angeles, and they gave him a 4 x 5 Kodak camera. He immediately fell in love with the camera, and within a few years he had developed a remarkable talent for both visual composition and technical proficiency in the &lt;t&gt;darkroom&lt;/t&gt;. (...)&lt;/p&gt; (...) &lt;/page&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Submission format</head><p>Participants could submit up to 3 runs. One run out of the 3 had to be completely automatic: participants had to use only the Wikipedia dump and possibly their own resources (even if the texts of tweets sometimes contain URLs, the Web must not be used as a resource). That is, a participant could not submit more than 3 runs in total.</p><p>A submitted summary has the following format: &lt;tid&gt; Q0 &lt;file&gt; &lt;rank&gt; &lt;rsv&gt; &lt;run_id&gt; &lt;text of passage 1&gt; &lt;tid&gt; Q0 &lt;file&gt; &lt;rank&gt; &lt;rsv&gt; &lt;run_id&gt; &lt;text of passage 2&gt; &lt;tid&gt; Q0 &lt;file&gt; &lt;rank&gt; &lt;rsv&gt; &lt;run_id&gt; &lt;text of passage 3&gt; ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>where:</head><p>-The first column tid is the topic number.</p><p>-The second column is currently unused and should always be Q0. It is just a formating requirement used by the evaluation programs to distinguish between official submitted runs and q-rels. -The third column file is the file name (without .xml) from which a result is retrieved, which is identical to the &lt;id&gt; of the Wikipedia document. It is only used to retrieve the raw text content of the passage, not to compute document retrieval capabilities. In particular, if two results only differ by their document id (because the text is repeated in both), then they will be considered as identical and thus redundant.</p><p>-The fourth column rank indicates the order in which passages should be read for readability evaluation, this differs from the expected informativeness of the passage which is indicated by the score rsv in the fifth column. Therefore, these two columns are not necessarily correlated. Passages with highest scores in the fifth column can be scattered at any rank in the result list for each topic. -The sixth column run id is called the "run tag" and should be a unique identifier for the participant group and for the method used. -The remaining column gives the result passage in raw text without XML tags and without formatting characters. The only requirement is that the resulting word sequence appears at least once in the file indicated in the third field.</p><p>Here is an example of such an output: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>In this task, readability of answers [9] is as important as the informative content. Summaries must be easy to read as well as relevant. Following INEX 2011 Question-Answering task <ref type="bibr" coords="5,244.95,483.64,9.96,8.74" target="#b0">[1]</ref>, these two properties have been evaluated separately by two distinct measures: informativeness and readability. This section describes the metrics and tools used to perform the evaluation and gives results obtained by participating systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline System</head><p>A baseline XML-element retrieval/summarization system has been made available for participants. This baseline is the same as 2011 QA@INEX task, and has been described in <ref type="bibr" coords="5,212.92,600.89,9.96,8.74" target="#b0">[1]</ref>. It relies on the search engine Indri<ref type="foot" coords="5,376.93,599.31,3.97,6.12" target="#foot_0">5</ref> and a fast summarizer algorithm <ref type="bibr" coords="5,179.74,612.84,9.96,8.74" target="#b1">[2]</ref>. The system was available to participants through a web interface <ref type="foot" coords="5,476.12,611.27,3.97,6.12" target="#foot_1">6</ref>or a perl API. Its default output has been added to the pool of submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Submitted Runs</head><p>33 valid runs by 13 teams from 10 countries (Canada, Chile, France, Germany, India, Ireland, Mexico, Russia, Spain, USA) were submitted.</p><p>This year only three teams used the provided perl API and Indri index of the collection.</p><p>The total number of submitted passages is 671,191 (31 596 328 tokens). The median number of distinct passages per tweet is 79.5 and the average is 146.5. Only passages starting and ending by the same 25 characters have been considered as duplicated, therefore short sub-passages could appear twice in longer ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Informativeness Evaluation</head><p>Informativeness evaluation has been performed by organizers on a pool of 63 tweets. For each tweet, we took the 60 best passages based on the rsv score in the fith column of the runs from all participants. After removing duplicates per tweet, 16,754 passages were evaluated by organizers. The median number of passages per tweet is 273 and the average is 265.9. Passages have been merged and displayed to the assessor in alphabetical order. Therefore, each passage informativeness has been evaluated independently from others, even in the same summary. The structure and readability of the summary was not assessed in this specific part, and assessors only had to provide a binary judgement on whether the passage was worth appearing in a summary on the topic, or not. 2,801 passages among 16,754 have been judged as relevant, with a median of 50 passages per tweet and an average of 55.1. The average length of a passage is 30.03 tokens.</p><p>Metrics Systems had to make a selection of the most relevant information, the maximal length of the abstract being fixed. Therefore focused IR systems could just return their top ranked passages meanwhile automatic summarization systems need to be combined with a document IR engine. In this task, readability of answers <ref type="bibr" coords="6,183.05,513.34,10.52,8.74" target="#b2">[3]</ref> is as important as the informative content. Both need to be evaluated. Therefore answers cannot be any passage of the corpus, but at least well formed sentences. As a consequence, informative content of passages cannot be evaluated using standard IR measures since QA and automatic summarization systems do not try to find all relevant passages, but to select those that could provide a comprehensive answer. Several metrics have been defined and experimented with at DUC <ref type="bibr" coords="6,227.86,585.07,10.52,8.74" target="#b3">[4]</ref> and TAC workshops <ref type="bibr" coords="6,330.93,585.07,9.96,8.74" target="#b4">[5]</ref>. Among them, Kullback-Leibler (KL) and Jenssen-Shanon (JS) divergences have been used <ref type="bibr" coords="6,394.12,597.03,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="6,406.30,597.03,7.75,8.74" target="#b6">7]</ref> to evaluate the informativeness of short summaries based on a bunch of highly relevant documents.</p><p>In previous 2010 and 2011 INEX Question Answering tracks, evaluations have been carry out using FRESA package which includes a special lemmatizer. In 2011 we provided the participants with a standalone evaluation toolkit based on Porter stemmer and implementing a new normalized ad-hoc dissimilarity defined as following:</p><formula xml:id="formula_1" coords="7,191.64,149.77,288.95,26.80">Dis(T, S) = t∈T f T (t) f T × 1 - min(log(P ), log(Q)) max(log(P ), log(Q))<label>(1)</label></formula><formula xml:id="formula_2" coords="7,225.91,181.39,254.68,50.19">P = f T (t) f T + 1 (2) Q = f S (t) f S + 1 (<label>3</label></formula><formula xml:id="formula_3" coords="7,476.35,215.10,4.24,8.74">)</formula><p>where T is the set of terms in the reference and for every t ∈ T , f T (t) is its frequency in the reference and f S (t) its frequency in the summary.</p><p>The idea was to have a dissimilarity which complement has similar properties to usual IR Interpolate Precision measures. Actually, 1-Dis(T, S) increases with the Interpolated Precision at 500 tokens where Precision is defined as the number of word n-grams in the reference. The introduction of the log is necessary to deal with highly frequent words.</p><p>As previously announced, we used this software to evaluate informativeness and like in INEX QA tracks, we considered as T three different sets based on Porter stemming:</p><p>-Unigrams made of single lemmas (after removing stop-words).</p><p>-Bigrams made of pairs of consecutive lemmas (in the same sentence).</p><p>-Bigrams with 2-gaps also made of pairs of consecutive lemmas but allowing the insertion between them of a maximum of two lemmas.</p><p>Bigrams with 2-gaps appeared to be the most robust metric. Sentences are not considered as simple bags of words and the measure is less sensitive to sentence segmentation than simple bi-grams. This is why bigrams with 2-gaps is our official ranking metric for informativeness.</p><p>Bigrams with 2-gaps appeared to be the most robust metric in previous INEX QA tracks, however in this edition where topics are real tweets, measures based on bigrams with or without 2-gaps are strongly correlated. Meanwhile the measure based on simple uni-grams is also stable but gives a different ranking. This will be discussed during the CLEF workshop.</p><p>Results Results are presented in Table <ref type="table" coords="7,311.48,540.02,3.87,8.74" target="#tab_1">1</ref>. The 3 top ranked runs improved the baseline. Runs with (*) have been submitted as "manual".</p><p>Dissimilarity values are very closed, however differences are often statistically significant as shown in table 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Readability evaluation</head><p>Human assessment Each participant had to evaluate readability for a pool of summaries of a maximum of 500 words each on an online web interface. Each summary consisted in a set of passages and for each passage, assessors had to tick four kinds of check boxes. The guideline was the following: -Syntax (S): tick the box if the passage contains a syntactic problem (bad segmentation for example), -Anaphora (A): tick the box if the passage contains an unsolved anaphora, -Redundancy (R): tick the box if the passage contains a redundant information, i.e. an information that has already been given in a previous passage, -Trash (T): tick the box if the passage does not make any sense in its context (i.e. after reading the previous passages). These passages must then be considered at trashed, and readability of following passages must be assessed as if these passages were not present.</p><p>-If the summary is so bad that you stop reading the text before the end, tick all trash boxes until the last passage.</p><p>For each summary, the text without tags of the tweet was displayed, thus this year readability was evaluated in the context of the tweet, and passages not related to the tweet could be considered as trash even if there were readable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics and results</head><p>To evaluate summary readability, we consider the number of words (up to 500) in valid passages. We used three metrics based on this:</p><p>-Relevancy or Relaxed metric: a passage is considered as valid if the T box has not been ticked, -Syntax: a passage is considered as valid if the T or S boxes have not been ticked, -Structure or Strict metric: a passage is considered as valid if no box has been ticked.</p><p>In all cases, participant runs are ranked according to the average, normalized number of words in valid passages.</p><p>A total of 594 summaries from 18 tweets have been assessed. The resulting 18 tweets are included in those used for informativeness assessment. Results are presented in Table <ref type="table" coords="9,217.69,483.85,3.87,8.74" target="#tab_4">3</ref>. The last column gives the number of evaluated summaries for correponding run. Only runs that were evaluated on more that 6 summaries, are ranked following the relaxed metric. Missing evaluations were due to formatting problems, too long passages (more than 500 tokens) or missing summaries in the submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In 2011 we experimented using the wikipedia to contextualize twitted New York times paper titles. There was a large overlapping between the two vocabularies. This year we selected a larger pool of public factual tweets with a much more diversified vocabulary. The robust baseline we provided was difficult to outperform on the average. This needs further analysis and will be discussed during the workshop. One reason could be that the baseline approach removes all nonnominals from tweet texts, keeping only nouns and adjectives and this can help   </p><formula xml:id="formula_4" coords="11,176.04,182.46,8.38,14.74">178</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,173.45,195.06,268.46,382.11"><head>Table 1 .</head><label>1</label><figDesc>Informativeness results(official results are "with 2-gap").</figDesc><table coords="8,222.57,195.06,170.22,370.70"><row><cell cols="3">Rank Run unigram bigram with 2-gap</cell></row><row><cell>1</cell><cell>178</cell><cell>0.7734 0.8616 0.8623</cell></row><row><cell>2</cell><cell>152</cell><cell>0.7827 0.8713 0.8748</cell></row><row><cell>3</cell><cell>170*</cell><cell>0.7901 0.8825 0.8848</cell></row><row><cell cols="3">4 Baseline 0.7864 0.8868 0.8887</cell></row><row><cell>5</cell><cell>169</cell><cell>0.7959 0.8881 0.8904</cell></row><row><cell>6</cell><cell>168</cell><cell>0.7972 0.8917 0.8930</cell></row><row><cell>7</cell><cell>193</cell><cell>0.7909 0.8920 0.8938</cell></row><row><cell>8</cell><cell>185</cell><cell>0.8265 0.9129 0.9135</cell></row><row><cell>9</cell><cell>171</cell><cell>0.8380 0.9168 0.9187</cell></row><row><cell>10</cell><cell>186</cell><cell>0.8347 0.9210 0.9208</cell></row><row><cell>11</cell><cell>187</cell><cell>0.8360 0.9235 0.9237</cell></row><row><cell>12</cell><cell>154</cell><cell>0.8233 0.9254 0.9251</cell></row><row><cell>13</cell><cell>162</cell><cell>0.8236 0.9257 0.9254</cell></row><row><cell>14</cell><cell>155</cell><cell>0.8253 0.9280 0.9274</cell></row><row><cell>15</cell><cell>153</cell><cell>0.8266 0.9291 0.9290</cell></row><row><cell>16</cell><cell>196b</cell><cell>0.8484 0.9294 0.9324</cell></row><row><cell>17</cell><cell>196c</cell><cell>0.8513 0.9305 0.9332</cell></row><row><cell>18</cell><cell>196a</cell><cell>0.8502 0.9316 0.9345</cell></row><row><cell>19</cell><cell>164*</cell><cell>0.8249 0.9365 0.9368</cell></row><row><cell>20</cell><cell>197</cell><cell>0.8565 0.9415 0.9441</cell></row><row><cell>21</cell><cell>163</cell><cell>0.8664 0.9628 0.9629</cell></row><row><cell>22</cell><cell>165</cell><cell>0.8818 0.9630 0.9634</cell></row><row><cell>23</cell><cell>150</cell><cell>0.9052 0.9871 0.9868</cell></row><row><cell>24</cell><cell>188</cell><cell>0.9541 0.9882 0.9888</cell></row><row><cell>25</cell><cell>176</cell><cell>0.8684 0.9879 0.9903</cell></row><row><cell>26</cell><cell>149</cell><cell>0.9059 0.9916 0.9916</cell></row><row><cell>27</cell><cell>156</cell><cell>0.9366 0.9913 0.9916</cell></row><row><cell>28</cell><cell>157</cell><cell>0.9715 0.9931 0.9937</cell></row><row><cell>29</cell><cell>191</cell><cell>0.9590 0.9947 0.9947</cell></row><row><cell>30</cell><cell>192</cell><cell>0.9590 0.9947 0.9947</cell></row><row><cell>31</cell><cell>161</cell><cell>0.9757 0.9949 0.9950</cell></row><row><cell>32</cell><cell>177</cell><cell>0.9541 0.9981 0.9984</cell></row><row><cell>33</cell><cell>151</cell><cell>0.9223 0.9985 0.9988</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,134.77,589.57,345.83,18.85"><head>Table 2 .</head><label>2</label><figDesc>Statistical significance for informativeness evaluation (t-test, 1 : 90%, 2 = 95%, 3 = 99%, α = 5%).</figDesc><table coords="12,214.19,195.36,186.97,371.10"><row><cell cols="3">Rank Run Relevancy Syntax Structure Nb</cell></row><row><cell>1</cell><cell>185</cell><cell>0.7728 0.7452 0.6446 17</cell></row><row><cell>2</cell><cell>171</cell><cell>0.6310 0.6060 0.6076 10</cell></row><row><cell>3</cell><cell>168</cell><cell>0.6927 0.6723 0.5721 15</cell></row><row><cell cols="3">4 Baseline 0.6975 0.6342 0.5703 13</cell></row><row><cell>5</cell><cell>186</cell><cell>0.7008 0.6676 0.5636 18</cell></row><row><cell>6</cell><cell>170*</cell><cell>0.6760 0.6529 0.5611 16</cell></row><row><cell>7</cell><cell>165</cell><cell>0.5936 0.6049 0.5442 10</cell></row><row><cell>8</cell><cell>152</cell><cell>0.5966 0.5793 0.5433 16</cell></row><row><cell>9</cell><cell>155</cell><cell>0.6968 0.6161 0.5315 16</cell></row><row><cell>10</cell><cell>178</cell><cell>0.6336 0.6087 0.5289 17</cell></row><row><cell>11</cell><cell>169</cell><cell>0.5369 0.5208 0.5181 16</cell></row><row><cell>12</cell><cell>193</cell><cell>0.6208 0.6115 0.5145 13</cell></row><row><cell>13</cell><cell>163</cell><cell>0.5597 0.5550 0.4983 12</cell></row><row><cell>14</cell><cell>187</cell><cell>0.6093 0.5252 0.4847 18</cell></row><row><cell>15</cell><cell>154</cell><cell>0.5352 0.5305 0.4748 13</cell></row><row><cell>16</cell><cell>196b</cell><cell>0.4964 0.4705 0.4204 16</cell></row><row><cell>17</cell><cell>153</cell><cell>0.4984 0.4576 0.3784 14</cell></row><row><cell>18</cell><cell>164*</cell><cell>0.4759 0.4317 0.3772 15</cell></row><row><cell>19</cell><cell>162</cell><cell>0.4582 0.4335 0.3726 17</cell></row><row><cell>20</cell><cell>197</cell><cell>0.5487 0.4264 0.3477 15</cell></row><row><cell>21</cell><cell>196c</cell><cell>0.4490 0.4203 0.3441 16</cell></row><row><cell>22</cell><cell>196a</cell><cell>0.4911 0.3813 0.3134 15</cell></row><row><cell>23</cell><cell>176</cell><cell>0.2832 0.2623 0.2388 13</cell></row><row><cell>24</cell><cell>156</cell><cell>0.2933 0.2716 0.2278 9</cell></row><row><cell>25</cell><cell>188</cell><cell>0.1542 0.1542 0.1502 11</cell></row><row><cell>26</cell><cell>157</cell><cell>0.1017 0.1045 0.1045 13</cell></row><row><cell>27</cell><cell>161</cell><cell>0.0867 0.0723 0.0584 14</cell></row><row><cell>-</cell><cell>151</cell><cell>0.8728 0.8728 0.8720 5</cell></row><row><cell>-</cell><cell>150</cell><cell>0.8493 0.8493 0.7270 3</cell></row><row><cell>-</cell><cell>192</cell><cell>0.6020 0.6020 0.6020 2</cell></row><row><cell>-</cell><cell>191</cell><cell>0.6173 0.5540 0.5353 3</cell></row><row><cell>-</cell><cell>177</cell><cell>0.5227 0.4680 0.4680 3</cell></row><row><cell>-</cell><cell>149</cell><cell>0.1880 0.0900 0.0900 4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="12,178.71,569.48,257.94,7.89"><head>Table 3 .</head><label>3</label><figDesc>Readability results with the relaxed and strict metric.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0" coords="5,144.73,647.48,131.81,7.47"><p>http://www.lemurproject.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_1" coords="5,144.73,658.44,103.56,7.47"><p>http://qa.termwatch.es</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>in wikipedia search. However, for specific tweets, to retrieve relevant information from the wikipedia, it was necessary to expand the tweet vocabulary or to use tags inside the tweet.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,138.35,199.38,342.24,7.86;10,146.91,210.33,333.68,7.86;10,146.91,221.29,333.68,7.86;10,146.91,232.25,333.68,7.86;10,146.91,243.21,333.68,7.86;10,146.91,254.17,107.03,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,392.20,199.38,88.39,7.86;10,146.91,210.33,183.46,7.86">Overview of the INEX 2011 Question Answering Track (QA@INEX)</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Moriceau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mothe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,182.78,221.29,297.81,7.86;10,146.91,232.25,243.76,7.86">Focused Retrieval of Content and Structure: 10th International Workshop of the Initiative for the Evaluation of XML Retrieval, INEX</title>
		<title level="s" coord="10,146.91,243.21,138.41,7.86">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Geva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Schenkel</surname></persName>
		</editor>
		<meeting><address><addrLine>Saarbrcken, Germany; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2011">2011. 2012</date>
			<biblScope unit="volume">7424</biblScope>
			<biblScope unit="page" from="188" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,265.13,342.24,7.86;10,146.91,276.06,333.68,7.89" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,311.32,265.13,169.27,7.86;10,146.91,276.09,199.60,7.86">The structure and dynamics of cocitation clusters: A multiple-perspective cocitation analysis</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ibekwe-Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,352.87,276.09,31.86,7.86">JASIST</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1386" to="1409" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,287.05,342.24,7.86;10,146.91,298.01,232.62,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,296.48,287.05,184.11,7.86;10,146.91,298.01,124.41,7.86">Automatic evaluation of linguistic quality in multi-document summarization</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,293.91,298.01,21.63,7.86">ACL</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="544" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,308.96,342.24,7.86;10,146.91,319.92,292.29,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,271.15,308.96,209.44,7.86;10,146.91,319.92,64.67,7.86">Evaluating content selection in summarization: The pyramid method</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Passonneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,234.48,319.92,113.01,7.86">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,330.88,342.24,7.86;10,146.91,341.84,284.49,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,192.82,330.88,287.78,7.86;10,146.91,341.84,54.80,7.86">Overview of the TAC 2008 Opinion Question Answering and Summarization Tasks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,224.08,341.84,174.39,7.86">Proc. of the First Text Analysis Conference</title>
		<meeting>of the First Text Analysis Conference</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,352.80,342.24,7.86;10,146.91,363.76,315.50,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,246.75,352.80,233.84,7.86;10,146.91,363.76,29.60,7.86">Performance confidence estimation for automatic summarization</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,198.37,363.76,200.05,7.86">EACL, The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="541" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,374.72,342.24,7.86;10,146.91,385.68,333.68,7.86;10,146.91,396.64,333.68,7.86;10,146.91,407.59,97.27,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,160.93,385.68,248.12,7.86">Multilingual summarization evaluation without human models</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Saggion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Torres-Moreno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Da Cunha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sanjuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Velázquez-Morales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,224.35,396.64,75.81,7.86">COLING (Posters)</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</editor>
		<imprint>
			<publisher>Chinese Information Processing Society of China</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1059" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
