<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.58,115.96,314.20,12.62;1,284.45,133.89,46.46,12.62">Using Collaborative Filtering in Social Book Search</title>
				<funder ref="#_kStQsJ9 #_CCCsR8f #_vnAhnmB">
					<orgName type="full">Netherlands Organization for Scientific Research (NWO</orgName>
				</funder>
				<funder ref="#_bCEHkTp #_Ghtj7aH">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,143.67,171.81,77.51,8.74"><forename type="first">Hugo</forename><surname>Huurdeman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Archives and Information Studies</orgName>
								<orgName type="department" key="dep2">Faculty of Humanities</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,231.74,171.81,54.44,8.74"><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Archives and Information Studies</orgName>
								<orgName type="department" key="dep2">Faculty of Humanities</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">ISLA</orgName>
								<orgName type="department" key="dep2">Faculty of Science</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,303.07,171.81,63.40,8.74"><forename type="first">Marijn</forename><surname>Koolen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Archives and Information Studies</orgName>
								<orgName type="department" key="dep2">Faculty of Humanities</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,396.41,171.81,70.81,8.74"><forename type="first">Justin</forename><surname>Van Wees</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">ILPS</orgName>
								<orgName type="department" key="dep2">Faculty of Science</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.58,115.96,314.20,12.62;1,284.45,133.89,46.46,12.62">Using Collaborative Filtering in Social Book Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">55E1CF6E0AEA9966B3C3E8F30A505780</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe our participation in INEX 2012 in the Social Book Search Track and the Linked Data Track. For the Social Book Search Track we compare the impact of query-and userindependent popularity measures and recommendations based on user profiles. Book suggestions are more than just topical relevance judgements and may include personal factors such as interestingness, fun and familiarity and book-related aspects such as quality and popularity. Our aim is to understand to what extent book suggestions are related to userdependent and -independent aspects of relevance. Our findings are that evidence that is both query-and user-independent is not effective for improving a standard retrieval model using blind feedback. User-dependent evidence, on the contrary, is highly effective, leading to significant improvements. For the Linked Data Track we compare different methods of weighted result aggregation using the DBpedia ontology relations as facets and values. Facets and values are aggregated using either document counts or retrieval scores. The reason to use retrieval scores for facet ranking is that we want the top retrieved results to be summarised by the top ranked facets and values. In addition, we look at the impact of taking overlap in aggregation into account. Facet values that give access to many of the same documents have high overlap. Selecting facet values that have low overlap may avoid frustrating the user.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we describe our participation in the INEX 2012 Social Book Search Track and the Link Data Track. For the Social Book Search Track we compare the impact of query-and user-independent popularity measures against recommendations based on user profiles. The web and social media have changed the way people search for books. The availability of user-reviews, ratings and tags allows users to find out more about a book than from the traditional descriptions made by professional cataloguers. This in turn may evoke more complex information needs from users, relating to issues such as how interesting, familiar or funny, educational, engaging, well-written or popular a book is. Some of these issues are user-independent, such as the popularity of a book and to some extent its quality-in the sense of the general opinion of a whole group readers-and can be derived from data such as the number of people who reviewed, rated or tagged a book. Others are more personal, such interestingness and familiarity, and would require individual user information from user profiles or browsing and purchase history. We combine the user-dependent and -independent evidence with query-dependent evidence from a retrieval system to find out whether book suggestion can benefit from user-dependent evidence.</p><p>For the Linked Data Track (LDT), we experiment with different ways of aggregating results. A standard approach is to rank facets and values using document counts. The facets and values that summarise the most retrieval results are considered the best summarisations. We compare this approach with aggregation based on retrieval scores, which prefers facet values that summarise the highest ranked documents. Assuming most of the relevant documents will be in the top ranks, result aggregation based on retrieval scores will be focused on the most relevant documents. The document collection of the LDT is rich in structure and offers multiple ways of summarising search results. We use the DBpedia ontology relations as facets and values for summarisation.</p><p>We describe our experiments and results for the Social Book Search Track in Section 2 and for the Linked Data Track in Section 3. In Section 4, we discuss our findings and draw conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Social Book Search Track</head><p>The effectiveness of user-generated content on social book search may be partly due to its relation to popularity <ref type="bibr" coords="2,273.78,402.50,9.96,8.74" target="#b2">[3]</ref>. The amount of user-generated content available for individual books is heavily skewed, with popular books have many more tags, reviews and ratings than more obscure books. Much like the impact of document length on traditional ad hoc search <ref type="bibr" coords="2,319.02,438.37,9.96,8.74" target="#b5">[6]</ref>, the longer descriptions of popular books have a higher probability of matching query terms and possibly better term distribution statistics as well, with the result that retrieval models favour them over shorter descriptions of less popular books. This prompts the question whether the forum suggestions are merely the most popular among the topically relevant books, or whether personal preferences of the suggestors and topic creators brings in other aspects of relevance as well. If relevance in social book search is merely a combination of topical relevance and popularity, it would seem that book suggestions are mainly user-independent.</p><p>We want to compare the effectiveness of popularity priors against recommendations based on user profiles. The goal of our experiments is to investigate whether the impact of user-dependent evidence outweighs the available evidence for popularity, which is both query-and user-independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">User-independent Priors</head><p>From the book descriptions in the A/LT collection we can derive several indicators of popularity and quality.</p><p>We look at the following popularity priors:</p><p>-Length: document length. Although document length is not directly related to popularity, we assume that descriptions with many tags and reviews are longer than descriptions with no or few tags and reviews. -Dirichlet: without smoothing, language models favour short documents. Dirichlet smoothing introduces an implicit document length bias <ref type="bibr" coords="3,405.30,166.08,9.96,8.74" target="#b6">[7]</ref>. As smoothing parameter µ increases, document length becomes less important with respect to term frequency. In other words, documents with high term frequency will be favoured over documents with low term frequency regardless of their document lengths. With equal term frequency, a long document will still score lower than a short document, but the difference is small if µ is higher than the length of either document. The advantage of increasing µ over using a document length prior is that it only prefers longer documents when they have a higher frequency of query terms. With a global document length prior, a very long document with few occurrences of query terms still gets a big boost. -NumReviews: the number of reviews. A large number of reviews means a large number of people know the book and voiced their opinion about it. Note that in constructing the A/LT collection, a maximum of 100 reviews per book were included. Books with at least 100 reviews are all considered equally popular even though the real number of reviews would differentiate between them. -SumTag: the sum of all tag frequencies. The tag frequency of a tag tfor a book b is the number of users who assigned t to b. We assume that popular books are tag by more users than more obscure books and therefore have a higher total number of tags. Of course, it is possible for a book to receive many tags from a small group of users, but we expect this to be the exception rather than the rule. Only the 50 most frequent tags of a book are included. The tag frequency is unlimited however, and therefore the total number of tags is also not capped. -MaxTag: the frequency of most popular tag. This avoids the problem of conflating cases where many people assign only a few tags each to a book and cases where few people each assign many tags to a book. If the most frequent tag is assigned by n different users, then at least n users know about this book.</p><p>Next, we define two quality priors:</p><p>-AvgRating: average rating. The arithmetic mean over all Amazon ratings for a work. -BARating: The Bayesian average rating. The Bayesian Average (BA) takes into account how many users have rated a work. As more users rates the same work, the average becomes more reliable and less sensitive to outliers. We make the BA dependent on the query, such that the BA of a book is based on books related to the query. The BA of a book b is computed as:</p><formula xml:id="formula_0" coords="3,261.74,634.44,218.85,34.57">BA(b) = n • m + r∈R(b) r n + n (1)</formula><p>where R(b) is the set of ratings for b m is the average unweighted rating over all books in the top 1000 results and n is the average number of ratings over all the books in the top 1000.</p><p>We crawled a random set of 10,000 books from LibraryThing to obtain popularity information. Each page dedicated to a book contains information on how many members have catalogued it, how popular it is (directly determined by ranking all books by the number of members who catalogued it), how many members have reviewed it and in how many forum discussions it is mentioned (derived from Touchstone mappings).</p><p>We use this set to compare the total number of tags and the frequency of the most frequent tag against the number of members who catalogued it. The correlation between these numbers indicates how well our tag-based priors reflect popularity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Collaborative Filtering</head><p>We want to compare the popularity based measures against methods that take the interests and preferences of the topic creator into account. Specifically, we want to look at collaborative filtering (CF) techniques to exploit the rich data available in the large network of users on LibraryThing. To build a recommender system based on CF, we had to obtain user profiles and personal catalogues of LibraryThing members. We started with a seed list of all the 1,104 users from the topic threads of the 211 topics of the 2011 SB task and crawled their personal catalogues and profiles. Links to other profiles (friends, members with interesting libraries) were extracted to continue the crawl. Because the members who participate in the forums may be different from other members, we also performed crawls based on random sets of 211, 1000 and 10,000 books. In each case, we extracted from each book page on LT the user names who have catalogued that book to generate another seed list. In total, we obtained 89,693 profiles (6% of all profiles) and 5,637,097 book ratings.</p><p>We experiment with neighbourhood-based and model-based recommendations and with rated transactions. Rated transactions indicate that a user catalogued a book and how she rated it. The k nearest neighbours ((k-NN) of a user u, denoted N i (u), are computed using the Pearson correlation of their transaction vectors. The rating r ui of an unseen item i for user u is estimated as:</p><formula xml:id="formula_1" coords="4,263.88,552.52,216.71,48.89">rui = v∈Ni(u) w uv r vi v∈Ni(u) |w uv | (2)</formula><p>where users v are the nearest neighbours who have rated i. For some books, none of the nearest neighbours gave a rating, and k-NN cannot make a rating prediction. In this case, the average of the ratings of all users in our crawl for this book is used. If there is only one user who has rated the book, no reliable average can be obtained and no prediction is made.</p><p>Model-based recommender systems learn a predictive model based on the transactions of a user. The Singular Value Decomposition (SVD) method reduces the domain complexity by reducing the number of dimensions in the item space to a smaller set of underlying dimensions which represent the latent topics and user preferences <ref type="bibr" coords="5,207.19,166.81,9.96,8.74" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Experimental Setup</head><p>We used Indri <ref type="bibr" coords="5,202.34,212.00,10.52,8.74" target="#b7">[8]</ref> for indexing, removed stopwords and stemmed terms using the Krovetz stemmer. Based on the results from the 2011 Social Search for Best Books task <ref type="bibr" coords="5,212.27,235.91,10.52,8.74" target="#b0">[1]</ref> we focus on the social metadata and indexed only usergenerated content-Amazon reviews and LibraryThing tags-and book identification fields: title, author, publisher, publication date, dimensions, weight, and number of pages.</p><p>The topics are taken from the LibraryThing discussion groups and contain a title field which contains the title of a topic thread, a group field which contains the discussion group name and a narrative field which contains the first message from the topic thread. In our experiments we only used the title fields of the topics as queries, which corresponds to the titles of the topic threads of the LT discussion forums. For the language model our baseline has default settings for Indri (Dirichlet smoothing with µ = 2500). We submitted two runs:</p><p>xml social : a standard LM run on the social metadata index. xml social.fb.10.50 : a run on the social metadata index with pseudo relevance feedback using 50 terms from the top 10 results.</p><p>For the priors, each of the scores can be turned into a prior probability by dividing it by the sum of scores of all books in the collection. For instance, the document length prior probability is calculated as P Length (d) = |d|/|D|, where D is the set of all books in the collection and |D| = d∈D |d|. The final document score is then:</p><formula xml:id="formula_2" coords="5,237.63,473.21,242.96,9.65">S Length (d) = P (d|q) • P Length (d)<label>(3)</label></formula><p>With some priors there are problems with zero scores. A book with no reviews would have a prior probability of zero, which would result in a score S N umReviews = 0. To solve this problem, we use the simple smoothing method known as Add-One, which adds one to the number of reviews of each book. The applies to the SumTag, MaxTag, AvgRating and BARating priors. In addition to linear prior probability, we experiment with log priors to compress the score range, thereby reducing the impact of the priors on the ranking. The log SumTag prior is calculated as:</p><formula xml:id="formula_3" coords="5,191.98,590.57,288.61,24.72">P Log(SumT ag) (d) = 1 + Log(1 + SumT ag(d)) d ∈D 1 + Log(1 + SumT ag(d ))<label>(4)</label></formula><p>To rerank the retrieval results with user-dependent evidence from the Collaborative Filtering method, we use a linear combination: </p><formula xml:id="formula_4" coords="5,224.56,653.63,256.03,9.65">S CF (d) = (1 -λ)P Ret (d|q) + λP CF (d)<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Results</head><p>The relevance judgements for the SBS task are based on the book suggestions from the LT fourms, and are mapped to three different relevance values: irrelevant (rv=0) for suggestions made by the topic creator herself, relevant (rv=1) for suggestions by others that the topic creator did not catalogue afterwards, and highly relevant (rv=4) for suggestions that the topic creator catalogued after starting the topic. We refer to the latter as post-catalogued suggestions (PCSs).</p><p>We first discuss the results of the official submissions (Table <ref type="table" coords="6,418.12,295.47,3.87,8.74" target="#tab_0">1</ref>). Differences between the two runs are tested for statistical significance using a one-tailed Bootstrap test with 100,000 resamples, at significance levels of 0.05 ( • ), 0.01 ( • • ) and 0.001 ( • ). The standard run on the xml social index scores 0.331 on MRR, which means the on average, the first relevant document is found at rank 3. In the 2011 SB task, for which similar topics were used but all suggestions were considered equally relevant, a run on the same index scored 0.2913 on nDCG@10, but with this year's judgements it scores only 0.130. Either the topics this year are harder, or the impact of the difference relevance values is big and the system fails to distinguish between the PCSs and the other suggestions. If we map the PCSs to relevance value rv = 1, the nDCG@10 score goes up from 0.130 to 0.171, and if we map all suggestions to rv = 1 (similar to operationalisation used for last year's task), it goes up to 0.224. This means that this year's topics are more difficult, but also that the distinction between PCSs and other suggestions has made the task more difficult.</p><p>The feedback run improves upon the standard run for all four measures, with significant improvements for MRR and P@10. Adding terms from the top 10 documents leads to a better description of the information need. However, the improvement in nDCG@10, which emphasises the suggestions that the topic creator selects to add to her catalogue, is not significant. For our experiments with popularity and quality priors and recommendations we use the feedback run p4.xml social.fb.10.50 as the baseline, which is the highest scoring run of all official submissions on nDCG@10.</p><p>The results are shown in Table <ref type="table" coords="6,281.68,572.43,3.87,8.74" target="#tab_1">2</ref>. We start with the quality priors. The ratings have little impact on performance. All variants are able to improve MRR, but on the other measures the improvements are smaller and not significant. The only exception is the plain Bayesian average prior, which is more effective than the others. This suggests that ratings are mainly useful for improving very early precision. The improvement of the BA Rating prior on nDCG@10 suggests that topic creators take ratings into account when selecting books. However, most improvements are not significant. Perhaps ratings do not reflect quality well, or quality is not effective as user-independent evidence. In the latter case, it might mean that quality is perceived differently by different users.</p><p>Next we discuss the popularity priors. The tag-based priors lead to significant drops in performance when used directly. Curbing their impact by taking the log of the MaxTag or SumTag scores is still not effective. Only the Log(SumTag) prior leads to small but insignificant improvements on MRR and nDCG@10. The number of reviews is more effective. The plain NumReviews prior only improves MRR but hurts performance on the other measures. The compressed score range of the Log(NumReviews) prior is more effective. Performance on all measures improves, with more than 11% improvements for MRR and nDCG@10. The larger improvement for nDCG@10 than for P@10 indicates the reviews are particularly useful for promoting suggestions that the topic creator decides to catalogue. Only the improvement on MRR is significant. This can mean that the number of reviews is a better indicator of popularity than SumTags and MaxTag, or that the topic creator tends to select books for which multiple reviews are available.</p><p>The Length prior is only effective when logged, and only improves performance on MRR and nDCG@10, but not significantly. The implicit length prior of the Dirichlet smoothing parameter µ is more stable, and improves performance on all measures for µ = 10, 000. With higher values for µ, performance starts to drop. Completely ignoring document length and only considering term frequency and document frequency is not good for performance. Even though promoting longer document is effective, it is still important to connect term frequency to the amount of text in a document.</p><p>Although some popularity and quality ratings can improve performance, any improvements on the official measure nDCG@10 are not significant. Evidence that is both user-and query-dependent seems not effective for social book search.</p><p>Finally, we turn to the impact of combining retrieval with recommendation. For the k-NN method we experimented with different neighbourhood sizes (25, 50, and 100 neighbours) and λ values. Typically, the best performance with k-NN is achieved with 20 ≤ k ≤ 50 ( <ref type="bibr" coords="8,304.23,215.08,10.52,8.74" target="#b1">[2]</ref>). We show only the best performing combination, where k = 50 and λ = 0.0001855. For the SVD method, best performance was achieved with 100 dimensions (K=100) and λ = 0.000185). The recommendations from both SVD and k-NN lead to significant improvements on all measures. User-dependent evidence is highly effective for social book search. The k-NN method performs better than the more complex SVD method.</p><p>In sum, evidence based on personal preferences of the user seems much more effective than user-independent evidence based on popularity and quality. The low impact of the quality priors might indicate that quality in book search is more user-dependent. The effectiveness of the number of reviews may be an indicator that popularity can be effective, but also that forum members looking for books only catalogue books for which reviews are available. This is in line with our previous findings that workers on Mechanical Turk, when judging the relevance of books for the same LT forum topics, find it hard to judge books for which no reviews are available Koolen et al. <ref type="bibr" coords="8,349.39,382.68,9.96,8.74" target="#b2">[3]</ref>. With the presence of user reviews, the nature of relevance judgements has become more complex and goes beyond mere topical relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Linked Data Track</head><p>For the Faceted Search Task of the Linked Data Track, systems are required to create a list of both facets and facet values for the explorative search queries contained in the topics of this task. The derived facets should describe relevant information for each of the queries featured in the task, preferably resulting in compact summaries of the available data. Our aim is to experiment with different ways of aggregating results, using either document counts or retrieval scores, and either ignoring or penalising document overlap in the ranking of facet values. The idea behind using retrieval scores for aggregation is that we want to focus on the top ranked results, as the retrieval model ranks documents by relevance, with the most relevant documents in the top of the list. Facet values that summarise many of the top documents give the user easy access to the most relevant documents.</p><p>Of course, the point of aggregation is to summarise long lists of results effectively and efficiently, so focussing on facet values that summarise only the top few documents defies the purpose of result aggregation. Good facet value selection requires a careful balance between high coverage and giving access to the most relevant documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head><p>We use Indri <ref type="bibr" coords="9,194.27,143.11,10.52,8.74" target="#b7">[8]</ref> with Krovetz stemming and default smoothing (Dirichlet with µ = 2500) for indexing. Up to 2000 documents were retrieved using title fields only. We submitted one run for the Ad Hoc Search Task. For the Faceted Search task we were not able to finish any runs in time for the submission deadline.</p><p>The Ad Hoc run is used as the basis for carrying out the Faceted Search Task. We explored possibilities to extract different facets and facet values from the data available in the Wikipedia-LOD collection of the Linked Data Track. The candidate facets consist of the DBpedia relations and properties for each Wikipedia article included in the collection. For our exploration, we are also using additional ontological data available from DBpedia itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Facet selection</head><p>As a basic approach to performing the selection of facets, we used the concept of 'facet coverage' <ref type="bibr" coords="9,215.49,321.68,9.96,8.74" target="#b0">[1]</ref>. This refers to the number of documents that are summarized by a facets top n values. The aim is to provide compact summaries of the available data using the selected facets, so these facets ideally should cover a high number of documents.</p><p>Using the ontology relations of DBpedia, we generated a list of all possible facets for a topic from the available DBpedia properties contained in each Wikipedia-LOD article in the collection. The list of facets includes the top 5 values for each facet, based on the number of documents a value covers, and the top 5 values based on their retrieval scores (originating from the baseline run created using Indri). To select a number of top facets out of the list of all facets for a given query, we are using different methods. One way to select the facets is based on the facet coverage. A disadvantage of this method, however, is that this does not take the overlap between facets into account. Therefore, a second method has been used, coverageNO, that focuses on the number of unique documents summarized by the facets top n values (see also <ref type="bibr" coords="9,427.63,489.89,10.30,8.74" target="#b0">[1]</ref>).</p><p>Based on a recursive selection method, it is possible to create a hierarchical list of facets and facet values. There are some issues with the available data from DBpedia, which influenced the facet selections that we explored in our research. First of all, there is a wide range of properties that are used for DBpedia entities, but not all of them are applied consistently. Furthermore, a substantial number of the top-ranked results from our baseline run do not have DBpedia properties, except for links to other pages, and therefore are not included in the generated facets. Finally, some of the entities have incorrect properties, possibly due to the semi-automatically generated structure of DBpedia, that is based on the userauthored data of Wikipedia. To overcome these limitations, we are also exploring ways to include additional data from DBpedia in the process of selecting facets, for example the ontological structure of DBpedia. <ref type="foot" coords="9,351.31,632.62,3.97,6.12" target="#foot_0">1</ref>In this paper we discussed our participation in the INEX 2012 Social Book Search Track and the Linked Data Track.</p><p>For the Social Book Search Track, we experimented with user-dependent and user-independent evidence in the form of document priors-length, book ratings, and numbers of tags and reviews-and user-dependent evidence in the form of recommendations from collaborative filtering approaches. We crawled a large set of user profiles and personal catalogues of LibraryThing members and experimented with neighbourhood-based and model-based recommender systems.</p><p>We found that document priors reflecting quality and popularity do not improve performance of a standard language model with blind feedback. The number of reviews of a book is the most effective prior, but does not lead to significant improvements. It is not clear whether the number of reviews is effective because it reflects popularity or because it promotes books for which the searcher can read multiple reviews and therefore make a more informed selection. Our findings suggest that evidence that is both query-and user-independent is not effective for social book search.</p><p>In contrast, user-dependent information from recommender systems is highly effective. Both k-nearest neighbour and SVD approaches lead to significant improvements. Although the k-NN method is less complex than SVD, it is the more effective of the two. Our findings suggest that user-dependent evidence is more important than user-independent information.</p><p>For the Linked Data Track, our aims are to compare the effectiveness of different result aggregation approach and of ignoring or penalising overlap in the results summarised by the chosen values of a selected facet. We are still implementing this model and the relevance judgements are not yet available, so we have no evaluation results yet.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,134.77,127.36,345.83,53.81"><head>Table 1 :</head><label>1</label><figDesc>Evaluation results for the official Social Book Search task runs. Significance levels are 0.05 ( • ), 0.01 ( • • ) and 0.001 ( • ).</figDesc><table coords="6,150.08,154.11,315.19,27.06"><row><cell>Run</cell><cell>MRR</cell><cell>nDCG@10</cell><cell>P@10</cell><cell>R@10</cell></row><row><cell>p4.xml social</cell><cell>0.331</cell><cell>0.130</cell><cell>0.125</cell><cell>0.139</cell></row><row><cell>p4.xml social.fb.10.50</cell><cell>0.370 • 11.8%</cell><cell>0.146 11.8%</cell><cell>0.138 • 10.3%</cell><cell>0.142 2.2%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,134.77,127.36,350.76,245.09"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results for the Social Book Search task runs. Significance levels are 0.05 ( • ), 0.01 ( • • ) and 0.001 ( • ).</figDesc><table coords="7,136.16,154.11,349.37,218.34"><row><cell>Run</cell><cell>MRR</cell><cell>nDCG@10</cell><cell>P@10</cell><cell>R@10</cell></row><row><cell>Baseline</cell><cell>0.362</cell><cell>0.144</cell><cell>0.122</cell><cell>0.149</cell></row><row><cell>Quality priors</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AvgRating</cell><cell>0.377 4.3%</cell><cell>0.143 -0.6%</cell><cell>0.122 0.0%</cell><cell>0.153 2.6%</cell></row><row><cell>Log(AvgRating)</cell><cell>0.373 3.2%</cell><cell>0.143 -0.2%</cell><cell>0.125 2.5%</cell><cell>0.152 1.9%</cell></row><row><cell>BA Rating</cell><cell>0.379 4.8%</cell><cell>0.151 5.1%</cell><cell>0.126 3.4%</cell><cell>0.158 5.6%</cell></row><row><cell>Log(BA Rating)</cell><cell>0.374 3.6%</cell><cell>0.142 -1.3%</cell><cell>0.124 1.7%</cell><cell>0.151 0.9%</cell></row><row><cell>Popularity priors</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MaxTag</cell><cell>0.290 • • -19.7%</cell><cell>0.082 • -43.3%</cell><cell>0.085 • -29.9%</cell><cell>0.093 • -38.0%</cell></row><row><cell>Log(MaxTag)</cell><cell>0.357 -1.2%</cell><cell>0.137 -4.4%</cell><cell>0.116 -5.2%</cell><cell>0.143 -4.4%</cell></row><row><cell>SumTags</cell><cell>0.274 • • -24.3%</cell><cell>0.080 • -44.1%</cell><cell>0.087 • -28.2%</cell><cell>0.089 • -40.6%</cell></row><row><cell>Log(SumTags)</cell><cell>0.371 2.6%</cell><cell>0.145 0.7%</cell><cell>0.119 -2.6%</cell><cell>0.149 -0.1%</cell></row><row><cell>NumReviews</cell><cell>0.370 2.4%</cell><cell>0.129 -10.5%</cell><cell>0.110 -9.4%</cell><cell>0.129 -13.9%</cell></row><row><cell>Log(NumReviews)</cell><cell>0.403 • 11.4%</cell><cell>0.161 12.1%</cell><cell>0.130 6.8%</cell><cell>0.158 5.9%</cell></row><row><cell>Length priors</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Length</cell><cell>0.357 -1.2%</cell><cell>0.126 -12.4%</cell><cell>0.112 -8.5%</cell><cell>0.137 -8.4%</cell></row><row><cell>Log(Length)</cell><cell>0.379 4.8%</cell><cell>0.149 4.0%</cell><cell>0.121 -0.9%</cell><cell>0.149 -0.3%</cell></row><row><cell>Dirichlet µ = 5000</cell><cell>0.357 -1.2%</cell><cell>0.150 4.2%</cell><cell>0.128 5.1%</cell><cell>0.160 7.2%</cell></row><row><cell>Dirichlet µ = 10000</cell><cell>0.371 2.7%</cell><cell>0.153 6.7%</cell><cell>0.128 5.1%</cell><cell>0.160 7.2%</cell></row><row><cell>Dirichlet µ = 15000</cell><cell>0.355 -1.9%</cell><cell>0.151 5.4%</cell><cell>0.124 1.7%</cell><cell>0.150 0.4%</cell></row><row><cell>Recommendation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>k-NN (N=50, λ=0.0001855)</cell><cell>0.411 • 13.6%</cell><cell>0.181 • 26.0%</cell><cell>0.154 • 26.5%</cell><cell>0.199 • 32.9%</cell></row><row><cell>SVD (K=100, λ=0.000185)</cell><cell>0.403 • • 11.3%</cell><cell>0.172 • 19.8%</cell><cell>0.149 • 22.2%</cell><cell>0.187 • 24.9%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="9,144.73,656.80,269.87,8.12"><p>URL: http://mappings.dbpedia.org/server/ontology/classes/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments This research was supported by the <rs type="funder">Netherlands Organization for Scientific Research (NWO</rs> projects # <rs type="grantNumber">612.066.513</rs>, <rs type="grantNumber">639.072.601</rs>, and <rs type="grantNumber">640.005.001</rs>) and by the <rs type="programName">European Communitys Seventh Framework Program</rs> (<rs type="grantNumber">FP7 2007/2013</rs>, Grant Agreement <rs type="grantNumber">270404</rs>).</p></div>
<div><head>Bibliography</head></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_kStQsJ9">
					<idno type="grant-number">612.066.513</idno>
				</org>
				<org type="funding" xml:id="_CCCsR8f">
					<idno type="grant-number">639.072.601</idno>
				</org>
				<org type="funding" xml:id="_vnAhnmB">
					<idno type="grant-number">640.005.001</idno>
					<orgName type="program" subtype="full">European Communitys Seventh Framework Program</orgName>
				</org>
				<org type="funding" xml:id="_bCEHkTp">
					<idno type="grant-number">FP7 2007/2013</idno>
				</org>
				<org type="funding" xml:id="_Ghtj7aH">
					<idno type="grant-number">270404</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,150.26,571.80,330.33,8.74;10,150.26,583.76,330.32,8.74;10,150.26,595.71,330.33,8.74;10,150.26,607.67,330.33,8.74;10,150.26,619.62,330.33,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,329.69,571.80,150.90,8.74;10,150.26,583.76,311.56,8.74">The importance of document ranking and user-generated content for faceted search and book suggestions</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Andriaans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,353.52,595.71,127.07,8.74;10,150.26,607.67,330.33,8.74;10,150.26,619.62,156.15,8.74">Focused Retrieval of Content and Structure: 10th International Workshop of the Initiative for the Evaluation of XML Retrieval (INEX 2011)</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Geva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Schenkel</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7424</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,150.26,632.21,330.33,8.74;10,150.26,644.16,330.33,8.74;10,150.26,656.12,80.81,8.74" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="10,293.37,632.21,187.22,8.74;10,150.26,644.16,139.25,8.74">A comprehensive survey of neighborhoodbased recommendation methods</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<editor>Ricci et al.</editor>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="107" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,150.26,118.99,330.33,8.74;11,150.26,130.95,330.33,8.74;11,150.26,142.90,330.33,8.74;11,150.26,154.86,118.04,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,310.26,118.99,170.33,8.74;11,150.26,130.95,258.45,8.74">Social Book Search: The Impact of Professional and User-Generated Content on Book Suggestions</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,430.93,130.95,49.65,8.74;11,150.26,142.90,330.33,8.74;11,150.26,154.86,57.88,8.74">Proceedings of the International Conference on Information and Knowledge Management (CIKM 2012)</title>
		<meeting>the International Conference on Information and Knowledge Management (CIKM 2012)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,150.26,166.81,330.33,8.74;11,150.26,178.77,193.63,8.74" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="11,265.53,166.81,147.65,8.74">Advances in collaborative filtering</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Bell</surname></persName>
		</author>
		<editor>Ricci et al.</editor>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="145" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,150.26,190.72,330.33,8.74;11,150.26,202.68,264.47,8.74" xml:id="b4">
	<monogr>
		<title level="m" coord="11,420.24,190.72,60.35,8.74;11,150.26,202.68,77.56,8.74">Recommender Systems Handbook</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Ricci</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Rokach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Shapira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Kantor</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,150.26,214.64,330.32,8.74;11,150.26,226.59,330.33,9.30;11,150.26,238.55,330.33,9.30;11,150.26,250.50,330.33,8.74;11,150.26,262.46,189.59,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,330.67,214.64,149.92,8.74;11,150.26,226.59,28.09,8.74">Pivoted document length normalization</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<idno type="DOI">10.1145/243199.243206</idno>
		<ptr target="http://doi.acm.org/10.1145/243199.243206" />
	</analytic>
	<monogr>
		<title level="m" coord="11,204.70,226.59,275.89,9.30;11,150.26,238.55,326.49,9.30">sigir &apos;96: Proceedings of the 19th annual international ACM sigir conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,150.26,274.41,330.33,8.74;11,150.26,286.37,207.93,8.74" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="11,281.31,274.41,199.28,8.74;11,150.26,286.37,98.91,8.74">An investigation of dirichlet prior smoothings performance advantage</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="11,150.26,298.32,330.33,8.74;11,150.26,310.28,330.33,8.74;11,150.26,322.23,178.65,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,379.20,298.32,101.39,8.74;11,150.26,310.28,171.55,8.74">Indri: a language-model based search engine for complex queries</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,342.50,310.28,138.09,8.74;11,150.26,322.23,148.29,8.74">Proceedings of the International Conference on Intelligent Analysis</title>
		<meeting>the International Conference on Intelligent Analysis</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
