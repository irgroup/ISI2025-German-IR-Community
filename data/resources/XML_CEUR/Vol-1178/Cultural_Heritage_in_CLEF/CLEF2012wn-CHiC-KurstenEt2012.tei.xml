<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,164.22,152.87,266.66,12.58;1,156.96,174.62,281.33,10.80">Chemnitz at the CHiC Evaluation Lab 2012</title>
				<funder ref="#_Z7BHCFF">
					<orgName type="full">German Federal Ministry of Education and Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,158.88,212.49,49.67,8.74"><forename type="first">Jens</forename><surname>Kürsten</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,215.30,212.49,67.52,8.74"><forename type="first">Thomas</forename><surname>Wilhelm</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,290.05,212.49,57.20,8.74"><forename type="first">Daniel</forename><surname>Richter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,370.71,212.49,65.82,8.74"><forename type="first">Maximilian</forename><surname>Eibl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,164.22,152.87,266.66,12.58;1,156.96,174.62,281.33,10.80">Chemnitz at the CHiC Evaluation Lab 2012</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B5E4DEE95AC4B5C96CD2E5DC0CF50855</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Ad-hoc Retrieval</term>
					<term>Semantic Enrichment</term>
					<term>Cultural Heritage ─ Ad-hoc Retrieval Task</term>
					<term>─ Variability Task</term>
					<term>─ Semantic Enrichment Task</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cultural heritage is one of the most valuable resources that describe the creative power of mankind. In this article we describe a total number of 96 experiments that have been submitted as contributions to the three subtasks of the Cultural Heritage in CLEF pilot evaluation lab. At the core of the majority of these experiments lies a prototype implementation for semantic enrichment based on DBpedia. The evaluation of the experiments demonstrate that semantic enrichment does not improve retrieval effectiveness in comparison to straightforward baseline experiments. The results also indicate that automatic query expansion does not improve retrieval performance for the pilot lab test collection. Further experiments are needed in order to be able to draw conclusions on whether semantic enrichment can improve retrieval results on cultural heritage collections or not.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cultural heritage is one of the most valuable resources that describe and document human creative power. Nowadays, many different types of organisations, such as libraries, museums, and audiovisual archives, own specific collections which provide an insight into contemporary history. <ref type="bibr" coords="1,277.19,539.56,11.71,8.74" target="#b0">[1]</ref> Web portals like Europeana 1 aim to provide access to a wide range of cultural heritage collections, but a variety of challenges like different types of documents (namely text, image, audio, and video), different metadata description schemes, or different languages contribute to the complexity of the underlying retrieval system. In order to provide the user with the information that is most valuable to her or him, the Cultural Heritage in CLEF (CHiC) pilot evaluation lab <ref type="bibr" coords="1,139.43,611.57,11.68,8.74" target="#b1">[2]</ref> addresses these key problems by means of three types of evaluation tasks:</p><p>This contribution describes the system and the resources that have been used to tackle all of the CHiC tasks. It continues with the description of a semantic enrichment module. Then it provides an overview on the experimental set-up that was employed to approach the individual subtasks. A summary of all submitted experiments is provided subsequently. They are presented together with an analysis of the obtained results and further experiments. The final section of this article provides a review of the most important observations and resulting directions for future work on the topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>The experiments for the CHiC evaluation lab set an important milestone for the Chemnitz retrieval group: the Xtrieval framework <ref type="bibr" coords="2,327.54,291.57,11.72,8.74" target="#b2">[3]</ref> has been used for five years and a variety of retrieval tasks in the context of CLEF (see <ref type="bibr" coords="2,357.56,303.57,11.64,8.74" target="#b3">[4]</ref> for an overview of past results). Naturally, in order to design and implement the experiments for the Ad-hoc and Variability tasks the Xtrieval framework was used again. An additional module has been developed to create contextual expansions for the semantic enrichment task. Figure <ref type="figure" coords="2,154.75,351.59,5.01,8.74" target="#fig_0">1</ref> illustrates the system architecture and the resources that were used in the experiments. The following resources were used to prepare and to conduct the retrieval experiments:</p><p>─ Apache Lucene<ref type="foot" coords="3,198.96,180.37,3.24,5.65" target="#foot_0">2</ref> in version 3.6 as the core retrieval engine, ─ The Snowball project <ref type="foot" coords="3,222.60,192.37,3.24,5.65" target="#foot_1">3</ref> for stop word removal and stemming, ─ Language detection <ref type="foot" coords="3,215.10,204.37,3.24,5.65" target="#foot_2">4</ref> to analyse the language distribution and validity of tags, ─ Microsoft Translator <ref type="foot" coords="3,219.60,216.37,3.24,5.65" target="#foot_3">5</ref> to generate queries in collection-specific languages, ─ DBpedia<ref type="foot" coords="3,172.08,228.37,3.24,5.65" target="#foot_4">6</ref> to extract enrichment terms for short topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Xtrieval Extension for Semantic Enrichment</head><p>A specific problem in the domain of web search and cultural heritage web portals are very short queries. Retrieving documents for such queries is very difficult due to the lack of context information. One approach to address the issue is to develop sophisticated algorithms for automatic or semi-automatic semantic enrichment. For the experiments presented hereafter, a term-based enrichment module has been developed as extension for Xtrieval. This module aims to return a set of entities containing broader or more specific terms for a given query or concept. In its first prototype implementation it provides access to DBpedia resources. DBpedia can be "considered [as] the Semantic Web mirror of Wikipedia". <ref type="foot" coords="3,312.66,381.37,3.24,5.65" target="#foot_5">7</ref> It allows the extraction of factual information from Wikipedia pages as well as the connections between pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. Abstract representation of the employed semantic enrichment</head><p>Specific features that make DBpedia attractive for the problem at hand are:</p><p>─ Inter-language links, which can be used for translation, ─ Disambiguation links, which help to direct a query to specific topics, ─ Relationship extraction, which provides conceptually related entities or terms, ─ Redirects, which allow to guess a related entity based on a given (set of) term(s), ─ Entity-specific information like geo-coordinates, location names, or person data, ─ Links to specific resources for term-based expansion such as article categories, category labels, general labels, or article abstracts.</p><p>Although the area of Semantic Web research created the foundations for the design and development of the semantic enrichment module for Xtrieval, the focus of this contribution lies in the experiments to approach the tasks of the CHiC evaluation lab. More information on Semantic Web technology and research as well as the details on the architecture and use cases of DBpedia can be found in <ref type="bibr" coords="4,358.74,310.57,10.61,8.74" target="#b4">[5]</ref>, <ref type="bibr" coords="4,375.42,310.57,10.61,8.74" target="#b5">[6]</ref>, and <ref type="bibr" coords="4,409.05,310.57,10.61,8.74" target="#b6">[7]</ref>.</p><p>An easy integration into Xtrieval was one particular requirement of the semantic enrichment extension (see Figure <ref type="figure" coords="4,256.04,334.57,3.61,8.74">2</ref>). For this reason the semantic enrichment module (SEM) has to be developed in Java <ref type="foot" coords="4,266.70,344.35,3.24,5.65" target="#foot_6">8</ref> . As the DBpedia provides web-based application programming interfaces, the extension is implemented on top of the Apache HttpComponents library <ref type="foot" coords="4,223.14,368.35,3.24,5.65" target="#foot_7">9</ref> . Three types of interfaces to DBpedia are accessed in order to obtain semantic enrichments:</p><p>─ The SPARQL endpoint <ref type="foot" coords="4,230.40,400.33,6.48,5.65" target="#foot_8">10</ref> , ─ The DBpedia lookup service <ref type="foot" coords="4,251.22,412.39,6.48,5.65" target="#foot_9">11</ref> , ─ The DBpedia named pages graph <ref type="foot" coords="4,269.82,424.39,6.48,5.65" target="#foot_10">12</ref> .</p><p>These interfaces are used for entity discovery (lookup service), verification (named pages graph), and expansion (SPARQL). To avoid the repeated fetching of identical resources the DBpedia Connector (see Figure <ref type="figure" coords="4,318.08,470.55,3.76,8.74">2</ref>, right) implements an HTTP client that supports local caching. Out of the four components of the SEM, only two (namely Entity Lookup and Entity Expansion) are employed to query the DBpedia resources.</p><p>The semantic enrichment process works as follows. In a first step the Term Lemmatiser transforms the given terms (i.e. the topic titles), which may or may not represent one or more named entities. This procedure includes the following steps:</p><p>─ Stop word removal (restricted to the first or last terms), ─ N-gram analysis for each individual term for a later comparison of similarity with discovered named entities, ─ Term order alternation to discover "hidden" named entities.</p><p>The resulting terms are treated as a stream of tokens and transferred to the Entity Lookup component. Here, the actual lookup via DBpedia is performed repeatedly for all possible combinations of terms and term orders, starting with the complete stream of tokens. This process is continued by removing individual terms until only a list of individual terms remains. These individual terms are also treated as entity candidates and checked via Entity Lookup. In case of a successful lookup, the level of the process determines the further course of action. The longer the stream of tokens that matched a DBpedia entity the more valuable this entity will be. For this reason the ratio of the length of the matching stream of tokens by the original length indicates the quality of the discovered entity. Since the value of semantic enrichment based on entities found at the individual term level might be very small, the Entity Expansion component is used to exploit the links and descriptive meta-data between such entities. This might be useful for queries like "Ulysses by Joyce", where Ulysses and Joyce will return a number of potential DBpedia entities, but each individual term alone does not yet allow drawing the conclusion to the actual concept. However, the missing link is contained in the connection between the two DBpedia entities "Ulysses (book)" and "James Joyce". For this reason the Entity Expansion component aims to resolve this relationship by exploring the links between DBpedia entities that were found for individual terms. Another main task of this component is to extract content descriptions from known DBpedia entities. This is the basis for the final step of the SEM that takes places in the Term Selection component. It receives a list of terms, which had been extracted from DBpedia entities, and it creates a weighted list of term candidates for the reformulation of the query. In the first prototype implementation used for the experiments, this procedure was treated as an automatic query expansion process. This allowed the use of standard query expansion algorithms, but the corresponding figures like local and global term counts or document frequency had to be obtained from the indices. Due to the flexibility of Xtrieval this could be implemented in a straightforward way. CSCorrect<ref type="foot" coords="5,167.40,484.39,6.48,5.65" target="#foot_11">13</ref> from the Terrier retrieval toolkit <ref type="bibr" coords="5,312.96,486.57,11.70,8.74" target="#b7">[8]</ref> was used as query expansion algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Set-up and Results</head><p>As the CHiC evaluation lab in 2012 was a pilot retrieval task, the first step was to analyse the structure of the document collection in order to create efficient and meaningful index structures. For mid-sized test corpora like the one at hand, previous experiments have shown that a manual selection of document content can help to reduce the noise in index structures, which results in better retrieval performance <ref type="bibr" coords="5,421.79,603.58,10.65,8.74" target="#b3">[4]</ref>.</p><p>In its latest version the Xtrieval framework has a very flexible and fast Data Collection Processor (see Figure <ref type="figure" coords="5,236.71,627.59,4.18,8.74" target="#fig_0">1</ref>) implementation that is based on the Jaxen library 14 . It exclusively relies on XPath for selecting the content from documents and determining the fields in the index. Each index that was created based on the mappings listed in Table <ref type="table" coords="6,149.99,162.57,3.77,8.74" target="#tab_0">1</ref>. Note that the base path to the root of each individual document has been omitted for better clarity. Table <ref type="table" coords="6,257.97,174.57,5.01,8.74" target="#tab_0">1</ref> also shows that most of the original content from the document collection was used for indexing.</p><p>The language-specific sub-collections for English, German, and French were indexed once for each language in order to conduct the experiments for the Mono-and Bilingual subtasks. A specific problem that was found in the entire document collection was that four types of language tags are used to indicate the language of the document descriptions. What made matters worse was the fact that for some documents these tags may indicate different languages for an individual document. In some other cases the indicated language for the document language was in fact wrong. This was problematic for our approach to apply language-specific content analysers for each individual document in the multi-lingual collection. For this reason an additional filter algorithm was implemented. It evaluates six available sources of evidence based on the following priority:</p><p>1. ims:language 2. europeana:country 3. europeana:language 4. dc:language 5. europeana:isShownAt 6. europeana:isShownBy Although all of these tags may not be present in each document, the algorithm compares the content of the existing tags. In case of a mismatch the language is compared with the language obtained by treating the country code top level domain in the URI of 5. and 6. as an ISO 3166-2 language code. If this language does not match any of the previous languages the document content is fed to a language detection library (see Section 2) in order to obtain the actual language of the document. applied were subsequently. Stemming filters for each language were applied if available. This was the case for the following languages: German, Swedish, French, Norwegian, Italian, Spanish, English, Dutch, Finnish, Estonian, Hungarian, Russian, Portuguese, Turkish, Romanian, Polish, Greek, Bulgarian, Czech, Slovak, Slovenian, and Danish. The token stream processing was implemented as follows:</p><p>1. LowerCaseFilter -converts the token to lower case. 2. RemoveShortWordsFilter* -removes words shorter than 3 characters.</p><p>3. StopFilter -removes stop words depending on the language. 4. SnowballFilter -stems the token according to the document language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ad-hoc Retrieval Task</head><p>The aim of our experiments that were submitted to the Ad-hoc Retrieval Task was to compare the implemented approach to semantic enrichment with an automatic pseudo-relevance feedback (qe_kl) and a baseline (base) run. The restriction that only four experiments could be submitted for each of the sub-tasks allowed two different configurations for SEM (qe_dbp_abs and qe_dbp_sub). For this reason only the source of the expansion terms of the DBpedia entities was alternated:</p><p>http://dbpedia.org/ontology/abstract http://purl.org/dc/terms/subject</p><p>Each of these two resources corresponds to specific content of the original Wikipedia page for a given DBpedia entity. The abstract contains a natural language description of the DBpedia entity and is available in different languages. This allows automatic translations based on the DBpedia abstracts. The subject description refers to a number of Wikipedia category pages that are identified by their corresponding concept label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monolingual Experiments</head><p>Four experiments were submitted to each of the monolingual sub-tasks on the English, German, and French collections. All experiments were based on a very simple retrieval algorithm that submits the created queries to the "content" field only (see Table <ref type="table" coords="7,149.99,554.63,3.63,8.74" target="#tab_0">1</ref>). The obtained experimental results are listed in Table <ref type="table" coords="7,376.02,554.63,3.74,8.74" target="#tab_2">2</ref>.</p><p>The results for the monolingual experiments demonstrate that the experiments based on the SEM have been clearly outperformed by the baseline runs on the English and German sub-collections. Only for the French sub-collection there is very little variance across the tested system configurations. Another observation is that automatic feedback also decreased retrieval performance when querying in the English and German languages. Regarding the two sources of expansion terms from DBpedia entities no clear conclusion can be drawn from the experiments in German and French. On the English test collection, however, extracting expansion terms from subject descriptions clearly outperformed the ones extracted from abstract descriptions of the DBpedia entities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bilingual Experiments</head><p>Eight runs were conducted for each of the three sub-collections in English, German, and French. A subset of experiments needed to be held back in order to comply with the restriction to four experiments per sub-task. To account for the translation problem there was a slight modification to the monolingual experiment set-up.</p><p>The baseline experiment (base) did not contain any kind of translation mechanism. Microsoft's translation service (see Section 2) was used to translate the queries into the collection language for a second experiment (ms). The third experiment (qe_dbp_abs) was modified in a way that the DBpedia expansion returned the abstracts in the required collection language. The final experiment (qe_dbp_sub_ms) took the returned subject contents from DBpedia as input and translated these with Microsoft's translation service. Table <ref type="table" coords="8,278.89,476.11,5.01,8.74" target="#tab_3">3</ref> lists the results for all experiments (including those that could not be submitted). Note that the official experiments are marked with a star (*).</p><p>Our experiments demonstrate that submitting queries in languages other than the actual language of the collection results in poor retrieval performance. In contrast to that using a translation service to translate the queries to the target language yielded substantial improvements over this baseline. For the German and French collections this experimental set-up performed better than any other configuration discussed in this contribution. Similar to the findings from the monolingual sub-task the semantic enrichment did not help to improve performance in general. For the English collection using terms extracted from subject descriptions of DBpedia entities did result in the best performance, but on the German and French collections this could not be confirmed. Another aspect that influences the retrieval performance in the bilingual retrieval scenario might be the source language of the topics. For the English collection French topics should be preferred over German ones and for the German collection English topics should be preferred over French ones. This effect is not present on the French collection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multilingual Experiments</head><p>For the multilingual sub-task 18 experiments were conducted in total. Again, only four of these runs could be submitted for evaluation. In fact, three different system configurations were compared, but each of these were tested using the English, German, and French topics. Here our baseline experiment (base_ms) relied on Microsoft's translation service. A second experiment used the SEM based on abstracts for expansion and translation (dbp_abs). And the final experiment (dbp_sub_ms) also used the SEM, but with subjects from DBpedia entities that were translated using Mircrosoft's translation service.</p><p>This general set-up was then repeated for two different translation-based query formulation procedures. For the first group the topics were translated to English, German, and French only (see Table <ref type="table" coords="9,276.22,657.61,3.77,8.74" target="#tab_4">4</ref>, "Xto2"). In the second group of experiments, the topics were translated to the nine most frequent languages of the CHiC collection: German, French, Swedish, Italian, Spanish, Norwegian, English, Dutch, and Finnish (see Table <ref type="table" coords="10,169.17,150.57,3.75,8.74" target="#tab_5">5</ref>, "Xto8"). All officially submitted experiments are in the latter group and are marked with a star (*). The obtained results suggest an interesting conclusion regarding multilingual collections: translating queries to more languages can decrease retrieval performance (comparing each row in Table <ref type="table" coords="10,252.35,513.58,5.01,8.74" target="#tab_4">4</ref> with each corresponding row in Table <ref type="table" coords="10,420.79,513.58,3.62,8.74" target="#tab_5">5</ref>), provided that the relevance assessments for the multilingual task covered documents in all languages. It can also be seen that using a translation service performs better than any other approach, regardless of the language of the topics. This observation is substantiated by the relationship between the two types of experiments with the SEM. Using the subject descriptions as source for translation outperformed the already translated abstract descriptions in all but on scenario (English as topic language, see Table <ref type="table" coords="10,446.94,585.60,3.61,8.74" target="#tab_4">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Variability Task</head><p>For the sake of simplicity, all experiments from the Ad-hoc task were also used for the Variability task. This resulted in a total of 45 experiments, whereof 32 have been submitted for evaluation. The following modification was made to each of the experiment configurations to increase the diversity of the result sets.</p><p>The least recently used (LRU) algorithm <ref type="bibr" coords="11,291.26,150.57,11.67,8.74" target="#b8">[9]</ref> was adapted in order to restrict the original result list to different types of documents from different providers. First the collections were queried by type, i.e. each experiment was conducted by querying text, image, audio, and video documents separately (see Table <ref type="table" coords="11,364.35,186.58,3.62,8.74" target="#tab_0">1</ref>). For each of these four result lists, only the first hit from each provider (up to the maximum of the twelve different providers) was stored in a list data structure with the addition of the type information and the document score (retrieval status value). According to the LRU algorithm the types were evaluated and if a type was returned too often, the corresponding document was refused, its score was discounted, and it was then put back into the list. This process did not ensure that a total number of twelve hits were included for each topic. The reason for this was the fact that the original result sets did not necessarily contain documents originating from twelve different data providers. At the time of writing, the evaluation of the experiments was exclusively based on MAP. Given the fact, that only twelve hits were returned for each of the experiments and that MAP does not reflect diversity at all, no results are reported here. The obtained results and suitable metrics are discussed in detail in <ref type="bibr" coords="11,363.54,342.63,10.63,8.74" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Semantic Enrichment Task</head><p>This task required to return the ten most relevant concepts for each of the topics. Therefore, it was not necessary to rely on a retrieval system. Our approach used the semantic enrichment module based on DBpedia (see Section 3) with two significant modifications. First, a number of elements of DBpedia entities were used (instead of only two in Section 4.1). And second, to keep matters as simple as possible the Term Selection component (see Figure <ref type="figure" coords="11,259.64,452.63,4.17,8.74">2</ref>) was replaced with a straightforward weighted list data structure. This ensured that the experiments could be made without any dependency on the data collection.</p><p>So the problem was reduced to the extraction of terms from DBpedia entities that were found for each query and to weight these terms accordingly. The term extraction procedure followed several steps that used different resources of DBpedia entities:</p><p>A final step that represented a fallback mechanism for cases where no entities could be found for a given query required a list of default terms in three languages:</p><p>─ English: "museum", "archive", "library", "text", "image", "audio", "video", "film", "memorial", "monument", "art", "photo", "architecture", "history", "painting", "picture", ─ French: "musée", "archives", "bibliothèque", "texte", "image", "audio", "video", "film", "mémorial", "monument", "art", "photo", "architecture", "histoire", "peinture", "dessin", ─ German: "museum", "archiv", "bibliothek", "text", "bild", "audio", "video", "film", "denkmal", "monument", "kunst", "foto", "architektur", "geschichte", "gemälde", "aufnahme".</p><p>The processing of the individual resources for the term extraction was straightforward for most of these steps (see <ref type="bibr" coords="12,237.44,310.57,16.65,8.74" target="#b9">[10]</ref> for more details). It has been decided to use different weighting schemes to select the terms for the four experiments that could be submitted to each of the sub-tasks (see Table <ref type="table" coords="12,279.62,334.57,3.62,8.74" target="#tab_6">6</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monolingual Experiments</head><p>For each of the three monolingual subtasks in English, French, and German, the four runs (see above) were submitted for evaluation. Table <ref type="table" coords="12,369.85,530.55,5.01,8.74" target="#tab_7">7</ref> illustrates the obtained results with respect to the three evaluation metrics Precision(weak), Precision(strong), and MAP. Note that for the calculation of the MAP metric the submitted concepts were tested with a retrieval system other than Xtrieval (see <ref type="bibr" coords="12,368.51,566.57,11.70,8.74" target="#b1">[2]</ref> for more details). For this reason, the MAP values are not directly comparable to the corresponding figures from Section 4.1.</p><p>The evaluation results show considerably weaker MAP values than in Section 4.1. This suggests that the approach used here may not be suitable for the task. Finding the reasons for this needs further analysis of the programming code. In general there seem to be only small differences in the result sets for the experiments, because all of the three evaluation metrics show only little variance across the four types of runs. The small variation of the weights for the term extraction process might be one reason for this. Another explanation could be the number of query terms, which was almost con-stant for all experiments and topics. The small amount of variance across the different experiments might also explain why the three metrics do not agree on the best experiment configuration. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bilingual Experiments</head><p>For the preparation of the bilingual experiments another modification had to be made to the experiment in order to find terms in the corresponding collection language. Besides the strategy to use the multilingual abstract descriptions from DBpedia entities (see Section 4.1), another straightforward approach is to use the label of the entities, which is also available in different languages. Since the labels are short in general, they are very characteristic. The results listed in Table <ref type="table" coords="14,233.41,150.57,5.01,8.74" target="#tab_8">8</ref> demonstrate that translation approach improved retrieval performance over all the corresponding monolingual experiments. This observation is also independent of the evaluation metric. Unfortunately, no MAP values could be obtained for some of the experiments, which may have helped to substantiate this conclusion. Obvious language-specific effects are a second key finding from the bilingual runs. Similar to the bilingual experiments discussed in Section 4.1, the topic language seems to have a considerable effect on the bilingual retrieval performance. French as the source language for the English sub-collection seems to be preferable over German, English topics seem to be superior to French ones for the German subcollection, and for the French sub-collection English is the better source language than German. Note that the effect is the same as in Section 4.1, but the language preferences are different for the English and French sub-collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multilingual Experiments</head><p>The multilingual experiments were based on the configuration of the bilingual runs, except for the translation. Here, the DBpedia entity labels were collected in all three target languages: English, French, and German. Four experiments have been submitted for evaluation. The corresponding results are presented in Table <ref type="table" coords="14,433.53,360.63,3.77,8.74" target="#tab_9">9</ref>. As for all the previously discussed semantic enrichment experiments, the results are considerably weaker in terms of MAP than the corresponding baseline runs submitted to the Ad-hoc task. The evaluation of the expansion terms resulted in Precision values comparable to the mono-and bilingual runs. A possible impact of the source language of the query is not as obvious as for the bilingual sub-tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The focus of our participation in the CHiC evaluation lab was on the development and evaluation of an extension for Xtrieval that exploits semantic resources like DBpedia.</p><p>A maximum number of 32 experiments were submitted for each of the three tasks of the CHiC lab and yet not all prepared experiments could be included. The outcome of the conducted experiments is as follows:</p><p>─ Ad-hoc Task:</p><p>In the monolingual scenario, from the four submitted system configurations the baseline experiment without any specific modification outperformed the other three runs on the English and German sub-collections. The two experiments that used the semantic enrichment module performed poorly compared to the straightforward baselines. For the bilingual scenario our confidence in the implemented concept enrichment process was proved wrong by the evaluation results. Again, the most straightforward configurations achieved the best MAP values. Using Microsoft's translation service for topic translation resulted in better bilingual retrieval performance than exploiting the DBpedia semantic web resource. In the multilingual retrieval scenario our confidence in the semantic enrichment process was proved wrong once more. In fact, we managed to choose the worst experiments for submission and hold back the best ones. Contrasting the latter evaluation results with the bilingual results indicates that multilingual retrieval performance is almost as good as bilingual performance. ─ Variability Task:</p><p>The experiments from the Ad-hoc task were re-used here. At the time of writing no detailed analysis can be provided for organisational reasons. ─ Semantic Enrichment Task:</p><p>Our experiments for the semantic enrichment task featured some additional challenges. Except for the DBpedia semantic web resource, no publicly available services or software projects were used. This may have contributed to the generally weak retrieval performance for the experiments based on our concept suggestions.</p><p>The evaluation results demonstrated that using very short DBpedia entity labels as source for translation improved the retrieval results considerably in comparison to the other tested descriptors. This might be another cue that query expansion is a hard problem on this particular test collection.</p><p>Although, most of the presented experiments indicate that longer queries are not effective for retrieval in this particular collection, we think that there is still room for improvement. Further experiments are needed to study the observed effects in more detail. This could be achieved by incorporating tools for component level evaluation <ref type="bibr" coords="15,124.68,482.65,15.86,8.74" target="#b10">[11,</ref><ref type="bibr" coords="15,143.19,482.65,11.90,8.74" target="#b11">12]</ref>. Another opportunity for further analyses might be to submit the results of all contributions to online evaluation databases like EvalutIR.org <ref type="bibr" coords="15,374.64,494.65,15.33,8.74" target="#b12">[13]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,155.22,381.09,285.02,8.10;2,126.24,401.40,344.40,242.52"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Schematic overview on the system architecture and employed resources</figDesc><graphic coords="2,126.24,401.40,344.40,242.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,126.24,433.44,345.72,151.44"><head></head><label></label><figDesc></figDesc><graphic coords="3,126.24,433.44,345.72,151.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,194.28,502.05,206.81,8.10"><head>Table 1 .</head><label>1</label><figDesc>Mapping of the document structure for indexing</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,124.68,519.90,346.00,168.38"><head>Field name XPath construct for document to index mapping content</head><label></label><figDesc></figDesc><table coords="6,124.68,532.53,346.00,155.75"><row><cell></cell><cell>dc:publisher|dcterms:isPartOf|dcterms:spatial|dcterms:alternative|</cell></row><row><cell></cell><cell>dcterms:created|dcterms:temporal|dc:creator|dc:date|dc:description|</cell></row><row><cell></cell><cell>dc:title|dc:subject</cell></row><row><cell>enrichment</cell><cell>*[ends-with(name(),'_label')]</cell></row><row><cell cols="2">enrichment_url *[ends-with(name(),'_term')]</cell></row><row><cell>provider</cell><cell>europeana:dataProvider|europeana:provider</cell></row><row><cell>type</cell><cell>europeana:type</cell></row><row><cell>type_desc</cell><cell>dc:type|dc:format|dcterms:medium</cell></row><row><cell cols="2">A total number of four indices has been created for the experimental evaluation. Start-</cell></row><row><cell cols="2">ing with the StandardTokenizer of Lucene that splits a text stream into tokens and</cell></row><row><cell cols="2">recognizes some entities like URLs or e-mail addresses. Additional filters (marked</cell></row><row><cell cols="2">with *) that are implemented in Xtrieval and further filters from Lucene packages</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,124.68,150.09,346.03,172.17"><head>Table 2 .</head><label>2</label><figDesc>Results for the monolingual sub-tasks</figDesc><table coords="8,124.68,167.88,346.03,154.39"><row><cell>run id</cell><cell cols="2">lang configuration summary</cell><cell>MAP</cell></row><row><cell>base</cell><cell>EN</cell><cell>Lucene for core retrieval, no feedback</cell><cell>0.4860</cell></row><row><cell>qe_kl</cell><cell>EN</cell><cell>Lucene retrieval, KLCorrect 15 exp., 3 docs, 10 terms</cell><cell>0.4072</cell></row><row><cell cols="2">qe_dbp_abs EN</cell><cell>Lucene retrieval, DBpedia exp., 20 terms (abstract)</cell><cell>0.3036</cell></row><row><cell cols="2">qe_dbp_sub EN</cell><cell>Lucene retrieval, DBpedia exp., 20 terms (subject)</cell><cell>0.4179</cell></row><row><cell>base</cell><cell>DE</cell><cell>see corresponding runs above</cell><cell>0.6039</cell></row><row><cell>qe_kl</cell><cell>DE</cell><cell></cell><cell>0.5854</cell></row><row><cell cols="2">qe_dbp_abs DE</cell><cell></cell><cell>0.4240</cell></row><row><cell cols="2">qe_dbp_sub DE</cell><cell></cell><cell>0.4141</cell></row><row><cell>base</cell><cell>FR</cell><cell>see corresponding runs above</cell><cell>0.3300</cell></row><row><cell>qe_kl</cell><cell>FR</cell><cell></cell><cell>0.3590</cell></row><row><cell cols="2">qe_dbp_abs FR</cell><cell></cell><cell>0.3227</cell></row><row><cell cols="2">qe_dbp_sub FR</cell><cell></cell><cell>0.3205</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,124.68,186.09,346.01,317.67"><head>Table 3 .</head><label>3</label><figDesc>Results for the bilingual sub-tasks (official runs are marked with *)</figDesc><table coords="9,124.68,203.88,346.01,299.89"><row><cell>run id</cell><cell>lang</cell><cell>configuration summary</cell><cell>MAP</cell></row><row><cell>base</cell><cell cols="2">DE2EN Lucene for core retrieval, no exp., no trans.</cell><cell>0.2784</cell></row><row><cell>ms</cell><cell cols="2">DE2EN Lucene for core retrieval, no exp.</cell><cell>0.3240</cell></row><row><cell>qe_dbp_abs*</cell><cell cols="2">DE2EN Lucene, DBpedia exp., 20 terms (abstract)</cell><cell>0.2805</cell></row><row><cell cols="3">qe_dbp_sub_ms* DE2EN Lucene, DBpedia exp., 20 terms (subject)</cell><cell>0.3399</cell></row><row><cell>base</cell><cell cols="2">FR2EN see corresponding runs above</cell><cell>0.3031</cell></row><row><cell>ms</cell><cell>FR2EN</cell><cell></cell><cell>0.3513</cell></row><row><cell>qe_dbp_abs*</cell><cell>FR2EN</cell><cell></cell><cell>0.2780</cell></row><row><cell cols="2">qe_dbp_sub_ms* FR2EN</cell><cell></cell><cell>0.3549</cell></row><row><cell>base</cell><cell cols="2">EN2DE see corresponding runs above</cell><cell>0.3866</cell></row><row><cell>ms</cell><cell>EN2DE</cell><cell></cell><cell>0.5092</cell></row><row><cell>qe_dbp_abs*</cell><cell>EN2DE</cell><cell></cell><cell>0.3396</cell></row><row><cell cols="2">qe_dbp_sub_ms* EN2DE</cell><cell></cell><cell>0.2898</cell></row><row><cell>base</cell><cell cols="2">FR2DE see corresponding runs above</cell><cell>0.4000</cell></row><row><cell>ms</cell><cell>FR2DE</cell><cell></cell><cell>0.4670</cell></row><row><cell>qe_dbp_abs*</cell><cell>FR2DE</cell><cell></cell><cell>0.3724</cell></row><row><cell cols="2">qe_dbp_sub_ms* FR2DE</cell><cell></cell><cell>0.3836</cell></row><row><cell>base</cell><cell cols="2">EN2FR see corresponding runs above</cell><cell>0.2216</cell></row><row><cell>ms</cell><cell>EN2FR</cell><cell></cell><cell>0.3238</cell></row><row><cell>qe_dbp_abs*</cell><cell>EN2FR</cell><cell></cell><cell>0.1941</cell></row><row><cell cols="2">qe_dbp_sub_ms* EN2FR</cell><cell></cell><cell>0.2646</cell></row><row><cell>base</cell><cell cols="2">DE2FR see corresponding runs above</cell><cell>0.1882</cell></row><row><cell>ms</cell><cell>DE2FR</cell><cell></cell><cell>0.2424</cell></row><row><cell>qe_dbp_abs*</cell><cell>DE2FR</cell><cell></cell><cell>0.2294</cell></row><row><cell cols="2">qe_dbp_sub_ms* DE2FR</cell><cell></cell><cell>0.3084</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,124.68,186.09,346.01,136.17"><head>Table 4 .</head><label>4</label><figDesc>Results for the multilingual subtask using 3 target languages</figDesc><table coords="10,124.68,203.88,346.01,118.39"><row><cell>run id</cell><cell>lang</cell><cell>configuration summary</cell><cell>MAP</cell></row><row><cell>base_ms</cell><cell cols="2">ENto2 Lucene, no feedback, transl.</cell><cell>0.3250</cell></row><row><cell>dbp_abs</cell><cell cols="2">ENto2 Lucene, DBpedia exp., 20 terms (abstract)</cell><cell>0.1535</cell></row><row><cell cols="3">dbp_sub_ms ENto2 Lucene, DBpedia exp., 20 terms (subject), transl.</cell><cell>0.1504</cell></row><row><cell>base_ms</cell><cell cols="2">DEto2 see corresponding runs above</cell><cell>0.3308</cell></row><row><cell>dbp_abs</cell><cell>DEto2</cell><cell></cell><cell>0.1746</cell></row><row><cell cols="2">dbp_sub_ms DEto2</cell><cell></cell><cell>0.2292</cell></row><row><cell>base_ms</cell><cell cols="2">FRto2 see corresponding runs above</cell><cell>0.2977</cell></row><row><cell>dbp_abs</cell><cell>FRto2</cell><cell></cell><cell>0.1578</cell></row><row><cell cols="2">dbp_sub_ms FRto2</cell><cell></cell><cell>0.1818</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,124.68,337.59,346.08,136.17"><head>Table 5 .</head><label>5</label><figDesc>Results for the multilingual subtask using 9 target languages (submitted runs: *)</figDesc><table coords="10,124.68,355.38,346.08,118.39"><row><cell>run id</cell><cell>lang</cell><cell>configuration summary</cell><cell>MAP</cell></row><row><cell>base_ms</cell><cell cols="2">ENto8 Lucene, no feedback, transl.</cell><cell>0.2377</cell></row><row><cell>dbp_abs*</cell><cell cols="2">ENto8 Lucene, DBpedia exp., 20 terms (abstract)</cell><cell>0.0983</cell></row><row><cell cols="3">dbp_sub_ms* ENto8 Lucene, DBpedia exp., 20 terms (subject), transl.</cell><cell>0.1085</cell></row><row><cell>base_ms</cell><cell cols="2">DEto8 see corresponding runs above</cell><cell>0.2484</cell></row><row><cell>dbp_abs</cell><cell>DEto8</cell><cell></cell><cell>0.1193</cell></row><row><cell>dbp_sub_ms</cell><cell>DEto8</cell><cell></cell><cell>0.1743</cell></row><row><cell>base_ms</cell><cell cols="2">FRto8 see corresponding runs above</cell><cell>0.2197</cell></row><row><cell>dbp_abs*</cell><cell>FRto8</cell><cell></cell><cell>0.1041</cell></row><row><cell cols="2">dbp_sub_ms* FRto8</cell><cell></cell><cell>0.1333</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="12,151.50,358.05,281.10,126.75"><head>Table 6 .</head><label>6</label><figDesc>Weighting scheme for the term selection process</figDesc><table coords="12,151.50,375.90,281.10,108.91"><row><cell>processing step</cell><cell>run1</cell><cell>run2</cell><cell>run3</cell><cell>run4</cell></row><row><cell>1a) entities</cell><cell>11</cell><cell>11</cell><cell>11</cell><cell>11</cell></row><row><cell>1b) disambiguation</cell><cell>10</cell><cell>10</cell><cell>10</cell><cell>10</cell></row><row><cell>2) category extraction</cell><cell>1.25</cell><cell>1.25</cell><cell>1.25</cell><cell>1.25</cell></row><row><cell>3a) cat. expansion (broader)</cell><cell>0.42</cell><cell>0.63</cell><cell>0.42</cell><cell>0.63</cell></row><row><cell cols="2">3b) cat. expansion (narrower) 0.14</cell><cell>0.21</cell><cell>0.21</cell><cell>0.31</cell></row><row><cell>4) related entities</cell><cell>0.13</cell><cell>0.19</cell><cell>0.19</cell><cell>0.28</cell></row><row><cell>5) exploring out-links</cell><cell>0.11</cell><cell>0.17</cell><cell>0.17</cell><cell>0.26</cell></row><row><cell>6) fallback list</cell><cell>0.10</cell><cell>0.16</cell><cell>0.16</cell><cell>0.16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="13,176.28,198.09,242.75,172.17"><head>Table 7 .</head><label>7</label><figDesc>Results for the monolingual sub-tasks</figDesc><table coords="13,176.28,215.88,242.75,154.39"><row><cell>run id</cell><cell cols="3">lang Prec(weak) Prec(strong)</cell><cell>MAP</cell></row><row><cell cols="2">cut_t3_run1 EN</cell><cell>0.8000</cell><cell>0.6160 0.1092</cell></row><row><cell cols="2">cut_t3_run2 EN</cell><cell>0.7640</cell><cell>0.6200 0.1069</cell></row><row><cell cols="2">cut_t3_run3 EN</cell><cell>0.7800</cell><cell>0.6160 0.1072</cell></row><row><cell cols="2">cut_t3_run4 EN</cell><cell>0.7880</cell><cell>0.6520 0.1056</cell></row><row><cell cols="2">cut_t3_run1 DE</cell><cell>0.7720</cell><cell>0.6080 0.2286</cell></row><row><cell cols="2">cut_t3_run2 DE</cell><cell>0.7640</cell><cell>0.6040 0.2383</cell></row><row><cell cols="2">cut_t3_run3 DE</cell><cell>0.7600</cell><cell>0.5840 0.2600</cell></row><row><cell cols="2">cut_t3_run4 DE</cell><cell>0.7640</cell><cell>0.5840 0.2403</cell></row><row><cell cols="2">cut_t3_run1 FR</cell><cell>0.5920</cell><cell>0.5480 0.1467</cell></row><row><cell cols="2">cut_t3_run2 FR</cell><cell>0.6240</cell><cell>0.5720 0.1450</cell></row><row><cell cols="2">cut_t3_run3 FR</cell><cell>0.6200</cell><cell>0.5680 0.1464</cell></row><row><cell cols="2">cut_t3_run4 FR</cell><cell>0.6120</cell><cell>0.5520 0.1458</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="13,169.62,487.59,256.08,172.17"><head>Table 8 .</head><label>8</label><figDesc>Results for the bilingual sub-tasks</figDesc><table coords="13,169.62,505.38,256.08,154.39"><row><cell>run id</cell><cell>lang</cell><cell cols="2">Prec(weak) Prec(strong)</cell><cell>MAP</cell></row><row><cell cols="2">cut_t3_run1 DE2EN</cell><cell>0.7520</cell><cell cols="2">0.6400 0.1312</cell></row><row><cell cols="2">cut_t3_run2 DE2EN</cell><cell>0.7680</cell><cell cols="2">0.6760 0.1273</cell></row><row><cell cols="2">cut_t3_run3 FR2EN</cell><cell>0.6960</cell><cell>0.6360</cell><cell>-</cell></row><row><cell cols="2">cut_t3_run4 FR2EN</cell><cell>0.6880</cell><cell>0.6040</cell><cell>-</cell></row><row><cell cols="2">cut_t3_run1 EN2DE</cell><cell>0.8400</cell><cell>0.7600</cell><cell>-</cell></row><row><cell cols="2">cut_t3_run2 EN2DE</cell><cell>0.8160</cell><cell>0.7480</cell><cell>-</cell></row><row><cell cols="2">cut_t3_run3 FR2DE</cell><cell>0.6000</cell><cell>0.5240</cell><cell>-</cell></row><row><cell cols="2">cut_t3_run4 FR2DE</cell><cell>0.5320</cell><cell>0.4840</cell><cell>-</cell></row><row><cell cols="2">cut_t3_run1 EN2FR</cell><cell>0.7920</cell><cell cols="2">0.6800 0.1913</cell></row><row><cell cols="2">cut_t3_run2 EN2FR</cell><cell>0.8000</cell><cell cols="2">0.6680 0.1892</cell></row><row><cell cols="2">cut_t3_run3 DE2FR</cell><cell>0.6400</cell><cell cols="2">0.5680 0.1414</cell></row><row><cell cols="2">cut_t3_run4 DE2FR</cell><cell>0.5720</cell><cell cols="2">0.5040 0.1223</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="14,169.62,444.09,256.08,75.21"><head>Table 9 .</head><label>9</label><figDesc>Results for the multilingual sub-task</figDesc><table coords="14,169.62,461.88,256.08,57.43"><row><cell>run id</cell><cell>lang</cell><cell cols="2">Prec(weak) Prec(strong)</cell><cell>MAP</cell></row><row><cell cols="2">cut_t3_run1 DE2X</cell><cell>0.7000</cell><cell cols="2">0.6040 0.0381</cell></row><row><cell cols="2">cut_t3_run2 FR2X</cell><cell>0.7360</cell><cell cols="2">0.6440 0.0614</cell></row><row><cell cols="2">cut_t3_run3 EN2X</cell><cell>0.6800</cell><cell cols="2">0.5800 0.0283</cell></row><row><cell cols="2">cut_t3_run4 DE2X</cell><cell>0.6680</cell><cell cols="2">0.5600 0.0246</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,136.02,631.42,195.00,7.85;3,330.96,629.36,4.68,5.23;3,335.64,631.42,25.49,7.85"><p>http://lucene.apache.org/core/ (retrieved on August 16 th , 2012)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="3,136.02,642.40,188.00,7.85;3,324.00,640.34,4.68,5.23;3,328.68,642.40,25.57,7.85"><p>http://snowball.tartarus.org/ (retrieved on August 16 th , 2012)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="3,136.02,653.44,251.48,7.85;3,387.48,651.38,4.68,5.23;3,392.16,653.44,25.57,7.85"><p>http://code.google.com/p/language-detection/ (retrieved on August 16 th , 2012)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="3,136.02,664.42,235.46,7.85;3,371.46,662.36,4.62,5.23;3,376.08,664.42,25.57,7.85"><p>http://www.microsofttranslator.com/dev/ (retrieved on August 16 th , 2012)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="3,136.02,675.40,154.74,7.85;3,290.76,673.34,4.68,5.23;3,295.44,675.40,25.50,7.85"><p>http://dbpedia.org/ (retrieved on August 16 th , 2012)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="3,136.02,686.44,221.57,7.85;3,357.54,684.38,4.68,5.23;3,362.22,686.44,25.42,7.85"><p>http://wiki.dbpedia.org/DBpediaLive (retrieved on August 16 th , 2012)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6" coords="4,136.02,642.40,166.53,7.85;4,302.52,640.34,4.68,5.23;4,307.20,642.40,25.43,7.85"><p>http://www.java.com/ (retrieved on August 16 th , 2012)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7" coords="4,136.02,653.44,162.07,7.85;4,298.02,651.38,4.68,5.23;4,302.70,653.44,25.42,7.85"><p>http://hc.apache.org/ (retrieved on August 16 th , 2012)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8" coords="4,136.02,664.42,176.76,7.85;4,312.78,662.36,4.62,5.23;4,317.40,664.42,25.57,7.85"><p>http://dbpedia.org/sparql (retrieved on August 16 th , 2012)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9" coords="4,136.02,675.40,242.16,7.85;4,378.18,673.34,4.68,5.23;4,382.86,675.40,25.57,7.85"><p>http://lookup.dbpedia.org/api/search.asmx/ (retrieved on August 16 th , 2012)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_10" coords="4,138.24,686.44,187.76,7.85;4,325.98,684.38,4.68,5.23;4,330.66,686.44,25.57,7.85"><p>http://dbpedia.org/resource/ (retrieved on August 16 th , 2012)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_11" coords="5,136.02,664.42,334.59,7.85;5,136.02,675.40,269.94,7.85"><p>The documentation of the Terrier platform provides the following description: "CSCorrect implements the un-simplified Chi-square divergence for query expansion."</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_12" coords="5,136.01,686.44,275.76,7.85;5,411.72,684.38,4.68,5.23;5,416.40,686.44,25.43,7.85"><p>Jaxen: Java XPath engine, http://jaxen.codehaus.org (retrieved on August 16 th , 2012)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_13" coords="8,136.02,675.40,334.55,7.85;8,136.02,686.44,269.06,7.85"><p>The documentation of the Terrier platform provides the following description: "This class implements the correct Kullback-Leibler divergence for query expansion."</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was partly accomplished within the research project ValidAX (http://www.validax.de), funded by the <rs type="funder">German Federal Ministry of Education and Research</rs> under the grant reference number <rs type="grantNumber">16V0058</rs>. The authors take sole responsibility for the contents of this publication.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Z7BHCFF">
					<idno type="grant-number">16V0058</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="16,129.80,177.34,340.81,7.85;16,136.01,188.32,334.64,7.85;16,136.01,199.05,334.59,8.10;16,136.01,210.34,84.43,7.85" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="16,277.99,177.34,192.62,7.85;16,136.01,188.32,320.90,7.85">CHiC 2011 -Cultural Heritage in CLEF: From Use Cases to Evaluation in Practice for Multilingual Information Access to Cultural Heritage</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gäde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">L</forename><surname>Paramita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,136.01,199.05,190.30,8.10">CLEF 2011 Labs and Workshop, Notebook Papers</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-09-22">19-22 September 2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,129.79,221.32,340.81,7.85;16,136.01,232.30,122.97,7.85" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="16,218.72,221.32,201.87,7.85">Overview of the Cultural Heritage pilot evaluation lab</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gäde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Petras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,427.26,221.32,43.33,7.85;16,136.01,232.30,49.96,7.85">CLEF 2012 working notes</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,129.78,243.34,340.82,7.85;16,136.00,254.08,334.46,8.10;16,136.00,265.30,61.29,7.85" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="16,288.36,243.34,182.23,7.85;16,136.00,254.32,28.27,7.85">Extensible Retrieval and Evaluation Framework: Xtrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kürsten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,328.67,254.08,36.83,8.10">LWA 2008</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Baumeister</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Atzmüller</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="107" to="110" />
		</imprint>
		<respStmt>
			<orgName>University of Würzburg</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="16,129.80,276.34,340.76,7.85;16,136.02,287.07,334.58,8.10;16,136.01,298.30,74.01,7.85" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,215.63,276.34,254.93,7.85;16,136.02,287.32,44.16,7.85">The Importance of being Grid: Chemnitz University of Technology at Grid@CLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kürsten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,198.60,287.07,167.98,8.10">Working Notes for the CLEF 2009 Workshop</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-10-02">September 30-October 2, 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,129.78,309.34,340.86,7.85;16,136.00,320.08,334.63,8.10;16,136.00,331.30,80.26,7.85" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,136.00,320.32,205.25,7.85">DBpedia -A crystallization point for the Web of Data</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hellmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,359.92,320.08,48.39,8.10">Web Semant</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="154" to="165" />
			<date type="published" when="2009-09">September 2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,129.77,342.34,340.83,7.85;16,135.98,353.32,334.60,7.85;16,135.98,364.06,334.65,8.10;16,136.02,375.09,334.59,8.10;16,136.02,386.32,126.72,7.85" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="16,182.41,353.32,288.17,7.85;16,135.98,364.30,66.96,7.85">Media Meets Semantic Web -How the BBC Uses DBpedia and Linked Data to Make Connections</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Raimond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sizemore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Smethurst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,219.75,364.06,250.89,8.10;16,136.02,375.09,246.67,8.10">Proceedings of the 6th European Semantic Web Conference on The Semantic Web: Research and Applications (ESWC 2009 Heraklion)</title>
		<meeting>the 6th European Semantic Web Conference on The Semantic Web: Research and Applications (ESWC 2009 Heraklion)<address><addrLine>Berlin; Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="723" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,129.79,397.30,340.79,7.85;16,136.01,408.10,334.69,8.10;16,136.02,419.08,280.96,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="16,291.05,397.30,179.53,7.85;16,136.01,408.34,60.65,7.85">DBpedia for NLP: A Multilingual Cross-domain Knowledge Base</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,214.79,408.10,255.91,8.10;16,136.02,419.08,124.22,8.10">Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC&apos;12)</title>
		<meeting>the Eight International Conference on Language Resources and Evaluation (LREC&apos;12)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-05-27">21-27 May 2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,129.80,430.30,340.81,7.85;16,136.02,441.10,334.59,8.10;16,136.02,452.08,61.97,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="16,413.45,430.30,57.16,7.85;16,136.02,441.34,212.39,7.85">Terrier: A High Performance and Scalable Information Retrieval Platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,366.42,441.10,104.19,8.10;16,136.02,452.08,35.46,8.10">Proceedings of OSIR Workshop 2006</title>
		<meeting>OSIR Workshop 2006</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,129.79,463.30,310.45,7.85" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">G</forename><surname>Coffman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Denning</surname></persName>
		</author>
		<title level="m" coord="16,266.60,463.30,94.65,7.85">Operating Systems Theory</title>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,129.41,474.34,341.16,7.85;16,136.00,485.32,252.65,7.85" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="16,181.24,474.34,289.33,7.85;16,136.00,485.32,30.01,7.85">Information Retrieval mit dem Xtrieval Framework für cultural heritage Datenbestände</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Richter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>Chemnitz University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Diploma Thesis</note>
</biblStruct>

<biblStruct coords="16,129.41,496.30,341.18,7.85;16,136.00,507.35,216.67,7.85" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="16,180.80,496.30,286.27,7.85">A Generic Approach to Component-Level Evaluation in Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kürsten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>Chemnitz University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Doctoral Thesis</note>
</biblStruct>

<biblStruct coords="16,129.40,518.33,341.15,7.85;16,135.99,529.06,334.49,8.10;16,135.99,540.35,104.25,7.85" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="16,275.12,518.33,195.42,7.85;16,135.99,529.31,18.99,7.85">A Tool for Comparative IR Evaluation on Component Level</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kürsten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,172.71,529.06,228.44,8.10">Proceedings of the 34th International ACM SIGIR conference</title>
		<meeting>the 34th International ACM SIGIR conference<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1291" to="1292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,129.39,551.33,341.25,7.85;16,135.98,562.07,334.67,8.10;16,135.98,573.11,251.80,8.10" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="16,397.18,551.33,73.46,7.85;16,135.98,562.31,146.52,7.85">An Online Tool for Evaluating and Comparing IR Systems</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">G</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Webber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Evaluatir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,302.47,562.07,168.19,8.10;16,135.98,573.11,62.71,8.10">Proceedings of the 32nd international ACM SIGIR conference</title>
		<meeting>the 32nd international ACM SIGIR conference<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="833" to="833" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
