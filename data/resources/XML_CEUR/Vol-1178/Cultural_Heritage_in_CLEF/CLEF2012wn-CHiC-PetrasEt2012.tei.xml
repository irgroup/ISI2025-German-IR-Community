<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,146.04,152.87,303.19,12.58">Cultural Heritage in CLEF (CHiC) Overview 2012</title>
				<funder ref="#_Us8EwxZ">
					<orgName type="full">European Commission</orgName>
				</funder>
				<funder ref="#_fBC5ARR">
					<orgName type="full">PROMISE (</orgName>
				</funder>
				<funder>
					<orgName type="full">Basque Country</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,134.52,190.21,54.26,11.09"><forename type="first">Vivien</forename><surname>Petras</surname></persName>
							<email>vivien.petras@ibi.hu-berlin</email>
						</author>
						<author>
							<persName coords="1,196.94,190.21,50.82,11.09"><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
							<email>ferro@dei.unipd.it</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padova</orgName>
								<address>
									<addrLine>Via Gradenigo 6/B</addrLine>
									<postCode>35131</postCode>
									<settlement>Padova</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,255.98,190.21,47.51,11.09"><forename type="first">Maria</forename><surname>Gäde</surname></persName>
							<email>maria.gaede@ibi.hu-berlin</email>
						</author>
						<author>
							<persName coords="1,311.68,190.21,55.25,11.09"><forename type="first">Antoine</forename><surname>Isaac</surname></persName>
							<email>aisaac@few.fu.nl</email>
							<affiliation key="aff2">
								<orgName type="department">The Europeana Office</orgName>
								<orgName type="laboratory">Europeana</orgName>
								<orgName type="institution">Koninklijke Bibliotheek</orgName>
								<address>
									<addrLine>Prins Willem-Alexanderhof 5</addrLine>
									<postCode>2595 BE Den</postCode>
									<settlement>Haag</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,375.21,190.21,79.67,11.09"><forename type="first">Michael</forename><surname>Kleineberg</surname></persName>
							<email>michael.kleineberg@ibi.hu-berlin</email>
						</author>
						<author>
							<persName coords="1,193.74,202.15,58.06,11.09"><forename type="first">Ivano</forename><surname>Masiero</surname></persName>
							<email>masieroi@dei.unipd.it</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padova</orgName>
								<address>
									<addrLine>Via Gradenigo 6/B</addrLine>
									<postCode>35131</postCode>
									<settlement>Padova</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,260.02,202.15,60.28,11.09"><forename type="first">Mattia</forename><surname>Nicchio</surname></persName>
							<email>nicchio@dei.unipd.it</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padova</orgName>
								<address>
									<addrLine>Via Gradenigo 6/B</addrLine>
									<postCode>35131</postCode>
									<settlement>Padova</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,343.01,202.15,55.12,11.09"><forename type="first">Juliane</forename><surname>Stiller</surname></persName>
							<email>juliane.stiller@ibi.hu-berlin</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">1Berlin</orgName>
								<orgName type="department" key="dep2">School of Library and Information Science</orgName>
								<orgName type="institution">Humboldt-Universität zu Berlin</orgName>
								<address>
									<addrLine>Dorotheenstr. 26</addrLine>
									<postCode>10117</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,146.04,152.87,303.19,12.58">Cultural Heritage in CLEF (CHiC) Overview 2012</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E677E201B91DC1A3353F0CD29C5E6357</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>cultural heritage</term>
					<term>Europeana</term>
					<term>variability</term>
					<term>diversity</term>
					<term>semantic enrichment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The paper for the CHiC pilot lab describes the motivation, tasks, Europeana collections and topics, evaluation measures as well as the submitted and analyzed information retrieval runs. In its first year, CHiC offered three tasks: ad-hoc, which measured retrieval effectiveness according to relevance of the ranked retrieval results (standard 1000 document TREC output), variability, which required participants to present a list of 12 records that represent diverse information contexts and semantic enrichment, which asked participants to provide a list of 10 semantically related concepts to the one in the query to be used in query expansion experiments. All tasks were offered in monolingual, bilingual and multilingual modes. 126 different experiments from 6 participants were evaluated using the DIRECT system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cultural heritage content is often multilingual and multimedia (e.g. text, photographs, images, audio recordings, and videos), usually described with metadata in multiple formats and of different levels of complexity. Institutions in this domain have different approaches to managing information and serve diverse user communities, often with specialized needs and information contexts (native language, search environment, etc.). Evaluation approaches (particularly system-oriented evaluation) in this domain have been fragmentary and often non-standardized. The CHiC 2012 pilot evaluation lab aimed at moving towards a systematic and large-scale evaluation of cultural heritage digital libraries and information access systems. The lab's goal is to increase our understanding on how to integrate examples from the cultural heritage community into a CLEF-style evaluation framework and how results can be fed back into the CH community.</p><p>The CHiC lab researches information retrieval systems for the cultural heritage environment by using real data, real user queries and real tasks. CHiC has teamed up with Europeana<ref type="foot" coords="2,189.60,194.87,3.24,7.17" target="#foot_0">1</ref> , Europe's largest digital library, museum and archive for cultural heritage objects to provide a realistic environment for experiments.</p><p>At the CLEF 2011 conference, a first workshop on information retrieval evaluation was put on by the organizers of the lab to discuss information needs, search practices and appropriate information retrieval tasks for this domain. The outcome of this workshop was a pilot lab proposal for the CLEF conference series suggesting three tasks relevant for cultural heritage information systems. Even as a pilot lab, CHiC was able to use real data and real search topics gathered from Europeana.</p><p>The paper is structured as follows: sections 2-4 explain the data collection, the preparation of topics and the CHiC tasks as well as the used evaluation measures. Sections 5 and 6 provide an overview of the participants and submitted experiments and describe the relevance assessment process. Section 7 discusses the experimental results, whereas section 8 provides an outlook for the next lab.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Collection</head><p>In March 2012, the complete Europeana data index was downloaded for collection preparation. The Europeana index as used in Europeana's Solr search portal contained 23,300,932 documents with a size of 132 GB. Europeana data consists of metadata records describing digital representations of cultural heritage objects, e.g. the scanned version of a manuscript, an image of a painting of sculpture or an audio or video recording. Roughly 62% of the metadata records describe images, 35% describe text, 2% describe audio and 1% video recordings. The metadata contains title and description data, media type and chronological data as well as provider information. For ca. 30% of the records, content-related enrichment keywords were added automatically by Europeana.</p><p>The original Europeana index contained fields from different schemas: Simple Dublin Core, e.g. dc:title, dc:description, Qualified Dublin Core, e.g. dcterms:provenance, dcterms:spatial and Europeana Semantic Elements, e.g. europena:type, europeana:isShownAt. On top of these schema-related fields, there were additional fields used internally in the Lucene index to improve search performance or to support specific application functionalities.</p><p>These fields were removed from the data collection and the index data was wrapped in a special XML format. The whole collection was then divided into 14 subcollections according to the language of the content provider of the record (which usually indicates the language of the metadata record). If all the provider languages had been used, the number of subcollections would have reached 30. Thus, in order to reduce this amount, a threshold was set: all the languages with less than 100,000 documents were grouped together under the name "Other".</p><p>The resultant 14 subcollections are listed in table <ref type="table" coords="3,345.72,172.21,3.74,11.09" target="#tab_0">1</ref>. For the CHiC 2012 experiments, only the English, French and German subcollections as well as the entire collection were used. The XML data for all collections were made available and released to participants. Figure <ref type="figure" coords="3,153.32,461.71,5.01,11.09">1</ref> shows an extract example record from the Europeana CHiC collection. In the Europeana portal, object records commonly also contain thumbnails of the object if it is an image and links to related records. The thumbnails were not contained in the collection given to CHiC participants, but relevance assessors were able to look at them at the original source. Finally, each file in the collection contained specific copyright information about the metadata record themselves and their providers. The XML code shown in Figure <ref type="figure" coords="4,465.67,288.19,5.01,11.09">2</ref> was used for this purpose. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Topics</head><p>For all experiments, original user queries were extracted from Europeana query logs. From all user search sessions in August 2010, those queries were extracted that resulted in a user viewing at least one complete object (in order to ensure that the session contained more than one user-system interaction). The queries were then further filtered to not include wildcards or automatically generated queries (for example by Europeana features). Over 500 queries were then annotated according to their query category, i.e. topical, personal name, geographical name, work title or other. Queries could be either in the English language or ambiguous in language but would also appear in English. Ambiguous queries could include personal or location names that do not change across languages, e.g. William Shakespeare.</p><p>For CHiC, 50 queries were selected that covered a wide range of topics and represented a distribution of query categories that was found in a previous study <ref type="bibr" coords="4,439.19,605.17,10.61,11.09" target="#b9">[9]</ref>. For later relevance assessments, descriptions of the underlying information need were added, but were not admissible for information retrieval. The underlying information need for a query can be ambiguous, if the intention of the query is not clear. In this case, the research group discussed the query and agreed on the most likely information need. Figure <ref type="figure" coords="4,207.44,665.17,5.01,11.09">3</ref> shows an example of an English query. &lt;topic lang="en"&gt; &lt;identifier&gt;CHIC-004&lt;/identifier&gt; &lt;title&gt;silent film&lt;/title&gt; &lt;description&gt;documents on the history of silent film, silent film videos, biographies of actors and directors, characteristics of silent film and decline of this genre&lt;/description&gt; &lt;/topic&gt; Fig. <ref type="figure" coords="5,246.80,231.09,3.38,8.10">3</ref>. CHiC English Example Query All 50 queries were then translated into French and German. For the variability and semantic enrichment tasks, only the first 25 topics were used for the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CHiC Tasks</head><p>For the pilot lab of CHiC, three experimental tasks were selected that represented realistic use cases for cultural heritage information systems like Europeana but were also relatively simple in their set-up and to evaluate. The goal for this year's lab was to create baselines for topic and task development but also generate ground-truth in relevance assessments for experimental results. All tasks were offered with the same set of topics and in three language modes: (i) monolingual (query and document language are the same), (ii) bilingual (query and document languages are different), (iii) multilingual (documents in multiple languages, i.e. the whole Europeana collection will be searched). This allowed the participants to experiment with a number of language variations (table <ref type="table" coords="5,385.78,429.19,3.63,11.09" target="#tab_3">2</ref>).</p><p>Participants were asked to submit at least one monolingual experiment in any language per chosen task and were allowed to submit up to 4 experiments in the same language mode and combination. X  DE, X  EN, X  FR, whereas X is a topic language the document language is not in Possible multilingual runs X  MUL, whereas X is either DE, FR or EN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ad-hoc Information Retrieval</head><p>This task is a standard ad-hoc retrieval task, which measures information retrieval effectiveness with respect to user input in the form of queries. No further user-system interaction is assumed although automatic blind feedback or query expansion mechanisms are allowed to improve the system ranking. The ad-hoc setting is the standard setting for an information retrieval system -without prior knowledge about the user need or context, the system is required to produce a relevance-ranked list of documents based entirely on the query and the features of the collection documents.</p><p>Participants were allowed to use all collection fields and had to submit 1000 ranked documents (TREC-style) for relevance assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Variability</head><p>A particular user type -the casual user or "information tourist" -does not follow the conventional pattern of a targeted information need being expressed in a targeted query but poses particular challenges for access or entry points and result presentation.</p><p>The variability task required systems to present a list of 12 objects (represents the first Europeana results page), which are relevant to the query and should present a particular good overview over the different object types and categories targeted towards a casual user, who might like the "best" documents possibly sorted into "must sees" and "other possibilities." This task is about returning diverse objects and resembles the diversity tasks of the Interactive TREC track or the CLEF Image photo tracks and other research <ref type="bibr" coords="6,226.78,330.19,10.66,11.09" target="#b0">[1]</ref>, <ref type="bibr" coords="6,243.46,330.19,10.66,11.09" target="#b5">[5]</ref>, <ref type="bibr" coords="6,260.15,330.19,7.50,11.09" target="#b7">[7]</ref><ref type="bibr" coords="6,271.40,330.19,7.50,11.09" target="#b8">[8]</ref>, <ref type="bibr" coords="6,285.19,330.19,15.30,11.09" target="#b11">[11]</ref>.</p><p>For CHIC, this task resembles a typical user of a cultural heritage information system, who would like to get an overview over what the system has with respect to a certain concept or what the best alternatives are. It is also a pilot task for this type of data collection using different assumptions about diversity or variability. Documents returned should be relevant but also as diverse as possible with respect to:  media type of object (text, image, audio, video)  content provider  query category  field match (which metadata field contains a query term) Several approaches or measures have been suggested to measure diversity in an information retrieval result set <ref type="bibr" coords="6,245.93,479.05,7.51,11.09" target="#b1">[2]</ref><ref type="bibr" coords="6,253.44,479.05,3.76,11.09" target="#b2">[3]</ref><ref type="bibr" coords="6,257.20,479.05,7.51,11.09" target="#b3">[4]</ref>, <ref type="bibr" coords="6,272.18,479.05,10.65,11.09" target="#b6">[6]</ref>, <ref type="bibr" coords="6,290.09,479.05,15.31,11.09" target="#b10">[10]</ref>, <ref type="bibr" coords="6,312.99,479.05,15.37,11.09" target="#b12">[12]</ref>. For the pilot variability task, we decided to measure cluster recall, i.e. the number of retrieved diverse categories (media type, content providers, query categories etc.) divided by the number of possible diverse categories per query. The evaluation of the results of this task was therefore two-fold. First, all returned documents were assessed for their relevance and then the cluster recall for relevant documents in the 4 categories above was determined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Semantic Enrichment</head><p>Semantic enrichment is an important task in cultural heritage information systems with short and ambiguous queries like Europeana, which will support the information retrieval process either interactively (the user is asked for clarification, e.g. "Did you mean?") or automatically (the query is automatically expanded with semantically related concepts to increase the likely search success). The semantic enrichment task required systems to present a ranked list of at most 10 related concepts for a query to semantically enrich the query and / or guess the user's information need or original query intent. For CHiC, this task resembles a typi-cal user interaction, where the system should react to an ambiguous query with a clarification request (or a result output from an expanded query).</p><p>Related concepts could be extracted from Europeana data (internal information) or from other resources in the LOD cloud or other external resources (e.g. Wikipedia). Europeana already enriches about 30% of its metadata objects with concepts, names and places (included in the test collection). It uses the vocabularies GeoNames, GEMET and DBPedia for its included semantic enrichments, which could be explored further as well.</p><p>For the semantic enrichment task, participants could also use the Europeana Linked Open Data collections. Europeana released metadata on 2.5 million objects as linked open data in a pilot project <ref type="foot" coords="7,238.08,266.87,3.24,7.17" target="#foot_1">2</ref> . The data is represented in the Europeana Data Model (RDF) and encompasses collections from ca. 300 content providers. Other external resources are allowed but need to be specified in the description from participants. The objects described in the LOD dataset are included in the Europeana test collection, but the RDF format might be convenient for accessing object enrichments.</p><p>System effectiveness was assessed in two phases. First all submitted enrichments were assessed manually for use in an interactive query expansion environment (e.g. "does this suggestion make sense with respect to the original query?").</p><p>During the second phase, the submitted terms and phrases were used in a query expansion experiment, i.e. the enrichments were added to the query and submitted as new experimental runs. All new topics were searched against the same standard Lucene indexes of the Europeana collections (according to the language of the enrichments). The results of those runs were then assessed according to ad-hoc retrieval standards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CHiC Participation and Experiments</head><p>Although 21 groups registered for participation in CHiC, only 6 research groups submitted experimental results for evaluation. Table <ref type="table" coords="7,324.95,493.21,5.01,11.09">3</ref> shows the experiment participants for CHiC.</p><p>Europeana's Solr index to retrieve results. Two multilingual Europeana experiments were submitted, one using Solr's standard vector space ranking model, the other an adapted version of the BM-25 ranking model. Table <ref type="table" coords="8,338.60,172.21,5.01,11.09" target="#tab_4">4</ref> the number of experiments per task and language. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.1</head><p>The DIRECT System DIRECT<ref type="foot" coords="8,160.80,487.37,3.24,7.17" target="#foot_2">3</ref> (Distributed Information Retrieval Evaluation Campaign Tool) has supported the different stages of the CHiC evaluation activity, from the experiment submission phase to the relevance assessment and metrics computation. DIRECT manages different types of users, i.e. participants, assessors, organizers, and visitors, who need to have access to different kinds of features and capabilities. A personal username and password has been assigned to each participant/assessor <ref type="bibr" coords="8,351.00,548.71,15.35,11.09" target="#b13">[13]</ref>.</p><p>6 Relevance Assessments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Pooling</head><p>The number of documents in large test collections such as CLEF makes it impractical to judge every document for relevance. Instead approximate recall values are calculated using pooling techniques. The results submitted by the groups participating in the tasks are used to form a pool of documents for each topic and language by collecting the highly ranked documents from selected runs according to a set of predefined criteria. One important limitation when forming the pools is the number of documents to be assessed. Traditionally, the top 100 ranked documents from each of the runs selected are included in the pool; in such a case we say that the pool is of depth 100. This pool is then used for subsequent relevance judgments. After calculating the effectiveness measures, the results are analyzed and run statistics produced and distributed. The main criteria used when constructing the pools in CLEF are:</p><p> favor diversity among approaches adopted by participants, according to the descriptions that they provide of their experiments;  for each task, include at least one experiment from every participant, selected from the experiments indicated by the participants as having highest priority;  ensure that, for each participant, at least one mandatory title+description experiment is included, even if not indicated as having high priority;  add manual experiments, when provided;  for bilingual tasks, ensure that each source topic language is represented. This year, we produced three pools, one for each target language (English, French, and German) using a depth of 100. The pools have been created using all the runs in the ad-hoc monolingual and variability tasks, two runs per participant in the ad-hoc bilingual tasks, and all the runs in the bilingual variability task. A fourth pool, for the multilingual task, is the union of the three pools described above.</p><p>Table <ref type="table" coords="9,161.82,417.25,5.01,11.09" target="#tab_5">5</ref> provides details about the created pools, their size, the number of relevant and not relevant documents, and the pooled runs. You can note that English and French pools one run was not pooled from the monolingual tasks: this is a late arriving run, submitted after the closure of the submission phase. The box plot of Fig. <ref type="figure" coords="10,208.08,373.62,5.01,9.02" target="#fig_0">4</ref> compares the distributions of the relevant documents across the topics of each pool for the different CHiC pools; the boxes are ordered by decreasing mean number of relevant documents per topic. We see that the French and German distributions appear similar and are slightly asymmetric towards topics with a greater number of relevant documents whereas the English distribution is almost balanced. All the distributions show some upper outliers, i.e. topics with a greater number of relevant documents with respect to the behavior of the other topics in the distribution. These outliers are probably due to the fact that CHiC topics have to be able to retrieve relevant documents in all the collections; therefore, they may be considerably broader than typical monolingual topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Assessment Rules</head><p>During the relevance assessment phase, all eight assessors followed the same guidelines for relevance. Unclear or ambiguous cases were discussed within the group. A final validation by one of the organizers went through all relevant documents to check for consistency among the assessments.</p><p>The following general assumption guided the decision process: a record is relevant, when it fulfills the information need represented by the original query (in title) and by the suggested information need description (in description). Three relevance criteria were defined:  Not relevant -the record does not fulfill the information need, the information is not relevant,  Relevant -the record as represented in the DIRECT system fulfills the information need,  Europeana relevant -the record only as represented in the Europeana portal fulfills the information need (only the whole Europeana record, i.e. the thumbnail and other related documents, contains enough information to make this object relevant, not just the record in the DIRECT system).</p><p>For the analysis, Europeana relevant and not relevant were counted as not relevant, the remaining documents as relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">The Assessment Interface</head><p>Figure <ref type="figure" coords="11,168.17,540.85,5.01,11.09">5</ref> shows the main assessment interface of the DIRECT framework. It provides the assessor with an overview on the status of each pool. In particular, it displays the current number of relevance judgments for each topic in a specific pool.</p><p>The assessment stage is supported by the interface shown in Figure <ref type="figure" coords="11,417.30,576.85,3.76,11.09">6</ref>. The assessor can easily navigate through the list of document for a given topic. The interface includes a set of buttons to select relevance criteria for each document (yellow color for the not assessed documents, red for not relevant documents, green for relevant documents, grey for Europeana relevant documents). The document preview displays two direct links to:</p><p>1. the original record in the Europeana website; 2. the content of the original europena_isShownAt field. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Semantic Enrichment Task</head><p>The semantic enrichment task results were first evaluated for the relevance or "semantic appropriateness" of the individual suggested terms or phrases. All enrichments for a query were looked at by the same assessor.</p><p>All submitted enrichments were assessed on a 3-point scale: definitely relevant as enrichment to the query, maybe relevant, and not relevant. If more than 10 suggestions were submitted, they were not included. If less than 10 suggestions were submitted, all suggestions were counted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7.1</head><p>Ad-hoc Information Retrieval Monolingual Experiments.</p><p>Monolingual retrieval was offered for the following target collections: English, German, and French. Table <ref type="table" coords="13,161.42,237.19,5.01,11.09" target="#tab_7">6</ref> shows the top five groups for each target collection, ordered by mean average precision. Note that only the best run is selected for each group, even if the group may have more than one top run. The table reports: the short name of the participating group; the experiment identifier; the mean average precision achieved by the experiment; and the performance difference between the first and the last participant.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bilingual Experiments.</head><p>The bilingual task was structured in three subtasks (X → DE, EN, or FR target collection). Table <ref type="table" coords="15,174.73,453.37,5.01,11.09" target="#tab_8">7</ref> shows the best results for this task. For bilingual retrieval evaluation, a common method is to compare results against monolingual baselines:  X  EN: 86.40% of best monolingual English IR system  X  DE: 63.52% of best monolingual German IR system  X  FR: 81.32% of best monolingual French IR system  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multilingual Experiments.</head><p>Table <ref type="table" coords="16,149.92,491.71,5.01,11.09" target="#tab_9">8</ref> shows the best results for this task with the same logic of Table <ref type="table" coords="16,413.34,491.71,5.01,11.09" target="#tab_7">6</ref> and<ref type="table" coords="16,437.83,491.71,3.75,11.09" target="#tab_8">7</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Variability</head><p>Unfortunately, at the time of writing, the cluster recall analysis was not completed so that only the first phase evaluation results (retrieval effectiveness in finding relevant documents) can be shown.</p><p>For now, we report precision@5 and precision@15 values. Recall that participants were asked to submit 12 results for each query, representing a Europeana result page. The calculated p@15 measure comes closes to evaluating how many relevant documents were found even though it overdraws the boundaries of the precision@k. The corrected evaluation measures will be published on the CHiC website 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monolingual Experiments.</head><p>Monolingual retrieval was offered for the following target collections: English, German, and French. Table <ref type="table" coords="17,224.37,601.39,5.01,11.09">9</ref> shows the best results for this task. 4  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bilingual and Multilingual Experiments.</head><p>Only one group (Chemnitz) submitted results for these tasks, so Table <ref type="table" coords="23,416.50,160.21,10.01,11.09" target="#tab_11">15</ref> shows the best runs without the difference to other runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Approaches</head><p>Five groups submitted experimental results for the ad-hoc experiments, two groups for the variability task, and five groups submitted experiments for the semantic enrichment task. Most groups concentrated on the monolingual tasks (mostly English), only Chemnitz participated in all monolingual, bilingual and multilingual tasks.</p><p>For the ad-hoc task, most groups used open information retrieval systems like Cheshire, Indri, Lucene (in its Chemnitz Xtrieval implementation) and Solr. Many ranking algorithms were tested: vector space, language modeling, DFR and Okapi.</p><p>For translations in the bilingual and multilingual tasks, Google Translate, Wikipedia entries (with associated translations) and Microsoft's translation service were used.</p><p>For the variability task, Chemnitz used its ad-hoc retrieval implementation to retrieve results and then used the least recently used (LRU) algorithm to prioritize documents describing different media types from different providers. UPV used different document collection fields and two approaches for retrieving diverse results: using maximal-marginal relevance (MMR) to cluster results and then use cosine similarity to select the most dissimilar documents.</p><p>For the semantic enrichment task, the most often used external source for terms was Wikipedia at different levels of detail (article titles, first paragraph, full text). Wordnet and DBpedia (two groups) were also used. Gesis also used co-occurrence analysis to add related terms from the Europeana collection itself.</p><p>More details on methodologies and approaches can be found in the working papers of the individual groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Outlook</head><p>The results of this year's pilot CHiC lab have shown that working with data from the cultural heritage domain is possible but also poses many challenges due to the ambi-guity of the users' information needs and the sparseness of the retrievable data. The preparation of new collections, the extraction of real queries and the organization of three realistic tasks with their respective evaluation measures was a challenge for organizers and participants, but it provided a lot of insight and more experience to continue this work in the next year. After reviewing the tasks, their descriptions and the results, we believe that we can work on improving the current tasks by fine-tuning both the requirements and the evaluation measures (especially in the variability and semantic enrichment tasks). For 2012, we have only used three of the 14 language subcollections that were prepared and didn't put a lot of focus on the entire collection. Using the other collections to introduce more languages into the evaluation as well as putting more focus on the entire dataset (the actual use case for the Europeana portal) are both viable directions for additional instances of this lab.</p><p>Europeana is moving towards a linked data model for its objects <ref type="foot" coords="24,395.04,302.87,3.24,7.17" target="#foot_3">5</ref> and one direction for this lab would be to combine experts from the information retrieval and linked data domains to research new retrieval approaches for this kind of data.</p><p>Finally, cultural heritage information systems are looking to incorporate more user interactions into their systems. The information retrieval evaluation field has often been criticized for viewing the viewer as outside of the scope of study. This domain and the available system (Europeana) enable us to combine and collaborate on information retrieval and information interaction research. CHiC is attempting to move towards this direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="10,171.24,683.18,252.83,9.97;10,136.86,406.80,323.66,269.64"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Distribution of the relevant Documents across the CHiC Pools.</figDesc><graphic coords="10,136.86,406.80,323.66,269.64" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="12,213.66,313.94,168.08,9.97"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. Main Assessment Interface in DIRECT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="15,147.90,400.40,299.45,9.97"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Monolingual French Top Groups. Interpolated Recall vs. Average Precision</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="16,150.90,402.74,293.45,9.97;16,124.68,425.71,345.84,11.09;16,124.68,437.71,345.88,11.09;16,124.68,449.71,95.32,11.09"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Bilingual English Top Groups. Interpolated Recall vs. Average Precision Figure 10 shows the interpolated recall vs. average precision graph for the top groups of the English bilingual tasks. Bilingual German and French had only one participant and are not shown here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="17,160.02,420.38,275.29,9.97"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Multilingual Top Groups. Interpolated Recall vs. Average Precision</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,124.68,220.22,334.83,216.17"><head>Table 1 .</head><label>1</label><figDesc>CHiC Collections by Language and Media Type.</figDesc><table coords="3,130.08,239.88,329.43,196.52"><row><cell>Language</cell><cell>Sound</cell><cell>Text</cell><cell>Image</cell><cell>Video</cell><cell>Total</cell></row><row><cell>German</cell><cell>23,370</cell><cell>664,816</cell><cell>3,169,122</cell><cell>8,372</cell><cell>3,865,680</cell></row><row><cell>French</cell><cell>13,051</cell><cell>1,080,176</cell><cell>2,439,767</cell><cell>102,394</cell><cell>3,635,388</cell></row><row><cell>Swedish</cell><cell>1</cell><cell>1,029,834</cell><cell>1,329,593</cell><cell>622</cell><cell>2,360,050</cell></row><row><cell>Italian</cell><cell>21,056</cell><cell>85,644</cell><cell>1,991,227</cell><cell>22,132</cell><cell>2,120,059</cell></row><row><cell>Spanish</cell><cell>1,036</cell><cell>1,741,837</cell><cell>208,061</cell><cell>2,190</cell><cell>1,953,124</cell></row><row><cell>Norwegian</cell><cell>14,576</cell><cell>207,442</cell><cell>1,335,247</cell><cell>555</cell><cell>1,557,820</cell></row><row><cell>Dutch</cell><cell>324</cell><cell>60,705</cell><cell>1,187,256</cell><cell>2,742</cell><cell>1,251,027</cell></row><row><cell>English</cell><cell>5,169</cell><cell>45,821</cell><cell>1,049,622</cell><cell>6,564</cell><cell>1,107,176</cell></row><row><cell>Polish</cell><cell>230</cell><cell>975,818</cell><cell>117,075</cell><cell>582</cell><cell>1,093,705</cell></row><row><cell>Finnish</cell><cell>473</cell><cell>653,427</cell><cell>145,703</cell><cell>699</cell><cell>800,302</cell></row><row><cell>Slovenian</cell><cell>112</cell><cell>195,871</cell><cell>50,248</cell><cell>721</cell><cell>246,952</cell></row><row><cell>Greek</cell><cell>0</cell><cell>127,369</cell><cell>67,546</cell><cell>2,456</cell><cell>197,371</cell></row><row><cell>Hungarian</cell><cell>34</cell><cell>14,134</cell><cell>107,603</cell><cell>0</cell><cell>121,771</cell></row><row><cell>Others</cell><cell>375,730</cell><cell>1,488,687</cell><cell>1,106,220</cell><cell>19,870</cell><cell>2,990,507</cell></row><row><cell>Total</cell><cell>455,162</cell><cell>8,371,581</cell><cell>14,304,289</cell><cell>169,899</cell><cell>23,300,932</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,136.44,489.20,253.39,41.08"><head>Table 2 .</head><label>2</label><figDesc>Language Modes for CHiC Experiments</figDesc><table /><note coords="5,136.44,506.71,106.14,11.09;5,257.70,506.64,132.13,11.16;5,136.44,519.19,91.15,11.09"><p>Possible monolingual runs DE  DE, EN  EN, FR  FR Possible bilingual runs</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,130.08,208.22,336.60,241.06"><head>Table 4 .</head><label>4</label><figDesc>CHiC Runs per Task and Language</figDesc><table coords="8,130.08,225.67,336.60,223.61"><row><cell></cell><cell>Language</cell><cell>Runs</cell><cell></cell><cell>Language</cell><cell>Runs</cell></row><row><cell>Ad-hoc</cell><cell></cell><cell></cell><cell>Variability</cell><cell></cell><cell></cell></row><row><cell cols="2">Monolingual English</cell><cell>17</cell><cell cols="2">Monolingual English</cell><cell>8</cell></row><row><cell></cell><cell>French</cell><cell>9</cell><cell></cell><cell>French</cell><cell>4</cell></row><row><cell></cell><cell>German</cell><cell>8</cell><cell></cell><cell>German</cell><cell>4</cell></row><row><cell>Bilingual</cell><cell>XEnglish</cell><cell>8</cell><cell>Bilingual</cell><cell>X  English</cell><cell>4</cell></row><row><cell></cell><cell>X French</cell><cell>4</cell><cell></cell><cell>X  French</cell><cell>4</cell></row><row><cell></cell><cell>X  German</cell><cell>4</cell><cell></cell><cell>X  German</cell><cell>4</cell></row><row><cell>Multilingual</cell><cell></cell><cell>6</cell><cell>Multilingual</cell><cell></cell><cell>4</cell></row><row><cell cols="2">Semantic Enrichment</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Monolingual English</cell><cell>17</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>French</cell><cell>4</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>German</cell><cell>8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bilingual</cell><cell>XEnglish</cell><cell>4</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>X French</cell><cell>4</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>X  German</cell><cell>4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Multilingual</cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,249.24,477.26,96.81,9.97"><head>Table 5 .</head><label>5</label><figDesc>CHiC 2012 Pools</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,130.32,496.17,334.71,193.79"><head>CHiC 2012 English Pool Size</head><label></label><figDesc></figDesc><table coords="9,130.32,506.66,334.71,183.30"><row><cell></cell><cell>Assessors</cell><cell>2</cell></row><row><cell></cell><cell>Pooled experiments / Submitted Experiments</cell><cell>out of 21</cell></row><row><cell></cell><cell>ad-hoc monolingual</cell><cell>out of 9</cell></row><row><cell>Experiments</cell><cell>ad-hoc bilingual</cell><cell>out of 4</cell></row><row><cell></cell><cell>variability monolingual</cell><cell>out of 4</cell></row><row><cell></cell><cell>variability bilingual</cell><cell>out of 4</cell></row><row><cell></cell><cell>CHiC 2012 German Pool</cell><cell></cell></row><row><cell></cell><cell>Total documents</cell><cell>22,828</cell></row><row><cell></cell><cell>Relevant documents</cell><cell>2,272</cell></row><row><cell>Size</cell><cell>Not relevant documents</cell><cell>20,556</cell></row><row><cell></cell><cell>Topics with relevant documents / Total Topics</cell><cell>out of 50</cell></row><row><cell></cell><cell>Assessors</cell><cell>2</cell></row><row><cell></cell><cell>Pooled experiments / Submitted Experiments</cell><cell>out of 20</cell></row><row><cell></cell><cell>ad-hoc monolingual</cell><cell>out of 8</cell></row><row><cell>Experiments</cell><cell>ad-hoc bilingual</cell><cell>out of 4</cell></row><row><cell></cell><cell>variability monolingual</cell><cell>out of 4</cell></row><row><cell></cell><cell>variability bilingual</cell><cell>out of 4</cell></row><row><cell></cell><cell>Total documents</cell><cell>35,161</cell></row><row><cell></cell><cell>Relevant documents</cell><cell>1,566</cell></row><row><cell></cell><cell>Not relevant documents</cell><cell>33,595</cell></row><row><cell></cell><cell>Topics with relevant documents / Total Topics</cell><cell>36 out of 50</cell></row><row><cell></cell><cell>Assessors</cell><cell>8</cell></row><row><cell></cell><cell>Pooled experiments / Submitted Experiments</cell><cell>32 out of 37</cell></row><row><cell></cell><cell>ad-hoc monolingual</cell><cell>16 out of 17</cell></row><row><cell>Experiments</cell><cell>ad-hoc bilingual</cell><cell>4 out of 8</cell></row><row><cell></cell><cell>variability monolingual</cell><cell>8 out of 8</cell></row><row><cell></cell><cell>variability bilingual</cell><cell>4 out of 4</cell></row><row><cell></cell><cell>CHiC 2012 French Pool</cell><cell></cell></row><row><cell></cell><cell>Total documents</cell><cell>22,378</cell></row><row><cell>Size</cell><cell>Relevant documents Not relevant documents</cell><cell>1,623 20,755</cell></row><row><cell></cell><cell>Topics with relevant documents / Total Topics</cell><cell>39 out of 50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="13,124.68,309.20,346.08,244.06"><head>Table 6 .</head><label>6</label><figDesc>Best monolingual Experiments and Performance Difference between best and last (up to 5) Experiment (in MAP)</figDesc><table coords="13,124.68,341.64,346.08,211.63"><row><cell>Track</cell><cell>Rank</cell><cell>Part.</cell><cell>Experiment Identifier</cell><cell>MAP</cell></row><row><cell></cell><cell>1 st</cell><cell>UPV</cell><cell>EXP_UKB_WN100</cell><cell>51.61%</cell></row><row><cell></cell><cell>2 nd</cell><cell cols="2">Chemnitz QE0X20NO</cell><cell>48.60%</cell></row><row><cell>Monolingual</cell><cell>3 rd</cell><cell cols="2">Neuchatel UNINEENEN1</cell><cell>44.87%</cell></row><row><cell>English</cell><cell>4 th</cell><cell>Gesis</cell><cell>GESIS_WIKI_ENTITY_EN_EN</cell><cell>43.96%</cell></row><row><cell></cell><cell>5 th</cell><cell>Berkeley</cell><cell>MONO_EN_TD_T2FB</cell><cell>36.40%</cell></row><row><cell></cell><cell>Diff.</cell><cell></cell><cell></cell><cell>41.78%</cell></row><row><cell>Monolingual German</cell><cell>1 st 2 nd Diff.</cell><cell cols="2">Chemnitz QE_NO Gesis GESIS_WIKI_ENTITY_DE_DE</cell><cell>60.39% 54.80% 10.20%</cell></row><row><cell></cell><cell>1 st</cell><cell cols="2">Neuchatel UNINEFRFR3</cell><cell>37.92%</cell></row><row><cell>Monolingual</cell><cell>2 nd</cell><cell cols="2">Chemnitz QE_BO2_3D_10T</cell><cell>35.90%</cell></row><row><cell>French</cell><cell>3 rd</cell><cell>Berkeley</cell><cell>MONO_FR_TD_T2FB</cell><cell>20.85%</cell></row><row><cell></cell><cell>Diff.</cell><cell></cell><cell></cell><cell>81.87%</cell></row><row><cell cols="5">Figures 7 to 9 show the interpolated recall vs. average precision for the top groups of</cell></row><row><cell cols="2">the monolingual tasks.</cell><cell></cell><cell></cell><cell></cell></row></table><note coords="14,146.70,398.90,301.91,9.97"><p>Fig. 7. Monolingual English Top Groups. Interpolated Recall vs. Average Precision</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="15,124.68,150.27,346.08,514.73"><head>Table 7 .</head><label>7</label><figDesc>Best bilingual Experiments and Performance Difference between best and last (up to 5) Experiment (in MAP)</figDesc><table coords="15,124.68,150.27,346.08,514.73"><row><cell></cell><cell cols="11">100% CHIC Ad-Hoc Monolingual French Task Top 5 Participants -Standard Recall Levels vs Mean Interpolated Precision</cell></row><row><cell></cell><cell></cell><cell cols="8">nada.naji@unine.ch [Experiment UNINEFRFR3; MAP 37.93%; Pooled]</cell><cell></cell></row><row><cell></cell><cell>90%</cell><cell cols="10">clef@tu-chemnitz.de [Experiment QE_BO2_3D_10T; MAP 35.90%; Pooled] ray@ischool.berkeley.edu [Experiment MONO_FR_TD_T2FB; MAP 20.86%; Not Pooled]</cell></row><row><cell></cell><cell>80%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>60%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell>50%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>40%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0% 0%</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell><cell>100%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Track</cell><cell>Rank</cell><cell cols="2">Part.</cell><cell></cell><cell cols="4">Experiment Identifier</cell><cell></cell><cell>MAP</cell></row><row><cell cols="2">Bilingual English</cell><cell>1 st 2 nd Diff.</cell><cell cols="8">Neuchatel UNINEDEEN3 Chemnitz FR2EN_QE_DBPEDIA_SUBJECTS_MICROSOFT</cell><cell>44.59% 35.49% 25.67%</cell></row><row><cell cols="2">Bilingual</cell><cell>1 st</cell><cell cols="8">Chemnitz FR2DE_QE_DBPEDIA_SUBJECTS_MICROSOFT</cell><cell>38.36%</cell></row><row><cell cols="2">German</cell><cell>Diff.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell></row><row><cell cols="2">Bilingual</cell><cell>1 st</cell><cell cols="8">Chemnitz DE2FR_QE_DBPEDIA_SUBJECTS_MICROSOFT</cell><cell>30.84%</cell></row><row><cell cols="2">French</cell><cell>Diff.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="16,124.68,150.20,348.37,483.64"><head>Table 8 .</head><label>8</label><figDesc>Best Multilingual Experiments and Performance Difference between best and last (up to 5) Experiment (in MAP)</figDesc><table coords="16,124.68,150.20,348.37,483.64"><row><cell></cell><cell cols="11">100% CHIC Ad-Hoc Bilingual English Task Top 5 Participants -Standard Recall Levels vs Mean Interpolated Precision</cell></row><row><cell cols="8">nada.naji@unine.ch [Experiment UNINEDEEN3; MAP 44.59%; Not Pooled]</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="12">90% clef@tu-chemnitz.de [Experiment FR2EN_QE_DBPEDIA_SUBJECTS_MICROSOFT; MAP 35.49%; Pooled]</cell></row><row><cell></cell><cell>80%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>60%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell>50%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>40%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0% 0%</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell><cell>100%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Recall</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Track</cell><cell></cell><cell>Rank</cell><cell>Part.</cell><cell></cell><cell cols="5">Experiment Identifier</cell><cell></cell><cell>MAP</cell></row><row><cell></cell><cell></cell><cell>1 st</cell><cell cols="3">Humboldt HUBEUNEW</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>23.02%</cell></row><row><cell cols="2">Multilingual</cell><cell>2 nd</cell><cell cols="6">Chemnitz FR2X_DBPEDIA_SUBJECTS_MS</cell><cell></cell><cell></cell><cell>13.33%</cell></row><row><cell></cell><cell></cell><cell>Diff.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>72.61%</cell></row><row><cell cols="12">Figure 11 shows the interpolated recall vs. average precision graph for the top partici-</cell></row><row><cell cols="4">pants of the multilingual task.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="17,129.96,150.41,342.43,543.87"><head></head><label></label><figDesc>http://www.culturalheritageevaluation.org Figures 12 to 14 show the interpolated recall vs. average precision for the top groups of the monolingual tasks.</figDesc><table coords="17,139.13,150.41,333.26,260.30"><row><cell cols="12">90% CHIC Ad-Hoc Multilingual Task Top 5 Participants -Standard Recall Levels vs Mean Interpolated Precision CHIC Semantic Enrichment Monolingual German Task Top 5 Participants -Standard Recall Levels vs Mean Interpolated Precisi 100% 100% chictest [Experiment ORIGINALQUERIESDE-se; MAP 57.01%; Not Pooled] chictest [Experiment HUBEUNEW; MAP 23.02%; Not Pooled] clef@tu-chemnitz.de [Experiment FR2X_DBPEDIA_SUBJECTS_MS; MAP 13.33%; Not Pooled] philipp.schaer@gesis.org [Experiment GESIS_WIKI_ENTITY_DE_DE-se; MAP 31.92%; Not Pooled] 90% clef@tu-chemnitz.de [Experiment CUT_T3_DE_DE_R3-se; MAP 26.00%; Not Pooled]</cell></row><row><cell></cell><cell>80% 80%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>60%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision Precision</cell><cell>50% 60% 40% 50%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>40%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30% 20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20% 10%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10% 0% 0%</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell><cell>100%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="12">0% 0% 100% CHIC Semantic Enrichment Monolingual English Task Top 5 Participants -Standard Recall Levels vs Mean Interpolated Precisio 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Recall Fig. 13. Monolingual German Top Groups. Interpolated Recall vs. Average Precision</cell></row><row><cell cols="12">chictest [Experiment ORIGINALQUERIESEN-se; MAP 34.11%; Not Pooled] nitish.aggarwal@deri.org [Experiment DERI_SE_CLEF_R1-se; MAP 30.23%; Not Pooled] CHIC Semantic Enrichment Monolingual French Task Top 5 Participants -Standard Recall Levels vs Mean Interpolated Precisio 100% 90% arantza.otegi@ehu.es [Experiment UKBWIKI-se; MAP 29.05%; Not Pooled] chictest [Experiment ORIGINALQUERIESFR-se; MAP 32.29%; Not Pooled]</cell></row><row><cell></cell><cell cols="11">80% philipp.schaer@gesis.org [Experiment GESIS_WIKI_ENTITY_EN_EN-se; MAP 23.38%; Not Pooled] clef@tu-chemnitz.de [Experiment CUT_T3_EN_EN_R1-se; MAP 10.92%; Not Pooled] 90% clef@tu-chemnitz.de [Experiment CUT_T3_FR_FR_R1-se; MAP 14.67%; Not Pooled]</cell></row><row><cell></cell><cell>80%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>60%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision Precision</cell><cell>40% 50% 60% 50%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>40%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0% 0%</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell><cell>100%</cell></row><row><cell></cell><cell>0% 0%</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>Recall 50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell><cell>100%</cell></row><row><cell cols="12">Fig. 12. Monolingual English Top Groups. Interpolated Recall vs. Average Precision Recall</cell></row><row><cell cols="12">Fig. 14. Monolingual French Top Groups. Interpolated Recall vs. Average Precision</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="23,124.68,196.22,345.97,119.13"><head>Table 15 .</head><label>15</label><figDesc>Best Bilingual and Multilingual Experiments (in MAP)</figDesc><table coords="23,124.68,217.62,345.97,97.74"><row><cell>Track</cell><cell>Rank</cell><cell>Part.</cell><cell>Experiment Identifier</cell><cell>MAP</cell></row><row><cell>Bilingual English</cell><cell>1 st</cell><cell cols="2">Chemnitz CUT_T3_DE_EN_R1-se</cell><cell>13.12%</cell></row><row><cell>Bilingual German</cell><cell>1 st</cell><cell>Chemnitz</cell><cell>CUT_T3_FR_DE_R4-se</cell><cell>00.00%</cell></row><row><cell>Bilingual French</cell><cell>1 st</cell><cell>Chemnitz</cell><cell>CUT_T3_EN_FR_R1-se</cell><cell>19.13%</cell></row><row><cell cols="2">Multilingual 1 st</cell><cell cols="2">Chemnitz CUT_T3_FR_EN_DE_R2-se</cell><cell>6.14%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,129.96,684.32,90.93,9.96"><p>http://www.europeana.eu</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="7,136.02,684.32,185.99,9.96"><p>http://pro.europeana.eu/web/guest/linked-open-data</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="8,129.96,684.32,87.82,9.96"><p>http://direct.dei.unipd.it/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="24,129.96,684.32,158.00,9.96"><p>http://pro.europeana.eu/edm-documentation</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Table 3. CHiC 2012 Participating Groups and <rs type="institution">Country Chemnitz University of Technology, Dept</rs>. of <rs type="institution">Computer Science Germany GESIS -Leibniz Institute for the Social Sciences Germany Unit for Natural Language Processing</rs>, Digital Enterprise <rs type="institution">Research Institute</rs>, <rs type="institution">National University of Ireland Ireland University</rs> of the <rs type="funder">Basque Country</rs>, <rs type="institution">UPV/EHU &amp; University of Sheffield Spain / UK School</rs> of Information at the <rs type="institution">University of California, Berkeley</rs>. <rs type="institution">USA Computer Science Department</rs>, <rs type="institution">University of Neuchatel Switzerland Humboldt Universität</rs> (one of the organizers) also submitted experiments for assessment, which can be seen as baselines, because these multilingual ad-hoc runs used Acknowledgements. This work was supported by <rs type="funder">PROMISE (</rs><rs type="programName">Participative Research Laboratory for Multimedia and Multilingual Information Systems Evaluation, Network of Excellence</rs> cofunded by the <rs type="programName">7th Framework Program</rs> of the <rs type="funder">European Commission</rs>, grant agreement no. <rs type="grantNumber">258191</rs>. We would like to thank <rs type="institution">Europeana</rs> for providing the data for collection and topic preparation and providing valuable feedback on task refinement and assessment. <rs type="person">Elaine Toms</rs> (<rs type="affiliation">University of Sheffield, UK</rs>) and <rs type="person">Birger Larsen</rs> (<rs type="affiliation">Royal School of Library and Information Science, Copenhagen, Denmark</rs>) have shaped the lab's organization from the beginning and are instrumental in integrating more interactive features into the lab's tasks in further instances. We would like to thank our external relevance assessors <rs type="person">Anthi Agoropoulou</rs>, <rs type="person">Christophe Onambélé</rs> and <rs type="person">Astrid Winkelmann</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_fBC5ARR">
					<orgName type="program" subtype="full">Participative Research Laboratory for Multimedia and Multilingual Information Systems Evaluation, Network of Excellence</orgName>
				</org>
				<org type="funding" xml:id="_Us8EwxZ">
					<idno type="grant-number">258191</idno>
					<orgName type="program" subtype="full">7th Framework Program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>--</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bilingual and Multilingual Experiments.</head><p>Only one group (Chemnitz) submitted results for these tasks, so Table <ref type="table" coords="18,416.50,312.19,10.01,11.09">10</ref> shows the best runs without the difference to other tasks. For bilingual retrieval evaluation, a common method is to compare results against monolingual baselines:</p><p> Mean of P@5 ─ X  EN: 17.83% of best monolingual English IR system ─ X  DE: 70.00% of best monolingual German IR system ─ X  FR: 87.49% of best monolingual French IR system  Mean of P@15 ─ X  EN: 74.06% of best monolingual English IR system ─ X  DE: 70.91% of best monolingual German IR system ─ X  FR: 73.14% of best monolingual French IR system </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Semantic Enrichment</head><p>We first report the overall results of the first phase evaluation of the semantic relevance (appropriateness) of the enrichments, then the overall results of the query expansion runs using the semantic enrichments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Relevance.</head><p>For the evaluation of the "semantic appropriateness" of the suggested enrichments, two relevance measures were used -definitely relevant and maybe relevant -to be able to distinguish a strict and a relaxed evaluation. Precision (strong) is the average precision (over 25 queries) of "relevant" suggestions over all suggestions. Precision (weak) is the average precision (over 25 queries) of "relevant" and "maybe relevant" over all suggestions.</p><p>Table <ref type="table" coords="19,162.18,232.21,9.99,11.09">11</ref> shows average precision numbers (over all topics and all runs) for each language mode in this task. The weaker precision measure is, as should be expected, higher than the strict precision measure, by an average of 10 percentage points. The strict precision measure shows that on average about half of the suggested terms or phrases can be considered a good fit for the query.</p><p>German monolingual suggestions seem to have a lower precision than other experiments. The reason for this is that two experiments were submitted containing errors that would assign enrichments to the wrong queries after about half of the topics. We kept the experiments in the analysis for completeness, however.</p><p>Bilingual and multilingual experiments also seem to perform better than the monolingual experiments on average. This is probably due to averaging as most of the bilingual and monolingual runs were submitted by one group (Chemnitz Univ. of Techn.), which achieved higher results.</p><p>The detailed results for every run can be found on the CHiC website. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monolingual Experiments.</head><p>Table <ref type="table" coords="19,149.94,576.67,10.02,11.09">12</ref> shows the best results for each group in this task. Only one group (Chemnitz) submitted results for these tasks, so Table <ref type="table" coords="20,416.50,362.17,10.01,11.09">13</ref> shows the best runs without the difference to other runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monolingual Experiments.</head><p>Monolingual retrieval was offered for the following target collections: English, German, and French. Table <ref type="table" coords="20,226.39,594.91,10.02,11.09">14</ref> shows the best results for this task. As can be seen, the original topic runs (without expansion) as denoted by the ORIGINALQUERIES identifier outperforms all other runs. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="24,132.68,619.22,338.01,9.96;24,141.66,630.20,328.97,9.96;24,141.66,641.18,124.99,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="24,339.31,619.22,96.82,9.96">Diversifying search results</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gollapudi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Halverson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ieong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,455.18,619.22,15.51,9.96;24,141.66,630.20,324.85,9.96">Proceedings of the Second ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Second ACM International Conference on Web Search and Data Mining<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,132.68,148.22,337.91,9.96;25,141.66,159.20,328.93,9.96;25,141.66,170.18,24.00,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="25,253.98,148.22,216.61,9.96;25,141.66,159.20,139.11,9.96">The use of MMR, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,303.95,159.20,38.78,9.96">SIGIR &apos;98</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,132.68,181.22,337.91,9.96;25,141.66,192.20,235.08,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="25,236.30,181.22,234.29,9.96;25,141.66,192.20,37.14,9.96">Less is more: probabilistic models for retrieving fewer relevant documents</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,197.43,192.20,36.39,9.96">SIGIR &apos;06</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="429" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,132.68,203.18,337.92,9.96" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="25,300.76,203.18,152.42,9.96">Overview of the TREC 2009 Web Track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboro</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="25,141.66,214.22,236.52,9.96" xml:id="b4">
	<monogr>
		<title level="m" coord="25,284.07,214.22,41.47,9.96">TREC 2009</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,132.67,225.20,337.92,9.96;25,141.66,236.18,328.96,9.96;25,141.66,247.22,134.06,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="25,201.10,236.18,208.50,9.96">Novelty and diversity in information retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ashkan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Mackinnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,429.69,236.18,37.36,9.96">SIGIR &apos;08</title>
		<meeting><address><addrLine>NewYork</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="659" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,132.67,258.20,337.97,9.96;25,141.66,269.18,89.11,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="25,176.31,258.20,113.61,9.96">TREC-6 interactive track report</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,447.68,258.20,22.96,9.96;25,141.66,269.18,16.21,9.96">TREC 1998</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page">73</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,132.67,280.22,337.88,9.96;25,141.66,291.20,123.48,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="25,200.48,280.22,194.75,9.96">Ambiguous queries: test collections need more sense</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,415.28,280.22,37.25,9.96">SIGIR &apos;08</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="499" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,132.67,302.18,337.87,9.96;25,141.66,313.22,328.93,9.96;25,141.66,324.20,136.22,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="25,308.33,302.18,162.21,9.96;25,141.66,313.22,14.21,9.96">What Else Is There? Search Diversity Examined</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Arni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,415.23,313.22,33.70,9.96">ECIR &apos;09</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Boughanem</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Berrut</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Mothe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Soule-Dupuy</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="562" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,132.67,335.18,337.94,9.96;25,141.66,346.22,328.98,9.96;25,141.66,357.20,328.92,9.96;25,141.66,368.18,46.58,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="25,259.83,335.18,210.79,9.96;25,141.66,346.22,57.54,9.96">Ambiguity of Queries and the Challenges for Query Language Detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gäde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Petras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,205.45,346.22,119.41,9.96;25,172.03,357.20,183.15,9.96">CLEF 2010 Labs and Workshops Notebook Papers</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Pianta</surname></persName>
		</editor>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09">September 2010. 2010</date>
			<biblScope unit="page" from="22" to="23" />
		</imprint>
	</monogr>
	<note>CLEF 2010 LogCLEF Workshop</note>
</biblStruct>

<biblStruct coords="25,132.30,379.22,338.27,9.96;25,141.66,390.20,172.93,9.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="25,207.04,379.22,182.54,9.96">Overview of the TREC 2004 robust retrieval track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,220.48,390.20,41.42,9.96">TREC 2004</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,132.30,401.18,338.38,9.96;25,141.66,412.22,130.18,9.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="25,202.15,401.18,206.06,9.96">Novelty and topicality in interactive information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="25,413.74,401.18,56.94,9.96;25,141.66,412.22,47.74,9.96">J. Am. Soc. Inf. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="215" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,132.30,423.20,338.40,9.96;25,141.66,434.18,291.29,9.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="25,265.56,423.20,205.14,9.96;25,141.66,434.18,103.16,9.96">Beyond independent relevance: methods and evaluation metrics for subtopic retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,262.53,434.18,36.46,9.96">SIGIR &apos;03</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,132.30,445.22,338.36,9.96;25,141.66,456.20,328.90,9.96;25,141.66,467.18,328.91,9.96;25,141.66,478.22,24.00,9.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="25,225.33,445.22,245.33,9.96;25,141.66,456.20,13.00,9.96">Towards an Evaluation Infrastructure for DL Performance Evaluation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Agosti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,330.29,456.20,140.27,9.96;25,141.66,467.18,154.04,9.96">Evaluation of Digital Libraries: An Insight to Useful Applications and Methods</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Tsakonas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Papatheodorou</surname></persName>
		</editor>
		<meeting><address><addrLine>Oxford, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Chandos Publishing</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="93" to="120" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
