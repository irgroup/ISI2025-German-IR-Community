<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,159.63,115.96,296.17,12.62;1,247.32,133.89,114.54,12.62;1,361.81,131.69,5.73,8.77">URJCyUNED at ImageCLEF 2012 Photo Annotation task ⋆</title>
				<funder ref="#_ND9s8cP">
					<orgName type="full">Ministerio de Ciencia e Innovación</orgName>
				</funder>
				<funder ref="#_ZY3B8q6">
					<orgName type="full">Education Council of the Regional Government of Madrid</orgName>
				</funder>
				<funder ref="#_ASj5mPy #_MYJKsAx">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,136.35,171.56,80.81,8.74"><forename type="first">Jesús</forename><surname>Sánchez-Oro</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Rey Juan Carlos</orgName>
								<address>
									<settlement>Móstoles</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,227.71,171.56,63.68,8.74"><forename type="first">Soto</forename><surname>Montalvo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Rey Juan Carlos</orgName>
								<address>
									<settlement>Móstoles</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,301.92,171.56,104.40,8.74"><forename type="first">Antonio</forename><forename type="middle">S</forename><surname>Montemayor</surname></persName>
							<email>antonio.sanz@urjc.esraul.cabido</email>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Rey Juan Carlos</orgName>
								<address>
									<settlement>Móstoles</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,416.84,171.56,54.95,8.74"><forename type="first">Raúl</forename><surname>Cabido</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Rey Juan Carlos</orgName>
								<address>
									<settlement>Móstoles</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,140.33,183.52,72.99,8.74"><forename type="first">Juan</forename><forename type="middle">J</forename><surname>Pantrigo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Rey Juan Carlos</orgName>
								<address>
									<settlement>Móstoles</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,223.84,183.52,74.37,8.74"><forename type="first">Abraham</forename><surname>Duarte</surname></persName>
							<email>abraham.duarte@urjc.es</email>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Rey Juan Carlos</orgName>
								<address>
									<settlement>Móstoles</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,308.76,183.52,59.21,8.74"><forename type="first">Víctor</forename><surname>Fresno</surname></persName>
							<email>vfresno@lsi.uned.es</email>
							<affiliation key="aff1">
								<orgName type="institution">Universidad Nacional de Educación a Distancia</orgName>
								<address>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,397.89,183.52,72.70,8.74"><forename type="first">Raquel</forename><surname>Martínez</surname></persName>
							<email>raquel@lsi.uned.es</email>
							<affiliation key="aff1">
								<orgName type="institution">Universidad Nacional de Educación a Distancia</orgName>
								<address>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,159.63,115.96,296.17,12.62;1,247.32,133.89,114.54,12.62;1,361.81,131.69,5.73,8.77">URJCyUNED at ImageCLEF 2012 Photo Annotation task ⋆</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EA6708728996C20DA4F132E8FA23EAB2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image classification</term>
					<term>Visual features</term>
					<term>Textual features</term>
					<term>Semantic similarity</term>
					<term>Bag of Words</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the URJCyUNED participation in the ImageCLEF 2012 Photo Annotation task. The proposed approach uses both visual image features and textual associated image information. The visual features are extracted after preprocessing the images, and the textual information are the provided Flickr user tags. The visual features describe the images in terms of color and interesting points, and the textual features make use of the semantic distance between the user tags and the concepts to annotate by using WordNet. The annotations are predicted by SVM classifiers, in some cases trained separately for each concept. The experimental results show that the best of our submissions is obtained by using only textual features.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we describe our submission to the ImageCLEF 2012 Photo Annotation task. The main aim of this task is the analysis of a set of images in order to detect one or more visual concepts. Those concepts can be used for automatically annotate the images. This year a total of 94 concepts are available, categorized as natural elements, environment, people, image elements, and human elements, each one subdivided in different sub-categories.</p><p>The goal of this task is to annotate the images with concepts detected by using both visual and textual features extracted from the images. Participants are given 15000 training images and 10000 test images. The problem can be solved using three different methods: using visual information only, using textual information only, and a hybrid approach that involves the fusion of visual and textual information.</p><p>This year a set of visual and textual features are provided by the ImageCLEF organization. On the one hand, the visual features include SIFT, C-SIFT, RGB-SIFT, OPPONENT-SIFT, SURF, TOP-SURF and GIST. On the other hand the textual features contain Flickr user tags, EXIF metadata and user information and Creative Commons license information. More details of the task and the features provided can be found in <ref type="bibr" coords="2,284.63,202.68,14.62,8.74" target="#b10">[11]</ref>.</p><p>However this work only uses the Flickr user tags as textual information and the visual features are extracted by our own methods. Specifically, we extract visual features and textual features to create a global descriptor for the images. Visual features are mostly based on the color and interesting points of the images, while textual features use a similarity measure to compare the Flickr user tags with the concepts and their synonyms in the WordNet lexical database.</p><p>Analyzing last year works, most used visual features involved Bag Of Words as well as SIFT and color features <ref type="bibr" coords="2,289.33,298.33,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="2,301.50,298.33,12.73,8.74" target="#b11">12,</ref><ref type="bibr" coords="2,315.90,298.33,11.63,8.74" target="#b9">10]</ref>. The textual features were mostly based on similarity metrics between concepts and Flickr user tags and tags enrichment <ref type="bibr" coords="2,177.56,322.24,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="2,189.75,322.24,7.75,8.74" target="#b8">9,</ref><ref type="bibr" coords="2,199.15,322.24,11.63,8.74" target="#b13">14]</ref>. In our approach a preprocessing of the image is added to the visual feature extraction. This preprocessing is based on the change of the resolution and the removal of a percentage of the external part of the image.</p><p>The rest of the paper is organized as follows. Section 2 describes the visual and textual features proposed in this work. Section 3 presents the results of the submissions. Finally, Section 4 draws the conclusions of the work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Features</head><p>Our approach uses visual image features and Flickr user tags as textual associated information. We did not use other textual features as EXIF metadata or user and license information. Sections 2.1 and 2.2 present, respectively, the visual and textual features considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual Features</head><p>The visual features proposed in this paper describe the images in terms of color and interesting points. The descriptor is created by joining all the features into one feature vector that is later used as input to the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Color quantization</head><p>The color histogram for an image is a representation of the color distribution of the image. In this work we use the histogram of the RGB color space, in which each component is represented by three values, namely R (red), G (green) and B (blue). The RGB color histogram is three-dimensional, and each dimension is divided in N bins, being N the only parameter of the histogram. Relying in a comparison of several values for N , we have selected a value of 32. This means that each dimension of the histogram is divided into 32 discrete intervals. Once the histogram has been generated, the feature extracted consists of the two most repeated bins in the image, setting up a vector of six components (R1,G1,B1,R2,G2,B2). We select the most repeated colors under the assumption that those are the most relevant colors for the images. Figure <ref type="figure" coords="3,134.76,154.86,4.98,8.74" target="#fig_0">1</ref> shows an example of the quantization of an image of the training set into 32 bins. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Horizontal and vertical edges</head><p>The edges of an image define the shape of the figures depicted in it. High frequency signals carry the most part of the information, so it seems reasonable to have a measure of edges. For that reason, edges have been commonly used in image classification in different ways. This work proposes the use of edges defining an image as the percentage of pixels belonging to horizontal and vertical edges respectively. The edges are extracted using the very common Canny edge detector <ref type="bibr" coords="3,342.61,429.12,9.97,8.74" target="#b1">[2]</ref>. With this information, the feature used is a two component vector that contains vertical and horizontal edge pixels percentages. More on edge analysis will be included with the use of Histogram of Oriented Gradients for a number of objects.</p><p>Grey color percentage Some of the concepts proposed this year are usually suggested by photographs that are partial or totally black and white pictures (melancholic, gray color, scary). Consequently, we propose an additional feature based on the percentage of gray pixels in the image. The selected pixels are not only those which are purely gray (i.e., an RGB code where R=G=B), but also those which can be considered gray within a threshold.</p><p>Face detection An adult human brain contains highly specialized neurons for the recognition of human faces. Moreover, some of the concepts contain some words that are related to people (family-friends, quantity, coworkers, . . . ). It would be interesting to have a face detector which can difference between pictures with and without faces, and, therefore, with and without people. The detector uses the Viola-Jones method <ref type="bibr" coords="3,266.71,644.17,15.50,8.74" target="#b12">[13]</ref> and later improved by Lienhart and Maydt <ref type="bibr" coords="3,134.76,656.12,10.52,8.74" target="#b5">[6]</ref> to store Haar features obtained from an image, using the integral image.</p><p>The method uses AdaBoost to combine several weak classifiers, resulting in a strong classifier. The output of the algorithm is the location of each face and its bounding box, but this work uses only the number of faces in the image. Figure <ref type="figure" coords="4,134.76,154.86,4.98,8.74">2</ref> shows the face detection over a training image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. Example of a face detection in a training image</head><p>Bag of Words Analyzing the previous ImageCLEF results <ref type="bibr" coords="4,399.70,393.11,10.52,8.74" target="#b0">[1]</ref> we can see that Bag Of Words (BOW) is one of the most extended features. The BOW method relies under the assumption that the spatial relationships of the key points in the image lacks of importance. Those key points can be obtained by using some wellknown descriptors like SURF or SIFT, for instance. The features are vectors of real numbers, and the construction of a dictionary from all the features obtained from the training set directly can be unaffordable, both in time and in memory. The solution is based on the use of a limited number of feature vectors which represent the feature space well for constructing a dictionary, which is usually carried out with k-means clustering. Once the dictionary has been constructed, the new images can be described by extracting their features and matching them with the features in the dictionary which are closest <ref type="bibr" coords="4,365.00,524.62,9.97,8.74" target="#b2">[3]</ref>.</p><p>The k-means algorithm used in this work is k-means++, which uses a heuristic for choosing good initial cluster centers, instead of the random centers chosen by the standard k-means. The encode of the images can be divided in three steps: feature detection, feature extraction and descriptor matching. The feature detection step identifies the keypoints, which are later extracted in a preset format in the feature extraction stage. Finally, the descriptor matching step matches the features extracted to features in the dictionary to construct the BOW representation of the image. We use the Good Features To Track (GFTT) detector implemented in OpenCV as feature extractor. It uses the Shi-Tomasi corner detector to detect keypoints in the images. Figure <ref type="figure" coords="4,340.45,644.17,4.98,8.74" target="#fig_1">3</ref> shows an example of the detection of keypoints using GFTT. The feature descriptor uses SURF as descriptor format and the descriptor matcher is FLANN based. FLANN (Fast Library for Approximate Nearest Neighbors) is a library that contains a collection of algorithms optimized for fast neighbor search. The keypoints are clustered into 50 groups, resulting in a vector of 50 elements per image. The HoG templates available covers 12 of the 94 concepts of the task. The feature obtained with this method is a boolean value that indicates whether the concept appears in the image or not. Figure <ref type="figure" coords="5,331.02,558.10,4.98,8.74" target="#fig_2">4</ref> shows the template for a bicycle and an example of the detection in one of the training images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Resolution</head><p>We propose the extraction of the previous features for different image resolutions. Reducing the spatial resolution of an image helps to reduce some unimportant details as well as noise. On the one hand, the histogram, the edges, the gray color, and the face detection work best at half resolution. On the other hand, BOW and HoG are better at the original resolution of the image. We take a trade off after experimentation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Textual Features</head><p>Our approach consists of building a text representation from Flickr user tags and using the lexical database WordNet <ref type="bibr" coords="6,289.94,656.12,9.97,8.74" target="#b7">[8]</ref>. We think these text features would allow capturing part of the meaning of the images, so that they could provide valuable information for the concept annotation process that is not easy to extract from the image features.</p><p>WordNet is used to enrich the concepts to be tagged with synonyms. After this enrichment procedure the number of total concepts is 310. This way each image is represented by means of a vector of those 310 components that represents the semantic distance between the image tags and the concepts. Then, the vector components are the minimal semantic distances between the word tags of each image and the concept or synonym of the vector representation.</p><p>Since the Flickr user tags are written in different languages, it is necessary translate them into the same language. As WordNet is in English, this is the selected axis language. For each image and each word tag we identify whether the word is written in English or not; and if it is not, it is translated into English by means of bilingual dictionaries. The word tags that do not have translation are discarded. After the elimination of the stop-words, we calculate the semantic distance matrix between the final word tags of each image and the 310 concepts using WordNet and the Leacock-Chodorow semantic measure <ref type="bibr" coords="7,405.59,310.60,9.97,8.74" target="#b4">[5]</ref>. The Leacock-Chodorow measure is based on path length, where the similarity between two concepts c 1 and c 2 is as follows:</p><formula xml:id="formula_0" coords="7,206.60,355.58,273.99,28.24">sim LCH (c 1 , c 2 ) = -log length(c 1 , c 2 ) 2 × max c∈W ordN et depth(c) (1)</formula><p>That is, the number of nodes along the shortest path between them, divided by two times the maximum depth of the hierarchy (from the lowest node to the top in the taxonomy in which c 1 and c 2 occur).</p><p>Several works in the ImageCLEF 2011 also took into account text features and semantic distances <ref type="bibr" coords="7,240.36,439.70,15.50,8.74" target="#b13">[14,</ref><ref type="bibr" coords="7,257.52,439.70,7.01,8.74" target="#b6">7]</ref>, using different semantic measures and different semantic resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Evaluation</head><p>The experimental computation has been carried out in an Intel Core i7-2600 3.40 GHz with 3 GB RAM. We have submitted a total of four runs: one visual, one textual and two mixed runs. Figure <ref type="figure" coords="7,336.34,532.97,4.98,8.74" target="#fig_4">6</ref> shows the steps followed in the classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Submitted runs</head><p>-Visual run: Independent SVM classifiers are used for each concept. This give us the opportunity of using only the best combination of features for each concept, instead of using all the features for all concepts, which could eventually end in worse results. The features are divided in three main groups. The first group contains the color features (histogram and gray color percentage) and the face detector, the second one refers to the edges percentage and the last one contains the bag of words representation. In a preliminary experiment, we tested all combinations of these groups for each concept, storing the best combination for each one.</p><p>-Textual run: In textual run we also use a different SVM classifier for each concept. Specifically, each classifier uses as input vector the semantic similarity values including only the concept that is being evaluated and its synomyms (described in 2.2), but rejecting the other concepts and their synonyms.</p><p>-"Best" combination: This run uses the results obtained in the visual and textual runs. In a previous experiment we have identified in which concepts visual features are better than textual features, so this run selects the best option for each concept, according to the results of the previous experiments. -"Or" combination: This run uses the results obtained in the visual and textual runs. Concretely, it marks a concept as relevant for an image if at least one of the classifiers (visual or textual) has marked it as relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>The evaluation of the submission is based on three measures: Mean interpolated Average Precision (MiAP), Geometric Mean interpolated Average Precision (GMiAP) and F1 measure (F-ex). The MiAP value is the average of the average interpolated precisions over all concepts, and the GMiAP gives more importance to the most difficult concepts. Both are measures defined in Information Retrieval context. The F-ex is a metric that uses the binary score to determine how well the annotations are, and it is mostly used in automatic classification problems, where the confidence scores are not commonly used to rank predictions. It is computed by determining the number of true positives, false positives, true negatives and false negatives in terms of detected concepts. Our submissions were obtained by using a binary classifier, and the confidence score for all evaluation are equal to 1. As MiAP and GMiAP are very dependent on the confidence scores, our MiAP and GMiAP results are not representative for the quality of the results (0.0622 and 0.0254, respectively, in all submissions). In previous experiments with the training data, and using the MAP of the previous year, we were able to obtain a maximum MAP of 0.20, so the final results have been unexpected. The results of the present competition could be affected due to the change on the evaluation method. However, the main problem has been the confidence score, that, in our case, has been always 1, because of the classifier used.</p><p>Table <ref type="table" coords="9,176.43,214.80,4.98,8.74">1</ref> shows the best run of each group ordered by the F-ex measure as well as our 4 different runs. It can be seen that our textual submission is in position 13 out of 18 groups. Moreover, the rest of our submissions would be placed in position 16. Taking into account all the runs submitted by the participants our submissions would be placed in 49, 59, 60 and 64 position out of the 79 runs (and taking into account only the F-ex measure, the only representative for our evaluation).</p><p>The textual run is the best run submitted, and the visual run is the worst one. Our own ranking has been also a surprising result for us, as in the previous experimentation the visual runs had always better scores than the textual ones. Our low results on the visual runs are probably due to the structure of the image descriptor. In spite of the features extracted are very representative, the bad use of them in the classification has led us to a lower F-ex value than the one expected.</p><p>Finally, Table <ref type="table" coords="10,214.94,119.00,4.98,8.74" target="#tab_0">2</ref> shows the average F-ex of all runs of each group ordered in descending order. We can see that our submissions (URJCyUNED) are in position 15 out of 18 groups, with a F-ex average of 0.2529. Overall results for all de groups participants and the experimental setup can be found in <ref type="bibr" coords="10,187.92,441.40,14.62,8.74" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper we describe our first participation in the ImageCLEF 2012 Photo Annotation task. We have used multiple visual features for representing the images, and also textual information, expecting that this information can be used to improve the performance of visual features. Some of the visual features have been defined taking into account the categories of concepts to extract relevant characteristics for the classification. On the other hand, we have used the Flickr user tags to measure the semantic distance between them and the concepts and their synonyms extracted from WordNet. Linear SVM classifiers have been used for the image classification in all submissions.</p><p>The evaluation results showed that the best of our submissions is obtained by using textual features only, with a F-ex of 0.35, followed by the multimodal run in which we choose the feature (visual or textual) that have been experimentally better in each concept, obtaining a F-ex of 0.231. Analyzing those results it is easy to see that the combination of visual and textual features has made the evaluation worse. This is probably caused by the methods of combination that we have chosen. Specifically, we have given the same importance to both features, textual and visual. That kind of combination makes the evaluation almost an average of both methods (textual and visual), which have lead us to a worse F-ex value.</p><p>As we can see in the results, visual features classification has obtained lower F-ex values than textual. This result is due to the image descriptor and the classifier used, as the visual features have been experimentally tested. Our main problem seems to be that we have probably chosen a bad descriptor for each visual feature extracted, which has ended in a bad classification result according to our expectation and preliminary experimentation.</p><p>Due to the selected classifier, the output for the confidence score is always 1, which means that the classifier is always sure of the presence of the concept in the image. That result has strongly penalized us in terms of MiAP and GMiAP. But the results obtained in the F-ex metric demonstrate that our proposal is competitive and not as bad as it seems to be according to the MiAP and GMiAP metrics. The main aim of future works is the choice of a better and non-binary classifier, as well as the improvement of the visual and textual features representation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,208.33,303.83,198.67,7.89;3,152.49,196.60,138.97,92.84"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Color quantization of an image in 32 bins</figDesc><graphic coords="3,152.49,196.60,138.97,92.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,175.49,380.30,264.35,7.89;5,170.57,183.88,273.12,182.52"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Interesting points detected using Good Features To Track</figDesc><graphic coords="5,170.57,183.88,273.12,182.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,219.46,298.61,176.42,7.89;6,242.95,176.31,128.69,108.01"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. HoG bicycle template and detection</figDesc><graphic coords="6,242.95,176.31,128.69,108.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,184.01,573.36,247.29,7.89;6,162.89,458.60,132.54,88.43"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Original training image (a) and frame elimination (b)</figDesc><graphic coords="6,162.89,458.60,132.54,88.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,178.01,279.41,259.30,7.89"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. General scheme of the image concept annotation process</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="10,134.76,176.48,345.78,216.46"><head>Table 2 .</head><label>2</label><figDesc>Results of the average of all runs of each group ordered by the F-ex measure</figDesc><table coords="10,389.56,176.48,20.68,7.89"><row><cell>F-ex</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>⋆ This work has been part-funded by the <rs type="funder">Education Council of the Regional Government of Madrid</rs>, <rs type="projectName">MA2VICMR</rs> (<rs type="grantNumber">S-2009/TIC-1542</rs>), and the research projects <rs type="projectName">Holopedia</rs> and <rs type="projectName">SAMOA3D</rs>, funded by the <rs type="funder">Ministerio de Ciencia e Innovación</rs> under grants <rs type="grantNumber">TIN2010-21128-C02</rs> and <rs type="grantNumber">TIN 2011-28151</rs> respectively.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_ZY3B8q6">
					<idno type="grant-number">S-2009/TIC-1542</idno>
					<orgName type="project" subtype="full">MA2VICMR</orgName>
				</org>
				<org type="funded-project" xml:id="_ASj5mPy">
					<orgName type="project" subtype="full">Holopedia</orgName>
				</org>
				<org type="funded-project" xml:id="_ND9s8cP">
					<idno type="grant-number">TIN2010-21128-C02</idno>
					<orgName type="project" subtype="full">SAMOA3D</orgName>
				</org>
				<org type="funding" xml:id="_MYJKsAx">
					<idno type="grant-number">TIN 2011-28151</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head><p>MiAP GMiAP F-ex Features LIRIS ECL 0.4367 0.3877 0.5766 Multimodal DMS. <ref type="bibr" coords="9,166.18,425.97,21.25,7.86">MTA</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="11,158.57,386.25,321.91,7.86;11,167.13,397.21,313.46,7.86;11,167.13,408.17,231.23,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,327.17,386.25,153.31,7.86;11,167.13,397.21,313.46,7.86;11,167.13,408.17,17.24,7.86">The joint submission of the TU Berlin and Fraunhofer FIRST (TUBFI) to the ImageCLEF2011 Photo Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,191.74,408.17,170.76,7.86">CLEF (Notebook Papers/Labs/Workshop)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,158.57,418.74,321.97,7.86;11,167.13,429.70,202.32,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,217.84,418.74,174.00,7.86">A computational approach to edge detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,397.99,418.74,82.56,7.86;11,167.13,429.70,136.15,7.86">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,158.57,440.28,321.88,7.86;11,167.13,451.24,313.40,7.86;11,167.13,462.20,108.70,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,423.65,440.28,56.81,7.86;11,167.13,451.24,120.53,7.86">Visual categorization with bags of keypoints</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,294.18,451.24,186.35,7.86;11,167.13,462.20,23.95,7.86">Workshop on Statistical Learning in Computer Vision</title>
		<imprint>
			<publisher>ECCV</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,158.57,472.78,321.94,7.86;11,167.13,483.74,255.30,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,257.54,472.78,219.02,7.86">Histograms of oriented gradients for human detection</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,167.13,483.74,171.33,7.86">Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="886" to="893" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,158.57,494.32,321.95,7.86;11,167.13,505.28,313.42,7.86;11,167.13,516.24,56.86,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="11,296.56,494.32,183.96,7.86;11,167.13,505.28,139.58,7.86">Combining Local Context and WordNet Similarity for Word Sense Identification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Chodorow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="265" to="283" />
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,158.57,526.81,321.96,7.86;11,167.13,537.77,199.02,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,282.53,526.81,198.00,7.86;11,167.13,537.77,67.67,7.86">An Extended Set of Haar-like Features for Rapid Object Detection</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Maydt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,242.01,537.77,44.78,7.86">IEEE ICIP</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="900" to="903" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,158.57,548.35,321.91,7.86;11,167.13,559.31,313.44,7.86;11,167.13,570.27,122.67,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,453.10,548.35,27.38,7.86;11,167.13,559.31,216.81,7.86">LIRIS-Imagine at ImageCLEF 2011 Photo Annotation task</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandréa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,391.76,559.31,88.82,7.86;11,167.13,570.27,86.87,7.86">CLEF (Notebook Papers/Labs/Workshop)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,158.57,580.85,321.92,7.86;11,167.13,591.81,98.05,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,223.39,580.85,157.37,7.86">WordNet: a lexical database for English</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,387.51,580.85,92.98,7.86;11,167.13,591.81,18.24,7.86">Communications of the ACM</title>
		<imprint>
			<biblScope unit="issue">38</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,158.57,600.02,321.99,10.23;11,167.13,613.35,313.44,7.86;11,167.13,624.31,122.67,7.86" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>K 'uhhirt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Wolter</surname></persName>
		</author>
		<title level="m" coord="11,413.24,602.39,67.32,7.86;11,167.13,613.35,313.44,7.86;11,167.13,624.31,86.87,7.86">The Fraunhofer IDMT at ImageCLEF 2011 Photo Annotation Task</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>CLEF (Notebook Papers/Labs/Workshop)</note>
</biblStruct>

<biblStruct coords="11,158.23,634.88,322.31,7.86;11,167.13,645.84,313.43,7.86;11,167.13,656.80,28.16,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="11,248.01,634.88,232.52,7.86;11,167.13,645.84,122.78,7.86">Semantic Contexts and Fisher Vectors for the ImageCLEF 2011 Photo Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>CLEF</orgName>
		</respStmt>
	</monogr>
	<note>Notebook Papers/Labs/Workshop</note>
</biblStruct>

<biblStruct coords="12,158.23,119.68,322.28,7.86;12,167.13,130.64,269.65,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,287.53,119.68,192.98,7.86;12,167.13,130.64,123.36,7.86">Overview of the ImageCLEF 2012 Flickr Photo Annotation and Retrieval Task</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,297.86,130.64,103.76,7.86">CLEF 2012 working notes</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,158.23,141.60,322.25,7.86;12,167.13,152.55,313.44,7.86;12,167.13,163.51,122.67,7.86" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="12,348.93,141.60,131.55,7.86;12,167.13,152.55,208.35,7.86">The University of Amsterdam&apos;s Concept Detection System at ImageCLEF 2011</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Notebook Papers/Labs/Workshop</publisher>
		</imprint>
	</monogr>
	<note type="report_type">CLEF</note>
</biblStruct>

<biblStruct coords="12,158.23,174.47,322.29,7.86;12,167.13,185.43,150.31,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,272.93,174.47,207.59,7.86;12,167.13,185.43,62.69,7.86">Rapid Object Detection using a Boosted Cascade of Simple Features</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,236.95,185.43,51.81,7.86">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,158.23,196.39,322.29,7.86;12,167.13,207.35,313.43,7.86;12,167.13,218.31,122.67,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="12,359.71,196.39,120.81,7.86;12,167.13,207.35,194.36,7.86">CEA LIST&apos;s Participation to Visual Concept Detection Task of ImageCLEF</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Znaidia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Le Borgne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<publisher>Notebook Papers/Labs/Workshop</publisher>
		</imprint>
	</monogr>
	<note type="report_type">CLEF</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
