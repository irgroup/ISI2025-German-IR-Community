<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.88,115.96,339.59,12.62;1,291.37,133.89,32.61,12.62">Overview of the ImageCLEF 2012 Robot Vision Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,150.30,171.56,98.58,8.74"><forename type="first">Jesus</forename><surname>Martinez-Gomez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Castilla-La Mancha Albacete</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,259.45,171.56,88.46,8.74"><forename type="first">Ismael</forename><surname>Garcia-Varea</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Castilla-La Mancha Albacete</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,377.84,171.56,70.76,8.74"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Idiap Research Institute</orgName>
								<address>
									<addrLine>Centre Du Parc, Rue Marconi 19</addrLine>
									<postBox>P.O. Box 592</postBox>
									<postCode>CH-1920</postCode>
									<settlement>Martigny</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.88,115.96,339.59,12.62;1,291.37,133.89,32.61,12.62">Overview of the ImageCLEF 2012 Robot Vision Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DC1005DAC4FAA67583AA43D74EC458CA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes the RobotVision@ImageCLEF 2012 challenge, which addresses the problem of multimodal place classification. Participants of the challenge were asked to classify rooms on the basis of image sequences captured by cameras mounted on a mobile robot. The proposals of the participants had to answer the question "where are you?" (I am in the elevator, in the toilet, etc) when presented with a test sequence, acquired within the same building and floor but with different lighting conditions than the training sequence. The 2012 edition of the challenge introduced the use of depth images in addition to visual images. Moreover, several techniques for feature extraction and cue integration were also proposed. As in previous editions, two different tasks were proposed: task 1 and task 2. In task 1 (mandatory) participants were asked to classify the frames separately, while the temporal continuity of the image sequence could only be exploited in task 2 (optional). Eight different groups participated to the 2012 edition of the Robot Vision challenge. The winner in both tasks was the Centro de Investigación en Informática para la Ingeniería (CIII), from the Universidad Tecnológica Nacional, Argentina (CIII UTN FRC). This participant obtained an overall score of 2071 (84.70% of the maximum score) in task 1 and 3930 (96.35% of the maximum score) in task 2.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ImageCLEF 2012 Robot Vision challenge has been the fourth edition of a competition <ref type="bibr" coords="1,190.63,559.89,15.50,8.74" target="#b9">[10]</ref> that started in 2009 within the ImageCLEF 1 as part of the Cross Lange Evaluation Forum (CLEF) Initiative 2 . Since its origin, the Robot This work was supported by the SNSF project MULTI (B. C.), and by the European Social Fund (FEDER), the Spanish Ministerio de Ciencia e Innovacion (MICINN), and the Spanish "Junta de Comunidades de Castilla-La Mancha" (MIPRCV Consolider Ingenio 2010 CSD2007-00018, TIN2010-20900-C04-03, PBI08-0210-7127 and PPII11-0309-6935 projects, J. M.-G. and I. G.-V.) 1 http://imageclef.org/ 2 http://www.clef-initiative.eu// Vision task has been addressing the problem of place classification for mobile robot localization.</p><p>The 2009@ImageCLEF edition of the task <ref type="bibr" coords="2,340.17,159.47,9.96,8.74" target="#b8">[9]</ref>, with 7 participating groups, defined some details that have been maintained for all the following editions. Participants were given training data consisting of sequences of frames recorded in indoor environments. These training frames were labelled with the name of the rooms they were acquired from. The task consisted on building a system capable to classify test frames using as class the name of the rooms previously seen. Moreover, the system could refrain from making a decision in the case of lack of confidence. Two different subtasks were then proposed: obligatory and optional. The difference between both subtasks was that the temporal continuity of the test sequence could only be exploited in the optional task. The score for each participant submission was computed as the sum of the frames that were correctly labelled minus a penalty that was applied to the frames that were misclassified. No penalties were applied for frames not classified.</p><p>In 2010, two editions of the challenge took place. The second edition of the task, 2010@ICPR <ref type="bibr" coords="2,214.70,343.41,10.52,8.74" target="#b6">[7]</ref> was held in conjunction with ICPR 2010. 9 groups participated to this edition, which introduced the use of stereo images and two types of different training sequences (easy and hard) that had to be used separately. The 2010@ImageCLEF edition <ref type="bibr" coords="2,272.80,379.27,9.96,8.74" target="#b7">[8]</ref>, with 7 participating groups, was focused on generalization: several areas could belong to the same semantic category.</p><p>Several changes have been proposed for the ImageCLEF 2012 Robot Vision task. Firstly, stereo images have been replaced by images acquired using two types of camera: a perspective camera for visual images and a depth camera (the Microsoft Kinect sensor) for range images. Therefore, each frame consists of two types of images and the challenge is focused on the problem of multimodal place classification. In addition to the use of depth images, the optional task contains kidnappings and no unknown rooms appear in the test sequences. Moreover, several techniques for features extraction and cue integration have been proposed to the participants.</p><p>We received a total of 23 runs from 8 different groups. 18 runs were submitted to the task 1 (mandatory) and 5 to the task 2 (optional). The best result in both tasks was obtained by the Centro de Investigación en Informática para la Ingeniería (CIII), from the Universidad Tecnológica Nacional, Argentina (CIII UTN FRC).</p><p>The rest of the paper details the challenge and is organized as follows: Section 2 describes the 2012 ImageCLEF edition of the RobotVision task. Section 3 presents all the participants groups, while the results are reported in Section 4. Finally, in Section 5, conclusions are drawn and future work is outlined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The RobotVision Task</head><p>This section describes the details concerning the setup of the ImageCLEF 2012 Robot Vision task. Section 2 gives a description of the task. Section 2.1 describes all the sequences of frames provided for training and test while the two subtasks are explained in Section 2.2. The performance evaluation is detailed in Section 2.3 and finally, Section 2.4 describes the information provided by the organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Description</head><p>The fourth edition of the Robot Vision challenge was focused on the problem of multi-modal place classification. Participants were asked to classify functional areas on the basis of image sequences, captured by a perspective camera and a Kinect mounted on a mobile robot (see Fig. <ref type="figure" coords="3,329.54,279.98,4.43,8.74" target="#fig_0">1</ref>) within an office environment. Participants had available visual images and range images that could be used to generate 3D point cloud files. The difference between visual images, range images and 3D point cloud files can be observed in Figure <ref type="figure" coords="3,390.95,560.24,3.87,8.74" target="#fig_1">2</ref>. Training and test sequences were acquired within the same building and floor but with some variations in the lighting conditions or the acquisition procedure (clockwise and counter clockwise).</p><p>Two different tasks were considered in the Robot Vision challenge: task 1 and task 2. For both tasks, participants should be able to answer the question "where are you?" when presented with a test sequence imaging a room category already seen during training. The difference between both tasks was the presence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Image</head><p>Range Image 3D point cloud file (or lack) of kidnappings in the final test sequence, and the availability on the use of the temporal continuity of the sequence. The kidnapping (only task 2) is affected by the robot changing room. Room changes in sequences without kidnappings were usually represented by a small number of images showing a smooth transition. On the other side, room changes with kidnappings were represented by a drastic change for frames, as can be observed in Figure <ref type="figure" coords="4,218.87,326.24,3.87,8.74" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Data</head><p>Training and validation sequences consisted of a set of the Robot Vision VIDA dataset. VIDA is a dataset with images acquired within an indoor environment using a robot platform in the IDIAP research building. This dataset contains sequences of several rooms belonging to different room categories such as "Corridor" or "Toilet". Sequences were acquired using two cameras: a perspective visual camera and a 3D range laser sensor Kinect camera. Therefore, there are  two different types of images: RGB images and depth images.</p><p>Three different sequences of frames were provided for training and two additional ones for the final experiment. All training frames were labelled with the name of the room they were acquired from. There were 9 different categories of rooms, and the distribution for the training sequences can be observed in Table <ref type="table" coords="5,475.61,356.53,4.98,8.74" target="#tab_0">1</ref> The difference between all the room categories can be observed in Figure <ref type="figure" coords="5,472.84,369.37,3.87,8.74" target="#fig_3">4</ref>, where an exemplar visual image for each one of the 9 room categories is shown.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Subtasks</head><p>All the participants of the ImageCLEF 2012 Robot Vision task were allowed to submit their runs to two different subtasks: task1 and task2.</p><p>Task 1 This task was mandatory and the test sequence had to be classified without using the temporal continuity of the sequence. Therefore, the order of the test frames cannot be taken into account. Moreover, there were not kidnappings in the final test sequence.</p><p>Task 2 This task was optional and participants could take advantage of the temporal continuity of the test sequence. There were kidnapping in the final test sequences that allowed participants to obtain additional points when they were managed correctly</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Performance Evaluation</head><p>The proposals of the participants were compared using the score obtained by their submissions. These submissions were the classes or room categories assigned to the frames of the test sequences, and the score was computed using the rules that are shown in Table <ref type="table" coords="6,237.72,370.27,3.87,8.74">2</ref>. Due to wrong classifications obtaining negative points, participants were allowed to not classify test frames.</p><p>Table <ref type="table" coords="6,219.65,413.89,4.13,7.89">2</ref>. Rules used to calculate the final score for a run.</p><p>Each correctly classified frame +1 points Each misclassified frame -1 points Each frame that was not classified +0 points (Task 2) All the 4 frames correctly classified after a kidnapping +1 points (additional)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Additional information provided by the organization</head><p>We proposed the use of several techniques for features extraction (PHOG and NARF) and cue integration (OBSCURE). Thanks to the use of these techniques, participants could focus on the development of new features while using the proposed method for cue integration or vice versa. We also provided information as the point cloud library <ref type="bibr" coords="6,248.08,589.37,15.50,8.74" target="#b10">[11]</ref> and a basic technique for taking advantage of the temporal continuity <ref type="foot" coords="6,221.46,599.75,3.97,6.12" target="#foot_0">3</ref> . In order to evaluate this information, we submitted two runs (task 1 and task 2) that were obtained using only the provided techniques. The results obtained with such proposal <ref type="bibr" coords="6,310.55,625.23,10.52,8.74" target="#b2">[3]</ref> can be considered as baseline results that all the groups were expected to improve.</p><p>Visual Features PHOG features are histogram-based global features that combine structural and statistical approaches. Other descriptors similar to PHOG that could also be used are: Sift-based Pyramid Histogram Of visual Words (PHOW) <ref type="bibr" coords="7,179.16,154.86,9.96,8.74" target="#b0">[1]</ref>, Pyramid histogram of Local Binary Patterns (PLBP) <ref type="bibr" coords="7,443.52,154.86,9.96,8.74" target="#b3">[4]</ref>, Self-Similarity-based PHOW (SS-PHOW) <ref type="bibr" coords="7,303.86,166.81,14.61,8.74" target="#b11">[12]</ref>, and Compose Receptive Field Histogram (CRFH) <ref type="bibr" coords="7,208.69,178.77,9.96,8.74" target="#b1">[2]</ref>.</p><p>Depth Features NARF features is a novel descriptor technique that has been included in the point cloud library <ref type="bibr" coords="7,289.27,224.42,14.61,8.74" target="#b10">[11]</ref>. The number of descriptors that can be extracted from a range image is not fixed, in the same manner as SIFT points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cue Integration</head><p>The algorithm proposed for cue integration was the Online-Batch Strongly Convex mUlti keRnel lEarning (OBSCURE) <ref type="bibr" coords="7,412.57,282.02,9.96,8.74" target="#b5">[6]</ref>. This SVMbased multiclass learning algorithm obtains state-of-the-art performance in a considerably lower training time. Other algorithm that could be used was the Online Independent Support Vector Machines <ref type="bibr" coords="7,327.62,317.89,10.52,8.74" target="#b4">[5]</ref> that, in comparison with SVM, dramatically reduces learning time and space requirements at the price of a negligible loss in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Participation</head><p>In 2012, 43 groups registered to the Robot Vision task but only 8 submitted, at least, one run, namely:</p><p>-CIII UTN FRC: Universidad Tecnológica Nacional, Córdoba, Argentina.</p><p>-NUDT: National University of Defense Technology, Changsha, China.</p><p>-UAIC2012: Alexandru Ioan Cuza University, Iasi, Romania.</p><p>-USUroom409: Ural Federal University, Yekaterinburg, Russian Federation.</p><p>-SKB Kontur Labs: Kontur Labs, Yekaterinburg, Russian Federation.</p><p>-CBIRITU: Istanbul Technical University, Istanbul, Turkey.</p><p>-SIMD: University of Castilla-La Mancha, Albacete, Spain.</p><p>-BuffaloVision: University at Buffalo, New York, United States.</p><p>A total of 23 runs were submitted, with 18 runs submitted to the task 1 (mandatory) and 5 runs submitted to the task 2 (optional). The limit to the number of runs that could be submitted was 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>This section presents the results of the Robot Vision task of ImageCLEF 2012 for the two subtasks: task1 and task 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task 1</head><p>Eight different groups submitted runs for the task 1 as can be observed in Table <ref type="table" coords="8,472.84,390.85,3.87,8.74" target="#tab_2">3</ref>.</p><p>The maximum score that could be achieved was 2445 and the winner (CII UTN FRC) obtained 2071 points. CII UTN FRC and NUDT teams ranked first and second respectively and their score was higher than 70% of the maximum score. The score obtained by the SIMD/IDIAP team could be considered as a baseline result that all the groups were expected to improve. Such score was obtained using the techniques provided by the organizers without new contributions. As it was expected, 6 out of 7 teams obtained higher scores. The results are summarized in Figure <ref type="figure" coords="8,214.30,486.62,3.88,8.74" target="#fig_4">5</ref>, where only the best run for each team has been considered.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Task 2</head><p>For the optional task, the maximum score was 4079 and only 4 groups submitted runs. The winner for the task 2 was the CIII UTN FRC group with 3930 points, only 71 more than the NUDT group, which ranked second. All the results can be seen in Table <ref type="table" coords="9,208.99,297.13,3.87,8.74" target="#tab_3">4</ref>.</p><p>In view of these results, it should be remarked the high quality of the participant proposals, due to the score obtained by CIII UTN FRC, NUDT and CBIRITU groups was higher than 75% of the maximum score. All the results for task 2 are summarized in Figure <ref type="figure" coords="9,298.53,346.92,3.87,8.74" target="#fig_5">6</ref>, where only the best run for each team has been considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>We have described in this article the fourth edition of the Robot Vision task at ImageCLEF 2012, which attracted a considerable attention with 8 groups submitting runs. There are 2 main conclusions that can be drawn from the proposals: (i) despite of two types of images were provided (visual and range), depth features were not commonly used, and (ii) most of the proposals were based on the use of SVMs. We plan to continue the task in the next years with new challenges related to place categorization. Concretely, we have plans to introduce object categorization in the following editions. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,194.71,502.62,225.94,7.89;3,276.56,312.43,62.25,175.42"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Mobile robot platform used for data acquisition.</figDesc><graphic coords="3,276.56,312.43,62.25,175.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,214.01,218.93,187.34,7.89;4,146.87,126.80,103.74,77.81"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visual, depth and 3D point cloud files.</figDesc><graphic coords="4,146.87,126.80,103.74,77.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,244.14,655.03,127.08,7.89;4,143.41,474.63,328.53,165.63"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Example of kidnapping.</figDesc><graphic coords="4,143.41,474.63,328.53,165.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,174.07,655.03,267.22,7.89;5,177.99,578.46,83.00,62.25"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Examples of images from the Robot Vision 2012 database.</figDesc><graphic coords="5,177.99,578.46,83.00,62.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,171.41,655.03,272.54,7.89;8,171.08,518.30,273.20,121.96"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Results obtained for the Task 1 as % of the maximum score</figDesc><graphic coords="8,171.08,518.30,273.20,121.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,171.41,655.03,272.54,7.89;9,186.64,553.42,242.06,86.84"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Results obtained for the Task 2 as % of the maximum score</figDesc><graphic coords="9,186.64,553.42,242.06,86.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,169.97,115.91,275.42,150.41"><head>Table 1 .</head><label>1</label><figDesc>Distribution of room categories for the training sequences.</figDesc><table coords="5,204.91,136.71,202.46,129.61"><row><cell></cell><cell cols="3">Number of frames</cell></row><row><cell cols="4">Room Category Training 1 Training 2 Training 3</cell></row><row><cell>Corridor</cell><cell>438</cell><cell>498</cell><cell>444</cell></row><row><cell>Elevator Area</cell><cell>140</cell><cell>152</cell><cell>84</cell></row><row><cell>Printer Room</cell><cell>119</cell><cell>80</cell><cell>65</cell></row><row><cell>Lounge Area</cell><cell>421</cell><cell>452</cell><cell>376</cell></row><row><cell>Professor Office</cell><cell>408</cell><cell>336</cell><cell>247</cell></row><row><cell>Student Office</cell><cell>664</cell><cell>599</cell><cell>388</cell></row><row><cell>Visio Conference</cell><cell>126</cell><cell>79</cell><cell>60</cell></row><row><cell>Technical Room</cell><cell>153</cell><cell>96</cell><cell>118</cell></row><row><cell>Toilet</cell><cell>198</cell><cell>240</cell><cell>131</cell></row><row><cell>All</cell><cell>2667</cell><cell>2532</cell><cell>1913</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,165.13,115.91,285.10,226.32"><head>Table 3 .</head><label>3</label><figDesc>Ranking of the runs submitted by the groups for the Task 1.</figDesc><table coords="8,188.62,136.71,235.05,205.52"><row><cell>Rank</cell><cell>Group Name</cell><cell cols="2">Score % Max. Score</cell></row><row><cell>1</cell><cell>CIII UTN FRC</cell><cell>2071</cell><cell>84.70</cell></row><row><cell>2</cell><cell>NUDT</cell><cell>1817</cell><cell>74.31</cell></row><row><cell>3</cell><cell>NUDT</cell><cell>1729</cell><cell>70.72</cell></row><row><cell>4</cell><cell>UAIC2012</cell><cell>1348</cell><cell>55.13</cell></row><row><cell>5</cell><cell>USUroom409</cell><cell>1225</cell><cell>50.10</cell></row><row><cell>6</cell><cell>USUroom409</cell><cell>1225</cell><cell>50.10</cell></row><row><cell>7</cell><cell>USUroom409</cell><cell>1193</cell><cell>48.79</cell></row><row><cell>8</cell><cell>UAIC2012</cell><cell>1049</cell><cell>42.90</cell></row><row><cell>9</cell><cell>UAIC2012</cell><cell>1049</cell><cell>42.90</cell></row><row><cell>10</cell><cell>SKB Kontur Labs</cell><cell>1028</cell><cell>42.04</cell></row><row><cell>11</cell><cell>SKB Kontur Labs</cell><cell>1006</cell><cell>41.15</cell></row><row><cell>12</cell><cell>SKB Kontur Labs</cell><cell>997</cell><cell>40.78</cell></row><row><cell>13</cell><cell>CBIRITU</cell><cell>551</cell><cell>22.54</cell></row><row><cell>14</cell><cell>CBIRITU</cell><cell>542</cell><cell>22.17</cell></row><row><cell cols="3">15 SIMD/IDIAP (baseline results) 462</cell><cell>18.86</cell></row><row><cell>16</cell><cell>BuffaloVision</cell><cell>-70</cell><cell>&lt;0.00</cell></row><row><cell>17</cell><cell>BuffaloVision</cell><cell>-110</cell><cell>&lt;0.00</cell></row><row><cell>18</cell><cell>BuffaloVision</cell><cell>-234</cell><cell>&lt;0.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,165.13,115.91,285.10,83.85"><head>Table 4 .</head><label>4</label><figDesc>Ranking of the runs submitted by the groups for the Task 2.</figDesc><table coords="9,188.62,136.71,235.05,63.06"><row><cell>Rank</cell><cell>Group Name</cell><cell cols="2">Score % Max. Score</cell></row><row><cell>1</cell><cell>CIII UTN FRC</cell><cell>3930</cell><cell>96.35</cell></row><row><cell>1</cell><cell>CIII UTN FRC</cell><cell>3925</cell><cell>96.22</cell></row><row><cell>2</cell><cell>NUDT</cell><cell>3859</cell><cell>94.61</cell></row><row><cell>3</cell><cell>CBIRITU</cell><cell>3169</cell><cell>77.69</cell></row><row><cell cols="3">4 SIMD/IDIAP (baseline results) 1041</cell><cell>25.52</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="6,144.73,656.80,130.86,7.86"><p>http://imageclef.org/2012/robot</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,142.59,337.64,7.86;10,151.52,153.55,329.07,7.86;10,151.52,164.51,20.99,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,315.86,142.59,164.74,7.86;10,151.52,153.55,36.36,7.86">Image classification using random forests and ferns</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Munoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,208.09,153.55,185.11,7.86">International Conference on Computer Vision</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,175.46,337.64,7.86;10,151.52,186.42,273.89,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,274.81,175.46,205.78,7.86;10,151.52,186.42,142.30,7.86">Object recognition using composed receptive field histograms of higher dimensionality</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Linde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,313.49,186.42,83.98,7.86">Proc. ICPR. Citeseer</title>
		<meeting>ICPR. Citeseer</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,197.38,337.64,7.86;10,151.52,208.34,329.07,7.86;10,151.52,219.30,20.99,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,420.67,197.38,59.93,7.86;10,151.52,208.34,203.64,7.86">Baseline multimodal place classifier for the 2012 robot vision task</title>
		<author>
			<persName coords=""><forename type="first">Jesus</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ismael</forename><surname>Garcia-Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,373.64,208.34,103.04,7.86">CLEF 2012 working notes</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,230.26,337.64,7.86;10,151.52,241.22,329.07,7.86;10,151.52,252.18,84.02,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,341.38,230.26,139.21,7.86;10,151.52,241.22,193.68,7.86">Gray scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mäenpää</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,355.77,241.22,99.38,7.86">Computer Vision-ECCV</title>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
			<biblScope unit="page" from="404" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,263.14,337.64,7.86;10,151.52,274.09,329.07,7.86;10,151.52,285.05,20.99,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,397.14,263.14,83.45,7.86;10,151.52,274.09,215.61,7.86">Indoor place recognition using online independent support vector machines</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Castellini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sandini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,385.57,274.09,48.09,7.86">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,296.01,337.64,7.86;10,151.52,306.97,317.69,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,302.91,296.01,177.68,7.86;10,151.52,306.97,33.81,7.86">Online-Batch Strongly Convex Multi Kernel Learning</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,205.16,306.97,203.69,7.86">Proc. of Computer Vision and Pattern Recognition</title>
		<meeting>of Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,317.93,337.63,7.86;10,151.52,328.89,329.07,7.86;10,151.52,339.85,84.02,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,332.33,317.93,148.26,7.86;10,151.52,328.89,71.85,7.86">Overview of the imageclef@ icpr 2010 robot vision track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pronobis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,233.32,328.89,243.11,7.86">Recognizing Patterns in Signals, Speech, Images and Videos</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="171" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,350.81,337.64,7.86;10,151.52,361.77,250.33,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,390.39,350.81,90.21,7.86;10,151.52,361.77,68.25,7.86">The robot vision track at imageclef 2010</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pronobis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fornoni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Hi Christensesn</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,228.06,361.77,120.53,7.86">Working Notes of ImageCLEF</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,372.72,337.63,7.86;10,151.52,383.68,123.48,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,304.29,372.72,172.45,7.86">Overview of the clef 2009 robot vision track</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pronobis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,394.64,337.98,7.86;10,151.52,405.60,329.07,7.86;10,151.52,416.56,329.07,7.86;10,151.52,427.52,56.09,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,312.67,394.64,84.39,7.86">The robot vision task</title>
		<author>
			<persName coords=""><forename type="first">Andrzej</forename><surname>Pronobis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,410.83,405.60,69.76,7.86;10,151.52,416.56,176.60,7.86">ImageCLEF, volume 32 of The Information Retrieval Series</title>
		<editor>
			<persName><forename type="first">Henning</forename><surname>Muller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="185" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,438.48,337.98,7.86;10,151.52,449.44,329.07,7.86;10,151.52,460.40,20.99,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,265.26,438.48,143.62,7.86">3d is here: Point cloud library (pcl)</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cousins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,429.17,438.48,51.42,7.86;10,151.52,449.44,250.75,7.86">Robotics and Automation (ICRA), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,471.35,337.97,7.86;10,151.52,482.31,329.07,7.86;10,151.52,493.27,109.84,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,276.68,471.35,203.91,7.86;10,151.52,482.31,23.52,7.86">Matching local self-similarities across images and videos</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,195.28,482.31,256.15,7.86;10,151.52,493.27,36.03,7.86">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR&apos;07</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
