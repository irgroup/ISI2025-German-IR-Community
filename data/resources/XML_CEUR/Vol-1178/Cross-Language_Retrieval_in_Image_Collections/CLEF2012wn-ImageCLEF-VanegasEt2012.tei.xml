<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,142.55,115.96,330.25,12.62;1,182.02,133.89,251.32,12.62">Bioingenium at ImageCLEF 2012: Textual and Visual Indexing for Medical Images</title>
				<funder ref="#_7Uaw46s">
					<orgName type="full">Sistema para la Recuperación de Imágenes Médicas Utilizando Indexación Multimodal</orgName>
				</funder>
				<funder ref="#_gPAqj62">
					<orgName type="full">Convocatoria Colciencias 521 de</orgName>
				</funder>
				<funder ref="#_NPHt2V6">
					<orgName type="full">Anotación Automática y Recuperación por Contenido de Imágenes Radiológicas Usando Semántica Latente</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,141.73,171.56,73.71,8.74"><forename type="first">Jorge</forename><forename type="middle">A</forename><surname>Vanegas</surname></persName>
						</author>
						<author>
							<persName coords="1,223.53,171.56,70.16,8.74"><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
						</author>
						<author>
							<persName coords="1,301.64,171.56,76.38,8.74"><forename type="first">Jorge</forename><forename type="middle">E</forename><surname>Camargo</surname></persName>
							<email>jecamargom@unal.edu.co</email>
						</author>
						<author>
							<persName coords="1,386.59,171.56,82.19,8.74"><forename type="first">Raul</forename><surname>Ramos-Pollán</surname></persName>
						</author>
						<author>
							<persName coords="1,277.30,183.51,80.12,8.74"><forename type="first">Fabio</forename><forename type="middle">A</forename><surname>González</surname></persName>
							<email>fagonzalezo@unal.edu.co</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Bioingenium Research Group</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Universidad Nacional de Colombia {javanegasr</orgName>
								<address>
									<settlement>jccaicedoru</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,142.55,115.96,330.25,12.62;1,182.02,133.89,251.32,12.62">Bioingenium at ImageCLEF 2012: Textual and Visual Indexing for Medical Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1FB1938AD36F569F5192AE00C6363B1B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Retrieval</term>
					<term>Medical Images</term>
					<term>Multimodal Indexing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the Bioingenium research group of Universidad Nacional de Colombia in the ImageCLEF2012 Medical Retrieval challenge, specifically in the ad-hoc image-based retrieval task. The methods used for solving textual and visual queries with which we submitted uni-modal runs are described. They were ranked 1st and 3rd respectively. These results have been obtained by using our own implementation of Okapi-BM25 weighting scheme for text retrieval, and by adding spatial layouts to the CEDD descriptors for visual retrieval. We also used these uni-modal features to learn multimodal representations using matrix factorization for solving visual queries. Despite the potential of multimodal indexes to improve the quality of visual queries, these experiments were not as successful as uni-modal indexes. We discuss the main findings of all these experiments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes the participation of the Bioingenium research group of Universidad Nacional de Colombia in the 2012 version of the Medical Image Retrieval challenge at ImageCLEF <ref type="bibr" coords="1,291.29,500.70,9.95,8.74" target="#b6">[7]</ref>. Our first motivation was to investigate the extent to which textual and visual indexes may be improved for searching in the collection of medical images, using keywords and visual examples separately. We aimed at designing suitable textual and visual representations by extending models that were successful in previous years, and preparing these representations for subsequent multimodal analysis.</p><p>For text indexing, we developed our own implementation of Okapi-BM25, which allows to determine limits on the number of terms used in the vector representation, by pruning irrelevant terms and keeping the most informative ones. For visual indexing, we introduced a spatial pyramid of CEDD features, making recursive partitions of the image and computing descriptors in each subregion. This representation extends the popular CEDD descriptor with spatial information, which results in an improved performance. We also implemented spatial extensions for bag-of-features histograms.</p><p>Our second motivation was to build an enhanced image index using both modalities, but for searching with visual examples only. The goal was to learn a multimodal representation that incorporates textual and visual information in the database, and then predict the multimodal representation for queries using visual features. This represents a very challenging problem since the medical image collection, with more than 300K images, constitutes a very large training set that poses computational difficulties for most learning algorithms. Other problems arise from this large multimodal image collection, such as the high dimensionality of textual and visual representations, and the presence of noise.</p><p>The results obtained with uni-modal strategies were successful in the general pooling, which ranked first in the case of textual queries among 54 other submissions, and third in the case of visual queries among 36 experiments. We consider that the multimodal indexing submission was not successful since it did not improve upon our own visual indexing strategy, which was the original goal. However, further experiments conducted off competition demonstrate interesting improvements. We believe that further research in this front may help to design more accurate image search systems working with the query-by-visual-example paradigm.</p><p>The structure of this paper is as follows: Section 2 briefly describes the medical image collection, Section 3 describes our text indexing approach, Section 4 presents the visual indexing strategies, Section 5 discusses the multimodal indexing approach for visual queries, and finally, conclusions and future works are outlined in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Data Set</head><p>The medical retrieval task of ImageCLEF 2012 is based on a subset of PubMed Central papers, containing 305,000 images extracted from biomedical articles. Participants have access to the selected images as well as all content of the corresponding articles. This year, a set of 22 topics was released for evaluation of the retrieval systems, where each one is composed of a variable number of images and associated text in 4 languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Text Indexing</head><p>Images in the collection belong to a medical article, so they can be indexed using the surrounding text content. Our goal was to build a term-document matrix using a vector space model with the Okapi-BM25 weighting scheme <ref type="bibr" coords="2,467.33,572.43,9.95,8.74" target="#b5">[6]</ref>. We developed an indexing tool using the Natural Language Toolkit for Python <ref type="bibr" coords="2,134.77,596.34,9.95,8.74" target="#b0">[1]</ref>, which provides a clean API and extensive functionalities for common text processing tasks.</p><p>The text representation adopted in this work included information from the title of the paper and the image caption, which can be found in the XML file corresponding to each image in the data set. With that, a text corpus for the image collection was built, and standard text processing operations were applied, including tokenization, stemming, and stop-word removal. These operations determined the initial list of indexing terms.</p><p>We designed a prunning criterion to discard irrelevant terms from the initial list, thus, preserving only the most informative ones. A limit for the number of terms was established depending on their document frequency. If it is outside of a predefined interval, the term is removed from the indexing list. The thresholds were computed according to a minimum and maximum number of documents in which a term is allowed to occur. The criterion is as follows:</p><formula xml:id="formula_0" coords="3,221.22,222.82,259.38,23.08">keep(t) = true if min &lt; df t &lt; max f alse otherwise (1)</formula><p>where df t is the number of documents that contain the term t, and min and max are parameters that define the minimum and maximum number of documents in which the term should appear. The definitive list of indexing terms is obtained by applying this rule, which is very useful to limit the dimensionality of the resulting vector space for indexing.</p><p>The term-document matrix is built using term frequencies in each document, and Okapi-BM25 <ref type="bibr" coords="3,214.44,330.22,10.51,8.74" target="#b5">[6]</ref> is used to highlight the importance of the most relevant terms. Usually, BM25 is used as a ranking function that involves different factors including: term frequencies, inverse document frequencies, and the length of both, the document and the query. However, in our approach, we wanted to use the ideas of BM25 as a term weighting scheme so that we can apply further processes to the term-document matrix (such as multimodal fusion). The following equation describes the BM25-based term weighting used:</p><formula xml:id="formula_1" coords="3,180.39,421.70,300.21,35.57">weight(t, d) = log N df t •   (k 1 + 1) tf t,d k 1 (1 -b) + b L d Lavg + tf t,d  <label>(2)</label></formula><p>where tf t,d is the frequency of term t in document d, L d and L avg are the length of document d and the average document length in the collection, respectively, and, k 1 and b are positive tuning parameters to calibrate the term frequency scaling. We fixed k 1 = 1.5 and b = 0.75 according to the suggestions presented by Manning et al. <ref type="bibr" coords="3,257.62,513.00,9.96,8.74" target="#b5">[6]</ref>. For queries, we only used term frequencies without weighting, and the dot product similarity score was employed for document ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results</head><p>We submitted 2 textual runs using the indexing strategy described above, with the goal of evaluating the difference in performance after pruning the list of indexing terms. In both cases, we set a minimum frequency of 20 documents in which a term should be present to keep it in the list. The first experiment used 20,000 documents as maximum frequency, and the second experiment used 5,000 documents. These parameters resulted in vector spaces with approximately 28,000 and 18,000 dimensions, respectively.</p><p>Table <ref type="table" coords="4,163.89,115.91,4.12,7.89">1</ref>. Retrieval performance of the submitted runs in the Medical Ad-hoc Image-Based Retrieval Task, using textual queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Position MAP P@10 unal.text.bm25.20000 1 0.2182 0.3409 unal.text.bm25.5000 14 0.2045 0.2955 Extra (50,000) N.A. 0.1991 0.3318</p><p>An additional experiment was evaluated after the competition finished to assess the contribution of the pruning strategy with respect to a longer list of terms. This experiment used 50,000 documents as maximum frequency, and produced a list of 29,000 terms. Notice how the number of indexing terms is controlled by the use of these two parameters, which remove rare terms as well as too common terms. We used this property to control the dimensionality of the resulting term-document matrix for further analysis, as is described later in Section 5.</p><p>Table <ref type="table" coords="4,176.36,312.46,4.98,8.74">1</ref> reports performance measures for the three experiments, the two first submitted to the official pooling and a third experiment run after the challenge. These results show the impact of the pruning strategy in the precision of the retrieval task, showing how the performance decreases by keeping or removing the wrong terms. The best response was obtained by the index limited by a frequency of 20,000 documents. This result ranked first in the category of textual experiments, and is the second best performance overall in the poolings for adhoc image-based medical retrieval.</p><p>Our second submission used 18,633 indexing terms, resulting in a significant dimensionality reduction, but also an important reduction in performance. This difference dropped the MAP performance in about 6%, leaving this experiment in the position number 14. However, notice that keeping more terms than those actually needed, can hurt the general retrieval precision even more. The additional experiment shows that a slight increase in the number of terms resulted in a decrease in performance of about 9%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Visual Indexing</head><p>Our research group is currently leading an initiative to develop a framework for large scale image analysis for academic and scientific applications. The framework, named BIGS <ref type="bibr" coords="4,213.92,560.40,13.97,8.74" target="#b7">[8]</ref>, is implemented in the Java programming language and integrates a wide variety of image processing tools, including feature extraction and learning algorithms. One of the most remarkable characteristics of BIGS is that it can easily run in a distributed environment with heterogeneous computing resources, from laptop and desktop computers to high-performance servers.</p><p>Obtaining a good quality representation for image contents in large databases is a challenging task, and BIGS was used to tune up image indexes by conducting experiments on the ImageCLEFmed 2011 data set. The experiments were run on different servers scattered throughout our lab, using BIGS to process all images stored on an HBase NoSQL database <ref type="foot" coords="5,301.00,117.42,3.97,6.12" target="#foot_0">1</ref> . In spite of the large size of the image collection, having an lightweight experimental lifecycle as provided by BIGS was key to be able to gain understanding on how to better tune up image indexes.</p><p>As a result, we designed two indexes for content-based image retrieval for this year's data set, focusing on including spatial information in the representation, since it can help to better discriminate medical image arrangements. The Color and Edge Directivity Descriptor (CEDD) <ref type="bibr" coords="5,347.95,191.23,10.52,8.74" target="#b2">[3]</ref> was used as basic low-level characteristic in both indexes, since it has demonstrated good performance in image retrieval tasks, while keeping a small and compact representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Spatial Pyramid CEDD</head><p>The CEDD descriptor is a compact representation of the image content, consisting in a histogram of 144 bins to codify information of colors and edges. The small size of this descriptor makes it an excellent choice for indexing large scale image collections. This descriptor has been previously evaluated in the context of medical image retrieval at ImageCLEF, exhibiting a competitive performance due to the variety of image modalities and visual configurations in this data set.</p><p>We extended this representation by computing the CEDD descriptor in a recursive partition of the image in quadrants, forming a pyramid of spatially organized regions <ref type="bibr" coords="5,214.32,366.11,9.95,8.74" target="#b4">[5]</ref>. We employed a configuration using the full image plus 2 pyramid levels, which results in 21 spatially distributed regions, ending up in a visual representation with 3,024 features. These descriptors were computed from high-resolution images, i.e., as they are distributed in the ImageCLEFmed data set.</p><p>This descriptor was computed by the BIGS framework using 40 workers deployed in several computers at our lab. The total time required to index the full image collection of 305,000 images using this strategy was 37 minutes. Finally, the similarity between two images is calculated on this descriptor using the Tanimoto coefficient. Assuming that x and y are vector representations of the spatial pyramids for two images, this is computed as:</p><formula xml:id="formula_2" coords="5,235.33,507.93,245.27,23.89">T D = t(x, y) = x T y x T x + y T y -x T y<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Spatial Bag-of-Features</head><p>An image index using the bag-of-features representation <ref type="bibr" coords="5,387.42,575.76,10.51,8.74" target="#b3">[4]</ref> was introduced in our experiments as well. The bag-of-features methodology is comprised of 3 main procedures: extraction of local features from images, construction of a dictionary of visual words, and the computation of the histogram for each image. Spatial layouts can be added to enhance the representation with the relative position of words in the image plane. In that sense, this representation incorporates local, low-level information of images as well as global, spatially distributed arrangements.</p><p>For local features, we extracted blocks of 32 × 32 pixels on a regular grid and the CEDD descriptor is computed in these patches. The k-means algorithm is used to cluster a large sample of patches extracted from the collection, for building a dictionary of 5,000 visual terms. The histogram is constructed by counting the occurrence of dictionary words in each image. Besides the global counting of visual patterns, each image is also split in 3 horizontal, non-overlapping strips, and an additional histogram is computed there to estimate the spatial distribution of visual words. This results in four bag-of-features histograms that are bounded together in a single image descriptor with 20,000 features.</p><p>This representation was also computed using the BIGS framework with 40 workers deployed in several computers at our lab. The total time required to extract this representation for all images in the collection was 116 minutes, which is less than one hour and a half. The similarity measure computed for this representation is the histogram intersection, for two images with histograms x and y:</p><formula xml:id="formula_3" coords="6,246.53,329.75,234.07,30.32">K HI (x, y) = n i=1 min {x i , y i } (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Visual Queries with Multiple Images</head><p>The topics proposed for this year's challenge included 22 different queries with multiple images, some of them with 6 or even 7 example images. Since a single ranking is required for queries with multiple image examples, a similarity integration rule was employed. The similarity score for a database image d with respect to a multi-image query q = {q 1 , q 2 , ..., q n }, is obtained as follows:</p><p>score(d, q) = n k=1 similarity(d, q k ) (5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>We submitted two runs, one using the spatial pyramid of CEDD features and another with the spatial bag-of-features. The results are reported in Table <ref type="table" coords="6,454.15,548.44,3.87,8.74">2</ref>, and shows that the spatial pyramid obtains a significantly better performance than the bag-of-features, both in general precision (MAP) and early precision (P@10 and P@30). The difference can also be observed in the positions obtained by these experiments in the general poolings, the spatial pyramid was ranked 3rd, whereas the bag-of-features was ranked 14th. The spatial pyramid extension for the CEDD descriptor demonstrated to be an effective representation to discriminate more relevant images in this task. In addition, computing the spatial pyramid did not result in an excessive load of both, computational effort and representation length. This representation is still very light to compute with respect to the bag-of-features and keeps a compact descriptor with about 3,000 features.</p><p>In our preliminary experiments, we observed that adding a spatial layout on the image representation improves the performance of the medical image retrieval task. The two visual representations proposed in this work include spatial information using recursive computations of the same descriptor in partitions of the image. One of the reasons the spatial pyramid CEDD presented better performance than the bag-of-features is because of the level of granularity in the recursive partition, that allows to introduce more spatial details. This can be achieved because of the short length of the original CEDD descriptor, as opposed to the large dictionary of visual features that we employed in these experiments.</p><p>Table <ref type="table" coords="7,163.81,269.58,4.12,7.89">2</ref>. Performance measures of the submitted runs in the Medical Ad-hoc Image-Based Retrieval Task for visual queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Position MAP P@10 P@30 unal.visual.pyramidal.cedd.tanimoto 3 0,0073 0,0636 0,05 unal.visual.spatial.bof.3x1 14 0,0033 0,0455 0,0364</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Multimodal Indexing for Visual Queries</head><p>One of our motivations to design textual and visual indexes for medical image collections is to develop a multimodal framework to integrate both modalities in a common representation. We focus our attention to the specific case of enhancing visual search functionalities by introducing available text information into the visual index. Thus, the goal is to improve the retrieval response using multimodal information even when users search with example images only.</p><p>In this work, we employed a multimodal latent factors model proposed in <ref type="bibr" coords="7,470.10,472.64,10.51,8.74" target="#b1">[2]</ref> for learning the relationships between visual features and text terms. The method is based on a matrix factorization algorithm, that proceeds with a multimodal decomposition of the visual and text matrices on a training data set. The matrix factorization problem is defined as follows: min</p><formula xml:id="formula_4" coords="7,153.76,538.64,326.84,22.31">P,Q,H 1 2 V -P H 2 F + T -QH 2 F + λ P 2 F + Q 2 F + H 2 F<label>(6)</label></formula><p>where V ∈ R n× is the matrix of n visual features for training examples, T ∈ R m× is the matrix of textual information with m terms, P ∈ R n×r is the transformation from the visual space to a multimodal space with r factors, Q ∈ R m×r is the transformation from the textual space to the multimodal space, and H ∈ R r× is the multimodal latent representation for the training images. λ is a regularization parameter for this learning problem.</p><p>The solution to this problem presented in <ref type="bibr" coords="7,329.93,644.16,10.51,8.74" target="#b1">[2]</ref> is an online matrix factorization algorithm that can be scaled up to large data sets. This is specially useful for the ImageCLEFmed 2012 collection, which has a large number of images that can be used for learning multimodal relationships between visual and textual information. When the linear transformation functions P and Q have been learned, new images can be projected to the multimodal space using the following equation:</p><formula xml:id="formula_5" coords="8,247.37,177.27,228.99,13.11">h = P T P + ξQ T Q -1 P T v (<label>7</label></formula><formula xml:id="formula_6" coords="8,476.36,181.64,4.24,8.74">)</formula><p>where h is the multimodal representation for an image with visual features v, and ξ is a regularization parameter. The purpose of using these algorithms is to obtain a multimodal latent factor representations for all images, even if they do not have available text annotations, as may be the case of the queries. Using the multimodal representation, the ranking of images is computed using the dot product similarity measure, which indicates the extent to which two images share the same latent factors.</p><p>This strategy has demonstrated to be an effective method to learn multimodal relationships from image collections with attached texts, resulting in a datadriven representation for images that incorporates both modalities. Previous studies have shown important performance gains for these approaches, since visual features are complemented by the semantics of text descriptions, providing an enhanced mechanism of representing images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results</head><p>To construct a multimodal index for image search, we employed the matrices of text terms and visual features described in Sections 3 and 4, respectively. More specifically, we used the term-document matrix with 18,000 terms weighted with Okapi-BM25, and the visual matrix with 3,024 spatial pyramid CEDD features. One of the reasons we were interested in designing textual and visual indexes with bounded dimensionality is to reduce the computational cost of learning multimodal relationships.</p><p>The online multimodal matrix factorization (OMMF) algorithm was trained with the full collection of images in this challenge, i.e., using the 305,000 images with their corresponding text annotations. An implementation of the algorithm in the Java programming language was employed, which decomposed the matrices of 18,000 rows for text data, and 3,024 rows for visual data, with 305,000 columns in both cases, in 131 minutes in average. This algorithm has been designed to learn from as many examples as possible in a short time.</p><p>To tune up the learning algorithm and determine appropriate parameters for the factorization, experiments were conducted with the ImageCLEFmed 2011 collection. We found good parameters to solve queries in the previous year's challenge, that included 600 multimodal latent factors and other regularization parameters as needed. The criteria to select parameters for this algorithm is to observe improvements with respect to the direct visual matching, i.e., with respect to the visual indexing methods presented in Section 4, since the queries used in this experiment are also based on example images only. With the parameters that showed improvements in the 2011 collection, we prepared and submitted a run to the official poolings. Table <ref type="table" coords="9,394.30,250.88,4.98,8.74" target="#tab_0">3</ref> reports the results of this submission, as well as two other experiments for comparison. The first experiment in the Table is our baseline method, based on direct matching of visual features. The second result is the performance of the prepared run that has shown a decrease in performance with respect to the baseline. This loss is mainly explained by the use of parameters tuned to improve the performance in the 2011 challenge.</p><p>There are several differences between the challenge of 2011 and 2012. First, the nature of the proposed topics varied significantly, as this year's queries included more example images per topic, in average. Second, the size of the collection was increased, which resulted in bigger matrices in both dimensions. Third, this year's visual queries seem to be more difficult to answer, judging by the relative decrease in MAP observed in the results from 2011 to 2012. All these aspects may require a different configuration for the learning algorithm, in order to make it effective to retrieve more relevant results.</p><p>The results reported in the third row of Table <ref type="table" coords="9,363.48,430.63,4.98,8.74" target="#tab_0">3</ref> present the performance measures for an additional experiment run off competition to estimate the potential of the OMMF algorithm to improve upon the baseline. This result was obtained by tuning the algorithm parameters more appropriately for this year's task, and shows an important relative improvement.</p><p>The main goal of a multimodal algorithm in this context is to extract meaningful relationships between visual features and text terms. An additional challenge that makes the multimodal indexing strategy difficult to setup correctly, is attributed to the properties of the textual modality, which is very noisy and unstructured. Extracting semantic information useful for image analysis in this condition is still a very interesting research problem that requires further analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions And Future Work</head><p>This paper presented the participation of the Bioingenium research group of Universidad Nacional de Colombia in the ad-hoc image-based medical retrieval task at ImageCLEF 2012. We submitted 5 runs: 2 textual and 3 visual, from which one was ranked first in the text modality and another was ranked third in the vi-sual modality. These results were obtained by incorporating simple and effective extensions to well-known strategies for this task. We also explored multimodal indexing to answer visual queries, which is a very challenging and interesting research problem, that still requires further analysis. We believe that this is a promising research direction for improving image search systems, and the study of these models are the focus of our future research.</p><p>One of the main difficulties of this year's challenge was the size of the database, which required efficient computational tools to process and index the collection. In this work, we supported all of our visual indexing experiments on a distributed computing framework for large scale image analysis, named BIGS <ref type="bibr" coords="10,134.77,238.55,9.95,8.74" target="#b7">[8]</ref>. This framework allowed us to accelerate the exploration of visual indexing strategies, and investigate new image representation designs, such as the spatial pyramid CEDD that ranked third among 36 other experiments. We also used online learning algorithms for extracting multimodal relationships efficiently by training with the full collection of medical images in short execution times.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,134.77,115.91,348.57,94.81"><head>Table 3 .</head><label>3</label><figDesc>Performance results for multimodal indexing to solve visual queries. The first row reports the baseline method based on visual features only. The second row presents the results of the run submitted to the official poolings. The third row reports the result of an additional experiment run off competition.</figDesc><table coords="9,136.16,169.55,347.18,41.16"><row><cell>Run</cell><cell cols="5">Position MAP Improvement P@10 Rel-Ret</cell></row><row><cell>unal.visual.pyramidal.cedd.tanimoto</cell><cell>3</cell><cell>0,0073</cell><cell>N.A.</cell><cell>0,0636</cell><cell>117</cell></row><row><cell>unal.cedd.factorization.600</cell><cell>19</cell><cell>0,0024</cell><cell>-67.1%</cell><cell>0,0091</cell><cell>45</cell></row><row><cell>Additional experiment</cell><cell cols="2">N.A. 0.0087</cell><cell>+19.2%</cell><cell>0.0182</cell><cell>137</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,144.73,656.80,101.06,7.86"><p>http://hbase.apache.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was partially funded by the project <rs type="funder">Anotación Automática y Recuperación por Contenido de Imágenes Radiológicas Usando Semántica Latente</rs>, number <rs type="grantNumber">110152128803</rs> and project <rs type="funder">Sistema para la Recuperación de Imágenes Médicas Utilizando Indexación Multimodal</rs>, number <rs type="grantNumber">110152128767</rs> by <rs type="funder">Convocatoria Colciencias 521 de</rs> <rs type="grantNumber">2010</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_NPHt2V6">
					<idno type="grant-number">110152128803</idno>
				</org>
				<org type="funding" xml:id="_7Uaw46s">
					<idno type="grant-number">110152128767</idno>
				</org>
				<org type="funding" xml:id="_gPAqj62">
					<idno type="grant-number">2010</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,138.34,439.66,228.82,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,201.00,439.66,138.50,7.86">Nltk: The natural language toolkit</title>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.34,450.28,342.25,7.86;10,146.90,461.24,333.69,7.86;10,146.90,472.20,20.96,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,310.10,450.28,170.49,7.86;10,146.90,461.24,59.19,7.86">Online matrix factorization for multimodal image retrieval</title>
		<author>
			<persName coords=""><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabio</forename><forename type="middle">A</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,227.41,461.24,212.86,7.86">17th Iberoamerican Congress on Pattern Recognition</title>
		<imprint>
			<publisher>CIARP</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.34,482.82,342.25,7.86;10,146.90,493.78,333.70,7.86;10,146.90,504.74,333.70,7.86;10,146.90,515.69,205.53,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,353.22,482.82,127.37,7.86;10,146.90,493.78,266.08,7.86">Cedd: color and edge directivity descriptor: a compact descriptor for image indexing and retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Savvas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiannis</forename><forename type="middle">S</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Boutalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,434.74,493.78,45.86,7.86;10,146.90,504.74,302.81,7.86">Proceedings of the 6th international conference on Computer vision systems, ICVS&apos;08</title>
		<meeting>the 6th international conference on Computer vision systems, ICVS&apos;08<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="312" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.34,526.31,342.24,7.86;10,146.90,537.27,333.70,7.86;10,146.90,548.23,44.98,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,338.13,526.31,142.46,7.86;10,146.90,537.27,36.64,7.86">Visual categorization with bags of keypoints</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,194.98,537.27,253.65,7.86">Workshop on Statistical Learning in Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.34,558.85,342.25,7.86;10,146.90,569.81,333.70,7.86;10,146.90,580.77,333.70,7.86;10,146.90,591.73,333.70,7.86;10,146.90,602.68,99.36,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,378.40,558.85,102.19,7.86;10,146.90,569.81,266.95,7.86">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName coords=""><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,434.74,569.81,45.86,7.86;10,146.90,580.77,333.70,7.86;10,146.90,591.73,52.31,7.86;10,245.23,591.73,40.14,7.86">Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
	<note>CVPR &apos;06</note>
</biblStruct>

<biblStruct coords="10,138.34,613.30,342.25,7.86;10,146.90,624.26,322.20,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,420.89,613.30,59.71,7.86;10,146.90,624.26,85.82,7.86">Introduction to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prabhakar</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hinrich</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Schtze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.34,634.88,342.25,7.86;10,146.90,645.84,333.69,7.86;10,146.90,656.80,155.66,7.86" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="10,297.79,645.84,182.80,7.86;10,146.90,656.80,126.81,7.86">Overview of the imageclef 2012 medical image retrieval and classification tasks</title>
		<author>
			<persName coords=""><forename type="first">Jayashree</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fushman</forename><surname>Sameer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antani</forename><surname>Ivan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eggel</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.34,119.67,342.25,7.86;11,146.90,130.63,333.69,7.86;11,146.90,141.59,333.69,7.86;11,146.90,152.55,333.70,7.86;11,146.90,163.51,248.53,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,309.62,141.59,170.98,7.86;11,146.90,152.55,318.01,7.86">BIGS: A framework for large-scale image processing and analysis over distributed and heterogeneous computing resources</title>
		<author>
			<persName coords=""><forename type="first">Raul</forename><surname>Ramos-Pollán</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabio</forename><forename type="middle">A</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Juan</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Angel</forename><surname>Cruz-Roa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jorge</forename><forename type="middle">E</forename><surname>Camargo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jorge</forename><forename type="middle">A</forename><surname>Vanegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">E</forename><surname>Arévalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paola</forename><forename type="middle">K</forename><surname>Rozo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Santiago</forename><forename type="middle">A</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jose</forename><forename type="middle">D</forename><surname>Bermeo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Juan</forename><forename type="middle">S</forename><surname>Otálora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,146.90,163.51,174.75,7.86">IEEE International Conference on eScience</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
