<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,127.92,152.87,339.41,12.58">KIDS Lab at ImageCLEF 2012 Personal Photo Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,124.68,190.21,51.23,11.09"><forename type="first">Chia-Wei</forename><surname>Ku</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Knowledge, Information</orgName>
								<orgName type="department" key="dep2">Database System Laboratory Department of Computer Science</orgName>
								<orgName type="department" key="dep3">Information Engineering</orgName>
								<orgName type="institution">National University of Tainan</orgName>
								<address>
									<settlement>Tainan</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,183.26,190.21,72.28,11.09"><forename type="first">Been-Chian</forename><surname>Chien</surname></persName>
							<email>bcchien@mail.nutn.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Knowledge, Information</orgName>
								<orgName type="department" key="dep2">Database System Laboratory Department of Computer Science</orgName>
								<orgName type="department" key="dep3">Information Engineering</orgName>
								<orgName type="institution">National University of Tainan</orgName>
								<address>
									<settlement>Tainan</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,262.41,190.21,60.85,11.09"><forename type="first">Guan-Bin</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Knowledge, Information</orgName>
								<orgName type="department" key="dep2">Database System Laboratory Department of Computer Science</orgName>
								<orgName type="department" key="dep3">Information Engineering</orgName>
								<orgName type="institution">National University of Tainan</orgName>
								<address>
									<settlement>Tainan</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,330.47,190.21,40.73,11.09"><forename type="first">Li-Ji</forename><surname>Gaou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Knowledge, Information</orgName>
								<orgName type="department" key="dep2">Database System Laboratory Department of Computer Science</orgName>
								<orgName type="department" key="dep3">Information Engineering</orgName>
								<orgName type="institution">National University of Tainan</orgName>
								<address>
									<settlement>Tainan</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,378.54,190.21,70.20,11.09"><forename type="first">Rong-Sing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Knowledge, Information</orgName>
								<orgName type="department" key="dep2">Database System Laboratory Department of Computer Science</orgName>
								<orgName type="department" key="dep3">Information Engineering</orgName>
								<orgName type="institution">National University of Tainan</orgName>
								<address>
									<settlement>Tainan</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.32,202.15,58.69,11.09"><forename type="first">Siao-En</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Knowledge, Information</orgName>
								<orgName type="department" key="dep2">Database System Laboratory Department of Computer Science</orgName>
								<orgName type="department" key="dep3">Information Engineering</orgName>
								<orgName type="institution">National University of Tainan</orgName>
								<address>
									<settlement>Tainan</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,127.92,152.87,339.41,12.58">KIDS Lab at ImageCLEF 2012 Personal Photo Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C9594A197E7574560550BEC99083B65C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image retrieval</term>
					<term>Concept retrieval</term>
					<term>Features clustering</term>
					<term>Similarity measure 1</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The personal photo retrieval task at ImageCLEF 2012 is a pilot task for testing QBE-based retrieval scenarios in the scope of personal information retrieval. This pilot task is organized as two subtasks: the visual concepts retrieval and the events retrieval. In this paper, we develop a framework of combining different visual features, EXIF data and similarity measures based on two clustering methods to retrieve the relevant images having similar visual concepts. We first analyze and select the effective visual features including color, shape, texture, and descriptor to be the basic elements of recognition. A flexible similarity measure is then given to achieve high precise image retrieval automatically. The experimental results show that the proposed framework can provide good effectiveness in distinct measures of evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The main aim of the ImageCLEF 2012 personal photo retrieval task is providing a test bed for image retrieval based on some given query images <ref type="bibr" coords="1,375.27,520.21,10.52,11.09" target="#b0">[ 1]</ref>. The task is further divided into two subtasks: the visual concepts retrieval and the events retrieval. Compare with traditional image retrieval, the topics of this task are more abstract or more general. It might cause image retrieval to be more difficult. The benchmark data set used in this task consists of 5,555 images downloaded from Flickr. Both the visual concepts retrieval and the events retrieval use the same dataset.</p><p>The visual concepts retrieval is a great challenge to the developers. Some of the concepts are abstract like the topic "sign," and some of them are very subjective like "art object." Even different people would draw different opinions on the same image. The events retrieval is to find the images with the same kinds of events. Some of the target topics like "fire" and "conference" are too general to define in visual concept. Parts of the events in this subtask connect with geographical topics. Thus, most of the topics are difficult to retrieve in visual. In such a case, EXIF features may support much more information about the event concept.</p><p>In our participation to the ImageCLEF 2012 personal photo retrieval task, we developed a framework for the visual concepts retrieval and the events retrieval. First, we selected 7 visual features from the given features set for the task. Each selected feature is used to cluster all the images into groups individually. We first define the similarity degree for visual features and EXIF's information. Then, the similarity measures for different image features are integrated to estimate the similarity scores between each image and the query image. The cluster of each feature is used to help weighting the image similarity. Finally, the framework combines and ranks the similarity degrees between an image and the different QBE images to retrieve the photos with the same concept.</p><p>The remainder of this paper is organized as follows. We describe the used features provided by the organizers in Section 2. Section 3 introduces the proposed similarity measures and retrieval methods. In Section 4, we present the experimental results of our proposed framework. Finally, we conclude the paper with a discussion and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>Process of Image Features</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual Features</head><p>The original datasets in the personal photo retrieval task provided 19 extracted visual features. After our estimating test, 7 features were selected from the 19 features. They are AutoColorCorrelogram <ref type="bibr" coords="2,236.04,417.19,10.58,11.09" target="#b1">[ 2]</ref>, BIC <ref type="bibr" coords="2,272.15,417.19,10.52,11.09" target="#b2">[ 3]</ref>, CEDD <ref type="bibr" coords="2,318.81,417.19,10.52,11.09" target="#b3">[ 4]</ref>, Color Structure, Edge Histogram, FCTH <ref type="bibr" coords="2,153.43,429.19,10.52,11.09" target="#b4">[ 5]</ref>, and SURF <ref type="bibr" coords="2,216.52,429.19,10.52,11.09" target="#b5">[ 6]</ref>. The selected features cover different kinds of popular visual perception including color, shape, and texture. SURF is a robustly scale-invariant and rotation-invariant descriptor feature. The features are summarized in Table <ref type="table" coords="2,443.31,453.19,3.76,11.09" target="#tab_0">1</ref>. SURF feature clustering. The SURF descriptor is the feature with scale-invariant and rotation-invariant. In this paper we defined the matching pair to measure the similar-ity between two images. If the SURF descriptor d i for the image I i matches another descriptor d j in the image I j and vice versa, the descriptors d i and d j form a matching pair. The distance between two images I i and I j is defined as ) , ( <ref type="formula" coords="3,319.02,192.95,4.75,10.53" target="#formula_0">1</ref>) , (</p><formula xml:id="formula_0" coords="3,226.68,197.83,243.82,21.92">j i mp j i SURF I I N I I dist  ,<label>(1)</label></formula><p>where N mp (I i , I j ) is the number of matching pairs between the two images I i and I j . The larger N mp is, more similar two images are. Based on the measure of the matching pair, we propose the clustering algorithm for SURF descriptors, shown as Table <ref type="table" coords="3,463.03,255.55,3.77,11.09" target="#tab_1">2</ref>.</p><p>Before describing the detailed algorithm, we define two cluster distances: the intracluster D intra (C k ) and the inter-cluster D inter (C k , C l ). </p><formula xml:id="formula_1" coords="3,146.64,382.87,306.88,305.83">I i  C k and I j  C l for C k , C l  C and C k  C l if ( D intra (C k ∪ C l ) ≦ 1 × min{D intra (C k ), D intra (C l )}) //  1 is a constant. C = C ∪ {C k ∪ C l } -{C k } -{C l }; else SurfCluster(C i ∪ C j ); end if Case 2: I i  C k and I j  C k for C k  C if ( D inter (I j , C k ) ≦  2 × D intra (C k ) ) //  2 is a constant. C = C ∪ {C k ∪ {I j }}; else C = C ∪ {{I i , I j }}; end if Case 3: I i  C k and I j  C k for all C k  C C = C ∪ {{I i , I j }}; end while for C k , C l in C if (C k ∩ C l  ) if ( D intra (C k ∪ C l ) ≦ 1 × min{D intra (C k ), D intra (C l )}) C = C ∪ {C k ∪ C l } -{C k } -{C l }; else SurfCluster(C i ∪ C j ); end if end if end for , ) , ( | | 1 ) ( 2   j i SURF k k intra I I dist C C D for I i , I j  C k ; (2) , ) , ( | | 1 ) , (   j i SURF k k j inter I I dist C C I D for I i  C k .<label>(3)</label></formula><p>According to our observation, if the number of matching pairs is larger than four, the images look similar in visual. Hence, we define the similarity for SURF feature as</p><formula xml:id="formula_2" coords="4,206.88,252.81,259.73,33.95">. 4 4 ) , ( , 1 max ) , (             j i mp j i SURF I I N I I S (<label>4</label></formula><formula xml:id="formula_3" coords="4,466.60,262.51,3.89,11.09">)</formula><p>Other Visual Features. For other visual features, the clustering methods consider only the similarity between two images using the distance measures of Table <ref type="table" coords="4,427.35,320.29,3.76,11.09" target="#tab_2">3</ref>. The detailed algorithm is list as Table <ref type="table" coords="4,250.82,332.29,3.77,11.09" target="#tab_3">4</ref>. </p><formula xml:id="formula_4" coords="4,145.38,558.85,304.45,96.66">I i  I C = C ∪{{I i }}; end for for C k , C l in C if ( min{dist(C k , C l )} &lt; ) // is the threshold of the minimum distance. C = C ∪ {C k ∪ C l } -{C k } -{C l }; end if end for 2.2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Textual Features</head><p>The textual features are mainly extracted from EXIFs. There are totally 63 features in EXIFs; for example, ApertureValue, BrightnessValue, ColorSpace, CompressedBits-PerPixel, Contrast, etc. However, only two features, the GPS and the time, were considered and used in our methods. The values of the GPS and the time are also clustered by the same clustering algorithm of general visual features shown in Table <ref type="table" coords="5,465.56,215.71,5.01,11.09" target="#tab_3">4</ref> using L 1 distance measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>The Measure for Similarity Image Retrieval</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Normalization of Visual Features</head><p>The ranges of feature distances are quite different for all visual features. Before combining all the features to measure the similarity of images, the normalization process is necessary. We use the approximation proposed by Abramowitz &amp; Stegun <ref type="bibr" coords="5,447.06,334.21,11.68,11.09" target="#b6">[ 7]</ref> to approximate the values of normalization. The approximation step is very fast and accurate. Let x be the similarity between two images of an image feature, the normalization was calculated by the following equation,</p><formula xml:id="formula_5" coords="5,162.00,391.05,308.49,27.13">x b t x t b t b t b t b t b x x 0 5 5 4 4 3 3 2 2 1 1 1 ), ( ) )( ( 1 ) (             ,<label>(5)</label></formula><p>where (x) is the normal probability density function of the similarity degrees among all images in the feature, b 0 to b 5 are constants, and the absolute error |ε(x)| would be smaller than 7.5 × 10 -8 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Similarity Measures of Image Features</head><p>The Similarity Measure of Visual Features. Let I i , I j denote two images. Then the visual similarity between the images I i and I j , S V (I i , I j ), is defined as</p><formula xml:id="formula_6" coords="5,195.84,532.37,274.66,28.52">      k j i f k j i SURF j i V I I S w I I S I I S k ) , ( ) , ( ) , ( ,<label>(6)</label></formula><p>where S fk (I i , I j ) means the similarity between the images I i and I j of the k-th feature, w k is the weight of the k-th feature. Two weighting methods, the cluster weighting and the non-cluster weighting, are proposed as follows:</p><p>• Cluster Weighting. We use the clustering results of Section 2 to automatically weight the features. If a query image belongs to a cluster for a specific visual feature, the average similarity between the query image and each image in the cluster is computed as the weight of the specific visual feature.</p><p>• Non-Cluster Weighting. In this method, the weights w k are set to 1, except for the weights of AutoColorCorrelogram, Color Structure, and SURF features double other visual features.</p><p>The Similarity Measure of the GPS feature. Two distance similarity measures are proposed for the geographical distance:</p><p>• Boolean measure. The Boolean measure of the GPS feature is defined as</p><formula xml:id="formula_7" coords="6,159.48,248.67,307.12,26.41">    otherwise; 0 cluster, same the in are ) ( and ) ( if 1 ) , ( ) B ( j i j i G I GPS I GPS I I S (<label>7</label></formula><formula xml:id="formula_8" coords="6,466.60,254.83,3.89,11.09">)</formula><p>where GPS(I i ) and GPS(I j ) denote the values of the GPS feature in EXIF for I i , I j . • Similarity measure. The continuous similarity measure on geographical distance is defined as</p><formula xml:id="formula_9" coords="6,213.06,328.53,257.50,24.07"> radius I GPS I GPS dist j i G j i e I I S    )) ( ), ( ( ) S ( 1 1 ) , ( ,<label>(8)</label></formula><p>where  and radius are smoothing parameters; dist(GPS(I i ), GPS(I j )) means the real geographical distance on earth between the two positions GPS(I i ), GPS(I j ).</p><p>The Similarity Measure of the Time feature. Two time similarity measures are proposed for time duration:</p><p>• Boolean measure. The Boolean measure of the time feature is defined as where T(I i ) and T(I j ) denote the time feature in EXIF of I i and I j . • Similarity Measure. The continuous similarity measure on time is defined as</p><formula xml:id="formula_10" coords="6,227.94,519.31,136.44,19.68">  )) ( ), (<label>( 1 ) , ( )</label></formula><formula xml:id="formula_11" coords="6,212.64,525.39,253.69,13.61">S ( j t j i T I T I T dist I I S    . (<label>10</label></formula><formula xml:id="formula_12" coords="6,466.33,526.21,4.18,11.09">)</formula><p>where dist(T(I i ), T(I j )) denote the real time difference in second between two timestamp T(I i ) and T(I j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Ranking of Image Similarity</head><p>Finally, we define the similarity between two images I i and I j by integrate the features S V (I i , I j ), S G() (I i , I j ), and S T() (I i , I j ) into a linear combination. The image similarity Sim(I i , I j ) is defined as</p><formula xml:id="formula_13" coords="6,158.88,654.80,307.45,13.90">) , ( ) , ( ) , ( ) , ( ) ( ) ( j i T T j i G G j i V V j i I I S w I I S w I I S w I I Sim         . (<label>11</label></formula><formula xml:id="formula_14" coords="6,466.33,655.93,4.18,11.09">)</formula><p>Given a set of query images Q j , 1  j  m, the similarity of each query image Q j and the image I i in the image set is measured by Sim(I i , Q j ). The maximum similarity Sim(I i , Q j ) is the similarity degree of the image I i for the visual concept via the m query images Q j . It can be formally defined as</p><formula xml:id="formula_15" coords="7,252.96,204.07,213.37,16.44">)} , ( { max 1 j i m j Q I Sim   . (<label>12</label></formula><formula xml:id="formula_16" coords="7,466.33,204.07,4.18,11.09">)</formula><p>4 Experiments and Discussion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Environments</head><p>The system is implemented on a Microsoft Windows XP SP 3, 2.33 GHz PC with 3.00GB RAM. The developed software and related systems are written in Java language, so the system is cross-platform. The methods in five runs used different image features, which are shown in Table <ref type="table" coords="7,270.19,324.31,3.76,11.09">5</ref>. The notations in the table are: V stands for the visual features; G denotes the GPS feature; T is the time feature. While the parameter C, N means the cluster weighting and the non-clustering weighting, respectively. Finally, the parameter B represents the Boolean measures and S is the similarity measures.</p><p>Table <ref type="table" coords="7,245.05,398.19,3.38,8.10">5</ref>. Features we used in our methods.</p><p>Visual Features GPS Time</p><formula xml:id="formula_17" coords="7,180.66,428.35,226.56,73.55">V C V + G N B V + T N B V + G + T N B B G + T S S T S</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results of Subtask 1: Retrieval of Visual Concepts</head><p>In this subtask, 24 visual concept queries were given to be evaluated from the totally 32 concepts. The retrieval results for the visual concepts are evaluated by three different measures: precision, NDCG (normalize discount cumulative gain) <ref type="bibr" coords="7,413.69,565.81,10.52,11.09" target="#b7">[ 8]</ref>, and MAP (mean average precision). The experimental results are shown in Table <ref type="table" coords="7,411.08,577.81,3.76,11.09" target="#tab_5">6</ref>.</p><p>As Table <ref type="table" coords="7,174.85,589.81,5.01,11.09" target="#tab_5">6</ref> shows, the Run 5 using all of the image features is the best one for all measures. The second place is the Run 2 which uses the time feature only. The Run 3 with the visual features and the GPS feature is the third place. The Run 1 and the Run 4 are worse than the above three runs.</p><p>The results show that the visual features are not useful for most of the visual concepts in the task. The reason is that most of the concept topics are semantically related to each other. There is not much common characteristic in visual features among QBE images. While combining the visual features with the EXIF features, the performance increases obviously. The GPS feature can help us to find the images photographed in the neighboring positions easily. The geographic-related topics like "Asian temple &amp; palace" and "temple (ancient)" have good results. However, some topics are not expected to be good, like "animals" and "submarine scene," which returned high precision. The main reason is that a photographer generally tries to take pictures with the similar topics at the same place. Although the GPS feature is precise for geographicrelated topics, the missing values on the GPS feature will degrade the precision greatly. Some non-geographical topics have obviously bad results in the runs of using the GPS features, like "clouds." The time feature is also an important factor for searching personal photos. Since the images photographed in short time are usually very similar or dependent in visual concept. As the above discussion, the Run 5 getting the best results shows that our image similarity measure method can combine the different image features effectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results of Subtask 2: Retrieval of Events</head><p>In the subtask, totally 15 different events queries are given to find the pictures with the same event. Each query contains three QBE images. The evaluations are done by precision, NDCG, and MAP as the subtask 1. The experimental results are shown in Table <ref type="table" coords="8,149.97,630.67,3.76,11.09" target="#tab_6">7</ref>. As Table <ref type="table" coords="8,176.37,642.67,5.01,11.09" target="#tab_6">7</ref> shows, the best results are the Run 2 and Run 5. The Run 1 using the visual features is still the worst as the subtask 1. The Run 3 using the visual and the GPS features is a little better than the Run 4 taking the visual and the time features.</p><p>Owing to the event queries usually describe the images with the properties of happening in specific time duration or location area, the time and the GPS features are relatively important here. For example, the topics "Australia," "Bali," and "Egypt" are related in geographical; the topics of activities like "conference," "party," and "rock concert" are temporal-related. Hence, the provided EXIFs of the images are very useful in this subtask of events retrieval. The Run 5 combining all features is not expected to be the best as the subtask 1. The reason might be that the event queries are not so related with the visual concept, but highly dependent on time and location. However, the proposed similarity measure method did not degrade the precision much.  </p><formula xml:id="formula_18" coords="9,201.54,313.22,263.94,23.30">V G + T V + G V + T V + G + T w V w G w T w V w G w G w T w V w G w</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we proposed a framework and similarity measure methods to combine different image features for retrieving images from a set of conceptual photos. The proposed method can handle the visual concepts retrieval subtask in part. However, the time and position information are more important than other visual features in the event retrieval subtask. Although the proposed method could adjust the weights to fit the requirements, it has still a lot of problems to be solved. The proposed framework retrieved the relevant images weighted by manual in most of the cases. As we know, the feature selection is important in retrieval individual concept. For example, the experimental results show that the GPS and the time features are very useful for re-trieval in this dataset. However, it may be not so effective in other dataset. The problem of selecting and weighting the features automatically is a challenge in the task. This pilot task is its first year announced at ImageCLEF. The dataset seems too small for evaluating modern applications. Further, the concept queries often contain some irrelevant images in visual. The procedure of determining concepts and their relevant images may need to be fixed for providing as a benchmark.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,124.68,477.20,346.03,176.56"><head>Table 1 .</head><label>1</label><figDesc>The selected 7 features.</figDesc><table coords="2,124.68,495.67,346.03,158.09"><row><cell>Visual Features</cell><cell>Color</cell><cell>Shape</cell><cell>Texture</cell><cell>Descriptor</cell></row><row><cell>AutoColorCorrelogram</cell><cell>○</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BIC</cell><cell></cell><cell>○</cell><cell>○</cell><cell></cell></row><row><cell>CEDD</cell><cell>○</cell><cell>○</cell><cell></cell><cell></cell></row><row><cell>Color Structure</cell><cell>○</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Edge Histogram</cell><cell></cell><cell>○</cell><cell></cell><cell></cell></row><row><cell>FCTH</cell><cell>○</cell><cell>○</cell><cell></cell><cell></cell></row><row><cell>SURF</cell><cell></cell><cell></cell><cell></cell><cell>○</cell></row><row><cell cols="5">Visual Features Clustering. We first cluster images by the individual visual features</cell></row><row><cell cols="5">to find the groups of images with the similar visual features. Two different clustering</cell></row><row><cell cols="5">methods are developed for the SURF descriptor and the others visual features, respec-</cell></row><row><cell cols="3">tively. We depict the clustering algorithms in the following.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,146.64,303.56,290.78,91.55"><head>Table 2 .</head><label>2</label><figDesc>The clustering algorithm for SURF feature.</figDesc><table coords="3,146.64,322.03,290.78,73.08"><row><cell>Algorithm: SurfCluster</cell></row><row><cell>Input: the set of images I</cell></row><row><cell>Output: the clusters of images C</cell></row><row><cell>C = {}; while ( min{dist SURF (I i , I j )} &lt; ) //  is the threshold of SURF distance</cell></row><row><cell>Case 1:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,188.58,356.30,215.89,103.06"><head>Table 3 .</head><label>3</label><figDesc>Features and their distance measures.</figDesc><table coords="4,188.58,374.77,215.89,84.59"><row><cell>Visual Feature</cell><cell>Distance Measure</cell></row><row><cell>AutoColorCorrelogram</cell><cell>L 1 measure</cell></row><row><cell>BIC</cell><cell>L 1 measure</cell></row><row><cell>CEDD</cell><cell>Tanimoto measure</cell></row><row><cell>Color Structure</cell><cell>L 1 measure</cell></row><row><cell>Edge Histogram</cell><cell>L 1 measure</cell></row><row><cell>FCTH</cell><cell>Tanimoto measure</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,145.38,491.78,262.50,79.31"><head>Table 4 .</head><label>4</label><figDesc>The clustering algorithm for general visual features.</figDesc><table /><note coords="4,146.70,510.25,104.76,11.10;4,145.38,523.75,103.62,11.10;4,145.38,535.75,129.69,11.10;4,145.38,547.75,30.32,11.10;4,145.38,562.08,12.85,9.02"><p><p>Algorithm: VisualCluster</p>Input: the set of images I Output: the cluster of images C C = {}; for</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,131.34,316.22,332.56,238.63"><head>Table 6 .</head><label>6</label><figDesc>Performance on retrieval of visual concepts.</figDesc><table coords="8,131.34,335.72,332.56,219.13"><row><cell></cell><cell>Run 1</cell><cell>Run 2</cell><cell>Run 3</cell><cell>Run 4</cell><cell>Run 5</cell></row><row><cell>Features</cell><cell>V</cell><cell>T</cell><cell>V + G</cell><cell>G + T</cell><cell>V + G + T</cell></row><row><cell>Weights</cell><cell>w V 1</cell><cell>w T 1</cell><cell>w V 0.45 0.17 w G</cell><cell cols="2">w G 0.975 0.025 0.45 0.18 0.22 w T w V w G w T</cell></row><row><cell>P@5</cell><cell>0.6750</cell><cell>0.8000</cell><cell>0.7667</cell><cell>0.6500</cell><cell>0.8333</cell></row><row><cell>P@10</cell><cell>0.6125</cell><cell>0.7292</cell><cell>0.6583</cell><cell>0.6500</cell><cell>0.7833</cell></row><row><cell>P@15</cell><cell>0.5778</cell><cell>0.6667</cell><cell>0.6222</cell><cell>0.6083</cell><cell>0.7222</cell></row><row><cell>P@20</cell><cell>0.5354</cell><cell>0.6354</cell><cell>0.6104</cell><cell>0.5771</cell><cell>0.6896</cell></row><row><cell>P@30</cell><cell>0.4486</cell><cell>0.6083</cell><cell>0.5639</cell><cell>0.5611</cell><cell>0.6347</cell></row><row><cell>P@100</cell><cell>0.3054</cell><cell>0.4117</cell><cell>0.3925</cell><cell>0.3925</cell><cell>0.4379</cell></row><row><cell>NDCG@5</cell><cell>0.5701</cell><cell>0.5858</cell><cell>0.5800</cell><cell>0.4073</cell><cell>0.6405</cell></row><row><cell>NDCG@10</cell><cell>0.5062</cell><cell>0.5348</cell><cell>0.5184</cell><cell>0.4268</cell><cell>0.6017</cell></row><row><cell>NDCG@15</cell><cell>0.4798</cell><cell>0.5028</cell><cell>0.4951</cell><cell>0.4123</cell><cell>0.5658</cell></row><row><cell>NDCG@20</cell><cell>0.4545</cell><cell>0.4836</cell><cell>0.4872</cell><cell>0.4066</cell><cell>0.5459</cell></row><row><cell>NDCG@30</cell><cell>0.4016</cell><cell>0.4728</cell><cell>0.4615</cell><cell>0.4046</cell><cell>0.5213</cell></row><row><cell cols="2">NDCG@100 0.3303</cell><cell>0.4144</cell><cell>0.3979</cell><cell>0.3717</cell><cell>0.4436</cell></row><row><cell>MAP@30</cell><cell>0.0632</cell><cell>0.0952</cell><cell>0.0906</cell><cell>0.0854</cell><cell>0.1026</cell></row><row><cell>MAP@100</cell><cell>0.0930</cell><cell>0.1589</cell><cell>0.1558</cell><cell>0.1518</cell><cell>0.1777</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,123.96,280.22,326.16,42.96"><head>Table 7 .</head><label>7</label><figDesc>Performance on retrieval of events.</figDesc><table coords="9,123.96,299.72,326.16,23.46"><row><cell>Run 1</cell><cell>Run 2</cell><cell>Run 3</cell><cell>Run 4</cell><cell>Run 5</cell></row><row><cell>Features</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,132.68,265.22,340.33,9.96;10,141.72,276.20,169.11,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,193.27,265.22,263.39,9.96">Overview of the Personal Photo Retrieval Pilot Task at ImageCLEF 2012</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zellhöfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,141.72,276.20,94.52,9.96">CLEF 2012 working notes</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.68,287.18,337.99,9.96;10,141.72,298.22,328.86,9.96;10,141.72,309.20,88.31,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,346.80,287.18,123.86,9.96;10,141.72,298.22,34.42,9.96">Image Indexing Using Color Correlograms</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,182.60,298.22,287.98,9.96;10,141.72,309.20,13.01,9.96">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="762" to="768" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.68,320.18,337.93,9.96;10,141.72,331.22,328.91,9.96;10,141.72,342.20,328.90,9.96;10,141.72,353.18,39.78,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,335.34,320.18,135.27,9.96;10,141.72,331.22,236.11,9.96">A Compact and Efficient Image Retrieval Approach Based on Border/Interior Pixel Classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">O</forename><surname>Stehling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">X</forename><surname>Falcão</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,399.78,331.22,70.85,9.96;10,141.72,342.20,289.46,9.96">Proceedings of the eleventh international conference on Information and knowledge management</title>
		<meeting>the eleventh international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="102" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.68,364.22,338.03,9.96;10,141.72,375.20,328.90,9.96;10,141.72,386.18,328.87,9.96;10,141.72,397.22,24.00,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,287.95,364.22,182.76,9.96;10,141.72,375.20,196.52,9.96">CEDD: Color and Edge Directivity Descriptor. A Compact Descriptor for Image Indexing and Retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,357.27,375.20,113.35,9.96;10,141.72,386.18,177.59,9.96">Proceedings of the 6th International Conference on Computer Vision Systems</title>
		<meeting>the 6th International Conference on Computer Vision Systems</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="volume">5008</biblScope>
			<biblScope unit="page" from="312" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.68,408.20,337.97,9.96;10,141.72,419.18,328.98,9.96;10,141.72,430.22,309.77,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,285.30,408.20,185.35,9.96;10,141.72,419.18,164.57,9.96">FCTH: Fuzzy Color and Texture Histogram a Low Level Feature for Accurate Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,327.78,419.18,142.92,9.96;10,141.72,430.22,236.33,9.96">Proceedings of the ninth International Workshop on Image Analysis for Multimedia Interactive Services</title>
		<meeting>the ninth International Workshop on Image Analysis for Multimedia Interactive Services</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="191" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.68,441.20,337.98,9.96;10,141.72,452.18,307.55,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,281.43,441.20,133.79,9.96">SURF: Speeded-Up Robust Features</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,435.33,441.20,35.32,9.96;10,141.72,452.18,135.08,9.96">9th European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">3951</biblScope>
			<biblScope unit="page" from="404" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.68,463.22,337.84,9.96;10,141.72,474.20,305.24,9.96" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="10,272.30,463.22,198.22,9.96;10,141.72,474.20,119.95,9.96">Handbook of Mathematical Functions: with Formulas, Graphs, and Mathematical Tables</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Abramowitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">A</forename><surname>Stegun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965">1965</date>
			<publisher>Dover Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,485.18,337.89,9.96;10,141.72,496.22,252.02,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,249.21,485.18,193.28,9.96">Cumulated Gain-based Evaluation of IR Techniques</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,450.06,485.18,20.50,9.96;10,141.72,496.22,132.89,9.96">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
