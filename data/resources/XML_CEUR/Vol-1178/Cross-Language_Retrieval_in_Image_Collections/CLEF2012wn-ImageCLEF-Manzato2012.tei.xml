<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,160.08,115.72,295.12,12.93;1,160.80,133.72,293.75,12.93">The participation of IntermidiaLab at the ImageCLEF 2012 Photo Annotation Task</title>
				<funder ref="#_9MnbabD">
					<orgName type="full">FAPESP</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,262.80,171.33,89.82,9.96"><forename type="first">Marcelo</forename><forename type="middle">G</forename><surname>Manzato</surname></persName>
							<email>mmanzato@icmc.usp.br</email>
							<affiliation key="aff0">
								<orgName type="department">Mathematics and Computing Institute</orgName>
								<orgName type="institution">University of São Paulo</orgName>
								<address>
									<addrLine>Av. Trabalhador Sancarlense, 400</addrLine>
									<postBox>PO Box 668</postBox>
									<postCode>-13560-970</postCode>
									<settlement>São Carlos</settlement>
									<region>SP</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,160.08,115.72,295.12,12.93;1,160.80,133.72,293.75,12.93">The participation of IntermidiaLab at the ImageCLEF 2012 Photo Annotation Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C67C861435BAD6CE1FB6F3EA23CFE776</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Automatic annotation</term>
					<term>concept classification</term>
					<term>user tags</term>
					<term>folksonomy</term>
					<term>enrichment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the results of our concept annotation tool in the ImageCLEF 2012 Photo Annotation Task. Our approach is based only on textual features, in particular collaborative user tags. It faces some of the challenges of using user-generated terms, such as noise and incompleteness. The first one is supported by a multi-phase filtering procedure that amends misspelled tags and considers only those semantically related to the actual context of the image. The second one, in turn, is reduced with a tag enrichment activity which aggregates related terms to the actual list of tags in order to make annotation more effective.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The overload of multimedia content available on the Web has introduced new challenges for search engines to retrieve, organize and classify the data according to the user's needs. One important and common step of these tasks is the extraction of meaningful information describing the content. Typically, the provision of such metadata is accomplished by content authors or providers, who create well-structured information about specific media items, helping search and analysis tasks. Automatic approaches may also be adopted, which create multimedia descriptions without human intervention, analyzing low level visual features in order to infer semantic information.</p><p>It is known that automatic and manual indexing techniques have limitations that may affect the process of gathering semantic information. Indeed, the lack of methods to provide meaningful descriptions has been named semantic gap <ref type="bibr" coords="1,134.76,595.65,14.69,9.96" target="#b14">[15]</ref>, and over ten years researchers have been working on the development of strategies to overcome related problems. For instance, in case of automatic approaches, usually they are highly dependent on the content domain, once they infer semantic information based on low-level visual features <ref type="bibr" coords="1,402.24,631.53,9.99,9.96" target="#b1">[2]</ref>. In manual approaches, in turn, the description is considered a time-consuming and error prone task <ref type="bibr" coords="1,156.12,655.41,14.69,9.96" target="#b12">[13]</ref>.</p><p>In face of the huge amount of multimedia indexing techniques proposed so far, the ImageCLEF competitions<ref type="foot" coords="2,286.44,129.18,3.97,5.12" target="#foot_0">1</ref> play an important role in the context of image retrieval. Each year, a set of specific challenging problems is proposed to researchers with the objective to push forward the state-of-art, allowing the resulting techniques to be discussed and compared against each other using a unique and standardized set of requirements. One of these problems is the Photo Annotation Task, which consists of a multi-label classification challenge, that aims to analyze a collection of Flickr<ref type="foot" coords="2,301.08,200.94,3.97,5.12" target="#foot_1">2</ref> photos in terms of their visual and/or textual features in order to detect the presence of one or more concepts.</p><p>Particularly in the case of textual features, the dataset of images made available by ImageCLEF to be annotated includes Flickr tags, which are a set of terms created by users to facilitate further search by themselves. Considering other benefits, some authors <ref type="bibr" coords="2,260.28,261.81,11.21,9.96" target="#b8">[9]</ref>[20][1] <ref type="bibr" coords="2,297.65,261.81,14.95,9.96" target="#b20">[21]</ref> have explored collaborative tagging to create folksonomies and semantic relationships based on co-occurrence of tags. With such structures, it is possible to gather semantic information from the content in a collaborative way, reducing the main problems related to traditional indexing approaches.</p><p>This paper presents the techniques developed by our group to handle the ImageCLEF 2012 Photo Annotation Task <ref type="bibr" coords="2,321.96,333.57,14.69,9.96" target="#b17">[18]</ref>. Unlike the majority of the participants who explore visual features or a combination of visual and textual features, we focus only on user tags. Although many of the defined concepts are difficult to be found using only text, we believe exploring textual information is a good start point to design a robust and multimodal annotation approach.</p><p>This paper is structured in the following way. Section 2 presents the related work which depict other text-based approaches proposed in previous years of ImageCLEF. Section 3 provides the prior steps in our algorithm which consist of a preparation procedure to gather semantic information from the training set. Section 4 describes the concept annotator algorithm proposed in this paper. Section 5 presents the official results of the evaluation. Finally, Sections 6 and 7 present the final remarks, future work and acknowledgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Analyzing the last two years of ImageCLEF Photo Annotation Task, it can be noted that the majority of approaches is based on visual features or a combination of visual and textual features. In 2010, from 54 groups registered for the task, only two groups submitted runs in the textual category only. One example is the work of Li et al. <ref type="bibr" coords="2,276.96,567.45,9.99,9.96" target="#b8">[9]</ref>, which used textual features (user tags and EXIF information) to perform automatic annotation. Their approach starts with a document expansion procedure which assigns additional content to concepts and image tags by consulting external information resources, such as DBpedia 3 . After that, expanded textual metadata is compared to expanded concepts in order to make concept assumptions. They also provide a method to assign concepts to images by analyzing EXIF data, such as the time a photo was shot. Finally, additional concepts are considered by infering affiliations and opposite relations among them.</p><p>In 2011, 48 groups registered for the challenge, and seven submitted at least one run in the textual category only. One of the submissions of Daróczy et al. <ref type="bibr" coords="3,467.28,178.53,9.99,9.96" target="#b5">[6]</ref>, for instance, was a text-based approach that uses Flickr tags to create a kernel weighting procedure based on similarity of images which is computed using the Jensen-Shannon divergence.</p><p>In addition to this similarity calculation, other metrics were also used, such as the ones based on WordNet<ref type="foot" coords="3,274.56,237.54,3.97,5.12" target="#foot_3">4</ref> ontology. Indeed, Liu et al. <ref type="bibr" coords="3,411.60,238.65,15.60,9.96" target="#b9">[10]</ref> proposed a method to extract text features for concept detection based on a semantic distance between words, which are expected to capture the semantic meanings from images. In total, ten features were extracted, which are based on different types of information, including semantic similarities according to WordNet ontology, affective norms for English words, etc.</p><p>Znaidia et al. <ref type="bibr" coords="3,211.92,310.89,15.60,9.96" target="#b20">[21]</ref> submitted a text-based approach that uses two distances among tags and visual concepts: one based on Wordnet ontology and the other based on social networks. The correlation between tags and visual concepts is also explored by Amel et al. <ref type="bibr" coords="3,241.20,346.65,9.99,9.96" target="#b0">[1]</ref>, who proposed an approach that uses Flickr tags to extract contextual relationships of tags and concepts. Two types of contextual graphs are modeled: an inter-concepts graph and a concept-tags graph. The similarity among tags and concepts is computed based on the principle of Google distance <ref type="bibr" coords="3,173.52,394.53,9.99,9.96" target="#b4">[5]</ref>.</p><p>In order to use the Flickr tags, it is important to pre-process them to reduce the occurrence of noise, and give more importance to particular terms. These two steps are performed by Izawa et al. <ref type="bibr" coords="3,304.44,430.77,9.99,9.96" target="#b6">[7]</ref>, who submitted an approach that first performs a morphological analysis on all terms of the training images. Next, a tf-idf score is assigned to each tag. Then, the system stores the correspondence information for the tags and concepts in a casebase. Finally, the annotation on a given test image is accomplished by matching the tags attached to the image to the tags of the casebase.</p><p>The tf-idf score is also used by Nagel et al. <ref type="bibr" coords="3,355.56,502.89,14.69,9.96" target="#b11">[12]</ref>, who submitted a textonly approach consisting of a pre-processing step on Flickr tags to reduce noise, followed by tf-idf assignment for each term in order to construct a text-based descriptor.</p><p>Spyromitros-Xioufis et al. <ref type="bibr" coords="3,261.72,551.13,15.60,9.96" target="#b15">[16]</ref> proposed a textual model that uses the boolean bag-of-words representation, in addition to stemming, stop words removal, and feature selection using the chi-squared-max method <ref type="bibr" coords="3,358.56,575.01,9.99,9.96" target="#b7">[8]</ref>. After that, the extracted features were used in a multi-label learning algorithm based on Ensemble of Classifier Chains <ref type="bibr" coords="3,210.84,599.01,15.60,9.96" target="#b13">[14]</ref> The aforementioned text-based approaches differ from ours because we provide a co-occurrence-based tag enrichment phase in order to overcome the incompleteness of terms in resources that do not have enough associated tags. In addition, based on the enriched tags list, we also calculate the co-occurrence of concepts and tags during training to highlight the most frequent concepts by given tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training</head><p>Our automatic annotator starts with a training procedure whose main objective is to gather semantic information from the available corpus already annotated. In addition, all user tags must be filtered and analyzed in order to remove incomplete and/or incorrect tags. Figure <ref type="figure" coords="4,303.00,232.41,4.98,9.96" target="#fig_0">1</ref> illustrates the overall schema. Although in this participation we are annotating only images, our approach may be used with any other multimedia modality, such as video and audio, unless the content does not have any associated tags. All available tags about an image (Figure <ref type="figure" coords="4,336.24,427.77,20.96,9.96" target="#fig_0">1 (a)</ref>) are submitted into a module which will filter incorrect and/or incomplete terms. This procedure has the objective to eliminate noise which can affect later the quality of concept classification. After that, all terms are used to create a folksonomy of tags (Figure <ref type="figure" coords="4,475.56,463.65,4.98,9.96" target="#fig_0">1</ref> (c)), so that semantically related terms can be obtained during the annotation of images with few associated tags. A folksonomy of concepts is also constructed based on their co-occurrence, as indicated in Figure <ref type="figure" coords="4,362.88,499.53,20.13,9.96" target="#fig_0">1 (d)</ref>. The idea behind this process is to identify semantically related concepts, and then, assign those related concepts based on the ones already found. For instance, if an image was annotated with the concept "combustion flames", so it will also be annotated with "combustion smoke". Finally, we use both concepts and terms of each image to create a table containing the most frequent concept for each tag (Figure <ref type="figure" coords="4,134.76,571.29,19.75,9.96" target="#fig_0">1 (e)</ref>). This table will be used during classification to predict possible concepts according to tags assigned by the user. Next subsections describe the training steps in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Filtering Tags</head><p>The process of filtering noisy tags is divided into two phases. The first one is conducted to amend or discard misspelled, incorrect and/or incomplete terms, being most of the process supported by lexicon resources and syntactic analysis of the terms. The second phase consists of checking the meaning of the tags in order to decide if such terms are semantically related to the most frequent concepts in the training set. This subsection describes the first phase of the filtering procedure; the second one, because it is executed as a further step of the annotation algorithm, is depicted in details in Subsection 4.3.</p><p>The synctatic analysis of the given tags is similar to the one proposed by Cantador et al. <ref type="bibr" coords="5,205.80,202.05,9.99,9.96" target="#b2">[3]</ref>. Figure <ref type="figure" coords="5,254.88,202.05,4.98,9.96" target="#fig_1">2</ref> illustrates the main steps. It starts with a lexical checking (Figure <ref type="figure" coords="5,209.52,213.93,4.98,9.96" target="#fig_1">2</ref> (a)) which will discard all stop-words and terms that contain digits and/or their length is less than 2 or bigger than 25 characters. After that, the tags are checked against WordNet (Figure <ref type="figure" coords="5,404.88,408.57,21.20,9.96" target="#fig_1">2 (b)</ref>) in order to find existent words. If it was not found, the tag may be misspelled. Consequently, we check it against a local dictionary (Figure <ref type="figure" coords="5,327.96,432.45,19.43,9.96" target="#fig_1">2 (c)</ref>) to find possible correct lexical variations of that term. As opposite to the algorithm proposed by Cantador et al., which uses a Google connector to check lexical variations, we use the Levenshtein distance <ref type="bibr" coords="5,173.04,468.33,15.60,9.96" target="#b10">[11]</ref> among the tag and possible candidates, working similarly to Google Did You Mean, though the latter also considers proper names. The reason for not having used the Google version is that it restricts the number of queries per day. In our case, tags consisting of proper names will be investigated in future work through the use of Wikipedia ontologies, such as Yago2<ref type="foot" coords="5,400.92,514.98,3.97,5.12" target="#foot_4">5</ref> or DBpedia.</p><p>After the local dictionary module has indicated possible lexical variations of the term, they are checked again on WordNet in order to make sure it is a valid English term existent in such database (Figure <ref type="figure" coords="5,342.00,551.97,20.04,9.96" target="#fig_1">2 (d)</ref>). If it was found, the term will be considered in the system (Figure <ref type="figure" coords="5,316.56,563.97,20.11,9.96" target="#fig_1">2 (e)</ref>); otherwise, it will be discarded (Figure <ref type="figure" coords="5,170.04,575.97,4.98,9.96" target="#fig_1">2</ref> (f)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Statistics Calculation</head><p>After syntactic analysis, the training images and their associated tags are used to create three statistic tables, as illustrated in Figure <ref type="figure" coords="5,379.56,636.57,41.07,9.96" target="#fig_0">1 (c), (d)</ref> and<ref type="figure" coords="5,444.60,636.57,11.16,9.96">(e)</ref>. The first two are the tag and concept folksonomies, and the last is the one which relates concepts and tags. As will be described in next section, they are used to infer related tags and concepts from the corpus, supporting the semantic classification.</p><p>The folksonomies are based on the relatedness measure described by Cattuto et al. <ref type="bibr" coords="6,159.24,178.17,9.99,9.96" target="#b3">[4]</ref>, who create a folksonomy based on co-occurrence of tags. They define it as a weighted undirected graph whose set of vertex is the set T of all tags from all images. Two tags t 1 and t 2 are connected by an edge if and only if t 1 , t 2 ∈ T s , where T s is the set of tags assigned to image s ∈ S. The weight of this edge is given by the number of times t 1 and t 2 co-occurred, i.e., w(t</p><formula xml:id="formula_0" coords="6,134.76,225.93,345.88,22.34">1 , t 2 ) = |{s ∈ S | t 1 , t 2 ∈ T s }|.</formula><p>For the concept folksonomy, the same process is adopted, but instead of considering co-occurrence of tags, we check the co-occurrence of concepts assigned to each image from the training set. Two concepts c 1 and c 2 are connected by an edge if and only if c 1 , c 2 ∈ C s , where C s is the set of concepts assigned to image s ∈ S during training. The weight of this edge is given by the number of times c 1 and c 2 co-occurred, i.e., w(c</p><formula xml:id="formula_1" coords="6,295.44,309.69,131.76,10.33">1 , c 2 ) = |{s ∈ S | c 1 , c 2 ∈ C s }|.</formula><p>Another statistic table created from the training set is the one which relates concepts and tags. We calculate the number of times a concept and a tag cooccurred in an image. Given a pair (c, t), where c ∈ C is a concept and C is the set of all concepts, we assume they are related if and only if c ∈ C s and t ∈ T s . A weight between them is also defined, i.e., w(c, t)</p><formula xml:id="formula_2" coords="6,352.92,369.57,127.68,10.34">= |{s ∈ S | c ∈ C s &amp; t ∈ T s }|.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Automatic Annotation</head><p>This section describes the automatic annotation algorithm proposed in this paper. It consists of analyzing the available tags of each image in order to infer possible related concepts according to the folksonomies and statistical data constructed during training.</p><p>Figure <ref type="figure" coords="6,182.52,473.61,4.98,9.96" target="#fig_2">3</ref> illustrates the overall schema. User tags associated to an image (Figure <ref type="figure" coords="6,169.68,485.61,4.98,9.96" target="#fig_2">3</ref> (a)) are first filtered using the same procedure described in Subsection 3.1 (Figure <ref type="figure" coords="6,183.96,497.49,19.08,9.96" target="#fig_2">3 (b)</ref>). After that, some images may have not enough terms to support correct concept classification. Thus, we add a tag enrichment phase (Figure <ref type="figure" coords="6,475.56,509.49,4.98,9.96" target="#fig_2">3</ref> (c)) which will expand the set of available tags with related ones obtained from the tag folksonomy created previously during training.</p><p>When an image has enough terms, it can be annotated. This task is accomplished by means of three strategies, illustrated in Figure <ref type="figure" coords="6,392.16,557.37,3.90,9.96" target="#fig_2">3</ref>: direct recognition (d), most frequent concepts by tag (e) and post processing (f). As a result of all three classification steps, the multimedia content is annotated with all obtained concepts (Figure <ref type="figure" coords="6,210.48,593.25,19.99,9.96" target="#fig_2">3 (g)</ref>). Next subsections describe in details all involved steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tag Enrichment</head><p>Although concepts are inferred based on the content of each tag, it is possible to improve the classification results by enriching the set of tags with related terms that users usually use together to tag a resource. When more tags are available, the classifier will be able to better predict concepts because more information about the content will be analyzed.</p><p>The tag enrichment procedure is implemented in our system by considering a number of related terms t ′ to a given tag t. This relatedness between tags is defined proportionally to the edge weights of the tag folksonomy; consequently, given a tag t ∈ T , the tags that are most related to it are all the tags t ′ ∈ T with t = t ′ such that w(t, t ′ ) is maximal.</p><p>Let us consider, for instance, the image illustrated in Figure <ref type="figure" coords="7,416.28,454.77,3.90,9.96" target="#fig_3">4</ref>, whose set of tags are "pittsburgh", "sky" and "clouds". Applying the tag enrichment procedure will result in the three subsets illustrated in Figure <ref type="figure" coords="7,383.16,478.65,19.89,9.96" target="#fig_3">4 (a)</ref>, where the terms that are most related to "pittsburgh" are "sky", "clouds", "yellow", "downtown" and "tree"; the same for the original tags "sky" and "clouds". As a result of gathering the related terms from all tags t ∈ T s , we obtain a list R s composed of related terms and associated weights. A weight value is calculated according to the corresponding term's frequency of retrieval by using all tags t. Then, these weights are used to sort the list in descending order. Figure <ref type="figure" coords="8,165.72,166.17,4.98,9.96" target="#fig_3">4</ref> (b) illustrates this step, which results in a list R s composed of the most frequent terms occurred in the previous subsets.</p><p>Following, we empirically define a minimal number of tags T to allow each image to be classified. The value of | T | includes the terms already associated by the user, plus the related terms obtained from the tag folksonomy. Thus, we consider the n s first terms from list R s , whose value is given by:</p><formula xml:id="formula_3" coords="8,224.88,245.25,251.50,24.36">n s = | T | -|T s | if |T s | &lt; | T | , or 0 otherwise. (<label>1</label></formula><formula xml:id="formula_4" coords="8,476.38,253.77,4.25,9.96">)</formula><p>Consequently, if we set | T | = 5, the image illustrated in Figure <ref type="figure" coords="8,421.92,278.85,4.98,9.96" target="#fig_3">4</ref> will be classified using the original tags "pittsburgh", "sky" and "clouds", plus the extended ones "tree" and "blue".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Direct Recognition</head><p>The annotation task effectively starts with direct recognition (Figure <ref type="figure" coords="8,451.08,353.61,21.48,9.96" target="#fig_2">3 (d)</ref>), which will check whether a user has tagged an image with a term that explicitly refers to a concept. In order to accomplish that, each predefined concept was manually designated a meaningful alias, which was chosen to be the most likely term used by an individual to tag a resource with that concept. For instance, for concept "flora flower", its alias was defined "flower"; for concept "weather rainbow", its alias was defined "rainbow". Furthermore, with the objective to improve the results of direct recognition, we also consider the fact that many tags may be in the plural or singular forms, and users may use a synonym word to refer to the same concept. Thus, we first use an inflector object to transform all tags and concepts to their singular form, in addition to a synonym vocabulary that contains different synonyms for each concept. Consequently, if a user has tagged an image with "kids", so the system will first transform it to the singular form "kid", which is a synonym of "child". As a result, as "child" is the alias for the concept "age child", this concept will be added into the annotation of the given image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Most Frequent Concepts</head><p>The next step of concept annotation is to obtain the most frequent concepts by tag using the concept-tag relation described in Subsection 3.2. Given a tag t ∈ T s , we collect the C t most frequent concepts related to it, i.e., C t will have a subset of concepts c ∈ C such that w(c, t) is maximal. Considering the same example presented in Figure <ref type="figure" coords="8,262.32,631.53,3.90,9.96" target="#fig_3">4</ref>, in Figure <ref type="figure" coords="8,317.04,631.53,4.98,9.96" target="#fig_4">5</ref> (a) we illustrate this step by listing the five most frequent concepts for each given tag: "pittsburgh", "sky", "clouds", "tree" and "blue". Once we have some concepts candidates given by C t , it is still necessary to check if they are semantically related to the tag t. This is because the concept-tag relation only provides concepts relying on statistic information, it does not ensure that the most frequent concepts have a semantic relation to the given tag. For instance, the tag "explore" is frequently used by users to tag resources in many contexts, though it is not semantically related to the image's actual concepts. Consequently, if this tag is used to retrieve the most frequent concepts, the list will be prone to noise. To solve this problem, we use two semantic similarity measures to decide whether c ′ ∈ C t will be adopted in final annotation or not. The first measure, sim wp , is the Wu &amp; Palmer approach <ref type="bibr" coords="9,382.92,452.49,14.69,9.96" target="#b18">[19]</ref>, which is based on the topology of the WordNet ontology. The second one, sim res , is the Resnik semantic similarity measure <ref type="bibr" coords="9,259.20,476.37,14.69,9.96" target="#b16">[17]</ref>, which is based on information content. Therefore, our algorithm will combine both similarity measures in order to find a concept c ′ ∈ C t that is semantically related to tag t, that is:</p><formula xml:id="formula_5" coords="9,180.84,529.14,299.78,26.91">c s + = c ′ if sim wp ( max c ′ ∈Ct [sim res (alias(c ′ ), t)], t) ≥ Γ ⊘ otherwise ,<label>(2)</label></formula><p>where Γ is a threshold, and alias(c ′ ) returns the alias of concet c ′ as explained in Subsection 4.2. This process is executed for all tags associated to image s. Thus, the system will find a set of concepts C s such that each element c s ∈ C s is a concept that is most semantically similar to a tag t ∈ T s .</p><p>Considering the previous example, Figure <ref type="figure" coords="9,339.96,619.53,4.98,9.96" target="#fig_4">5</ref> (b) illustrates the application of Resnik semantic similarity between each tag and the alias of each concept candidate. For each tag, we select the concept with highest similarity, which is computed again using the Wu &amp; Palmer metric (Figure <ref type="figure" coords="9,387.24,655.41,4.98,9.96" target="#fig_4">5</ref> (c)). Then, using a predefined value for Γ , we select the concepts to be included in the annotation of the given image (Figure <ref type="figure" coords="10,253.32,130.29,20.28,9.96" target="#fig_4">5 (d)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Post Processing</head><p>As the final step of the annotation task, we designed a post processing procedure whose goals are twofold: i) to include related concepts to the ones already found; and ii) to discard mutual exclusive concepts by comparing each pair of annotated concepts using a predefined set of heuristics.</p><p>In the first case, we use a Naïve Bayes approach <ref type="bibr" coords="10,367.08,234.57,15.60,9.96" target="#b10">[11]</ref> in order to determine whether, given an annotated concept c, other concepts should be included as well. Thus, we define:</p><formula xml:id="formula_6" coords="10,217.32,282.33,263.31,21.84">c s + = c j ∈ C if P (c j |c) ≥ P (!c j |c) ⊘ otherwise ,<label>(3)</label></formula><p>where P (c j |c) is the probability of including c j in the annotation given the annotated concept c; and P (!c j |c) is the probability of not including c j in the annotation given the annotated concept c. We estimate both probabilities as:</p><formula xml:id="formula_7" coords="10,206.64,361.53,273.99,37.45">P (c j |c) ≈ P (c j )P (c|c j ) = |S cj | |S| w(c j , c) ci∈C w(c i , c j ) ,<label>(4)</label></formula><formula xml:id="formula_8" coords="10,199.92,411.81,280.71,37.45">P (!c j |c) ≈ P (!c j )P (c|!c j ) = |S !cj | |S| w(!c j , c) ci∈C w(c i , !c j ) ,<label>(5)</label></formula><p>where S cj is the subset of images from the training set annotated with concept c j ; and S !cj is the subset of images from the training set which were not annotated with concept c j . After including related concepts, the final procedure of our annotation tool is to discard mutual exclusive concepts. To do that, we first design manually a list of pairs of concepts which are mutual exclusive, such as "view indoor" versus "view outdoor", "sentiment active" versus "sentiment inactive", and so on. Then, given a pair of conflicting concepts (c 1 , c 2 ) in the preliminary annotation list, we apply the following heuristics in order to discard the concept(s) in conflict:</p><p>1. If c 1 and c 2 were found by direct recognition, then keep both concepts. 2. If c 1 was found by direct recognition and c 2 was not, then discard c 2 . 3. If c 1 and c 2 were not found by direct recognition, discard the concept with less votes from all classifiers. 4. If c 1 and c 2 were not found by direct recognition and both have the same amount of votes, discard both concepts.</p><p>This section presents the official results obtained by our group in the Image-CLEF 2012 Photo Annotation Task <ref type="bibr" coords="11,298.92,156.21,14.69,9.96" target="#b17">[18]</ref>. Table <ref type="table" coords="11,349.80,156.21,4.98,9.96" target="#tab_0">1</ref> presents the overall results. We submitted two runs to be evaluated: the "proposed" one, which is the implementation of the techniques described in this paper; and a simpler version based only on tags enrichment and Naïve Bayes. This year, ten groups submitted at least one run in the textual category; comparing the best results from each group, our technique was able to achieve the eighth place. Figure <ref type="figure" coords="11,181.44,341.49,4.98,9.96" target="#fig_5">6</ref> presents the F1 results for each of the 93 predefined concepts. We note that better results were achieved for the concepts in the classes "fauna" and "transport", once users tend to use the concepts names to tag their resources. Furthermore, other particular concepts were also able to be correctly classified, such as "combustion fireworks", "setting fooddrink", "quantity none" and "quality noblur", being the first two recognized mostly by direct classification and most frequent concepts by tag, and the last two by the post processing step.  This paper presented the results of the automatic annotation tool evaluated by ImageCLEF 2012 Photo Annotation Task. It uses different strategies based on co-occurrence of tags and concepts in order to reduce two problems related to the use of tags in classification: noise and incompleteness. The first one is supported by a multi-phase filtering procedure that amends misspelled tags and considers only those semantically related to the actual context of the image. The second one, in turn, is reduced with a tag enrichment activity which aggregates related terms to the actual list of tags in order to make annotation more effective.</p><p>The results shown by this evaluation indicate that more research should be made in order to improve the quality of annotations. This shall be made in future work, as we plan to investigate how to recognize specific concepts which are difficult to be found using only textual features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,252.48,394.56,110.44,8.97;4,197.45,300.18,220.14,80.05"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Training procedure.</figDesc><graphic coords="4,197.45,300.18,220.14,80.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,252.12,375.72,111.04,8.97;5,203.33,258.59,208.44,102.92"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Filtering procedure.</figDesc><graphic coords="5,203.33,258.59,208.44,102.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,246.60,301.92,122.20,8.97;7,217.13,116.02,180.94,171.58"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Annotation procedure.</figDesc><graphic coords="7,217.13,116.02,180.94,171.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,238.20,654.48,138.99,8.97;7,180.05,546.56,255.04,93.59"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Tag enrichment procedure.</figDesc><graphic coords="7,180.05,546.56,255.04,93.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,225.48,308.40,164.20,8.97;9,166.61,116.14,282.04,178.06"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Most frequent concepts classifier.</figDesc><graphic coords="9,166.61,116.14,282.04,178.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="11,252.96,654.48,109.48,8.97"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Results by concept.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="11,167.16,248.40,274.84,63.69"><head>Table 1 .</head><label>1</label><figDesc>Overall Results.</figDesc><table coords="11,167.16,267.00,274.84,45.09"><row><cell></cell><cell cols="2">By Photo</cell><cell cols="2">By Concept</cell></row><row><cell cols="5">Classifier MiAP GMiAP Micro-F1 Macro-F1 Micro-F1 Macro-F1</cell></row><row><cell cols="2">Naïve Bayes 0.1521 0.0894 0.3532</cell><cell>0.3877</cell><cell>0.3532</cell><cell>0.2152</cell></row><row><cell>Proposed</cell><cell>0.1724 0.1140 0.3389</cell><cell>0.3460</cell><cell>0.3389</cell><cell>0.2586</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.72,634.32,85.60,8.97"><p>http://imageclef.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.72,645.24,94.72,8.97"><p>http://www.flickr.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,144.72,656.16,76.24,8.97"><p>http://dbpedia.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,144.72,656.16,118.75,8.97"><p>http://wordnet.princeton.edu</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="5,144.72,656.16,179.68,8.97"><p>http://www.mpi-inf.mpg.de/yago-naga/yago</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgments</head><p>The authors would like to thank the financial support from <rs type="funder">FAPESP</rs>, process number <rs type="grantNumber">2011/17366-2</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_9MnbabD">
					<idno type="grant-number">2011/17366-2</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.88,394.08,337.79,8.97;12,151.56,405.00,328.99,8.97;12,151.56,415.91,328.98,8.97;12,151.56,426.96,198.76,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,328.56,394.08,152.11,8.97;12,151.56,405.00,328.99,8.97;12,151.56,415.91,34.92,8.97">REGIMvid at ImageCLEF2011: Integrating Contextual Information to Enhance Photo Annotation and Concept-based Retrieval</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Amel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Benammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,408.60,416.22,71.94,8.65;12,151.56,427.26,44.86,8.65">Working Notes of CLEF 2011</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.88,437.76,337.74,8.97;12,151.56,448.68,328.60,8.97;12,151.56,459.59,20.80,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,278.76,437.76,201.86,8.97;12,151.56,448.68,39.20,8.97">Automatic Video Classification: A Survey of the Literature</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Brezeale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,198.36,448.98,216.79,8.65">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="416" to="430" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.88,470.51,337.79,8.97;12,151.56,481.43,328.96,8.97;12,151.56,492.36,329.02,8.97;12,151.56,503.39,231.52,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,423.84,470.51,56.83,8.97;12,151.56,481.43,324.21,8.97">Enriching Ontological User Profiles with Tagging History for Multi-Domain Recommendations</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Cantador</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Szomszor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Alani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Castells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,163.44,492.66,317.14,8.65;12,151.56,503.70,137.23,8.65">1st International Workshop on Collective Semantics: Collective Intelligence &amp; the Semantic Web (CISWeb 2008)</title>
		<meeting><address><addrLine>Tenerife, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.88,514.19,337.64,8.97;12,151.56,525.11,329.08,8.97;12,151.56,536.03,328.97,8.97;12,151.56,547.07,328.95,8.97;12,151.56,557.99,176.08,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,365.64,514.19,114.88,8.97;12,151.56,525.11,176.96,8.97">Semantic Grounding of Tag Relatedness in Social Bookmarking Systems</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cattuto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Benz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hotho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Stumme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,432.12,536.34,48.41,8.65;12,151.56,547.38,87.46,8.65">The Semantic Web -ISWC 2008</title>
		<title level="s" coord="12,310.56,547.38,140.63,8.65">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Sheth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Staab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Dean</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Paolucci</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Maynard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Finin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Thirunarayan</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin/Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5318</biblScope>
			<biblScope unit="page" from="615" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.88,568.79,337.54,8.97;12,151.56,579.71,192.04,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,302.52,568.79,119.11,8.97">The google similarity distance</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Cilibrasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">M B</forename><surname>Vitanyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,429.48,569.10,50.94,8.65;12,151.56,580.02,102.51,8.65">IEEE Trans. on Knowl. and Data Eng</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="370" to="383" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.88,590.63,337.79,8.97;12,151.56,601.55,328.96,8.97;12,151.56,612.47,146.32,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,340.44,590.63,98.53,8.97">SZTAKI @ ImageCLEF</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Daróczy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Pethes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Benczúr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,354.24,601.86,122.02,8.65">Working Notes of CLEF 2011</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.88,623.27,337.67,8.97;12,151.56,634.31,328.96,8.97;12,151.56,645.23,329.08,8.97;12,151.56,656.15,93.76,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,324.96,623.27,155.59,8.97;12,151.56,634.31,264.32,8.97">Annotation and Retrieval System Using Confabulation Model for ImageCLEF2011 Photo Annotation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Izawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Motohashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Takagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,304.08,645.54,119.50,8.65">Working Notes of CLEF 2011</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.88,119.04,337.43,8.97;13,151.56,130.08,296.43,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,332.52,119.04,147.79,8.97;13,151.56,130.08,131.62,8.97">RCV1: A New Benchmark Collection for Text Categorization Research</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">G</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,291.48,130.38,83.60,8.65">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.88,140.99,337.49,8.97;13,151.56,151.91,328.96,8.97;13,151.56,162.96,202.24,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,300.84,140.99,179.53,8.97;13,151.56,151.91,113.45,8.97">A Text-Based Approach to the ImageCLEF 2010 Photo Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,151.56,163.26,119.14,8.65">Working Notes of CLEF 2010</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Pianta</surname></persName>
		</editor>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.54,171.59,338.03,11.25;13,151.56,184.80,329.07,8.97;13,151.56,195.84,329.08,8.97;13,151.56,206.76,20.80,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,408.60,173.88,71.98,8.97;13,151.56,184.80,171.65,8.97">LIRIS-Imagine at ImageCLEF 2011 Photo Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandr Ã A</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,221.64,196.14,124.54,8.65">Working Notes of CLEF 2011</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.54,217.68,337.97,8.97;13,151.56,228.71,228.52,8.97" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="13,329.64,217.98,147.06,8.65">Introduction to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schtze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.54,239.64,338.00,8.97;13,151.56,250.56,328.96,8.97;13,151.56,261.59,305.80,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,348.60,239.64,131.95,8.97;13,151.56,250.56,144.41,8.97">The Fraunhofer IDMT at Image-CLEF 2011 Photo Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Kuhhirt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Wolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,184.32,261.90,119.26,8.65">Working Notes of CLEF 2011</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.54,272.51,338.07,8.97;13,151.56,283.44,328.97,8.97;13,151.56,294.47,328.84,8.97;13,151.56,305.39,79.72,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,284.40,272.51,196.22,8.97;13,151.56,283.44,70.64,8.97">The ContextCam: Automated Point of Capture Video Annotation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">N</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">D</forename><surname>Abowd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,393.00,283.74,87.53,8.65;13,151.56,294.78,68.85,8.65">UbiComp 2004: Ubiquitous Computing</title>
		<title level="s" coord="13,290.88,294.78,139.43,8.65">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Khendek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Dssouli</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3205</biblScope>
			<biblScope unit="page" from="301" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.54,316.31,337.97,8.97;13,151.56,327.24,328.95,8.97;13,151.56,338.27,328.84,8.97;13,151.56,349.19,187.35,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,355.68,316.31,124.84,8.97;13,151.56,327.24,49.54,8.97">Classifier chains for multi-label classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,224.76,327.54,255.75,8.65;13,151.56,338.27,276.81,8.97">Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases: Part II, ECML PKDD &apos;09</title>
		<meeting>the European Conference on Machine Learning and Knowledge Discovery in Databases: Part II, ECML PKDD &apos;09<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="254" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.54,360.11,338.12,8.97;13,151.56,371.15,329.05,8.97;13,151.56,382.07,273.15,8.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,445.32,360.11,35.35,8.97;13,151.56,371.15,224.29,8.97">Content-Based Image Retrieval at the End of the Early Years</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,387.96,371.46,92.65,8.65;13,151.56,382.38,170.33,8.65">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1349" to="1380" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.54,392.99,338.09,8.97;13,151.56,404.03,329.08,8.97;13,151.56,414.95,329.05,8.97;13,151.56,425.87,170.80,8.97" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,446.04,392.99,34.59,8.97;13,151.56,404.03,329.08,8.97;13,151.56,414.95,20.93,8.97">MLKD&apos;s Participation at the CLEF 2011 Photo Annotation and Concept-Based Retrieval Tasks</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Spyromitros-Xioufis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sechidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,382.92,415.26,97.69,8.65;13,151.56,426.18,17.02,8.65">Working Notes of CLEF 2011</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.54,436.91,338.00,8.97;13,151.56,447.83,328.99,8.97;13,151.56,458.75,135.04,8.97" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,198.48,436.91,282.07,8.97;13,151.56,447.83,14.07,8.97">Using information content to evaluate semantic similarity in a taxonomy</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">R</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,202.08,448.14,278.47,8.65;13,151.56,459.06,44.45,8.65">Proceedings of the 14th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 14th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="448" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.54,469.79,338.12,8.97;13,151.56,480.71,315.64,8.97" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,270.00,469.79,210.67,8.97;13,151.56,480.71,111.29,8.97">Overview of the ImageCLEF 2012 Flickr Photo Annotation and Retrieval Task</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,282.72,481.02,103.91,8.65">CLEF 2012 working notes</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.54,491.63,337.97,8.97;13,151.56,502.67,328.72,8.97;13,151.56,513.59,294.04,8.97" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,245.04,491.63,145.64,8.97">Verbs semantics and lexical selection</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,409.20,491.94,71.31,8.65;13,151.56,502.98,263.69,8.65">Proceedings of the 32nd annual meeting on Association for Computational Linguistics</title>
		<meeting>the 32nd annual meeting on Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.54,524.51,337.85,8.97;13,151.56,535.55,328.87,8.97;13,151.56,546.47,329.08,8.97;13,151.56,557.39,45.28,8.97" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="13,351.48,524.51,128.91,8.97;13,151.56,535.55,149.95,8.97">On the sampling of web images for learning visual concept classifiers</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,324.48,535.86,155.95,8.65;13,151.56,546.47,220.29,8.97">Proceedings of the ACM International Conference on Image and Video Retrieval, CIVR &apos;10</title>
		<meeting>the ACM International Conference on Image and Video Retrieval, CIVR &apos;10<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.54,568.43,338.00,8.97;13,151.56,579.35,328.96,8.97;13,151.56,590.27,272.92,8.97" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="13,274.56,568.43,205.99,8.97;13,151.56,579.35,119.24,8.97">CEA LISTs participation to Visual Concept Detection Task of ImageCLEF 2011</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Znaidia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">Le</forename><surname>Borgne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,151.56,590.58,119.14,8.65">Working Notes of CLEF 2011</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
