<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,157.95,116.95,299.45,12.62;1,250.43,134.89,114.49,12.62">LIRIS-Imagine at ImageCLEF 2012 Photo Annotation task</title>
				<funder ref="#_DWVbFQp">
					<orgName type="full">French research agency ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,143.75,172.56,55.87,8.74"><forename type="first">Ningning</forename><surname>Liu</surname></persName>
							<email>ningning.liu@ec-lyon.fr</email>
						</author>
						<author>
							<persName coords="1,207.26,172.56,94.86,8.74"><forename type="first">Emmanuel</forename><surname>Dellandréa</surname></persName>
							<email>emmanuel.dellandrea@ec-lyon.fr</email>
						</author>
						<author>
							<persName coords="1,309.96,172.56,54.27,8.74"><forename type="first">Liming</forename><surname>Chen</surname></persName>
							<email>liming.chen@ec-lyon.fr</email>
						</author>
						<author>
							<persName coords="1,372.64,172.56,68.45,8.74"><forename type="first">Aliaksandr</forename><surname>Trus</surname></persName>
							<email>aliaksandr.trus@ec-lyon.fr</email>
						</author>
						<author>
							<persName coords="1,448.91,172.56,22.69,8.74;1,144.86,184.51,14.74,8.74"><forename type="first">Chao</forename><surname>Zhu</surname></persName>
							<email>chao.zhu@ec-lyon.fr</email>
						</author>
						<author>
							<persName coords="1,167.82,184.51,40.41,8.74"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
							<email>yu.zhang@ec-lyon.fr</email>
						</author>
						<author>
							<persName coords="1,216.54,184.51,102.73,8.74"><forename type="first">Charles-Edmond</forename><surname>Bichot</surname></persName>
							<email>charles-edmond.bichot@ec-lyon.fr</email>
						</author>
						<author>
							<persName coords="1,327.03,184.51,60.85,8.74"><forename type="first">Stéphane</forename><surname>Bres</surname></persName>
							<email>stephane.bres@insa-lyon.fr</email>
						</author>
						<author>
							<persName coords="1,414.98,184.51,55.51,8.74"><forename type="first">Bruno</forename><surname>Tellez</surname></persName>
							<email>bruno.tellez@univ-lyon1.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lyon</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Ecole Centrale de Lyon</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">LIRIS</orgName>
								<address>
									<postCode>UMR5205, F-69134</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,157.95,116.95,299.45,12.62;1,250.43,134.89,114.49,12.62">LIRIS-Imagine at ImageCLEF 2012 Photo Annotation task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">00DE50D15C3DE86E22B95C6E31B212DB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>textual features</term>
					<term>visual feature</term>
					<term>feature fusion</term>
					<term>concept detection</term>
					<term>photo annotation</term>
					<term>multimodality</term>
					<term>ImageCLEF</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present the methods we have proposed and evaluated through the ImageCLEF 2012 Photo Annotation task. More precisely, we have proposed the Histogram of Textual Concepts (HTC) textual feature to capture the relatedness of semantic concepts. In contrast to term frequency-based text representations mostly used for visual concept detection and annotation, HTC relies on the semantic similarity between the user tags and a concept dictionary. Moreover, a Selective Weighted Late Fusion (SWLF) is introduced to combine multiple sources of information which by iteratively selecting and weighting the best features for each concept at hand to be classified. The results have shown that the combination of our HTC feature with visual features through SWLF can improve the performance significantly. Our best model, which is a late fusion of textual and visual features, achieved a MiAP (Mean interpolated Average Precision) of 43.67% and ranked first out of the 80 submitted runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine-based recognition of visual concepts aims at recognizing automatically from images high-level semantic concepts (HLSC), including scenes (indoor, outdoor, landscape, etc.), objects (car, animal, person, etc.), events (travel, work, etc.), or even emotions (melancholic, happy, etc.). It proves to be extremely challenging because of large intra-class variations (clutter, occlusion, pose changes, etc.) and inter-class similarities <ref type="bibr" coords="1,271.00,609.29,7.75,8.74" target="#b0">[1]</ref><ref type="bibr" coords="1,278.75,609.29,3.87,8.74" target="#b1">[2]</ref><ref type="bibr" coords="1,278.75,609.29,3.87,8.74" target="#b2">[3]</ref><ref type="bibr" coords="1,282.62,609.29,7.75,8.74" target="#b3">[4]</ref>. The past decade has witnessed tremendous efforts from the research communities as testified the multiple challenges in the field, e.g., ImageCLEF <ref type="bibr" coords="1,240.24,633.20,7.75,8.74" target="#b4">[5]</ref><ref type="bibr" coords="1,247.99,633.20,3.87,8.74" target="#b5">[6]</ref><ref type="bibr" coords="1,247.99,633.20,3.87,8.74" target="#b6">[7]</ref><ref type="bibr" coords="1,251.86,633.20,7.75,8.74" target="#b7">[8]</ref>, TRECVID <ref type="bibr" coords="1,319.61,633.20,10.52,8.74" target="#b8">[9]</ref> and Pascal VOC <ref type="bibr" coords="1,413.77,633.20,14.61,8.74" target="#b9">[10]</ref>. Increasing works in the literature have discovered the wealth of semantic meanings conveyed by the abundant textual captions associated with images <ref type="bibr" coords="1,422.51,657.11,12.45,8.74" target="#b10">[11]</ref><ref type="bibr" coords="1,434.96,657.11,4.15,8.74" target="#b11">[12]</ref><ref type="bibr" coords="1,439.11,657.11,12.45,8.74" target="#b12">[13]</ref>. As a result, multimodal approaches have been increasingly proposed visual concept detection and annotation task (VCDT) by making joint use of user textual tags and visual descriptions to bridge the gap between low-level visual features and HLSC <ref type="bibr" coords="2,164.51,155.86,9.96,8.74" target="#b6">[7]</ref>.</p><p>The VCDT is a multi-label classification challenge. It aims at the automatic annotation of a large number of consumer photos with multiple annotations. There were remarkable works have been proposed for ImageCLEF photo annotation tasks. The LEAR and XRCE group <ref type="bibr" coords="2,327.48,203.84,15.50,8.74" target="#b13">[14]</ref> in ImageCLEF 2010 employed the Fisher vector image representation with the TagProp method for image autoannotation. The TUBFI group <ref type="bibr" coords="2,274.52,227.75,15.50,8.74" target="#b14">[15]</ref> in ImageCLEF 2011 built textual features using a soft mapping of textual Bag-of-Words (BoW) and Markov random walks based on frequent Flickr user tags. Our group in ImageCLEF 2011 <ref type="bibr" coords="2,435.87,251.66,15.50,8.74" target="#b15">[16]</ref> firstly proposed a novel textual representation, named Histogram of Textual Concept (HTC), which captures the relatedness of semantic concepts. Meanwhile we also proposed a novel selective weighted late fusion (SWLF) method, which automatically selects and weights the best discriminative features for each visual concept to be predicted in optimizing the overall mean average precision. This year, we have improved our approaches in the following aspects:</p><p>-We evaluated different textual preprocessing methods, and proposed enhanced HTC features using term frequency information. Meanwhile, we implemented two types of distributional term representations: documents occurrence representation (DOR) and DOR TFIDF <ref type="bibr" coords="2,370.89,382.00,14.61,8.74" target="#b16">[17]</ref>. -We investigated a set of mid-level features, which are related to harmony, dynamism, aesthetic quality, emotional color representation, etc.. Meanwhile, we improved the harmony and dynamism features by adding a local information.</p><p>The rest of this paper is organized as follows. The features are introduced in Section 2, including textual and visual features as well as the fusion scheme proposed to combine them. The results are analysed in Section 3. Finally, Section 4 draws the conclusion and gives some hints for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Features for semantic concepts recognition</head><p>In this section, we firstly present the textual features including HTC and enhanced HTC in Section 2.1, following with (Section 2.2) description of visual features which can be categorized into four groups: color, texture, shape and mid-level. The feature fusion scheme, SWLF, is presented in Section 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Textual features</head><p>The Histogram of Textual Concepts, HTC, of a text document is defined as a histogram based on a vocabulary or dictionary where each bin of this histogram represents a concept of the dictionary, whereas its value is the accumulation of the contribution of each word within the text document toward the underlying concept according to a predefined semantic similarity measure.</p><p>The advantages of HTC are multiple. First, for a sparse text document as image tags, HTC offers a smooth description of the semantic relatedness of user tags over a set of textual concepts defined within the dictionary. More importantly, in the case of polysemy, HTC helps disambiguate textual concepts according to the context. For instance, the concept of "bank" can refer to a financial intermediary but also to the shoreline of a river. However, when a tag "bank" comes with a photo showing a financial institution, correlated tags such as "finance", "building", "money", etc., are very likely to be used, thereby clearly distinguishing the concept "bank" in finance from that of a river where correlated tags can be "water", "boat", "river", etc. Similarly, in the case of synonyms, the HTC will reinforce the concept related to the synonym as far as the semantic similarity measurement takes into account the phenomenon of synonyms. The algorithm for the extraction of a HTC feature is detailed in the following algorithm:</p><p>The Histogram of Textual Concepts (HTC) Algorithm:</p><formula xml:id="formula_0" coords="3,136.16,349.83,315.52,19.10">Input: Tag data W = {wt} with t ∈ [1, T], dictionary D = {di} with i ∈ [1, d]. Output: Histogram f composed of values fi with 0 ≤ fi ≤ 1, i ∈ [1, d].</formula><p>-Preprocess the tags by using a stop-words filter.</p><p>-If the input image has no tags (W = ∅), return f with ∀i fi = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>-Do for each word wt ∈ W :</p><p>1. Calculate dist(wt, di), where dist is a semantic similarity distance between wt and di. 2. Obtain the semantic matrix S as: S(t, i) = dist(wt, di).</p><p>-Calculate the feature f as: fi = T t=1 S(t, i), and normalize it to [0 1] as: fi = fi/ d j=1 fj.</p><p>1</p><p>When an input image has no tag at all, in this work we simply assume that every bin value is 0.5, therefore at halfway between a semantic similarity measurement 0 (no relationship at all with the corresponding concept in the dictionary) and 1 (full similarity with the corresponding concept in the dictionary). Alternatively, we can also set these values to the mean of HTCs over the captioned images of a training set.</p><p>The computation of HTC requires the definition of a dictionary and a proper semantic relatedness measurement over textual concepts. For the ImageCLEF 2012 photo annotation task, we used two types of dictionaries. The first one is dictionary based on the term frequency on the training set, e.g. dictionary TF 10T consists of top 10 thousand words sorted by their frequencies in the training set. While the second one, D Anew, is the set of 1034 English words used in the ANEW study <ref type="bibr" coords="3,249.00,645.16,14.61,8.74" target="#b17">[18]</ref>. The interest of the ANEW dictionary lies in the fact that each of its word is rated on a scale from 1 to 9 using affective norms in terms of valence (affective dimension expressing positive versus negative), arousal (affective dimension expressing active versus inactive) and dominance (affective dimension expressing dominated versus in control). For instance, according to ANEW, the concept "beauty" has a mean valence of 7.82, a mean arousal of 4.95 and a mean dominance of 5.23 while the concept "bird" would have a mean valence of 7.27, a mean arousal of 3.17 and a mean dominance of 4.42. Using the affective ratings of the ANEW concepts and the HTCs computed over image tags, one can further define the coordinates of an image caption in the three dimensional affective space <ref type="bibr" coords="4,296.70,505.73,14.61,8.74" target="#b18">[19]</ref>, in terms of valence, arousal and dominance by taking a linear combination of the ANEW concepts weighted by the corresponding HTC values. More precisely, given a HTC descriptor f extracted from a text document, the valence, arousal and dominance coordinates of the text document can be computed as follows:</p><formula xml:id="formula_1" coords="4,248.03,579.35,232.56,19.91">f valence = (1/d) i (f i * V i )<label>(1)</label></formula><formula xml:id="formula_2" coords="4,247.02,614.35,233.57,19.91">f arousal = (1/d) i (f i * A i )<label>(2)</label></formula><formula xml:id="formula_3" coords="4,240.65,649.34,239.94,19.91">f dominance = (1/d) i (f i * D i )<label>(3)</label></formula><p>where V i , A i and D i are respectively the valence, the arousal and the dominance of the i th word w i in the D Anew dictionary, and d is the size of D Anew.</p><p>The HTC features fail to calculate the semantic distance of two terms when the semantic relatedness measurement are not defined between these two terms. In order to cope with this problem, we enhanced the HTC features by combining it with TF/IDF features in a simple way: sum the value on each bin, and then normalize for the same dictionary. Meanwhile, we employed the distributional term representation DOR and DOR-TF/IDF <ref type="bibr" coords="5,330.01,203.68,14.61,8.74" target="#b16">[17]</ref>. A summary of textual features is given in Table <ref type="table" coords="5,209.82,215.63,3.87,8.74" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual features</head><p>For ImageCLEF 2011 photo annotation task, we have introduced various visual features to describe interesting details and to catch the global image atmosphere. Thus, 5 groups of features have been considered: color, texture, shape, local descriptor and mid-level features <ref type="bibr" coords="5,303.76,301.02,14.61,8.74" target="#b15">[16]</ref>. This year, we have enriched this set of visual features by adding color SIFT features with 4000 codewords and soft assignment <ref type="bibr" coords="5,186.83,324.93,15.50,8.74" target="#b19">[20]</ref> and TOPSURF feature <ref type="bibr" coords="5,313.46,324.93,14.61,8.74" target="#b20">[21]</ref>. Moreover, we have enhanced the mid-level features harmony and dynamism by adding a local information through their computation using a pyramid grid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Feature fusion through SWLF</head><p>In order to combined efficiently textual and visual features, we have proposed a Selective Weighted Late Fusion (SWLF) scheme which learns to automatically select and weight the best features for each visual concept to be recognized.</p><p>SWLF scheme has a learning phase which requires a training dataset for the selection of the best experts and their corresponding weights for each visual concept. Specifically, given a training dataset, we divide it into two disjoint parts composed of a training set and a validation set. For each visual concept, a binary classifier (concept versus no concept) is trained, which is also called expert in the subsequent, for each type of features using the data in the training set. Thus, for each concept, we generate as many experts as the number of different types of features. The quality of each expert can then be evaluated through a quality metric using the data in the validation set. In this work, the quality metric is chosen to be the interpolated Average Precision (iAP). The higher iAP is for a given expert, the more weight should be given to the score delivered by that expert for the late fusion. This fusion is performed as the sum of the weighted scores. More details on SWLF can be found in <ref type="bibr" coords="5,340.89,577.70,14.61,8.74" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Our methods have been evaluated through the ImageCLEF 2012 photo annotation task, and particularly through the visual concept detection, annotation and retrieval subtask whose details are provided in <ref type="bibr" coords="5,362.03,657.11,14.61,8.74" target="#b22">[23]</ref>. There are 94 concepts to automatically detect, that can be categorized into 5 groups: natural elements (day, night, sunrise, etc. ), environment (desert, coast, landscape, etc. ), people (baby, child, teenager, etc. ), image elements (in focus, city life, active, etc.), human elements (rail vehicle, water vehicle, air vehicle, etc. ).</p><p>In order to obtain a stable and better performance, we divided the training set into a training part (50%, 7501 images) and a validation part (50%, 7499 images) as required by SWLF presented in section 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The submitted runs</head><p>We submitted 5 runs to the ImageCLEF 2012 photo annotation challenge (2 textual model, 1 visual model and 2 multimodal models). All runs were based on the features described in the previous sections, including 11 textual ones and 32 visual ones. For the example evaluation, we propose two methods to chose the threshold. One is based on the distribution of training data. More specifically, we firstly calculate the distribution of concepts on the training set, then for each concept, we set the threshold as the boundary which makes the proportion of positive sample as same as it is in the training data. This idea is that we consider the training and test set share the same distribution for each concept. The other is to select a best threshold, which receives the best FMeasure value on the validation set. Based on the previous experiments and observations, we performed our runs based on the following configuration: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>The results obtained by our 5 runs are given in Table <ref type="table" coords="6,376.65,621.25,3.87,8.74" target="#tab_1">2</ref>. The best performance was provided by our multimodal models which outperformed the purely textual and purely visual ones. Moreover, our best model obtained the first rank based on the MiAP among the 80 runs submitted to the challenge. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion</head><p>For the textual features, we proposed to apply two preprocessing methods. One is the removing of stopping words. The other one is stemming on 4 language (English, Germany, French, Italian). Based on the ImageCLEF 2012 photo annotation dataset, we find that after these two preprocessing, the MiAP performance of term frequency features e.g. TF/IDF, DOR improves about 1%. But the stemming is not proper for HTC features as it fails to calculate the semantic similarity measurement after stemming.</p><p>For the visual features, the harmony and dynamism features computed locally using a pyramid grid achieved 3% improvement on MiAP compared to the original ones.</p><p>For the HTC, we tested several semantic distances methods of WordNet including path, wup and lin. It is found that the path distance obtained the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have presented in this paper the models that we have evaluated through the ImageCLEF 2012 photo annotation challenge. Our best multimodal prediction model which relies on the fusion through SWLF of our textual features (HTC) and visual features including low-level and mid-level information achieved a MiAP of 43.6% and ranked the best performance out of the 80 submitted runs. From the experimental results, we can conclude the following: (i) the proposed multimodal approach greatly improve the performance of purely textual and purely visual ones, with about 9% higher than the best visual-only model; (ii) the fused experts through weighted score-based SWLF, display a very good generalization skill on unseen test data and prove particularly useful for the image annotation task with multi-label scenarios in efficiently fusing visual and textual features.</p><p>In our future work, we envisage further investigation of the interplay between textual and visual content, in studying in particular the visual relatedness in regard to textual concepts. We also want to study some mid-level visual features or representations, for instance using an attentional model, which better account for affect related concepts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,138.97,418.91,341.62,8.77;6,151.70,430.90,322.71,8.74;6,138.97,444.32,341.62,8.77;6,151.70,456.31,322.71,8.74;6,138.97,469.73,341.63,8.77;6,151.70,481.72,302.22,8.74;6,138.97,495.14,341.62,8.77;6,151.70,507.13,328.89,8.74;6,151.70,519.08,65.37,8.74;6,138.97,532.51,341.62,8.77;6,151.70,544.49,328.89,8.74;6,151.70,556.45,65.37,8.74"><head>1. textual model 1 :</head><label>1</label><figDesc>the combination of the top 4 features among the 11 textual features for each concept based on the weighted score SWFL scheme. 2. textual model 2: the combination of the top 6 features among the 11 textual features for each concept based on the weighted score SWFL scheme. 3. visual model 3: the combination of the top 5 features among the 24 visual features for each concept based on the weighted score SWFL scheme. 4. multimodal model 4: the combination of the top 22 features among the 43 visual and textual features for each concept based on the weighted score SWFL scheme. 5. multimodal model 5: the combination of the top 26 features among the 43 visual and textual features for each concept based on the weighted score SWFL scheme.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,136.16,116.91,344.81,249.88"><head>Table 1 .</head><label>1</label><figDesc>The summary of textual features.</figDesc><table coords="4,136.16,150.11,344.81,216.68"><row><cell>Short name</cell><cell>Description</cell><cell></cell></row><row><cell>1</cell><cell>txtFtr DOR</cell><cell>implement features of documents occurrence repre-sentation [17].</cell></row><row><cell>2</cell><cell>txtFtr DOR TFIDF</cell><cell></cell></row><row><cell>3</cell><cell>txtFtr HTC Danew</cell><cell>obtained by using WordNet path distance on ANEW dictionary.</cell></row><row><cell>4</cell><cell cols="2">txtFtr TFIDF Danew obtained on ANEW dictionary.</cell></row><row><cell>5</cell><cell>txtFtr eHTC Danew</cell><cell>obtained by adding each bins of txtFtr 4 and txtFtr 5.</cell></row><row><cell>6</cell><cell>txtFtr TFIDF TF 10T</cell><cell>obtained on the dictionary TF 10T, which is the top 10 thousand words sorted by the term frequency.</cell></row><row><cell>7</cell><cell>txtFtr HTC VAD</cell><cell>obtained using Eq. 1, Eq. 2 and Eq. 3.</cell></row><row><cell>8</cell><cell>txtFtr HTC TF 10T</cell><cell>obtained by using WordNet path distance on TF 10T dictionary.</cell></row><row><cell>9</cell><cell>txtFtr HTC TF 20T</cell><cell>obtained by using WordNet path distance on TF 20T dictionary.</cell></row><row><cell>10</cell><cell cols="2">txtFtr TFIDF TF 20T obtained on TF 20T dictionary.</cell></row><row><cell>11</cell><cell>txtFtr eHTC TF 20T</cell><cell>obtained by adding each bins of txtFtr 9 and txtFtr 10.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,202.29,102.74,210.78,104.24"><head>Table 2 .</head><label>2</label><figDesc>The results of our submitted runs.</figDesc><table coords="7,202.29,119.31,210.78,87.67"><row><cell>Submitted runs</cell><cell cols="3">mAP(%) GMiAP(%) F-ex(%)</cell></row><row><cell>text model 1</cell><cell>33.28</cell><cell>27.71</cell><cell>39.17</cell></row><row><cell>text model 2</cell><cell>33.38</cell><cell>27.59</cell><cell>46.91</cell></row><row><cell>visual model 3</cell><cell>34.81</cell><cell>28.58</cell><cell>54.37</cell></row><row><cell cols="2">multimodal model 4 43.66</cell><cell>38.75</cell><cell>57.63</cell></row><row><cell cols="2">multimodal model 5 43.67</cell><cell>38.77</cell><cell>57.66</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work was supported in part by the <rs type="funder">French research agency ANR</rs> through the <rs type="projectName">VideoSense</rs> project under the grant <rs type="grantNumber">2009 CORD 026 02</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_DWVbFQp">
					<idno type="grant-number">2009 CORD 026 02</idno>
					<orgName type="project" subtype="full">VideoSense</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,142.96,215.38,337.63,7.86;8,151.52,226.34,329.07,7.86;8,151.52,237.30,111.61,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,422.68,215.38,57.90,7.86;8,151.52,226.34,181.67,7.86">Content-based image retrieval at the end of the early years</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,340.85,226.34,139.74,7.86;8,151.52,237.30,23.55,7.86">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1349" to="1380" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,248.53,337.63,7.86;8,151.52,259.49,329.07,7.86;8,151.52,270.45,99.63,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,310.55,248.53,170.04,7.86;8,151.52,259.49,256.18,7.86">Semantic-friendly indexing and quering of images based on the extraction of the objective semantic cues</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mojsilović</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Rogowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,415.62,259.49,64.98,7.86;8,151.52,270.45,25.39,7.86">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="79" to="107" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,281.68,337.63,7.86;8,151.52,292.64,281.44,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,221.32,281.68,259.27,7.86;8,151.52,292.64,34.83,7.86">Automatic linguistic indexing of pictures by a statistical modeling approach</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,193.78,292.64,163.41,7.86">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1075" to="1088" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,303.87,337.63,7.86;8,151.52,314.83,269.30,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,323.61,303.87,156.98,7.86;8,151.52,314.83,160.84,7.86">Content-based multimedia information retrieval: State of the art and challenges</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Djeraba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,319.32,314.83,48.76,7.86">TOMCCAP</title>
		<imprint>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,326.05,337.64,7.86;8,151.52,337.01,191.39,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,306.15,326.05,134.88,7.86">The mir flickr retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,461.39,326.05,19.20,7.86;8,151.52,337.01,118.91,7.86">Multimedia Information Retrieval</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="39" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,348.24,337.63,7.86;8,151.52,359.20,329.07,7.86;8,151.52,370.16,329.07,7.86;8,151.52,381.12,74.75,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,316.92,348.24,163.66,7.86;8,151.52,359.20,219.96,7.86">New trends and ideas in visual concept detection: The mir flickr retrieval evaluation initiative</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,392.42,359.20,88.17,7.86;8,151.52,370.16,325.19,7.86">MIR &apos;10: Proceedings of the 2010 ACM International Conference on Multimedia Information Retrieval</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="527" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,392.35,337.64,7.86;8,151.52,403.31,264.48,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,295.07,392.35,185.52,7.86;8,151.52,403.31,80.87,7.86">The clef 2011 photo annotation and conceptbased retrieval tasks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liebetrau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="8,252.58,403.31,134.97,7.86">CLEF Workshop Notebook Paper</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,414.54,337.63,7.86;8,151.52,425.50,329.07,7.86;8,151.52,436.45,20.99,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,259.51,414.54,221.08,7.86;8,151.52,425.50,144.22,7.86">New strategies for image annotation: Overview of the photo annotation task at imageclef</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Huiskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="8,338.72,425.50,137.50,7.86">CLEF Workshop Notebook Paper</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.96,447.68,337.63,7.86;8,151.52,458.64,329.07,7.86;8,151.52,469.60,116.62,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,292.99,447.68,133.81,7.86">Evaluation campaigns and trecvid</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,445.42,447.68,35.17,7.86;8,151.52,458.64,329.07,7.86;8,151.52,469.60,34.93,7.86">MIR &apos;06: Proceedings of the 8th ACM International Workshop on Multimedia Information Retrieval</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="321" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,480.83,337.97,7.86;8,151.52,491.79,329.07,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,464.72,480.83,15.88,7.86;8,151.52,491.79,167.49,7.86">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,325.41,491.79,89.79,7.86">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,503.02,337.98,7.86;8,151.52,513.98,167.41,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,297.64,503.02,182.95,7.86;8,151.52,513.98,27.85,7.86">Building text features for object image classification</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,202.72,513.98,23.34,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1367" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,525.21,337.98,7.86;8,151.52,536.17,167.72,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,244.28,525.21,236.31,7.86;8,151.52,536.17,34.28,7.86">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,206.10,536.17,20.89,7.86">ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1470" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,547.39,337.98,7.86;8,151.52,558.35,205.91,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,319.17,547.39,161.42,7.86;8,151.52,558.35,75.54,7.86">Multimodal semi-supervised learning for image classification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,250.43,558.35,23.34,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="902" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,569.58,337.98,7.86;8,151.52,580.54,329.07,7.86;8,151.52,591.50,92.31,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,417.17,569.58,63.42,7.86;8,151.52,580.54,223.74,7.86">Lear and xrce&apos;s participation to visual concept detection task -imageclef</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Snchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="8,413.35,580.54,67.24,7.86;8,151.52,591.50,63.87,7.86">CLEF Workshop Notebook Paper</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,602.73,337.98,7.86;8,151.52,613.69,329.07,7.86;8,151.52,624.65,245.63,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,443.52,602.73,37.07,7.86;8,151.52,613.69,329.07,7.86;8,151.52,624.65,61.98,7.86">The joint submission of the tu berlin and fraunhofer first (tubfi) to the imageclef2011 photo annotation task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="8,233.75,624.65,134.97,7.86">CLEF Workshop Notebook Paper</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.62,635.88,337.98,7.86;8,151.52,646.84,329.07,7.86;8,151.52,657.79,92.31,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="8,151.52,646.84,240.16,7.86">LIRIS-Imagine at ImageCLEF 2011 Photo Annotation task</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandréa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-E</forename><surname>Bichot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Tellez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="8,412.34,646.84,68.25,7.86;8,151.52,657.79,63.87,7.86">CLEF Workshop Notebook Paper</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,120.67,337.98,7.86;9,151.52,131.63,268.32,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="9,315.25,120.67,165.34,7.86;9,151.52,131.63,109.71,7.86">Multimodal indexing based on semantic cohesion for image retrieval</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sucar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,267.86,131.63,86.95,7.86">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,142.59,337.97,7.86;9,151.52,153.55,329.07,7.86;9,151.52,164.51,260.11,7.86" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="9,266.23,142.59,214.36,7.86;9,151.52,153.55,163.27,7.86">Affective norms for english words (ANEW): Stimuli, instruction manual, and affective ratings</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Gainesville, Florida</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Tech. rep. ; Center for Research in Psychophysiology, University of Florida</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,175.46,337.98,7.86;9,151.52,186.42,224.85,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="9,198.82,175.46,241.86,7.86">Appraisal Processes in Emotion: Theory, Methods, Research</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="9,447.26,175.46,33.34,7.86;9,151.52,186.42,65.88,7.86">Series in Affective Science</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,197.38,337.98,7.86;9,151.52,208.34,329.07,7.86;9,151.52,219.30,99.83,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="9,455.20,197.38,25.39,7.86;9,151.52,208.34,60.37,7.86">Visual word ambiguity</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Geusebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,218.95,208.34,261.64,7.86">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1271" to="1283" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,230.26,337.98,7.86;9,151.52,241.22,310.88,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="9,313.59,230.26,128.13,7.86">Top-surf: a visual words toolkit</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,463.04,230.26,17.56,7.86;9,151.52,241.22,219.40,7.86">Proceedings of the international conference on Multimedia</title>
		<meeting>the international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1473" to="1476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,252.18,337.97,7.86;9,151.52,263.14,329.07,7.86;9,151.52,274.09,234.51,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="9,378.81,252.18,101.78,7.86;9,151.52,263.14,148.24,7.86">A selective weighted late fusion for visual concept recognition</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-E</forename><surname>Bichot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,321.97,263.14,158.62,7.86;9,151.52,274.09,206.25,7.86">ECCV 2012 Workshop on Information fusion in Computer Vision for Concept Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.62,285.05,337.98,7.86;9,151.52,296.01,274.08,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="9,255.03,285.05,225.57,7.86;9,151.52,296.01,69.46,7.86">Overview of the imageclef 2012 flickr photo annotation and retrieval task</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,241.21,296.01,103.78,7.86">CLEF 2012 working notes</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
