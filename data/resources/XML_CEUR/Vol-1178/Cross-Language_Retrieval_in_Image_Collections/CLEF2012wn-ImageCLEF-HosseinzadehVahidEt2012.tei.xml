<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,195.17,151.67,205.09,12.54;1,193.85,169.07,207.77,12.54;1,195.89,186.47,203.69,12.54">DEMIR at ImageCLEFMed 2012: Inter-modality and Intra-Modality Integrated Combination Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,132.86,224.66,101.83,11.04"><forename type="first">Ali</forename><forename type="middle">Hosseinzadeh</forename><surname>Vahid</surname></persName>
							<email>ali.vahid@st.cs.deu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Dokuz</orgName>
								<orgName type="institution">Eylul University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Engineering DEMIR Dokuz Eylul Multimedia Information Retrieval Research Group Tinaztepe</orgName>
								<address>
									<postCode>35160</postCode>
									<settlement>Izmir</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,241.97,224.66,57.93,11.04"><forename type="first">Adil</forename><surname>Alpkocak</surname></persName>
							<email>alpkocak@cs.deu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Dokuz</orgName>
								<orgName type="institution">Eylul University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Engineering DEMIR Dokuz Eylul Multimedia Information Retrieval Research Group Tinaztepe</orgName>
								<address>
									<postCode>35160</postCode>
									<settlement>Izmir</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,307.25,224.66,118.83,11.04"><forename type="first">Roghaiyeh</forename><surname>Gachpaz Hamed</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dokuz</orgName>
								<orgName type="institution">Eylul University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Engineering DEMIR Dokuz Eylul Multimedia Information Retrieval Research Group Tinaztepe</orgName>
								<address>
									<postCode>35160</postCode>
									<settlement>Izmir</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,434.47,224.66,28.17,11.04;1,204.05,240.17,65.87,11.04"><forename type="first">Nefise</forename><forename type="middle">Meltem</forename><surname>Ceylan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dokuz</orgName>
								<orgName type="institution">Eylul University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,296.45,240.17,95.08,11.04"><forename type="first">Okan</forename><surname>Ozturkmenoglu</surname></persName>
							<email>okan.ozturkmenoglu@deu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Dokuz</orgName>
								<orgName type="institution">Eylul University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="2,408.29,121.07,53.87,9.00"><forename type="first">Meltem</forename><surname>Nefise</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Engineering DEMIR Dokuz Eylul Multimedia Information Retrieval Research Group Tinaztepe</orgName>
								<address>
									<postCode>35160</postCode>
									<settlement>Izmir</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="2,124.82,132.23,22.81,9.00;2,169.21,132.23,19.13,9.00"><forename type="first">Okan</forename><surname>Ceylan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Engineering DEMIR Dokuz Eylul Multimedia Information Retrieval Research Group Tinaztepe</orgName>
								<address>
									<postCode>35160</postCode>
									<settlement>Izmir</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="2,190.31,132.23,56.65,9.00"><surname>Ozturkmenoglu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Engineering DEMIR Dokuz Eylul Multimedia Information Retrieval Research Group Tinaztepe</orgName>
								<address>
									<postCode>35160</postCode>
									<settlement>Izmir</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,195.17,151.67,205.09,12.54;1,193.85,169.07,207.77,12.54;1,195.89,186.47,203.69,12.54">DEMIR at ImageCLEFMed 2012: Inter-modality and Intra-Modality Integrated Combination Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FDA830BE44589DB2903DFEF50A109222</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Integrated Combination</term>
					<term>fusion methods</term>
					<term>Relevance Feedback</term>
					<term>Multimodal Information Retrieval</term>
					<term>Content-based Image Retrieval</term>
					<term>Medical Image Retrieval</term>
					<term>Information Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dokuz Eylul University Multimedia Information Retrieval) research team to the ImageCLEF 2012 Medical Retrieval task. This year, we evaluated impact of our proposed Integrated Combination method and Explicit Graded relevance feedback on the most descriptive low level features of images and best text retrieval result. We improved results by examination of different level of integrated approach for retrieved text data and low level features. We tested multi-modality image retrieval in ImageCLEF 2012 medical retrieval task and obtained the best rank in visual retrieval due to our experiments. The results clearly show that proper combination of different modalities improve the overall retrieval performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we present the experiments performed by Dokuz Eylul University Multimedia Information Retrieval (DEMIR) Group, Turkey, in the context of our participation to the ImageCLEF 2012 Medical Image retrieval task <ref type="bibr" coords="1,147.47,603.70,11.37,11.04" target="#b0">[1]</ref>. The main focus of this work is evaluation of result improvement using our proposed Integrated Combination Multimodal Retrieval (ICMR) system. To acquire the optimum result, not only we applied inter-modality integrated combination of text and low-level image features; also we utilized this method to combine result of different low level features of images. On the other hand, we performed the experiments for narrowing down the data collection by defining and filtering out of irrelevant documents. Also we checked the impact of relevance feedback on performance of medical image retrieval system. After analysis of integrated combination method (Section 2), we describe our techniques for filtering the data collection out and for image query expansion using relevance feedback (Section 3 , 4). After we present experiments on ImageCLEF 2012 Medical track data (Section 5), then Section 6 concludes the paper by pointing out the open issues and possible avenues of further research in the area of multimodal fusion techniques for contentbased image retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">INTEGRATED COMBINATION multimodal RETRIEVAL</head><p>It seems that the best performance of multi-modality fusion appears when we can put all relevant retrieved documents of each fused modalities in response to a query into relevant retrieved document set of fused modalities. More formally, Union of modalities' relevant retrieved document sets can achieve the best result in fusion. However, there are some documents that are relevant but not appear in any individual modalities' result set. This is the limit of multimodality fusion. Because of that, in literature of multimodal data fusion, some of the authors claimed that data fusion algorithms are competitive in performance and is not devoid of risks and sometimes can degrade the retrieval performance.</p><p>Our previous studies illustrated that main reason for this downside originated from threshold on number of fused document <ref type="bibr" coords="2,387.55,503.47,11.27,11.04" target="#b0">(1)</ref>. In most fusion methods, combination applied partially, often on top 1000 relevant retrieved documents of each modality. While in our proposed integrated approach, all documents of data collections in each modality participate in combination, based on their similarity score or rank position.</p><p>In order to evaluate the impact of our proposed method on improvement of overall system performance, we designed and implemented an experimental integrated combination multimodal retrieval system. In this system, each modality was preprocessed and indexed separately. Then the results obtained from different modalities in response to each query combined based on selected combination methods and produced the final result set of retrieved documents. This combination can be inter-modality (between results of different modalities) or intra-modality (between different features of a modality).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Textual Features</head><p>In order to simplify the work, we split the XML file for textual metadata and represented each image in the collection as a structured document of xml file. We also expanded the XML file using related article full text, abstract and title as new tags. We used Terrier IR Platform API, open source search engine written in Java and is developed at the School of Computing Science, University of Glasgow (2), for our Text Based Information Retrieval subsystem. Terrier provides both efficient and effective search methods for large-scale document collections. To introduce flexibility to the processing and transformation of textual information, it requires a preprocessing in different ways. The order in which transformations were applied is as follows: 1) special characters deletion: characters with no meaning, like punctuation marks or blanks, are all eliminated; 2) stop words removal: discarding of semantically empty words, very high frequency words, 3) token normalization: converting all words to lower case 4) stemming: we used the Porter stemmer (3) as a process for removing the commoner morphological endings from words in English. Since the choice of the weighting model may crucially affect the performance of any information retrieval system, first of all we decided to work on evaluating the relative merits and drawbacks of different weighting models using Terrier IR Platform <ref type="bibr" coords="3,236.69,497.23,11.25,11.04" target="#b2">(2)</ref>, open source search engine written in Java and is developed at the School of Computing Science, University of Glasgow.</p><p>Since choice of the weighting model may crucially affect the performance of any information retrieval system, we performed our experiments on textual features using TF-IDF, the most common and famous weighting model in IR systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual Features</head><p>The visual content of the images is mapped into a new space named the feature space. A feature is a set of characteristics of the image. In addition, a feature can further be enriched with information about the spatial distribution of the characteristic that it describes. Selection of right features, typically represented as vectors in this space, is the major aspect to attain discriminative and sufficient retrieval systems. We extracted features for all images in test collection and query examples using Rummager tool (4), which is developed in the Automatic Control Systems &amp; Robotics Laboratory at the Democritus University of Thrace-Greece. Although considerable design and experimental work, and rigorous testing, have been performed in MPEG-7 to arrive at efficient image descriptors for similarity matching, but no single generic color descriptor exists that can be used for all foreseen applications. As a result, a range of descriptors has been standardized, each suitable for achieving specific similarity-matching functionalities. On the other hand, there is some new and well known set of composite descriptors that combine two or more feature types. Here we introduce some of such new and well known set of composite descriptors then present the experimental results of comparison on performance of extracted features to select the most effective features as a visual modality:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Color and edge directivity descriptor (CEDD)</head><p>The CEDD includes texture information produced by the six-bin histogram of the fuzzy system that uses the five digital filters proposed by the MPEG-7 EHD. Additionally, for color information the CEDD uses the 24-bin color histogram produced by the 24-bin fuzzy-linking system. Overall, the final histogram has 144 regions. Each Image Block interacts successively with all the fuzzy systems. In the Texture Unit, the Image Block is separated into four regions called Sub Blocks. The value of each Sub Block is the mean value of the luminosity of the pixels it contains. The luminosity values are derived from a YIQ color space transformation. Each Image Block interacts with the five digital filters proposed by MPEG-7 EHD, and with the use of the pentagonal diagram it is classified in one or more texture categories. Then, in the Color Unit, every Image Block is converted to the HSV color space. The mean values of H, S and V are calculated and become inputs to the fuzzy system that produces the fuzzy ten-bin histogram. Then, the second fuzzy system uses the mean values of S and V as well as the position number of the bin (or bins) resulting from the previous fuzzy ten-bin unit, calculates the hue of the color and produces the fuzzy 24-bin histogram. The combination of the three fuzzy systems will finally classify the Image Block. The process is repeated for all the image blocks. At the completion of the process, the histogram is normalized and quantized (5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Fuzzy color and texture histogram (FCTH)</head><p>The FCTH descriptor includes the texture information produced in the eightbin histogram of the fuzzy system that uses the high frequency bands of the Haar wavelet transform. For color information, the descriptor uses the 24bin color histogram produced by the 24-bin fuzzy-linking system. Overall, the final histogram includes192 regions. Each Image Block interacts successively with all the fuzzy systems in the exact manner demonstrated in CEDD production. Each Image Block is transformed into the YIQ color space and transformed with the Haar Wavelet transform. The fLH, fHL and fHH values are calculated and with the use of the fuzzy system that classifies the f coefficients, this Image Block is classified in one of the eight output bins. Next, the same Image Block is transformed into the HSV color space and the mean H, S and V block values are calculated. These values become inputs to the fuzzy system that forms the ten-bin fuzzy color histogram. Then, the next fuzzy system uses the mean values of S and V as well as the position number of the bin (or bins) resulting from the previous fuzzy ten-bin unit, to calculate the hue of the color and create the fuzzy 24-bin histogram. The combined three fuzzy systems therefore classify the Image Block. The process is repeated for all the blocks of the image. At the completion of the process, the histogram is normalized and quantized (6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Brightness and Texture Directionality Histogram (BTDH)</head><p>This feature is very similar to FCTH feature. The main difference from FCTH feature is using brightness instead of color histogram. This descriptor uses brightness and texture characteristics as well as the spatial distribution of these characteristics in one compact 1D vector. The most important characteristic of the proposed descriptor is that its size adapts according to the storage capabilities of the application that is using it. This characteristic renders the descriptor appropriate for use in large medical (or gray scale) image databases. To extract the proposed descriptor, a two unit fuzzy system is used. To extract the brightness information, a fuzzy unit classifies the brightness values of the image's pixels into L_{Bright} clusters. The cluster centers are calculated using the Gustafson Kessel Fuzzy Classifier. Ceylan, and Okan Ozturkmenoglu</p><p>The texture information embodied in the proposed descriptor comes from the Directionality histogram. This feature is part of the well-known Tamura texture features. Fractal Scanning method through the Hilbert Curve or the Z-Grid method is used to capture the spatial distribution of brightness and texture information <ref type="bibr" coords="6,215.81,214.82,12.21,11.04" target="#b8">(7)</ref> After extracting features, we examined the performance of all extracted feature and perceived that compact composite features like CEDD and FCTH have satisfactorily retrieval result on our image collection and require noticeably lower computational power and storage space.(as Figure <ref type="figure" coords="6,430.23,290.81,4.50,11.04" target="#fig_0">1</ref>)  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Expansion with Relevance feedback</head><p>The idea behind relevance feedback is to take the results that are initially returned from a given query and to use information about whether or not those results are relevant to perform a new query. Firstly we examined blind or Pseudo relevance feedback on ImageCLEFmed 2011 data collection and assumed that the top 3 ranked documents were most relevant then considered them as query image for each topic but found that this approach could not increase the performance of system. Therefore we performed Explicit Graded relevance feedback to indicate the most relevant images retrieved from a typical query based on assessment of non-professional person. Then we employ those images as query image of the topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Narrowing down</head><p>The database includes over 300,000 images of the biomedical open access literature from journals of BioMed Central at the PubMed Central database associated with their original articles in the journals. Therefore, there are many irrelative generic biomedical illustrations that make the collection so noisy. This noisy documents influence on reduction of system precision. To solve this problem, we filter out such documents using classification decision tree methods and narrowing data collection down to about 35000 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimentations</head><p>In order to assess the above mentioned methods, we set up a set of experiments on the data collection of ImageCLEF 2012 Medical retrieval. We submitted 24 runs to ImageCLEF Ad-hoc Medical Retrieval in five categories as following:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baseline text-only runs</head><p>This category includes the runs for baseline retrieval in textual modality on xml metadata, called as DEMIR_R1 to DEMIR_R4. Difference between runs of this category is on tags that are indexed. DEMIR_R1 index just caption of images in original article while DEMIR_R2 indexed full articles correspond to image. Caption of the images and abstract of the corresponded articles were indexed at DEMIR_R3. Title of the article joined to indexed contents in DEMIR_R4. As illustrated in Table1 results shows that caption of images is comprehensive and effective content of metadata to index. On the other hand, although full article has more irrelative content but is more effective than abstract.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baseline and combined visual-only runs</head><p>As we mentioned in previous sections, CEDD, FCTH and CLD descriptors outperform other low level features. We submitted retrieval result of them as DEMIR_R5, DEMIR_R6 and DEMIR_R7 respectively. Beside these runs, we also investigate on impact of intra-modality combination between these features. We applied our proposed integrated approach on these features in DEMIR_R8 and DEMIR_R9. Also we used feature concatenation method on the synchronous feature vectors of all images in data collection and topics and concatenated them as the joint feature vector in DEMIR_R10 and DEMIR_R11. Results show that the best performance of this category appears in DEMIR_R9, where we combined all of these descriptors based on our method, using CombSUM function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ICMR based runs</head><p>In this category, we examine our claims in ImageCLEF and confirm them again. We performed integrated weighted CombSUM combination such that coefficient of text modality was 1.7 folds of visual modality. Consequently, system performance arise to the best performance in contrast with other fusion methods in same condition. It is obvious that system performance in DEMIR_R14 and DEMIR_R15 that combined baseline textual and visual runs in integrated approach using weighted sum, are more batter than performance of DEMIR_R12 and DEMIR_R13 respectively. This viewpoint is true also in subject of DEMIR_R17 compared by DEMIR_16. In these later runs, we apply two-level integrated combination; in intra-modality level between low level image features and in inter-modality level between result of pervious level as a visual modality (DEMIR_R9) and baseline run of textual modality (DEMIR_R1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Runs on narrowed down dataset</head><p>In this category, we repeated some of submitted runs on new narrowed down and filtered out dataset but any of these new versions of runs did not improve the performance of total system. We submitted this category as DEMIR_R18 till DEMIR_R21.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Query expansion with relevance feedback</head><p>To submit runs of this class, firstly we chose three most relevance document from retrieved document set of each base run , if it was existence in top 50 retrieved document set. Then these chosen images were joint to image query set of each topic then run retrieval system again to obtained new result. In our experiments, selection of most relevant document is based on only textual, only visual and both of visual and textual information respectively in DEMIR_R22, DEMIR_R23, DEMIR_R24. Although this manner decrease the performance of system when we consider at textual properties for selection of relevant documents, but it improve the effectiveness of system in visual modality so that DEMIR_R23 could obtain the best rank of visual runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this year, we examined the impact of our integrated combination method on different level of multimodality retrieval system and found that proper combination model can improve the performance of multimodal retrieval systems. In order to investigate on possible combinations methods, we first do some evaluations to determine the appropriate low-level features and distance functions. Then we presented a two level integrated combination method for multimodal CBIR systems. We also perform a visual explicit relevance feedback method and examine its effects on visual, textual and mixed runs. Meanwhile we narrowed down data collection by filtering irrelative generic biomedical illustrations out of data collection using visual low level features. Our finding illustrated that:</p><p> Explicit relevance feedback based on visual similarity of image improves the performance of medical image retrieval system.</p><p> Elimination of noisy and irrelevant images from dataset can progress effectiveness and efficiency of medical image retrieval system.</p><p> It is clear that integrated combination multimodal retrieval improves the performance of result system in all combination level, regardless of inter or intra-modality level.</p><p>Ali Hosseinzadeh Vahid, Adil Alpkocak, Roghaiyeh Gachpaz Hamed, Nefise Meltem Ceylan, and Okan Ozturkmenoglu  In the best combination of textual and visual modalities, weight for textual modality is about 1.7 folds of visual modality weight.</p><p>Our study can be extended in several ways, in future. First, it would be good to apply this experimentation results to other medical image collection and verify that our findings produces the similar results on similar CBIR systems. Second, the impact of combination functions on system performance can also be further investigated. Lastly, our study can be extended into other domain of CBIR Systems rather than medical domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,186.86,471.64,221.51,8.04;6,124.82,491.95,345.66,11.04;6,124.82,512.11,345.73,11.04;6,124.82,532.15,345.98,11.04;6,124.82,552.31,345.94,11.04;6,124.82,572.50,345.77,11.04;6,124.82,592.66,345.60,11.04;6,124.82,612.82,320.37,11.04"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Comparison on performance of different low level features Also, we assessed performance of different similarity function on Compact Composite features and comprehended Euclidean distance on CEDD and FCTH features and Cosine distance function on SPCD and BTDH produce the best performance(as illustrated in Figure2). Then we calculate the similarity between query and dataset objects using Euclidean distance in matching phase. Then we sorted all of dataset images in a descending list based on the value of similarity score in corresponding to each query example image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,180.38,413.68,234.59,8.04;7,215.57,423.52,166.17,8.04"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Distance function performance evaluation of different features based on number of relevant retrieved documents.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="4,155.42,121.07,306.74,9.00;4,124.82,132.23,122.14,9.00"><p>Ali Hosseinzadeh Vahid, Adil Alpkocak, Roghaiyeh Gachpaz Hamed, Nefise Meltem Ceylan, and Okan Ozturkmenoglu</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,129.03,336.65,308.33,11.04;12,124.82,352.13,233.21,11.04" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="12,279.70,336.65,157.66,11.04;12,124.82,352.13,228.98,11.04">Evaluation of Fusion Techniques for Multimodal Content-based Medical Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Imageclefmed</forename><surname>Demir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,360.55,352.13,61.96,11.04;12,124.82,367.49,342.22,11.04;12,124.82,383.09,257.31,11.04" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Alpkocak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Ozturkmenoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Berber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hosseinzadeh Vahid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>Gachpaz Hamed</surname></persName>
		</author>
		<title level="m" coord="12,145.19,383.09,176.71,11.04">CLEF (Notebook Papers/Labs/Workshop</title>
		<imprint>
			<date type="published" when="2010">2011. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,129.03,408.43,331.82,11.04" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="12,135.74,408.43,320.47,11.04">Terrier: A High Performance and Scalable Information Retrieval Platform</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,124.82,423.91,339.53,11.04;12,124.82,439.51,284.70,11.04" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ch</forename><surname>Lioma</surname></persName>
		</author>
		<title level="m" coord="12,124.82,439.51,280.43,11.04">ACM SIGIR&apos;06 Workshop on Open Source Information Retrieval</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,129.03,464.83,327.56,11.04;12,124.82,480.31,208.83,11.04" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,135.74,464.83,140.38,11.04">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,381.04,464.83,75.55,11.04;12,124.82,480.31,108.75,11.04">Electronic library and information systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,129.03,505.63,330.22,11.04;12,124.82,521.11,307.14,11.04;12,124.82,536.59,328.99,11.04;12,124.82,552.07,246.90,11.04" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,223.42,505.63,235.83,11.04;12,124.82,521.11,32.82,11.04">AN INTERACTIVE CONTENT BASED IMAGE RETRIEVAL SYSTEM</title>
		<author>
			<persName coords=""><surname>Img</surname></persName>
			<affiliation>
				<orgName type="collaboration">RUMMAGER</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,327.51,536.59,126.30,11.04;12,124.82,552.07,183.74,11.04">International Conference on Similarity Search and Applications (SISAP)</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</editor>
		<meeting><address><addrLine>Lux, M. Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="151" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,129.03,577.42,297.36,11.04;12,124.82,592.90,333.77,11.04;12,124.82,608.38,338.45,11.04;12,124.82,623.86,308.09,11.04" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,248.60,577.42,177.79,11.04;12,124.82,592.90,157.04,11.04">DIRECTIVITY DESCRIPTOR -A COMPACT DESCRIPTOR FOR IMAGE INDEXING</title>
		<author>
			<persName coords=""><forename type="first">Cedd: Color And</forename><surname>Edge</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Retrieval</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,322.72,608.38,140.55,11.04;12,124.82,623.86,240.94,11.04">6th International Conference in advanced research on Computer Vision Systems (ICVS)</title>
		<meeting><address><addrLine>Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="312" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,129.03,649.18,328.08,11.04;12,124.82,664.66,320.12,11.04;12,124.82,680.14,308.33,11.04;13,124.82,149.42,312.16,11.04;13,124.82,165.02,96.75,11.04" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,416.54,649.18,40.57,11.04;12,124.82,664.66,150.31,11.04">FEATURE FOR ACCURATE IMAGE RETRIEVAL</title>
		<author>
			<persName coords=""><forename type="first">Fcth: Fuzzy Color And Texture Histogram-A Low</forename><surname>Level ; Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Boutalis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,264.85,680.14,168.29,11.04;13,124.82,149.42,238.21,11.04">9th International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS)</title>
		<meeting><address><addrLine>Klagenfurt, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="191" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,129.03,190.34,317.43,11.04;13,124.82,205.82,338.93,11.04;13,124.82,221.30,321.90,11.04" xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">Content Based Radiology Image Retrieval Using A Fuzzy Rule Based Scalable Composite</forename><surname>Descriptor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,193.87,221.30,153.23,11.04">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="493" to="519" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,129.03,246.65,291.49,11.04;13,124.82,262.13,258.56,11.04" xml:id="b9">
	<analytic>
		<title/>
		<author>
			<orgName type="collaboration" coords="13,188.13,262.13,189.50,11.04">AND RELEVANCE FEEDBACK INFORMATION</orgName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,135.74,246.65,284.78,11.04;13,124.82,262.13,60.89,11.04">ACCURATE IMAGE RETRIEVAL BASED ON COMPACT COMPOSITE DESCRIPTORS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,385.99,262.13,83.27,11.04;13,124.82,277.61,340.48,11.04;13,124.82,292.97,328.90,11.04;13,124.82,308.57,19.56,11.04" xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zagoris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Papamarkos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,372.01,277.61,93.29,11.04;13,124.82,292.97,249.01,11.04">International Journal of Pattern Recognition and Artificial Intelligence (IJPRAI)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
