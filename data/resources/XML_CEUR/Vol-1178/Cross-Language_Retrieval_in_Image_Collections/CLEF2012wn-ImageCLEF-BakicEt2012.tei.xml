<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,144.58,115.96,326.20,12.62;1,204.99,133.89,205.38,12.62">Inria IMEDIA2&apos;s participation at ImageCLEF 2012 plant identification task</title>
				<funder>
					<orgName type="full">Agropolis</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,140.55,171.56,47.76,8.74"><forename type="first">Vera</forename><surname>Bakić</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria IMEDIA2 Team</orgName>
								<address>
									<settlement>Rocquencourt</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,198.87,171.56,65.62,8.74"><forename type="first">Itheri</forename><surname>Yahiaoui</surname></persName>
							<email>itheri.yahiaoui@univ-reims.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Inria IMEDIA2 Team</orgName>
								<address>
									<settlement>Rocquencourt</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Laboratoire CReSTIC</orgName>
								<orgName type="institution">Université de Reims</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,281.38,171.56,66.15,8.74"><forename type="first">Sofiene</forename><surname>Mouine</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria IMEDIA2 Team</orgName>
								<address>
									<settlement>Rocquencourt</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,358.08,171.56,109.48,8.74"><forename type="first">Saloua</forename><surname>Ouertani-Litayem</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria IMEDIA2 Team</orgName>
								<address>
									<settlement>Rocquencourt</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,142.93,183.51,68.38,8.74"><forename type="first">Wajih</forename><surname>Ouertani</surname></persName>
						</author>
						<author>
							<persName coords="1,221.86,183.51,100.98,8.74"><forename type="first">Anne</forename><surname>Verroust-Blondet</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria IMEDIA2 Team</orgName>
								<address>
									<settlement>Rocquencourt</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,333.41,183.51,56.56,8.74"><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria IMEDIA2 Team</orgName>
								<address>
									<settlement>Rocquencourt</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,419.89,183.51,48.07,8.74"><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<email>alexis.joly@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Inria IMEDIA2 Team</orgName>
								<address>
									<settlement>Rocquencourt</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Inria ZENITH Team</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,144.58,115.96,326.20,12.62;1,204.99,133.89,205.38,12.62">Inria IMEDIA2&apos;s participation at ImageCLEF 2012 plant identification task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">26970B246289595DC0C02D321EB315BB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pl@ntNet</term>
					<term>IMEDIA</term>
					<term>Inria</term>
					<term>ImageCLEF</term>
					<term>plant</term>
					<term>leaves</term>
					<term>images</term>
					<term>collection</term>
					<term>identification</term>
					<term>classification</term>
					<term>evaluation</term>
					<term>benchmark</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of Inria IMEDIA2 Team, within the Pl@ntNet project 4 , at the ImageCLEF2012 plant identification task. The runs used very distinct approaches, sometimes relying on similar extracted features. For Scan and Scan-like categories, the first two runs combine distinct local and contour approaches in two ways (late and early fusion), while the third run explores the learning capacity of a multi-class SVM technique on a contour based descriptor. For Photograph our runs used local features positioned towards the center of the image to reduce the impact of background features. In the second run, an automatic segmentation with a rejection criterion was attempted. In the third run, points were associated with interesting zones. In general, even if they were distinct, the methods used performed very well.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The plant identification task of ImageCLEF2012 is a tree species identification based on leaf images. It was organized as a plant species retrieval task over 126 species with visual content being the main available information. Three types of image content were considered: Scans -scans with a white background, Scan-like -photographs with a white, uniform background and Photographsunconstrained leaf's images acquired on trees with natural background.</p><p>A part of image dataset was provided as training data with full class labels at the beginning of the task, while the test dataset was provided several weeks later without labels. The training and test subsets were built so that the images from an individual plant are not present in both sets, which makes the task more similar to real external queries. The table below shows the composition of the data sets: The identification score S was related to the rank of the correct species in the list of retrieved species as follows:</p><formula xml:id="formula_0" coords="2,217.29,207.46,100.45,30.31">S = 1 U U u=1 1 P u Pu p=1 1 N u,p</formula><p>Nu,p n=1 S u,p,n , where U = number users (who have at least one image in the test data), Pu = number of individual plants observed by the u th user, Nu,p = number of pictures taken from the p th plant observed by the u th user, Su,p,n = score between 1 and 0 equal to the inverse of the rank of the right species (for the n th picture taken from the p th plant observed by the u th user).</p><p>Inria IMEDIA2 team, within the Pl@ntNet project, submitted three runs, covering all three image categories. For Scan and Scan-like we tested early and late fusion of local and shape boundary features using KNN and SVM classifiers. For Photograph local features were selected using different geometric constraints and leaf detection and segmentation. In the following text, we will describe the Scan and Scan-like approaches in Section 2, while the Photograph ones will be presented in Section 3. The results are discussed in Section 4. Concluding remarks are in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods used for Scan and Scan-like</head><p>Both contour and local interest points descriptions are useful for the leaf species identification: contour based descriptors capture the global shape of the leaves, while local descriptors associated to the extracted interest points retain their micro-texture. Our three runs test different approaches based on these two kinds of descriptors: the first two runs combine distinct local and contour approaches in two ways (late and early fusion in Sections 2.1 and 2.2), while the third run (Section 2.3) explores the learning capacity of a multi-class SVM technique on a contour based descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Late fusion of a large-scale local features matching method and a shape boundary feature based method → RUN1</head><p>This method is inspired by two runs submitted by Inria IMEDIA Team at Im-ageCLEF 2011 <ref type="bibr" coords="2,202.64,596.34,9.96,8.74" target="#b7">[8]</ref>: the large-scale local features matching method was the best in the Scan category, while contour based method was the best in the Scan-like category. The two approaches are complementary because they retain both local and boundary shape information and we observed that they performed well last year on distinct test images. Thus, we decided to use a basic combination of these two methods with a fusion of their responses on the image level. This allows us to keep the good results of each, while minimizing the impact of the erroneous response of one of the approaches. In addition, several major changes in both approaches were made to improve the performance of each or reduce the use of computing resources.</p><p>Large-scale local features matching. The basic algorithm applied for this method is: (i) Interest points detection, (ii) chosen local description for each interest point, (iii) local features matching.</p><p>(i) Interest points detection -200 Harris points were used at four distinct resolutions with a scale factor equal to 0.8 between each resolution <ref type="bibr" coords="3,436.65,219.99,10.52,8.74" target="#b8">[9,</ref><ref type="bibr" coords="3,448.82,219.99,11.62,8.74" target="#b10">11]</ref>. We output up to 4 significant orientations for a patch around each point, where applicable. This method enabled us to boost the number of training samples and compensate for non-ideal single orientation detection, while keeping the number of points rather small. Figure <ref type="figure" coords="3,324.75,267.81,4.98,8.74" target="#fig_0">1</ref> illustrates the process for sample patches. Finally, the number of points rose to an average of 343 per image 5 .</p><p>orig.patch major ori. + additional ori. orig.patch major ori. + additional ori. (ii) Local features are extracted around each Harris point from an image patch oriented and scaled according to the given orientation and scale: SURF <ref type="bibr" coords="3,470.08,405.53,10.52,8.74" target="#b1">[2]</ref> is based on sums of 2D Haar wavelet responses, we used OpenSURF implementation <ref type="bibr" coords="3,212.07,429.44,9.96,8.74" target="#b4">[5]</ref>; a 16-dim. histogram based on the Hough transform <ref type="bibr" coords="3,458.69,429.44,9.96,8.74" target="#b5">[6]</ref>; a 20-dim. Fourier histogram <ref type="bibr" coords="3,269.80,441.40,10.52,8.74" target="#b5">[6]</ref> and an 8-dim. Edge Orientation Histogramwere concatenated, resulting in a 108-dimensional vector. (iii) Matching -Signatures in the training dataset are compressed and indexed using RMMH method <ref type="bibr" coords="3,248.40,476.74,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="3,265.56,476.74,7.01,8.74" target="#b7">8]</ref>. Local features of a query image are compressed through a 256-bit hash code and its approximate 30-nearest neighbors are searched to obtain a set of candidate matches. To an image, a score equal to the number of its local features matched is assigned. Then, images are re-ranked according to their score. For the global geometric properties of the leaf form, we used six shape parameters as in <ref type="bibr" coords="3,176.75,638.04,9.96,8.74" target="#b7">[8]</ref>: circularity, convexity, solidity, rectangularity, sphericity and ellipse variance. Segmentation was done automatically using Otsu algorithm <ref type="bibr" coords="4,439.73,118.99,14.61,8.74" target="#b14">[15]</ref>, with addition of automatic selection of a channel that gives the best separation.</p><p>Late fusion. The two methods described above will each return a list of images belonging to the same training database but ordered differently: a same image may be present in both lists, but at a different rank. Then, the two lists are merged by setting the rank of an image to a minimum of the ranks in each list.</p><p>In this way, we preserve a good position of an image returned by one method and ignore the other, presumably, incorrect rank. After the fusion, the unified image list is re-ranked according to the new scores.</p><p>Classification with a top-knn decision rule. To obtain the species list from the images list, we counted the number of occurrences of each species in the top 15 images. The list of species was then re-ranked according to their score, to obtain the final ordering of the proposed species.</p><p>Training data and descriptor choices. In order to determine the efficiency of a descriptor and which image category to use for training, we performed a series of "leave-individual-out" tests on the training data itself. That is, in the score calculation for an image from the training database, we excluded from the returned response all the images belonging to the same individual as the query image. In addition, we averaged the score as for the official one. We concluded that the combination of the presented descriptors gives the best results on the train data sets and that the Scan images should be searched in Scan dataset, while the Scan-like should be searched in the union of Scan and Scanlike. Assuming that the test images will perform similarly to the train images, we decided to keep the above parameters for the submitted run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Advanced shape context → RUN2</head><p>The methods used in this run are based on an advanced shape context approach <ref type="bibr" coords="4,134.77,476.78,14.61,8.74" target="#b13">[14]</ref>, which extends the standard shape context <ref type="bibr" coords="4,348.40,476.78,9.96,8.74" target="#b2">[3]</ref>. Here, two different sets of points are distinguished when computing the shape contexts: the voting set, i.e. the points used to describe the coarse arrangement of the shape and the computing set containing the points where the shape contexts are computed. Two scenarios are proposed by varying the computing set C and the voting set V of points in the image.</p><p>SC0: Spatial relations between margin points. Here the computing set C and the voting set V are identical. They involve only margin points i.e. n points extracted from the margin by a uniform quantization:</p><formula xml:id="formula_1" coords="4,134.77,578.41,345.83,20.69">C = V = {margin points} (Figure 2 (a)</formula><p>). This description corresponds to the shape context proposed by Belongie et al. <ref type="bibr" coords="4,203.89,602.32,10.52,8.74" target="#b2">[3]</ref> with a different matching method. Note that the venation network is not introduced here. Segmentation algorithm was the same as in Section 2.1.</p><p>SC2: Spatial relations between salient and margin points. Here we want to measure the spatial relationships between the salient points described in the context defined by the leaf margin (Figure <ref type="figure" coords="5,323.70,244.38,4.98,8.74" target="#fig_1">2</ref> (b)). The voting set of points V is composed of all the margin points. The Harris points form the computing set C: C = V, C = {salient points} and V = {margin points}. As mentioned above, the salient points may lay inside the leaf or may belong to the leaf margin. Our aim is to study the correlation between the venation network and the margin of the leaves belonging to the same species.</p><p>Local features. The advanced shape context captures a spatial configuration of points without taking into account local properties of the image around the set C of computing points. Thus, to enrich the description, a set of local features computed on the neighborhood of each point of C is introduced. As the color is not a discriminant feature for leaves, we focus on texture and shape. Three local features are extracted from the gray-level of an image patch located around each point: a 16-dim. Hough histogram; a 40-dim. Fourier histogram <ref type="bibr" coords="5,436.85,393.83,10.52,8.74" target="#b5">[6]</ref> and an 8-dim. classical Edge Orientation Histogram, which is known to be suitable for non-uniform textures. These three features have given promising results when associated with Harris points on scans of leaves in <ref type="bibr" coords="5,356.60,429.70,9.96,8.74" target="#b7">[8]</ref>.</p><p>Matching Method. The features matching, is done by a Multi Probe Locality Sensitive Hashing technique <ref type="bibr" coords="5,262.94,459.60,15.50,8.74" target="#b11">[12]</ref> and the distance L 2 is used to compute the similarity between two feature vectors. The principle of this algorithm is to project all the features in an L dimensional space and to use hash functions to reduce the search and the time cost.</p><p>Classification with a top-knn decision rule is as in Section 2.1.</p><p>Descriptor choices. After an evaluation similar to the one described in Section 2.1 on the training data, RUN2 was constructed as follows: -A combination of SC2 and local features with 200 Harris points for Scan; -SC0 with 200 sample points on the leaf margin for Scan-like.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">SVM multi-class classification → RUN3</head><p>This run is performed in order to explore the enhancement of shape feature behaviour through a learning schema. In our experiments we have tested 6 strategies to do classification (see Table <ref type="table" coords="5,291.40,644.16,3.87,8.74">2</ref>). To do so, we split the training dataset into train and validation sets with different proportions which also respects the non-split of images coming from individual plants (the relative images of a given individual plant are either used in train or in validation and not in both). We performed a multi-class SVM technique on a contour based descriptor <ref type="bibr" coords="6,446.27,142.90,14.61,8.74" target="#b16">[17]</ref>. Indeed, we adopted a one-vs-one schema <ref type="bibr" coords="6,305.00,154.86,14.61,8.74" target="#b12">[13]</ref>, which offers more balanced elementary binary classification compared to one-vs-all. It decomposes the classification problem into several binary classification tasks. They are built to discriminate between each pair of classes, while discarding the rest of the classes. If K is the number of classes, one will need to train one binary classifier for each of the possible two classes combinations. This procedure will generate K(K -1)/2 binary classifiers. When applied to a test data, a voting is performed among the classifiers and the class is predicted according to the maximum voting strategy <ref type="bibr" coords="6,453.09,238.55,9.96,8.74" target="#b6">[7]</ref>. As a kernel choice we conventionally adopted a linear kernel on the 630-dimensional shape features.</p><p>We notice the surprising effect of performance's increase between the 70%-30% splitting and the 50%-50% one. That might come from less overfitting on the training subset. For the run, we have chosen to keep learning on both Scan and Scan-like images together without a prior distinction. That sounds more stable according to the preliminary tests (see rows 5 and 6 in the table 2). For more classes recall and since the final run evaluation takes that into consideration, we have also kept the whole classes ranked according to the voting strategy <ref type="bibr" coords="6,452.82,346.14,9.96,8.74" target="#b6">[7]</ref>. Scan + Scan-like Scan-like 0.4145 0.4236 Table <ref type="table" coords="6,164.11,442.99,4.13,7.89">1</ref>. Strategies tested and their performance on two splits of scan and scan like datasets. From left to right: (70% train, 30% test) and (50% train, 50% test)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strategy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods used for Photograph</head><p>For the Photograph images, the background around the leaf is not uniform (sand, stones, other leaves), and the leaves may be deformed or mutually occluded. The shape boundary features used for Scan and Scan-like in Section 2.1 are unsuitable in this case: the automatic segmentation of the leaf and background is far from perfect and the detected shape is not good. In addition, the Harris points detector may detect mainly the points in the background. In our runs we explored the following directions: the fact that the leaf is, in general, centered (Section 3.1); whether the automatic segmentation improves performance (Section 3.2) and finally, using multi-class SVM on embedded local features (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Rhomboid masking and local features matching → RUN2</head><p>The basic algorithm applied for this method is the same as described in Section 2.1 for local features. The major difference is in the selection of Harris points, where masking and points weighting were applied.</p><p>Rhomboid filtering. In order to minimize the effect of the cluttered background, we modified the input image for the Harris points detector. The assumption is that the leaf is centered, so we masked with an adaptable rhomboid-shape the corners of the image; the transition from foreground to masked out region was smooth to avoid that the points get detected on the mask boundary.  Grid-based points weighting. In the above detection, we noticed that even if the small amount of the cluttered background appears, most of the points are still located in the background. Thus, before the selection of the best 200 points, we applied a weighting scheme based on a grid (of size 7x7): the number of points allocated for the current scale is distributed in the grid cells using Gaussian-like distribution -the closer to the center, the more points allocated and selected. Training data and descriptors choice. For the selection of descriptors and training dataset we used the same procedure as in Section 2.1. Only the photograph data were used as the training database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Segmentation, filtering and local features matching → RUN1</head><p>Another approach for the Photograph images was to attempt the segmentation, where it was possible, and to reject the points that do not belong to the leaf region. In this case, we use the complete training database, with Scan, Scan-like and Photograph images. The advantage of this method is that we could use it for all types of test images. We used the provided "content" annotations: Leaf, Picked leaf and Leafage images and applied different processing for each type. The algorithm and the decision rule were the same as in Section 3.1.</p><p>Scan and Scan-like processing All training images of these types were processed using the same algorithm as in Section 2.1, local features part.</p><p>Leafage processing All images of this type were processed using the same algorithm as in Section 3.1. The reason for this choice is that Leafage images have multiple leaves and are extremely cluttered and hard for segmentation.</p><p>Leaf and Picked leaf processing. For each image we attempted segmentation using Otsu algorithm <ref type="bibr" coords="8,249.28,130.95,14.61,8.74" target="#b14">[15]</ref>, with addition of automatic selection of a channel that gives the best separation and LUV channels were used as they give better separation for cluttered backgrounds. Then, we automatically verified if the region was well-formed or if the foreground and background classes were too mixed: under the assumption that we have two classes (Figure <ref type="figure" coords="8,374.86,178.77,4.98,8.74" target="#fig_6">4</ref> (b)), we calculated the average distance of each point from the region centers. If the regions were mainly centered and the difference of the distances was more than 20%, segmentation was accepted as good, otherwise, we rejected it and the image was processed as if it was labeled Leafage. For the correct segmentation, the biggest found region that does not touch the image boundary was considered as the leaf. Figure <ref type="figure" coords="8,475.61,238.55,4.98,8.74" target="#fig_6">4</ref> illustrates the process of correct (top) and failed (bottom) segmentation. We output 400 points from the Harris detector, however, for the final points list, we keep up to 200 points that do belong to the leaf region (Figure <ref type="figure" coords="8,410.95,274.41,19.47,8.74" target="#fig_6">4 (c)</ref>). With the addition of the multi-orientation points, the number of points per image rose to an average of 395 for the Photograph.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-Class SVM on embedded local features → RUN3</head><p>In this run, we explore a data-independent bag of word schema for description and automated partial images zones definition. On this representation, we perform multi-class SVM for learning and predicting plant leaves' classes. The run is motivated by the joint retrieval and learning schema presented in <ref type="bibr" coords="8,437.84,590.67,14.61,8.74" target="#b15">[16]</ref>, however, we do not have image's interesting zones annotations provided by a user. Instead, we were interested on bounding boxes sampling approaches defining the zones.</p><p>Bounding box definition. We used the objectness measure introduced in <ref type="bibr" coords="8,467.30,644.16,9.96,8.74" target="#b0">[1]</ref>, which is a class generic object detector, quantifying how likely a part (e.g. win-dow) of an image contains an object of any class. During learning objectness cues' parameters, we did not use plants' images with ground truth relevant zones. Instead, we learned using generic VOC Pascal classes. We expect a broad learning transfer from generic objects to leaves' world. We arguably consider few number of windows per image <ref type="bibr" coords="9,236.55,166.81,16.38,8.74" target="#b9">(10)</ref>, since images mostly have a single or few interest zones. Figure <ref type="figure" coords="9,197.03,178.77,4.98,8.74" target="#fig_7">5</ref> shows an automatic selection of windows expected as object's delimiter. Representations. We used the local features from Section 3.1 projected through an efficient approach for feature set representation based on random histograms <ref type="bibr" coords="9,134.77,318.60,9.96,8.74" target="#b3">[4]</ref>. Two representations were generated for each image: (i) the features falling within bounding boxes, and (ii) the features in the whole image. Embedding parameters are chosen such that the final representation's histograms have a considerably high dimension of 20480 bins. To evaluate which representation is more suitable, we tested the following combinations using a train/validation dataset splits and a linear multi-class SVM (as in Section 2.3): bounding box or whole image features used for train or validation sets (Table <ref type="table" coords="9,401.46,390.33,3.87,8.74">2</ref>, left 3 columns).</p><p>For both 1 st and 4 th strategies, we adopt a ranking based on a voting of classes. Each window votes one time for the predicted class, the final vote was the number of times a class is voted by the bounding boxes of a given image. Table <ref type="table" coords="9,475.61,426.20,4.98,8.74">2</ref> shows the decision results for the three splits. It is clear that predicting on sampled bounding boxes, while learning on the whole image noticeably outperforms other strategies. Thus, we used the 4 th strategy. Since the final score explores a ranking of classes, to prevent loosing the relevant class, we keep the best 4 classes voted within each image.</p><p>Train set Test set Decision, 3 splits Avg. 1 st bounding box bounding box 0.001 0.001 0.000 0.001 2 nd bounding box whole image 0.250 0.254 0.269 0.257 3 rd whole image whole image 0.251 0.265 0.255 0.257 4 th whole image bounding box 0.325 0.291 0.345 0.320 Table <ref type="table" coords="9,185.08,564.94,4.13,7.89">2</ref>. Strategies tested and their performance on three splits of dataset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Table <ref type="table" coords="9,161.73,620.25,4.98,8.74" target="#tab_3">3</ref> resumes the official scores for the first 12 submitted runs out of 30 total. Our three runs give generally good results and are placed in the 10 top runs for different categories. For the Scan category, RUN1 and RUN3 are in the top 3 while RUN2 is placed 8 th . For the Scan-like, the three runs were in the top 5 and RUN2 gave the best result of the task while RUN1 is 3 rd and RUN3 5 th . For the more difficult category of Photograph RUN1, RUN2 and RUN3 were respectively 4 th , 5 th and 10 th among 21 runs that used a full automatic approach. Scan and Scan-like. With respect to last year's absolute score values, we expected that this year's scores achieve at least the same value if not higher. Indeed, this year the score reflects the correct species rank (which gives always a value distinct from zero), while last year the score was a pure classification score (0 or 1). However, this was neither the case for last year's participants.</p><p>The reasons are probably that this year the task was more challenging, with the number of species almost doubled and higher number of contributors, which increased the overall visual diversity of images and species. For instance we observed that train and test Scan images are rather different: most of the train dataset leaves are mature and green, while a good number of those in the test dataset are young or dead. For Scan-like, the image dataset has varying quality like illumination changes, more or less pronounced shadows and variations in the background.</p><p>In spite of these difficulties RUN1 performs well and confirms the good results obtained in the last year's task. The late fusion of two complementary approaches keeps the good performances for both Scan and Scan-like categories, while last year each method had the best performances in only one category.</p><p>For Scan-like, RUN2 achieved the best performances in this year's task. We suppose that the delayed use of the boundary points for the matching phase brought as more similar the partial shape boundary information. This may have compensated for the variations in the leaf boundary (leaf poses, missing leaflets) and imperfect segmentations (shadows), as they were not taken as global information. For Scan results are lower than expected. As we noted a significant intra-class variations in the micro-texture, we suppose that the early fusion between the shape and local descriptors was less robust to them. However, this problem could be compensated by more suitable local features to this visual content.</p><p>The RUN3 gave also very good results on both Scan and Scan-like categories, with more or less the same score, which seems coherent with the fact that the two types of images were considered as one in order to compensate a lack of examples for some species. Moreover, this method tends toward the score of RUN1, notably for Scan, while it used only a subset of contour shape descriptors.</p><p>Photographs. RUN1 and RUN2 gave good results, if we consider only fully automatic runs. Our approaches gave even more or less the same results as some methods with human interactions.</p><p>RUN1 had slightly higher score than RUN2, which shows us that the automatic segmentation and the use of the complete database for training proved useful. It is important to note here that the segmentation was performed over 35% of all Photograph images following the "leaf" and "picked leaf" tags and the rejection criterion. Within the segmented images, only 2% were complete misses (e.g. no leaf part included), and 5% with partial leaf information. This illustrates the idea that it is better to focus on well framed (and segmentable) pictures and reject the ones that are too cluttered.</p><p>For its part, RUN3 gave intermediate performances within a group of runs with automatic approaches, which had more or less similar scores around 0.15. These results are lower than expected, which could be due to the fact that the bounding boxes did not quite correspond to the central image part with the object of interest (the leaf). We are convinced that this method could be improved for instance by using points detection for each interest box separately, and also by using more specialized models of object detection learned on plant images rather than on the general objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Inria IMEDIA2 Team submitted runs that used distinct approaches, sometimes relying on similar extracted features. Despite these differences, the methods used performed well and the three runs are placed in the 10 top runs for each category.</p><p>Again for the second year we obtained very good results on Scan and Scanlike. However, with respect to last year's absolute score value, we achieved lower scores, as other previous participants, in spite of the improvement of our methods, the proposition of new approaches and a less strict metric. This highlight the fact that plant identification from leaf Scans and Scan-like images is far from being resolved, especially when will consider more than 500 tree species as it can be observed in France for instance. We will try to improve our methods with the study of new features even more suitable to the visual diversity introduced by new image contributors, by exploring new classification and combination approaches with the metadatas (gps, dates, hierarchical taxonomy information).</p><p>For Photograph, it is difficult to compare results obtained with full automatic approaches to semi-automatic approaches with human assistance. If we consider only full-automatic approaches, we obtained promising results that we hope to reproduce on plant organs like flower or fruit, for which it is far more difficult to have Scan or Scan-like images. Considering fully automatic or humanly assisted approaches, on one hand, we notice that with human assistance runs from other teams tend to have quite similar absolute scores as best Scan and Scan-like runs. On the other hand, we notice also that several automatic approaches give better performances than assisted ones. Maybe we will have to consider, alongside improving automatic approaches, several human interactions, like semi-supervised segmentation for test images only.</p><p>For all three categories, we have to pursuit that the correct species is returned within the top 5 proposed species. This would make our methods suitable for a mobile based recognition application.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,134.77,362.14,345.83,7.89;3,134.77,373.12,55.06,7.86;3,189.83,371.36,8.44,5.24;3,201.84,373.12,210.59,7.86;3,412.43,371.36,7.50,5.24;3,420.43,373.12,10.24,7.86;3,430.67,371.36,7.19,5.24;3,441.43,373.12,38.91,7.86"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Multiple orientations: from an original patch (left), in addition to the major orientation (2 nd column), other significant orientations are output (3 rd , 4 th column)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,196.55,186.61,11.78,7.86;5,322.85,186.61,12.29,7.86;5,134.77,199.05,345.82,7.89;5,134.77,210.03,311.38,7.86"><head>Fig. 2 .</head><label>2</label><figDesc>Points used in scenario SC0 (a) and SC2 (b). The small circles represent the sample points on the leaf margin. The crosses represent Harris salient points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,452.47,166.81,28.12,8.74;7,134.77,178.77,345.82,8.74;7,134.77,190.72,313.50,8.74"><head></head><label></label><figDesc>Figure 3 illustrates (a) the points detected in the original image, (b) the masked image used as the new input and (c) the points detected in the masked image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,134.77,315.87,345.83,7.89;7,134.77,326.85,292.48,7.86;7,178.01,206.89,55.33,98.37"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Rhomboid filtering and grid weighting: (a) original points, (b) masked image, (c) points detected in image (b), (d) points detected with grid weighting</figDesc><graphic coords="7,178.01,206.89,55.33,98.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,193.95,416.49,286.65,8.74;7,134.77,428.44,130.98,8.74"><head>Figure 3 (</head><label>3</label><figDesc>d) illustrates the final selection of the points using gridand centered-based weighting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,314.82,298.32,165.76,8.74;8,134.77,310.28,298.29,8.74"><head></head><label></label><figDesc>Figures 4 (a) and (d) show the points distribution for the original detection and after filtering or rejecting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="8,137.38,523.54,340.61,7.89;8,136.16,439.72,82.99,62.24"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Segmentation of photographs: correct (top) and failed and rejected (bottom)</figDesc><graphic coords="8,136.16,439.72,82.99,62.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="9,165.12,275.04,285.12,7.89;9,136.16,206.33,95.40,55.33"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Automatically defined bounding boxes (on sample test images)</figDesc><graphic coords="9,136.16,206.33,95.40,55.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,134.77,165.34,345.82,182.56"><head>Table 3 .</head><label>3</label><figDesc>Normalized classification scores and estimated species rank for the first 12 runs. The best result per image type is highlighted in bold.</figDesc><table coords="10,387.69,165.34,55.41,7.86"><row><cell>Official scores</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="1,144.73,656.80,136.49,7.86"><p>http://www.plantnet-project.org/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. Part of this work was funded by the <rs type="funder">Agropolis</rs> foundation through the project Pl@ntNet (http://www.plantnet-project.org/)</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,291.81,306.90,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,300.32,291.81,77.11,7.86">What is an object?</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,394.57,291.81,26.62,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,301.83,337.64,7.86;12,151.52,312.79,253.31,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,344.89,301.83,131.91,7.86">Surf: Speeded up robust features</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,165.60,312.79,210.56,7.86">Computer Vision and Image Understanding (CVIU)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,322.80,337.63,7.86;12,151.52,333.76,256.95,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,297.56,322.80,183.03,7.86;12,151.52,333.76,57.81,7.86">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,216.39,333.76,163.41,7.86">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,343.77,337.64,7.86;12,151.52,354.73,265.37,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,332.90,343.77,147.70,7.86;12,151.52,354.73,96.80,7.86">Efficiently matching sets of features with random histograms</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,269.67,354.73,118.55,7.86">ACM Multimedia Conference</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,364.75,333.04,7.86" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<title level="m" coord="12,195.99,364.75,117.15,7.86">Notes on the opensurf library</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Bristol</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep.</note>
</biblStruct>

<biblStruct coords="12,142.96,374.76,337.63,7.86;12,151.52,385.72,329.07,7.86;12,151.52,396.68,62.26,7.86" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ferecatu</surname></persName>
		</author>
		<title level="m" coord="12,207.65,374.76,272.94,7.86;12,151.52,385.72,105.51,7.86">Image retrieval with active relevance feedback using both visual and keyword-based descriptors</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>University of Versailles St-Quentin-en-Yvelines</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="12,142.96,406.70,337.63,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,216.88,406.70,198.73,7.86">Another approach to polychotomous classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,421.88,406.70,39.61,7.86">Tech. rep</title>
		<imprint>
			<biblScope unit="issue">96</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,416.71,337.64,7.86;12,151.52,427.67,329.07,7.86;12,151.52,438.63,173.75,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,399.84,416.71,80.75,7.86;12,151.52,427.67,282.21,7.86">Participation of IN-RIA&amp; Pl@ntNet to ImageCLEF 2011 plant images classification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mouysset</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,455.89,427.67,24.70,7.86;12,151.52,438.63,140.35,7.86">CLEF (Notebook Papers/Labs/Workshop</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,448.64,337.64,7.86;12,151.52,459.60,322.22,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,265.00,448.64,215.59,7.86;12,151.52,459.60,208.40,7.86">Object-based queries using color points of interest. Content-Based Access of Image and Video Libraries</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Gouet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,366.80,459.60,78.28,7.86">IEEE Workshop on</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,469.62,336.35,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,240.45,469.62,140.41,7.86">Random maximum margin hashing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,402.17,469.62,48.12,7.86">CVPR 2011</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,479.63,337.98,7.86;12,151.52,490.59,284.76,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,187.52,479.63,199.13,7.86">New local descriptors based on dissociated dipoles</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,406.88,479.63,73.71,7.86;12,151.52,490.59,256.09,7.86">Proceedings of the 6th ACM international conference on Image and video retrieval</title>
		<meeting>the 6th ACM international conference on Image and video retrieval</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,500.61,337.97,7.86;12,151.52,511.57,212.01,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,240.73,500.61,200.49,7.86">A posteriori multi-probe locality sensitive hashing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,462.67,500.61,17.92,7.86;12,151.52,511.57,183.34,7.86">16th ACM international conference on Multimedia</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,521.58,337.97,7.86;12,151.52,532.54,329.07,7.86;12,151.52,543.50,185.81,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,310.48,521.58,170.11,7.86;12,151.52,532.54,212.67,7.86">Single-layer learning revisited: A stepwise procedure for building and training a neural network</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Knerr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Personnaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Dreyfus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,385.96,532.54,94.62,7.86;12,151.52,543.50,157.14,7.86">Neurocomputing: Algorithms, Architectures and Applications</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,553.51,337.98,7.86;12,151.52,564.47,329.07,7.86;12,151.52,575.43,229.83,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,343.84,553.51,136.76,7.86;12,151.52,564.47,188.39,7.86">Advanced shape context for plant species identification using leaf image retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mouine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Verroust-Blondet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,361.55,564.47,119.04,7.86;12,151.52,575.43,201.16,7.86">Proceedings of the 2nd ACM International Conference on Multimedia Retrieval</title>
		<meeting>the 2nd ACM International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,585.45,337.98,7.86;12,151.52,596.41,110.15,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,190.25,585.45,233.72,7.86">A Threshold Selection Method From Gray-Level Histogram</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,430.73,585.45,49.86,7.86;12,151.52,596.41,81.48,7.86">IEEE Trans. Syst., Man, Cybern</title>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,606.42,337.97,7.86;12,151.52,617.38,329.07,7.86;12,151.52,628.34,187.18,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,331.62,606.42,148.97,7.86;12,151.52,617.38,134.69,7.86">Interactive learning of heterogeneous visual concepts with local features</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ouertani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Crucianu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,306.03,617.38,174.56,7.86;12,151.52,628.34,158.51,7.86">MM &apos;10: Proceedings of the seventeen ACM international conference on Multimedia</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,638.35,337.97,7.86;12,151.52,649.31,329.07,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,313.68,638.35,166.90,7.86;12,151.52,649.31,40.41,7.86">Shape-based image retrieval in botanical collections</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Herve</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,211.29,649.31,241.35,7.86">Advances in Multimedia Information Processing -PCM 2006</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
