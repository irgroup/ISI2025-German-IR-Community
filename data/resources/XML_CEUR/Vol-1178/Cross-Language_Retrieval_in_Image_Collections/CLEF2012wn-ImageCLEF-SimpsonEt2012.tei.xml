<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,154.78,115.96,306.20,12.62;1,158.21,133.17,298.95,12.62">ITI&apos;s Participation in the ImageCLEF 2012 Medical Retrieval and Classification Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,160.76,170.64,88.40,8.74"><forename type="first">Matthew</forename><forename type="middle">S</forename><surname>Simpson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Lister Hill National Center for Biomedical Communications</orgName>
								<orgName type="institution" key="instit2">U. S. National Library of Medicine</orgName>
								<orgName type="institution" key="instit3">NIH</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,257.40,170.64,78.73,8.74"><roleName>Md</roleName><forename type="first">Daekeun</forename><surname>You</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Lister Hill National Center for Biomedical Communications</orgName>
								<orgName type="institution" key="instit2">U. S. National Library of Medicine</orgName>
								<orgName type="institution" key="instit3">NIH</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,339.45,170.64,85.30,8.74"><forename type="first">Mahmudur</forename><surname>Rahman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Lister Hill National Center for Biomedical Communications</orgName>
								<orgName type="institution" key="instit2">U. S. National Library of Medicine</orgName>
								<orgName type="institution" key="instit3">NIH</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,433.70,170.64,20.89,8.74;1,188.58,182.11,74.47,8.74"><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Lister Hill National Center for Biomedical Communications</orgName>
								<orgName type="institution" key="instit2">U. S. National Library of Medicine</orgName>
								<orgName type="institution" key="instit3">NIH</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,271.68,182.11,62.89,8.74"><forename type="first">Sameer</forename><surname>Antani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Lister Hill National Center for Biomedical Communications</orgName>
								<orgName type="institution" key="instit2">U. S. National Library of Medicine</orgName>
								<orgName type="institution" key="instit3">NIH</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,361.93,182.11,64.86,8.74"><forename type="first">George</forename><surname>Thoma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Lister Hill National Center for Biomedical Communications</orgName>
								<orgName type="institution" key="instit2">U. S. National Library of Medicine</orgName>
								<orgName type="institution" key="instit3">NIH</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,154.78,115.96,306.20,12.62;1,158.21,133.17,298.95,12.62">ITI&apos;s Participation in the ImageCLEF 2012 Medical Retrieval and Classification Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">232CB1FBE920789FA8EA51BA619266A8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Retrieval</term>
					<term>Case-based Retrieval</term>
					<term>Image Modality</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes the participation of the Image and Text Integration (ITI) group in the 2012 ImageCLEf medical retrieval and classification tasks. We present our methods for each of the three tasks and discuss our submitted textual, visual, and mixed runs as well as their results. While our methods generally perform well for each task, our best ad-hoc image retrieval submission was ranked first among all the submissions from the participating groups.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This article describes the participation of the Image and Text Integration (ITI) group in the ImageCLEF 2012 medical retrieval and classification tasks. Our group is from the Communications Engineering Branch of the Lister Hill National Center for Biomedical Communications, which is a division of the U. S. National Library of Medicine.</p><p>The medical track <ref type="bibr" coords="1,234.81,449.26,15.81,8.74" target="#b14">[15]</ref> of ImageCLEF 2012 consists of an image modality classification task and two retrieval tasks. For the classification task, the goal is to classify a given set of medical images according to thirty-one modalities (e.g., "Computerized Tomography," "Electron Microscopy," etc.). The modalities are organized hierarchically into meta-classes such as "Radiology" and "Microscopy," which are themselves types of "Diagnostic Images." In the first retrieval task, a set of ad-hoc information requests is given, and the goal is to retrieve the most relevant images from a collection of biomedical articles for each topic. Finally, in the second retrieval task, a set of case-based information requests is given, and the goal is to retrieve the most relevant articles describing similar cases.</p><p>In the following sections, we describe the textual and visual features that comprise our image and article representations (Sections 2-3) and our methods for the modality classification (Section 4) and medical retrieval tasks (Sections 5-6). Our textual approaches primarily utilize the Unified Medical Language System R (UMLS R ) <ref type="bibr" coords="1,221.54,610.21,15.30,8.74" target="#b10">[11]</ref> synonymy to identify concepts in topic descriptions and article text, and our visual approaches rely on computed distances between descriptors of various low-level visual features. In developing mixed approaches, we explore the use of clustered visual features that can be represented using text, attribute selection, and ranked list merging strategies.</p><p>In Section 7 we describe our submitted runs, and in Section 8 we present our results. For the modality classification task, our best submission achieved a classification accuracy of 63.2% and was ranked within the submissions from the top three participating groups. Our best submission for the ad-hoc image retrieval task was ranked first overall, achieving a mean average precision of 0.2377, which is a statistically significant improvement over the second ranked submission. For the case-based article retrieval task, our best submission achieved a mean average precision of 0.1035 and was ranked within the submissions from the top four participating groups, but this submission is statistically indistinguishable from our other case-based submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Image Representation for Ad-hoc Retrieval</head><p>We represent the images contained in biomedical articles using a combination of the textual and visual features described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Textual Features</head><p>We represent each image in the collection as a structured document of imagerelated text called an enriched citation. Our representation includes the title, abstract, and MeSH R terms<ref type="foot" coords="2,259.68,322.87,3.97,6.12" target="#foot_0">1</ref> of the article in which the image appears as well as the image's caption and "mentions" (snippets of text within the body of an article that discuss an image). These features can be indexed with a traditional text-based information retrieval system, or they may be exposed as term vectors and combined with the visual feature vectors described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual Features</head><p>In addition to the above textual features, we also represent the visual content of images using various low-level visual descriptors. Table <ref type="table" coords="2,401.30,411.79,5.08,8.74" target="#tab_0">1</ref> summarizes the descriptors we extract and their dimensionality. Due to the large number of these features, we forego describing them in any detail. However, they are all well-known and discussed extensively in existing literature.</p><p>Cluster Words. To avoid the computational complexity of computing distances between the above visual descriptors, we create a textual representation of visual features that is easily integrated with our existing textual features. For each visual descriptor listed in Table <ref type="table" coords="2,275.94,496.44,3.89,8.74" target="#tab_0">1</ref>, we cluster the vectors assigned to all images using the k-means++ <ref type="bibr" coords="2,229.62,507.91,10.31,8.74" target="#b2">[3]</ref> algorithm. We then assign each cluster a unique "cluster word" and represent each image as a sequence of these words. We add an image's cluster words to its enriched citation as a "global image feature" field, which can be searched using a traditional text-based information retrieval system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attribute</head><p>Selection. An orthogonal approach to transforming our visual descriptors into a computationally manageable representation is attribute selection. By eliminating unneeded or redundant information, these techniques can also improve our modality classification and image retrieval methods. We perform attribute selection using the WEKA <ref type="bibr" coords="2,299.06,604.04,10.73,8.74" target="#b7">[8]</ref> data mining software. First, we group all our visual descriptors into a single combined vector, and we then perform attribute selection to reduce the dimensionality of this combined feature. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Descriptor Dimensionality</head><p>Autocorrelation 25 Color and edge directivity (CEDD) <ref type="bibr" coords="3,311.37,170.34,9.73,7.86" target="#b4">[5]</ref> 144 Color layout (CLD) <ref type="bibr" coords="3,248.49,180.86,9.73,7.86" target="#b3">[4]</ref> 16 Color moment 3 Edge frequency 25 Edge histogram (EHD) <ref type="bibr" coords="3,262.49,212.42,9.73,7.86" target="#b3">[4]</ref> 80 Fuzzy color and texture histogram (FCTH) <ref type="bibr" coords="3,344.07,222.94,9.73,7.86" target="#b5">[6]</ref> 192 Gabor moment 60 Gray-level co-occurrence matrix moment (GLCM) <ref type="bibr" coords="3,367.12,243.98,14.34,7.86" target="#b18">[19]</ref> 20 Local binary pattern (LBP1) <ref type="bibr" coords="3,281.11,254.50,14.34,7.86" target="#b13">[14]</ref> 256 Local binary pattern (LBP2) <ref type="bibr" coords="3,281.11,265.02,14.34,7.86" target="#b13">[14]</ref> 256 Local color histogram (LCH) 1024 Primitive length 5 Scale-invariant feature transformation (SIFT) <ref type="bibr" coords="3,353.18,296.58,14.34,7.86" target="#b11">[12]</ref> 256 Semantic concept (SCONCEPT) <ref type="bibr" coords="3,296.89,307.10,14.34,7.86" target="#b15">[16]</ref> 30 Shape moment 5 Tamura moment <ref type="bibr" coords="3,235.69,328.15,14.34,7.86" target="#b19">[20]</ref> 18</p><p>Combined 2415</p><p>Feature computed using the Lucene Image Retrieval library <ref type="bibr" coords="3,388.71,360.77,13.52,7.86" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Article Representation for Case-based Retrieval</head><p>We represent articles using the textual features of each image appearing in the article. Thus, each article enriched citation consists of its title, abstract, and MeSH terms as well as the caption and mention of each contained image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Modality Classification Task</head><p>We experimented with both flat and hierarchical modality classification methods. Below we describe our flat classification strategy, an extension of this approach that exploits the hierarchical structure of the classes, and a post-processing method for improving the classification accuracy of illustrations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Flat Classification</head><p>Figure <ref type="figure" coords="3,166.68,522.82,10.13,8.74" target="#fig_0">1a</ref> provides an overview of our basic classification approach. We utilize multi-class support vector machines (SVMs) as our flat modality classifiers. First, we extract our visual and textual image features from the training images (representing the textual features as term vectors). Then, we perform attribute selection to reduce the dimensionality of the features. We construct the lowerdimensional vectors independently for each feature type (textual or visual) and combine the resulting attributes into a single, compound vector. Finally, we use the lower-dimensional feature vectors to train multi-class SVMs for producing textual, visual, or mixed modality predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hierarchical Classification</head><p>Unlike the flat classification strategy described above, it is possible to exploit the hierarchical organization of the modality classes in order to decompose the task into several smaller classification problems that can be sequentially applied. Based on our visual observation of the training samples and our initial experiments, we modified the original modality hierarchy <ref type="bibr" coords="4,308.40,318.79,15.19,8.74" target="#b14">[15]</ref> proposed for the task. The hierarchy we used for our experiments is shown in Figure <ref type="figure" coords="4,343.81,330.27,8.86,8.74" target="#fig_0">1b</ref>. We train flat multi-class SVMs, as shown in Figure <ref type="figure" coords="4,366.30,341.74,8.32,8.74" target="#fig_0">1a</ref>, for each meta-class. For recognizing compound images, we utilize the algorithm proposed by Apostilova et al. <ref type="bibr" coords="4,148.43,364.70,9.79,8.74" target="#b0">[1]</ref>, which detects sub-figure labels and the border of each sub-figure within a compound image. To arrive at a final class label, an image is sequentially classified beginning at the root of the hierarchy until a leaf class can be determined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Illustration Post-processing</head><p>Because our initial classification experiments resulted in only modest accuracy for the fourteen "Illustration" classes shown in Figure <ref type="figure" coords="4,381.51,428.87,9.03,8.74" target="#fig_0">1b</ref>, we concluded that our current textual and visual features may not be sufficient for representing these figures. Therefore, in addition to the aforementioned machine learning modality classification methods, we also developed several complimentary rulebased strategies for increasing the classification accuracy of "Illustration" classes.</p><p>A majority of the training samples contained in the "Illustration" meta-class, unlike other images in the collection, consist of line drawings or text superimposed on a white background. For example, program listings mostly consist of text; thus, the use of text and line detection methods may increase the classification accuracy of Class GPLI. Similarly, polygons (e.g., rectangles, hexagons, etc.) contained in flowcharts (GFLO), tables (GTAB), system overviews (GSYS), and chemical structures (GCHE) are a distinctive feature of these modalities. We utilize the methods of Jung et al. <ref type="bibr" coords="4,291.22,566.59,15.81,8.74" target="#b9">[10]</ref> and OpenCV<ref type="foot" coords="4,370.54,565.02,3.97,6.12" target="#foot_1">2</ref> functions to assess the presence of text and polygons, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ad-Hoc Image Retrieval Task</head><p>In this section we describe our textual, visual and mixed approaches to the ad-hoc image retrieval task. Descriptions of the submitted runs that utilize these methods are presented in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Textual Approaches</head><p>To allow for efficient retrieval and to compare their relative performance, we index our enriched citations with the Essie <ref type="bibr" coords="5,318.81,143.34,10.31,8.74" target="#b8">[9]</ref> and Lucene/SOLR<ref type="foot" coords="5,412.00,141.76,3.97,6.12" target="#foot_2">3</ref> search engines. Essie is a search engine developed by the U.S. National Library of Medicine and is particularly well-suited for the medical retrieval task due to its ability to automatically expand query terms using the UMLS synonymy. Lucene/SOLR is a popular search engine developed by the Apache Software Foundation that employs the well-known vector space model of information retrieval. We have extracted the UMLS synonymy from Essie and use it for term expansion when indexing enriched citations with Lucene/SOLR.</p><p>We organize each topic description into a frame-based (e.g., PICO<ref type="foot" coords="5,444.46,234.39,3.97,6.12" target="#foot_3">4</ref> ) representation following the method similar to that described by Demner-Fushman and Lin <ref type="bibr" coords="5,171.71,258.92,9.88,8.74" target="#b6">[7]</ref>. Extractors identify concepts related to problems, interventions, age, anatomy, drugs, and modality. We also identify modifiers of the extracted concepts and a limited number of relationships among them. We then transform the extracted concepts into queries appropriate for either Essie or Lucene/SOLR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Visual Approaches</head><p>Our visual approaches to image retrieval are based on retrieving images that appear visually similar to the given topic images. We compute the visual similarity between two images as the Euclidean distance between their visual descriptors. For the purposes of computing this distance, we represent each image as a combined feature vector composed of a subset of the visual descriptors listed in Table <ref type="table" coords="5,161.80,383.89,4.98,8.74" target="#tab_0">1</ref> after attribute selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Mixed Approaches</head><p>We explore several methods of combing our textual and visual approaches. One such approach involves the use of our image cluster words. For performing multimodal retrieval using cluster words, we first extract the visual descriptors listed in Table <ref type="table" coords="5,200.79,451.47,5.02,8.74" target="#tab_0">1</ref> from each example image of a given topic. We then locate the clusters to which the extracted descriptors are nearest in order to determine their corresponding cluster words. Finally, we combine these cluster words with words taken from the topic description to form a multimodal query appropriate for either Essie or Lucene/SOLR.</p><p>While the use of cluster words allows us to create multimodal queries, we can instead directly combine the independent outputs of our textual and visual approaches. In a score merging approach, we apply a min-max normalization to the ranked lists of scores produced by our textual and visual retrieval strategies. We then linearly combine the normalized scores given to each image to produce a final ranking. Similarly, a rank merging approach combines the results of our textual and visual approaches using the ranks of the retrieved images instead of their normalized scores. To produce the final image ranking using this strategy, we re-score each retrieved image as the reciprocal of its rank and then repeat the above procedure for combining scores.</p><p>Another means of incorporating visual information with our retrieval approaches is through the use of a modality classifier. Using our hierarchical modality classification approach, we can first determine the most probable modalities for a topic's example images. After retrieving a set of images using either our textual or visual methods, we can eliminate retrieved images that are not of the same modality as the topic images. An advantage of performing hierarchical classification is that we can filter the retrieved results using the meta-classes within the hierarchy (e.g., "Radiology").</p><p>Finally, we often combine the retrieval results produced by several queries into a single ranked list of images. We perform this query combination, or padding, by simply appending the ranked list of images retrieved by a subsequent query to the end of the ranked list produced by the preceding query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Case-Based Retrieval Task</head><p>Our method for performing case-based retrieval is analogous to our textual approaches for ad-hoc image retrieval. Here, we index the enriched citations described in Section 3 using the Essie and Lucene/SOLR search engines (for performance comparison). We generate textual and mixed queries appropriate for both search engines according to the approaches described in Sections 5.1.</p><p>As a form of query expansion for case-based topics, we also explore the possibility of determining relevant disease names to correspond with signs and symptoms found in a topic case. To determine a set of potential diseases, we first use the Google Search API<ref type="foot" coords="6,249.95,377.04,3.97,6.12" target="#foot_4">5</ref> to search the World Wide Web using a topic case as a query. We then process the top five documents with MetaMap <ref type="bibr" coords="6,423.14,390.09,10.72,8.74" target="#b1">[2]</ref> to extract terms having the UMLS semantic type "Disease or Syndrome." Finally, we select the top three most frequent diseases for query expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Submitted Runs</head><p>In this section we describe each of our submitted runs for the modality classification, ad-hoc image retrieval, and case-based article retrieval tasks. Each run is identified by its submission file name or trec_eval run ID and mode (textual, visual or mixed). All submitted runs are automatic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Modality Classification Task</head><p>We submitted the following nine runs for the modality classification task:</p><p>M1 all the images contained in the retrieval collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Ad-hoc Image Retrieval Task</head><p>We submitted the following ten runs for the ad-hoc image retrieval task: A1. nlm-se (mixed): A combination of three queries using Essie. (A1.Q1) A disjunction of modality terms extracted from the query topic must occur within the caption or mention fields of an image's enriched citation; a disjunction of the remaining terms is allowed to occur in any field. (A1.Q2) A lossy expansion of the verbatim topic is allowed to occur in any field. (A1.Q3) A disjunction of the query images' cluster words must occur within the global image feature field. A2. nlm-se-cw-mf (mixed): A combination of Query A1.Q1 with the additional query below using Essie. (A2.Q2) A lossy expansion of the verbatim topic is allowed occur in any field of an image's enriched citation and a disjunction of the query images' cluster words can optionally occur within the global image feature field. Additionally, the retrieved images are filtered so that they share a least common ancestor modality with the query images, as determined by the modality classifier used in Run M9. Query A2.Q2 is distinct from Queries 1.Q2-3 in that the occurrence of a lossy expansion of the topic is not necessarily weighted more heavily than the occurrence of image cluster words. A3. nlm-se-scw-mf (mixed): Like Run A2 but image cluster words are only considered if the modality classifier used in Run M9 identically labels all the example images of a topic. A4. nlm-lc (mixed): A combination of three queries using Lucene with BM25 similarity and UMLS synonymy. (A4.Q1) A fuzzy phrase-based occurrence of the verbatim topic is allowed in any field of an image's enriched citation. (A4.Q2) A disjunction of the topic words is allowed to occur in any field. (A4.Q3) A disjunction of the query images' cluster words must occur within the global image feature field. A5. nlm-lc-cw-mf (mixed): A combination of Query A4.Q1 with the additional query below using Lucene with BM25 similarity and UMLS synonymy. (A5.Q2) A disjunction of the topic words is allowed occur in any field of an image's enriched citation and a disjunction of the query images' cluster words can optionally occur within the global image feature field. Additionally, the retrieved images are filtered so that they share a least common ancestor modality with the query images, as determined by the modality classifier used in Run M9.</p><p>A6. nlm-lc-scw-mf (mixed): Like Run A5 but image cluster words are only considered if the modality classifier used in Run M9 identically labels all the example images of a topic. A7. Combined Selected Fileterd Merge (visual): Similarity matching using 62 min-max normalized attributes selected from a combined visual descriptor of 15 features (all descriptors in Table <ref type="table" coords="8,334.30,176.38,5.08,8.74" target="#tab_0">1</ref> except LCH and SCONCEPT).</p><p>Retrieval is performed separately for each query image, and the retrieved results are filtered, according to the modality classifier used in Run M9, so that they share the top two modality levels with the query. Images are scored according the query image resulting in the maximum score. A8. Combined LateFusion Fileterd Merge (visual): Like Run A8 but similarity matching is performed separately for seven features (CLD, GLCM, SCON-CEPT, and the color, Gabor, shape, and Tamura moments from Table <ref type="table" coords="8,472.94,256.72,4.41,8.74" target="#tab_0">1</ref>) whose scores are linearly combined with predefined weights. A9. Txt Img Wighted Merge (mixed): A combination of visual Run A7 with a textual run consisting solely of Query A1.Q2 using score merging. A10. Merge RankToScore weighted (mixed): A combination of visual Run A8 with a textual run consisting solely of Query A1.Q2 using rank mering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Case-based Article Retrieval Task</head><p>We submitted the following eight runs for the case-based article retrieval task: C1. nlm-se-max (textual): A combination of three queries for each topic sentence using Essie. (C1.Q1) A disjunction of modality terms extracted from the sentence must occur within the caption or mention fields of an article's enriched citation; a disjunction of the remaining terms is allowed to occur in any field. (C1.Q2) A lossy expansion of the verbatim sentence is allowed to occur in any field. (C1.Q3) A disjunction of all extracted words and discovered diseases in the sentence is allowed to occur in any field. Articles are scored according to the sentence resulting in the maximum score. C2. nlm-se-sum (textual): Like Run C1 but articles are scored according to the sum of the scores produced for each sentence. C3. nlm-se-frames-max (textual): A combination of the query below with Query C1.Q2 for each topic sentence using Essie. (C3.Q1) An expansion of the frame-based representation of the sentence is allowed to occur in any field of an article's enriched citation. Articles are scored according to the sentence resulting in the maximum score. C4. nlm-se-frames-sum (textual): Like Run C3 but articles are scored according to the sum of the scores produced for each sentence. C5. nlm-lc-max (textual): A combination of two queries for each topic sentence using Lucene with language model similarity, Jelinek-Mercer smoothing, and UMLS synonymy. (C5.Q1) A fuzzy phrase-based occurrence of the verbatim sentence is allowed in any field of an article's enriched citation. (C5.Q2) A disjunction of all words and discovered diseases in the sentence is allowed to occur in any field. Articles are scored according to the sentence resulting in the maximum score. C6. nlm-lc-sum (textual): Like Run C5 but articles are scored according to the sum of the scores produced for each sentence. C7. nlm-lc-total-max (textual): A combination of the query below with Queries C5.Q1-2 (as C7.Q2-3) using Lucene with language model similarity, Jelinek-Mercer smoothing, and UMLS synonymy. (C7.Q1) A fuzzy phrase-based occurrence of the entire verbatim topic is allowed in any field of an article's enriched citation. Articles are scored according to the sentence resulting in the maximum score. C8. nlm-lc-total-sum (textual): Like Run C7 but articles are scored according to the sum of the scores produced for each sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Results</head><p>We present and discuss the results of our modality classification, ad-hoc image retrieval, and case-based article retrieval task submissions below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Modality Classification Task</head><p>Table <ref type="table" coords="9,160.70,426.58,4.88,8.74" target="#tab_2">2</ref> presents the classification accuracy of our submitted runs for the modality classification task. Visual Text Hierarchy w Postprocessing 4 Illustration.txt, a mixed approach, achieved the highest accuracy (63.2%) of our submitted runs and was ranked fifth overall. However, it ranked within the submissions from the top three participating groups. This result validates our post-processing method used to improve the recognition of "Illustration" classes, and provides, with our previous experience <ref type="bibr" coords="9,226.53,495.44,14.90,8.74" target="#b16">[17]</ref>, further evidence that hierarchical classification is a successful strategy. Each of our hierarchical classification methods outperforms the corresponding flat approach having the same feature representation. While our submitted runs were only judged on their ability to identify each of the thirty-one modality classes <ref type="bibr" coords="9,271.86,541.35,14.60,8.74" target="#b14">[15]</ref>, Table <ref type="table" coords="9,320.83,541.35,4.98,8.74" target="#tab_3">3</ref> presents the classification accuracy of the intermediate classifiers we used for our hierarchical approaches. For each meta-class in the hierarchy shown in Figure <ref type="figure" coords="9,336.33,564.30,9.03,8.74" target="#fig_0">1b</ref>, Table <ref type="table" coords="9,382.33,564.30,5.08,8.74" target="#tab_3">3</ref> gives the number of classes they contain; the classification accuracy associated with the textual, visual, and mixed feature representations; and the dimensionality of the mixed feature representation after attribute selection. These results demonstrate that the accuracies of the intermediate classifiers generally improve as the number of class labels decreases. Given the limited amount of training data in relation to the number of total modalities, the smaller number of labels per classifier likely is significant for explaining why our hierarchical classification approaches consistently outperform their corresponding flat approaches.   <ref type="table" coords="10,162.01,434.38,5.02,8.74" target="#tab_4">4</ref> presents the mean average precision (MAP), binary preference (bpref), and early precision (P@10) of our submitted runs for the ad-hoc image retrieval task. nlm-se achieved the highest MAP (0.2377) among our submitted runs and was ranked first overall. Merge RankToScore weighted, the run achieving our second highest MAP (0.2166), was ranked second overall. Comparing these two runs using Fisher's paired randomization test <ref type="bibr" coords="10,341.78,491.76,14.90,8.74" target="#b17">[18]</ref>, a recommended statistical test for evaluating information retrieval systems, we find that nlm-se achieved a statistically significant increase (9.7%, p = 0.0016) over the performance of Merge RankToScore weighted.</p><p>That the two highest ranked runs were multimodal, as apposed to textual, is an encouraging result, and provides evidence that our ongoing efforts at integrating textual and visual information will be successful. In particular, the use by nlm-se and other runs of cluster words, which are indexed and retrieved using a traditional text-based information retrieval system, is an effective way, not only of incorporating visual information with text, but of avoiding the computational expense common among content-based retrieval methods. Furthermore, Merge RankToScore weighted demonstrates the value of rank merging when combining textual and visual retrieval results. Some of our other mixed runs, in utilizing the results of our modality classifiers, may have been weakened due to the modest performance of our classification methods.  <ref type="table" coords="11,161.81,270.92,4.98,8.74" target="#tab_5">5</ref> presents the MAP, bpref, and P@10 of our submitted runs for the casebased article retrieval task. nlm-lc-total-sum, a textual approach using language model similarity, achieved the highest MAP (0.1035) among our submitted runs and was ranked seventh overall. However, it ranked within the submissions from the top four participating groups. Using Fisher's paired randomization test, we find that there is no statistically significant difference (p &lt; 0.05) in MAP among any of our submitted runs. The relatively low performance of most of the ImageCLEF 2012 case-based submissions may be due, in part, to the existence in the collection of only a small number of case reports, clinical trials, or other types of documents relevant for case-based topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>This article describes the methods and results of the Image and Text Integration (ITI) group in the ImageCLEF 2012 medical retrieval and classification tasks.</p><p>For the modality classification task, our best submission was ranked within the submissions from the top three participating groups. Our best submission for the ad-hoc image retrieval task was ranked first overall. Finally, for the case-based article retrieval task, our best submission was ranked within the submissions from the top four participating groups, though we found no statistical significance between this run and our other case-based submissions. The effectiveness of our multimodal approaches are encouraging and provide evidence that our ongoing efforts at integrating textual and visual information will be successful.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,425.43,172.50,8.59,3.03;4,422.81,178.52,8.07,2.53;4,423.03,183.84,7.61,2.53;4,422.88,189.15,7.91,2.53"><head>Fig. 1 :</head><label>1</label><figDesc>Photo DVDM DVEN DVOR</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,223.05,127.00,168.86,8.77"><head>Table 1 :</head><label>1</label><figDesc>Extracted visual descriptors.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,136.16,127.00,343.04,124.85"><head>Table 2 :</head><label>2</label><figDesc>Accuracy results for the modality classification task.</figDesc><table coords="9,136.16,144.27,343.04,107.58"><row><cell>File Name</cell><cell cols="2">Mode Accuracy (%)</cell></row><row><cell>Visual Text Hierarchy w Postprocessing 4 Illustration.txt</cell><cell>Mixed</cell><cell>63.2</cell></row><row><cell>Visual Text Flat w Postprocessing 4 Illustration.txt</cell><cell>Mixed</cell><cell>61.7</cell></row><row><cell>Visual Text Hierarchy.txt</cell><cell>Mixed</cell><cell>60.1</cell></row><row><cell>Visual Text Flat.txt</cell><cell>Mixed</cell><cell>59.1</cell></row><row><cell>Visual only Hierarchy.txt</cell><cell>Visual</cell><cell>51.6</cell></row><row><cell>Visual only Flat.txt</cell><cell>Visual</cell><cell>50.3</cell></row><row><cell>Image Text Hierarchy Entire set.txt</cell><cell>Mixed</cell><cell>44.2</cell></row><row><cell>Text only Hierarchy.txt</cell><cell>Textual</cell><cell>41.3</cell></row><row><cell>Text only Flat.txt</cell><cell>Textual</cell><cell>39.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,139.75,127.00,335.87,125.73"><head>Table 3 :</head><label>3</label><figDesc>Accuracy results for our intermediate modality classifiers.</figDesc><table coords="10,139.75,144.27,335.87,108.46"><row><cell cols="2">ID Number of Classes</cell><cell cols="4">Feature Accuracy (%) Dims.</cell></row><row><cell></cell><cell></cell><cell cols="3">Mixed Visual Textual</cell><cell></cell></row><row><cell>1</cell><cell>2 (Illustration, General)</cell><cell>96.3</cell><cell>95.6</cell><cell>78.6</cell><cell>159</cell></row><row><cell>2</cell><cell>3 (Radiology 3D, Microscopy, Photo)</cell><cell>93.5</cell><cell>87.4</cell><cell>83.8</cell><cell>112</cell></row><row><cell>3</cell><cell>8 (DRUS, DRMR, . . . , D3DR)</cell><cell>75.9</cell><cell>64.4</cell><cell>71.3</cell><cell>89</cell></row><row><cell>4</cell><cell>4 (DMLI, DMEL, . . . , DMFL)</cell><cell>85.0</cell><cell>83.6</cell><cell>69.4</cell><cell>58</cell></row><row><cell>5</cell><cell>4 (DVDM, DVEN, . . . , GNCP)</cell><cell>77.6</cell><cell>62.3</cell><cell>89.1</cell><cell>108</cell></row><row><cell cols="2">6 14 (GTAB, GPLI, . . . , DSEM)</cell><cell>63.5</cell><cell>53.0</cell><cell>41.2</cell><cell>69</cell></row><row><cell cols="4">Feature dimensionality is given for mixed mode classifiers only.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,134.41,267.45,338.22,175.66"><head>Table 4 :</head><label>4</label><figDesc>Retrieval results for the ad-hoc image retrieval task</figDesc><table coords="10,134.41,284.72,338.22,158.39"><row><cell>ID</cell><cell>Mode</cell><cell>MAP</cell><cell>bpref</cell><cell>P@10</cell></row><row><cell>nlm-se</cell><cell>Mixed</cell><cell>0.2377</cell><cell>0.2542</cell><cell>0.3682</cell></row><row><cell>Merge RankToScore weighted</cell><cell>Mixed</cell><cell>0.2166</cell><cell>0.2198</cell><cell>0.3682</cell></row><row><cell>nlm-lc</cell><cell>Mixed</cell><cell>0.1941</cell><cell>0.1871</cell><cell>0.2727</cell></row><row><cell>nlm-lc-cw-mf</cell><cell>Mixed</cell><cell>0.1938</cell><cell>0.1924</cell><cell>0.2636</cell></row><row><cell>nlm-lc-scw-mf</cell><cell>Mixed</cell><cell>0.1927</cell><cell>0.1940</cell><cell>0.2636</cell></row><row><cell>nlm-se-scw-mf</cell><cell>Mixed</cell><cell>0.1914</cell><cell>0.2062</cell><cell>0.2864</cell></row><row><cell>Txt Img Wighted Merge</cell><cell>Mixed</cell><cell>0.1846</cell><cell>0.2039</cell><cell>0.3091</cell></row><row><cell>nlm-se-cw-mf</cell><cell>Mixed</cell><cell>0.1774</cell><cell>0.1868</cell><cell>0.2909</cell></row><row><cell>Combined LateFusion Fileterd Merge</cell><cell>Visual</cell><cell>0.0046</cell><cell>0.0107</cell><cell>0.0318</cell></row><row><cell>Combined Selected Fileterd Merge</cell><cell>Visual</cell><cell>0.0009</cell><cell>0.0028</cell><cell>0.0227</cell></row><row><cell>8.2 Ad-hoc Image Retrieval Task</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="11,134.41,127.00,338.22,152.66"><head>Table 5 :</head><label>5</label><figDesc>Retrieval results for the case-based article retrieval task</figDesc><table coords="11,134.41,144.27,338.22,135.39"><row><cell>ID</cell><cell>Mode</cell><cell>MAP</cell><cell>bpref</cell><cell>P@10</cell></row><row><cell>nlm-lc-total-sum</cell><cell>Textual</cell><cell>0.1035</cell><cell>0.1053</cell><cell>0.1000</cell></row><row><cell>nlm-lc-total-max</cell><cell>Textual</cell><cell>0.1027</cell><cell>0.1055</cell><cell>0.0923</cell></row><row><cell>nlm-se-sum</cell><cell>Textual</cell><cell>0.0929</cell><cell>0.0738</cell><cell>0.0769</cell></row><row><cell>nlm-se-max</cell><cell>Textual</cell><cell>0.0914</cell><cell>0.0736</cell><cell>0.0769</cell></row><row><cell>nlm-lc-sum</cell><cell>Textual</cell><cell>0.0909</cell><cell>0.0933</cell><cell>0.1231</cell></row><row><cell>nlm-lc-max</cell><cell>Textual</cell><cell>0.0840</cell><cell>0.0886</cell><cell>0.0923</cell></row><row><cell>nlm-se-frames-sum</cell><cell>Textual</cell><cell>0.0771</cell><cell>0.0693</cell><cell>0.0692</cell></row><row><cell>nlm-se-frames-max</cell><cell>Textual</cell><cell>0.0672</cell><cell>0.0574</cell><cell>0.0538</cell></row><row><cell cols="2">8.3 Case-based Article Retrieval Task</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,646.28,335.87,7.86;2,144.73,656.80,102.22,7.86"><p>MeSH is a controlled vocabulary created by U. S. National Library of Medicine to index biomedical articles.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,144.73,657.44,169.46,7.47"><p>http://opencv.willowgarage.com/wiki/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,144.73,636.10,117.68,7.47"><p>http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="5,144.73,646.28,335.87,7.86;5,144.73,656.80,330.63,7.86"><p>PICO is a mnemonic for structuring clinical questions in evidence-based practice and represents Patient/Population/Problem, Intervention, Comparison, and Outcome.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="6,144.73,657.44,259.40,7.47"><p>https://developers.google.com/custom-search/v1/overview</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. We would like to thank <rs type="person">Antonio Jimeno-Yepes</rs> for assisting in expanding case-based topics with disease names, <rs type="person">Russell Loane</rs> for proving source code for converting frame-based topics to Essie queries, and <rs type="person">Srinivas Phadnis</rs> for constructing enriched citations and extracting visual features.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,625.24,338.92,7.86;11,151.52,635.76,329.07,7.86;11,151.52,646.28,329.07,7.86;11,151.52,656.80,146.72,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,151.52,635.76,329.07,7.86;11,151.52,646.28,123.80,7.86">Image retrieval from scientific publications: Text and image content processing to separate multi-panel figures</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Apostolova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Thoma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,282.07,646.28,198.53,7.86;11,151.52,656.80,95.49,7.86">Journal of the American Society for Information Science and Technology</title>
		<imprint/>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct coords="12,142.96,119.67,338.92,7.86;12,151.19,130.19,329.40,7.86;12,151.52,140.72,203.93,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,214.77,119.67,267.10,7.86;12,151.19,130.19,93.91,7.86">Effective mapping of biomedical text to the UMLS metathesaurus: The MetaMap program</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Aronson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,267.01,130.19,213.58,7.86;12,151.52,140.72,128.23,7.86">Proc. of the Annual Symp. of the American Medical Informatics Association (AMIA)</title>
		<meeting>of the Annual Symp. of the American Medical Informatics Association (AMIA)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,150.73,339.18,7.86;12,151.52,161.25,330.86,7.86;12,151.52,171.77,132.47,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,263.03,150.73,181.91,7.86">k-means++: The advantages of careful seeding</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,464.93,150.73,17.20,7.86;12,151.52,161.25,326.64,7.86">Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">07</biblScope>
			<biblScope unit="page" from="1027" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,181.79,337.64,7.86;12,151.19,192.31,330.48,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,300.61,181.79,148.49,7.86">Overview of the MPEG-7 standard</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Puri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,458.01,181.79,22.58,7.86;12,151.19,192.31,240.23,7.86">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="688" to="695" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,202.32,338.92,7.86;12,151.18,212.84,330.69,7.86;12,151.19,223.36,329.59,7.86;12,151.18,233.88,329.60,7.86;12,150.44,244.40,25.60,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,301.35,202.32,180.53,7.86;12,151.18,212.84,207.10,7.86">CEDD: Color and edge directivity descriptor: A compact descriptor for image indexing and retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,232.45,223.36,248.33,7.86;12,151.18,233.88,57.35,7.86">Proceedings of the 6th International Conference on Computer Vision Systems</title>
		<title level="s" coord="12,215.15,233.88,135.77,7.86">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Gasteratos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vincze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</editor>
		<meeting>the 6th International Conference on Computer Vision Systems</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5008</biblScope>
			<biblScope unit="page" from="312" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,254.42,337.98,7.86;12,151.52,264.94,329.07,7.86;12,151.05,275.46,330.62,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,301.08,254.42,179.86,7.86;12,151.52,264.94,170.14,7.86">FCTH: Fuzzy color and texture histogram: A low level feature for accurate image retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,340.44,264.94,140.15,7.86;12,151.05,275.46,250.69,7.86">Proceedings of the 9th International Workshop on Image Analysis for Multimedia Interactive Services</title>
		<meeting>the 9th International Workshop on Image Analysis for Multimedia Interactive Services</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="191" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,285.48,337.64,7.86;12,151.52,296.00,320.13,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,275.68,285.48,204.92,7.86;12,151.52,296.00,100.54,7.86">Answering clinical questions with knowledge-based and statistical techniques</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,259.19,296.00,106.72,7.86">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="103" />
			<date type="published" when="2007-03">Mar 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,306.01,337.64,7.86;12,151.05,316.53,315.83,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,464.89,306.01,15.70,7.86;12,151.05,316.53,164.54,7.86">The WEKA data mining software: An update</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,323.02,316.53,91.13,7.86">SIGKDD Explorations</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,326.55,339.17,7.86;12,151.52,337.07,329.07,7.86;12,151.18,347.59,132.19,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,343.64,326.55,138.49,7.86;12,151.52,337.07,139.78,7.86">Essie: A concept-based search engine for structured biomedical text</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">C</forename><surname>Ide</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F</forename><surname>Loane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,298.14,337.07,182.45,7.86;12,151.18,347.59,46.17,7.86">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="253" to="263" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,357.60,339.25,7.86;12,151.18,368.12,212.82,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,283.87,357.60,198.00,7.86;12,151.18,368.12,33.74,7.86">Text information extraction in images and video: A survey</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,191.96,368.12,81.43,7.86">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="977" to="997" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,378.14,339.77,7.86;12,151.52,388.66,236.61,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,329.16,378.14,148.87,7.86">The unified medical language system</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lindberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mccray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,151.52,388.66,145.99,7.86">Methods of Information in Medicine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="281" to="291" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,398.67,337.97,7.86;12,151.52,409.19,330.86,7.86;12,151.06,419.71,70.14,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,193.69,398.67,216.64,7.86">Object recognition from local scale-invariant features</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,431.56,398.67,49.03,7.86;12,151.52,409.19,280.25,7.86">Proceedings of the Seventh IEEE International Conference on Computer Vision</title>
		<meeting>the Seventh IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,429.73,337.98,7.86;12,151.52,440.25,330.61,7.86;12,151.52,450.77,115.70,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,283.00,429.73,197.60,7.86;12,151.52,440.25,50.55,7.86">LIRe: Lucene image retrival-an extensible java CBIR library</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,222.53,440.25,259.60,7.86;12,151.52,450.77,22.19,7.86">Proceedings of the 16th ACM International Conference on Multimedia</title>
		<meeting>the 16th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1085" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,460.79,337.97,7.86;12,151.52,471.31,232.41,7.86" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="12,206.51,460.79,274.08,7.86;12,151.52,471.31,66.87,7.86">The Local Binary Pattern Approach to Texture Analysis-Extensions and Applications</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Menp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>University of Oulu</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="12,142.62,481.32,339.52,7.86;12,151.52,491.84,329.07,7.86;12,151.52,502.36,231.81,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,226.23,491.84,254.36,7.86;12,151.52,502.36,72.92,7.86">Overview of the ImageCLEF 2012 medical image retrieval and classification tasks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,245.42,502.36,109.24,7.86">CLEF 2012 Working Notes</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,512.38,337.98,7.86;12,151.52,522.90,329.07,7.86;12,151.52,533.42,309.48,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,318.03,512.38,162.56,7.86;12,151.52,522.90,205.20,7.86">A medical image retrieval framework in correlation enhanced visual concept feature space</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Thoma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,379.67,522.90,100.92,7.86;12,151.52,533.42,280.82,7.86">Proceedings of the 22nd IEEE International Symposium on Computer-Based Medical Systems</title>
		<meeting>the 22nd IEEE International Symposium on Computer-Based Medical Systems</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,543.43,339.26,7.86;12,151.18,553.95,329.75,7.86;12,151.52,564.47,330.15,7.86" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="12,251.51,553.95,229.43,7.86;12,151.52,564.47,301.51,7.86">Text-and content-based approaches to image modality classification and retrieval for the ImageCLEF 2011 medical retrieval track</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Phadmis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Apostolova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Thoma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,574.49,337.97,7.86;12,151.52,585.01,329.07,7.86;12,151.52,595.53,309.80,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="12,321.57,574.49,159.02,7.86;12,151.52,585.01,164.69,7.86">A comparison of statistical significance tests for information retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,337.69,585.01,142.90,7.86;12,151.52,595.53,225.00,7.86">Proceedings of the Sixteenth ACM Conference on Information and Knowledge Management</title>
		<meeting>the Sixteenth ACM Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="623" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,605.55,337.98,7.86;12,151.18,616.07,308.88,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,257.72,605.55,109.90,7.86">Statistical texture analysis</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">N</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="12,390.80,605.55,89.79,7.86;12,151.18,616.07,197.09,7.86">Proceedings of World Academy of Science, Engineering and Technology</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1264" to="1269" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,626.08,337.98,7.86;12,151.52,636.60,329.30,7.86;12,150.44,647.12,25.60,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,309.41,626.08,171.18,7.86;12,151.52,636.60,41.56,7.86">Textural features corresponding to visual perception</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yamawaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,200.64,636.60,225.85,7.86">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="460" to="473" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
