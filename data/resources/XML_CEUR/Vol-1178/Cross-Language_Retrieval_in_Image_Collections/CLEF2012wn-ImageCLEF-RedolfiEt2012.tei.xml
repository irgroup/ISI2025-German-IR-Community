<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.29,115.89,324.78,12.90;1,234.35,133.82,146.66,12.90">Leveraging robust signatures for mobile robot semantic localization</title>
				<funder>
					<orgName type="full">CONICET, Argentina</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,227.99,171.44,60.07,9.96"><forename type="first">Javier</forename><surname>Redolfi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centro de Investigación en Informática para la Ingeniería</orgName>
								<orgName type="institution" key="instit1">Universidad Tecnológica Nacional</orgName>
								<orgName type="institution" key="instit2">Facultad Regional Córdoba</orgName>
								<address>
									<postCode>X5016ZAA</postCode>
									<settlement>Córdoba</settlement>
									<region>Argentine</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,315.23,171.44,61.31,9.96"><forename type="first">Jorge</forename><surname>Sánchez</surname></persName>
							<email>jsanchez@scdt.frc.utn.edu.ar</email>
							<affiliation key="aff0">
								<orgName type="department">Centro de Investigación en Informática para la Ingeniería</orgName>
								<orgName type="institution" key="instit1">Universidad Tecnológica Nacional</orgName>
								<orgName type="institution" key="instit2">Facultad Regional Córdoba</orgName>
								<address>
									<postCode>X5016ZAA</postCode>
									<settlement>Córdoba</settlement>
									<region>Argentine</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">CIEM-CONICET</orgName>
								<orgName type="department" key="dep2">FaMAF</orgName>
								<orgName type="institution">Universidad Nacional de Córdoba</orgName>
								<address>
									<addrLine>X5000HUA</addrLine>
									<settlement>Córdoba</settlement>
									<region>Argentine</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.29,115.89,324.78,12.90;1,234.35,133.82,146.66,12.90">Leveraging robust signatures for mobile robot semantic localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1E5ED6F9365B1BE2BF2ACB2F4C362107</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fisher vectors</term>
					<term>place recognition</term>
					<term>semantic localization</term>
					<term>temporal segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the CIII UTN FRC team in the ImageCLEF 2012 Robot Vision Challenge. The challenge was focused on the problem of visual place classification in indoor environments. During the competition, participants were asked to classify images according to the room in which they were acquired, using the information provided by RGB and depth images only. We based our approach on the Fisher Vector representation -a robust signature recently proposed in the literature-and the use of efficient linear classifiers. In order to exploit the information provided by different information channels, we adopted a simple fusion strategy and generated classification scores for each image in the sequence. Two tasks were proposed during the competition: in the first, images had to be classified independently of one another while, in the second, it was possible to exploit the temporal continuity of the stream. For the first task, we adopted a simple threshold based classification scheme. For the second, we considered the classification of groups of images instead of single frames. These groups, i.e. temporal segments, were automatically generated based on the visual similarity of the images in the sequence. Our team ranked first on both tasks, showing the effectiveness of the proposed schemes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the 2012 edition of the ImageCLEF Robot Vision Challenge, participant were asked to classify functional areas based on sequences of images acquired by a mobile robot within an office environment, either in a frame-by-frame basis (obligatory task) or by exploiting the temporal continuity of the image stream (optional task). For learning the classifiers, the organizers provided training sequences consisting on RGB and depth images acquired under different lighting conditions.</p><p>Fig. <ref type="figure" coords="2,152.05,176.12,3.58,8.97">1</ref>: Sample images from the "training2" sequence of the ImageCLEF Robot Vision Challenge 2012 dataset. All images belong to the "ProfessorOffice" class. Note the great amount of variability in the visual appearance within this group of images. Also, the last two seems more closely related to the "Corridor" class instead of that provided as ground truth.</p><p>This paper describes the participation of the CIII UTN FRC team in both tasks. Our methods leverage recent advances in the fields of image classification and retrieval, in which robust and efficient representations have been devised. Particularly, we consider the state-of-the-art Fisher Vector (FV) representation <ref type="bibr" coords="2,134.77,300.76,10.95,9.96" target="#b5">[6,</ref><ref type="bibr" coords="2,145.72,300.76,7.30,9.96" target="#b7">8]</ref> which has been recently shown to give excellent results in a wide range of problems <ref type="bibr" coords="2,177.46,312.71,10.79,9.96" target="#b7">[8,</ref><ref type="bibr" coords="2,188.25,312.71,7.19,9.96" target="#b2">3,</ref><ref type="bibr" coords="2,195.44,312.71,7.19,9.96" target="#b1">2]</ref>.</p><p>Before introducing the core components in our system, we first highlight some of the differences between the problem of visual place classification (VPC) in robotics and the more general problem of automatic image annotation (AIA) <ref type="foot" coords="2,473.35,347.20,3.97,6.97" target="#foot_0">3</ref> , i.e. the problem of assigning labels to images based on its content. First, the labeling of images in VPC is based on the physical location of the robot instead of a visually well defined concept. This makes the labeling of training images in some cases ambiguous, as images acquired at a particular location might reflect a different visual concept than the one assigned to them (e.g. last images in the sequence of Fig. <ref type="figure" coords="2,242.59,420.31,3.87,9.96">1</ref>). Second, images acquired for VPC exhibit a great degree of redundancy due to the temporal continuity of the image stream, i.e. labels associated to images acquired close in time are likely to belong to the same concept and share similar appearance. These peculiarities, originated in the very definition of the problem, make the visual classification of places a very challenging task.</p><p>The paper is organized as follows. In Sec. 2 we give a high level description of the different stages in our system. In Sec. 3, 4 and 5 we describe in detail the representation we use as well as the different classification schemes we applied. In Sec. 6 we present our experimental setup and in Sec. 7 we show results using the training set provided by the organizers of the challenge. Finally, in Sec. 8 we draw some conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Overview</head><p>In this section we describe the core components of our system. The methods we applied in solving both the obligatory and optional tasks comprise the following processing steps:</p><p>-Encoding: images must be robustly represented in order to capture high level properties of the scene. We rely on the state-of-the-art Fisher Vector image signature. As far as we know, this is the first time such a representation is applied in robotics.</p><p>-Scoring: we generate, for each image and concept, a score that provide us with a measure on how likely is for an image to have been acquired at a particular location. We use simple linear classifiers, which are efficient both to train and to evaluate. -Classification: based on the scores obtained in the previous step, we generate a prediction of the robot actual location. We consider two cases:</p><p>1. Individual frames are classified without taking into account the temporal consistency of the stream. We treat this problem as a simple (baseline) image classification task using FV and efficient linear classifiers. 2. The image stream is automatically segmented into visually similar groups of images and the classification is performed in a segment-by-segment basis. We propose an efficient temporal segmentation algorithm based on representation properties of the FV signature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Encoding</head><p>We provide a brief overview of the representation in which our method is based, namely, the Fisher vector image signature. Further details can be found in <ref type="bibr" coords="3,459.56,407.84,10.51,9.96" target="#b5">[6,</ref><ref type="bibr" coords="3,470.07,407.84,7.01,9.96" target="#b7">8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Fisher Vectors and the Similarity Between Images</head><p>Let u λ : R D → R + be a pdf of parameter vector λ modelling the generation process of low-level descriptors in any image. Let X = {x n , n = 1, • • • , N } be a iid sample of such D-dimensional descriptors extracted from a given image. The Fisher vector is defined as</p><formula xml:id="formula_0" coords="3,278.90,525.88,201.68,18.58">G X λ = L λ G X λ .<label>(1)</label></formula><p>Here, G X λ denotes the gradient of the (average) log-likelihood of X under u λ :</p><formula xml:id="formula_1" coords="3,244.80,575.91,235.78,30.49">G X λ = 1 N N n=1 ∇ λ log u λ (x n ) ,<label>(2)</label></formula><p>and L λ is a diagonal normalizer. We model u λ as a mixture of M Gaussians with diagonal covariances, i.e.</p><formula xml:id="formula_2" coords="3,134.77,629.20,345.81,31.43">u λ (x) = M i=1 w i u i (x), parametrized in terms of λ = {w i , µ i , σ i , i = 1, • • • , M }.</formula><p>Here, w i , µ i and σ 2 i denote, respectively, the mixing weight, mean and variance vector corresponding to the ith component of the mixture. It can be shown that <ref type="bibr" coords="4,286.50,118.43,10.51,9.96" target="#b5">[6]</ref> <ref type="foot" coords="4,297.01,117.05,3.97,6.97" target="#foot_1">4</ref> :</p><formula xml:id="formula_3" coords="4,214.70,137.18,265.87,30.49">G X µi = 1 N √ w i N n=1 γ n (i) x n -µ i σ i<label>(3)</label></formula><formula xml:id="formula_4" coords="4,214.94,172.01,265.64,30.49">G X σi = 1 N √ 2w i N n=1 γ n (i) x n -µ i σ i 2 -1<label>(4)</label></formula><p>with γ n (i) representing the soft assignment of low-level descriptors to components of the mixture, i.e. γ n (i</p><formula xml:id="formula_5" coords="4,259.96,220.76,130.43,14.40">) = w i u i (x n ) M j=1 w j u j (x n ).</formula><p>The image signature is the concatenation of partial terms<ref type="foot" coords="4,293.50,235.28,3.97,6.97" target="#foot_2">5</ref> , i.e.</p><formula xml:id="formula_6" coords="4,208.83,253.86,271.75,23.46">G X = G X µ1 T , • • • , G X µM T , G X σ1 T , • • • , G X σM T T ,<label>(5)</label></formula><p>resulting in a vector of dimensionality E = 2M D. Following <ref type="bibr" coords="4,406.33,282.16,9.95,9.96" target="#b7">[8]</ref>, we apply the transformation f (z) = sign(z) |z| independently on each dimension and L 2normalize the transformed vector. These transformations have been shown to be highly beneficial in classification <ref type="bibr" coords="4,293.65,318.03,10.95,9.96" target="#b7">[8,</ref><ref type="bibr" coords="4,304.61,318.03,7.30,9.96" target="#b1">2]</ref> as well as in image retrieval problems <ref type="bibr" coords="4,134.77,329.98,10.51,9.96" target="#b6">[7,</ref><ref type="bibr" coords="4,145.28,329.98,7.01,9.96" target="#b2">3]</ref>. An important property of the transformed representation is that it allows the similarity between images to be measured efficiently by a simple dot-product between their FVs <ref type="bibr" coords="4,215.53,353.90,10.51,9.96" target="#b7">[8,</ref><ref type="bibr" coords="4,226.04,353.90,7.01,9.96" target="#b2">3]</ref>. Moreover, as the transformed vectors are L 2 -normalized, the similarity between FVs -as measured by the dot-product-is upper-bounded by <ref type="bibr" coords="4,148.60,377.80,7.74,9.96">1.</ref> In what follows, we use G X norm to denote the transformed signature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Low-level Descriptors</head><p>We extract two sets of low-level feature descriptors per image, computed independently from the luminance (Y) and depth (D) channels of each input frame. These sets of descriptors are used to compute two separate FVs that we denote by G X lum norm and G X depth norm respectively. Note that these two FVs originate from different probabilistic models, i.e. u lum λ and u depth λ . The parameters for these models can be estimated using the expectation maximization (EM) algorithm and a large set of descriptors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Scoring</head><p>Let us denote by C = {1, . . . , C} the set of concepts, i.e. locations defining the problem. For each channel (luminance and depth), we learn a set of C binary classifiers that provide us with a measure of how likely is for a given image to have been acquired on a particular location in the environment. Concretely, we generate a set of C linear predictors per channel, of the form:</p><formula xml:id="formula_7" coords="4,264.10,612.82,216.48,19.93">s ξ c = θ c T G X ξ norm + b ξ c ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_8" coords="5,162.75,117.38,165.65,18.09">θ ξ c ∈ R E , b ξ c ∈ R and ξ ∈ {lum, depth}.</formula><p>As we rely on simple linear models, learning the parameters for the 2C classifiers can be done very efficiently, e.g. by using Stochastic Gradient Descent (SGD) <ref type="bibr" coords="5,318.28,142.33,9.95,9.96" target="#b0">[1]</ref>. Given a test image, we generate a single score per class by computing the unweighted average of s lum c and s depth c . We denote this score by s c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Classification</head><p>In this section we describe our approach for robust place classification for both tasks of the challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Obligatory Task (Task 1)</head><p>For the obligatory task, images have to be classified without considering the temporal continuity of the image stream, i.e. frame-by-frame. In cases of uncertainty, the system is allowed to refrain from making a decision (thus avoiding penalization points).</p><p>We treat this task as a simple (baseline) classification problem. Deciding to which class an image belongs was done according to the following rule:</p><formula xml:id="formula_9" coords="5,235.23,367.78,245.35,24.31">ĉ = c = arg max i s i , if s c &gt; α 0, otherwise<label>(7)</label></formula><p>Here, 0 denotes "no classification", i.e. the image is left unclassified. The parameter α is set empirically by cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Optional Task (Task 2)</head><p>For this task, participants were allowed to exploit the temporal continuity of the image stream. This task also characterizes by the presence of kidnappings: situations in which the robot abruptly changes its location from one room to another.</p><p>Before introducing the methods we applied for this task, let us first introduce some notation. Let I 1:T = {I t , t = 1 . . . T }, I t ∈ I, be a sequence of images acquired by a mobile robot up to time T . With a slight abuse of notation, we denote by s c (t) the classification score computed for image I t and class c ∈ C. We define the score vector s(t) := (s 1 (t), . . . , s C (t))</p><p>T . Similarly, we use the notation G X ξ norm (t) to represent the FV computed for image I t using the set of descriptors extracted from channel ξ ∈ {lum, depth}.</p><p>Temporal Segmentation. Let us assume that for every t and t ′ there exists a function m(t, t ′ ) that provide us with a measure of the similarity between images I t and I t ′ . Given a reference image I a , we define a temporal segment as the sequence I a:b = {I t , t = a, . . . , b}, where b is the greatest integer such that m(a, b) ≥ m 0 and m 0 a free parameter. Reference images are selected as the first image that follows a previously computed segment. The first of such reference frames is chosen as the first image in the sequence. The classification of images is performed segment-by-segment instead of frame-by-frame. Algorithm 1 provides an overview of our approach for temporal segmentation and classification.</p><p>As a proxy for m(t, t ′ ) we use the dot product between FVs computed from either the luminance or depth channel features of I t and I t ′ (Sec. <ref type="bibr" coords="6,437.21,352.07,15.48,9.96">3.1)</ref>. This provides us with a measure consistent with the classification model. It holds that |m(t, t ′ )| ≤ 1.</p><p>Using the above procedure, dealing with kidnapping situations becomes rather natural since, in such cases, the visual appearance of images is likely to change considerably from one frame to another. This abrupt change in appearance will trigger the generation of a new reference image and the classification of the segment extracted just before the kidnap point.</p><p>Segment classification. Based on the above definition, we classify all images in the segment I a:b according to one of the following rules: a) Maximum confidence and threshold (MCT), which takes into account the confidence of the classifiers w.r.t. the best alternative hypothesis within a given segment; or b) Majority vote and threshold (MVT), which tries to exploit the temporal and semantic consistency of the images. Details for these rules are given next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maximum confidence and threshold (MCT).</head><p>Let c 1 , c 2 ∈ {1, • • • , C} denote the indices to the best and second best scoring classifiers at time t and let d(t) := s c1 (t)s c2 (t) ≥ 0 denote the difference between the corresponding scores. All images in the segment I a:b are classified as belonging to class ĉ according to the following rule:</p><formula xml:id="formula_10" coords="6,205.88,636.30,274.69,24.31">ĉ = c 1 , if s c1 (u) &gt; β, u = arg max t∈[a,b] d(t) 0, otherwise<label>(8)</label></formula><p>Majority vote and threshold (MVT). Let v a:b (c) := #{s c (t) &gt; β, t = a, . . . , b} denote the number of times the classification score for the cth classifier is above β for images in the segment I a:b . We consider the following voting strategy for the classification of the temporal segment I a:b :</p><formula xml:id="formula_11" coords="7,267.44,174.57,213.14,14.94">ĉ = arg max c v a:b (c)<label>(9)</label></formula><p>As before, ĉ = 0 means that images in I a:b are left unclassified. In both cases, the parameter β is set empirically by cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Setup</head><p>In this section we provide a detailed explanation of our experimental procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Dataset</head><p>The training set for the Robot Vision Challenge 2012 consists of three sequences of 2667, 2532 and 1913 RGBD images respectively, acquired under different illumination conditions within the same floor of an office environment. They include motions in both clockwise and counter clockwise directions. Performance is measured based on the number of correctly and misclassified images in a given sequence and it varies from task to task.</p><p>Further details regarding the dataset and the evaluation methodology can be readily found in <ref type="bibr" coords="7,207.30,393.79,9.95,9.96" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Low-level Features</head><p>Both RGB and depth images were reduced at half their original resolution before computations. We extracted 128-dimensional SIFT descriptors <ref type="bibr" coords="7,424.61,452.94,10.51,9.96" target="#b3">[4]</ref> from local patches of 32 × 32 pixels located at the nodes of a regular grid (step size of 4 pixels). We used the DSIFT implementation of <ref type="bibr" coords="7,340.02,476.85,9.95,9.96" target="#b8">[9]</ref>. We did not perform any normalization (rotation, intensity, etc.) on the image patches before computations. To account for variations in scale, we built a resolution pyramid of 5 levels using a scale factor of 0.707 between them. SIFT descriptors were extracted independently on each level using the procedure described above. In the case of depth images, we considered only descriptors whose magnitude was greater than a small value (set to 10 -3 in our experiments). The dimensionality of SIFT descriptors was further reduced to 80 by Principal Components Analysis (PCA). PCA projection matrices were learned from a set of 10 6 randomly sampled descriptors from the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Generative Model</head><p>For each channel, we trained a Gaussian Mixture Model (GMM) with M components under a Maximum Likelihood (ML) criterion using the Expectation-Maximization (EM) algorithm. We used 10<ref type="foot" coords="7,323.43,654.17,3.97,6.97" target="#foot_3">6</ref> random samples from the training set. We initialized the EM iterations by running k-means and using the statistics of cluster assignments (relative count, mean and variance vectors) as initial estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Base Classifiers</head><p>As base classifiers we used linear SVMs trained on the primal using Stochastic Gradient Descent (SGD) <ref type="bibr" coords="8,247.51,205.39,9.95,9.96" target="#b0">[1]</ref>, i.e. minimizing the L 2 regularized hinge-loss in a sample-by-sample basis. The regularization parameter λ was chosen by crossvalidation on the training set. We trained C classifiers per channel following a one-vs-all strategy. i.e. when training the models for class c we used the samples of that class as positives and the rest as negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>In order to allow the system to cope with changes in illumination and the robot motion direction, we considered the following data augmentation strategies: i) adding new images by simulating uniform changes in illumination, i.e. generating darker/brighter versions of randomly sampled images; ii) generating mirrored (left-to-right) versions of the images provided as training material. In the first case, we did not observe any noticeable improvement while, in the second, we observed an increase of +30% (on average) w.r.t. a system trained using the original data only. This is to be expected, as our low-level features (i.e. SIFT vectors) are based on gradient information which makes them insensitive to uniform changes in the illumination. On the other hand, adding mirrored samples to the training set let the system learn up to some degree the symmetries originated from the changes in the robot motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Task 1</head><p>In this subsection we evaluate the performance of our system in classifying images independently (frame-by-frame). In particular, we consider the following aspects: i) the impact of using increasingly complex models (i.e. number of Gaussians, M ); ii) the benefits of using different representation channels. For each configuration, we ran three experiments using different train/test splits of the data, using two of the sequences for training and the third for testing. Results are reported on Table <ref type="table" coords="8,262.08,564.61,3.87,9.96" target="#tab_1">1</ref>. We show recognition performance 6 for models with M = 256, 512 and 1024 Gaussians and systems based on single and multiple descriptor channels. Results on Table <ref type="table" coords="8,323.79,588.52,4.98,9.96" target="#tab_1">1</ref> were obtained by setting α = -∞ in Eq. ( <ref type="formula" coords="8,167.61,600.47,3.87,9.96" target="#formula_9">7</ref>), i.e. argmax rule without thresholding. The classification performance obtained with α = -0.5 is shown in parentheses.</p><p>If we consider the different train/test configurations, it can be observed a big drop in performance for the system trained on sequences 1 and 2. This drop can be explained by noting that sequence 3 was acquired under very poor illumination. A system trained using only images acquired under "normal" illumination does not generalizes well to this previously unseen scenario. In contrast, systems to which this sequence was shown during training exhibit much better performance (second and third row in the table). As expected, the system based on the luminance channel alone performs worse than the system using depth information when testing on sequence 3. On the contrary, the luminance channel shows better performance on test sequences 1 and 2. The combination of both channels brings large improvements in all scenarios. Increasing the model complexity (number of components in the mixture) can bring additional improvements at the cost of a greater computational cost.</p><p>The system we submitted during the challenge included both luminance and depth features and models with M = 1024 Gaussians. The threshold parameter was set to α = -0.5. Our system ranked first, achieving a score of 2071 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Task 2</head><p>For this task, we first evaluate the influence of the parameter m 0 in classification performance (Sec. 5.2, temporal segmentation). m 0 controls the degree to which an image is considered similar to another of reference, i.e if it belongs to the temporal segment defined by the second. Fig. <ref type="figure" coords="9,367.89,499.68,4.98,9.96" target="#fig_0">2</ref> (left) shows the average score as a function of m 0 for different choices of the similarity measure (dotproduct between luminance or depth FVs) and classification rules (Sec. 5.2), e.g. Lum+MVT corresponds to the system using the dot-product between luminance FVs for segmentation and the MVT rule for classification. Fig. <ref type="figure" coords="9,443.47,547.50,4.98,9.96" target="#fig_0">2</ref> (right) show the average length of the temporal segments obtained by using FVs from either channel.</p><p>It can be observed that for m 0 above 0.2, using luminance FVs for segmentation leads to better results than with depth FVs. Using luminance FVs, performance reaches a peak at m 0 = 0.3 (Lum+MCT: 1836, Lum+MVT: 1871). Within this range, MVT performs better than MCT. For values of m 0 &lt; 0.2, the segmentation using depth FVs leads to better results. In this case, a peak is observed at m 0 = 0.1 (Depth+MCT: 1919, Lum+MVT: 1888) with MCT performing better. As a comparison, the average score obtained by setting m 0 = 1 (frame-by-frame classification) on this task is 1801. The average segment length at the above points is 5.4 and 6.86 for luminance (m 0 = 0.3) and depth (m 0 = 0.1) features, respectively. For the same value of m 0 , luminance FVs lead to larger segments.</p><p>For the competition we submitted two systems: Depth+MCT and Depth+MVT using, as before, 1024 Gaussians. We set the segmentation and classification thresholds to m 0 = 0.1 and β = -0.4, respectively. Our systems ranked first, achieving 3930 points on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Timings</head><p>Finally, we report computation times for the system based on models with M = 1024 mixture components. Reported times were measured on a AMD Opteron machine (8 cores @ 2GHz) with 8 GB of RAM. Table <ref type="table" coords="10,377.36,486.18,4.98,9.96" target="#tab_2">2</ref> show estimated times for both offline and online processes. Additionally, we report FV computation times for M = 256 and 512 Gaussians. This paper describes the CIII participation at ImageCLEF Robot Vision Challenge 2012. Our approach leverages recent advances in the fields of image classification and retrieval. We proposed a temporal segmentation methodology based on the visual similarity of images that allowed us to classify groups of images in a robust manner. Our team ranked first on both the obligatory and optional tasks of the challenge, showing the potentiality and effectiveness of our approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="10,134.77,253.93,345.81,8.97;10,134.77,264.90,345.83,8.97;10,134.77,275.85,345.83,8.97;10,134.76,286.81,63.28,8.97"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Experiments for Task 2 using models with M = 1024 Gaussians and β = -0.4, given as a function of the segmentation threshold m0. Left: average classification score for different choices of the similarity measure and classification rule; Right: average segment length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,134.77,118.17,330.27,130.95"><head></head><label></label><figDesc>Algorithm 1 Temporal segmentation and classification</figDesc><table coords="6,143.98,131.02,321.05,118.10"><row><cell>t0 ← 1</cell></row><row><cell>S ← {s(1)}</cell></row><row><cell>for t = 2, . . . do</cell></row><row><cell>if m(t0, t) &lt; m0 then</cell></row><row><cell>Classify images in It 0 :t-1 according to MCT or MVT using the scores in S</cell></row><row><cell>t0 ← t</cell></row><row><cell>S ← {s(t)}</cell></row><row><cell>else</cell></row><row><cell>S ← S ∪ {s(t)}</cell></row><row><cell>end if</cell></row><row><cell>end for</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,134.77,116.88,345.81,109.19"><head>Table 1 :</head><label>1</label><figDesc>Recognition performance for "Task 1" for models involving M = 256, 512 and 1024 Gaussian and different representation channels. See text for details.</figDesc><table coords="9,177.74,150.16,256.79,75.92"><row><cell cols="3">Luminance</cell><cell></cell><cell>Depth</cell><cell cols="2">Lum+Depth</cell></row><row><cell cols="6">Train Test 256 512 1024 256 512 1024 256</cell><cell>512 1024</cell></row><row><cell cols="6">1 &amp; 2 3 275 411 491 511 489 609 835</cell><cell>897</cell><cell>965</cell></row><row><cell cols="7">2 &amp; 3 1 1941 1945 1969 1403 1429 1461 1947 1961 1941</cell></row><row><cell cols="7">3 &amp; 1 2 2150 2158 2154 1548 1564 1598 2050 2078 2094</cell></row><row><cell cols="7">Average 1455 1505 1538 1154 1161 1223 1611 1645 1667</cell></row><row><cell>(α = -0.5) -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">-(1657) (1718) (1722)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,134.77,542.22,345.81,119.35"><head>Table 2 :</head><label>2</label><figDesc>Computation times for models with M = 1024 Gaussians. PCA and GMM parameters were estimated on 10 6 randomly sampled features.</figDesc><table coords="10,180.72,575.50,253.92,86.07"><row><cell></cell><cell>PCA training</cell><cell>4min / channel</cell></row><row><cell>Offline</cell><cell>GMM training (M = 1024)</cell><cell>1h 30min / channel</cell></row><row><cell></cell><cell>Classifier training</cell><cell>5min / class / channel</cell></row><row><cell></cell><cell>SIFT+PCA</cell><cell>170msec / image / channel</cell></row><row><cell></cell><cell>FV (M = 256)</cell><cell>220msec / image / channel</cell></row><row><cell>Online</cell><cell>FV (M = 512)</cell><cell>310msec / image / channel</cell></row><row><cell></cell><cell>FV (M = 1024)</cell><cell>490msec / image / channel</cell></row><row><cell></cell><cell>Scoring</cell><cell>2.2msec / image / class</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="2,144.73,645.33,335.84,8.97;2,144.73,656.30,43.53,8.97"><p>The problem is also known in the literature as image classification, categorization or tagging.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="4,144.73,634.38,263.06,8.97"><p>Vector divisions must be understood as term-by-term operations.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2" coords="4,144.73,645.34,335.86,8.97;4,144.73,656.30,335.85,8.97"><p>We consider the gradient w.r.t. the mean and variance vectors only as the gradient w.r.t. the mixing weights has shown to provide little discriminative information<ref type="bibr" coords="4,468.31,656.30,9.21,8.97" target="#b5">[6]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3" coords="8,144.73,656.30,335.85,8.97"><p>The score was computed using the scripts provided by the organizers of the challenge.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. <rs type="person">Jorge Sánchez</rs> was partially supported by a grant from <rs type="funder">CONICET, Argentina</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,138.35,299.50,274.61,8.97" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<ptr target="http://leon.bottou.org/projects/sgd" />
		<title level="m" coord="11,194.53,299.50,16.46,8.97">SGD</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.35,310.45,342.22,8.97;11,146.92,321.42,299.88,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,376.44,310.45,104.13,8.97;11,146.92,321.42,196.72,8.97">The devil is in the details: an evaluation of recent feature encoding methods</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,365.38,321.42,52.75,8.97">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.35,332.38,342.23,8.97;11,146.92,343.33,333.66,8.97;11,146.92,354.29,185.33,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,431.68,332.38,48.91,8.97;11,146.92,343.33,178.67,8.97">Aggregating local image descriptors into compact codes</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,334.27,343.33,146.31,8.97;11,146.92,354.29,82.42,8.97">IEEE Tr. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1704" to="1716" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.35,365.25,342.23,8.97;11,146.92,376.21,121.66,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,197.57,365.25,225.43,8.97">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,429.86,365.25,50.72,8.97;11,146.92,376.21,68.92,8.97">Intl. Jrnl. on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.35,387.17,342.24,8.97;11,146.92,398.12,196.63,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,331.11,387.17,149.48,8.97;11,146.92,398.12,41.81,8.97">Overview of the imageclef 2012 robot vision task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">G</forename><surname>Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,209.73,398.12,105.15,8.97">CLEF 2012 working notes</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.35,409.08,342.23,8.97;11,146.92,420.05,130.20,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,255.12,409.08,225.46,8.97;11,146.92,420.05,29.59,8.97">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,197.36,420.05,51.09,8.97">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.35,431.00,342.23,8.97;11,146.92,441.96,201.11,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,348.55,431.00,132.03,8.97;11,146.92,441.96,100.35,8.97">Large-scale image retrieval with compressed fisher vectors</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Poirier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,268.27,441.96,51.09,8.97">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.35,452.92,342.24,8.97;11,146.92,463.88,176.74,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,312.28,452.92,168.30,8.97;11,146.92,463.88,76.40,8.97">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,244.04,463.88,50.97,8.97">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.35,474.84,342.22,8.97;11,146.92,485.80,180.42,8.97" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="11,253.87,474.84,226.70,8.97;11,146.92,485.80,41.01,8.97">VLFeat: An open and portable library of computer vision algorithms</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<ptr target="http://www.vlfeat.org/" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
