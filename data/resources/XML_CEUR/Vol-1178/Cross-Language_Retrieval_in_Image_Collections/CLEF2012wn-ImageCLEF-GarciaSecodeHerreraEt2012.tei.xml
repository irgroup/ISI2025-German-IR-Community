<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,142.72,115.96,329.93,12.62">The medGIFT Group in ImageCLEFmed 2012</title>
				<funder ref="#_5KCu4tW">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_Fvq9k2q #_wk3rccq #_N2NPg7K">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,145.75,153.68,105.28,8.74"><forename type="first">Alba</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Applied Sciences Western Switzerland (HES-SO)</orgName>
								<address>
									<settlement>Sierre</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,258.82,153.68,82.46,8.74"><forename type="first">Dimitrios</forename><surname>Markonis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Applied Sciences Western Switzerland (HES-SO)</orgName>
								<address>
									<settlement>Sierre</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,349.36,153.68,44.41,8.74"><forename type="first">Ivan</forename><surname>Eggel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Applied Sciences Western Switzerland (HES-SO)</orgName>
								<address>
									<settlement>Sierre</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,401.54,153.68,68.15,8.74"><forename type="first">Henning</forename><surname>MÃ¼ller</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Applied Sciences Western Switzerland (HES-SO)</orgName>
								<address>
									<settlement>Sierre</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,142.72,115.96,329.93,12.62">The medGIFT Group in ImageCLEFmed 2012</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">753D6E3C234384EDA29E640AC8AC8010</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article presents the participation of the medGIFT group in ImageCLEFmed 2012. Since 2004, the group has participated in the medical image retrieval tasks of ImageCLEF each year. There are three types of tasks for ImageCLEFmed 2012: modality classification, imagebased retrieval and case-based retrieval. The medGIFT group participated in all three tasks. MedGIFT is developing a system named Par-aDISE (Parallel Distributed Image Search Engine), which is the successor of GIFT (GNU Image Finding Tool). The alpha version of ParaDISE was used to run most of the experiments in the competition. Results show that our approach using Bag-of-Visual-Words (BoVW), Bag-of-Colors (BoC) and Lucene for the image captions is the best run for mixed modality classification. The same approach is also the best for the image retrieval task in terms of bpref. In the case-based retrieval task, the Lucene baseline is the best run in terms of mean average precision (MAP). We were the only group presenting mixed and visual runs in these tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.28" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>ImageCLEF is the cross-language image retrieval track<ref type="foot" coords="1,375.61,458.81,3.97,6.12" target="#foot_0">1</ref> of the Cross Language Evaluation Forum (CLEF). ImageCLEFmed is part of ImageCLEF focusing on medical images <ref type="bibr" coords="1,206.18,484.30,7.75,8.74" target="#b0">[1]</ref><ref type="bibr" coords="1,213.94,484.30,3.88,8.74" target="#b1">[2]</ref><ref type="bibr" coords="1,213.94,484.30,3.88,8.74" target="#b2">[3]</ref><ref type="bibr" coords="1,213.94,484.30,3.88,8.74" target="#b3">[4]</ref><ref type="bibr" coords="1,217.81,484.30,7.75,8.74" target="#b4">[5]</ref>. The medGIFT<ref type="foot" coords="1,298.83,482.72,3.97,6.12" target="#foot_1">2</ref> research group has participated in Im-ageCLEFmed since 2004. MedGIFT is currently developing ParaDISE (Parallel Distributed Image Search Engine), which is the successor of GIFT<ref type="foot" coords="1,427.01,506.63,3.97,6.12" target="#foot_2">3</ref> (GNU Image Finding Tool). The alpha version of ParaDISE was used to run most of the experiments.</p><p>The Bag-of-Visual-Words (BOVW) and Bag-of-Colors (BOC) approaches were used for visual retrieval and the textual baseline is based on the open source Lucene<ref type="foot" coords="1,165.34,566.46,3.97,6.12" target="#foot_3">4</ref> system. In ImageCLEFmed 2011, the BoVW approach was applied using the Scale Invariant Feature Transform (SIFT) <ref type="bibr" coords="1,370.76,579.99,10.52,8.74" target="#b5">[6]</ref> but color information was not used. In 2012, there are two main novelties in our system: first, we introduce the BoC <ref type="bibr" coords="1,217.86,603.90,10.52,8.74" target="#b6">[7]</ref> descriptor that represents local image colors. We combine BoC with BoVW and textual descriptors in order to yield better results. Second, we use training set expansion strategies since for some of the image categories only very few annotated examples were available. This significantly improves classification performance.</p><p>The widely used BoVW <ref type="bibr" coords="2,260.69,166.82,10.52,8.74" target="#b7">[8]</ref> method is applied as follows: a training set of images is chosen and a number of local descriptors are extracted from each image of this set. The descriptors are then clustered using a clustering method (such as k-means or DENCLUE). The centroids of the clusters are treated as visual words that represent the specific local patterns. Hence, we have a visual-word vocabulary describing all types of local image patterns. Local features are then also extracted from each image in the database and mapped onto the visual-word vocabulary. An image is then represented as a histograms of the visual-word occurrences, the BoVW used as feature vector in the classification task. When an image is queried, a similarity measure (such as histogram intersection) is used to compare the query image histogram and the database image histogram, providing a similarity score.</p><p>The BoC representation is analogous to the BoVW representation and to the Bag-of-Words representation of text documents. As the two methods are quite complementary and as the representations as histograms are very similar, the two approaches can easily be fused for medical image classification.</p><p>In Section 2, we describe the datasets and the techniques used. We evaluate the runs submitted to the ImageCLEFmed 2012 benchmark in Section 3. Finally, conclusions are presented in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Datasets and Techniques</head><p>This section describes the basic techniques used in ImageCLEFmed 2012 by the medGIFT group. For the the majority of the runs, the alpha version of ParaDISE (Parallel Distributed Image Search Engine) developed by the medGIFT group is used. This image search engine is built on top of Hadoop <ref type="bibr" coords="2,385.85,466.36,9.97,8.74" target="#b8">[9]</ref>, a MapReduce <ref type="bibr" coords="2,465.13,466.36,15.50,8.74" target="#b9">[10]</ref> paradigm of parallel computing and the Cassandra<ref type="foot" coords="2,366.75,476.74,3.97,6.12" target="#foot_4">5</ref> DBMS (Database Management System). This allows for computational and storage scalability. The ParaDISE component-based architecture is designed to simplify the plugin of different image features and representations. Apart from the 3 baseline visual runs that were also given publicly to the participants, we submitted 10 runs (2 textual, 4 visual and 2 mixed runs) to the image-based retrieval task, 8 runs (2 textual, 4 visual and 2 mixed runs) to the case-based retrieval task and 8 runs (4 visual and 4 mixed runs) to the modality classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Collection</head><p>We used the database provided by ImageCLEFmed 2012. The database contains over 300,000 images of 75'000 articles of the biomedical open access literature. This is a subset from the PubMed Central 6 database containing over one million images. This set of articles contains all articles in PubMed that are open access but the exact copyright for redistribution varies among the journals. A more detailed description of the ImageCLEFmed 2012 setup is given in <ref type="bibr" coords="3,424.05,142.91,9.97,8.74" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Textual Techniques</head><p>For text retrieval the standard settings of the Apache Lucene text retrieval system are used. The documents containing the journal texts are cleaned off their XML elements and only the remaining text is used.</p><p>Two indexations were done for ImageCLEF 2012: <ref type="bibr" coords="3,379.88,228.56,12.73,8.74" target="#b0">(1)</ref> the full text of all articles was indexed on an article basis, and (2) the captions were indexed on a figure basis. In the past it was shown that for case-based retrieval an indexation of the full text had best results whereas for image-based retrieval the caption text delivered much better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Visual Techniques</head><p>The baseline for the visual description of the image is the Bag-of-Visual-Words (BoVW) approach. In this approach, local SIFT descriptors are extracted from each image. Then, the descriptors of the image are quantized, assigning each one to its nearest neighbour from a fixed set of local descriptors, called "visual vocabulary". The image is then represented by a histogram of the frequency of the "visual words". Similarity between two images can be quantified using a distance metric to measure the distance between the two histograms.</p><p>In our runs, the SIFT implementation in the fiji<ref type="foot" coords="3,369.13,408.28,3.97,6.12" target="#foot_6">7</ref> image processing package was used for the extraction of the local descriptors as in our participation in ImageCLEFmed 2011. In order to create the visual vocabulary, our implementation of the density-based clustering algorithm DENCLUE <ref type="bibr" coords="3,419.95,445.72,15.50,8.74" target="#b10">[11]</ref> was used. The reasons for this choice are the features and the nature of the data set that needs to be clustered. The data set to be clustered is large (1,000 training images produce approximately 2'500'000 descriptors) and high dimensional (SIFT descriptors are 128-dimensional). The DENCLUE algorithm is highly efficient for clustering large-scale data sets, can detect arbitrarily shaped clusters and handles outliers and noise well. Moreover, opposed to other density-based clustering algorithms it performs well for high-dimensional data. However, when using a density-based clustering algorithm care needs to be taken for data sets containing clusters of different densities. To deal with this, the parameter Î¾ that controls the significance of the candidate cluster in respect to its density was set to zero. The same visual vocabulary used in last year's participation <ref type="bibr" coords="3,445.20,577.23,15.50,8.74" target="#b11">[12]</ref> was also used for the creation of the bags of visual words.</p><p>Based on the BoVW approach, the Bag-of-Colors (BoC) <ref type="bibr" coords="3,413.31,601.14,10.52,8.74" target="#b6">[7]</ref> is an image description technique introduced in <ref type="bibr" coords="3,292.39,613.09,14.62,8.74" target="#b12">[13]</ref>. Similarly to the BoVW, the technique uses a color vocabulary C previously learned on a sub set of the collection to represent the image. A color vocabulary</p><formula xml:id="formula_0" coords="3,306.55,637.00,174.05,9.66">C = {c 1 , . . . , c kc }, with c i = (L i , a i , b i ) â</formula><p>CIELab is constructed by finding the most frequently occurring colors in each image of a sub set of the collection. The modality classification training set was used for the creation of the color vocabulary. The CIE (International Commission on Illumination) 1976 L*a*b (CIELab) space was used, being a perceptually uniform color space. CIELab is a space defined by L for luminance and a, b for the color-opponent dimensions for chrominance <ref type="bibr" coords="4,344.33,178.77,15.50,8.74" target="#b13">[14,</ref><ref type="bibr" coords="4,361.49,178.77,11.63,8.74" target="#b14">15]</ref>. The BoC of an image I is defined as a vector h BoC = {c 1 , . . . , ck } such that, for each pixel p k â I âk â {1, . . . , n p }, with n p being the number of pixels of the image I:</p><formula xml:id="formula_1" coords="4,134.76,222.77,345.83,74.13">ci = np â k=1 np â j=1 g j (p k ) âi â {1, . . . , k c } where g j (p) = { 1 if d Îµ (p, c j ) â¤ d Îµ (p, c l ) âl â {1, . . . , k c } 0 otherwise (1)</formula><p>From experiments on the modality classification task of ImageCLEFmed 2012, k c = 100 and k vw = 238 were chosen as the sizes of the color and visual vocabulary, respectively. For the comparison of the images the histogram intersection <ref type="bibr" coords="4,155.24,341.86,15.50,8.74" target="#b15">[16]</ref> is used as distance measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Fusion Techniques</head><p>In image retrieval and classification tasks, often different features, systems and results can be combined to deliver improved results. Moreover, multiple query images can describe in more detail the visual characteristics to be retrieved. In these cases a fusion strategy needs to be used. Two main fusion strategies exist, early and late fusion. In the early fusion, the vectors of the features or the systems are merged into a single vector. For the early fusion of multiple positive and/or negative query images, Rocchio's algorithm can be used:</p><formula xml:id="formula_2" coords="4,204.71,481.30,275.88,30.67">q m = Î±q o + Î² 1 |D r | â dj âDr d j -Î³ 1 |D nr | â dj âDnr d j (2)</formula><p>where Î±, Î² and Î³ are weights, q m is the modified query, q o is the original query, D r is the set of relevant images and D nr is the set of non-relevant images. In our scenario there is a lack of non-relevant images, so only the second term of the right part of the equation is used. This algorithm can be applied only for vectors of the same feature spaces so it is not applicable for the fusion of different visual features or retrieval systems, in general.</p><p>In the late fusion, the retrieval results of the features or the systems are fused. Two main categories of late fusion techniques exist, the score-based and rank-based methods. In 2009, the ImageCLEF@ICPR fusion task was organized to compare late fusion techniques using the best ImageCLEFmed visual and textual results <ref type="bibr" coords="4,185.66,644.17,14.62,8.74" target="#b16">[17]</ref>. Studies such as <ref type="bibr" coords="4,274.33,644.17,15.50,8.74" target="#b17">[18]</ref> show that combSUM (3) and combMNZ(4), score-based methods, proposed by <ref type="bibr" coords="4,285.50,656.12,15.50,8.74" target="#b18">[19]</ref> in 1994 are robust fusion strategies. With the data from the ImageCLEF@ICPR fusion task, combMNZ performed slightly better than combSUM but the difference was small and not statistically significant.</p><formula xml:id="formula_3" coords="5,256.50,151.71,224.09,30.72">S combSUM (i) = N k â k=1 S k (i)<label>(3)</label></formula><formula xml:id="formula_4" coords="5,236.11,188.47,244.48,10.83">S combMNZ (i) = F (i) * S combSUM (i)<label>(4)</label></formula><p>where F (i) is the frequency of image i being returned by one input system with a non-zero score, and S(i) is the score assigned to image i.</p><p>In general, rank-based fusion (RRF) worked better than score-based fusion. The reciprocal rank fusion <ref type="bibr" coords="5,253.09,241.02,15.50,8.74" target="#b19">[20]</ref> is a simple fusion method based on ranks <ref type="bibr" coords="5,449.97,241.02,13.04,8.74" target="#b4">(5)</ref>.</p><formula xml:id="formula_5" coords="5,233.54,255.41,247.05,30.20">RRF score(d â D) = â râR 1 k + r(d) (5)</formula><p>where, D is the set of documents retrieved, R is the set of rankings of the documents and k = 60.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>This section details the techniques that were used to produce the runs for Im-ageCLEFmed 2012 and then evaluates the runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image and Case Retrieval Techniques</head><p>Two strategies were compared for the fusion of the multiple queries, early fusion and late fusion (see Section 2.4). For the fusion of visual features and the fusion of visual and textual systems, late fusion was applied. Score-based and rankbased fusion techniques were compared. To summarize, fusion was used in three cases:</p><p>fusing multiple visual features to produce visual runs; fusing textual and visual runs to produce mixed runs; fusing vectors (early fusion) of query images which belong to the same topic or their results (late fusion).</p><p>In the image retrieval task, as the fulltext search retrieved articles instead of images, an article-to-image mapping was used. If an image was contained in multiple articles, only the article with the highest score was taken into account giving its score to the image. If multiple images were contained in the same article, all the images received the common article's score. The same strategy was applied in the case-based task, because the image search retrieved images instead of articles. The article received the score of the best scored image that it contained. If multiple articles contained the same image they all received the common image's score.</p><p>Table <ref type="table" coords="5,177.99,644.17,4.98,8.74" target="#tab_0">1</ref> contains a summary of the techniques used, while Tables <ref type="table" coords="5,446.18,644.17,4.98,8.74" target="#tab_1">2</ref> and<ref type="table" coords="5,475.63,644.17,4.98,8.74" target="#tab_2">3</ref> show the details of the submitted runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modality Classification Techniques</head><p>Driven by the good performance of <ref type="bibr" coords="7,296.89,142.20,10.52,8.74" target="#b2">[3]</ref> in the ImageCLEFmed 2011 modality classification task, a similar approach was followed by the medGIFT group in In practice, smaller sizes were obtained, mainly because of two reasons: retrieved images that are already contained in the training set were discarded and images retrieved multiple times by query images of different classes were discarded as well.</p><p>For a run to qualify as visual, we considered that the expanded training set used in this run needs to be created only by visual means. This means that the queries on the full data set used only visual features for the retrieval. Similarly, this was repeated using mixed (visual and textual) queries for the mixed runs. This resulted in a final number of 5 training sets (2 balanced, 2 non-balanced and the original training set).</p><p>A k -nn classifier using weighted voting was used to classify the test images. For the choice of the classifier parameters the results of <ref type="bibr" coords="7,395.12,454.99,10.52,8.74" target="#b6">[7]</ref> were taken into account and k = 11, k = 7 were used for the visual runs. However, since the non-balanced expanded training set was significantly larger, double the value k = 14 was also tested for this case. The inverse of the similarity score of the k -nn images was used to weight the voting.</p><p>Table <ref type="table" coords="7,177.10,515.42,4.98,8.74" target="#tab_3">4</ref> gives the details of the submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Image Retrieval Evaluation</head><p>Table <ref type="table" coords="7,162.55,571.78,4.98,8.74" target="#tab_4">5</ref> displays the results of the medGIFT runs for the image retrieval task. ir9, a mixed approach, achieved the highest MAP, GM-MAP and bpref among all our submitted runs in the image-based retrieval task. This result highlights the potential benefits of combining textual and visual features.</p><p>For the visual runs, the baseline of BoVW, ir1 and ir2, did not demonstrate good results. However, when BoVW is fused with BoC the results improve significantly. Furthermore, when using RRF for the fusion of techniques the MAP decreases, which contradicts our hypothesis. Our best results in early precision (P10 and P30) on the other hand were obtained with a text retrieval approach, ir8. This is in contradiction with past results where the best overall results were often textual whereas early precision was often better with combined or visual approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Case Retrieval Evaluation</head><p>Table <ref type="table" coords="9,163.56,207.05,4.98,8.74" target="#tab_5">6</ref> shows the results of the medGIFT runs for the case based retrieval task. In this task, cr8 achieved the highest MAP (0,169) among all submitted runs. It demonstrates that a Lucene baseline still obtains very good results with relatively low effort. MedGIFT was the only lab that submitted purely visual and mixed runs for the case-based task. Although the results of our visual runs are lower than textual runs, cr9, a mixed approach, performs better than the average of all submitted runs in this task. Visual 0,0016 0 0,0032 0,0038 0,0013 cr3 Visual 0,0302 0,001 0,0293 0,0231 0,009 cr4 Visual 0,0366 0,0014 0,0347 0,0269 0,0141 cr5 Visual 0,0007 0 0 0 0,0013 cr6 Visual 0,0008 0,0001 0,0007 0 0,0013 second best textual run 0,1508 0,0322 0,1279 0,1538 0,1167 cr7 Textual 0,169 0,0374 0,1499 0,1885 0,109 cr8 Textual 0,0696 0,0028 0,0762 0,0962 0,0615 cr9 Mixed 0,1017 0,0175 0,0857 0,1115 0,0679 cr10 Mixed 0,0514 0,009 0,0395 0,0654 0,0564</p><p>As all of the best results are from text retrieval runs it becomes clear that visual techniques need to be used in different ways than was the case for obtaining good results. Most likely a matching of image occurrences in articles in a more complex way would be necessary for obtaining good results for the case-based tasks using visual data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Modality Classification Evaluation</head><p>Finally, Table <ref type="table" coords="9,200.45,596.35,4.98,8.74" target="#tab_6">7</ref> presents the classification accuracy of the submitted medGIFT runs for the modality classification task. The runs c8, mc6, mc7 achieved the three best accuracies in the mixed run category. The visual runs achieved an average performance with the inclusion of BoC as a global descriptor to improving the classification accuracy. It can be observed that the runs mc4, mc8 using the non-balanced expanded training sets and k = 14 are outperforming the runs mc6, mc2 that use the original training set. These runs also perform better than the runs mc3, mc7 that use k = 7, confirming our hypothesis that using a larger k can improve results. Moreover, in experiments not submitted as official runs using larger values of k leads to a better performance, with the mixed run reaching an accuracy of 68.5% using the triple value k = 21.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This article describes the methods and results of the the medGIFT group for the ImageCLEF 2012 medical tasks. We submitted ten runs each for the adhoc image retrieval and the modality classification tasks and nine runs for the case-based retrieval task.</p><p>In ImageCLEFmed 2012 we concentrated on fusion methods of the visual and textual features and training set expansion strategies. We included the BoC approach, which significantly improves classification performance.</p><p>In the image-based retrieval task, our strategy leads to limited results obtained by our submitted runs. However, we still need to do a more detailed analysis to try to understand what is hurting performance in these runs. Our Lucene baseline achieves the best MAP for the case-based retrieval task. Finally, in the modality classification task we submitted the three runs with the best accuracies in the mixed run category and with only one group delivering better results with a visual approach.</p><p>Future work will to go beyond in the work of fusion methods, as well as further research in new visual features. Visual information can be used in better ways, for example to classify images by modality and if modality names appear in a query then filtering by this modality. If the same modalities occur in a query and an article this can also give evidence for a higher relevance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="7,134.76,166.11,345.89,8.74;7,134.76,178.06,345.86,8.74;7,134.76,190.02,345.85,8.74;7,134.76,201.97,345.86,8.74;7,134.76,213.93,345.85,8.74;7,134.76,225.88,215.55,8.74;7,149.71,238.49,330.93,8.74;7,134.76,250.45,345.82,8.74;7,134.76,262.40,345.86,8.74;7,134.76,274.36,345.86,8.74;7,134.76,286.31,345.87,8.74;7,134.76,298.27,345.89,8.74;7,134.76,310.22,345.89,8.74"><head></head><label></label><figDesc>2012. The approach involves automatically expanding the labelled training set to improve the performance of the classification for classes that are poorly represented (e.g. the training set containing very few images of that class). To achieve this, training images are used as queries in the full 300,000 images of the Image-CLEFmed 2012 data set and the l highest ranked retrieved images are added as training images into the class of the query image. Two methods of expanding the training set were used. In the first expansion technique, s images taken randomly of each class were used as queries. As in the original training set the number of images per class varies from 5 to 50, by choosing s = 5, l = 20 we can theoretically obtain a relatively balanced training set (105-150 per class) of 4,100 images. In the second, all the training images were queried, resulting in a larger non-balanced training set. E. g. for l = 20 an expanded training set of 21,000 images can be obtained theoretically.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,142.03,136.81,328.19,72.49"><head>Table 1 .</head><label>1</label><figDesc>Overview of the techniques used for the retrieval.</figDesc><table coords="6,142.03,157.21,328.19,52.09"><row><cell>Name</cell><cell>Technique</cell></row><row><cell cols="2">BoVW Content-based image search using Bag-of-Visual-Words representation</cell></row><row><cell>BoC</cell><cell>Content-based image search using Bag-of-Colors representation</cell></row><row><cell>Full text</cell><cell>Lucene textual search, searching into the full text of the articles</cell></row><row><cell>Captions</cell><cell>Lucene textual search, searching into the captions of images</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,136.16,270.65,366.73,148.26"><head>Table 2 .</head><label>2</label><figDesc>Techniques used for the image-based runs.</figDesc><table coords="6,136.16,300.27,366.73,118.64"><row><cell cols="2">Run ID Queries fusion</cell><cell>Techniques</cell><cell cols="2">Techniques fusion Run Type</cell></row><row><cell>ir1</cell><cell>Reciprocal</cell><cell>BoVW</cell><cell>n/a</cell><cell>Visual</cell></row><row><cell>ir2</cell><cell>combMNZ</cell><cell>BoVW</cell><cell>n/a</cell><cell>Visual</cell></row><row><cell>ir3</cell><cell>combMNZ</cell><cell>BoVW + BoC</cell><cell>combMNZ</cell><cell>Visual</cell></row><row><cell>ir4</cell><cell>Reciprocal</cell><cell>BoVW + BoC</cell><cell>Reciprocal</cell><cell>Visual</cell></row><row><cell>ir5</cell><cell>Rocchio</cell><cell>BoVW + BoC</cell><cell>combMNZ</cell><cell>Visual</cell></row><row><cell>ir6</cell><cell>Rocchio</cell><cell>BoVW + BoC</cell><cell>Reciprocal</cell><cell>Visual</cell></row><row><cell>ir7</cell><cell>n/a</cell><cell>Full text</cell><cell>n/a</cell><cell>Textual</cell></row><row><cell>ir8</cell><cell>n/a</cell><cell>Captions</cell><cell>n/a</cell><cell>Textual</cell></row><row><cell>ir9</cell><cell cols="2">combMNZ BoVW + BoC + Full text + Captions</cell><cell>combMNZ</cell><cell>Mixed</cell></row><row><cell>ir10</cell><cell cols="2">Reciprocal BoVW + BoC + Full text + Captions</cell><cell>Reciprocal</cell><cell>Mixed</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,136.16,481.26,366.73,148.26"><head>Table 3 .</head><label>3</label><figDesc>Techniques used for the case-based runs.</figDesc><table coords="6,136.16,510.88,366.73,118.64"><row><cell cols="2">Run ID Queries fusion</cell><cell>Techniques</cell><cell cols="2">Techniques fusion Run Type</cell></row><row><cell>cr1</cell><cell>Reciprocal</cell><cell>BoVW</cell><cell>n/a</cell><cell>Visual</cell></row><row><cell>cr2</cell><cell>combMNZ</cell><cell>BoVW</cell><cell>n/a</cell><cell>Visual</cell></row><row><cell>cr3</cell><cell>combMNZ</cell><cell>BoVW + BoC</cell><cell>combMNZ</cell><cell>Visual</cell></row><row><cell>cr4</cell><cell>Reciprocal</cell><cell>BoVW + BoC</cell><cell>Reciprocal</cell><cell>Visual</cell></row><row><cell>cr5</cell><cell>Rocchio</cell><cell>BoVW + BoC</cell><cell>combMNZ</cell><cell>Visual</cell></row><row><cell>cr6</cell><cell>Rocchio</cell><cell>BoVW + BoC</cell><cell>Reciprocal</cell><cell>Visual</cell></row><row><cell>cr7</cell><cell>n/a</cell><cell>Full text</cell><cell>n/a</cell><cell>Textual</cell></row><row><cell>cr8</cell><cell>n/a</cell><cell>Captions</cell><cell>n/a</cell><cell>Textual</cell></row><row><cell>cr9</cell><cell cols="2">combMNZ BoVW + BoC + Full text + Captions</cell><cell>combMNZ</cell><cell>Mixed</cell></row><row><cell>cr10</cell><cell cols="2">Reciprocal BoVW + BoC + Full text + Captions</cell><cell>Reciprocal</cell><cell>Mixed</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,140.73,170.19,330.81,127.69"><head>Table 4 .</head><label>4</label><figDesc>Modality classification runs</figDesc><table coords="8,140.73,190.60,330.81,107.28"><row><cell>Run ID</cell><cell>Techniques</cell><cell>Fusion Rule</cell><cell>Training Set</cell><cell>k Run Type</cell></row><row><cell>mc1</cell><cell>BoVW</cell><cell>n/a</cell><cell>original</cell><cell>11 Visual</cell></row><row><cell>mc2</cell><cell>BoVW + BoC</cell><cell>combMNZ</cell><cell>original</cell><cell>7 Visual</cell></row><row><cell>mc3</cell><cell>BoVW + BoC</cell><cell cols="3">combMNZ visual non-balanced 7 Visual</cell></row><row><cell>mc4</cell><cell>BoVW + BoC</cell><cell cols="3">combMNZ visual non-balanced 14 Visual</cell></row><row><cell>mc5</cell><cell>BoVW + BoC</cell><cell>combMNZ</cell><cell>visual balanced</cell><cell>7 Visual</cell></row><row><cell cols="3">mc6 BoVW + BoC + Captions Reciprocal</cell><cell>original</cell><cell>7 Mixed</cell></row><row><cell cols="5">mc7 BoVW + BoC + Captions Reciprocal mixed non-balanced 7 Mixed</cell></row><row><cell cols="5">mc8 BoVW + BoC + Captions Reciprocal mixed non-balanced 14 Mixed</cell></row><row><cell cols="3">mc9 BoVW + BoC + Captions Reciprocal</cell><cell>mixed balanced</cell><cell>7 Mixed</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,183.76,426.00,244.78,171.13"><head>Table 5 .</head><label>5</label><figDesc>Image retrieval results</figDesc><table coords="8,183.76,446.41,244.78,150.72"><row><cell cols="4">Run ID Run type MAP GM-MAP bpref</cell><cell>P10</cell><cell>P30</cell></row><row><cell cols="5">best visual run 0,0101 0,0004 0,0193 0,0591 0,0439</cell></row><row><cell>ir1</cell><cell>Visual 0,0016</cell><cell>0</cell><cell cols="2">0,0048 0,0273 0,0318</cell></row><row><cell>ir2</cell><cell>Visual 0,0017</cell><cell>0</cell><cell cols="2">0,0058 0,0227 0,0318</cell></row><row><cell>ir3</cell><cell cols="4">Visual 0,0049 0,0003 0,0138 0,0364 0,0364</cell></row><row><cell>ir4</cell><cell>Visual 0,004</cell><cell cols="3">0,0002 0,0103 0,0227 0,0318</cell></row><row><cell>ir5</cell><cell cols="4">Visual 0,0033 0,0003 0,0133 0,0364 0,0333</cell></row><row><cell>ir6</cell><cell>Visual 0,003</cell><cell>0,0001</cell><cell cols="2">0,01 0,0273 0,0227</cell></row><row><cell cols="2">best textual run 0.2182</cell><cell>0.082</cell><cell cols="2">0.2173 0.3409 0.2045</cell></row><row><cell>ir7</cell><cell cols="4">Textual 0.1397 0.0436 0.1565 0.2227 0.1379</cell></row><row><cell>ir8</cell><cell cols="2">Textual 0.1562 0.0424</cell><cell cols="2">0.167 0.3273 0.1864</cell></row><row><cell cols="5">best mixed run 0.2377 0.0665 0.2542 0.3682 0.2712</cell></row><row><cell>ir9</cell><cell cols="4">Mixed 0.2005 0.0917 0.1947 0.3091</cell><cell>0.2</cell></row><row><cell>ir10</cell><cell cols="4">Mixed 0.1167 0.0383 0.1238 0.1864 0.1485</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,176.49,310.75,259.31,48.84"><head>Table 6 .</head><label>6</label><figDesc>Case based retrieval results</figDesc><table coords="9,176.49,329.41,259.31,30.18"><row><cell>Run ID</cell><cell>Run type</cell><cell cols="3">MAP GM-MAP bpref</cell><cell>P10</cell><cell>P30</cell></row><row><cell>cr1</cell><cell>Visual</cell><cell>0,0008</cell><cell>0</cell><cell>0</cell><cell cols="2">0,0038 0,0013</cell></row><row><cell>cr2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="10,172.77,115.91,266.74,50.58"><head>Table 7 .</head><label>7</label><figDesc>Modality classification results</figDesc><table coords="10,172.77,136.32,266.74,30.17"><row><cell></cell><cell>Visual</cell><cell>Mixed</cell></row><row><cell>Run ID</cell><cell cols="2">mc1 mc2 mc3 mc4 mc5 best run mc6 mc7 mc8 mc9</cell></row><row><cell cols="3">Accuracy (%) 11.1 38.1 41.8 42.2 34.2 69.7 64.2 63.6 66.2 58.8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,624.57,117.68,7.47"><p>http://www.imageclef.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,144.73,635.52,108.26,7.47"><p>http://medgift.hevs.ch/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="1,144.73,646.48,155.34,7.47"><p>http://www.gnu.org/software/gift/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="1,144.73,657.44,117.68,7.47"><p>http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="2,144.73,646.48,131.80,7.47"><p>http://cassandra.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="2,144.73,657.44,150.63,7.47"><p>http://www.ncbi.nlm.nih.gov/pmc/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="3,144.73,657.44,160.04,7.47"><p>http://fiji.sc/wiki/index.php/Fiji</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Acknowledgments</head><p>The research leading to these results has received funding from the <rs type="funder">European Union</rs>'s <rs type="programName">Seventh Framework Programme</rs> under grant agreement <rs type="grantNumber">257528</rs> (<rs type="grantNumber">KHRES-MOI</rs>), <rs type="grantNumber">249008</rs> (Chorus+) and <rs type="grantNumber">258191</rs> (Promise).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5KCu4tW">
					<idno type="grant-number">257528</idno>
					<orgName type="program" subtype="full">Seventh Framework Programme</orgName>
				</org>
				<org type="funding" xml:id="_Fvq9k2q">
					<idno type="grant-number">KHRES-MOI</idno>
				</org>
				<org type="funding" xml:id="_wk3rccq">
					<idno type="grant-number">249008</idno>
				</org>
				<org type="funding" xml:id="_N2NPg7K">
					<idno type="grant-number">258191</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.95,645.84,337.54,7.86;10,151.52,656.80,328.96,7.86;11,151.52,119.68,328.97,7.86;11,151.52,130.64,329.01,7.86;11,151.52,141.60,295.25,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,311.31,645.84,169.18,7.86;10,151.52,656.80,100.95,7.86">The CLEF cross-language image retrieval track (ImageCLEF) 2004</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,268.23,119.68,212.26,7.86;11,151.52,130.64,222.55,7.86">Multilingual Information Access for Text, Speech and Images: Result of the fifth CLEF evaluation campaign</title>
		<title level="s" coord="11,450.17,130.64,30.36,7.86;11,151.52,141.60,135.39,7.86">Lecture Notes in Computer Science (LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<meeting><address><addrLine>Bath, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3491</biblScope>
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,151.92,337.48,7.86;11,151.52,162.88,328.97,7.86;11,151.52,173.84,329.02,7.86;11,151.52,184.80,296.81,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,216.48,162.88,264.01,7.86;11,151.52,173.84,57.15,7.86">Overview of the ImageCLEFmed 2007 medical retrieval and annotation tasks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Deserno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,235.50,173.84,98.12,7.86">CLEF 2007 Proceedings</title>
		<title level="s" coord="11,411.08,173.84,69.46,7.86;11,151.52,184.80,98.74,7.86">Lecture Notes in Computer Science (LNCS</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5152</biblScope>
			<biblScope unit="page" from="473" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,195.12,337.52,7.86;11,151.52,206.08,328.95,7.86;11,151.52,217.04,329.03,7.86;11,151.52,227.99,22.02,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,205.86,206.08,255.67,7.86">The CLEF 2011 medical image retrieval and classification tasks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,151.52,217.04,274.36,7.86">Working Notes of CLEF 2011 (Cross Language Evaluation Forum)</title>
		<imprint>
			<date type="published" when="2011-09">September 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,238.32,337.54,7.86;11,151.52,249.28,328.96,7.86;11,151.52,260.23,329.02,7.86;11,151.52,271.19,71.19,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,204.90,249.28,275.58,7.86;11,151.52,260.23,37.75,7.86">Overview of the imageclef 2012 medical image retrieval and classication tasks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,210.55,260.23,265.36,7.86">Working Notes of CLEF 2012 (Cross Language Evaluation Forum)</title>
		<imprint>
			<date type="published" when="2012-09">September 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,281.52,337.50,7.86;11,151.52,292.47,328.99,7.86;11,151.52,303.43,329.00,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,388.64,281.52,91.81,7.86;11,151.52,292.47,207.47,7.86">ImageCLEF -Experimental Evaluation in Visual Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,426.59,292.47,53.92,7.86;11,151.52,303.43,182.41,7.86">The Springer International Series On Information Retrieval</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,313.76,337.55,7.86;11,151.52,324.68,223.82,7.89" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,206.88,313.76,230.84,7.86">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,448.20,313.76,32.30,7.86;11,151.52,324.71,138.94,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,335.04,337.55,7.86;11,151.52,346.00,329.01,7.86;11,151.52,356.95,75.93,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,362.70,335.04,117.80,7.86;11,151.52,346.00,120.44,7.86">Bag-of-colors for biomedical document image classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Markonis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,296.77,346.00,127.93,7.86">MICCAI workshop MCBR-CDS</title>
		<imprint>
			<date type="published" when="2012-09">September 2012</date>
		</imprint>
	</monogr>
	<note>Forthcoming</note>
</biblStruct>

<biblStruct coords="11,142.95,367.28,337.57,7.86;11,151.52,378.24,329.01,7.86;11,151.52,389.20,329.00,7.86;11,151.52,400.15,102.15,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,251.36,367.28,229.16,7.86;11,151.52,378.24,52.08,7.86">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,230.62,378.24,249.91,7.86;11,151.52,389.20,159.28,7.86">Proceedings of the Ninth IEEE International Conference on Computer Vision -Volume 2. ICCV &apos;03</title>
		<meeting>the Ninth IEEE International Conference on Computer Vision -Volume 2. ICCV &apos;03<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1470" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,410.48,293.69,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="11,197.82,410.48,119.87,7.86">Hadoop: The Definitive Guide</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>White</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>O&apos;Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,420.80,337.88,7.86;11,151.52,431.73,329.08,7.89" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,253.20,420.80,223.64,7.86">MapReduce: simplified data processing on large clusters</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,151.52,431.76,218.62,7.86">Communications of the ACM -50th anniversary issue</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="2008-01">January 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,442.08,337.90,7.86;11,151.52,453.04,329.00,7.86;11,151.52,464.00,197.09,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,264.30,442.08,216.21,7.86;11,151.52,453.04,83.05,7.86">An efficient approach to clustering in large multimedia databases with noise</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hinneburg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,258.83,453.04,221.69,7.86;11,151.52,464.00,25.81,7.86">Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">5865</biblScope>
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,474.32,337.87,7.86;11,151.52,485.28,269.82,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,394.71,474.32,85.77,7.86;11,151.52,485.28,76.64,7.86">The medGIFT group in ImageCLEFmed</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Markonis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G</forename><surname>Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,270.38,485.28,118.09,7.86">Working Notes of CLEF 2011</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,495.60,337.90,7.86;11,151.52,506.56,329.02,7.86;11,151.52,517.52,184.58,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,299.85,495.60,160.93,7.86">Bag-of-colors for improved image search</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Wengert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>JÃ©gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,151.52,506.56,325.44,7.86">Proceedings of the 19th ACM international conference on Multimedia. MM &apos;11</title>
		<meeting>the 19th ACM international conference on Multimedia. MM &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1437" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,527.84,337.86,7.86;11,151.52,538.77,126.78,7.89" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,267.30,527.84,84.44,7.86">Digital color imaging</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Trussell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,362.07,527.84,118.40,7.86;11,151.52,538.80,42.47,7.86">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="901" to="932" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,549.12,337.90,7.86;11,151.52,560.08,189.40,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,275.36,549.12,205.14,7.86;11,151.52,560.08,131.23,7.86">Analysis of color feature extraction techniques for pathology image retrieval system</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nallaperumal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,290.12,560.08,22.13,7.86">IEEE</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,570.40,337.89,7.86;11,151.52,581.33,100.34,7.89" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,270.04,570.40,58.34,7.86">Color indexing</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,338.66,570.40,141.84,7.86;11,151.52,581.36,25.38,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="32" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,591.69,337.88,7.86;11,151.52,602.64,329.01,7.86;11,151.52,613.60,329.03,7.86;11,151.52,624.56,315.71,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,291.00,591.69,189.49,7.86;11,151.52,602.64,273.23,7.86">The ImageCLEF medical retrieval task at icpr 2010 -information fusion to combine viusal and textual information</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,445.31,602.64,35.22,7.86;11,151.52,613.60,291.91,7.86">Proceedings of the International Conference on Pattern Recognition (ICPR 2010)</title>
		<title level="s" coord="11,450.20,613.60,30.36,7.86;11,151.52,624.56,141.73,7.86">Lecture Notes in Computer Science (LNCS)</title>
		<meeting>the International Conference on Pattern Recognition (ICPR 2010)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010-08">August 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,634.88,337.88,7.86;11,151.52,645.84,329.02,7.86;11,151.52,656.80,188.83,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="11,317.74,634.88,162.75,7.86;11,151.52,645.84,109.81,7.86">Information fusion for combining visual and textual image retrieval</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Depeursinge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,285.35,645.84,195.20,7.86;11,151.52,656.80,78.87,7.86">20th IEEE International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2010-08">August 2010</date>
			<biblScope unit="page" from="1590" to="1593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,119.68,337.87,7.86;12,151.52,130.64,110.87,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="12,254.29,119.68,137.15,7.86">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,419.15,119.68,61.33,7.86;12,151.52,130.64,42.60,7.86">Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,141.60,337.87,7.86;12,151.52,152.55,329.01,7.86;12,151.52,163.51,329.01,7.86;12,151.52,174.47,214.04,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="12,337.99,141.60,142.49,7.86;12,151.52,152.55,199.13,7.86">Reciprocal rank fusion outperforms condorcet and individual rank learning methods</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>BÃ¼ttcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,379.19,152.55,101.33,7.86;12,151.52,163.51,329.01,7.86;12,151.52,174.47,32.06,7.86">Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 32nd international ACM SIGIR conference on Research and development in information retrieval<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="758" to="759" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
