<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,149.13,116.95,317.10,12.62;1,201.57,134.89,212.21,12.62">The PRA and AmILAB at ImageCLEF 2012 Photo Flickr Annotation Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,148.20,172.56,46.85,8.74"><forename type="first">Luca</forename><surname>Piras</surname></persName>
							<email>luca.piras@diee.unica.it</email>
							<affiliation key="aff0">
								<orgName type="department">DIEE -Department of Electric and Electronic Engineering</orgName>
								<orgName type="institution">University of Cagliari</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,205.61,172.56,66.61,8.74"><forename type="first">Roberto</forename><surname>Tronci</surname></persName>
							<email>roberto.tronci@diee.unica.it</email>
							<affiliation key="aff0">
								<orgName type="department">DIEE -Department of Electric and Electronic Engineering</orgName>
								<orgName type="institution">University of Cagliari</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">AmILAB -Laboratorio Intelligenza d&apos;Ambiente Ricerche</orgName>
								<address>
									<settlement>Sardegna</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,289.11,172.56,71.25,8.74"><forename type="first">Gabriele</forename><surname>Murgia</surname></persName>
							<email>gabriele.murgia@sardegnaricerche.it</email>
							<affiliation key="aff1">
								<orgName type="institution">AmILAB -Laboratorio Intelligenza d&apos;Ambiente Ricerche</orgName>
								<address>
									<settlement>Sardegna</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,390.30,172.56,72.39,8.74"><forename type="first">Giorgio</forename><surname>Giacinto</surname></persName>
							<email>giacinto@diee.unica.it</email>
							<affiliation key="aff0">
								<orgName type="department">DIEE -Department of Electric and Electronic Engineering</orgName>
								<orgName type="institution">University of Cagliari</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,149.13,116.95,317.10,12.62;1,201.57,134.89,212.21,12.62">The PRA and AmILAB at ImageCLEF 2012 Photo Flickr Annotation Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">39584E2A4217B81D349C77762B3E589F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>image annotation</term>
					<term>dynamic score combination</term>
					<term>SVM</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the first participation of the Pattern Recognition and Application Group (PRA Group), and the Ambient Intelligence Lab (AmILAB) at the ImageCLEF 2012 Photo Flickr Concept Annotation Task. In this task, the teams' goal is to detect the presence of 94 concepts in the images, and to provide a confidence score related to the confidence of the decision of each concept detector. We faced the challenge by relying on visual information only, combining different image descriptors by means of different score combination techniques. Experimental results show that just combining concept detectors not specifically designed for handling the large variety of concepts does not allow reaching satisfactory results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The visual concept annotation task is a multi-label classification challenge where the goal consists in the analysis of a collection of photos in order to detect the presence of one or more concepts. The number of selected concepts is equal to 94, and their semantics cover a wide range. They include categories related to persons (e.g. baby, child, teenager, adult), animals (e.g. cat, dog, horse), and sentiments (e.g. unpleasant, euphoric). In addition to the images and the associated concepts, participants are provided with textual features, and visual features. Our main objective to solve this task is to use the combination of outputs of visual concept detectors based on visual descriptors. A detailed overview of the data set, and the related task can be found in <ref type="bibr" coords="1,366.81,572.13,9.96,8.74" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Visual features</head><p>For this task, a subset of the MIRFLICKR 3 collection has been used. This subset comprises 25 thousand images that have been manually annotated using a limited number of concepts. With respect to the previous editions of the competition, this year the annotation process has been carried out by resorting to crowdsourcing mechanisms. Several concepts have been reused of last year's task, and, for most of these concepts, the remaining photos of the MIRFLICKR-25K collection that had not yet been used in the previous task, have been annotated. In order to boost the quality of all 25,000 images, they have been reannotated for several concepts too. All the images have been naturally annotated for the new concepts. All images have been accompanied by different kind of features: textual, and visual features. Detailed information about the feature sets can be found in <ref type="bibr" coords="2,174.33,227.59,9.96,8.74" target="#b3">[4]</ref>.</p><p>In our approach, we focused on visual descriptors only. The visual descriptors proposed for this task are the following: sift, c-sift, RGB-sift, and Opponentsift. For each descriptor, the histogram of the occurrence frequencies has been extracted by using the Color Descriptors toolkit <ref type="bibr" coords="2,353.35,275.49,9.96,8.74" target="#b2">[3]</ref>. As expected, the K-means clustering used to produce a "bag of visual words" representation, is quite slow for the data sizes at hand, as clustering 250,000 points takes at least 12 hours per iteration. The solution usually proposed is to reduce the number of points to cluster. By default, the toolkit extracts 250,000 points regardless of the number of training images, thus reducing the number of point per image automatically as the number of images increase. It means that the toolkit extracts less than 17 points per image, thus loosing dozens of descriptors. At the same time, if up to 200 points are extracted for each of 15,000 training images, the K-means algorithm should cluster 3,000,000 points! For this reasons we decided to divide the 15,000 training images into four groups, by retaining the same proportion of image per concept as in the whole training set. Then, for each group, we clustered around 750,000 points in order to obtain four different codebooks (one codebook for each descriptor) with 2,048 visual words. Each codebook has then be used to produce the Bag of Visual Words descriptors. This procedure allowed obtaining a large vocabulary of "visual words", and at the same time reduced the number of points to cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Concept detection by dynamic combination of visual classifiers</head><p>We submitted three runs in total. All of these runs are based only on the bag of visual words descriptors illustrated in the previous section. For all the runs, we used the Multiple Classifier System paradigm, and the Support Vector Machine has been used as the base classifier for its good performances on various image classification tasks <ref type="bibr" coords="2,340.48,585.30,9.96,8.74" target="#b0">[1]</ref>. We trained a single SVM for each global image descriptor and each visual concept. Thus, for each concept i, a set of four SVMs {S sif t i , S rgb i , S color i , S opponent i } is available. We classified all the pattern of the test set by means of these sets of SVMs. Thus, for each test pattern x j , we obtained as output the class decision d k ij taken by the classifier k (i.e., 1 if the pattern belongs or not to the concept i, 0 otherwise), and the distance from the decision border is transformed through a min-max normalization into a classification score s k ij , in the range [0, 1], of the test pattern with respect to the concept. We used the following three combination rules:</p><p>-The Mean rule -The Dynamic Score Selection by Majority Vote -The Dynamic Score Selection by Mean rule</p><p>For the Mean rule, we computed the average of the classification scores obtained from the classifiers <ref type="bibr" coords="3,249.83,212.94,9.96,8.74" target="#b1">[2]</ref>:</p><formula xml:id="formula_0" coords="3,266.99,229.77,213.60,26.88">s mean ij = 1 k • k s k ij<label>(1)</label></formula><p>In the case of the other two combination rules, we used the Dynamic Score Selection (DSC) approach <ref type="bibr" coords="3,251.16,276.28,9.96,8.74" target="#b4">[5]</ref>:</p><formula xml:id="formula_1" coords="3,221.35,293.09,259.24,16.73">s dsc ij = (1 -α) • min k {s k ij } + α • max k {s k ij }<label>(2)</label></formula><p>This combination rule is able to perform a dynamic combination at the score level, by allowing to dynamically chose the best scores and weights to be combined. In <ref type="bibr" coords="3,177.93,341.41,10.52,8.74" target="#b4">[5]</ref> different methods to compute dynamically the weights α are proposed. In these runs, we used one of those methods, and one that has been specifically designed for the task at hand. The rule for computing α for the Dynamic Score Selection by Majority Vote is the following:</p><formula xml:id="formula_2" coords="3,201.56,405.98,279.04,22.63">α = 1, if at least half of the d k ij are equal to 1 0, otherwise<label>(3)</label></formula><p>The rule for computing α for the Dynamic Score Selection by Mean Rule is the following:</p><formula xml:id="formula_3" coords="3,276.42,459.61,204.17,26.88">α = 1 k • k s k ij (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>The performances (Interpolated Mean Average Precision (MiAP), Interpolated Geometric Mean Average Precision (GMiAP), and F1-measure on all concepts related to our runs are listed in Table <ref type="table" coords="3,311.20,549.52,3.87,8.74" target="#tab_0">1</ref>, and they are compared to the performances obtained by the other team that used visual features only. Detailed information about the evaluation process can be found in <ref type="bibr" coords="3,388.42,573.43,9.96,8.74" target="#b3">[4]</ref>.</p><p>A first conclusion that can be drawn from Table <ref type="table" coords="3,356.80,585.38,4.98,8.74" target="#tab_0">1</ref> is that using a combination of general purpose classifiers does not permit to obtain very satisfactory results, as we obtained just the tenth position out of thirteen participants.</p><p>The proposed results also show that the Dynamic Score Selection by Majority Vote does not work as expected, as it is outperformed by the Dynamic Score Selection by Mean Rule for the MiAP and GMiAP measures, and by the Mean rule for the F1-measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0"</head><p>0,1" 0,2" 0,3" 0,4" 0,5" 0,6" 0,7" 0,8" 0,9" </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In our participation to the ImageCLEF photo annotation task, multiple visual features has been used for representing the images. We combine the different information using the Bag-of-Words model taking care that a number of image descriptor big enough was used for each image. After the BoW extraction, we combined the four feature spaces in three different ways. The evaluation results showed that a simple combination of different feature spaces using classifiers not specifically designed for taking into account the big variety of concepts is not able to reach satisfactory results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,152.46,135.51,310.44,501.76"><head>Table 1 .</head><label>1</label><figDesc>Interpolated Mean Average Precision (MiAP), Interpolated Geometric Mean Average Precision (GMiAP), and F1-measure of teams' annotations on all concepts combined</figDesc><table coords="4,403.62,141.23,2.00,1.45"><row><cell>1"</cell></row></table><note coords="4,293.15,135.51,27.21,3.97;4,411.75,368.03,26.47,3.19;4,424.25,376.47,13.97,3.19;4,407.82,384.92,30.40,3.19;4,426.62,393.36,11.60,3.19;4,152.46,629.38,310.44,7.89"><p><p>Average'Precision'</p>Dynamic"Mean"Rule" Mean"rule" Dynamic"Majority"Vote" MEDIAN" Fig. 1. Interpolated Mean Average Precision (MiAP) for each of 94 concepts</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="1,144.73,657.79,122.33,7.86"><p>http://press.liacs.nl/mirflickr/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,138.35,531.05,342.24,7.86;5,146.91,542.01,302.21,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="5,281.91,531.05,198.68,7.86;5,146.91,542.01,153.32,7.86">An Introduction to Support Vector Machines and Other Kernel-based Learning Methods</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,552.30,342.25,7.86;5,146.91,563.26,228.81,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,351.60,552.30,99.10,7.86">On combining classifiers</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hatef</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,458.46,552.30,22.14,7.86;5,146.91,563.26,138.20,7.86">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="226" to="239" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,573.55,342.24,7.86;5,146.91,584.51,333.68,7.86;5,146.91,595.47,47.11,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,353.55,573.55,127.05,7.86;5,146.91,584.51,112.43,7.86">Evaluating color descriptors for object and scene recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,266.11,584.51,162.22,7.86">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1582" to="1596" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,605.76,342.24,7.86;5,146.91,616.72,263.58,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,255.35,605.76,225.24,7.86;5,146.91,616.72,69.46,7.86">Overview of the imageclef 2012 flickr photo annotation and retrieval task</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,223.30,616.72,103.78,7.86">CLEF 2012 working notes</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,138.35,627.01,342.25,7.86;5,146.91,637.97,333.67,7.86;5,146.91,648.93,169.48,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,289.54,627.01,191.06,7.86;5,146.91,637.97,163.04,7.86">Dynamic score combination: A supervised and unsupervised score combination method</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tronci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="5,318.99,637.97,161.60,7.86;5,146.91,648.93,81.43,7.86">Machine Learning and Data Mining in Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">5632</biblScope>
			<biblScope unit="page" from="163" to="177" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
