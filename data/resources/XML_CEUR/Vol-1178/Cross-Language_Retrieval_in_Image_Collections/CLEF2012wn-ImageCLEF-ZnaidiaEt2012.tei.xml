<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,144.95,115.90,325.46,12.90;1,229.23,133.83,156.89,12.90">CEA LIST&apos;s participation to the Concept Annotation Task of ImageCLEF 2012</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,167.67,171.88,53.44,8.64"><forename type="first">Amel</forename><surname>Znaidia</surname></persName>
							<email>amel.znaidia@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="department">LIST</orgName>
								<orgName type="laboratory">Laboratory of Vision and Content Engineering</orgName>
								<orgName type="institution">CEA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,227.71,171.88,58.66,8.64"><forename type="first">Aymen</forename><surname>Shabou</surname></persName>
							<email>aymen.shabou@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="department">LIST</orgName>
								<orgName type="laboratory">Laboratory of Vision and Content Engineering</orgName>
								<orgName type="institution">CEA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,293.49,171.88,61.39,8.64"><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
							<email>adrian.popescu@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="department">LIST</orgName>
								<orgName type="laboratory">Laboratory of Vision and Content Engineering</orgName>
								<orgName type="institution">CEA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,378.71,171.88,68.98,8.64"><forename type="first">Hervé</forename><surname>Le Borgne</surname></persName>
							<email>herve.le-borgne@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="department">LIST</orgName>
								<orgName type="laboratory">Laboratory of Vision and Content Engineering</orgName>
								<orgName type="institution">CEA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,144.95,115.90,325.46,12.90;1,229.23,133.83,156.89,12.90">CEA LIST&apos;s participation to the Concept Annotation Task of ImageCLEF 2012</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5A1D337E618A257AB072D0357CC06623</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multimedia fusion</term>
					<term>Bag-of-Visual-Words</term>
					<term>Bag-of-Multimedia-Words</term>
					<term>image annotation</term>
					<term>classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation to the ImageCLEF2012 Photo Annotation Task. We focus on how to use the tags associated to the images to improve the annotation performance. We submitted one textual-only and three multimodal runs. Our first textual model <ref type="bibr" coords="1,314.71,280.61,14.94,7.77" target="#b13">[14]</ref> is based on the local soft coding of images tags over a dictionary of most frequent tags. A second model of tag is an adaptation of the TF-IDF model to the social space in order to compute the social relatedness of two tags <ref type="bibr" coords="1,270.51,313.49,10.18,7.77" target="#b8">[9]</ref>. For the fusion we used a trainable combiner, called stacked generalization <ref type="bibr" coords="1,271.87,324.45,14.94,7.77" target="#b11">[12]</ref> which uses predictions from base classifiers to learn a new model. Results have shown that combination of textual and visual features can improve the annotation performance significantly. Our best run achieves 41.59 % in terms of MAP, allowing us to rank 3 rd team.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ImageCLEF 2012 Photo Annotation Task <ref type="bibr" coords="1,324.90,452.60,11.62,8.64" target="#b7">[8]</ref> is a multi-label classification problem, with 15.000 image for training, 10.000 for testing and 94 concepts to detect. Images are extracted from the MIR Flickr dataset <ref type="bibr" coords="1,321.78,476.51,11.62,8.64" target="#b3">[4]</ref> and the Flickr user tags and/or EXIF information are available for most photos.</p><p>In our participation to the ImageCLEF Photo Annotation Task, we focus on how to use the tags associated to the images to enhance the annotation performance. We propose three different models: textual only and two multimodal models.</p><p>This paper is organized as follows. In Section 2 we describe our visual features. In Section 3 we give an overview of our textual model which uses user tags. Then in Section 4 we present in more detail the experiments we did, the submitted runs and the obtained results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Visual Features 2.1 Bag-of-Visual-Words model</head><p>In recent works addressing object recognition and scene classification tasks, the Bagof-Visual-Words (BoVW) is one of the most popular model for feature design. Given an image, its visual features are built in three steps (i) codebook learning, (ii) local features coding and (iii) pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Codebook Learning</head><p>The codebook, which entries are termed codewords, is a collection of basic patterns used to reconstruct the input local features. A simple way to generate the codebook is to use clustering based methods such as K-means <ref type="bibr" coords="2,357.59,184.72,10.58,8.64" target="#b4">[5]</ref>. In the rest of this paper, we denote by</p><formula xml:id="formula_0" coords="2,193.18,194.78,287.42,11.23">B = b k ; b k ∈ d ; k = 1, ..., K a codebook of K codeword vectors,</formula><p>which is learned on a training subset of local features x i ; x i ∈ d ; i = 1, ..., N extracted from the learning dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Coding</head><p>For each image dense local descriptors (such as SIFTS <ref type="bibr" coords="2,366.69,245.02,11.20,8.64" target="#b6">[7]</ref>) are extracted and mapped to codes. Following recent observations in scene classification, we chose to implement the locality-constraint coding based on local soft coding <ref type="bibr" coords="2,407.30,268.93,10.58,8.64" target="#b5">[6]</ref>, because of its effectiveness and robustness toward quantization errors. In <ref type="bibr" coords="2,385.99,280.89,10.58,8.64" target="#b5">[6]</ref>, authors propose another efficient implementation of the locality-constrained coding by restricting the probabilistic soft coding approach <ref type="bibr" coords="2,291.47,304.80,11.62,8.64" target="#b2">[3]</ref> to only the M -nearest-codewords to a local feature, i.e.,</p><formula xml:id="formula_1" coords="2,211.63,333.08,268.96,30.19">z i,j = exp (-β||xi-bj || 2 2 ) M k=1 exp (-β||xi-b k || 2 2 ) if b j ∈ N M (x i ) ,<label>0 otherwise, (1)</label></formula><p>where N M (x i ) denotes the M -nearest neighborhood of x i , under the Euclidean distance for instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pooling</head><p>Given the coding coefficients of all local features within one image, a pooling operation has to be performed to obtain a compact signature h, while preserving important information and discarding irrelevant details. This operation can be formulated as the following:</p><formula xml:id="formula_2" coords="2,214.68,468.20,265.91,9.65">h j = g z i,j ; i ∈ {1, ..., N } ; ∀j ∈ {1, ..., K} ,<label>(2)</label></formula><p>with g a pooling function such as the average, the sum or the maximum functions.</p><p>The sum-pooling is the sum of the coding coefficients obtained on local features while the average-pooling is its normalized form. Both have been usually considered in the original BoW model. Recent works <ref type="bibr" coords="2,343.58,526.27,10.79,8.64" target="#b1">[2,</ref><ref type="bibr" coords="2,356.03,526.27,8.30,8.64" target="#b5">6]</ref> show, both theoretically and empirically, that max-pooling is best suited to the recognition task. Max-pooling is obtained by selecting the maximum coding coefficient (or codeword response) over local features for each codeword. Furthermore, since the classic BoVW is an orderless signature that disregards the location of the visual words in the image, the spatial pyramid matching (SPM) <ref type="bibr" coords="2,468.98,585.58,11.62,8.64" target="#b4">[5]</ref> is an interesting way to incorporate some global spatial contextual information into the signature. The image is divided into P different regions and a pooling is conducted in each of them. The final signature is then obtained by a concatenation of all the region-relative R i signatures, i.e.,</p><formula xml:id="formula_3" coords="2,261.36,650.58,219.23,13.31">h = [h T R1 , h T R2 , ..., h T R P ] T .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Bag-of-Multimedia-Words model</head><p>The Bag-of-Multimedia-Words is a method of early fusion that combines textual and visual features. Since the late fusion method presented in section 2.1 gives better result, we do not present this method in this paper and refer to <ref type="bibr" coords="3,357.23,164.10,16.60,8.64" target="#b14">[15]</ref> for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Textual Features</head><p>It is commonly accepted <ref type="bibr" coords="3,236.61,221.81,16.60,8.64" target="#b9">[10]</ref> that visual features alone do not convey a high level semantic description of image content. In order to build robust BoW based tag-signatures toward quantization errors, we rely on the locality-constrained coding method that has proved to be effective for visual features when paired with max-pooling. This model is detailed in <ref type="bibr" coords="3,178.24,269.63,15.27,8.64" target="#b13">[14]</ref>. The coding step of a given tag over a codebook requires a tag-similarity measure.</p><p>The two similarity measures that we detail below, capture complementary facets of tags and their combination improves the quality of predicted tags.</p><p>-Hierarchical similarity:</p><p>WordNet concepts are structured as synsets (sets of synonyms) that are arranged as a hierarchy whose main structural axis is defined by conceptual inheritance. Wu-Palmer measure <ref type="bibr" coords="3,219.83,364.42,16.60,8.64" target="#b12">[13]</ref> gives a similarity between two tags as their distance in the WordNet hierarchy. Since a tag can belong to more than one synset in WordNet (i.e., can have more than one meaning), we opt to determine the semantic relationship between two tags t 1 and t 2 as the maximum Wu-Palmer similarity between the synset or the synsets that include syns(t 1 ) and syns(t 2 ):</p><formula xml:id="formula_4" coords="3,161.66,461.48,318.93,25.72">sim hierarchical (t 1 , t 2 ) = max sim wup (s 1 , s 2 ) ; (s 1 , s 2 ) ∈ syns(t 1 ) × syns(t 2 ) ,<label>(4)</label></formula><p>where sim wup is the Wu-Palmer similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-Contextual similarity</head><p>In <ref type="bibr" coords="3,162.80,524.27,10.58,8.64" target="#b8">[9]</ref>, an adaptation of the TF-IDF model to the social space is proposed in order to compute the social relatedness of two tags.</p><p>Let S be the matrix of size N × K defined by:</p><formula xml:id="formula_5" coords="3,229.18,564.29,251.41,21.34">S(i, j) = users(ti, tj) × log( userscollection users collection(t j ) ) ,<label>(5)</label></formula><p>where t i is the target tag, t j is an element of the codebook, users(t i , t j ) is the number of distinct users who associate the tag t i to the tag t j among the top results returned by the Flickr API for t i ; users collection(tj ) is the number of distinct users from a pre-fetched subset of Flickr users that have tagged photos with tag t j , and N is the number of unique tags associated to photos of the dataset and K is the size of the codebook.</p><p>Relying on this matrix, a Flickr model for a given tag t i is proposed in <ref type="bibr" coords="4,442.54,119.31,11.62,8.64" target="#b8">[9]</ref> as the following vector of weights:</p><formula xml:id="formula_6" coords="4,258.01,149.27,222.58,11.72">w i = [w i,1 , w i,2 , ..., w i,K ] T ,<label>(6)</label></formula><p>with w i,j the normalized social weight defined by:</p><formula xml:id="formula_7" coords="4,241.17,191.47,239.42,22.34">w i,j = S(i, j) max{S(i, k) , k = 1, ..., K} .<label>(7)</label></formula><p>Thereby, given two tag-Flickr models w i and w j , we compute the contextual similarities between their related tags t i and t j using the cosine similarity:</p><formula xml:id="formula_8" coords="4,246.77,254.55,229.95,24.80">sim contextual (t i , t j ) = w T i w j ||w i ||||w j || . (<label>8</label></formula><formula xml:id="formula_9" coords="4,476.72,263.18,3.87,8.64">)</formula><p>Coding/pooling Once the similarity measures are calculated, we perform local soft coding for each t i in order to achieve the assignment step. Consequently, a tag is mapped to only its M-nearest tags under a similarity measure.</p><formula xml:id="formula_10" coords="4,227.84,337.26,252.75,23.33">z i,j = sim(t i , b j ) if b j ∈ N M (t i ) , 0 otherwise,<label>(9)</label></formula><p>where N M (t i ) denotes the M -nearest neighbors of t i , under the hierarchical or the contextual similarity denoted by sim(t i , b j ). The locality assumption in the tag-space induces sparse codes while reducing the reconstruction errors, mainly in terms of semantic reconstruction.</p><p>Given the tag-related codes within one image, a max-pooling is performed in order to obtain the final tag-signature vector. In our case, separate signatures are generated considering each similarity measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Classifier Fusion</head><p>A linear SVM classifier is used for the features obtained from each modality. To combine classifiers learned on different modalities and/or features, we use a trainable combiner, called stacked generalization, originally introduced in <ref type="bibr" coords="4,388.39,516.79,15.27,8.64" target="#b11">[12]</ref>. It is an ensemble learning technique, which aims to increase the performance of individual classifiers by combining them in a hierarchical architecture. The key idea is to learn a meta-level (level-1) classifier based on the outputs of base-level (level-0) classifiers, estimated via cross-validation. An example of combination of one visual and two textual classifiers is presented in Figure <ref type="figure" coords="4,214.18,576.57,3.74,8.64" target="#fig_0">1</ref>.</p><p>Given a training dataset D = {(x F i , y i ), i = 1, ..., n} where x F i is the F-feature vector among the visual (V-feature), the contextual tag (C-feature) and the hierarchical tag (S-feature) and y i is the associated vector of labels, the algorithm operates as follows:</p><p>1. A K-fold cross-validation process randomly splits D into disjoint parts of almost equal size D 1 , ..., D K ; 4 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Submitted runs</head><p>We submitted four runs to the campaign, allowing relevant comparison between methods:</p><p>-textual tagflickr tagwordnet uses only the textual feature described in section 3.</p><p>The codebook size is fixed to 2500 (resp. 5134) for the hierarchical (resp. contextual) similarity. the codewords for the soft assignment. The optimal size of the neighborhood has been estimated by cross-validation on the training dataset leading to a number of 5 (resp. 50) neighboring codewords for the hierarchical (resp. contextual) based tag-distance measures. A one-versus-all linear kernel based Support Vector Machine (SVM) classier is used, for each measure. They are combined using the stack generalization using a 10-fold cross validation. -multimedia visualrootsift tagflickr tagwordnet This run is a combination of the previous textual run and the Bag-Of-Visual-Words model detailed in section 2.1.</p><p>The pipeline is as follows :</p><p>• Local visual descriptors: dense SIFTs of size 128 are extracted within a regular spatial grid and only one scale. The patch-size is fixed to 16 × 16 pixels and the step-size for dense sampling to 6 pixels; • Codebook: a visual codebook of size 4, 000 is created using the K-means clustering method on a randomly selected subset of SIFTs from the training dataset (∼ 10 5 SIFTs). • Coding/pooling: for coding the local visual descriptors SIFTS, we also fix the patch-size to 16 × 16 pixels and the step-size for dense sampling to 6 pixels. Then for the extracted visual descriptors associated to one image, we consider a neighborhood in the visual feature space of size 5 for local soft coding and the softness parameter β is set to 10. The max-pooling operation is performed to aggregate the obtained codes and a spatial pyramid decomposition into 3 levels (1 × 1, 2 × 2, 3 × 3) is adopted for the visual-signature.</p><p>A one-versus-all linear kernel based Support Vector Machine (SVM) classifier is used, since it has shown good performances in scene categorization task when paired with the max-pooling operation on local features <ref type="bibr" coords="6,416.83,404.79,15.77,8.64" target="#b10">[11,</ref><ref type="bibr" coords="6,434.26,404.79,7.19,8.64" target="#b5">6]</ref>. • Classifier fusion: base classifiers are trained on the considered modalities (visual, contextual and hierarchical) and combined by the stack generalization approach using 10-cross-validations on the training set as shown in figure <ref type="figure" coords="6,463.94,442.15,3.74,8.64" target="#fig_0">1</ref>.</p><p>-multimedia visualcsift tagflickr tagwordnet Is the same run as the previous one except the SIFT version. In this run, we use a colored SIFT (CSIFT) <ref type="bibr" coords="6,426.17,470.56,10.58,8.64" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Discussion of Submission Outcomes</head><p>The official results of our runs are illustrated in Table <ref type="table" coords="6,347.98,535.38,3.74,8.64" target="#tab_0">1</ref>. Among the multimodal runs (2 and 3), we notice that using a Colored SIFT works better than the conventional SIFT. The multimodal run (run 4) scores shows the competitive performances of the Bagof-Multimedia-Words, ensuring a trade-off between classification accuracy and computation cost. This model is easier to scale for large-scale datasets since it achieves comparable performances compared to the other multimodal runs while using only a feature vector of size 512.</p><p>The first purely textual submission is the combination of the semantic and the contextual classifiers detailed in section 3. Its performance was almost identical to the best textual submission of LIRIS ECL Group (the best MAP in the textual modality) as shown in Table <ref type="table" coords="6,197.64,656.44,3.74,8.64" target="#tab_1">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgment</head><p>This work is supported by grants from DIGITEO and Région Ile-de-France, and has been partially funded by I2S in the context of the project Polinum. We acknowledge support from the French ANR (Agence Nationale de la Recherche) via the YOJI (ANR-09-CORD-104) and PERIPLUS (ANR-10-CORD-026) projects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,134.77,329.00,345.83,8.12;5,134.77,340.31,111.25,7.77;5,137.60,115.84,340.17,198.43"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Left Part is the training scheme of the meta-level classifier and right part is the classification using stacking framework.</figDesc><graphic coords="5,137.60,115.84,340.17,198.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,201.37,656.44,3.74,8.64"><head>Table 1 .</head><label>1</label><figDesc>. Overview of the different submissions.</figDesc><table coords="7,150.45,118.07,310.78,115.97"><row><cell>Run</cell><cell cols="2">Modality MAP GMAP F-ex</cell></row><row><cell>1: textual tagflickr tagwordnet</cell><cell>Textual</cell><cell>0.3314 0.2698 0.4452</cell></row><row><cell cols="3">2: multimedia visualrootsift tagflickr tagwordnet Multimodal 0.4086 0.3472 0.5374</cell></row><row><cell cols="3">3: multimedia visualcsift tagflickr tagwordnet Multimodal 0.4159 0.3615 0.5404</cell></row><row><cell>4: multimedia bomw</cell><cell cols="2">Multimodal 0.4084 0.3487 0.5295</cell></row><row><cell>Run</cell><cell cols="2">MAP GMAP F-ex</cell></row><row><cell>Our textual run</cell><cell cols="2">0.3314 0.2698 0.4452</cell></row><row><cell>LIRIS ECL textual run</cell><cell cols="2">0.3338 0.2759 0.4691</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,178.59,236.78,258.18,8.12"><head>Table 2 .</head><label>2</label><figDesc>Comparison of our textual submission and the best textual one.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,142.61,401.99,337.98,7.77;7,150.95,412.95,329.64,7.77;7,150.95,423.91,329.64,7.77;7,150.95,434.86,108.12,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,276.63,401.99,203.96,7.77;7,150.95,412.95,11.76,7.77">Csift: A sift descriptor with color invariant characteristics</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">E</forename><surname>Abdel-Hakim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Farag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,181.13,412.95,299.47,7.77;7,150.95,423.91,87.86,7.77;7,347.51,423.91,36.97,7.77">Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1978" to="1983" />
		</imprint>
	</monogr>
	<note>CVPR &apos;06</note>
</biblStruct>

<biblStruct coords="7,142.61,446.16,337.98,7.77;7,150.95,457.12,120.05,7.77" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,321.31,446.16,155.69,7.77">Learning mid-level features for recognition</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,163.16,457.12,25.66,7.77">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2559" to="2566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,468.42,337.98,7.77;7,150.95,479.38,79.94,7.77" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,372.19,468.42,80.62,7.77">Visual word ambiguity</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Geusebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,459.00,468.42,21.59,7.77">PAMI</title>
		<imprint>
			<biblScope unit="page" from="1271" to="1283" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,490.67,337.98,7.77;7,150.95,501.63,329.64,7.77;7,150.95,512.59,82.28,7.77" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,248.74,490.67,121.59,7.77">The mir flickr retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,389.47,490.67,91.12,7.77;7,150.95,501.63,281.92,7.77">MIR &apos;08: Proceedings of the 2008 ACM International Conference on Multimedia Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,523.89,337.98,7.77;7,150.95,534.85,329.64,7.77;7,150.95,545.81,187.41,7.77" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,286.70,523.89,193.89,7.77;7,150.95,534.85,152.95,7.77">Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,323.58,534.85,157.01,7.77;7,150.95,545.81,100.70,7.77">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,557.10,307.47,7.77" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,247.86,557.10,136.12,7.77">In Defense of Soft-assignment Coding</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,402.51,557.10,21.42,7.77">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,568.40,337.98,7.77;7,150.95,579.36,174.07,7.77" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,197.82,568.40,203.36,7.77">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,407.12,568.40,73.47,7.77;7,150.95,579.36,94.88,7.77">International journal of computer vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,590.66,337.98,7.77;7,150.95,601.61,180.77,7.77" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,283.89,590.66,196.70,7.77;7,150.95,601.61,41.51,7.77">The clef 2011 photo annotation and concept-based retrieval tasks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liebetrau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,210.28,601.61,95.30,7.77">CLEF 2011 working notes</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.61,612.91,337.98,7.77;7,150.95,623.87,242.86,7.77" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,263.30,612.91,129.28,7.77">Social media driven image retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,411.71,612.91,68.88,7.77;7,150.95,623.87,160.21,7.77">ACM International Conference on Multimedia Retrieval (ICMR)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,142.24,635.17,338.35,7.77;7,150.95,646.13,329.64,7.77;7,150.95,657.08,154.32,7.77" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,391.79,635.17,88.80,7.77;7,150.95,646.13,129.03,7.77">Content-based image retrieval at the end of the early years</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,286.69,646.13,193.91,7.77;7,150.95,657.08,72.13,7.77">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1349" to="1380" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,119.96,338.35,7.77;8,150.95,130.92,149.93,7.77" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="8,355.89,119.96,124.70,7.77;8,150.95,130.92,82.64,7.77">Locality-constrained linear coding for image classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,251.31,130.92,23.42,7.77">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,141.88,279.99,7.77" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,206.26,141.88,80.04,7.77">Stacked generalization</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,292.08,141.88,61.41,7.77">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="241" to="259" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,152.84,338.35,7.77;8,150.95,163.80,205.98,7.77" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,224.05,152.84,128.25,7.77">Verb semantics and lexical selection</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,369.95,152.84,110.64,7.77;8,150.95,163.80,127.01,7.77">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,174.76,338.35,7.77;8,150.95,185.71,329.64,7.77;8,150.95,196.67,174.81,7.77" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,364.33,174.76,116.26,7.77;8,150.95,185.71,163.82,7.77">Multimodal Feature Generation Framework for Semantic Image Classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Znaidia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shabou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P H L</forename><surname>Borgne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hudelot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,332.60,185.71,148.00,7.77;8,150.95,196.67,85.50,7.77">ACM International Conference on Multimedia Retrieval (ICMR)</title>
		<meeting><address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06">Jun 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.24,207.63,338.35,7.77;8,150.95,218.59,329.64,7.77;8,150.95,229.55,141.96,7.77" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,407.87,207.63,72.72,7.77;8,150.95,218.59,119.49,7.77">Bag-of-Multimedia-Words for Image Classification</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Znaidia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shabou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Le Borgne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,294.34,218.59,186.26,7.77;8,150.95,229.55,29.86,7.77">International Conference on Pattern Recognition ICPR&apos;12</title>
		<meeting><address><addrLine>Tsukuba, JAPAN</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-11">Nov 2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
