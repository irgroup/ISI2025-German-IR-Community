<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,138.50,151.59,318.33,12.64;1,283.01,168.99,29.74,12.64">BUAA AUDR at ImageCLEF 2012 Photo Annotation Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,254.93,207.32,39.99,9.05"><forename type="first">Lei</forename><surname>Huang</surname></persName>
							<email>huanglei@nlsde.buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Software Development Enviroment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,302.33,207.32,38.07,9.05"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<email>liuyang@nlsde.buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Software Development Enviroment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,138.50,151.59,318.33,12.64;1,283.01,168.99,29.74,12.64">BUAA AUDR at ImageCLEF 2012 Photo Annotation Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">45EF61C6C366AEA86319C783022C5B50</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>Photo Annotation</term>
					<term>Flickr</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the participation of the BUAA AUDR group at ImageCLEF 2012 in the Photo Annotation and Retrieval task. We selected Flickr photos as data set to perform visual concept detection, annotation and retrieval. In this task, we had proposed multi-modal approaches that considered visual information and Flickr user tag information. We presented our visual-based and tag-based photo annotation methods, and also proposed Annotation Refining Algorithm (ARA), which attempted to make use of the relation between concepts to improve the annotation result. It was our first time to participate the Photo Annotation and Retrieval task. We submitted 3 runs totally and the purely visual submission using the global visual features and BoW features get better performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper presents the first participation of the BUAA AUDR group at ImageCLEF photo annotation and retrieval task.</p><p>ImageCLEF 2012 includes four types of tasks: medical image retrieval, photo annotation, plant identification and robot vision. In the photo annotation task, the aim is to analyze a collection of Flickr photos in terms of their visual or textual features in order to detect the presence of one or more concepts. The detected concepts can then be used for the purpose of automatically annotating the images or for retrieving the best matching images to a given concept-oriented query. This task provides 15000 images for training and requires the annotation of 10000 images in the provided test corpus according to the 94 pre-defined categories.</p><p>We proposed multi-modal approaches that considered visual information and Flickr user tag information. We presented our visual-based and tag-based photo annotation methods, and also attempted to make use of the relation between concepts to improve the annotation result.</p><p>The remainder of this paper is organized as follows. In section 2 we describe our approaches in detail. And our submitted runs are discussed in section 3. Then we conclude in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approaches</head><p>For the visual concept annotation task, we proposed multi-modal approaches that considered visual information and Flickr user tag information. We presented our visual-based and tag-based photo annotation methods in this section.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual-based Photo Annotation</head><p>Recently, the Bag-of-Words (BoW) [1] model has been very popular in image recognition and retrieval. In this model, the key points extracted are quantized to visual words, and an image is represented as a frequency histogram of these words. We followed BoW model. We adopted the implementation described in [2] for extracting local feature. Harris -Laplace was used to detect interest points and SIFT descriptor was extracted. These descriptors were then quantized in visual words. To form the codebook, we randomly selected approximate 1.5 million descriptors from all descriptors extracted from the training images for clustering. We used k-means clustering method to group these descriptors into K (K=200, 1000, 2000) clusters. The codebook was formed by picking K cluster centers computed from the K clusters.</p><p>Soft assignment was used to form the feature vector. We used the mapping from [3]. Let us define V a visual vocabulary set, and v d the visual word from its corresponding dimension d and l a local feature. Then the BoW mapping m(l) is defined as:</p><formula xml:id="formula_0" coords="3,127.98,220.65,341.51,83.83">() 1 exp( ( , )) ( ( , )) 1 () exp( ( , )) 0 topN v w V l w dist l v D if Rank dist l v N ml dist l w D otherwise              (2.1)</formula><p>Where dist(l,v) is the distance between the local feature l and the visual word v, Rank(dist(l,v)) is the rank of the distance between l with v∈V, sorted in increasing order. V topN (l) = ｛v|v∈V,Rank(dist(l,v))≤N｝.parameter D v is estimated as the average distances to all local features from train images which had v as the nearest visual word.</p><p>For an image, the BoW mapping can be calculated as follows:</p><formula xml:id="formula_1" coords="3,138.08,385.75,331.42,9420.65">() 1 ( ) ( ) () l F im m im m l F im    (2.2)</formula><p>Where F(im) is the set of all local features from the image. Besides Bow Feature, for per image we also extracted the global features such as RGB histogram, CEDD and FCTH.</p><p>For per semantic concept, we trained an SVM classifier <ref type="bibr" coords="3,373.24,450.73,12.11,9.05">[4]</ref> to perform a binary assignment to an image. We used the probabilistic output of the SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tag-based Photo Annotation</head><p>Since metadata of images were provided for annotation task, using text-based method in image annotation became possible. Compared with visual features of images, metadata was usually more semantic, so it could be used for identification of many abstract concepts, which was difficult with visual features.</p><p>For the experiment presented in this paper, a tag-based image annotation method, which makes use of custom tags in image metadata in determination of existence of specific concepts, was implemented and used besides visual-feature-based annotation methods. It is a rather simple algorithm, and its progress is presented below: 1．Extract unique tags from the training dataset. Since tags are attached by users, they don't have any common rules, so even tags that share the same meaning may appear different in the same dataset. Therefore, the amount of tags could be intolerably large, and it would be difficult to generate the codebook.</p><p>2．Select tags with high frequency to form a small tag set. In this experiment, totally 2400 most frequently used tags were selected from the extracted tag set. 3．Each image is given a tag-based feature vector. 4．Train SVM classifiers for each concept with tag-based feature vectors of images.</p><p>Through the above steps, SVM classifiers are obtained, and they could be used to identify the concepts in annotation task just like any other kind of classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Annotation Refining Algorithm</head><p>Annotation refining algorithm (ARA) works after all previous annotation process. Its input is a complete annotation which used to be directly submitted. That is to say, Annotation refining is an extra process after common annotation process, and it is used to improve the annotation result. It could be observed that each concept is not totally individual, e.g. "night" is usually along with "moon" and hardly with "sun". Therefore, if each concept in a picture is identified separately, the information provided by their relations could be neglected, so the annotation refining algorithm presented here attempts to make use of this usually ignored information to improve the annotation result.</p><p>In short, the process of annotation refining is simply minimization of an evaluation function. Moreover, in order to learn parameters in this evaluation function in specific dataset, ARA needed a training process. The training process and evaluation function form the main part of ARA.</p><p>The ARA training process is a series of solution of a Linear Programming (LP) problem, which is</p><formula xml:id="formula_2" coords="4,239.35,415.90,229.43,14.49">T j j j xa  bx . (<label>2.3)</label></formula><p>In equation (2.3), x is a vector with the jth element being x j , which means the possibility of the existence of the jth concept in the given picture. In this experiment, x j is the score of the jth concept in annotations_raw dataset. For training, x is given by the training dataset. b j and a j are parameters that need to be learned. They are simply the parameters in general LP problems. By solving this problem in the whole training dataset, a pair of parameters could be obtained for each concept.</p><p>The minimization problem needed to be solved in ARA can be presented as below: (2.5)</p><formula xml:id="formula_3" coords="4,126.29,528.48,304.91,44.71">11 min ( ) ( ) ( )( ) ' 22 s.t. [ 1,1], 1, 2, , T T T T T T i f x i n           </formula><p>x' is the input annotation result. α is tuning parameter used to balance the importance of previous annotation process and refining, i.e. the refining result would be closer to x' with a bigger eα, and vice versa. This is a convex optimization problem and has only one solution. Refined annotation result could be obtained as the solution x*.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>We submitted three different runs. The purely visual submission (BUAA_AUDR_1 in Table <ref type="table" coords="5,151.58,275.27,4.18,9.05" target="#tab_1">1</ref>) was adopted using the global visual features and BoW features. For each concept, we selected separately the best classifier from a set of classifiers by MAP values obtained on 5-fold cross-validation on the training data. BUAA_AUDR_1 obtained the rank 52 on 80 submissions, with a MAP value of 0.142. This value was 0.08 lower than the median value for these runs, 0.228.</p><p>The purely text submission (BUAA_AUDR_2) obtained the rank 76. The result was poor, but it got a better result than the visual method for the valid set which we adopted to validate these methods. Maybe the tag-based photo annotation method was not robust.</p><p>BUAA_AUDR_3 was adopted as a multi-modal approaches with ARA. However, it didn't obtain better results than BUAA_AUDR_1. For two reasons:</p><p>1. We used a linear integration which might lead to a worse result than purely visual submission for that our purely text submission had a poor result and was not robust.</p><p>2. ARA had a shortcoming that poorly recognized concepts could spoil classification rates of better performing classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This article describes the approaches and results of BUAA AUDR group at ImageCLEF 2012 photo annotation task. We submitted 3 runs totally and their results were not competitive among all the submitted runs. As this is our first time to participate in this task, we will investigate our methods to find its weak and improve its performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,234.53,545.68,144.34,9.05"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Phone Annotation Process</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,167.73,544.34,6.11,1.00;4,189.64,544.34,24.44,1.00;4,230.44,544.34,15.48,1.00;4,266.05,544.34,56.10,1.00;4,338.52,544.34,15.48,1.00;4,382.01,544.34,17.50,1.00;4,417.94,544.34,22.96,1.00;4,452.95,546.16,19.19,9.05;4,136.22,588.52,283.53,9.05;4,314.46,628.06,3.97,14.33;4,378.38,628.06,3.97,14.33;4,278.23,609.33,3.51,6.27;4,278.35,627.42,3.51,6.27;4,325.55,636.45,18.96,6.27;4,290.24,629.87,3.03,10.83;4,329.65,629.87,34.78,10.83;4,372.79,641.21,3.51,1.00;4,278.28,668.35,3.51,1.00;4,272.99,610.98,6.05,1.00;4,272.36,629.06,6.05,1.00;4,272.31,665.23,6.05,1.00;4,266.05,603.58,23.02,10.94;4,266.05,614.64,23.02,10.94;4,266.05,626.18,23.02,10.94;4,257.02,629.75,55.08,10.94;4,266.05,637.75,23.02,10.94;4,266.05,649.29,23.02,10.94;4,266.05,660.11,23.02,10.94;4,248.19,638.09,6.05,1.00;4,294.56,638.09,8.08,1.00;4,319.05,638.09,21.42,1.00;4,365.49,638.09,6.73,1.00"><head></head><label></label><figDesc>problem, a and B are learned parameters in training stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,141.38,470.95,322.28,72.50"><head>Table 1 .</head><label>1</label><figDesc>Results by MAP for the photo annotation</figDesc><table coords="5,141.38,487.87,322.28,55.58"><row><cell>Submission</cell><cell>Result File</cell><cell>Modality</cell><cell>MAP</cell></row><row><cell>BUAA_AUDR_1</cell><cell>result1_visualOnly</cell><cell>V</cell><cell>0.1423</cell></row><row><cell>BUAA_AUDR_2</cell><cell>result2_textOnly</cell><cell>T</cell><cell>0.0723</cell></row><row><cell>BUAA_AUDR_3</cell><cell>result3_textAndVisual</cell><cell>V&amp;T</cell><cell>0.1307</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,144.86,177.74,325.45,8.18;6,124.82,188.18,203.16,8.18" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,229.99,177.74,240.33,8.18;6,124.82,188.18,113.77,8.18">Representing and recognizing the visual appearance of materials using three-dimensional textons</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>．leung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,244.40,188.18,18.88,8.18">IJCV</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="29" to="44" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,144.86,199.10,325.64,8.18;6,124.82,209.54,345.55,8.18;6,124.82,219.86,220.62,8.18" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,407.19,199.10,63.32,8.18;6,124.82,209.54,173.25,8.18">Evaluating Color Descriptors for Object and Scene Recognition</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Theo</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,305.92,209.54,164.46,8.18;6,124.82,219.86,76.19,8.18">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1582" to="1596" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,144.86,230.81,325.90,8.18;6,124.82,241.25,313.82,8.18" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="6,351.79,230.81,118.97,8.18;6,124.82,241.25,309.91,8.18">The joint submission of the TU Berlin and Fraunhofer FIRST(TUBFI) to the ImageCLEF2011 Photo Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">Wojciech</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marius</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kloft</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="6,144.86,252.17,327.95,8.18;6,124.82,262.61,248.30,8.18" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,298.34,252.17,170.45,8.18">LIBSVM: a library for support vector machines</title>
		<author>
			<persName coords=""><forename type="first">Chung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/~cjlin/libsvm" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
