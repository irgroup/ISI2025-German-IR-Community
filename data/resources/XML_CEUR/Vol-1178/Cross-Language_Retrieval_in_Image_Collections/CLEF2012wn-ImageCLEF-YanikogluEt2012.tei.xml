<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,162.33,116.95,290.71,12.62;1,170.87,134.89,273.62,12.62;1,239.21,152.82,136.92,12.62">Sabanci-Okan System at ImageClef 2012: Combining Features and Classifiers for Plant Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,183.43,190.49,74.08,8.74"><forename type="first">Berrin</forename><surname>Yanikoglu</surname></persName>
							<email>berrin@sabanciuniv.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Sabanci University</orgName>
								<address>
									<postCode>34956</postCode>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.07,190.49,69.36,8.74"><forename type="first">Erchan</forename><surname>Aptoula</surname></persName>
							<email>erchan.aptoula@okan.edu.tr</email>
							<affiliation key="aff1">
								<orgName type="institution">Okan University</orgName>
								<address>
									<postCode>34959</postCode>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,367.35,190.49,60.10,8.74"><forename type="first">Caglar</forename><surname>Tirkaz</surname></persName>
							<email>caglart@sabanciuniv.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Sabanci University</orgName>
								<address>
									<postCode>34956</postCode>
									<settlement>Istanbul</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,162.33,116.95,290.71,12.62;1,170.87,134.89,273.62,12.62;1,239.21,152.82,136.92,12.62">Sabanci-Okan System at ImageClef 2012: Combining Features and Classifiers for Plant Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9042E6BFCFE60D9006A5EA4A79185D7D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Plant identification</term>
					<term>mathematical morphology</term>
					<term>classifier combination</term>
					<term>support vector machines</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe our participation in the plant identification task of ImageClef 2012. We submitted two runs, one fully automatic and another one where human assistance was provided for the images in the photo category. We have not used the meta-data in either one of the systems, for exploring the extent of image analysis for the plant identification problem. Our approach in both runs employs a variety of shape, texture and color descriptors (117 in total). We have found shape to be very discriminative for isolated leaves (scan and pseudoscan categories), followed by texture. While we have experimented with color, we could not make use of the color information. We have employed the watershed algorithm for segmentation, in slightly different forms for automatic and human assisted systems. Our systems have obtained the best overall results in both automatic and manual categories, with 43% and 45% identification accuracies respectively. We have also obtained the best results on the scanned image category with 58% accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A content-based image retrieval (CBIR) system for plants would be very useful for plant enthusiasts or botanists who would like to learn more about a plant they encounter. Until the first ImageCLEF Plant Identification Competition organized in 2011 <ref type="bibr" coords="1,194.11,561.47,9.96,8.74" target="#b8">[9]</ref>, existing research concentrated on isolated leaf identification <ref type="bibr" coords="1,470.07,561.47,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="1,134.77,573.43,7.75,8.74" target="#b3">4,</ref><ref type="bibr" coords="1,144.18,573.43,12.73,8.74" target="#b11">12,</ref><ref type="bibr" coords="1,158.56,573.43,12.73,8.74" target="#b16">17,</ref><ref type="bibr" coords="1,172.96,573.43,12.73,8.74" target="#b21">22,</ref><ref type="bibr" coords="1,187.35,573.43,11.62,8.74" target="#b19">20]</ref>, while few systems attempted the identification of unconstrained whole or partial plant images <ref type="bibr" coords="1,265.78,585.38,15.50,8.74" target="#b10">[11,</ref><ref type="bibr" coords="1,282.93,585.38,11.62,8.74" target="#b18">19]</ref>.</p><p>As with the first competition, the plant identification task in ImageCLEF 2012 consisted of identifying images of plants that were captured by different means: scans, scan-like photos (called pseudo-scans) and unrestricted photos, as shown in Fig. <ref type="figure" coords="1,193.93,633.20,3.87,8.74" target="#fig_0">1</ref>. In this way, the competition aimed to benchmark state-of-the-art in both isolated leaf shape and unrestricted plant image recognition problems. The details of this competition are described in <ref type="bibr" coords="1,345.03,657.11,14.61,8.74" target="#b9">[10]</ref>.</p><p>Content-based plant identification problem faces many challenges such as color, illumination, size variations that are also common in other CBIR problems, as well as some specific problems, such as variations in the composition of the leaves that change the plant shape. In addition, one can see that color is less identifying in the plant retrieval problem compared to many other retrieval problems, since most plants have green tones as their main color with subtle differences. In the rare cases that color is discriminative for a certain plant, then it may still be the case that the leaves of that plant may change colors due to seasonal variations. Shape features are useful in identifying isolated leafs, but not really useful in identifying full or partial plant images <ref type="bibr" coords="2,413.01,227.59,15.50,8.74" target="#b10">[11,</ref><ref type="bibr" coords="2,430.16,227.59,11.62,8.74" target="#b20">21]</ref>. In that regard, isolated leaf identification is appears to be a significantly simpler problem compared to the identification of partial or full plants. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of the System</head><p>As a collaboration between SabancÄ± and Okan Universities, we submitted two runs, one fully automatic and another one where human assistance was provided for the images in the photo category. The only distinction between our two runs is that the images in the photo category undergo a human-assisted segmentation process, as explained in Section 3.2. Hence, in the remainder of this paper, we will talk about one system, since everything besides the segmentation of photos are the same in the two systems corresponding to the two submitted runs.</p><p>The system is designed as two separate sub-systems, one for scan and scan-like images and another one for photos. Since the meta-data included the acquisition type, an input image is automatically sent to the correct subsystem, but that was the extent of the use of the meta-data. Our system shares many common parts with the system we sent to ImageCLEF2011, described in <ref type="bibr" coords="2,462.33,573.43,14.61,8.74" target="#b20">[21]</ref>. In this paper, we give an overview of the system, with detailed descriptions of the changes done this year. The main changes are as follows:</p><p>In ImageCLEF2011, we had concentrated our efforts almost exclusively to scan and pseudo-scan images, while photos were neglected due to lack of time. This year, we developed automatic and human-assisted segmentation algorithms for the photo category. In both segmentation algorithms, we aimed to segment the image to leave only a single leaf, so as to use our isolated leaf recognition engine. The segmentation was reasonably successful, in that our system had obtained 5.3% accuracy in photos last year, while the automatic photo identification accuracy was 16% this year. The segmentation steps are explained in Section 3.</p><p>For recognizing photographs, we found that many of our other feature descriptors, especially global shape descriptors, would not be useful if the photo was one of a partial plant. On the other hand for scan and scan-like categories containing isolated leaves, all three main feature categories are expected to be useful: shape, texture and color. After experimenting with a large number of descriptors, we selected a 117-dimensional feature vector for the scan/scan-like sub-system and a subset of these for the photo category. The features used in our system are explained in Section 5.</p><p>Another major problem we encountered last year was the over-fitting problem in classifier training: our results obtained in official tests were seriously lower compared to the results obtained with cross-validation experiments done on the training set (around 40% difference in accuracy). This year our feature selection and classifier optimization approaches were run on a separate test set which was partitioned from the original training data such that the two did not to include images of the same individual plant (e.g. the same exact tree). For the same reason, we have excluded our powerful new morphological texture features, in order to study them further. This issue is explained in 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Segmentation</head><p>The ultimate goal of segmentation is the separation of the leaf from its background. If no a priori knowledge is available about the image, the segmentation problem becomes equivalent to unconstrained color image segmentation, where the various leaves can be located within an equally immense variety of backgrounds, such as those shown in Fig. <ref type="figure" coords="3,329.39,465.59,3.87,8.74" target="#fig_1">2</ref>. One might argue at this stage that the background can provide information facilitating the recognition of the leaf/plant in the foreground. For instance if the background consists of forest ground versus the sky, we can eliminate some alternatives as implausible. However, this would additionally require the description and recognition of the background, which given its limitless diversity potential, increases the complexity of an already challenging problem.</p><p>The accurate separation of the background from the plant/leaf under consideration is of crucial importance for the subsequent stage of feature extraction, since a poor result would affect many of the features. Given the ill-posed nature of segmentation, in addition to the lack of any useful a priori knowledge, it becomes evident that some form of human intervention or feedback is necessary for an accurate and reliable segmentation. However, we also have to take into account practical considerations, since no user/expert would like to spend a prolonged amount of time on this stage, especially when dealing with voluminous amounts of data. That is why we have explored both automatic and human assisted segmentation strategies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Automatic segmentation</head><p>Although the ImageClef dataset is classified into three categories, namely scan, pseudo-scan and photos, from a segmentation viewpoint scan and pseudo-scan type images are of similar quality, in other words they both possess a mostly noise-free, spectrally homogeneous background, occasionally containing some amount of shadow in the pseudo-scans. Consequently, as far as scan and pseudoscan type images are concerned, automatic segmentation has been trivially resolved through Otsu's method <ref type="bibr" coords="4,277.29,306.29,15.50,8.74" target="#b12">[13]</ref> as it is both efficient and effective (Fig. <ref type="figure" coords="4,468.97,306.29,3.87,8.74" target="#fig_2">3</ref>). Photos on the other hand consist of unconstrained acquisitions of leaves, as shown in Fig. <ref type="figure" coords="4,194.65,489.74,3.87,8.74" target="#fig_1">2</ref>, and thus their automatic segmentation is a far greater challenge.</p><p>In order to counter this problem we adopted a combination of spectral and spatial techniques.</p><p>More precisely the only assumption concerning the input has been that the object of interest, i.e. the leaf, is located roughly at the center of the image and possesses a single dominant color. The image was first simplified by means of marginal color quasi-flat zones <ref type="bibr" coords="4,336.98,561.47,14.61,8.74" target="#b14">[15]</ref>, a morphology based image partitioning method based on constrained connectivity, only recently extended to color data <ref type="bibr" coords="4,192.80,585.38,14.61,8.74" target="#b17">[18]</ref>, that creates flat zones based on both local and global spectral variational criteria (Fig. <ref type="figure" coords="4,241.28,597.34,8.30,8.74" target="#fig_3">4a</ref>). Next we computed its morphological color gradient in the LSH color space <ref type="bibr" coords="4,237.98,609.29,9.96,8.74" target="#b1">[2]</ref>, taking into account both chromatic and achromatic variations (Fig. <ref type="figure" coords="4,207.50,621.25,8.57,8.74" target="#fig_3">4b</ref>), followed by the application of the watershed transform. Hence we obtained a first partition with spectrally homogeneous regions and spatially consistent borders, albeit with a serious over-segmentation ratio which was compensated for by merging basins below a certain area threshold (Fig. <ref type="figure" coords="4,464.54,657.11,8.03,8.74" target="#fig_3">4c</ref>).</p><p>At this point our initial assumption about the object of interest's central location was used, as we employed the central 2/3 area of the image in order to determine its dominant color, which was obtained by means of histogram clustering in the LSH color space. Assuming that the mean color of the most significant cluster (i.e. reference color) belongs to the leaf/plant, we then switched to spectral techniques, so as to determine its watershed basins. Since camera reflections can be problematic due to their low saturation, we computed both the achromatic, i.e. grayscale distance image from the reference gray (Fig. <ref type="figure" coords="5,159.59,215.63,9.59,8.74" target="#fig_3">4d</ref>) and the angular hue distance image (Fig. <ref type="figure" coords="5,365.12,215.63,7.93,8.74" target="#fig_3">4f</ref>) from the reference hue h ref <ref type="bibr" coords="5,156.68,227.59,9.96,8.74" target="#b2">[3]</ref>:</p><formula xml:id="formula_0" coords="5,150.39,249.93,330.20,21.61">â h, h ref â [0, 2Ï], d Î¸ (h, h ref ) = |h -h ref | if |h -h ref | &lt; Ï 2Ï -|h -h ref | if |h -h ref | â¥ Ï<label>(1)</label></formula><p>We then applied Otsu's method on both distance images, that provided us with two masks (Figs. <ref type="figure" coords="5,213.80,295.47,9.41,8.74" target="#fig_3">4e</ref> &amp;<ref type="figure" coords="5,239.16,295.47,8.30,8.74" target="#fig_3">4g</ref>), representing spectrally interesting areas w.r.t. the reference color. The intersection of the two masks was used as the final object mask (Figs. 4h). As shown in Fig. <ref type="figure" coords="5,294.67,319.38,7.01,8.74" target="#fig_3">4i</ref>, spectral and spatial techniques indeed complement each other well, while the use of both chromatic and achromatic distances increases the method's robustness. However the main difficulty is the accurate determination of the reference or dominant color; this can easily corrupt the entire process if computed incorrectly or if the leaf under consideration has more than one dominant colors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Human assisted segmentation</head><p>As the automatic segmentation module relies strongly on its initial assumption, it does not work very well, resulting in over or wrong segmentation. We investigated human assisted segmentation for the semi-manual category of the competition. Ideally, given an input image we would like the user/expert to spend at most a few tens of seconds in order to provide some amount of high level knowledge. Assuming that the provided knowledge is valid, we chose to use the marker based watershed transform <ref type="bibr" coords="5,288.42,501.61,9.96,8.74" target="#b5">[6]</ref>. Specifically, the marker based watershed transform is a powerful, robust and fast segmentation tool that given a number of seed areas or markers, results in watershed lines representing the skeleton of their influence zones.</p><p>To accomplish this we need a suitable topographic relief as input, where object borders are denoted as peaks and flat zones as valleys; which is why we employ the morphological color gradient of the input image <ref type="bibr" coords="5,405.61,573.43,10.52,8.74" target="#b1">[2]</ref> (Fig. <ref type="figure" coords="5,445.40,573.43,8.58,8.74" target="#fig_4">5b</ref>). We then require at least two markers provided by the user/expert, one denoting the background and another representing the foreground, both of which could be easily provided for instance through the touchscreen of a smartphone by indicating once the leaf and once its background (Fig. <ref type="figure" coords="5,399.54,621.25,8.03,8.74" target="#fig_4">5c</ref>). Next, having superposed the markers on the gradient, the marker based watershed transform provides the binary image partition (Fig. <ref type="figure" coords="5,330.70,645.16,8.57,8.74" target="#fig_4">5d</ref>). Although both efficient and effective, this method depends utterly on the quality of the provided markers, since if they are too small they can lead to partial leaf detection and conversely if they are excessively large, the leaf will be confounded with its background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Preprocessing</head><p>After segmentation, we have a single leaf which is segmented from its background, regardless of the type of the original image (scans or photos). For the case of photos, this isolated image is well segmented in the case of humanassisted segmentation, except possibly for some noise at the contour; in the case of automatic segmentation, the leaf may be over or under-segmented. Note that in all photos, the segmented image may also be rotated in an arbitrary direction, while most of the scanned leaves are oriented with their axes aligned with the vertical, stem part down.</p><p>Our preprocessing consists of orientation normalization for photos, followed by height normalization (keeping the height/width ratio unchanged). We have also experimented with stem location and orientation normalization for scans and pseudo-scans, however these steps are not yet mature, causing errors in as many images as the ones they correct. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Features</head><p>In a difficult problem such as this, it was clear that we needed to use all main feature categories (shape, texture and color), even though some of them were expected to be more useful than others. We have kept most of the features that were used in last years's competition, and we added some new ones that we thought would be useful.</p><p>Considering the intra-class color variations levels, it became evident at an early stage that our approach would have to be mainly texture and shape based, even though the discriminatory potential of color has not been ignored completely. Moreover, it was chosen not to employ any further complicated descriptors, such as scale invariant feature transforms (SIFT) or maximally stable extremal regions (MSER), for two reasons. First, our past experience with plant retrieval <ref type="bibr" coords="7,223.24,383.70,15.50,8.74" target="#b10">[11]</ref> has led to results indicating them as not very suitable for this content type, and second due to time limitations that hindered efforts to test more sophisticated descriptors.</p><p>In total, we evaluated about 20 different shape, texture and color features. The evaluated features consist of region-based (e.g. regional moments) and contour-based (e.g. Fourier descriptors, border covariance) shape features; texture features (morphological, Gabor, orientation histograms); and color (color moments and different length and quantization of the color histogram in LSH space). Here we summarize the new features, while others are described in detail in <ref type="bibr" coords="7,146.39,491.34,14.61,8.74" target="#b20">[21]</ref>.</p><p>1. Convex Perimeter Ratio returns the ratio of the perimeter of the convex hull to the perimeter of the original binary mask. 2. Regional Moments of Inertia is computed on grayscale data. We first divide the input into n horizontal slices as in area width factor and then compute independently for each slice the mean distance to the image centroid. 3. Angle Code Histogram is computed on binary data. We first compute the object contour, which is then subsampled. We then proceed to calculate the angle of every point triplet and return as feature their normalized histogram <ref type="bibr" coords="7,151.70,609.25,14.61,8.74" target="#b16">[17]</ref>. 4. Contour Point Distribution Histogram is applied on the binarized internal morphological gradient of the input image. Given n concentric disks centered at the image centroid, we calculate for each disk the percentage of gradient pixels in it <ref type="bibr" coords="7,201.29,657.11,14.61,8.74" target="#b13">[14]</ref>.</p><p>5. Orientation Histogram is computed on grayscale data. We first compute the orientation map using a 11 Ã 11 mask for determining the dominant orientation of each pixel. The feature vector consists of the normalized histogram of dominant orientations. 6. Lobe Descriptor is calculated by over-segmenting the scanned images using morphological tools and extracting the median value of convexity, elongation etc. parameters of the segmented lobes.</p><p>Feature Selection We evaluated individual features using the global classifier described in Section 6.2, to observe their relative merits, but more importantly, to see the errors made in the system and to add more features as a remedy. Some of the above mentioned features (e.g. lobe descriptor) were added as a result of this process. In this process, we have also excluded the morphological texture features that we used in ImageCLEF2011.</p><p>The full set of our final features (117 dimensional), along with their effectiveness on cross-validation and test data, can be found in the Results section, in Table <ref type="table" coords="8,208.12,317.58,4.98,8.74" target="#tab_0">1</ref> and Table <ref type="table" coords="8,261.48,317.58,4.98,8.74" target="#tab_1">2</ref> for shape and texture features respectively. Color features were evaluated but we did not find them useful, so they do not appear in the final feature list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Classifier Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head><p>The ImageClef2012 plant database contains 126 tree species and the images for each species are contributed by various people, some of the images being taken from the same individual plant <ref type="bibr" coords="8,269.24,441.83,14.61,8.74" target="#b9">[10]</ref>. A straightforward cross-validation done over the whole training set thus results in splitting similar images of the same plant into training and test sets. We believe this is the main reason for the overfitting that we experienced in last year's system. Consider that the images of an individual plant, often captured with the same lighting and camera parameters, are split into training and test sets. In this case, color and texture descriptors remain quite similar between two images, leading to over-fitting.</p><p>This year, we took a different approach and divided the database into training and testing, rather than using cross-validation that may result in a random split of very similar images. In short, we put all of the images of an individual plant either in training or testing. Specifically, for each of the species, if the species images contained two or more individual plants, one of them (the one with the least number of pictures) is separated for testing while all others are selected for training. In total 5163 scan and scan-like images are used for training and 1526 scan and pseudo-scan images and all photos (1733 images) are used for testing during our experiments.</p><p>Note that this approach for dividing the training set has the disadvantage that few of the species having images from a single individual plant did not have any representative images in test data. However, in terms of feature selection, we thought that this should not have a significant effect, as features can be said to be species-independent.</p><p>As seen in Tables <ref type="table" coords="9,244.95,145.34,4.98,8.74" target="#tab_0">1</ref> and<ref type="table" coords="9,273.11,145.34,3.87,8.74" target="#tab_1">2</ref>, cross-validation accuracies are almost always higher than test set accuracies, and especially so for texture features. This supports the association between cross-validation and the over-fitting that applies in this particular problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Classifiers</head><p>We trained three classifiers in total for the automatic run: one Support Vector Machine (SVM) classifier for scans, pseudo-scans and manually segmented photos using all the features for isolated leaf recognition; one SVM classifier for automatically segmented photos using a subset of the features, and finally a single local classifier. The details of these classifiers are explained below. Finally, a score level combination of the two classifiers is performed for each category of images.</p><p>This year, the ambiguity resolution step, where we had used a third classifier trained to distinguish between the top-5 choices <ref type="bibr" coords="9,392.81,342.45,9.96,8.74" target="#b8">[9]</ref>, is skipped, as it was observed that it was significantly extending the test stage without significant improvement.</p><p>Global Classifier Using the selected features described in Section 5, we trained two SVM classifiers, one for scans (scan and pseudo-scan categories) and manually segmented photos, and another for automatically segmented photos. When the photos are manually segmented, the results are similar to scanned images, hence we used the same classifier as for scans. For automatically segmented photos, the noise on the contours are more apparent, so we discarded the Fourier descriptors obtained via the Fast Fourier Transform (FFT) as they may be affected from contour noise. For these classifiers, we used a SVM using the radial basis function kernel whose parameters were optimized using crossvalidation and grid search on the training data.</p><p>The training samples for these two classifiers were obtained from scan and pseudo-scan images only, so that the feature extraction was not affected from segmentation noise.</p><p>Local Classifier Another classifier was built to compare local features obtained from the stable points on the boundary of the leaves. The local features used here were shape context <ref type="bibr" coords="9,222.71,609.29,9.96,8.74" target="#b4">[5]</ref>, Histogram of Oriented Gradients (HOG) <ref type="bibr" coords="9,423.40,609.29,9.96,8.74" target="#b7">[8]</ref>, and local curvature. The stable points were obtained using an unsupervised clustering of boundary points, based on location and feature similarity. All leaf images from each plant type were processed to find the stable point of that plant. This classifier was expanded from our work in sketch recognition <ref type="bibr" coords="9,397.48,657.11,14.61,8.74" target="#b15">[16]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experimental Results</head><p>Throughout the experiments, we considered the scan and pseudo-scans as one group and treated them using the same methodology. We evaluated the efficiency of our features and the performance of the classifiers on this group, as shown in Table <ref type="table" coords="10,176.19,464.24,4.98,8.74" target="#tab_0">1</ref> and Table <ref type="table" coords="10,234.89,464.24,3.87,8.74" target="#tab_1">2</ref>. Here, the cross-validation results are obtained using cross-validation on the training data, while test results are obtained using the separate test data that was split from the original training set. Color features were evaluated but we did not find them useful, so they do not appear in the final feature list. As shown in these tables, all shape features achieve 67.89% accuracy while texture features achieve 33.42%. Finally, using all the features (117-long feature vector), we obtained a 85.9% accuracy on cross-validation tests and 66.1% accuracy over the test data, while the results reaches almost 71% with classifier combination. Note that the test accuracy with all features show a slight decrease compared to using all shape features, but there is a significant increase in cross-validation accuracy when using all features. We believe that these issues will be less pronounced in the future when training and test data sizes increase.</p><p>Also note that our accuracies are measured as average accuracy over the test images, while the official scoring function computes an average across users (the people who have collected the images being seen as users of such a plant identification system).eing seen as users of such a plant identification system).  Hence, a significant amount of the difference between the test results reported here and the official results may be due to this. Accuracies for automatically processed photos from the test set are given in Table <ref type="table" coords="11,175.69,342.42,3.87,8.74" target="#tab_4">4</ref>. The photos are categorized into 4 categories by the organizers: i) picked leaf ii) leaf iii) branch iv) leafage, containing roughly a picked single leaf on a variety of backgrounds, a leaf which may be hanging from the branch, and plant foliage with or without branches <ref type="bibr" coords="11,306.08,378.29,14.61,8.74" target="#b9">[10]</ref>. As expected, photos perform much worse compared to scan categories. Furthermore, we observe that different photo categories differ in difficulty level, picked leaf images being the easiest, pointing to the difficulty of segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">ImageCLEF2012 Results and Discussion</head><p>This year the organizers categorized automatic and manual or human-assisted runs separately, while this information was not clear in last year's results. According to the official results given in Table ??, our runs achieved 1 st place in overall classification, for both automatic and human-assisted categories. We also obtained significantly better results on the scan category, compared to the next best system. Note that we only give a partial results table, including only the best automatic systems in addition to our two systems.</p><p>Although our results are satisfactory comparatively to other participants, we believe there is still much room for improvement. Currently, plant identification systems cannot be of practical use, maybe only to narrow down the alternatives. That is why we have established our future work plan in two directions, with both converging on the common goal of accuracy improvement. For one, having resolved the issues of over-fitting, we now possess a sound feature selection and optimization setup, that we intend to exploit in order to explore the latest and especially morphology related content descriptors <ref type="bibr" coords="11,417.10,633.20,9.96,8.74" target="#b0">[1]</ref>.</p><p>Moreover, we are well aware of our comparatively poor results in the photo category, as well as of its strong ties to segmentation quality. Hence we will focus particularly on improving the performance of leaf isolation, in both automatic and human-assisted approaches. To this end, we plan to further incorporate the latest results obtained from our ongoing work on color quasi-flat zones <ref type="bibr" coords="12,445.69,143.90,14.61,8.74" target="#b17">[18]</ref>.</p><p>Finally, we will think of ways to from color and other new descriptors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,139.77,380.96,335.83,8.74;2,185.89,282.49,50.49,72.00"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Samples from scan (a,b), pseudo-scan (c,d) and photo categories (e,f).</figDesc><graphic coords="2,185.89,282.49,50.49,72.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,183.86,189.38,247.63,8.74"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Examples of background variations in leaf photos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,134.77,418.76,345.83,8.74;4,134.77,430.72,345.82,8.74;4,134.77,442.67,128.02,8.74"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Leaf isolation example with scan and pseudo-scan type images using Otsu's method. (a) Original image (b) Segmentation mask (c) Mask superposition on the original.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,181.71,411.41,251.95,8.74;6,159.56,316.49,86.45,57.49"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Stages of automatic photo segmentation of Fig. 2c.</figDesc><graphic coords="6,159.56,316.49,86.45,57.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,155.54,191.67,304.27,8.74"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Stages of human assisted photo segmentation on image #4835.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="10,163.97,118.38,287.42,230.82"><head>Table 1 :</head><label>1</label><figDesc>Individual shape feature accuracies</figDesc><table coords="10,163.97,118.38,287.42,230.82"><row><cell>Feature Name</cell><cell></cell><cell>Length</cell><cell>Cross Val. Acc.(%)</cell><cell>Test Acc.(%)</cell></row><row><cell>FFT</cell><cell></cell><cell>50</cell><cell>49.95</cell><cell>46.26</cell></row><row><cell>Lobe Descriptor</cell><cell></cell><cell>7</cell><cell>48.65</cell><cell>43.97</cell></row><row><cell>Regional Moments</cell><cell></cell><cell>7</cell><cell>48.58</cell><cell>41.74</cell></row><row><cell>Area Width Factor</cell><cell></cell><cell>13</cell><cell>49.89</cell><cell>38.79</cell></row><row><cell>Shape Statistics</cell><cell></cell><cell>4</cell><cell>39.66</cell><cell>34.34</cell></row><row><cell cols="2">Contour Pnt. Dist. Hist.</cell><cell>7</cell><cell>33.16</cell><cell>30.14</cell></row><row><cell>Ang. Code Hist.</cell><cell></cell><cell>10</cell><cell>33.76</cell><cell>26.61</cell></row><row><cell>Morph. Elongation</cell><cell></cell><cell>1</cell><cell>9.93</cell><cell>11.66</cell></row><row><cell>Convex Area Ratio</cell><cell></cell><cell>1</cell><cell>11.50</cell><cell>11.60</cell></row><row><cell>Convex Perim. Ratio</cell><cell></cell><cell>1</cell><cell>12.36</cell><cell>11.14</cell></row><row><cell>Compactness</cell><cell></cell><cell>1</cell><cell>11.47</cell><cell>9.24</cell></row><row><cell>All shape features</cell><cell></cell><cell>102</cell><cell>77.44</cell><cell>67.89</cell></row><row><cell>Feature Name</cell><cell cols="2">Length</cell><cell>Cross Val. Acc.(%)</cell><cell>Test Acc.(%)</cell></row><row><cell>Orientation Histogram</cell><cell></cell><cell>6</cell><cell>38.81</cell><cell>24.64</cell></row><row><cell>Texture</cell><cell></cell><cell>8</cell><cell>26.26</cell><cell>15.60</cell></row><row><cell>Smoothness</cell><cell></cell><cell>1</cell><cell>7.22</cell><cell>5.44</cell></row><row><cell>All texture features</cell><cell></cell><cell>15</cell><cell>66.26</cell><cell>33.42</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,208.67,352.71,198.01,8.74"><head>Table 2 :</head><label>2</label><figDesc>Individual texture feature accuracies</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,215.41,163.03,184.53,95.88"><head>Table 3 :</head><label>3</label><figDesc>Classifier combination accuracies.</figDesc><table coords="11,250.38,195.86,114.60,63.06"><row><cell>Photo type</cell><cell>Accuracy (%)</cell></row><row><cell>Picked</cell><cell>36.47</cell></row><row><cell>Leaf</cell><cell>15.49</cell></row><row><cell>Branch</cell><cell>8.33</cell></row><row><cell>Leafage</cell><cell>4.43</cell></row><row><cell>All categories</cell><cell>15.06</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,147.10,262.42,321.16,8.74"><head>Table 4 :</head><label>4</label><figDesc>Recognition accuracies for photos using automatic segmentation.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,134.77,200.49,345.83,120.14"><head>Table 5 :</head><label>5</label><figDesc>Partial results from the plant identification competition in ImageClef2012, showing the top-performing systems ordered according to mean performances.</figDesc><table coords="12,150.35,200.49,308.55,74.02"><row><cell></cell><cell cols="5">Auto/Manual Scan Scan-like Photo Mean</cell></row><row><cell>Sabanci Okan-run2</cell><cell>Manual</cell><cell>0.58</cell><cell>0.55</cell><cell>0.22</cell><cell>0.45</cell></row><row><cell>Sabanci Okan-run1</cell><cell>Automatic</cell><cell>0.58</cell><cell>0.55</cell><cell>0.16</cell><cell>0.43</cell></row><row><cell cols="2">INRIA Imedia plantnet run1 Automatic</cell><cell>0.49</cell><cell>0.54</cell><cell>0.22</cell><cell>0.42</cell></row><row><cell cols="2">INRIA Imedia plantnet run2 Automatic</cell><cell>0.39</cell><cell>0.59</cell><cell>0.21</cell><cell>0.40</cell></row><row><cell>LSYS DYNI run 3</cell><cell>Automatic</cell><cell>0.41</cell><cell>0.42</cell><cell>0.32</cell><cell>0.38</cell></row><row><cell>ARTELAB run 1</cell><cell>Automatic</cell><cell>0.40</cell><cell>0.37</cell><cell>0.14</cell><cell>0.30</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,397.87,59.76,7.86;12,219.38,397.87,156.32,7.86;12,392.38,397.87,88.21,7.86;12,151.52,408.83,139.16,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,219.38,397.87,152.32,7.86">Extending morphological covariance</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Aptoula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,392.38,397.87,84.02,7.86">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4524" to="4535" />
			<date type="published" when="2012-12">December 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,419.45,337.64,7.86;12,151.52,430.41,329.07,7.86;12,151.52,441.36,317.96,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,279.90,419.45,200.69,7.86;12,151.52,430.41,132.97,7.86">A basin morphology approach to colour image segmentation by region merging</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Aptoula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>LefÃ¨vre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,312.27,430.41,168.32,7.86;12,151.52,441.36,67.50,7.86">Proceedings of the Asian Conference in Computer Vision</title>
		<meeting>the Asian Conference in Computer Vision<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-11">November 2007</date>
			<biblScope unit="volume">4843</biblScope>
			<biblScope unit="page" from="935" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,451.98,337.64,7.86;12,151.52,462.94,202.84,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,267.34,451.98,160.66,7.86">On the morphological processing of hue</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Aptoula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>LefÃ¨vre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,437.83,451.98,42.77,7.86;12,151.52,462.94,71.32,7.86">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1394" to="1401" />
			<date type="published" when="2009-08">August 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,473.55,337.64,7.86;12,151.52,484.51,320.33,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,287.07,473.55,193.52,7.86;12,151.52,484.51,114.08,7.86">Shape classification using complex network and multi-scale fractal dimension</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">M</forename><surname>Bruno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,274.00,484.51,111.56,7.86">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="51" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,495.13,337.63,7.86;12,151.52,506.09,329.07,7.86;12,151.52,517.05,158.72,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,366.39,495.13,114.20,7.86;12,151.52,506.09,133.33,7.86">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName coords=""><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,296.67,506.09,183.92,7.86;12,151.52,517.05,81.29,7.86">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="509" to="522" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,527.66,337.64,7.86;12,151.52,538.62,329.07,7.86;12,151.52,549.58,249.81,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,271.04,527.66,209.56,7.86;12,151.52,538.62,101.51,7.86">The morphological approach to segmentation: the watershed transformation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Beucher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,373.80,538.62,106.80,7.86;12,151.52,549.58,79.10,7.86">Mathematical Morphology in Image Processing</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Dougherty</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Dekker</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="433" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,560.19,337.63,7.86;12,151.52,571.15,290.36,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,375.83,560.19,104.76,7.86;12,151.52,571.15,86.44,7.86">Fractal dimension applied to plant identification</title>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">M</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">O</forename><surname>Plotze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Falvo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,245.72,571.15,82.16,7.86">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2722" to="2733" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,581.77,337.64,7.86;12,151.52,592.73,171.30,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,293.92,581.77,186.67,7.86;12,151.52,592.73,35.49,7.86">Histograms of oriented gradients for human detection</title>
		<author>
			<persName coords=""><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,206.55,592.73,23.34,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,603.34,337.64,7.86;12,151.52,614.30,329.07,7.86;12,151.52,625.26,292.22,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,324.86,614.30,155.73,7.86;12,151.52,625.26,15.40,7.86">The clef 2011 plant image classification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthelemy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Birnbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mouysset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,186.37,625.26,103.78,7.86">CLEF 2011 working notes</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,635.88,337.98,7.86;12,151.52,646.84,329.07,7.86;12,151.52,657.79,100.27,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,198.63,646.84,178.17,7.86">The imageclef 2012 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>GoÃ«au</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthelemy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Molino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,398.45,646.84,82.14,7.86;12,151.52,657.79,19.67,7.86">CLEF 2011 working notes</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,120.67,337.98,7.86;13,151.52,131.63,279.05,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,319.47,120.67,161.12,7.86;13,151.52,131.63,80.28,7.86">Plant image retrieval using color, shape and texture features</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Kebapci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yanikoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Unal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,239.68,131.63,91.29,7.86">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2010-04">April 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,142.59,337.98,7.86;13,151.52,153.55,37.37,7.86;13,221.85,153.55,258.75,7.86;13,151.52,164.51,250.67,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,329.16,142.59,151.44,7.86;13,151.52,153.55,37.37,7.86;13,221.85,153.55,114.69,7.86">Multiple classification of plant leaves based on transform and lbp operator</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Man</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,364.16,153.55,116.43,7.86;13,151.52,164.51,86.63,7.86">International Conference on Intelligent Computing</title>
		<meeting><address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="432" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,175.46,337.98,7.86;13,151.52,186.42,266.27,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="13,200.41,175.46,242.12,7.86">A threshold selection method from gray-level histograms</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,458.45,175.46,22.14,7.86;13,151.52,186.42,190.41,7.86">IEEE Transactions on Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,197.38,337.98,7.86;13,151.52,208.34,329.07,7.86;13,151.52,219.30,20.99,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="13,246.11,197.38,234.48,7.86;13,151.52,208.34,115.18,7.86">A novel contour descriptor for 2d shape matching and its application to image retrieval</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,273.06,208.34,114.24,7.86">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="286" to="294" />
			<date type="published" when="2011-03">March 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,230.26,337.98,7.86;13,151.52,241.22,329.07,7.86;13,151.52,252.18,111.99,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="13,202.89,230.26,277.70,7.86;13,151.52,241.22,52.61,7.86">Constrained connectivity for hierarchical image partitioning and simplification</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Soille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,212.84,241.22,264.05,7.86">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1132" to="1145" />
			<date type="published" when="2008-07">July 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,263.14,337.98,7.86;13,151.52,274.09,329.07,7.86;13,151.52,285.05,150.31,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="13,370.82,263.14,109.77,7.86;13,151.52,274.09,76.32,7.86">Memory conscious sketched symbol recognition</title>
		<author>
			<persName coords=""><forename type="first">Caglar</forename><surname>Tirkaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Berrin</forename><surname>Yanikoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Metin Sezgin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,253.68,274.09,222.73,7.86">21st International Conference on Pattern Recognition</title>
		<meeting><address><addrLine>City, JAPAN</addrLine></address></meeting>
		<imprint>
			<publisher>Tsukuba Science</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,296.01,337.97,7.86;13,151.52,306.97,280.01,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="13,277.94,296.01,128.35,7.86">Shape based leaf image retrieval</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,413.71,296.01,66.87,7.86;13,151.52,306.97,155.68,7.86">IEE Proceedings in Vision, Image and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="43" />
			<date type="published" when="2003-02">February 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,317.93,337.97,7.86;13,151.52,328.89,263.59,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,302.86,317.93,173.48,7.86">Extension of quasi-flat zones to color images</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Aptoula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>LefÃ¨vre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,151.52,328.89,174.67,7.86">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Under review</note>
</biblStruct>

<biblStruct coords="13,142.62,339.85,337.98,7.86;13,151.52,350.81,329.07,7.86;13,151.52,361.77,329.07,7.86;13,151.52,372.73,143.78,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,458.70,339.85,21.89,7.86;13,151.52,350.81,329.07,7.86;13,151.52,361.77,75.84,7.86">Plant species identification, size, and enumeration using machine vision techniques on near-binary images</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Woebbecke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Von Bargen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">A</forename><surname>Mortensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,250.76,361.77,142.23,7.86">Optics in Agriculture and Forestry</title>
		<meeting><address><addrLine>Boston, USA</addrLine></address></meeting>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">1836</biblScope>
			<biblScope unit="page" from="208" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,383.68,337.97,7.86;13,151.52,394.64,221.65,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="13,368.74,383.68,111.85,7.86;13,151.52,394.64,91.35,7.86">Shape-based image retrieval in botanical collections</title>
		<author>
			<persName coords=""><forename type="first">Itheri</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>HervÃ©</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nozha</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,262.15,394.64,17.95,7.86">PCM</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="357" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,405.60,261.93,7.86;13,425.05,405.60,55.54,7.86;13,151.52,416.56,329.07,7.86;13,151.52,427.52,126.14,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="13,425.05,405.60,55.54,7.86;13,151.52,416.56,222.41,7.86">Sabanci-Okan system at imageclef 2011: Plant identification task</title>
		<author>
			<persName coords=""><forename type="first">Berrin</forename><surname>Yanikoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erchan</forename><surname>Aptoula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Caglar</forename><surname>Tirkaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,406.93,416.56,73.67,7.86;13,151.52,427.52,97.44,7.86">CLEF (Notebook Papers/Labs/Workshop)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,438.48,337.97,7.86;13,151.52,449.44,329.07,7.86;13,151.52,460.40,79.86,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="13,312.12,438.48,168.47,7.86;13,151.52,449.44,212.43,7.86">Identification of idealized leaf types using simple dimensionless shape factors by image analysis</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Yonekawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,372.07,449.44,102.97,7.86">Transactions of the ASAE</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1525" to="2533" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
