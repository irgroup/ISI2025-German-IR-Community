<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,148.22,151.13,298.75,14.04;1,133.70,169.13,327.66,14.04;1,241.97,187.13,111.28,14.04">REGIMvid at ImageCLEF2012: Concept-based Query Refinement and Relevance-based Ranking Enhancement for Image Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,124.70,227.42,58.32,11.04"><forename type="first">Rim</forename><surname>Fakhfakh</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">REGIM: REsearch Group on Intelligent Machines</orgName>
								<orgName type="institution" key="instit1">University of Sfax</orgName>
								<orgName type="institution" key="instit2">ENIS</orgName>
								<address>
									<addrLine>BP W</addrLine>
									<postCode>3038</postCode>
									<settlement>Sfax</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,190.32,227.42,49.58,11.04"><forename type="first">Ghada</forename><surname>Feki</surname></persName>
							<email>ghada.feki@yahoo.framel.ksibi</email>
							<affiliation key="aff0">
								<orgName type="laboratory">REGIM: REsearch Group on Intelligent Machines</orgName>
								<orgName type="institution" key="instit1">University of Sfax</orgName>
								<orgName type="institution" key="instit2">ENIS</orgName>
								<address>
									<addrLine>BP W</addrLine>
									<postCode>3038</postCode>
									<settlement>Sfax</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,247.49,227.42,70.80,11.04"><roleName>Anis</roleName><forename type="first">Amel</forename><surname>Ksibi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">REGIM: REsearch Group on Intelligent Machines</orgName>
								<orgName type="institution" key="instit1">University of Sfax</orgName>
								<orgName type="institution" key="instit2">ENIS</orgName>
								<address>
									<addrLine>BP W</addrLine>
									<postCode>3038</postCode>
									<settlement>Sfax</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,320.78,227.42,52.66,11.04"><forename type="first">Ben</forename><surname>Ammar</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">REGIM: REsearch Group on Intelligent Machines</orgName>
								<orgName type="institution" key="instit1">University of Sfax</orgName>
								<orgName type="institution" key="instit2">ENIS</orgName>
								<address>
									<addrLine>BP W</addrLine>
									<postCode>3038</postCode>
									<settlement>Sfax</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,395.25,227.42,75.37,11.04"><forename type="first">Chokri</forename><forename type="middle">Ben</forename><surname>Amar</surname></persName>
							<email>chokri.benamar@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="laboratory">REGIM: REsearch Group on Intelligent Machines</orgName>
								<orgName type="institution" key="instit1">University of Sfax</orgName>
								<orgName type="institution" key="instit2">ENIS</orgName>
								<address>
									<addrLine>BP W</addrLine>
									<postCode>3038</postCode>
									<settlement>Sfax</settlement>
									<country key="TN">Tunisia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,148.22,151.13,298.75,14.04;1,133.70,169.13,327.66,14.04;1,241.97,187.13,111.28,14.04">REGIMvid at ImageCLEF2012: Concept-based Query Refinement and Relevance-based Ranking Enhancement for Image Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BD3DD89D2D3701FF80F1602E831950FD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Concept based image retrieval</term>
					<term>Query to concept mapping</term>
					<term>Relevance based ranking</term>
					<term>Inter-concept similarity graph</term>
					<term>Inter-images semantic similarity graph</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present our proposed approach that deals with two important steps in the retrieval process, which are query analysis and relevance-based ranking. First, query analysis takes into account two forms of queries: textual and visual which present the form of queries provided by ImageCLEF in the task "Visual concept detection, annotation, and retrieval using Flickr photos ". The main idea in the query analysis process is to interpret the textual and visual queries and select the most appropriate concepts according to the query to concept mapping. Even the list of concepts is retained, we perform a query refinement process to improve the quality of the interpretation of the query. Second, we try to achieve a high-quality relevance-based result not only with choosing an adequate similarity measure but also with enhancing obtained scores by elaborating a random walk with restart, which is performed over inter-images semantic similarity graph.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="769.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper presents the second participation of our group REGIMvid, within REGIM research unit, in the "Visual concept detection, annotation, and retrieval using Flickr photos " task <ref type="bibr" coords="1,269.21,618.07,11.28,11.04" target="#b0">[1]</ref>.</p><p>The aim in this task is to analyze a collection of Flickr photos in terms of their visual and/or textual features in order to detect the presence of one or more concepts. The detected concepts can then be used to automatically annotate images or to retrieve the best matching images to a given concept-oriented query. The main challenge for this task is to select the most appropriate concepts related to a query and so to ensure a good relevance-based result ranking. For the selection of the most adequate concepts from a query, we execute a query to concept mapping requiring a calculation of the semantic similarity measures between query keywords and concepts. Concerning the ranking step, we try to achieve a high-quality relevance-based result not only with choosing an adequate similarity measure but also with enhancing obtained scores by elaborating a random walk with restart. This refinement step is based on information, which are occurred from the inter-images semantic similarity graph. The rest of the paper is organized as follows. Section 2 describes our proposed relevance-based approach; Section 3 discusses the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>Relevance-based approach: Visual concept retrieval using Flickr photos task</p><p>Relevance computing in our approach implies 3 phases : First, we analyze the query to enhance the user request understanding . Second, we compare query concepts with concepts that annotate each image from the collection. Finally, scores obtained in the previous step are refined by the means of random walk with restart.</p><p>The following notations will be used. Given a set of query concepts = { , ,.., }, denote by = { , ,.., } the collection of images that are associated with the set of query concepts . This collection, which is a part of the large collection , is obtained by the inverted file construction in an off-line stage.</p><p>For image , denote by = { , ,.., }the set of its associated concepts.</p><p>The relevance scores of all images in D are represented in a vector , whose element denotes the relevance score of image with respect to the set of query concepts .</p><p>Relevance score reflects the degree of the existence of a given concept in the image . This score is normalized that we range it from 0 to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Query analysis</head><p>Query analysis in a retrieval process is a fundamental and critical step. The main challenge is to enhance the interpretation of user request in order to improve retrieved result quality. The proposed process takes into account the query format: title, description and images (fig2). Firstly, a textual analy-sis is performed on title and description components in order to deduct the main concepts within text. Secondly, a matching process is performed based on a concept within query and image data. The obtained results are, thirdly, merged. Finally, a refinement process is executed.</p><p>The following figure shows the query analysis process which goal is the selection of the most appropriate concepts related to each query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Textual query analysis .</head><p>The proposed treatments within textual analysis inherit from natural language processing standard. First we extract the representative information by the elimination of non informative terms called stoplist <ref type="bibr" coords="4,406.90,520.03,11.37,11.04" target="#b1">[2]</ref>. Keywords extracted from the initial query are divided into positive and negative terms by following a linguistic study: the positive keywords represent what is required in the query and the negative ones represent some details that should be avoided. Fig3 shows an example for a query keywords classification into positive and negative. Each retained keyword in the query representation is projected on the concept space; a query to concept mapping. Using WordNet <ref type="bibr" coords="5,380.16,310.97,12.46,11.04" target="#b2">[3]</ref> as taxonomy, we evaluate the similarity values between a keyword and the 94 proposed concepts. We retain the most related concept to each query term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual query analysis .</head><p>Regarding the initial query textual description, we distinguish between positive and negative images. Image is considered as positive if it translates the textual description and negative if there are some particularities which are not congruent to the textual description. Alike the textual treatment, we deduce a set of positive and negative concepts. The related concepts per image are extracted from the annotation file <ref type="bibr" coords="6,124.70,180.50,11.36,11.04" target="#b3">[4]</ref>.</p><p>In some query, the same concept appears in positive and negative images. In this particular case, we retain this concept as positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Textual and visual concepts fusion .</head><p>Textual and visual analyses bring out a set of positive and negative concept per each modality. The obtained sets are firstly merged. We compute a weight value for each positive concept.</p><p>Regarding the concept frequency in textual and visual component, the associated weight is computed as follows:</p><p>When the same concept appears in the positive and negative sets, we decide to maintain it as positive or negative as follows.</p><p>Let's note:  w: the weight of the redundant concept  moy: the average of all weights of positive concepts  nb: the number of positive concept  : the standard deviation between weight of positive concept.</p><p>According to the following formula we decide if either the redundant concept is positive or negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>If w [ moy-</head><p>, moy+ ], the concept is considered as positive and keeps the same weight else, we cancel its weight value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query refinement.</head><p>The final stage of the proposed process is an inter-concept relation study which goal is to enhance the query interpretation. Two possible scenarios can occur:  Query expansion: when adding new concepts to the previous set.  Concepts reweighting: when modifying existing concepts weights.</p><p>We built an inter-concepts semantic graph where concepts are interconnected between them. Weights within edges represent the semantic similarity value computed according to the WordNet hierarchy.  We perform a random walk over the semantic graph to refine our concepts' list either by reweighting concepts or by adding new concepts according to the semantic similarity measure between concepts. We attempt to use another type of graph called contextual graph <ref type="bibr" coords="7,321.14,605.95,12.42,11.04" target="#b4">[5]</ref> to bring the best refinement.</p><p>We also use another graph called contextual one <ref type="bibr" coords="7,358.46,631.27,12.32,11.04" target="#b4">[5]</ref> to bring the best refinement. This last, is built based on the contextual information interconcepts dealing with the co-occurrence by exploring Flicker resources as a largest public available multimedia corpus. To more refine our query after random walk step and the addition of new relevant concepts, we try to manage the semantic conflict <ref type="bibr" coords="8,396.76,164.90,12.42,11.04" target="#b5">[6]</ref> between the selected concepts: exclusion and implication. An exclusion conflict traduces a contradiction between selected concepts, i.e concepts "indoor" and "outdoor" cannot simultaneously occur in the same query. For such case, we have to decide which one to eliminate. The implication case is implied when two concepts are closely related (according to the semantic similarity value). In such case, concepts in relation with theses in the query are added to this last.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Query-Image similarity</head><p>Based on experimental semantic similarity measures study <ref type="bibr" coords="8,393.72,329.93,11.41,11.04" target="#b6">[7]</ref>, we decide to adopt an approach for semantic similarity between a given query and an image that is analogous to Cosine similarity measure. The semantic similarity between and , which are respectively the sets of query concepts and image concepts, is defined as: This semantic similarity is computed between the query concepts and each concept sets of images that belong to the sub-collection relative to this query. For other images belonging to , their similarity scores are equal to zero.</p><p>Instead of searching through a large collection, a user query is concerned in only a part of selected image results from the whole collection. An inverted file is a data structure where images are indexed by (concept × image) structure instead of the conventional (image × concept) structure. It allows saving much time since the algorithm search concerns only the sub-collections. We denote by the vector of semantic similarity between a query and the collection . It is defined as follows:</p><p>This vector will be used as an input for refinement phase, which is provided thanks to a random walk with restart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Result refinement</head><p>We try to achieve a high-quality relevance-based result not only with choosing an adequate similarity measure but also with improving obtained scores. Therefore, elaborating a random walk <ref type="bibr" coords="9,296.77,252.74,12.30,11.04" target="#b7">[8]</ref> is an attempt for enhancing results.</p><p>In this section, we denote by a similarity matrix whose element indicates the similarity between images and and the elements on the diagonal are null.</p><p>Semantic similarity scores vector according to a given query and the interimages semantic similarity graph are the input of random walk process <ref type="bibr" coords="9,124.70,367.37,11.36,11.04" target="#b8">[9]</ref>. It is defined as: consider a random image that starts from node i, the image iteratively transmits to its neighborhood with the probability that is proportional to their edge weights. Also at each step, it has some probability c to return to the node i.</p><p>After some iteration we have an optimal scores vector .</p><p>The inter-images semantic similarity graph illustrates the semantic relationship between images in the collection. It informs on the semantic similarity degree between each pair of images. We adopt an approach for semantic similarity between tow images that is analogous to Jaccard similarity. The similarity between and , which are respectively the sets of image concepts and image concepts, is defined as:</p><p>The semantic similarity graph consists in the matrix , which must be normalized. In fact, there are several ways to normalize the weighted matrix .</p><p>The most natural way might be the row normalization. Complementarily, we normalize W using the normalized graph Lapalician, as follows:</p><p>With the vector is defined as:</p><p>The refinement method consists in a random walk process, and it will converge to a fixed point. We use the graph that illustrates relations between images for the random walk since the relevance scores of semantic similar images should be close. The convergence condition consists in comparing the difference between two successive scores vectors resulting by the Random Walk with a fixed parameter ɛ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental results</head><p>We describe the experimental study conducted to evaluate the proposed approach within the relevance computing. The experiments were performed on ImageCLEF12 benchmark.</p><p>The relevance-based approach for image retrieval is experimented with the Visual concept retrieval using Flickr photos1 task. It consists in analyzing a Flickr photos collection. We look for extracting concepts from the visual and textual features. These concepts are used in image retrieval.</p><p>The global results of our system are relatively bad. This fact is explained by the huge number of selected concepts related to each query. This flow of information can deviate the real meaning of the initial query.</p><p>For example the treatment of the query n°29 "sleeping baby" with the description "The user is looking for photos showing a sleeping (calm, quiet) ba-by" , provides a list of concepts composed from 11 items which are: one, baby, partial_blur, day, no_blur ,portrait, indoor, happy, calm, overlay, smoke.</p><p>We notice that the appearance of wrong concepts is the consequence of the query to concept mapping shortage. For example, the keywords "sleeping" and "quiet" appearing in the previous textual query are respectively projected to "smoke" and "day" after performing the step of query to concept mapping.</p><p>For the query n°5 "hot air ballon", selected concepts after query analysis process are far from the user request. In fact, these concepts are textually close to the query (air ballon, airplane) but semantically different.</p><p>The deep study of queries individually shows that our approach can perform well in some cases. for expamle , the query n°25 "grass field recreation" previously mentioned in Fig2. In effect, the majority of extracted keywords from the textual query belong to the concepts set. (grass is a concept) Finally, we notice that the setting of the parameter in the random walk has relatively minor impact on the ordering of the images in the result list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we present our proposed approach that deals with two important steps in the retrieval process, which are query analysis and relevance-based ranking. Query analysis takes into account two forms of queries: textual and visual which present the form of queries provided by ImageCLEF.</p><p>Relevance-based result is based on choosing an adequate similarity measure and enhancing obtained scores by elaborating a random walk with restart, which is performed over inter-images semantic similarity graph.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,256.01,642.64,83.03,9.00;2,126.20,327.26,345.60,301.55"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. General scheme</figDesc><graphic coords="2,126.20,327.26,345.60,301.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,246.77,439.94,158.43,9.00"><head>Fig. 2 .QueryFig. 3 .</head><label>23</label><figDesc>Fig. 2. : Overview of query analysis process</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,210.17,662.83,174.83,9.00;5,126.20,470.39,345.75,178.50"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Positive and negative image classification</figDesc><graphic coords="5,126.20,470.39,345.75,178.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,124.70,327.77,217.08,11.04"><head>Fig 5</head><label>5</label><figDesc>Fig 5 shows a partial view of the semantic graph:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,229.97,536.20,135.25,9.00"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. inter-concepts semantic graph</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="10,127.70,675.55,142.50,9.00"><p>http://imageclef.org/2012/photo-flickr</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="11,132.67,659.95,337.82,9.00;11,141.74,670.99,328.84,9.00;11,141.74,682.15,26.88,9.00" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Amel</forename><surname>Ksibi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anis</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chokri</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amarregimvid</forename></persName>
		</author>
		<title level="m" coord="11,375.70,659.95,94.80,9.00;11,141.74,670.99,328.84,9.00;11,141.74,682.15,7.68,9.00">ImageCLEF2011: Integrating Contextual Information to Enhance Photo Annotation and Concept-based Retrieval</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.67,149.27,328.55,9.00" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="12,219.10,149.27,168.26,9.00">Introduction to Modern Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>McGraw-Hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.67,170.15,337.58,9.00;12,141.74,181.31,20.51,9.00" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="12,255.04,170.15,109.38,9.00">An Electronic Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wordnet</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.67,202.19,338.03,9.00;12,141.74,213.23,328.39,9.00;12,141.74,224.27,67.19,9.00" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,308.53,202.19,162.18,9.00;12,141.74,213.23,191.23,9.00">A Fuzzy Ontology-Based Framework for reasoning in Visual Video Content Analysis and Indexing</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zarka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Elleuch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Alimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,347.96,213.23,122.18,9.00;12,141.74,224.27,42.93,9.00">the intl Workshop on Multimedia Data Mining</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.67,245.15,337.91,9.00;12,141.74,256.31,221.31,9.00" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="12,317.55,245.15,153.04,9.00;12,141.74,256.31,195.26,9.00">Integrating Contextual Information to Enhance Photo Annotation and Concept-based Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Amel</forename><surname>Ksibi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anis</forename><surname>Ben Ammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chokri</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Amar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.67,277.22,337.89,9.00;12,141.74,288.26,328.72,9.00;12,141.74,299.30,234.09,9.00" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,233.08,288.26,237.38,9.00;12,141.74,299.30,6.11,9.00">Exploiting Visual Concepts to Improve Text-Based Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Sabrina</forename><surname>Tollari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marcin</forename><surname>Detyniecki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christophe</forename><surname>Marsala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Fakeri Tabrizi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Massih-Reza</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,156.02,299.30,193.78,9.00">European Conference on Information Retrieval (ECIR)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.67,320.18,337.70,9.00;12,141.74,331.22,328.72,9.00;12,141.74,342.38,126.04,9.00" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,181.78,320.18,288.59,9.00;12,141.74,331.22,64.93,9.00">Comprehensive Survey on Distance/Similarity Measures between Probability Density Functions</title>
		<author>
			<persName coords=""><forename type="first">S.-H</forename><surname>Cha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,212.57,331.22,257.89,9.00;12,141.74,342.38,49.60,9.00">International Journal Of Mathematical Models And Methods In Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.67,363.26,337.91,9.00;12,141.74,374.30,143.06,9.00" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,267.13,363.26,203.46,9.00;12,141.74,374.30,16.87,9.00">Random walk with restart: fast solutions and applications</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-Yu</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,164.03,374.30,47.04,9.00">KnowlInfSyst</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="327" to="346" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.67,395.18,337.79,9.00;12,141.74,406.22,328.87,9.00;12,141.74,417.38,73.28,9.00" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,326.78,395.18,143.68,9.00;12,141.74,406.22,50.87,9.00">Social Image Search with Diverse Relevance Ranking</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X.-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,268.58,406.22,41.59,9.00">MMM 2010</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Boll</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag Berlin</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">5916</biblScope>
			<biblScope unit="page" from="174" to="184" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
