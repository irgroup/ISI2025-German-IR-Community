<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,128.04,152.87,339.14,12.58;1,253.50,170.88,88.33,12.58">KIDS-NUTN at ImageCLEF 2012 Photo Annotation and Retrieval Task</title>
				<funder ref="#_NvqU8hd">
					<orgName type="full">National Science Council of Taiwan</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,171.90,208.21,72.30,11.09"><forename type="first">Been-Chian</forename><surname>Chien</surname></persName>
							<email>bcchien@mail.nutn.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Knowledge, Information</orgName>
								<orgName type="department" key="dep2">Database System Laboratory Department of Computer Science</orgName>
								<orgName type="department" key="dep3">Information Engineering</orgName>
								<orgName type="institution">National University of Tainan</orgName>
								<address>
									<settlement>Tainan</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,251.08,208.21,60.86,11.09"><forename type="first">Guan-Bin</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Knowledge, Information</orgName>
								<orgName type="department" key="dep2">Database System Laboratory Department of Computer Science</orgName>
								<orgName type="department" key="dep3">Information Engineering</orgName>
								<orgName type="institution">National University of Tainan</orgName>
								<address>
									<settlement>Tainan</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,319.16,208.21,40.69,11.09"><forename type="first">Li-Ji</forename><surname>Gaou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Knowledge, Information</orgName>
								<orgName type="department" key="dep2">Database System Laboratory Department of Computer Science</orgName>
								<orgName type="department" key="dep3">Information Engineering</orgName>
								<orgName type="institution">National University of Tainan</orgName>
								<address>
									<settlement>Tainan</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,367.20,208.21,51.24,11.09"><forename type="first">Chia-Wei</forename><surname>Ku</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Knowledge, Information</orgName>
								<orgName type="department" key="dep2">Database System Laboratory Department of Computer Science</orgName>
								<orgName type="department" key="dep3">Information Engineering</orgName>
								<orgName type="institution">National University of Tainan</orgName>
								<address>
									<settlement>Tainan</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,221.04,220.16,70.17,11.09"><forename type="first">Rong-Sing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Knowledge, Information</orgName>
								<orgName type="department" key="dep2">Database System Laboratory Department of Computer Science</orgName>
								<orgName type="department" key="dep3">Information Engineering</orgName>
								<orgName type="institution">National University of Tainan</orgName>
								<address>
									<settlement>Tainan</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,315.54,220.16,58.61,11.09"><forename type="first">Siao-En</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Knowledge, Information</orgName>
								<orgName type="department" key="dep2">Database System Laboratory Department of Computer Science</orgName>
								<orgName type="department" key="dep3">Information Engineering</orgName>
								<orgName type="institution">National University of Tainan</orgName>
								<address>
									<settlement>Tainan</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,128.04,152.87,339.14,12.58;1,253.50,170.88,88.33,12.58">KIDS-NUTN at ImageCLEF 2012 Photo Annotation and Retrieval Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6C463EE8E83B64CEC8EE16DA15FBF346</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>Image Annotation</term>
					<term>Image Classification</term>
					<term>Image retrieval</term>
					<term>Random Forest</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The task of visual concept detection, annotation, and retrieval using Flickr photos at ImageCLEF 2012 was organized as two subtasks: concept annotation and concept retrieval. In this paper, we present the effort of KIDS lab for the two subtasks. The proposed approaches combine various visual and textual features, dimension reduction methods, the random forest classification models, and the semi-supervised learning strategy. For the concept annotation subtask, the annotation results show that combination of tags and visual features outperforms visual-only features while using the same classification model. The results also show that semi-supervised learning is not superior to supervised learning in this subtask. Further, it does not seem able to gain more advantage on F-measure when more different visual features were used. For the concept retrieval task, the results illustrate that the textual features contain much richer informatics than visual features in general retrieved concepts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ImageCLEF 2012 visual concept detection, annotation, and retrieval task using Flickr photos arranged two subtasks <ref type="bibr" coords="1,278.24,556.22,15.40,11.09" target="#b12">[ 13]</ref>: the concept annotation task and the concept-based retrieval task. The challenge of the first subtask, concept annotation, is to assign each image to a set of concepts taken from a list of 94 pre-defined concepts automatically. This task takes a subset of the MIRFLICKR-1M collection containing 15,000 images for training and 10,000 images for testing. The second subtask, concept-based retrieval, aims on retrieving target images from a subset of the MIRFLICKR collection comprising of 200,000 photos for 42 concept queries. The queries are provided in the XML format containing title, description, and three annotated images located in the training set of subtask 1. In this paper, we describe the approaches used in the two subtasks including extraction of image features, concept learning models and concept retrieval methods.</p><p>The annotated concepts in this task cover a wide range of topics such as natural scene, animal kinds, human gender, human emotions, transportation tools, etc. Some of the concepts are so abstract and ambiguous in semantics that even users can not annotate the images well. In order to annotate images precisely, we worked at the tasks in the following aspects. First, various visual features are extracted from images to investigate the correlation between the visual features and the annotation concepts. Second, the high-dimension and multilingual textual tags need to be analyzed, processed and reduced for improving the efficiency of image annotation and retrieval in large datasets. Third, effective classification models and efficient learning methods are necessary for integrating visual and textual features to generate multi-label classifiers in annotating numerous concepts.</p><p>Our concept annotation approaches are the combinations of different techniques including image features extraction, text processing, dimension reduction, the random forest classification models, and the semi-supervised learning strategy. The validation set used 5,000 images selected from the given 15,000 training images and the left 10,000 images were used for training in our evaluation. After tuning the parameters, the final classification models are learned from the total 15,000 training images and the 10,000 testing images were annotated. We also used the modified MBRM <ref type="bibr" coords="2,458.89,352.21,11.69,11.09" target="#b3">[ 4]</ref> method as a baseline method to observe and compare the effectiveness of the annotation approaches. The concept retrieval approaches are based on the concept annotation approaches. This paper will also discuss the ranking method of images retrieval from the given three query images for the subtask 2.</p><p>The rest of this paper is organized as follows. In section 2, we describe the extraction and preprocess of visual features and textual features, respectively. The feature reduction method, concept learning methods and classification models are presented in Section 3. The concepts annotation and retrieval methods are also given in this section. Section 4 shows and discusses the experimental results for different submission runs. Finally, we draw a conclusion for our labs in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Extraction and Preprocess of Image Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature Extraction</head><p>The original image data set in the Flickr photo task consists of JPEG images, EXIFs in image files, and supplementary tags for each image. The main image features are thus considered to be extracted from the JPEG image and the textual part. In this subsection we first introduce the extraction of visual features and textual features, respectively. Then, the process of normalization on visual features is described in the next subsection.</p><p>Visual Features. The annotated concepts in the Flickr photo task are very diverse.</p><p>Although the total annotated 94 concepts are categorized as natural elements, environment, people, image elements, and human elements, the job of concept annotating is still ambiguous and vague in visual for photos from the viewpoints of different persons. For collecting visual features as many as possible from an image, first, an image was equally segmented into 16 sub-images (in 4 by 4 blocks). The original image and its corresponding 16 sub-images, totally 17 images, are the sources of generating visual features. Basically, the four visual features, AutoColorCorrelogram <ref type="bibr" coords="3,456.23,184.21,10.59,11.09" target="#b4">[ 5]</ref>,</p><p>ColorLayout <ref type="bibr" coords="3,178.61,196.21,10.55,11.09" target="#b0">[ 1]</ref>, FCTH <ref type="bibr" coords="3,223.78,196.21,10.55,11.09" target="#b1">[ 2]</ref>, Gabor <ref type="bibr" coords="3,268.43,196.21,15.21,11.09" target="#b10">[ 11]</ref>, were extracted from each original image and 16 sub-images. Gist <ref type="bibr" coords="3,210.07,208.21,16.73,11.09" target="#b11">[ 12]</ref> feature is only applied to the original images. Each image generates a list of multi-dimensional data.</p><p>Except for extracting the five visual features, the region of interest (ROI) in original images are also marked automatically by the visual attention model proposed by Itti, et al. <ref type="bibr" coords="3,164.89,256.21,10.52,11.09" target="#b6">[ 7]</ref>. We modified the method by applying 6-level Gaussian pyramid to generate the saliency map representing the degree of concern in an image. Then, the region growing method <ref type="bibr" coords="3,214.96,280.21,16.67,11.09" target="#b12">[ 13]</ref> was used to mark the appropriate ROIs. For the 16 blocks of sub-image, each block of sub-image is marked as foreground if the area of a block is covered over 60% by marked ROIs; otherwise, the block is marked as background. The AutoColorCorrelogram values in the blocks of background sub-image then were averaged as the visual feature of ROI background.</p><p>To recognize the number of people in photos, the package of face detection in OpenCV was used to detect and estimate the number of persons in each photo. The numbers of dimensions for the extracted visual features are summarized in Table <ref type="table" coords="3,450.85,364.21,3.76,11.09" target="#tab_0">1</ref>.</p><p>The visual features including AutoColorCorrelogram, ColorLayout, FCTH, and Gabor were extracted by applying LIRE (Lucene Image REtrieval) JAVA library. <ref type="foot" coords="3,467.28,386.87,3.24,7.17" target="#foot_0">1</ref>The face detection tool was implemented by using OpenCV. <ref type="foot" coords="3,387.72,398.87,3.24,7.17" target="#foot_1">2</ref> The ROI marking method and the Gist method were designed and implemented by ourselves. Textual Features. The textual information for the Flickr photos comes from the EXIF (Exchangeable image file format) and announced tags of each image. The EXIF is a standard specification that specifies the formats of media data like images and sounds produced by digital cameras. A given EXIF contains 407 fields totally in each image, but only 24 EXIF fields were selected (e.g. black level, blur warning, brightness, compression, contrast, data and time, zoom, expiration, ISO, noise, etc.). The other source of textual features is the description file of tags for each image. The tags describe some kinds of related semantic information of the images. Before applying the tags information to annotate images, two problems should be solved. First, the practical tags of images are multilingual. Actually, more than 68 different languages are found in the set of tags. Synonyms of terms need to be unified. Second, the problem of high-dimension features must be reduced. To resolve the two problems, the Google translation tools <ref type="foot" coords="4,265.20,194.87,3.24,7.17" target="#foot_2">3</ref> were used to translate the multilingual tags into English, and the stop words then were deleted from the set of tags. The number of the final tags is 60821 terms in English. The term frequency for each tag was also counted and recorded.</p><p>Further, in order to support the detection of humans in photos, the package of face detection in OpenCV was used to detect and estimate the number of persons in each photo. The range of the estimated number of people is between 0 and 13. The face number for each photo is marked by binary information as 14 features. The numbers of final textual features are listed in Table <ref type="table" coords="4,294.81,292.21,3.74,11.09" target="#tab_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Preprocess of Visual Features</head><p>The final extracted visual features and textual features in subsection 2.1 are quite various in dimensions and ranges of values. Furthermore, the high-dimension feature is an important problem for learners to generate annotating models. For reducing the dimensions and combining the extracted visual and textual features in a unified representation, the visual features are processed as follows.</p><p>Let I be an image set with n images and I i be an image in I. The x denotes the vector of a specified visual feature and x i represents the multi-dimension vector of the specified visual feature for the image I i . We have x i = (x i1 , …, x im ), an m dimensional vector, and x ij is the value of jth-dimension of the x i for the image I i , 1  j m. We assume that C 1 , C 2 , … , C K represent the K possible annotated concepts in the system.</p><p>|C k | denotes the number of images belonging to the concept C k in the image set I. We first calculate the mean vector  k and deviation vector  k of the visual feature for each concept C k , as follows:</p><formula xml:id="formula_0" coords="4,193.08,599.60,277.49,43.40">k C I i k C k i    x  , k C I k i k C k i     2 ) (   x , 1  k K.<label>(1)</label></formula><p>Then, the concept similarity of the visual feature x i corresponding to the concept C k can be defined as</p><formula xml:id="formula_1" coords="5,209.16,180.92,261.41,38.77">                     m j kj kj ij ik x y 1 2 exp   , 1  k K,<label>(2)</label></formula><p>where  kj and  kj are the jth-dimension values of the vectors  k and  k respectively, for the image I i I, 1  i n. Hence, a m-dimension visual feature x i of an image I i will be normalized as a K-dimension features y i = [y ik ], 0 y ik 1  k K. Since multidimensional visual features of an image, as shown in Table <ref type="table" coords="5,389.50,266.25,3.77,11.15" target="#tab_0">1</ref>, were transformed into 94 dimensions (the number of concepts), the total number of features is 6580 after the processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Feature Reduction and Concept Learning Models</head><p>Although we had reduced part of the number of features, the extracted visual features and textual features in the previous section still have very high dimensions (67439 features in total). Generally, it is not easy for any classification model to learn effective classifiers from high dimensional datasets efficiently. For dealing with the high dimensional datasets, we applied a feature reduction method, discriminant coefficient <ref type="bibr" coords="5,124.74,407.25,10.86,11.15" target="#b8">[ 9,</ref><ref type="bibr" coords="5,139.00,407.25,11.91,11.15" target="#b9">10]</ref>, to reduce dimensions before learning the classifiers. The submitted runs are mainly based on two learning models: the random decision trees <ref type="bibr" coords="5,388.96,419.25,11.69,11.15" target="#b2">[ 3]</ref> and the Multiple Bernoulli Relevance Models (MBRM) <ref type="bibr" coords="5,286.68,431.25,10.56,11.15" target="#b3">[ 4]</ref>. Except for the supervised learning strategy, the semi-supervised learning strategy is also considered for investigating the feasibility in image annotation. In this section, we briefly present the main methods used in this task including the feature reduction method, the concept classification models and the leaning strategies in the following subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Features Reduction</head><p>The reduction method is based on the discriminant coefficient proposed by Lin &amp; Chien <ref type="bibr" coords="5,152.09,541.29,10.84,11.15" target="#b8">[ 9,</ref><ref type="bibr" coords="5,166.24,541.29,11.90,11.15" target="#b9">10]</ref>. In the method, the discriminant coefficients are calculated by the difference between the statistics of two classes. Before calculating the discriminant coefficients, the image features need to be normalized according to the class of concept. Let y i be the concept similarity of visual features x i as defined in Section 2.2 and y ij is the jth dimension of transformed concept similarity for a visual feature in the image I i . For textual features, y i is the term frequency of textual features and y ij is the term frequency of the term j for the image I i . The normalization of visual and textual features are defined as</p><formula xml:id="formula_2" coords="5,235.32,642.33,235.25,42.91">| | k C I ij kj C y f k i    , for 1  k K.<label>(3)</label></formula><p>The normalized features can be denoted as a matrix F = [f ij ] KP , K is the number of conceptual classes, and P is the number of all transformed visual features and all final extracted textual features. The feature reduction method in <ref type="bibr" coords="6,373.99,172.41,16.71,11.15" target="#b9">[ 10]</ref> first calculates the relative discriminant variables of each feature for all K conceptual classes. Then, the discriminant variables are normalized to be the log-scaled discriminant coefficient matrix J = [J ij ] KP . The range of J ij is between 0 and 1. A large J ij represents that the jth feature has high discrimination on the concept C i . On the contrary, a small J ij value means that the jth feature provides less discernable information for the concept C i .</p><p>We assume that the matrix Y = [y ij ] nP , y ij is the visual and textual features for the image I i and n is the number of images in the training set. Finally, the goal of feature reduction is to find a transformation matrix T such that the number of visual and textual features is much smaller than the original features. The transformation of feature reduction can be completed by the following equation:</p><formula xml:id="formula_3" coords="6,267.30,311.44,199.38,12.96">T = Y  J t , (<label>4</label></formula><formula xml:id="formula_4" coords="6,466.68,313.11,3.90,11.15">)</formula><p>where J t is the transpose of matrix J. After transforming of the equation ( <ref type="formula" coords="6,420.72,333.15,3.55,11.15" target="#formula_3">4</ref>), the T is a n  K matrix which is used to replace the matrix Y as the reduced features of training set for learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Random Forest</head><p>The random decision tree method <ref type="bibr" coords="6,263.03,407.43,11.70,11.15" target="#b2">[ 3]</ref> is an ensemble classifier that first builds a number of decision trees randomly. Each decision tree is constructed by selecting a nontested feature randomly as the decision node at each level. The training data are not used in the tree construction and is independent from the tree structures completely. After the decision trees are built, the training data are used to update the statistics of the classes at each node for all random decision trees. While classifying an unknown example, the predicted class is estimated by trees voting or averaging the possibilities of all decision trees to determine the classification result. The Matlab code <ref type="foot" coords="6,209.46,502.13,3.24,7.17" target="#foot_3">4</ref> of the Random Forests was used in the task. For dealing with the multi-label problem, a two-class classifier was learned for each annotation concept. Although totally 94 classifiers should be learned, the random forest method is still efficient because the reduced matrix T is used to be the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Semi-supervised Learning</head><p>Generally, the training set containing only labeled data are applied to build classifiers by supervised learning method. In this paper, we also investigated the feasibility of applying semi-supervised learning. Semi-supervised learning uses both labeled data and unlabeled data to perform the learning process. The goal is to integrate the unlabeled data to improve the effectiveness of classification.</p><p>The first step of using semi-supervised learning is to use all ground truth labeled data to learn classifiers as Section 3.2. Next, the unlabeled data are classified and ranked by their voting ratios of random decision trees. The top 10 positive examples and top 10 negative examples from the unlabeled data are then added to the training set, and new classifiers are re-trained. Such a learning process proceeds k times iteratively. The final classification models are used to annotate the concept of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multiple Bernoulli Relevance Models method</head><p>The Multiple Bernoulli Relevance Models method (MBRM) was proposed by <ref type="bibr" coords="7,447.99,258.15,22.54,11.15">Feng,</ref><ref type="bibr" coords="7,124.74,270.15,32.85,11.15">et al. 4]</ref> to solving the problem of automatic image annotation. This method and its modified weighting version were implemented as the baseline methods in the annotation task. We briefly introduce the method in the following.</p><p>Let I denote the training set of annotated images, and I i be an image of I. Every image I i were cut into 16 blocks in 4 by 4 rectangular sub-images. We obtain one original image r 0 and the 16 sub-images r 1 , r 2 … r 16 . We then extract features from the 17 regions separately and assume that the features are denoted as f 0 … f 16 . Now let I j be a test image, and f' 0 … f' 16 denote the features of image I j . The joint probability P(I j , w) is computed for each word w in the annotation vocabulary. The annotations of image I j would be the top several words which have maximum probabilities. The joint probability P(I j , w) is defined as following equation:</p><formula xml:id="formula_5" coords="7,180.06,411.39,290.51,33.67">                   I i I p i q q p i T j I w P f f Sim I P w I P 16 0 16 0 ) | ( ) , ( ) ( ) , ( .<label>(5)</label></formula><p>We assume that the distribution of the training set is an uniform distribution, the probability P T (I i ) = 1/n, where n is the number of images in the training set I. The Sim(f p , f′ q ) stands for the similarity degree between the features f p and f′ q , and the P(w|I i ) is defined as following equation:</p><formula xml:id="formula_6" coords="7,230.70,510.06,239.87,28.48">n N N I w P w I w i i       ) , ( ) , ( ) | ( I ,<label>(6)</label></formula><p>where N (w, Ii) denotes the number of times the annotation w occurs in the image I i , N (w,I) denotes the number of times the annotation w occurs in the training set I, and  is the smooth parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Concept Retrieval Method</head><p>The concept retrieval task gave 42 queries containing a concept title, text description, and three images for each query. The query images are all in the training set and the test database comprises 20,000 photos selected from the MIRFLICKR collection. The approaches of retrieving the images with the same concept as the query are based on the concept annotation approaches in the subtask 1.</p><p>We first analyzed the concept ratios of the three images for each query. Next, we applied the concept annotation approaches used in the subtask 1 to annotate images in the test database. Finally, the images were ranked by the concept ratios and the voting ratios of random decision trees. Formally, we assume that the three query images are annotated by a few concepts individually. Let  ij be the voting ratio of the jth concept on the image I i , and w j be the ratio of the concept C i annotated by the three images, 1  j  K, where K is the number of concepts. The similarity degree of the image I i for the query Q are defined as</p><formula xml:id="formula_7" coords="8,244.02,254.03,222.66,29.22">    K j j ij i w I Q Sim 1 ) , (  . (<label>7</label></formula><formula xml:id="formula_8" coords="8,466.68,261.93,3.90,11.15">)</formula><p>5 Experimental Results and Discussions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The Concept Annotation Task</head><p>First, we would like to introduce the methods for the runs we submitted. In concept annotation subtask, we totally submitted five runs which based on two methods. One is the approach based on the feature reduction and random decision trees described in Section 3.1 to 3.3. The other is based the Multiple Bernoulli Relevance Models (MBRM) described in Section 3.4. The run_a1, run_a2, and run_a3 use the former method. Especially, the run_a2 applies the semi-supervised learning instead of supervised-learning. The run_a4 and run_a5 take the latter one. The run_a4 used annotation scores to weight the probabilities of words in images. The run_a5 only considered binary annotations. The used features and methods are summarized in Table <ref type="table" coords="8,431.90,453.63,3.77,11.15">3</ref>.</p><p>Table <ref type="table" coords="8,206.95,479.55,3.38,8.10">3</ref>. The features and methods used in the submission runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features run_a1 run_a2 run_a3 run_a4 run_a5</head><p>AutoColorCorrelogram</p><formula xml:id="formula_9" coords="8,132.84,509.67,321.08,169.68">○ ○ ○ ColorLayout ○ ○ ○ ○ ○ FCTH ○ ○ ○ Gabor ○ ○ ○ Gist ○ ○ ○ Visual features ROI ○ ○ ○ Face detection ○ ○ ○ EXIF ○ ○ ○ Textual features Tags ○ Random Forest ○ ○ ○ Classification models MBRM ○ ○ Semi-supervised learning ○ Feature reduction ○ ○ ○ Learning methods</formula><p>Weighting features</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>○</head><p>As Table <ref type="table" coords="9,180.71,148.17,5.03,11.15">3</ref> shows, the used features in run_a1 and run_a2 are the same including visual features and part of the EXIF metadata. Except for the visual and EXIF features, the run_a3 employed tags as a part of textual features. Since the MBRM method is time consuming and the time complexity is dependent on the number of features, the run_a4 and run_a5 only considered the visual feature ColorLayout. The two runs are used to be the baselines while comparing with the other runs having multi-features.</p><p>The results of evaluation are shown in Table <ref type="table" coords="9,329.19,232.17,3.76,11.15" target="#tab_2">4</ref>. The measures are low in MiAP and GMiAP because our methods are only concerned with binary annotation for each image. The confidence scores in the submission runs were produced by the voting ratios of ensemble random decision trees. We use the voting ratio 0.5 to be threshold of annotating images for a concept. However, the voting ratio generally cannot stand for the annotation confidence score of an image. Actually, we think that neither MiAP nor GMiAP measure is appropriate to be the metric in this subtask. Table <ref type="table" coords="10,162.40,160.17,5.03,11.15" target="#tab_2">4</ref> shows that all runs have high precision and low recall. The two baseline methods, MBRM and weighted MBRB, used only one visual feature to classify the concept. The run_4 applying weight scores is not improved much in comparison with the run_5. The results of supervised learning on random trees (run_a1) are better than semi-supervise learning (run_a2) in this task. Further, the results of run_a3 using supervised learning, visual features, and tags are the best. The evaluation results of F1-measure for different concept categories are also shown in Table <ref type="table" coords="10,399.67,232.17,3.76,11.15" target="#tab_3">5</ref>.</p><p>From the results some remarkable characteristics are discussed as follows:</p><p> The information of tags does improve the performance of automatic image annotation no matter what measures are in general.  All our approaches have higher precision rates and lower recall rates. For run_a1, run_a2 and run_a3, the features extracted by the feature reduction method based on discriminant coefficient are high discernible. The lower discernible features are eliminated. These effects should be the main reason of high precision rates and low recall rates for such a kind of method. As run_a4 and run_a5, the selected high threshold of probability P(I j , w) might be the cause of low recall for the MBRM method.  The semi-supervised learning did not outperform supervised learning in this subtask. The reason might be caused by the ranking of voting ratio in random trees. As above mentioned, the voting ratio cannot reflect the confidence score of image annotation. The classified test images did not ranked and added to the training set correctly.  Generally, the concept categories with high annotation rates like quality and quantity have much more positive examples and obvious visual features in the training set and testing set.  The concept categories with very low annotation rates such as combustion and relationship are usually few examples, abstract concept or highly dependent on semantics. It is difficult for image analyzers to find a general model for different kinds of special visual concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The Concept Retrieval Task</head><p>The results of concept retrieval are listed in Table <ref type="table" coords="10,344.72,552.15,3.77,11.15">6</ref>. The three submission runs, run_r1, run_r2 and run_r3, are based on the annotation methods used in run_a1, run_a2 and run_a3, respectively. Since the methods in the concept retrieval subtask were accomplished by the annotation results of subtask 1, the performances are highly dependent upon the effectiveness of annotation results. It is obvious that run_r3 has the best results because of the higher annotation rate in run_a3. The others get low precisions. The results also show that the tags are the important factors of retrieving relevant images. Using visual features only may not retrieve correct concepts from a large amount of general images. The semantics inside images still need appropriate textual notation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This is the first time to participate the photo annotation task for our lab. Owing to many abstract concepts cannot be described by general visual features, the innovating effective visual features for representing various concepts is important to annotate images precisely. In this paper we present the annotation methods based on precise feature reduction and the random decision trees model. All the used visual features and textual features can be extracted from images generally and easily. The best result is the model combining general visual features and tags. The combination of various visual features does not seem to improve the performance much more than only one visual feature. We also found that the different visual features usually worked well in specific concepts. The performance should be able to be improved if the appropriate visual features could be selected and used in the specific concept.</p><p>After submission of the task, some analyses on general visual features and the learning strategies were made. Special features extracting models are necessary for learning classifiers to annotate concepts correctly. For example, the visual concepts, like shadow and refection in the lighting effects category, can be marked or modeled as specific regions or representations. We believe that the representative features, effective feature selection methods and machine learning models will be the solution of annotating specific concepts. However, it should be no direct answer for general visual features to detect concepts effectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,145.14,436.22,286.36,103.06"><head>Table 1 .</head><label>1</label><figDesc>The used visual features.</figDesc><table coords="3,145.14,455.72,286.36,83.56"><row><cell>Visual features</cell><cell>Feature dimensions</cell><cell>#Images</cell><cell>Total</cell></row><row><cell>AutoColorCorrelogram [ 5]</cell><cell>1024</cell><cell>17</cell><cell>17408</cell></row><row><cell>ColorLayout [ 1 1]</cell><cell>120</cell><cell>17</cell><cell>2040</cell></row><row><cell>FCTH [ 2]</cell><cell>192</cell><cell>17</cell><cell>3264</cell></row><row><cell>Gabor [ 11]</cell><cell>60</cell><cell>17</cell><cell>1020</cell></row><row><cell>Gist [ 12]</cell><cell>192</cell><cell>1</cell><cell>192</cell></row><row><cell>ROI background [ 7, 13]</cell><cell>16</cell><cell>1</cell><cell>1024</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,186.42,316.22,206.93,79.44"><head>Table 2 .</head><label>2</label><figDesc>The used textual features.</figDesc><table coords="4,186.42,335.72,206.93,59.94"><row><cell>Textual features</cell><cell>original</cell><cell>After extraction</cell></row><row><cell>Number of faces</cell><cell>1</cell><cell>14</cell></row><row><cell>EXIF</cell><cell>407</cell><cell>24</cell></row><row><cell>Tags</cell><cell>69099</cell><cell>60821</cell></row><row><cell>Total</cell><cell>69507</cell><cell>60859</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,137.22,328.22,312.53,81.00"><head>Table 4 .</head><label>4</label><figDesc>The results of the concept annotation task.</figDesc><table coords="9,137.22,346.04,312.53,63.18"><row><cell>Measures</cell><cell>run_a1</cell><cell>run_a2</cell><cell>run_a3</cell><cell>run_a4</cell><cell>run_a5</cell></row><row><cell>MiAP</cell><cell>0.1022</cell><cell>0.1018</cell><cell>0.1717</cell><cell>0.0947</cell><cell>0.0985</cell></row><row><cell>GMiAP</cell><cell>0.0470</cell><cell>0.0472</cell><cell>0.0984</cell><cell>0.0495</cell><cell>0.0537</cell></row><row><cell>Precision</cell><cell>0.6257</cell><cell>0.5860</cell><cell>0.6313</cell><cell>0.6339</cell><cell>0.6414</cell></row><row><cell>Recall</cell><cell>0.2588</cell><cell>0.2153</cell><cell>0.3384</cell><cell>0.2422</cell><cell>0.2385</cell></row><row><cell>F-ex</cell><cell>0.3662</cell><cell>0.3149</cell><cell>0.4406</cell><cell>0.3505</cell><cell>0.3478</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,133.62,423.80,321.15,258.96"><head>Table 5 .</head><label>5</label><figDesc>The results of F1-measure for different concept categories.</figDesc><table coords="9,133.62,441.93,321.15,240.84"><row><cell>Concept categories</cell><cell>run_a1</cell><cell>run_a2</cell><cell>run_a3</cell><cell>run_a4</cell><cell>run_a5</cell></row><row><cell>time of day</cell><cell>0.2751</cell><cell>0.2828</cell><cell>0.3447</cell><cell>0.1419</cell><cell>0.1065</cell></row><row><cell>celestial bodies</cell><cell>0.0000</cell><cell>0.0000</cell><cell>0.0767</cell><cell>0.0000</cell><cell>0.0000</cell></row><row><cell>weather</cell><cell>0.1043</cell><cell>0.1043</cell><cell>0.1154</cell><cell>0.0000</cell><cell>0.0000</cell></row><row><cell>combustion</cell><cell>0.0000</cell><cell>0.0000</cell><cell>0.0000</cell><cell>0.0000</cell><cell>0.0000</cell></row><row><cell>lighting effects</cell><cell>0.0052</cell><cell>0.0039</cell><cell>0.0423</cell><cell>0.0000</cell><cell>0.0000</cell></row><row><cell>scnery</cell><cell>0.0156</cell><cell>0.0175</cell><cell>0.1376</cell><cell>0.0000</cell><cell>0.0000</cell></row><row><cell>water</cell><cell>0.0029</cell><cell>0.0029</cell><cell>0.0587</cell><cell>0.0000</cell><cell>0.0000</cell></row><row><cell>flora</cell><cell>0.1064</cell><cell>0.1111</cell><cell>0.2656</cell><cell>0.0000</cell><cell>0.0000</cell></row><row><cell>fauna</cell><cell>0.0000</cell><cell>0.0024</cell><cell>0.2557</cell><cell>0.0000</cell><cell>0.0000</cell></row><row><cell>quantity</cell><cell>0.6847</cell><cell>0.5905</cell><cell>0.7362</cell><cell>0.6902</cell><cell>0.6902</cell></row><row><cell>age</cell><cell>0.1239</cell><cell>0.0994</cell><cell>0.4011</cell><cell>0.0425</cell><cell>0.0385</cell></row><row><cell>gender</cell><cell>0.0622</cell><cell>0.0598</cell><cell>0.3423</cell><cell>0.0035</cell><cell>0.0081</cell></row><row><cell>relationship</cell><cell>0.0000</cell><cell>0.0022</cell><cell>0.0574</cell><cell>0.0000</cell><cell>0.0000</cell></row><row><cell>quality</cell><cell>0.6707</cell><cell>0.5959</cell><cell>0.6902</cell><cell>0.6622</cell><cell>0.6619</cell></row><row><cell>style</cell><cell>0.0000</cell><cell>0.0074</cell><cell>0.0618</cell><cell>0.0000</cell><cell>0.0000</cell></row><row><cell>view</cell><cell>0.1829</cell><cell>0.1931</cell><cell>0.3428</cell><cell>0.0964</cell><cell>0.0933</cell></row><row><cell>type</cell><cell>0.0093</cell><cell>0.0043</cell><cell>0.1920</cell><cell>0.0000</cell><cell>0.0000</cell></row><row><cell>impression</cell><cell>0.0144</cell><cell>0.0137</cell><cell>0.1048</cell><cell>0.0003</cell><cell>0.0003</cell></row><row><cell>transportation</cell><cell>0.0027</cell><cell>0.0000</cell><cell>0.1509</cell><cell>0.0000</cell><cell>0.0000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,176.82,148.22,229.25,78.96"><head>Table 4 .</head><label>4</label><figDesc>The results of the concept retrieval task.</figDesc><table coords="11,176.82,167.72,229.25,59.46"><row><cell>Measures</cell><cell>run_r1</cell><cell>run_r2</cell><cell>run_r3</cell></row><row><cell>MnAP</cell><cell>0.0009</cell><cell>0.0007</cell><cell>0.0313</cell></row><row><cell>AP@10</cell><cell>0.0003</cell><cell>0.0006</cell><cell>0.0051</cell></row><row><cell>AP@20</cell><cell>0.0010</cell><cell>0.0014</cell><cell>0.0077</cell></row><row><cell>AP@100</cell><cell>0.0096</cell><cell>0.0081</cell><cell>0.0729</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,136.08,673.28,135.52,9.96"><p>http://www.semanticmetadata.net/lire</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,136.08,684.32,61.72,9.96"><p>http://opencv.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,136.08,684.32,96.44,9.96"><p>http://translate.google.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="6,130.02,684.32,170.45,9.96"><p>http://code.google.com/p/randomforest-matlab/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This research was supported in part by the <rs type="funder">National Science Council of Taiwan</rs>, R. O. C. under contract <rs type="grantNumber">NSC101-2221-E-024-026</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_NvqU8hd">
					<idno type="grant-number">NSC101-2221-E-024-026</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,128.12,583.70,342.49,9.96;11,138.96,594.68,232.38,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,263.91,583.70,120.88,9.96">Overview of the mpeg-7 standard</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Puri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,391.04,583.70,79.57,9.96;11,138.96,594.68,156.22,9.96">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="page" from="688" to="695" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,128.12,605.72,342.50,9.96;11,138.96,616.70,331.60,9.96;11,138.96,627.68,310.69,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,288.46,605.72,182.15,9.96;11,138.96,616.70,153.60,9.96">FECH: Fuzzy color and texture histogram a low level feature for accurate image retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,313.59,616.70,156.97,9.96;11,138.96,627.68,160.53,9.96">the 9th International Workshop on Image Analysis for Multimedia Interactive Services</title>
		<meeting><address><addrLine>Klagenfurt, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="191" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,128.12,638.72,342.52,9.96;11,138.96,649.70,260.61,9.96" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="11,262.60,638.72,166.30,9.96">Using random forest to learn imbalanced data</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<idno>. no. 666</idno>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>Department of Statistics, University of Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="11,128.12,660.68,342.43,9.96;11,138.96,671.72,331.64,9.96;11,138.96,682.70,214.75,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,297.88,660.68,172.67,9.96;11,138.96,671.72,75.80,9.96">Multiple Bernoulli reference models for image and video annotation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,234.65,671.72,235.95,9.96;11,138.96,682.70,42.41,9.96">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1002" to="1009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,128.12,148.22,342.52,9.96;12,138.96,159.20,331.56,9.96;12,138.96,170.18,153.84,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,346.84,148.22,123.80,9.96;12,138.96,159.20,34.47,9.96">Image Indexing Using Color Correlograms</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,193.46,159.20,273.21,9.96">the International Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="762" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,128.12,181.22,342.54,9.96;12,138.96,192.20,314.98,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,247.67,181.22,132.33,9.96">The MIR Flickr retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,400.61,181.22,70.05,9.96;12,138.96,192.20,211.53,9.96">ACM International Conference on Multimedia Information Retrieval (MIR&apos;08)</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,128.12,203.18,342.58,9.96;12,138.96,214.22,331.60,9.96;12,138.96,225.20,65.22,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,251.93,203.18,218.76,9.96;12,138.96,214.22,27.83,9.96">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,174.56,214.22,243.53,9.96">IEEE Transaction on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,128.12,236.18,342.47,9.96;12,138.96,247.22,160.70,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,226.09,236.18,228.48,9.96">An extended set of Haar-like features for rapid object detection</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Maydt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,138.96,247.22,38.79,9.96">IEEE ICIP</title>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="900" to="903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,128.12,258.20,342.40,9.96;12,138.96,269.18,331.66,9.96;12,138.96,280.22,289.22,9.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,230.88,258.20,223.93,9.96">A discriminant based document analysis for text classification</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">C</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,138.96,269.18,331.66,9.96;12,138.96,280.22,109.19,9.96">the International Computer Symposium, Workshop of Artificial Intelligence, Knowledge Discovery, and Fuzzy Systems</title>
		<meeting><address><addrLine>Tainan, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">Dec. 16-18, 2010</date>
			<biblScope unit="page" from="594" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.24,291.20,338.26,9.96;12,138.96,302.18,331.63,9.96;12,138.96,313.22,114.27,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,233.88,291.20,233.31,9.96">Efficient feature reduction for high-precision Text classification</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">C</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,152.46,302.18,318.13,9.96;12,138.96,313.22,22.08,9.96">the National Computer Symposium on Databases, Data Mining, and Information Retrieval</title>
		<meeting><address><addrLine>Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>Chia-Yi</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.24,324.20,338.26,9.96;12,138.96,335.18,331.64,9.96;12,138.96,346.22,72.03,9.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,253.72,324.20,216.78,9.96;12,138.96,335.18,13.82,9.96">Texture features for browsing and retrieval of large image data</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,158.59,335.18,233.50,9.96">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="837" to="842" />
			<date type="published" when="1996-08">August. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.24,357.20,338.38,9.96;12,138.96,368.18,331.68,9.96;12,138.96,379.22,108.12,9.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,215.09,357.20,255.53,9.96;12,138.96,368.18,72.67,9.96">Rapid biologically-inspired scene classification using features shared with visual attention</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Siagian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,217.38,368.18,233.70,9.96">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="300" to="312" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.24,390.20,338.42,9.96;12,138.96,401.18,237.09,9.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,236.24,390.20,234.42,9.96;12,138.96,401.18,51.08,9.96">Overview of the ImageCLEF 2012 Flickr Photo Annotation and Retrieval Task</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,208.50,401.18,94.44,9.96">CLEF 2012 working notes</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.24,412.22,338.32,9.96;12,138.96,423.20,324.29,9.96" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,217.92,412.22,252.64,9.96;12,138.96,423.20,14.61,9.96">Visual attention detection in video sequences using spatiotemporal cues</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,171.75,423.20,213.15,9.96">14th Annual ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="815" to="824" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
