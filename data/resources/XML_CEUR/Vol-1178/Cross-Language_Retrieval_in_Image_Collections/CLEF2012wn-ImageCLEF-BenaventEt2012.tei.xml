<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.16,152.65,314.62,12.64;1,153.01,170.65,289.20,12.64;1,152.29,188.65,290.57,12.64">Visual Concept Features and Textual Expansion in a Multimodal System for Concept Annotation and Retrieval with Flickr Photos at ImageCLEF2012</title>
				<funder ref="#_A35t7z8 #_pc8Tg8s">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_re4Ez86">
					<orgName type="full">Spanish Government</orgName>
				</funder>
				<funder ref="#_yhgqrWn">
					<orgName type="full">Regional Government of Madrid under Research Network MA2VIRMR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,141.84,228.16,46.58,8.96"><forename type="first">J</forename><surname>Benavent</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universitat de Val√®ncia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,198.00,228.16,58.13,8.96"><forename type="first">A</forename><surname>Castellanos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universidad Nacional de Educaci√≥n a Distancia</orgName>
								<orgName type="institution" key="instit2">UNED</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,264.12,228.16,49.82,8.96"><forename type="first">X</forename><surname>Benavent</surname></persName>
							<email>xaro.benavent@uv.es</email>
							<affiliation key="aff1">
								<orgName type="institution">Universitat de Val√®ncia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,321.96,228.16,40.60,8.96"><forename type="first">E</forename><surname>De Ves</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universitat de Val√®ncia</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,370.44,228.16,79.86,8.96"><forename type="first">Ana</forename><surname>Garc√≠a-Serrano</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universidad Nacional de Educaci√≥n a Distancia</orgName>
								<orgName type="institution" key="instit2">UNED</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.16,152.65,314.62,12.64;1,153.01,170.65,289.20,12.64;1,152.29,188.65,290.57,12.64">Visual Concept Features and Textual Expansion in a Multimodal System for Concept Annotation and Retrieval with Flickr Photos at ImageCLEF2012</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CDFDC08DC3996DD3AEDF2A82FA50689D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multimedia Retrieval</term>
					<term>Flickr Expansion</term>
					<term>Concept Features</term>
					<term>Lowlevel features</term>
					<term>Logistic regression relevance feedback</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents our submitted experiments in the Concept annotation and Concept Retrieval tasks using Flickr photos at ImageCLEF 2012. This edition we applied new strategies for both the textual and the visual subsystems included in our multimodal retrieval system. The visual subsystem has focus on extending the low-level features vector with concept features. These concept features have been calculated by means of a logistic regression model. The textual subsystem has focus on expanding the query information using external resources. Our best concept retrieval run, a multimodal one, is at the ninth position with a MnAP of 0.0295, being the second best group of the contest for the multimodal modality. This is also our best run in the global ordered list (where eleven textual runs are also better than it). We have adapted our multimodal retrieval process for the annotation task obtaining non-very good results for this first participation, with a MiAP of 0.1020.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The UNED-UV is a research group with researchers from two universities in Spain, the Universidad Nacional de Educaci√≥n a Distancia (UNED) and the Valencia University (UV). The group is working together since ImageCLEF08 edition. Notice that this is our first participation in the Photo Annotation and Retrieval Task using Flickr photos, being our previous participations at the Wikipedia retrieval <ref type="bibr" coords="1,411.85,593.20,11.71,8.96" target="#b5">[6]</ref> and at the Medical <ref type="bibr" coords="1,159.97,605.21,11.71,8.96" target="#b3">[4]</ref> tasks.</p><p>The visual concept detection, annotation, and retrieval task is a multi-label classification challenge. The participants are asked to annotate the presence of one or more concepts at the annotation subtask using visual and/or textual features, and use this information in the retrieval process <ref type="bibr" coords="1,269.39,653.21,10.68,8.96" target="#b1">[2]</ref>. We have participated in the annotation and in the retrieval subtask using visual and textual information. Our multimedia retrieval system very similar to the ones already used at previous ImageCLEF editions <ref type="bibr" coords="2,215.03,162.16,11.57,8.96" target="#b3">[4,</ref><ref type="bibr" coords="2,226.60,162.16,7.71,8.96" target="#b4">5]</ref> is composed of three subsystems (Fig. <ref type="figure" coords="2,407.15,162.16,3.73,8.96" target="#fig_1">1</ref>): the Textual Based Information Retrieval (TBIR) system, the Content Based Information Retrieval System (CBIR), and the Fusion subsystem. The main three steps are the following: TBIR subsystem acts first as a pre-filter, and then the CBIR system works over this pre-filtered collection by re-ranking it. The final ranked list is the fusion of the both mono-modal lists. This retrieval process is based on the idea that textual retrieval subsystem better captures the meaning of the query. So it is expected that the textual subsystem eliminates images that are similar from a visual point of view but completely different from a semantic point of view.</p><p>At this edition, the TBIR system has been improved by expanding the textual information of the query to improve the retrieval. Most of the participating groups at the previous retrieval task try to take advantage of Flickr tag annotation of the images for the retrieval process. In this regard, Ksibi et al <ref type="bibr" coords="2,313.19,306.18,11.71,8.96" target="#b7">[8]</ref> use Flickr tags to extract contextual relationships between them. Izawa et al <ref type="bibr" coords="2,288.10,318.18,11.72,8.96" target="#b6">[7]</ref> also use Flickr tags. They combine a TF-IDF model over the tags with a visual word co-occurrence approximation. Another approach investigated for Spyromitros-Xious et al. <ref type="bibr" coords="2,334.16,342.19,16.77,8.96" target="#b10">[11]</ref> use the concepts, instead the tags, in order to improve the textual-based retrieval. Unlike the papers presented before, we decided to go beyond in the use of textual information about the images (including tags). For that we have carried out an expansion of the original collection, using the information of the images existing on Flickr.</p><p>The CBIR system uses low-level features for image retrieval. This low-level information although gives quite good results depending on the visual information of the query, it is not able to reduce the "semantic gap" in a semantic complex query. Our proposal <ref type="bibr" coords="2,180.19,438.20,11.71,8.96" target="#b2">[3]</ref> is to generate Concept features extracted from the low-level features to obtain the probability of the presence of each trained concept. We call this new vector, the expanded low-level Concept vector that is calculated for each image of the collection and also for the example images of the query to process the retrieval task. A model for each concept is trained using a logistic regression <ref type="bibr" coords="2,384.54,486.21,10.70,8.96" target="#b8">[9]</ref>. We use these regression models as multi-label classifiers at the annotation subtask and as a features vector for the retrieval subtask.</p><p>Our proposals both for the textual as for the visual systems are more oriented to a retrieval process than to an annotation subtask. Anyway, we have adapted them for the multi-label annotation subtask. Section 2 describes the visual, textual and multimodal approaches for the concept annotation subtask with Flickr photos. Section 3 explains our multimodal retrieval system use for the concept retrieval subtask. After that section 4 shows the submitted runs and the results obtained for annotation and for retrieval. Finally, in section 5 we extract conclusions and outlines possible future research lines.</p><p>Concept annotation subtask with Flickr photos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.1</head><p>Annotation approach using visual information.</p><p>For the annotation subtask we train a logistic regression model <ref type="bibr" coords="3,392.52,203.20,11.71,8.96" target="#b8">[9]</ref> for each of the concepts defined by the concept annotation subtask <ref type="bibr" coords="3,338.50,215.20,10.68,8.96" target="#b1">[2]</ref>. Each trained model predicts the probability that a given image belongs to a certain concept. The concept annotation subtask gives to the participants a training set, ùêº ùë† , for each of the concepts. Being ùêº ùë† ùëÉ the training image set for each concept, we refer to them as the relevant or positive images. And, being ùêº ùë† ùëÅ the set of no relevant images for a given concept referred as non-relevant or negative images. The logistic regression analysis calculates the probability for a given image to belong to a certain concept. Each image of the training set, ùêº ùë† is represented by a K-dimensional low-level features vector{ùë• 1 , . . , ùë• ùëñ , . . , ùë• ùëò }. The relevance probability for a certain concept ùëê ùëñ for a given image ùêº ùëó will be represented as ùëÉ ùëê ùëñ ÔøΩùêº ùëó ÔøΩ. A logistic regression model can estimate these probabilities. Let us consider for a binary Y, and k explanatory variablesùë• = (ùë• 1, ‚Ä¶ , ùë• ùëò ), the model for œÄ(x) = P(Y=1| X) (probability ùëå = 1) for the x values ùëôùëúùëîùëñùë° [ùúã(ùë•)] = ùõº + ùõΩ 1 ùë• 1 + ‚ãØ + ùõΩ ùëò ùë• ùëò , where logit (œÄ(x))=ln(œÄ(x) / (1-œÄ(x)). The model parameters are obtained by maximizing the likelihood estimator (MLE) of the parameter vector Œ≤ by using an iterative method.</p><p>We have a major difficulty when having to adjust an overall regression model in which we take the whole set of variables into account because the number of selected images (the number of positive plus negative images, k) is typically smaller than the number of characteristics (k &lt; p). In this case the adjusted regression model has as many parameters as the amount of data and many relevant variables could be not considered. In order to solve this problem our proposal is to adjust different smaller regression models: each model considers only a subset of variables consisting of semantically related characteristics of the image. Consequently each sub-model will associate a different relevance probability to a given image x and we have to combine them in order to rank the database according to the image probability or image score (Si).</p><p>The explanatory variables ùë• = (ùë• 1, ‚Ä¶ , ùë• ùëò ) to train the model are the visual lowlevel features based on color and texture information that are calculated by our group. We have a low-level features vector of 293 components divided by five different visual information families.</p><p>‚Ä¢ Color information: Color information has been extracted by calculating both local and overall histograms of the images. Overall histograms have been calculated using 10x3 bins on the HS color system. Meanwhile, local histograms have been calculated by dividing the images into four fragments of the same size. A bidimensional HS histogram with 12x4 bins is computed for each patch. Therefore, a feature vector of 222 components represents the color information of the image. ‚Ä¢ Texture information: This information is embodied as the granulometric distribution function. A granulometry is defined from the morphological opening of the texture using a convex and compact subset containing the origin as structuring el-ement <ref type="bibr" coords="4,164.53,150.16,10.68,8.96" target="#b0">[1]</ref>. In our case we have used a horizontal and a vertical segment as the structuring elements, being 60 components in total for both structuring elements. And the Spatial Size Distribution that is another morphological operation defined in <ref type="bibr" coords="4,146.27,186.04,11.71,8.96" target="#b0">[1]</ref> using a horizontal segment as structuring element, being 10 components.</p><p>Once we have the 99 trained models, we calculate for each image the probability of belonging to a given concept, P c i ÔøΩI j ÔøΩ. This probability is a floating-point value between 0 and 1 that is the confidence score for the annotation run. For calculating the binary score, if the concept probability is greater than 0.5 (P c i ÔøΩI j ÔøΩ &gt; 0.5) is assumed that the concept is present at the image and then it is marked as 1, otherwise is marked as 0 meaning the absence of the concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2</head><p>Annotation approach using visual and textual information.</p><p>Based on visual annotation, presented above, we propose a multimodal annotation by an IR-based approach. Our proposal uses a two-step process. In the first step, the visual annotation approach generates a visual-based results list. Then, in the second step, the textual system refines this visual annotated list as follows:</p><p>‚Ä¢ The textual system only checks the annotated concept as present in an image according to the visual system (set to 1 at the binary annotation). ‚Ä¢ The textual system retrieves the concepts, which are most likely in the image, ranked by score, using the textual information of the image as a query against the information associated to the concepts. ‚Ä¢ If the textual system identifies the concept as present, the concept is fixed as present and the confidence score is calculated as the product of both textual and visual confidence scores. ‚Ä¢ But, if the textual system does not identify the concept as present, the concept is fixed as not present, regardless of the criteria of the visual annotation.</p><p>This proposal entailed a problem, there was not enough information associated with both the images and the concepts. Due to this lack of information, it was decided to expand textual information of the collection by external sources. The expansion was posed both for images information and concepts information.</p><p>In order to expand the information associated to the images, Flickr was used to provide an adequate textual description for each image. Two different expansion processes are posed:</p><p>‚Ä¢ Expansion using Flickr Description of Image: To all of the images on the collection, we have retrieved the Flickr Description and we have aggregated it to the image description for all the images on the collection. ‚Ä¢ Expansion using Flickr Description of Similar Images: It was decided to complement user descriptions with the descriptions of other users on similar images. In order to find images that are similar to each image of the collection, we use the tag annotation of the images. For each image, the Flickr API was queried to retrieve images that share the same tags (all of them or a subset) and aggregate to the image description, the descriptions of the 50 first images retrieved.</p><p>We propose three methods for the expansion of the concepts based on two external sources (Flickr and ImageNET1):</p><p>‚Ä¢ Expansion using user descriptions of the concept on Flickr: The name of the concept is used as the query for the Flickr API and gets a set of relevant images.</p><p>Then, the descriptions of these images are aggregated to the concept description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>‚Ä¢ Expansion using user descriptions on Flickr of images annotated with the same concept:</head><p>The idea is similar to the expansion presented previously; but instead of querying for images relevant to each concept, we used the images annotated with the given concept. The method is as follows: 1) for each concept the images annotated with it are identified, 2) the descriptions of these images are taken and finally 3) the image description is aggregated to the concept description. ‚Ä¢ Expansion using structured information (ImageNET): For this approximation, each concept was manually extended by searching them on ImageNET and adding the definition provided by ImageNET to the concept definition.   The TBIR subsystem is responsible of the preprocessing, the expansion, the indexing and, finally, the retrieval process using textual information. The TBIR subsystem recovers only relevant images with a given query and assigns to each image a score (St), based on textual similarity between associated text and query. These relevant images returned by TBIR module are submitted as candidates to the CBIR system as a list, sorted by the score. The TBIR subsystem acts over all images on the collection as a filter. After, the CBIR subsystem assigns another score, Si, to each image based on its work with visual features. In the last step the image list is re-ranked, fusing the scores given by TBIR and CBIR modules by the product of both scores, St*Si.</p><p>Each subsystem is described in detail in the two following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Text Based Information Retrieval Subsystem</head><p>This subsystem carries all the work related with the textual information of the collection (preprocess, query reformulation, collection expansion, indexing, and finally the retrieval). The operation of all these stages is presented below:</p><p>‚Ä¢ Preprocess: Since the indexing and retrieval process are based on terms frequencies, it is important to perform a previous work in favor of normalize and remove noise terms. The preprocessed includes: 1) the special characters, with no statistical meaning, are eliminated; 2) deletion of semantically empty words (i.e. stop-words) in English language, 3) stemming: reduction of word to their base form, using Porter Algorithm and, finally, 4) convert all words to lower case. ‚Ä¢ Query Processed: The query is processed in two senses. First, meaningless terms or expressions are deleted; more concretely, expressions like The user is looking for photos showing‚Ä¶ are removed, as it doesn't add any semantic information to the query content. On the other hand, for each query, the concept (or concepts) expected for the results of a given query is identified. This identification is manually done. An example of processing of a query is: ‚îÄ Original query:</p><p>The user is looking for photos showing only one or more elderly men, so no other people should be additionally visible ‚îÄ Query without meaningless terms:</p><p>one or more elderly men, so no other people should be additionally visible. ‚îÄ Concept/s identified:</p><p>Elder Male ‚Ä¢ Collection Expansion: Textual information associated to the images available in the collection is scarce (both for images and concepts). Due to that our approach requires a significant amount of textual information to work; it became necessary raise an expansion process, using the information available for each image to query external sources. The expansion information, created according the process explained in section 2.2, is aggregated to the collection. ‚Ä¢ Indexing: For indexing the collection has been used Apache Solr 2 . Solr is an opensource search platform from Apache Lucene<ref type="foot" coords="6,316.68,656.89,3.24,5.83" target="#foot_1">3</ref> project. Through Solr it has been in-dexed the textual information that the collection had, as well as, the descriptions, generated in the expansion of the collection. ‚Ä¢ Retrieval: The search process is done by Solr, over Lucene operation. The score function used for calculate the similarity between a given query and the documents is BM25. The results are TREC-format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Content Based Information Retrieval Subsystem</head><p>The work of the CBIR subsystem is based on three main stages: Extraction of the low-level, calculating the Concept features of the images to expand the features vector, and the calculation of the similarity (Si) of each of the images to the image examples given by a query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Extraction of low-level features:</head><p>The first step in the CBIR system is to extract the visual low-level and the Concept features for all the images of the database as well as from the example images given in each question. The low-level features we use are calculated by our group and give color and texture information about the images. These features are the same that we have used for the modality classification task (see section 2.1 for more detailed information). 2. Calculating the Concept features vector. The regression models trained for each of the concepts gives for each image on the database and for the example query the probability of the presence of each conceptP c i ÔøΩI j ÔøΩ. With this probability information for each concept, we extend the low-level features vector to m components, being m the number of concepts trained. Each image I j on the database is described by the extended vectorFÔøΩI j ÔøΩ = (x 1 , ‚Ä¶ x k , c 1 , . . , c m } ‚àà R k+m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Similarity Module:</head><p>The similarity module instead of using the classical distance method to calculate the similarity of each of the images of the database to the example images for a given topic uses our own logistic regression relevance algorithm to get the probability of an image belonging to the query set. The sub-models regressions are set to five features inside each features family that are the number of example images given for each topic (see more details of the regression method at section 2.1.). The relevant images are the example images, and the non-relevant images are randomly taken from outside the pre-textual filtered list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fusion subsystem</head><p>The fusion subsystem is in charge of merging the two score result lists from the TBIR and the CBIR subsystem. In the present work we use the product fusion algorithm (Si*St). The two results lists are fused together to combine the relevance scores of both textual and visually retrieved images (St and Si). Both subsystems will have the same importance for the resulting list: the final relevance of the images will be calculated using the product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Concept annotation</head><p>In this our first participation on the concept annotation subtask, we have participated with three visual and two multimodal runs (see table <ref type="table" coords="8,340.18,215.20,4.98,8.96" target="#tab_0">1</ref> for detailed information of the submitted runs).</p><p>Our main objective for the visual runs is to test the behavior of our logistic regression model as a classifier for the annotation task, and to adjust the parameters of the regression model. As explained in section 2.1., one of the important parameter is the set of relevant images. The number of images per concept range significantly from 30 to 200 images <ref type="bibr" coords="8,194.83,287.21,10.68,8.96" target="#b1">[2]</ref>. We have manually selected the relevant images for runs UNED_UV_02 and UNED_UV_03, and for run UNED_UV_01 all given images are taken up to 100 images. The number of positive plus negative images, k has to be greater than the number of regression parameters to be estimated (see section 2.2). We have fixed for all submitted runs the same number of regression models: a regression for each low-level feature sub-family, being eight regression models varying between 9, 30 or 48 low-level components. It means that we would need between 30 and 50 relevant images. We have fixed the number of relevant images for run UNED_UV_02 and UNED_UV_03 to 30 images.</p><p>The other input that the regression model needs is the set of non-relevant images. The number of non-relevant images should be the double of the number of relevant images. The other fact is how to choose the non-relevant images for each concept. At this edition of the Photo annotation Flickr subtask, the concepts have been categorized in family groups <ref type="bibr" coords="8,194.69,443.23,10.68,8.96" target="#b1">[2]</ref>. We have used this information to select the number of the nonrelevant images. Therefore, at runs UNED_UV_01 and UNED_UV_02 the nonrelevant images are selected from a subset of the images outside the family. Meanwhile, at run 3 are selected from subset of images from the same family that not belong to the training concept. The two multimodal runs, UNED_UV_04 and UNED_UV_05 use a different visual run baseline, (run UNED_UV_02 and UNED_UV_03 respectively), and then the textual algorithm described at section 2.2 acts as a filter for the visual run. The two expansion approaches used are the one performed using the Flickr Description of Similar Images for the image descriptions and the second expansion using the user descriptions on Flickr of images annotated with the same concept for the concept descriptions. This two expansion approaches are those that provide more information.</p><p>Table <ref type="table" coords="9,162.23,234.17,4.98,8.96" target="#tab_1">2</ref> shows our submitted runs results measured by means of The Interpolated Mean Average Precision (MiAP), Geometric Interpolated Mean Average Precision (GMiAP) and the photo based micro-F1 measure (F-ex). Our best result by MiAP, run UNED_UV_01, is at position 55 from the global result list (80 runs).</p><p>In the configurations tested for the visual runs, our results ordered by MiAP from best to worst are run UNED_UV_01, UNED_UV_02 and UNED_UV_03 respectively. It can signify that as more relevant images we have better is the regression model performance. Both runs UNED_UV_01 and UNED_UV_02 outperform run UNED_UV_03 meaning that it is better to select the non-relevant images outside the categorized group.</p><p>Concerning these multimodal results, it is clear that the combination of visual and textual annotation proposed does not provide the expected performance. Both multimodal runs (UNED_UV_04 and UNED_UV_05) do not outperform the visual baselines for any of the evaluation measures (MiAP, GMiAP and F-ex).Anyway we think that these not very good results are because the inaccuracy of the information associated with the concepts, that is obtained in the expansion process. Need also to be studied if the filter effect of the textual information over the visual one is too restrictive or not.</p><p>It is needed to point out that all the results ordered by the F-ex values are on the opposite way than ordered by MiAP. This fact would have to be analyzed in detail query by query. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Concept Retrieval using Flickr photos</head><p>We have submitted two textual and eight multimodal runs. Table <ref type="table" coords="9,388.79,634.96,4.98,8.96" target="#tab_2">3</ref> shows the detailed information for the submitted runs. For the textual baseline, run UNED_UV_01, the content of the topic/query is previously preprocessed and is used to query over the image description in Flickr and also against the description obtained by the expansion using user descriptions on Flickr of images annotated with the same concept (see section 2.2.). For the UNED_UV_02 run the query process is similar to the previous one, but in addition to use the content of the topic to query over the descriptions, the concept expected for the results of a given query is also used. Given that the concept expected for the queries is not provided, we have identified it in a manually way for each query. For the concepts no expansion information has been used. The multimodal runs (runs 3 to 10) have been designed to test the behavior of the expanded features vector. The expanded features vector is obtained as explained at section 2.1. by the regressions models trained at the annotation subtask. We have used two of the four regressions models used at the concept annotation subtask: the experiments two and three from table 2 denoted at table 4 as base2 and base3 respectively. The extended vector F(I i ) = (x 1 , ‚Ä¶ x k , c 1 , . . , c m } ‚àà R k+m can be calculated as a unique vector with the low-level and the Concept features (denoted as [LF‚Ä¶CF] at table 4), and as two different vectors (denoted as [LF]*[CF]). For the last scheme, two different probabilities are obtained by the low-level features ùëÜ ùë• (ùêº ùëñ ), and for the Concept features ùëÜ ùëê (ùêº ùëñ ), combining both probabilities by the product ùëÜ(ùêº ùëñ ) = ùëÜ ùë• (ùêº ùëñ ) * ùëÜ ùëê (ùêº ùëñ ). All multimodal runs use the textual pre-filter algorithm, so the visual system only works over this pre-filtered sub-collection. We have presented four multimodal runs with textual baseline (UNED_UV_01) and the other four with the concept extended textual run (UNED_UV_02). The multimodal runs merged both image and textual scores by the product (St*Si).</p><p>The evaluation is done according to the following measures: The overall noninterpolated MAP (MnAP), average of the non-interpolated precisions for each concept and the Average Precision at different values AP@10, AP@20 and AP@100. Table <ref type="table" coords="10,149.98,654.77,4.98,8.96" target="#tab_3">4</ref> shows our submitted run results.</p><p>Our best result, the multimodal run UNED_UV_10 (MnAP of 0.0295) is at the 20th position of the overall result list and at the ninth position for the multimodal runs, being our group, the UNED_UV, the third group for the multimodal runs, and the fourth best group in the overall results for the concept retrieval results subtask.</p><p>Looking at textual runs, our best result is obtained with UNED_UV_02_TXT_AUTO_EN (MnAP of 0.0250), which uses the information about the concept for query expansion. This run improves the results of baseline run without concept information (UNED_UV_01_TXT_AUTO_EN with MnAP of 0.0208).</p><p>Our two best multimodal runs, UNED_UV_6 (MnAP of 0.0286) and UNED_UV_10 (MnAP of 0.0295) outperforms its corresponding pre-filtered textual baseline (run UNED_UV_2). This shows that the use of the expanded concept features vector as an unique vector or as two different vectors do not make any important difference given that run UNED_UV_6 uses only one vector and UNED_UV_09 run uses two different vectors and both obtain a very similar MnAP values. A similar behavior can be also observed for the annotation base regression model used to get the expanded concept features vector so that UNED_UV_6 uses base2 and UNED_UV_09 uses base3, and the MnAP values are similar for both runs. This is also observed for the concept annotation results obtained, in which models 2 and 3 have similar results by MiAP (see Table <ref type="table" coords="11,288.32,354.19,3.63,8.96" target="#tab_1">2</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Concluding Remarks and Future Work</head><p>Our best result is obtained at the concept retrieval subtask in in the multimodal modality. This multimodal run is the UNED_UV_10 (MnAP of 0.0295), at the 20th position of the overall result list and at the ninth position at the multimodal runs list. For the different textual approaches presented, we can conclude that the expansion using information about the concepts outperform the standard retrieval process; even by a simple expansion approach as the presented here. In this regard, we will continue exploring this research line with some remarks. First, a better definition of each con-cept is desirable in order to improve a better representation of the concept and then a better retrieval process. In this work, we use a simple TF-IDF-based, but more sophisticated approach could be proposed as divergence-based techniques. Second, to address the lack of detailed descriptions of the concepts, we have presented an expansion based on external sources. Although the results shows that these technique improve the baseline results, it has been shown that these expansion introduces a significant amount of noise information. This noise information gets low precision values at the first results.</p><p>For the multimodal approaches presented for the concept retrieval subtask, our combination of the textual pre-filtered list as input to the visual system outperform the textual baseline, as it has already been tested in other ImageClef collections, Wikipedia <ref type="bibr" coords="12,139.89,282.18,11.71,8.96" target="#b5">[6]</ref> and Medical <ref type="bibr" coords="12,207.81,282.18,10.60,8.96" target="#b3">[4]</ref>. Focusing on the visual system, the expanded Concept vector outperforms the use of the low-level features vector in Flickr photo collection and in the Medical collection <ref type="bibr" coords="12,218.37,306.18,10.69,8.96" target="#b4">[5]</ref>. Therefore, we will continue working in adjusting the best configuration for the regression models so that no definitive conclusions for the best configuration can be extracted for the present work.</p><p>The results obtained at the Concept Annotation subtask have not been as good as the ones obtained at the retrieval subtask. In this first participation our best result is at 56 th position from 80 runs. This is due to the fact that both our textual and our visual approaches are retrieval approaches adapted for a classification task. Nevertheless, the regression model system proposed as a multilabel classifier for the annotation concept subtask will deeply be studied. The multimodal approaches do not outperform the visual baseline so that they will also be redefined. We think the textual filter has been too strict, and a relax combination of both confidence scores, textual and visual will get better multimodal annotation results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,124.68,378.20,6.00,10.80;5,153.00,378.20,231.96,10.80;5,124.68,403.84,345.87,8.96;5,124.68,415.84,345.82,8.96;5,124.68,427.84,106.40,8.96"><head>3</head><label></label><figDesc>Concept retrieval subtask with Flickr photos.The system is composed by three subsystems: the Textual Based Image Retrieval (TBIR) Subsystem and the Content Based Image Retrieval (CBIR) Subsystem and the Fusion subsystem (Fig.1.).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,234.72,653.98,125.60,8.10"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Retrieval System overview.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,124.68,684.22,3.00,5.40;5,129.96,686.13,95.86,8.10;5,417.97,508.08,8.69,5.64;5,412.99,515.21,18.49,5.64;5,417.54,522.35,9.47,5.64;5,407.67,615.88,22.02,5.64;5,409.41,623.02,18.49,5.64;5,407.46,574.02,27.36,6.52;5,414.40,557.38,13.68,5.64;5,369.40,590.36,12.24,5.64;5,360.73,597.61,29.64,5.64;5,222.71,590.02,18.15,6.52;5,302.50,599.17,44.09,4.87;5,302.50,605.33,43.26,4.87;5,302.50,611.60,43.37,4.87;5,357.47,509.91,14.42,5.64;5,217.28,488.81,17.83,6.52;5,354.98,546.35,17.24,5.64;5,162.96,515.00,19.31,5.64;5,153.86,522.24,37.47,5.64;5,160.15,493.05,23.37,5.64;5,157.65,611.88,26.34,5.64;5,161.66,619.12,18.17,5.64;5,160.58,564.18,20.61,6.52;5,166.76,549.54,9.86,4.87;5,163.83,581.44,15.55,4.87;5,262.61,469.91,15.00,5.64;5,296.76,469.91,28.02,5.64;5,277.89,510.02,26.68,5.64;5,282.55,538.57,15.83,5.64;5,271.93,546.14,36.86,5.64;5,229.32,538.78,10.44,5.64;5,225.09,546.14,18.93,5.64;5,256.53,599.60,21.58,4.87;5,256.53,605.77,17.99,4.87;5,256.53,612.25,14.82,4.87;5,219.56,601.98,16.29,4.87;5,216.42,608.25,22.55,4.87"><head></head><label></label><figDesc>1 http://www.image-net.org/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,127.44,515.01,335.61,155.50"><head>Table 1 .</head><label>1</label><figDesc>Detailed information of the submitted experiments for the concept annotation task.</figDesc><table coords="8,127.44,542.33,334.48,128.18"><row><cell></cell><cell></cell><cell></cell><cell cols="2">Visual information</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Textual information</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Relevant images</cell><cell>Non-relevant images</cell><cell></cell><cell></cell></row><row><cell>Run</cell><cell>Modality</cell><cell>#num-ber</cell><cell>Selection method</cell><cell>Selection method</cell><cell>Visual baseline</cell><cell>Textual algorithm</cell></row><row><cell cols="2">UNED_UV_01 Visual</cell><cell>All up</cell><cell>If &gt;100 the nearest to</cell><cell>Outside the family</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>to 100</cell><cell>the centroid</cell><cell>concept.</cell><cell></cell><cell></cell></row><row><cell cols="2">UNED_UV_02 Visual</cell><cell>30</cell><cell>Manually selected</cell><cell>Outside the family</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>concept</cell><cell></cell><cell></cell></row><row><cell cols="2">UNED_UV_03 Visual</cell><cell>30</cell><cell>Manually selected</cell><cell>Inside the family</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>concept</cell><cell></cell><cell></cell></row><row><cell cols="2">UNED_UV_04 Multimodal</cell><cell></cell><cell></cell><cell></cell><cell>Run2</cell><cell>Textual filter</cell></row><row><cell cols="2">UNED_UV_05 Multimodal</cell><cell></cell><cell></cell><cell></cell><cell>Run3</cell><cell>Textual filter</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,124.68,497.97,345.13,94.18"><head>Table 2 .</head><label>2</label><figDesc>Results for the submitted concept annotation experiments.</figDesc><table coords="9,124.68,516.77,12.87,6.26"><row><cell>Run</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,138.12,221.68,318.70,196.71"><head>Table 3 .</head><label>3</label><figDesc>-Detailed information of the submitted concept retrieval experiments.</figDesc><table coords="10,316.92,242.32,93.20,6.53"><row><cell>TBIR</cell><cell>CBIR</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,144.24,377.97,313.92,180.69"><head>Table 4 .</head><label>4</label><figDesc>Results for the submitted concept retrieval experiments.</figDesc><table coords="11,144.24,396.77,313.92,161.89"><row><cell>Run</cell><cell>Mode</cell><cell>MnAP</cell><cell>AP@10</cell><cell>AP@20</cell><cell>AP@100</cell></row><row><cell>UNED_UV_01_TXT_AUTO_EN</cell><cell>Textual</cell><cell>0.0208</cell><cell>0.0032</cell><cell>0.0021</cell><cell>0.0653</cell></row><row><cell>UNED_UV_02_TXT_AUTO_EN</cell><cell>Textual</cell><cell>0.0250</cell><cell>0.0004</cell><cell>0.0019</cell><cell>0.0250</cell></row><row><cell>Best textual (IMU)</cell><cell></cell><cell>0.0933</cell><cell>0.0187</cell><cell>0.0338</cell><cell>0.1715</cell></row><row><cell>UNED_UV_03_TXTIMG</cell><cell>Multimodal</cell><cell>0.0271</cell><cell>0.0125</cell><cell>0.0203</cell><cell>0.0813</cell></row><row><cell>UNED_UV_04_ TXTIMG</cell><cell>Multimodal</cell><cell>0.0271</cell><cell>0.0131</cell><cell>0.0199</cell><cell>0.0837</cell></row><row><cell>UNED_UV_05_ TXTIMG</cell><cell>Multimodal</cell><cell>0.0260</cell><cell>0.0121</cell><cell>0.0224</cell><cell>0.0807</cell></row><row><cell>UNED_UV_06_ TXTIMG</cell><cell>Multimodal</cell><cell>0.0286</cell><cell>0.0116</cell><cell>0.0223</cell><cell>0.0819</cell></row><row><cell>UNED_UV_07_ TXTIMG</cell><cell>Multimodal</cell><cell>0.0275</cell><cell>0.0112</cell><cell>0.0203</cell><cell>0.0859</cell></row><row><cell>UNED_UV_08_ TXTIMG</cell><cell>Multimodal</cell><cell>0.0275</cell><cell>0.0122</cell><cell>0.0198</cell><cell>0.0854</cell></row><row><cell>UNED_UV_09_ TXTIMG</cell><cell>Multimodal</cell><cell>0.0270</cell><cell>0.0104</cell><cell>0.0217</cell><cell>0.0822</cell></row><row><cell>UNED_UV_10 _TXTIMG</cell><cell>Multimodal</cell><cell>0.0295</cell><cell>0.0125</cell><cell>0.0206</cell><cell>0.0848</cell></row><row><cell>Best Multimodal (MLKD)</cell><cell></cell><cell>0.0702</cell><cell>0.0214</cell><cell>0.0342</cell><cell>0.1495</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="6,129.96,686.13,105.12,8.10"><p>http://lucene.apache.org/solr/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="7,129.96,686.13,107.04,8.10"><p>http://lucene.apache.org/core/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work has been partially supported for <rs type="funder">Regional Government of Madrid under Research Network MA2VIRMR</rs> (<rs type="grantNumber">S2009/TIC-1542</rs>), for <rs type="funder">Spanish Government</rs> by projects <rs type="projectName">BUSCAMEDIA</rs> (<rs type="grantNumber">CEN-20091026</rs>), <rs type="projectName">HOLOPEDIA</rs> (<rs type="grantNumber">TIN 2010-21128-C02</rs>) and <rs type="grantNumber">MCYT TEC2009-12980</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_yhgqrWn">
					<idno type="grant-number">S2009/TIC-1542</idno>
				</org>
				<org type="funded-project" xml:id="_re4Ez86">
					<idno type="grant-number">CEN-20091026</idno>
					<orgName type="project" subtype="full">BUSCAMEDIA</orgName>
				</org>
				<org type="funded-project" xml:id="_A35t7z8">
					<idno type="grant-number">TIN 2010-21128-C02</idno>
					<orgName type="project" subtype="full">HOLOPEDIA</orgName>
				</org>
				<org type="funding" xml:id="_pc8Tg8s">
					<idno type="grant-number">MCYT TEC2009-12980</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,132.65,560.97,338.10,8.10;12,141.72,572.02,328.93,8.10;12,141.72,583.06,88.53,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,235.57,560.97,235.18,8.10;12,141.72,572.02,29.95,8.10">Spatial Size Distributions. Applications to Shape and Texture Analysis</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ayala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Domingo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,177.97,572.02,234.54,8.10">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1430" to="1442" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.66,593.98,337.95,8.10;12,141.73,605.02,239.96,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,255.38,593.98,215.23,8.10;12,141.73,605.02,66.14,8.10">Overview of the ImageCLEF 2012 Flickr Photo Annotation and Retrieval Task</title>
		<author>
			<persName coords=""><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,214.08,605.02,94.59,8.10">CLEF 2012 working notes</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.66,616.06,337.99,8.10;12,141.74,626.98,329.07,8.10;12,141.74,638.02,172.52,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,283.34,616.06,187.32,8.10;12,141.74,626.98,79.52,8.10">Recuperaci√≥n de Informaci√≥n visual utilizando descriptores conceptuales</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>De Ves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,238.59,626.98,232.22,8.10;12,141.74,638.02,86.81,8.10">Conference Proceedings of the Conferencia Espa√±ola de Recuperaci√≥n de Informaci√≥n</title>
		<meeting><address><addrLine>CERI; Valencia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.66,649.07,338.00,8.10;12,141.74,659.98,243.56,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,372.85,649.07,97.81,8.10;12,141.74,659.98,99.07,8.10">UNED-UV at Medical Retrieval Task of ImageCLEF</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia-Serrano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,275.53,659.98,87.22,8.10">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.65,149.97,338.02,8.10;13,141.72,161.02,328.80,8.10;13,141.72,172.06,201.93,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="13,422.79,149.97,47.87,8.10;13,141.72,161.02,328.80,8.10;13,141.72,172.06,50.24,8.10">Using Visual Concept Features in a Multimodal Retrieval System for the Medical collection at Im-ageCLEF2012</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garc√≠a-Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>De Ves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,198.85,172.06,94.45,8.10">CLEF 2012 working notes</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.65,182.74,337.99,8.10;13,141.72,194.02,321.81,8.10" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="13,404.52,182.74,66.11,8.10;13,141.72,194.02,228.72,8.10">Multimodal Information Approaches for the Wikipedia Collection at ImageCLEF</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Granados</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia-Serrano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Working Notes</note>
</biblStruct>

<biblStruct coords="13,132.66,205.06,338.00,8.10;13,141.74,215.98,309.93,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,279.73,205.06,190.93,8.10;13,141.74,215.98,181.74,8.10">Annotation and Retrieval System Using Confabulation Model for ImageCLEF2011 Photo Annotation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Izawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Motohashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Takagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,341.91,215.98,87.22,8.10">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.66,227.02,338.00,8.10;13,141.75,238.07,328.86,8.10;13,141.74,248.98,76.65,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,278.53,227.02,192.13,8.10;13,141.75,238.07,278.34,8.10">REGIMvid at ImageCLEF2011: Integrating Contextual Information to Enhance Photo Annotation and Concept-based Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ksibi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">B</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">B</forename><surname>Amar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,439.70,238.07,30.91,8.10;13,141.74,248.98,54.10,8.10">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.67,260.03,337.95,8.10;13,141.75,271.07,324.93,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,357.39,260.03,113.23,8.10;13,141.75,271.07,163.87,8.10">Applying logistic regression to relevance feedback in image retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zuccarello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ayala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>De Ves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Domingo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,311.76,271.07,71.29,8.10">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">2621</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.41,281.99,338.21,8.10;13,141.75,293.03,176.85,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,276.49,281.99,194.13,8.10;13,141.75,293.03,49.03,8.10">The CLEF 2011 photo annotation and concept-based retrieval tasks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liebetrau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,208.82,293.03,87.23,8.10">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,132.41,304.07,338.25,8.10;13,141.76,314.99,328.98,8.10;13,141.76,326.03,53.85,8.10" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,380.79,304.07,89.87,8.10;13,141.76,314.99,255.08,8.10">MLKD&apos;s Participation at the CLEF 2011 Photo Annotation and Concept-Based Retrieval Tasks</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Spyromitros-Xious</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sechidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,416.33,314.99,54.41,8.10;13,141.76,326.03,31.30,8.10">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
