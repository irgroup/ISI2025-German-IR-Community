<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,145.70,151.59,315.68,12.64;1,283.37,169.07,29.18,12.54">BUAA AUDR at ImageCLEF 2012 Medical Retrieval Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,224.45,207.32,37.51,9.05"><forename type="first">Wei</forename><surname>Song</surname></persName>
							<email>songwei@nlsde.buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.09,207.32,61.45,9.05"><forename type="first">Danchen</forename><surname>Zhang</surname></persName>
							<email>zhangdanchen@nlsde.buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,337.75,207.32,44.54,9.05"><forename type="first">Junwu</forename><surname>Luo</surname></persName>
							<email>luojunwu@nlsde.buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,145.70,151.59,315.68,12.64;1,283.37,169.07,29.18,12.54">BUAA AUDR at ImageCLEF 2012 Medical Retrieval Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EC180156D14E7F2DF257DE82C1A295E8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>medical image retrieval</term>
					<term>MeSH</term>
					<term>query expansion</term>
					<term>Modality Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the participation of the BUAA AUDR group at ImageCLEF 2012 at the Medical Image classification and Retrieval task. We performed two subtasks: modality classification and ad-hoc image-based retrieval. It was our first time to select modality classification task and we concentrated on mono-modal visual-based image classifier. We used LibSVM to train the classifier, and edge histogram feature to represent images. To improve its performance, we tried to extend the training set. However, due to size of the training set and other reasons, its accuracy was even worse. For adhoc image-based retrieval, we utilized MeSH as source of query expansion and only textual information were considered. We also explored mixed approaches that combine modality predication and query expansion and our best runs ranked second among all the textual runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper presents the second participation of the BUAA AUDR group at ImageCLEF.</p><p>ImageCLEF 2012 includes four types of tasks: medical image retrieval, photo annotation, plant identification and robot vision. On the basis of the work in the last year, we continued to focus on medical image retrieval and extended to modality classification subtask. The medical retrieval task of ImageCLEF 2012 uses a subset of PubMed Central containing 305,000 images. For modality classification, previous studies have shown that imaging modality is an important aspect of the image for medical retrieval. In user-studies, clinicians have indicated that modality is one of the most important filters that they would like to be able to limit their search by. Studies have shown that the modality can be extracted from the image itself using visual features. Additionally, using the modality classification, the search results can be improved significantly. In the ad-hoc image-based retrieval, participants will be given a set of 30 textual queries with 2-3 sample images for each query. The queries will be classified into textual, mixed and semantic, based on the methods that are expected to yield the best results.</p><p>This year we performed two subtasks of medical image retrieval: modality classification and ad-hoc image-based retrieval. For modality classification, we concentrated on mono-modal visual-based image classifier. We used LibSVM to train the classifier, and edge histogram feature to represent images. To improve its performance, we tried to extend the training set. In ad-hoc retrieval, we applied a MeSH-based query expansion strategy and added a modality prediction step to further improve the retrieval performance. Our runs were also competitive among textual runs.</p><p>The remainder of this paper is organized as follows. In section 2 we describe our approaches in detail. And our submitted runs are discussed in section 3. Then we conclude in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approaches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Modality Classification</head><p>In the experiment of medical image modality classification, we only concentrated on mono-modal visual-based image classifier. We performed experiments on three kinds of features: Edge Histogram Feature (EH), Tamura Feature (T) and Gabor Feature (G). We first used Medical Retrieval Images of 2011 as test data and then trained the classifier with data of 2012.</p><p>In the experiment, we used LibSVM to train one-versus-all classifiers. At first, we trained three classifiers per feature type and the classifier trained by Edge Histogram Feature had the highest accuracy. To fuse visual features, we simply averaged scores of classification result of different feature types per image. Finally, the combination of three features outperformed Edge Histogram Feature slightly. (see Table <ref type="table" coords="2,428.06,500.65,3.77,9.05" target="#tab_0">1</ref>). To simplify the training process, we decided to use Edge Histogram Feature only. We tested several LibSVM parameters and in this case, the best result were obtained with a radial basis function kernel, parameter gamma = 0.5 and cost = 2.</p><p>To improve accuracy, we decided to extend the image training set. To begin with, we used two approaches to obtain pre-extended images of each modality from the image collection of the medical retrieval task. In the first approach, we trained an initial classifier with provided training images, with which we classified 40,000 images randomly selected from the collection. In the second approach, we selected top 200 images from image collection which were queried by images selected from provided training set [1]. Next, we manually selected appropriate images of each modality from the pre-extended images and added them to the training set to train the final modality classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ad-hoc Image-based Retrieval</head><p>In ImageCLEF 2011, several groups adopted query expansion as the key techniques with the help of some external sources such as Unified Medical Language System (UMLS) and MeSH and achieved good results. After in-depth research of their approaches we decided to use MeSH as our source of query expansion. MeSH is the National Library of Medicine's controlled vocabulary thesaurus. It consists of three types of record: Descriptors, Qualifiers and Supplementary Concept Records. Each type of records are organized in tree structure with the same hierarchy and stored in a xml file. As the records in MeSH are always a sequence of terms, so the topics given are pre-processed to get all the combinations of the terms after elimination of the stop words. Then we applied various strategies to explore the related information in MeSH to expand the query.</p><p>When we used MeSH to expand the query, we noticed a situation in which two ngrams that both their related child records and entry terms could be used expand the query and one was contained in the other at the same time. Let's take cell and muscle cell for example. Obviously, it will introduce too much noise if we add the child descriptors of the n-gram cell. So under the circumstance only the related child descriptors or entry terms of muscle cell are used to expand the query. Both methods below applied this rule.</p><p>1．If the query is a record, its child descriptors are added to the query. No expansion otherwise.</p><p>2．If the query is a record, we use its child descriptors to expand the query. If it is an entry term, the child descriptors of the descriptor which the query belongs to are used for expansion.</p><p>All our submitted runs applied the second method for query expansion as it gave better results compared with the first one.</p><p>To further improve the retrieval performance we combined our retrieval system with modality prediction. Firstly, a standard retrieval step was performed, and 2,000 images were obtained per topic. Then we used the modality classifier to predict modality of these images. Next, we extracted expressed modality (EM) from the images and the query of each topic. Finally, we combined expressed modality and predicted modality (PM) with retrieval result. In this process, the retrieval scores of 2,000 images were modified and top 1,000 images were the final result.</p><p>We investigated three fusion strategies with Medical Retrieval Image Collection of 2011 as test data. The first approach is score fusion (SF): the final scores are obtained with the retrieval score and modality classification score following the below equation (d standards for document):</p><formula xml:id="formula_0" coords="3,216.87,634.65,257.14,11.54">) ( ) 1 ( ) ( ) ( d S d S d s M R      (2.1)</formula><p>The second approach is a variant of SF (SF_variant): if EM is the same to PM, the final score is the sum of R S and M S , otherwise, the final score remains the original retrieval score R S . The third strategy is Filtering (F): if EM is the same to PM, the final score is 1.75 times of R S , otherwise, the final score remains the original retrieval score R S <ref type="bibr" coords="4,160.94,165.44,10.66,9.05">[2]</ref>. Finally, we chose Filtering to enhance the performance of retrieval performance. Unfortunately, we submitted a wrong run. Table <ref type="table" coords="4,339.67,471.37,4.98,9.05" target="#tab_2">3</ref> describes the original run and the second run is the one which should have been submitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modality Classification</head><p>-V1: only used the provided training set to train classifier.</p><p>-V2: we extended the training set with 170 selected images from image collection.</p><p>However, the classification result with training set extended decreased slightly compared with the result of the original training set. We think the reason is that the training set is not well balanced, its size is not big enough and images which are added into training set are not classified precisely.</p><p>The text retrieval approaches are all based on the bag-of-words model, a text (such as image caption or full article) is represented as an unordered collection of words after tokenization and standard stopword removal. After the pre-processing step, two information models are considered: vector space model and topic model. We also use a query expansion mechanism, pseudo relevance feedback based on informationtheoretic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ad-hoc Retrieval</head><p>Two different indexes were built. One contained caption of the image and the other also added article information. To further improve the performance, we also added a modality prediction step to filter retrieve result. Our best runs were indexed with image caption and article with both query expansion and modality predication applied and it ranked second among all the textual runs. The results show that both query expansion and modality predication could improve the retrieval performance. Most of our runs which used the captions and article as indexes performed better than those use captions only. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>This article describes the approaches and results of BUAA AUDR group at ImageCLEF 2012. We submitted 9 runs for modality classification and ad-hoc retrieval. For modality classification, the result was poor. We need to do more work to get it better. For ad-hoc retrieval, we adopted a Mesh-based query expansion and enhanced the results by a modality prediction step. The best of our runs ranked second in all textual runs. The results indicate that query expansion and modality predication can improve the retrieval performance. As our modality classification was not satisfied, our retrieval performance still has room for improvement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,124.82,523.99,330.97,46.85"><head>Table 1 .</head><label>1</label><figDesc>experiments on Medical Retrieval Images of 2011.</figDesc><table coords="2,124.82,546.82,330.97,24.02"><row><cell>Features</cell><cell>EH</cell><cell>T</cell><cell>G</cell><cell>EH+T</cell><cell>EH+G</cell><cell>G+T</cell><cell>EH+G+T</cell></row><row><cell>Accuracy</cell><cell>59.72%</cell><cell>49.93%</cell><cell>49.17%</cell><cell>59.72%</cell><cell>58.16%</cell><cell>53.96%</cell><cell>60.01%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,136.22,189.26,335.72,88.25"><head>Table 2 .</head><label>2</label><figDesc>experiments on Medical Retrieval Image collection of 2011.</figDesc><table coords="4,136.22,206.06,335.72,71.45"><row><cell>Fusion Type</cell><cell>Map</cell><cell>Bpref</cell><cell>P10</cell><cell>P20</cell></row><row><cell>Original retrieval Result</cell><cell>0.175</cell><cell>0.2187</cell><cell>0.3133</cell><cell>0.27</cell></row><row><cell>Original retrieval Result +SF</cell><cell>0.1702</cell><cell>0.2133</cell><cell>0.3033</cell><cell>0.2683</cell></row><row><cell>Original retrieval Result +SF_variant</cell><cell>0.1854</cell><cell>0.2314</cell><cell>0.3267</cell><cell>0.2833</cell></row><row><cell>Original retrieval Result+F</cell><cell>0.1872</cell><cell>0.2361</cell><cell>0.3533</cell><cell>0.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,138.02,406.87,290.24,56.66"><head>Table 3 .</head><label>3</label><figDesc>experiments on Medical Retrieval Images of 2012.</figDesc><table coords="4,138.02,423.79,290.24,39.74"><row><cell>Run</cell><cell>Modality</cell><cell>Accuracy</cell></row><row><cell>ModalityClassificationSubmit</cell><cell>visual</cell><cell>39.7%</cell></row><row><cell>ModalityClassificationSubmit_Extend</cell><cell>visual</cell><cell>39%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,132.50,218.30,329.66,225.55"><head>Table 4 .</head><label>4</label><figDesc>Descriptions of ad-hoc image-based retrieval runs</figDesc><table coords="5,132.50,235.13,329.66,208.72"><row><cell>Run</cell><cell>Description</cell></row><row><cell>TFIDF_CAPTION_ARTICLE[QE2]_MC</cell><cell>Use image caption and article as</cell></row><row><cell></cell><cell>indexes with query expansion and</cell></row><row><cell></cell><cell>modality classification</cell></row><row><cell>TFIDF_CAPTION_ARTICLE[QE2]</cell><cell>Use image caption and article as</cell></row><row><cell></cell><cell>indexes with query expansion</cell></row><row><cell>TFIDF_CAPTION_ARTICLE_MC</cell><cell>Use image caption and article as</cell></row><row><cell></cell><cell>indexes with modality classification</cell></row><row><cell>TFIDF_CAPTION_ARTICLE</cell><cell>Use image caption and article as</cell></row><row><cell></cell><cell>indexes, no query expansion and</cell></row><row><cell></cell><cell>modality classification</cell></row><row><cell>TFIDF_CAPTION[QE2]_MC</cell><cell>Use image caption as indexes with</cell></row><row><cell></cell><cell>query expansion and modality</cell></row><row><cell></cell><cell>classification</cell></row><row><cell>TFIDF_CAPTION[QE2]</cell><cell>Use image caption as indexes with</cell></row><row><cell></cell><cell>query expansion</cell></row><row><cell>TFIDF_CAPTION_MC</cell><cell>Use image caption as indexes with</cell></row><row><cell></cell><cell>modality classification</cell></row><row><cell>TFIDF_CAPTION</cell><cell>Use image caption as indexes,no</cell></row><row><cell></cell><cell>modality classification</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,205.61,458.47,184.21,8.18"><head>Table 5 .</head><label>5</label><figDesc>retrieval results for ad-hoc retrieval task</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,144.86,318.17,325.72,8.18;6,124.82,328.61,345.91,8.18;6,124.82,338.93,242.06,8.18" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,388.41,318.17,82.17,8.18;6,124.82,328.61,331.00,8.18">Xrce&apos;s participation in Medical Image Modality Classification and Ad-hoc Retrieval Tasks of ImageCLEF 2011</title>
		<author>
			<persName coords=""><forename type="first">Gabriela</forename><surname>Jacquet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">St´ephane</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Clinchant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,124.82,338.93,107.74,8.18">Working Notes of CLEF 2011</title>
		<meeting><address><addrLine>Amsterdam, the Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,144.86,349.85,325.48,8.18;6,124.82,360.29,345.71,8.18;6,124.82,370.61,206.30,8.18" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,332.64,349.85,137.70,8.18;6,124.82,360.29,189.53,8.18">On modality classification and its use in text-based image retrieval in medical databases</title>
		<author>
			<persName coords=""><surname>．p</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Tirilly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,333.85,360.29,136.67,8.18;6,124.82,370.61,179.97,8.18">Proceedings of the 9th International Workshop on Content-based Multimedia Indexing</title>
		<meeting>the 9th International Workshop on Content-based Multimedia Indexing</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,144.86,381.55,325.85,8.18;6,124.82,391.99,345.67,8.18;6,124.82,402.31,345.91,8.18;6,124.82,412.75,186.34,8.18" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,317.05,391.99,153.44,8.18;6,124.82,402.31,329.80,8.18">Xrce&apos;s participation in wikipedia retrieval, medical image modality classification and ad-hoc retrieval tasks of ImageCLEF 2010</title>
		<author>
			<persName coords=""><forename type="first">Gabriela</forename><surname>．stéphane Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillaume</forename><surname>Ah-Pine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Florent</forename><surname>Jacquet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jorge</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Keyvan</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Minoukadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,124.82,412.75,107.68,8.18">Working Notes of CLEF 2010</title>
		<meeting><address><addrLine>Padova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,144.86,423.67,325.73,8.18;6,124.82,434.11,345.87,8.18;6,124.82,444.43,68.37,8.18" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,361.51,423.67,109.08,8.18;6,124.82,434.11,255.29,8.18">LABERINTO at ImageCLEF 2011 Medical Image Retrieval Task In Working Notes of CLEF</title>
		<author>
			<persName coords=""><forename type="first">Mariano</forename><surname>Mata</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manuel</forename><forename type="middle">J</forename><surname>Crespo</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Maña</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<pubPlace>Amsterdam, the Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,144.86,455.35,325.79,8.18;6,124.82,465.79,242.06,8.18" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Chengbo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Uestc</forename><surname>Tian</surname></persName>
		</author>
		<title level="m" coord="6,307.10,455.35,163.55,8.18;6,124.82,465.79,107.74,8.18">ImageCLEF 2011 Medical Retrieval Task In Working Notes of CLEF 2011</title>
		<meeting><address><addrLine>Amsterdam, the Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
