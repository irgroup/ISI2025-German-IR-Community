<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.93,116.95,331.51,12.62;1,291.37,134.89,32.61,12.62">DBRIS at ImageCLEF 2012 Photo Annotation Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,222.89,172.56,84.05,8.74"><forename type="first">Magdalena</forename><surname>Rischka</surname></persName>
							<email>rischka@cs.uni-duesseldorf.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">Heinrich-Heine-University of Duesseldorf</orgName>
								<address>
									<postCode>D-40204</postCode>
									<settlement>Duesseldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,329.63,172.56,62.85,8.74"><forename type="first">Stefan</forename><surname>Conrad</surname></persName>
							<email>conrad@cs.uni-duesseldorf.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">Heinrich-Heine-University of Duesseldorf</orgName>
								<address>
									<postCode>D-40204</postCode>
									<settlement>Duesseldorf</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.93,116.95,331.51,12.62;1,291.37,134.89,32.61,12.62">DBRIS at ImageCLEF 2012 Photo Annotation Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0794065A8C84137D2B6E36A067063FAD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>SIFT</term>
					<term>bow</term>
					<term>spatial pyramids</term>
					<term>visual phrases</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For our participation in the ImageCLEF 2012 Photo Annotation Task we develope an image annotation system and test several combinations of SIFT-based descriptors with bow-based image representations. Our focus is on the comparison of two image representation types which include spatial layout: the spatial pyramids and the visual phrases. The experiments on the training and test set show that image representations based on visual phrases significantly outperform spatial pyramids.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper presents our participation in the ImageCLEF 2012 Photo Annotation Task. The ImageCLEF 2012 Photo Annotation Task is a multi-label image classification challenge: given a training set of images with underlying concepts the aim is to detect the presence of these concepts for each image of a test set using an annotation system based on visual or textual features or a combination of both. Detailed information on the task, the training and test set of images, the concepts and the evaluation measures can be found in the overview paper <ref type="bibr" coords="1,467.31,476.82,9.96,8.74" target="#b0">[1]</ref>. Our automatic image annotation system bases only on visual features. We focus on the comparison of two image representations which regard spatial layout: the spatial pyramid <ref type="bibr" coords="1,198.80,512.69,13.96,8.74" target="#b3">[4]</ref> and the visual phrases <ref type="bibr" coords="1,307.42,512.69,12.42,8.74" target="#b2">[3]</ref>. The spatial pyramid is very popular and often used, especially in the context of scene categorization, whereas visual phrases seem to pass out of mind in the literature.</p><p>The remainder of this paper is organized as follows: in section 2 we describe the architecture and the technical details of our image annotation system, in section 3 we present the evaluation on the training and the test set and discuss the results to end with a conclusion in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Architecture of the DBRIS image annotation system</head><p>The architecture of our automatic image annotation system together with the methods used in each step is illustrated in figure <ref type="figure" coords="1,342.56,645.16,3.87,8.74" target="#fig_0">1</ref>. To obtain the image representation of the training and test images, local features are extracted by applying the Harris-Laplace detector and the SIFT <ref type="bibr" coords="2,308.55,119.99,14.29,8.74" target="#b4">[5]</ref> descriptor in different color variants. The extracted local features are then summarized to the bag-of-words (bow) image representation as well as the image representations spatial pyramid <ref type="bibr" coords="2,446.73,143.90,13.96,8.74" target="#b3">[4]</ref> and visual phrases <ref type="bibr" coords="2,193.27,155.86,12.42,8.74" target="#b2">[3]</ref>. For the classifier training and classification steps we use an KNN-like classifier with one representative per concept. In the following subsections we describe each step in detail.   </p><formula xml:id="formula_0" coords="2,200.99,257.23,90.79,40.67">Harris-Laplace • C-SIFT • rgSIFT • OpponentSIFT • RGB-SIFT • SIFT • BOW • Spatial Pyramids</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Features</head><p>For the choice of local features we refer to the evalution of color descriptors presented in <ref type="bibr" coords="2,190.71,456.54,9.96,8.74" target="#b1">[2]</ref>. We adopt the features C-SIFT, rgSIFT, OpponentSIFT, RGB-SIFT and SIFT as they are shown to perform best on the evaluation's underlying image benchmark, PASCAL VOC Challenge 2007. To extract these features with the Harris-Laplace point sampling strategy as the base, we use the color descriptor software <ref type="bibr" coords="2,220.47,504.36,9.96,8.74" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image representations</head><p>For each of the features, we quantize its descriptor space (225.000 descriptors) into 500 and 5000 visual words using K-Means. The visual words serve as a basis for the BoW, spatial pyramid and visual phrases representations. The representations are created in the common way using hard assignment of image features to visual words. We use the spatial pyramid constructions 1x3, 1x1+1x3 and 1x1+2x2+4x4 in a weighted and unweighted version. To construct visual phrases we follow <ref type="bibr" coords="2,176.39,633.20,10.52,8.74" target="#b2">[3]</ref> and define a visual phrase as a pair of adjacent visual words. Assume an image contains the keypoints kp a = {(x a , y a ), scale a , orient a , descr a } and kp b = {(x b , y b ), scale b , orient b , descr b } with their assigned visual words vw i and vw j , respectively. Then the image contains the visual phrase vp ij = {vw i , vw j } if the Euclidean distance of the keypoints' location in the image satisfy the term EuclideanDistance((x a , y a ), (x b , y b )) &lt; max(scale a , scale b ) • λ</p><p>We set λ = 3. Analogously to the bow representation an image is represented by a histogram of visual phrases. Furthermore we create a representation combining bow with visual phrases, weighting bow with a value of 0.25 and the visual phrases histogram with 0.75. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Classifier</head><p>We use a KNN-like classifier, where concepts are not represented by the set of the corresponding images, but only by one representative. The representative of a concept is obtained by averaging the image representations of all images belonging to the concept. To classify a test image the similarities between the test image and the representatives of all concepts are determined. As similarity function we use the histogram intersection. To receive binary decisions on the membership to the concepts, we set an image-dependent threshold: a concept is present in the test image if the similarity between the test image and the concept is equal or greater than 0.75 times the maximum of the similarities of the test image to all concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>In the following we describe two evaluations: firstly we present the results of our experiments made on the training set. Secondly we discuss the evaluation of the five runs submitted to ImageCLEF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training and classification on the training set</head><p>To train and evaluate the DBRIS image annotation system, we split the training set of images into two disjoint parts (of size 7500), whereby both parts contain almost equal size of images for each concept. For each training and test pair we train the classifier on the one part and then use this classifier to classify the other part of images. The evaluation results are then averaged over the two training and test pairs. In the first experiment we train one image annotation system for each of the 35 combinations of descriptors and image representations. Figure <ref type="figure" coords="4,446.21,223.20,4.98,8.74" target="#fig_1">2</ref> shows the results in terms of MiAP values (averaged over all concepts). Comparing the systems with regard to the descriptors we observe an almost identical performance behaviour as shown in <ref type="bibr" coords="4,279.60,259.06,9.96,8.74" target="#b1">[2]</ref>. Except for the rgSIFT combined with the visual phrases based image representations, C-SIFT outperforms all the other descriptors in every image representation. The worst results are obtained by the SIFT descriptor. When we consider the image representations, we can see that the image representations based on visual phrases perform significantly better than the other ones for all descriptors. In the case of the descriptors C-SIFT, rgSIFT and OpponentSIFT the representations vp and bow &amp; vp achieve similar values. When using RGB-SIFT and SIFT, the bow &amp; vp representation is the better choice of the two. Bow and the representations based on spatial pyramid differ slightly from each other. Which one to choose depends on the descriptor used. In the next experiment we join all descriptors into one annotation system, i.e. for each of the seven image representations we train an image annotation system whose classifier consists of five classifiers corresponding to the five descriptors. At the classification step, the similarities between the test image and the concept representatives obtained in each of the five classifiers are averaged over these five classifiers. The binary decisions on the membership to the concepts are calculated in the same way as described in section 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classification of the test set</head><p>For the classification of the test set, we train the classifier on the whole training set. For the five submission runs we choose five of the image representations from the second experiment presented in section 3.1: sp 1x1+2x2+4x4 w as run DBRIS 1, combined as DBRIS 2, sp 1x1+1x3 as DBRIS 3, vp as DBRIS 4 and bow &amp; vp as DBRIS 5. Figure <ref type="figure" coords="5,269.64,561.47,4.98,8.74" target="#fig_3">4</ref> and figure <ref type="figure" coords="5,325.33,561.47,4.98,8.74" target="#fig_4">5</ref> present the results of the configurations for each concept (MiAP values) and as averages (MiAP, MnAP, GMiAP, GMnAP) over all concepts. Best values or values which are significantly better than others within a certain concept are highlighted in green. To evaluate the image representations as a whole, firstly we consider the averages MiAP, MnAP, GMiAP, GMnAP in figure <ref type="figure" coords="5,289.52,621.25,3.87,8.74" target="#fig_4">5</ref>. The image representations vp and bow &amp; vp yield the best values again, followed by combined, sp 1x1+2x2+4x4 w and sp 1x1+1x3. These results reflect the evaluation in figure <ref type="figure" coords="5,372.84,645.16,3.87,8.74">3</ref>. When we consider the concepts with their concept categories, we can see that there are some concept categories where the image representations based on visual phrases dominate. These concept categories are quantity, age, (gender ) and view. These observations have also been made in the experiments on the training set. Other concept categories which yield best results with the visual phrases on the training set are relation and setting. A possible reason for the success of the visual phrases in these concept categories can be that these concepts contain a lot of pictures of persons. Visual phrases can catch human features like eyes, mouth, etc. better than the spatial pyramids because they work on a finer level. The success of the visual phrases in the concept category water can not be confirmed by the experiments on the training set. As visual phrases are popular for object detection tasks, it is surprising that these image representations fail in the concept category fauna. The best results in the concept category fauna are achieved with the image representations based on spatial pyramids. Spatial pyramids are also successful in sentiment and transport.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>At the end we want to summarize the experiences we gathered in the experiments. The best performing descriptor, which is C-SIFT in the experiments, yields better performance than joining all descriptors together. For the choice on image representations, the image representations based on visual phrases significantly outperform the spatial pyramids and the bow representation. The evaluation shows that visual phrases are especially appropriate for concepts dealing with persons. Although visual phrases are often used in object detection tasks, they are also successful in scene categorization.   0,0057 0,0059 0,0058 0,0056 0,0057 0,0774 0,081 0,0788 0,0818 0,0818 0,0927 0,0938 0,0925 0,0976 0,0972 0,0355 0,0374 0,0363 0,0385 0,0385 0,0441 0,0454 0,0445 0,0476 0,047 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,186.08,364.90,243.20,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Architecture of the DBRIS image annotation system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,143.53,585.73,328.31,7.89"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. MiAP values for each combination of descriptor and image representation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,205.52,440.22,204.31,7.89"><head>3 .Fig. 3 .</head><label>33</label><figDesc>Fig. 3. MiAP values for each image representation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,205.82,703.52,203.73,7.89"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Results of the submitted runs 1 (in MiAP)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,205.82,703.52,203.73,7.89"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Results of the submitted runs 2 (in MiAP)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,134.77,215.09,345.83,161.00"><head>Table 1 .</head><label>1</label><figDesc>Table 1 summarizes all image representations with their number of dimensions we used in combination with each feature. Image representations</figDesc><table coords="3,217.21,272.15,180.93,87.62"><row><cell cols="2">image representation number of dimensions</cell></row><row><cell>bow</cell><cell>5.000</cell></row><row><cell>sp 1x3</cell><cell>15.000</cell></row><row><cell>sp 1x1+1x3</cell><cell>20.000</cell></row><row><cell>sp 1x1+2x2+4x4</cell><cell>105.000</cell></row><row><cell>sp 1x1+2x2+4x4 w</cell><cell>105.000</cell></row><row><cell>vp</cell><cell>125.250</cell></row><row><cell>bow &amp; vp</cell><cell>130.250</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,138.35,470.36,342.25,7.86;6,146.91,481.32,288.01,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,253.16,470.36,227.43,7.86;6,146.91,481.32,93.42,7.86">Overview of the ImageCLEF 2012 Flickr Photo Annotation and Retrieval Task</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,247.72,481.32,103.78,7.86">CLEF 2012 working notes</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,492.28,342.24,7.86;6,146.91,503.24,333.68,7.86;6,146.91,514.19,333.68,8.12;6,146.91,525.80,94.15,7.47" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,374.27,492.28,106.33,7.86;6,146.91,503.24,157.94,7.86">Evaluating Color Descriptors for Object and Scene Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<ptr target="http://www.colordescriptors.com" />
	</analytic>
	<monogr>
		<title level="j" coord="6,328.30,503.24,152.30,7.86;6,146.91,514.19,123.26,7.86">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1582" to="1596" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,536.11,342.24,7.86;6,146.91,547.07,333.67,7.86;6,146.91,558.03,113.93,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,284.76,536.11,195.83,7.86;6,146.91,547.07,133.91,7.86">Constructing visual phrases for effective and efficient object-based image retrieval</title>
		<author>
			<persName coords=""><forename type="first">Qing-Fang And</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,301.13,547.07,179.46,7.86;6,146.91,558.03,20.69,7.86">ACM Trans. Multimedia Comput. Commun. Appl</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,568.99,342.25,7.86;6,146.91,579.95,333.67,7.86;6,146.91,590.91,333.68,7.86;6,146.91,601.87,61.95,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,306.12,568.99,174.48,7.86;6,146.91,579.95,211.16,7.86">Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,381.09,579.95,99.49,7.86;6,146.91,590.91,228.64,7.86">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,138.35,612.82,342.24,7.86;6,146.91,623.78,297.70,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,219.54,612.82,241.32,7.86">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,146.91,623.78,166.73,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
