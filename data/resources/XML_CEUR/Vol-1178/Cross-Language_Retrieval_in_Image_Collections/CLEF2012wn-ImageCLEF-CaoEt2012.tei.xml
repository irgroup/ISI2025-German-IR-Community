<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,182.08,116.95,251.21,12.62;1,228.78,134.89,157.79,12.62;1,167.94,152.82,279.48,12.62;1,184.83,170.75,245.70,12.62">IBM T.J. Watson Research Center, Multimedia Analytics: Modality Classification and Case-Based Retrieval tasks of ImageCLEF2012</title>
				<funder ref="#_3kMsBsb">
					<orgName type="full">DCTD/NCI/NIH</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,167.00,208.42,63.79,8.74"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
							<email>liangliang.cao@us.ibm.com</email>
						</author>
						<author>
							<persName coords="1,239.09,208.42,70.65,8.74"><forename type="first">Yuan-Chi</forename><surname>Chang</surname></persName>
						</author>
						<author>
							<persName coords="1,318.24,208.42,54.48,8.74"><forename type="first">Noel</forename><surname>Codella</surname></persName>
							<email>nccodell@us.ibm.com</email>
						</author>
						<author>
							<persName coords="1,380.50,208.42,63.39,8.74"><forename type="first">Michele</forename><surname>Merler</surname></persName>
							<email>mimerler@us.ibm.com</email>
						</author>
						<author>
							<persName coords="1,223.20,220.38,77.23,8.74"><forename type="first">Quoc-Bao</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName coords="1,328.23,220.38,63.92,8.74"><forename type="first">John</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
							<email>jsmith@us.ibm.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<addrLine>19 Skyline Dr</addrLine>
									<postCode>10532</postCode>
									<settlement>Hawthorne</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">courtesy of TM Deserno</orgName>
								<orgName type="department" key="dep2">Dept. of Medical Informatics</orgName>
								<orgName type="institution">RWTH Aachen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,182.08,116.95,251.21,12.62;1,228.78,134.89,157.79,12.62;1,167.94,152.82,279.48,12.62;1,184.83,170.75,245.70,12.62">IBM T.J. Watson Research Center, Multimedia Analytics: Modality Classification and Case-Based Retrieval tasks of ImageCLEF2012</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D4B5AEE2751F0B3649F1A9C6E4CC20DA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>SVM</term>
					<term>Multiclass</term>
					<term>Kernel Approximation 1</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present the modeling strategies that were applied by the IBM T.J. Watson research team to the modality classification and case-based retrieval tasks of ImageCLEF 2012. The primary challenges of this year's medical modality classification task were as follows: 1) the supplied training data was extremely limited, with some categories having as few as 5 positive examples, leaving little room for internal testing, and 2) some modalities appeared to be visually similar.</p><p>In order to address these challenges, we approached the task from two fronts: 1) we attempted to augment the training data with additional examples of each category, and 2) we experimented with a broad range of modeling strategies and feature extraction techniques. For the case based retrieval task, we employed a semantic similarity approach to measure the relatedness among medical concepts found in the text corpus. We believe the lack of using additional lexical database besides the UMLS-methathesaurus led to poor performance in relation to other approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ImageCLEF 2012 Medical Modality Classification Task is a standardized benchmark for systems to automatically classify medical image modality from PubMed journal articles. The 2012 dataset has changed from the previous year in 3 significant ways: 1) there are more categories, 2) the number of training examples is far fewer, and 3) some modalities are more similar.</p><p>Our approach can be described as scaling up to utilize as many features and data as possible. Our experiments demonstrate that increasing either axes tends to boost performance. In addition, we present a method for kernel approximation to help address the computational time costs of using a wide variety of methods. For data augmentation, we drew from several sources outside the Image-CLEF2012 collection, such as a Bing web-crawl for each category, as well as publicly available medical image datasets, such as The Cancer Imaging Archives (TCIA), and Image Retrieval in Medical Applications (IRMA). For our modeling approaches, we selected multiple features extracted from a set of image granularities, such as SIFT variants <ref type="bibr" coords="2,292.80,339.14,9.96,8.74" target="#b0">[1]</ref>, GIST <ref type="bibr" coords="2,336.59,339.14,9.96,8.74" target="#b1">[2]</ref>, Local Binary Patterns (LBP) <ref type="bibr" coords="2,134.77,351.10,9.96,8.74" target="#b2">[3]</ref>, edge and color histograms, and Curvelets <ref type="bibr" coords="2,335.81,351.10,9.96,8.74" target="#b3">[4]</ref>. In addition, we experimented with a variety of feature fusion and learning approaches, including early, late, and kernel fusion, kernel approximation, multiclass SVM, and one-vs-all. We discovered that multiclass SVM using an early fusion of many features with an augmented dataset yielded the best performance. Kernel approximation methods were able to significantly increase the efficiency of modeling at a small cost to performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Modality Classification Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Datasets</head><p>Our experiments are performed on the following datasets:  3 , and the Japanese Society of Radological Technology (JSRT) <ref type="bibr" coords="2,463.82,593.84,9.96,8.74" target="#b4">[5]</ref>.</p><p>Due to the low number of positive examples in some categories in the original training dataset, we chose to construct an additional augmented dataset. The number of examples per category for both is shown in Fig. <ref type="figure" coords="3,393.93,143.90,3.87,8.74" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature Collections</head><p>All our experiments are run using 1 of 7 sets of low-level visual features. Here we name and describe the contents of 7 sets of features that were used for modeling. Each feature is described by name, with the granularity of extraction in parenthesis. The granularities are as follows:</p><p>• Global: Feature extracted from entire image. Native feature dimensionality preserved. • Grid: 5x5 image grid, with feature vector extracted from each grid block and concatenated. Increases dimensionality by factor of 25. • Grid7: 7x7 image grid, with feature vector extracted from each grid block and concatenated. Increases dimensionality by factor of 49. • Layout: 5 image regions including the center and the 4 quarters. Increases dimensionaltiy by a facor of 5. • Pyramid: Spatial pyramid, with global as first level, and 2x2 image grid as second level. Increases dimensionaltiy by a facor of 5.</p><p>The feature sets referenced later in the section are as follows:</p><p>• Feature Set </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multiclass SVM</head><p>We employed the LibSVM library <ref type="bibr" coords="4,283.42,266.78,15.50,8.74" target="#b15">[17]</ref> to perform Multiclass SVM classification.</p><p>The process involves learning a set of 1-vs-1 classifiers, one for each pair of classes in the dataset. In order to classify a new example, each 1-vs-1 model is evaluated on it and the most likely label is selected based on a majority voting scheme. No data sampling was performed: the Multiclass SVM model was learned directly from the whole training set (either original and augmented). As such, one of the advantages of this learning strategy consists in explicitly modeling the priors of the classes in the dataset. All the parameters of the models chosen for the final submissions to ImageCLEF 2012 were chosen according to 5-fold (for the original dataset) and 3-fold (for the augmented one) cross validation performances. After experimenting on the internal cross-validation splits, a set of the best performing descriptors was selected for fusion. For the original dataset, Feature Set 1, as described in Section 2.2, was chosen. Feature Sect 2 was instead adopted for the extended dataset. Furthermore, the Chi-square kernel was selected (in preference over linear, RBF and histogram intersection) for all the Multiclass SVM runs, computed as</p><formula xml:id="formula_0" coords="4,241.14,455.94,239.45,29.85">K(x, y) = 1 - d (x d -y d ) 2 1 2 (x d + y d ) ,<label>(1)</label></formula><p>Three types of feature fusion methods were experimented: early, late, and kernel fusion.</p><p>• Early fusion: consists of a concatenation of different descriptors, before SVM modeling • Kernel Fusion: consists of a point-wise pooling (max or average operator) over the kernel matrices produced by each descriptor. The Multiclass SVM is then learned on top of the aggregate matrix • Late Fusion: consists of a pooling (max, average or product) operator over the predictions of the models learned from individual features for each test image. For this type of fusion we employed the probabilistic output option in each SVM, which converts the 1-vs-1 comparisons into class probabilities.</p><p>For each test image, each model produced a vector with N probabilities (where N is the number of classes, 31 in our case). After the pooling was applied in a point-wise manner over the prediction vectors of the models, the class with the maximum aggregate probability was chosen as the final prediction.</p><p>Each strategy is exemplified in Figure <ref type="figure" coords="5,326.94,154.93,3.87,8.74" target="#fig_1">2</ref>. As reported in Section 2.6, the Multiclass SVM trained from the augmented dataset with early fusion strategy (Experiment 12) provided the best performance. Kernel fusion proved to be equivalent, while late fusion performed worse than the other methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Efficient Feature Fusion with Kernel Approximation</head><p>To explore different aspects of visual phenomenon, we employed 19 different features, described in Feature Set 5. We used an early fusion strategy by concatenating these features together and training a kernelized Supporting Vector Machine (SVM). However, a practical problem of using so many features lies in the computational cost, both in the training and testing stage. When the number of images grows, or when the feature dimension increases, traditional SVM solvers may not work well or take a very long time to compute the optimal solution.</p><p>Among all the kernels in practice, the Chi-square kernel often yields very good performance compared with the others. Moreover, a large amount of our features, including LBP histogram, edge histogram, color histogram, and SIFT histogram, are in the form of histogram features. Chi-square kernel is arguable regarded as the first choice for histogram form features. In our work, we focus on how to efficiently solve Chi-square kernel only. We do not consider the problem of general kernels.</p><p>We consider the Chi-square kernel in the form of</p><formula xml:id="formula_1" coords="6,257.61,139.20,222.98,26.88">K(x, y) = d 2x d y d x d + y d ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_2" coords="6,163.57,177.22,197.35,9.68">x = [x 1 , x 2 , • • • , x d , • • •, y = [y 1 , y 2 , • • • , y d , • • •.</formula><p>It is easy to see that Eq.( <ref type="formula" coords="6,258.21,189.21,4.59,8.74" target="#formula_1">2</ref>) is defined as the additive sum of different dimensions. Such a kernel is referred to as an additive kernel. As suggested by <ref type="bibr" coords="6,462.33,201.16,14.61,8.74" target="#b11">[13]</ref>, such a group of kernels can be approximated by mapping the feature into a high dimensional space. By the representer theorem <ref type="bibr" coords="6,339.50,225.07,14.61,8.74" target="#b12">[14]</ref>, the solution of classification model can be written in the form of</p><formula xml:id="formula_3" coords="6,263.37,255.68,88.61,30.32">f (x) = N i=1 K(x, x i ),</formula><p>where i denotes the index of training samples. For any positive definite kernel, there exists a mapping x → φ(x) so that the final classification model becomes</p><formula xml:id="formula_4" coords="6,265.89,328.32,83.57,12.69">f (x) = w T φ φ(x) + b</formula><p>where w φ denotes the weights of the linear model in the mapped space. In this work, we will use Nystrom's approximation to construct the mapping function explicitly.</p><p>To make the representation simply we let</p><formula xml:id="formula_5" coords="6,268.13,407.28,79.10,23.22">k(x, y) = 2x d y d x d + y d ,</formula><p>then we can see the kernel is</p><formula xml:id="formula_6" coords="6,256.05,462.35,224.54,20.17">K(x, y) = d k(x d , y d ).<label>(3)</label></formula><p>Next we will discuss how to approximate k(x, y), which is a function on 1D space. To approximate k(x, y), we employ</p><formula xml:id="formula_7" coords="6,210.91,520.93,269.69,51.89">φ j (x) =        √ κ 0 if j = 0 2κ j+1 2 cos( j+1 2 Lx) if j &gt; 0 odd 2κ j 2 sin( j 2 Lx) if j &gt; 0 even<label>(4)</label></formula><p>where φ j and κ j constitute one pair of Fourier transform, L is the frequency parameter, and we use L = 2π/15 in practice. Then we can convert the nonlinear kernels with the linear model over φ j (x). For more details, please refer to <ref type="bibr" coords="6,458.00,609.29,14.61,8.74" target="#b11">[13]</ref>. So far we have discussed how to approximate the kernelized SVM using a linear model. Now we can compare the computational complexity of both methods. Suppose we have m features, 1 ≤ m ≤ M , and each feature is of dimension d m . The number of training samples is N , and size of testing set is T . For each d m features, we map it to the space of dimension 7d m . Note that the evaluation of kernel SVM depends on the number of support vectors, which in practice is proportional to the number of training examples. Also our linear approximation requires the extra cost of feature mapping, which is 7dN for training and 7dT for testing. Table <ref type="table" coords="7,211.52,167.81,4.98,8.74" target="#tab_2">1</ref> compares the computational complexity of the two methods. It is easy to see our linear approximation is much more efficient in both training and testing stage. Our linear approximation is even plausible for the scenario with a lot of features. To measure the effectiveness of our kernel approximation method, we compare how much difference exists between Chi-square kernels and our approximated kernels. Table <ref type="table" coords="7,227.51,420.76,4.98,8.74" target="#tab_3">2</ref> illustrates the speed up and percentage of kernel approximation error using randomly-generated features. It is easy to see the approximation error is low, while the speed up will be increasingly significant when the number of training samples grows. It is also interesting to see the classification accuracy after our kernel approximation. We implement the Chi-square kernel with LibSVM, and also use liblinear <ref type="bibr" coords="7,173.27,621.25,15.50,8.74" target="#b14">[16]</ref> to train a linear classifier using our projected features. As Figure <ref type="figure" coords="7,475.61,621.25,4.98,8.74" target="#fig_2">3</ref> shows, the linear model based on the high dimensional approximation is not necessarily worse than the original model. In fact in some categories, the liblinear model even works slightly better. Note that we do not tune the parameters of both models in this toy experiments. In the future, we plan to improve our model with heterogeneous kernel learning methods <ref type="bibr" coords="8,359.12,348.81,14.61,8.74" target="#b13">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">One-vs-All Ensemble SVM with Data Sampling</head><p>IBM Multimedia Analytics and Retrieval System (IMARS) <ref type="bibr" coords="8,398.89,408.36,10.52,8.74" target="#b5">[6]</ref> has been developed and applied previously for semantic classification of unstructured images and videos. The general framework is depicted in Fig. <ref type="figure" coords="8,372.41,432.27,3.87,8.74" target="#fig_3">4</ref>, and is primarily based on generating collections of 1-vs-All SVM classifiers.</p><p>Using the IMARS learning paradigm, we explored several variants of the pipeline to better understand the contribution of each to system performance:</p><p>• Feature Fusion: Early (vector concatenation) or Late (Unit Model score averaging). • Data Sampling: Random subsamples of negative examples, or using the entire dataset. • Number of Bags: Changing the number of Unit Models that are trained for a particular feature by choosing different subsamplings of example data. • Feature Sets: Varying sets of features used for modeling.</p><p>• Kernel Type: A single RBF kernel with kernel parameter -4, C=100, and sigmoid feature normalization was used for most model learning; however, we performed one experiment with a Chi 2 kernel, similar parameters, to understand any effect the variation of kernels might have.</p><p>1-vs-All scores of multiple Ensemble Models were converted to Multiclass labels by choosing the max over all the classifier scores.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Results</head><p>One-vs-All Ensemble SVM with Data Sampling 1-vs-ALL experiments are depicted in Table <ref type="table" coords="9,232.68,645.16,3.87,8.74" target="#tab_4">3</ref>. Experiment index number, feature fusion type (Early or Late), number of examples for each of and negative categories per bag, number of bags across data, number of bags across features, the feature set used, the average dimensionality across feature bags, the proportion of data used for Validation Split (VS), multiclass accuracy scores, and relative performance scaled by computational complexity (P/C) are shown. Computational complexity is computed simply as the running time required to compute a kernel matrix. Experiment 8 was reproduced using a Chi 2 kernel, yielding MACC of 64.4%, and was not listed in this table.</p><p>In the first experiment, we established a baseline with a very small data sampling rate (100 examples in each of positive and negative) and only two SIFT features utilized. In the second experiment, we examined the performance of our other features, excluding SIFT. In the third, we used both sets, and in the fourth we added the newer FourierPolarPyramid feature vector. In the fifth experiment, we examined the effect of using late fusion, instead of early fusion. In the sixth experiment, we began to study the effects of increasing the number of examplars. In the seventh and eigth experiments, we examined the effect of increasing data sampling either by using larger data bag sizes, or a larger number of bags. The ninth experiment represented our submission NCFC ORIG 2 EXTERNAL SUBMIT.txt; this dataset used a restricted number of features and late fusion, due to time constraints. In the last two experiments, we finally examine the effects of using larger amounts of data, up to the limit of our dataset.</p><p>In summary, our experiments yield a number of notable observations:</p><p>1. With early fusion, adding more features improves performance, SIFT contributing the most (see Exp. 1-4). 2. Adding additional data improves performance (Exp. 4, 6-11) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Early fusion of features outperforms late fusion with averaging, and provides</head><p>a better P/C ratio (Exp. 4-5). 4. Bagging along the data dimension reduces overall performance, but improves P/C (Exp. 7-8). 5. Best performance is acheived using early fusion of all features and all data (Exp. 11). 6. Chi 2 kernels and searching over SVM parameters holds the potential to boost performance further.</p><p>Multiclass SVM Multiclass SVM experiments are depicted in Table <ref type="table" coords="10,444.08,559.78,3.87,8.74" target="#tab_5">4</ref>. Three different aspects were explored in the experiments: fusion type, dataset size, and aggregation type. From the results in the Table emerges that:</p><p>1. Early and kernel fusion are comparable, and perform better than late fusion 2. a larger training set leads to a better classification performance (Dataset 2 is better than Dataset 1) 3. Averaging seems to be the best aggregation strategy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Official Submissions</head><p>In Figure <ref type="figure" coords="11,177.07,320.49,4.98,8.74" target="#fig_4">5</ref> are reported the official runs submitted to the ImageCLEF 2012 website, in comparison to all other official submissions (43 in total). IBM achieved the top three visual only classification performances, as well as the best overall accuracy.</p><p>The submissions were all purely visual, and corresponded to the following experiments (in decreasing order of performance): 12, 13 with Kernel approximation fusion (as described in Section 2.4), 13, 11, 12 with Kernel approximation fusion, 18, 21. After the submissions we further improved the worst performing ones through better normalization, parameter selection, and further debugging, yielding to the performances reported in Tables <ref type="table" coords="11,345.58,428.56,4.98,8.74" target="#tab_4">3</ref> and<ref type="table" coords="11,373.26,428.56,3.87,8.74" target="#tab_5">4</ref>.</p><p>Fig. <ref type="figure" coords="11,169.72,440.98,4.57,8.74" target="#fig_5">6</ref>(b) shows the confusion matrix of the best performing run. Overall the matrix presents a strong diagonal. However, some clear mis-classifications are evident. In particular, "GSYS -System overview" resulted to be the hardest class to categorize, being quite often confused with "GFLO -Flowcharts". Looking at the appearance of the images in such classes, it is evident that based on visual features alone they are in most cases indistinguishable. This confusion might be mitigated by exploiting the textual information associated with, or in, the images. We plan to follow this direction in future experiments.</p><p>We expect that extracting textual information and combining it with our strong visual modeling will boost classification performance, given the complementary information of those two representations, and also looking at the performances of other groups.</p><p>In conclusion, appropriately modeling the visual appearance can provide strong modality classification performance, even without text analysis. In our experiments we found that adding more features and training data lead to models with better classification performance. Early fusion and kernel fusion seem to be the best combination strategies. Our kernel approximation provides a principled and efficient framework to perform such fusion, while significantly increasing the efficiency of the computation of the best performing CHI2 kernel. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Case-Based Retrieval Task</head><p>In this section, we give an overview of the application of our methods to casebased medical image retrieval and present the results of our submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Techniques and Approaches</head><p>Techniques used for case-based retrieval task are mainly based on methods from Information Retrieval (IR) and Natural Language processing (NLP), including rule-based and machine-learning techniques. In order to facilitate the classification of a large dataset and to retrieve the most relevant documents for a given query, an IR system applies various NLP methods to construct a semantic view of each document indexed via relational database or text index. This semantic view is summarized by a set of relevant keywords (i.e., index terms) as a signature of this document.</p><p>In the biomedical domain, the terminology is very important because the words used in the document are related to medical terms that can refer to the same concept with different semantic interpretation (i.e., senses) based on the textual context. In addition to the NLP techniques for reducing the size of the relevant keywords by eliminating stopwords and stemming words, our system also applies semantic similarity methods to improve the understanding of textual terms and remedy potential ambiguity among medical concepts. The focus of the semantic similarity is to find the strength of the semantic relatedness or the semantic connections between textual terms. The taxonomic proximity between terms measures the degree of overlapping between contextual word vectors using Information Content (IC) based measures. To reduce computational complexity, the semantic relatedness is applied within an ordered window to find relevant terms between adjacent terms in the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Retrieval Framework</head><p>To build our retrieval framework based on the approaches we described above, we use the YTEX (Yale cTAKES extensions) system for computing the semantic IC-based measures and the cTAKES (clinical Text Analysis and Knowledge Extraction) as a NLP system based on the Unstructured Information Management Architecture (UIMA) that combines rule-based and machine-learning. The cTAKES system uses the OpenNLP Maximum Entropy package for sentence detection, tokenization (words), part-of-speech (POS) tagging. It performs the name entity recognition of biomedical from the Unified Medical Language system (UMLS) Metathesaurus, and other biomedical source such as Systematized Nomenclature of Medicine, Clinical Terms (SNOMED CT).</p><p>To classify the medical articles, our system used the YTEX semantic similarity to identify and disambiguate medical terms before storing the annotations on the documents in the relational database, and indexing relevant medical concepts that can facilitate the topic case query matching. For each case-based query, we first executed the NLP pipeline to extract relevant medical concepts and gen- erate an SQL expression by combining the concepts using logical OR operator (meaning all of the concepts to be optional). To limit the number of query results and select the most relevant annotations, we define the weight measures for sorting and ranking them based on the cosine distance between medical concept vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental Results</head><p>Fig. <ref type="figure" coords="14,154.59,452.20,4.98,8.74" target="#fig_6">7</ref> describes the processing flow of the system. The data processing splits the whole article file into multiple individual files in order to distribute them across multiple nodes. Each article is stored in the database as an annotation including its most relevant medical concepts. The case-based NLP pipeline processes each case-based file to find relevant medical concepts. Finally, the search engine composes a SQL logical expression and ranks the result set retrieved from the annotation database. Due to time constraint, we only submitted one run for case-based retrieval task, shown in Table <ref type="table" coords="14,233.85,547.84,3.87,8.74">5</ref>. First, we experimented with the semantic similarity approach, and found good correlation among medical concepts within the text corpus with appropriate senses. However, by matching only the medical concepts, the results are not as good as could be if we had used additional lexical databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Conclusion</head><p>The IC-based measures applied to adjacent words in a defined text window improves the semantic relatedness performance and is less expensive than computing all the word-pairs in the corpus. However, we also observed low system performance when the SQL expressions are complex and the number of concepts is high. In the future, we would like to improve the semantic similarity methods by incorporating the medical concepts with the lexical semantic analysis. We also want to have a better matching measures to improve the accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,134.77,245.48,345.83,7.89;2,163.11,116.83,289.13,113.88"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Number of visual examples per category in each dataset used for experiments.</figDesc><graphic coords="2,163.11,116.83,289.13,113.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,134.77,394.92,345.83,7.89;5,134.77,405.91,249.78,7.86;5,156.11,224.25,303.15,155.90"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Multiclass SVM fusion strategies: (a) early fusion, (b) kernel fusion and (c) late fusion. The fusion element in the pipeline is indicated in gray.</figDesc><graphic coords="5,156.11,224.25,303.15,155.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,134.77,290.21,345.83,7.89;8,134.77,301.20,51.21,7.86;8,137.60,116.83,340.18,158.61"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparing the performance using LibSVM and Liblinear with our kernel approximation.</figDesc><graphic coords="8,137.60,116.83,340.18,158.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,134.77,307.35,345.83,7.89;9,134.77,318.34,345.83,7.86;9,134.77,329.29,345.82,7.86;9,134.77,340.25,345.82,7.86;9,134.77,351.21,345.83,7.86;9,134.77,362.17,345.82,7.86;9,134.77,373.13,345.82,7.86;9,134.77,384.09,345.82,7.86;9,134.77,395.05,345.82,7.86;9,134.77,406.01,345.82,7.86;9,134.77,416.97,240.61,7.86;9,190.07,116.84,235.22,175.74"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. IBM Multimedia Analytics and Retrieval System (IMARS) Flowchart. Training data is seperated into a Learning and Validation partition, used for model learning and model selection, respectively. Within the Learning partition, data is segmented across features and data subsamples. These segments are referred to as "bags." Within the Validation partition, data is only segmented across features. Unit Models are trained for each bag within the Learning partition using a 1-vs-All SVM. The performance of each Unit Model is assessed on the Validation data partition, and Unit Models are selected for inclusion in the Ensemble Model based on which models boost Ensemble Model performance on the Validation data the most. If no Validation partition is definied (0%), all Unit Models are fused into an Ensemble Model. Fusion is performed by weighted averaging, with weigts based on average precision.</figDesc><graphic coords="9,190.07,116.84,235.22,175.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="12,134.77,315.93,345.83,7.89;12,134.77,326.92,345.83,7.86;12,134.77,337.88,345.83,7.86;12,134.77,348.83,345.82,7.86;12,134.77,359.79,345.82,7.86;12,134.77,370.75,90.74,7.86;12,138.22,116.83,338.91,184.33"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Distribution of mean accuracy scores across all the submissions to ImageCLEF 2012. Runs are divided by type (visual only, textual only, mixed). IBM submitted only visual runs (in blue). IBM's runs achieved the top three visual only performance. IBM's top run (Multiclass SVM trained from extended data, Experiment 12, circled in red) performed even better than the mixed runs, thus achieving the best overall classification accuracy.</figDesc><graphic coords="12,138.22,116.83,338.91,184.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="13,134.77,256.89,345.83,7.89;13,134.77,267.88,345.83,7.86;13,134.77,278.83,345.83,7.86;13,134.77,289.79,345.82,7.86;13,134.77,300.75,345.83,7.86;13,134.77,311.71,345.83,7.86;13,134.77,322.67,345.83,7.86;13,134.77,333.63,345.82,7.86;13,134.77,344.59,57.03,7.86;13,152.06,116.83,311.24,125.29"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (a) IBM runs grouped according to training dataset, number of descriptors adopted, and comparison with top performing non-IBM submissions. IBM's run with kernel approximation fusion trained on the original (limited in size) ImageCLEF dataset performs comparably to the top Mixed non-IBM run and largely better than other non-IBM visual only submissions. With additional training examples, IBM's system was able to significantly outperform all other runs, including Mixed ones. (b) Confusion matrix of the best IBM run (Experiment 12). "GSYS -System overview" resulted to be the hardest class to categorize, being quite often confused with "GFLO -Flowcharts".</figDesc><graphic coords="13,152.06,116.83,311.24,125.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="14,232.78,267.67,149.80,7.89;14,248.29,287.51,118.77,7.89;14,177.25,298.06,27.39,7.86;14,240.56,298.06,197.55,7.86;14,177.25,309.42,132.53,7.86;14,327.35,309.42,110.76,7.86;14,190.37,116.84,234.63,136.06"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Case Based System OverviewTable 5. Case-Based Results RunID Run Type MAP GM-MAP bpref P10 P30 ibm-case-based Textual 0.0484 0.0023 0.0439 0.0577 0.0449</figDesc><graphic coords="14,190.37,116.84,234.63,136.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,141.74,522.00,338.86,80.58"><head>•</head><label></label><figDesc>Dataset 1: The original ImageCLEF2012 training dataset. • Dataset 2: A dataset augmented with up to 100 additional examples per category. Augmenting data was collected from Bing Image Search, a Cornell University Vision and Image Analysis Group &amp; International Early Lung Cancer Action Program (VIA/I-ELCAP) Public CT datase 1 , The Cancer Imaging Archives (TCIA) 2 , Image Retrieval in Medical Applications (IRMA)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,134.77,237.18,345.83,106.19"><head>Table 1 .</head><label>1</label><figDesc>Comparing the computational cost of kernelized SVM and linear approximation.</figDesc><table coords="7,152.39,266.79,261.01,43.30"><row><cell></cell><cell cols="2">Training</cell><cell>Testing</cell></row><row><cell>Kernel SVM</cell><cell>O(N 2</cell><cell>m dm)</cell><cell>O(T N</cell></row></table><note coords="7,405.22,304.85,6.54,5.24;7,413.79,300.07,15.39,7.86;7,149.24,333.34,83.41,7.86;7,244.88,338.13,6.54,5.24;7,253.46,333.34,44.88,7.86;7,310.56,338.13,6.54,5.24;7,319.14,333.34,40.33,7.86;7,372.00,338.13,6.54,5.24;7,380.58,333.34,42.89,7.86;7,436.00,338.13,6.54,5.24;7,444.58,333.34,15.39,7.86"><p><p>m dm)</p>Linear approx O(αN m dm) + O(N m dm) O(αT m dm) + O(T m dm)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,136.16,492.06,343.74,72.50"><head>Table 2 .</head><label>2</label><figDesc>Comparing the kernel matrix approximation using our method.</figDesc><table coords="7,136.16,512.46,343.74,52.10"><row><cell cols="5">Number of samples Dim. Dim. of projection Speed up (x times) Approx. error (ratio)</cell></row><row><cell>100</cell><cell>100</cell><cell>700</cell><cell>14.83</cell><cell>0.0027</cell></row><row><cell>500</cell><cell>100</cell><cell>700</cell><cell>28.19</cell><cell>0.0026</cell></row><row><cell>1000</cell><cell>100</cell><cell>700</cell><cell>76.69</cell><cell>0.0026</cell></row><row><cell>2000</cell><cell>100</cell><cell>700</cell><cell>118.41</cell><cell>0.0026</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,136.16,436.24,359.85,141.82"><head>Table 3 .</head><label>3</label><figDesc>Multiclass</figDesc><table coords="9,293.63,436.26,112.19,7.86"><row><cell>Accuracy of 1-vs-ALL SVM</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="11,158.75,116.91,297.86,140.64"><head>Table 4 .</head><label>4</label><figDesc>Multiclass SVM Experiments with Mean Accuracy performance</figDesc><table coords="11,192.17,137.71,231.03,119.84"><row><cell cols="6">Exp. Fusion Aggregation Feature Set Dataset Mean ACC</cell></row><row><cell>12</cell><cell>Early</cell><cell>-</cell><cell>2</cell><cell>2</cell><cell>69.7%</cell></row><row><cell>13</cell><cell>Early</cell><cell>-</cell><cell>1</cell><cell>1</cell><cell>66.0%</cell></row><row><cell cols="2">14 Kernel</cell><cell>Avg</cell><cell>2</cell><cell>2</cell><cell>69.7%</cell></row><row><cell cols="2">15 Kernel</cell><cell>Avg</cell><cell>1</cell><cell>1</cell><cell>66.1%</cell></row><row><cell>16</cell><cell>Late</cell><cell>Avg</cell><cell>2</cell><cell>2</cell><cell>68.2%</cell></row><row><cell>17</cell><cell>Late</cell><cell>Prod</cell><cell>2</cell><cell>2</cell><cell>67.8%</cell></row><row><cell>18</cell><cell>Late</cell><cell>Max</cell><cell>2</cell><cell>2</cell><cell>65.2%</cell></row><row><cell>19</cell><cell>Late</cell><cell>Avg</cell><cell>1</cell><cell>1</cell><cell>62.8%</cell></row><row><cell>20</cell><cell>Late</cell><cell>Prod</cell><cell>1</cell><cell>1</cell><cell>62.2%</cell></row><row><cell>21</cell><cell>Late</cell><cell>Max</cell><cell>1</cell><cell>1</cell><cell>57.9%</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>research were obtained from The Cancer Imaging Archive (http://cancerimagingarchive.net/) sponsored by the <rs type="programName">Cancer Imaging Program</rs>, <rs type="funder">DCTD/NCI/NIH</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3kMsBsb">
					<orgName type="program" subtype="full">Cancer Imaging Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="15,138.35,206.81,342.24,7.86;15,146.91,217.77,225.54,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="15,187.93,206.81,232.22,7.86">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,428.35,206.81,52.24,7.86;15,146.91,217.77,111.43,7.86">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">91110</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,138.35,228.23,342.24,7.86;15,146.91,239.19,333.68,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,296.35,228.23,184.24,7.86;15,146.91,239.19,146.28,7.86">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName coords=""><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,299.64,239.19,87.98,7.86">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,138.35,249.65,342.24,7.86;15,146.91,260.60,147.45,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,322.54,249.65,158.04,7.86;15,146.91,260.60,18.84,7.86">Face recognition with local binary patterns</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,172.59,260.60,23.24,7.86">ECCV</title>
		<imprint>
			<biblScope unit="page" from="469" to="481" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,138.35,271.06,342.24,7.86;15,146.91,282.02,333.68,7.86;15,146.91,292.98,61.94,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,451.03,271.06,25.34,7.86;15,146.91,282.02,136.76,7.86">Fast Discrete Curvelet Transforms</title>
		<author>
			<persName coords=""><forename type="first">Emmanuel</forename><surname>Cands</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurent</forename><surname>Demanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ying</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,291.09,282.02,144.37,7.86">Multiscale Modeling and Simulation</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="861" to="899" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Lexing</note>
</biblStruct>

<biblStruct coords="15,138.35,303.44,342.24,7.86;15,146.91,314.40,333.68,7.86;15,146.91,325.35,333.68,7.86;15,146.91,336.31,315.93,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,305.55,314.40,175.04,7.86;15,146.91,325.35,333.68,7.86;15,146.91,336.31,222.21,7.86">Development of a digital image database for chest radiographs with and without a lung nodule: Receiver operating characteristic analysis of radiologists&apos; detection of pulmonary nodules</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shiraishi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Katsuragawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ikezoe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Komatsu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kodera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Doi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,376.32,336.31,18.43,7.86">AJR</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page" from="71" to="74" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,138.35,346.77,342.25,7.86;15,146.91,357.73,333.68,7.86;15,146.91,368.69,67.83,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="15,384.08,346.77,96.51,7.86;15,146.91,357.73,304.66,7.86">Large-Scale Multimedia Semantic Concept Modeling using Robust Subspace Bagging and MapReduce</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">O</forename><surname>Fleury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Merler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,458.84,357.73,21.75,7.86;15,146.91,368.69,46.34,7.86">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,138.35,379.15,342.24,7.86;15,146.91,390.10,133.88,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="15,273.44,379.15,202.78,7.86">Identification of Common Molecular Subsequences</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">F</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Waterman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,146.91,390.10,50.43,7.86">J. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page" from="195" to="197" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,138.35,400.56,342.24,7.86;15,146.91,411.52,333.67,7.86;15,146.91,422.48,333.68,7.86;15,146.91,433.44,71.71,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="15,292.66,400.56,187.93,7.86;15,146.91,411.52,223.35,7.86">ZIB Structure Prediction Pipeline: Composing a Complex Biological Workflow through Web Services</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">C</forename><surname>Ehrlich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Steinke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,247.29,422.48,85.17,7.86">Euro-Par 2006. LNCS</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Nagel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Walter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Lehner</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4128</biblScope>
			<biblScope unit="page" from="1148" to="1158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,138.35,443.89,342.24,7.86;15,146.91,454.85,166.09,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="15,250.23,443.89,226.46,7.86">The Grid: Blueprint for a New Computing Infrastructure</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Kesselman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,465.31,337.98,7.86;15,146.91,476.27,333.68,7.86;15,146.91,487.23,327.67,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="15,377.52,465.31,103.08,7.86;15,146.91,476.27,130.75,7.86">Grid Information Services for Distributed Resource Sharing</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Czajkowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Kesselman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,298.52,476.27,182.07,7.86;15,146.91,487.23,145.36,7.86">10th IEEE International Symposium on High Performance Distributed Computing</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,497.69,337.97,7.86;15,146.91,508.64,333.68,7.86;15,146.91,519.60,106.27,7.86" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="15,334.25,497.69,146.34,7.86;15,146.91,508.64,256.74,7.86">The Physiology of the Grid: an Open Grid Services Architecture for Distributed Systems Integration</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Kesselman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tuecke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Global Grid Forum</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="15,142.62,540.52,337.97,7.86;15,146.91,551.48,275.39,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="15,263.04,540.52,212.68,7.86">Efficient Additive Kernels via Explicit Feature Maps</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,146.91,551.48,223.56,7.86">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,561.93,337.98,7.86;15,146.91,572.89,242.57,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,259.13,561.93,183.96,7.86">Some results on tchebychefan spline functions</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kimeldorf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,450.24,561.93,30.35,7.86;15,146.91,572.89,174.48,7.86">Journal of Mathematical Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="82" to="95" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,583.35,337.98,7.86;15,146.91,594.31,300.53,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="15,331.05,583.35,149.54,7.86;15,146.91,594.31,101.42,7.86">Heterogeneous Feature Machines for Visual Recognition IEEE</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,251.40,594.31,169.79,7.86">Proc. Int&apos;l Conf. Computer Vision (ICCV)</title>
		<meeting>Int&apos;l Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,604.77,337.97,7.86;15,146.91,615.72,175.18,7.86" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="15,265.81,604.77,210.66,7.86">Large-scale Linear Support Vector Regression</title>
		<author>
			<persName coords=""><forename type="first">C.-H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/cjlin/liblinear/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.62,626.18,337.98,7.86;15,146.91,637.14,333.68,7.86;15,146.91,648.10,48.13,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="15,320.21,626.18,160.38,7.86;15,146.91,637.14,34.86,7.86">LIBSVM: A library for support vector machines</title>
		<author>
			<persName coords=""><forename type="first">Chih-Chung And</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,190.59,637.14,238.71,7.86">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
