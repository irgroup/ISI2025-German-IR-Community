<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,217.92,116.80,33.76,12.93;1,274.92,116.80,122.42,12.93">IMU</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,175.44,154.05,59.05,9.96"><forename type="first">Xueliang</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Inner Mongolia University</orgName>
								<address>
									<postCode>010021</postCode>
									<settlement>Hohhot</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,245.04,154.05,34.81,9.96"><forename type="first">Wei</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Inner Mongolia University</orgName>
								<address>
									<postCode>010021</postCode>
									<settlement>Hohhot</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,290.64,154.05,60.54,9.96"><forename type="first">Gao</forename><surname>Guanglai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Inner Mongolia University</orgName>
								<address>
									<postCode>010021</postCode>
									<settlement>Hohhot</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,351.24,153.01,8.16,6.33;1,385.20,154.05,39.73,9.96"><forename type="first">Qianqian</forename><surname>⋆⋆</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Inner Mongolia University</orgName>
								<address>
									<postCode>010021</postCode>
									<settlement>Hohhot</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,428.16,154.05,11.77,9.96"><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Inner Mongolia University</orgName>
								<address>
									<postCode>010021</postCode>
									<settlement>Hohhot</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,217.92,116.80,33.76,12.93;1,274.92,116.80,122.42,12.93">IMU</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">08B1CBDADB3A890C6A5E1C8DF81A9670</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Concept Annotation</term>
					<term>Concept Retrieval</term>
					<term>User Tags</term>
					<term>Wiki Expansion</term>
					<term>Maximum Conditional Probability</term>
					<term>Language Modeling</term>
					<term>Bagof-Visual Words</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inner Mongolia University have participated the Visual concept detection, annotation, and retrieval using Flickr photos task of ImageCLEF for the first time in 2012. We have conducted experiments and submitted results for both the Concept Annotation and the Conceptbased Retrieval subtasks. This paper describes the methods we have adopted and the analysis of the results for the two subtasks. We focus our attention mainly on the user's tag since we believe that user annotation provides strong semantic information which can be used to accurately determine the presence or absence of each concept and the relevance level between the images and queries. For the Concept Annotation subtask, we use only a simple statistical method that scores the confidence of the presence of each concept by the maximum conditional probability of the concept between the different given tags. For the Concept-based Retrieval task, we adopted the language modeling approach which has been widely used in text information retrieval field. Official evaluations show that the performance of our method is competitive. We rank in the middle of the pack for the Concept Annotation subtask with the best run's MiAP equal 0.2441. For the Concept-based Retrieval subtask, we rank at the top with the best run's MnAP equal 0.0933. Beside the main submissions, we also submit two visual runs, although no very good, with the MiAP for Concept Annotation is 0.0819 and the MnAP for Concept Retrieval is 0.0045. As a whole, the results confirm that although the methods we have adopted are simple, the performances we have achieved are satisfied.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we will describe the approaches we have adopted to accomplish the "Visual concept detection, annotation, and retrieval using Flickr photos" task for ImageCLEF 2012. We have participated both the Concept Annotation subtask and the Concept-based Retrieval subtask and submitted 15 runs in total. The official evaluation shows that we rank in the middle pack in the Concept Annotation subtask and get the highest rank in the Concept-based Retrieval subtask <ref type="bibr" coords="2,164.31,167.13,12.67,9.96" target="#b0">[1]</ref>. We base our methods mainly on User Tags, both for the Concept Annotation and for the Concept-based Retrieval subtask. The main reason for choosing User Tag as feature is that we believe that users generally annotate an image with the words that have strong relationships to its meaning. This semantic information can then be used to determine the contents of the image accurately. In all, we used statistical methods to address both the two subtasks. For the annotation subtask, we use the training set to estimate a conditional probability distribution and use the probability of the most supportive tag as the confidence of the presence of a concept in an image. For the retrieval subtask, we construct language models for the tags of each image and take the probability of the query being generated by the tags model as ranking score. Beside the main methods, we also test some visual feature based method.</p><p>The rest of the paper is organized as follows: in section 2 we firstly discuss the method we have used to accomplish the Concept Annotation subtask, including the experiments and the results achieved. Then in section 3, we similarly discuss the method for Concept Retrieval, again, experiments and result analysis are included. Finally, we conclude our work and shed lights on the future work in section 4 and 5 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Concept-based Annotation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">User Tag based Method for Annotation</head><p>We have adopted a very simple statistical method for the Concept-based Annotation subtask here. That is: we just calculate the conditional probability of the presence of the concept given the tags of the image and then taking the maximum probability among the different tags as the confidence. This can be shown by <ref type="bibr" coords="2,178.80,494.61,11.61,9.96" target="#b0">(1)</ref>:</p><p>Conf idence(C) = max</p><formula xml:id="formula_0" coords="2,314.88,506.73,165.75,15.25">T ag P (C|T ag) ,<label>(1)</label></formula><p>where C denotes the concept being considered; Conf idence(C) is the confidence of the presence of concept C. The flowchart of our Tag-based Concept Annotation system is illustrated in Fig. <ref type="figure" coords="2,155.16,565.89,3.90,9.96">1</ref>.</p><p>We use the training set which has been released by CLEF organization to estimate the conditional probability distribution. Since the number of tags for each image is relatively small (6.34 on average according to our statistics), we think that it's reasonable to expand them first <ref type="bibr" coords="2,310.04,613.77,10.20,9.96" target="#b1">[2]</ref>. We use the Official INEX 09 collection <ref type="bibr" coords="2,150.60,625.77,11.88,9.96" target="#b2">[3]</ref> which contains about 2640000 articles to perform User Tag expansion. 1  More specifically, we first use the tags for each image as query and the INEX corpora as document collection to perform retrieval and then extend the tags by using the top 15 words among the returned top 200 documents. We test our annotation approach on the officially released testing set, which contains 10000 images in total. The collections we used for our Tag-basd annotation approach are listed in Tab. 1. We use Indri <ref type="bibr" coords="3,204.15,465.45,11.97,9.96" target="#b3">[4]</ref> in the Lemur Toolkit<ref type="foot" coords="3,311.04,464.56,3.97,5.59" target="#foot_0">2</ref> to perform retrieval. Indri is a search engine which has been widely used in the Information Retrieval field. We will mention it again in the Concept-based Retrieval section (section 3). The parameters set in Indri for the Tag-based Annotation are listed in Tab. 2.</p><p>The experimental results are summarized in Tab. Comparing the first two results Max CondProb and WE Train Max CondProb in Tab. 3, we can see that it really reaps the benefit of the Wiki Expansion greatly (more than 1.0 percent increase for the MiAP matric).  The huge drop from the performance of WE Train Max CondProb to that of WE Train Test Max CondProb indicates that it's not reasonable to expand the tags for the images to be annotated. We can explain this as that the expanded tags are more likely to drift away to the concepts that are not in fact exist in the given image.</p><p>Intuition tells us that every image should contain some concepts in general, otherwise it will not be chosen for sharing. This means that even though the absolute probability of a concept may be low, we should still have great confidence that the concept is present if it has a relatively higher probability than the other concepts. This assumption can be confirmed by comparing WE Train Test Max CondProb to the run with the probability value normalized across the concepts (WE Train Test Norm Max CondProb). The great improvement in performance shows that normalization plays an important role in transforming the conditional probability to the confidence of the concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual Feature based Method for Annotation</head><p>Extraction of Visual Features We extracted three features that are mostly considered in the literatures we found (i.e., Color Histograms, Fuzzy-Color-and-Texture-Histogram (FCTH) and Bag-of-Visual Words).</p><p>Color Histograms are among the most basic approaches and are widely used in image retrieval. The color space is partitioned and for each partition the pixels with their color within this range are counted, resulting in a representation of the relative frequencies of the colors. We use the RGB color space for the histograms <ref type="bibr" coords="4,178.13,656.49,13.01,9.96" target="#b4">[5]</ref>. And we use the Jensen-Shannon divergence (JSD) as shown in <ref type="bibr" coords="5,134.76,119.37,12.75,9.96" target="#b1">(2)</ref> to compute the distance:</p><formula xml:id="formula_1" coords="5,168.84,143.89,311.79,30.45">d JSD (H, H ′ ) = M m=1 H m log 2H m H m + H ′ m + H ′ m log 2H ′ m H m + H ′ m ,<label>(2)</label></formula><p>where H and H' are the histograms to be compared. Fuzzy Color and Texture Histogram (FCTH) is appropriate for accurately retrieving images even in distortion cases such as deformations, noise and smoothing. FCTH is a low level descriptor that contains both quantized histogram color and texture information <ref type="bibr" coords="5,235.87,236.25,12.82,9.96" target="#b5">[6]</ref>. For the measurement of the distance of this feature between the images, we use Tanimoto coefficient as shown by (3):</p><formula xml:id="formula_2" coords="5,221.04,274.09,259.59,26.49">T ij = t(x i , x j ) = x T i x j x T i x i + x T j x j -x T i x j .<label>(3)</label></formula><p>We extracted the SIFT local features from harris-laplace region of interest detection. Each of these features is represented as a Bag-of-Visual Word. The visual words vocabulary is generated by adopting the K-means clustering algorithm on the features of the training set, which is implemented in the LIRE Toolkit<ref type="foot" coords="5,166.08,359.44,3.97,5.59" target="#foot_1">3</ref>  <ref type="bibr" coords="5,170.52,360.33,9.99,9.96" target="#b6">[7]</ref>. In our experiment, we take 10000 as the size of the Visual Word Vocabulary and adopt the same d JSD in (2) as the distance masure for clusting.</p><p>The three features described above are then used independently for k-NN classifier.</p><p>Classification Firstly, we use visual features mentioned above to build classifier. We use distance-weighted k-nearest neighbour (k-NN) approach to build our classifiers <ref type="bibr" coords="5,174.45,458.37,10.82,9.96" target="#b7">[8]</ref>. For each concept, we selected some positive images and a number of negative images. The distances, for feature f , from the test image T i to each of the k nearest positive or negative images are determined. Then we computed the similarity between the test image T i and Concept C as (4):</p><formula xml:id="formula_3" coords="5,204.72,518.46,275.91,27.05">Sim f (C, T i ) = p∈P (dist f (T i , p) + ε) -1 n∈N (dist f (T i , n) + ε) -1 + ε ,<label>(4)</label></formula><p>where P and N are the k-nearest positive and negative images for each concept and satisfy |Q| + |N | = k and ε is a small positive number to avoid division by zero.</p><p>However, the experiment results on the training set are not very good, so we submitted only one visual feature based run (Bag-of-Visual Words).</p><p>Tab. 4 lists the official evaluation result of our visual submission, which again confirms that by now our visual based method is not good. Language modeling is a formal probabilistic framework that has been widely used in the text retrieval field. The language modeling approach to text retrieval is to model the idea that a document is a good match to a query if the document model is likely to generate the query <ref type="bibr" coords="6,291.00,271.29,12.60,9.96" target="#b8">[9]</ref>. Formally, we want to estimate a model M d for each document d and rank the documents for a query Q according to the probability of Q being generated by M d . When further assuming beg-of-words modeling, we get (5):</p><formula xml:id="formula_4" coords="6,234.36,327.37,246.27,30.57">log p (Q|M d ) = m i=1 log p (Q i |M d ) .<label>(5)</label></formula><p>Observed that, in the official image collection, if an image is relevant to a query, its tags annotated by the user are more likely to occur in the query, we guess that the Concept Retrieval task can be addressed by building language models for the image tags and retrieving the images by ranking them according to the likelihood of generating the query (6):</p><formula xml:id="formula_5" coords="6,197.16,436.09,283.46,30.57">log p (Q|M ImageT ags ) = m i=1 log p (Q i |M ImageT ags ) .<label>(6)</label></formula><p>To test our assumption, we use the Indri Search Engine in the Lemur Toolkit which we have mentioned in section 2 to perform retrieval. Indri is a search engine that implements the language modeling approach under the Bayes Inference Network Framework <ref type="bibr" coords="6,250.20,512.73,14.69,9.96" target="#b9">[10]</ref>. The Tag models are smoothed by the Dirichlet Smoothing <ref type="bibr" coords="6,177.96,524.73,19.20,9.96" target="#b10">[11]</ref> method. For each topic, we retrieve 1000 images. A total of 418 stop words from the standard InQuery <ref type="bibr" coords="6,297.27,536.61,18.81,9.96" target="#b11">[12]</ref> stoplist are removed from the queries. Pseudo Relevance Feedback (PRF) <ref type="bibr" coords="6,285.66,548.61,14.58,9.96" target="#b8">[9]</ref> is adopted for both Tag Expansion and Concept Retrieval with fbTerms=15 and fbDocs=200. The detailed parameters set in Indri for Tag-based Retrieval are listed in Tab. 5.</p><p>In Tab. 6, we list the collections used to perform our Tag-based retrieval experiments and illustrate the flowchart of our system in Fig. <ref type="figure" coords="6,406.20,596.49,3.90,9.96">2</ref>.</p><p>In our experiments, we have tested different methods to form the queries for language modeling based retrieval and the results we have achieved are listed in Tab. 7 and Tab. 8.</p><p>In Tab. 7, Title means that the queries are formulated by just using the terms in the &lt;title&gt; field of the topic file queries.xml which has been officially released in the test set. Image(all 3) denotes that the queries are constructed by using all the three images' tags, i.e., the &lt;image&gt; fields in the topic file; Image(first not null) refers to the query construction method that only the first image's tags in the topic file are used. If the first image's tag set is empty or the tags cannot return any result, use the next one's tags, and so forth. Title&amp;Image(all 3) means that the queries are constructed by combining the title portion and tags associated with all the three images in the topic file. Since the number of tags associated with each image is relatively small (6 on average according to our statistics), we think it would be helpful to extend the tags for the images to perform retrieval. We perform the tag expansion similarly as in section 2 and the results are listed in Tab. 8, where runs with the prefix WE denote the Wiki expanded version of the corresponding runs.</p><p>Fig. <ref type="figure" coords="7,169.68,498.33,4.98,9.96" target="#fig_0">3</ref> illustrates the comparision of the results in Tab. 7 with that in Tab. 8. We can see that tag expansion really improve the performance greatly (with only one exception that for Image(all 3), which decrease about 3 percent on MnAP matric). We explain this as follows: the tags for the images may not be so accurate as that in the title field. So the expanded terms may drift away seriously. But if the title terms is also present, the expanding precess will be directed by them and the performance will increase finally as shown by the results of Title&amp;Image(all 3) V.S. WE Title&amp;Image(all 3).</p><p>From Tab. 7 and Tab. 8, we can see that using the title filed as query perform better than using the tags of the images in the image fields, both for the original tags and for the Wiki Expanded ones. This means that the title field in the topic has more strong descriptive ability than the tags of the images. We can also see that when combining the title part with the image part to form the queries, the retrieval performance can always be greatly improved. This phenomenon is in accordance with our intuition that the more information provided, the better we can determine the user's Information Need.</p><p>We also have performed experiment to test the combination method at result level, which is denoted by WE Combine Title&amp;Image(all 3) at Result Level. To do this, we first generate the ranked Image Lists (WE Title and WE Image(all 3)) by the two methods independently. After normalizing the score for each Image Im to the same scale (0 -1), we re-rank the documents according to equation <ref type="bibr" coords="8,134.76,551.49,12.75,9.96" target="#b6">(7)</ref>  <ref type="bibr" coords="8,150.84,551.49,14.69,9.96" target="#b12">[13]</ref>: score T+I (Im) = w T * norm score T (Im) + w I * norm score I (Im) , <ref type="bibr" coords="8,467.88,578.73,12.75,9.96" target="#b6">(7)</ref> where w T and w I are the combination parameters for Title and Image field whose ratio is 1 : 1 for simplicity in our experiment. More reasonable ratio can be tested in the future.</p><p>Comparing the result to that at query level (WE Title&amp;Image(all 3)), we can see that they achieve similar performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Visual Feature based Retrieval Method</head><p>Extraction of Visual Features For visual feature extraction, we take the same method as that for visual concept annotation. So we omit their description here to reduce the lenght of the paper. Please refer to section 2 for more detail.</p><p>Classification We use the weighted distance from the query images in the topic file to the image being considered as the ranking score, which can be shown by <ref type="bibr" coords="9,134.76,490.65,11.61,9.96" target="#b7">(8)</ref>:</p><formula xml:id="formula_6" coords="9,206.52,501.13,274.11,30.57">RankingScore f (Q, T i ) = n j=1 w j dist f (T i , Q j ) ,<label>(8)</label></formula><p>where Q denotes the query and T i denotes the test image being considered; Q j means the j th image that belongs to the &lt;image&gt; field in the queries.xml file for query Q and n is the totoal number of &lt;image&gt; fields for query Q.</p><p>Tab. 9 lists the result of our visual based retrieval submission, which shows that by now our method does not achieve good performance either.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we described the experiments we have performed for the "Visual concept detection, annotation, and retrieval using Flickr photos" task in detail. We based our methods mainly on the Tags annotated by the users since we believe that there is strong relationship between the user's tag and the presence of a concept for annotation and between the user's tag and the query for retrieval.</p><p>Official evaluation show that we achieved satisfied results for the User Tag based methods. Beside the main submission, we also perform some initial visual feature based experiments. However, the results we can achieved by now are not very good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Future Works</head><p>Since this is the first time we participated the ImageCLEF task, we just did some initial work. More detailed experiments should be performed in the future. For example, for the Concept Annotation subtask, we didn't consider the relationships between different concepts. In reality, there are correlations between different concepts, such as the probability of the presence of timeofday day is usually low given the presence of timeofday night whereas the probability of view outdoor will be high given the presence of the concept flora tree. For the Concept-based Retrieval subtask, we just applied the traditional Language Modeling approach to the tag modeling application but did not take the specific characteristic of image tags into consideration, like that the terms in the documents in text retrieval are sufficient in general whereas the amount of tags for each image are relatively small, even after being expanded. More refined modification should be made to address these problems. We will do all these works in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="9,181.80,357.84,251.71,8.96"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Tag-based Retrieval with and without Wiki Expansion</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,137.52,656.14,265.83,9.86"><head>Table 1 .</head><label>1</label><figDesc>Collections for Tag-based Concept Annotation</figDesc><table coords="2,137.52,656.14,265.83,9.86"><row><cell>1 http://www.mpi-inf.mpg.de/departments/d5/software/inex/</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,187.44,116.28,240.43,105.32"><head>Table 2 .</head><label>2</label><figDesc>Parameters set in Indri for Tag-based Annotation</figDesc><table coords="4,198.96,139.88,217.36,81.71"><row><cell>Parameter Name</cell><cell>Parameter Value</cell></row><row><cell>Score Function</cell><cell>KL-divergence</cell></row><row><cell>Smoothing Method</cell><cell>Dirichlet with =2500</cell></row><row><cell>#doc Returned</cell><cell>1000 per query</cell></row><row><cell>Stop Words</cell><cell>418 for query</cell></row><row><cell cols="2">Feedback for Tag Expansion fbTerm=15, fbDocs=200</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,143.76,237.24,327.87,90.56"><head>Table 3 .</head><label>3</label><figDesc>Evaluation Results for Concept Annotation with User Tags as Features</figDesc><table coords="4,181.80,260.72,251.56,67.07"><row><cell>Result Name</cell><cell>MiAP GMiAP F-ex</cell></row><row><cell>Max CondProb</cell><cell>0.2241 0.1698 0.4128</cell></row><row><cell>WE Train Max CondProb</cell><cell>0.2368 0.1825 0.4685</cell></row><row><cell>WE Train Test Max CondProb</cell><cell>0.2174 0.1665 0.4535</cell></row><row><cell cols="2">WE Train Test Norm Max CondProb 0.2441 0.1917 0.4535</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,134.76,116.28,343.95,108.33"><head>Table 4 .</head><label>4</label><figDesc>Evaluation Results for Concept Annotation with Visual Words as Features</figDesc><table coords="6,134.76,139.88,255.64,84.72"><row><cell>Result name MiAP GMiAP F-ex</cell></row><row><cell>BoV Annotation 0.0819 0.0387 0.0429</cell></row><row><cell>3 Concept Retrieval</cell></row><row><cell>3.1 Language Models for User Tag Retrieval</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,192.24,116.28,230.92,119.96"><head>Table 5 .</head><label>5</label><figDesc>Parameters set in Indri for Tag-based Retrieval</figDesc><table coords="7,198.96,139.88,217.36,96.35"><row><cell>Parameter Name</cell><cell>Parameter Value</cell></row><row><cell>Score Function</cell><cell>KL-divergence</cell></row><row><cell>Smoothing Method</cell><cell>Dirichlet with =2500</cell></row><row><cell>#Doc Returned</cell><cell>1000 per query</cell></row><row><cell>Stop Words</cell><cell>418 for query</cell></row><row><cell cols="2">Feedback for Tag Expansion fbTerm=15, fbDocs=200</cell></row><row><cell>Feedback for Retrieval</cell><cell>fbTerm=15, fbDocs=200</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,162.12,255.48,291.18,61.28"><head>Table 6 .</head><label>6</label><figDesc>Collections for Tag-based Concept Retrieval</figDesc><table coords="7,162.12,278.96,291.18,37.79"><row><cell>Collection Name</cell><cell>Collection Scale</cell></row><row><cell cols="2">Testing set for Concept Retrieval of ImageCLEF2012 200000 images</cell></row><row><cell>Wiki INEX09</cell><cell>2640000 articles</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="8,140.64,125.80,334.06,306.40"><head>Table 7 .</head><label>7</label><figDesc>Flowchart of the Tag-based Retrieval System Concept Retrieval with User Tags as Features(Without Wiki Expansion)</figDesc><table coords="8,183.12,125.80,249.14,306.40"><row><cell>Wiki Expansion</cell><cell></cell><cell></cell></row><row><cell>Wiki INEX09</cell><cell>User Tags</cell><cell cols="2">Topic File</cell></row><row><cell>Indri Build Index</cell><cell>Indri Build Index</cell><cell cols="2">Query Construction</cell></row><row><cell>Wiki INEX09</cell><cell></cell><cell></cell></row><row><cell>Index</cell><cell>Tags Index</cell><cell cols="2">Queries</cell></row><row><cell cols="2">Expanded Tags</cell><cell></cell></row><row><cell></cell><cell>Indri Retrieval</cell><cell></cell></row><row><cell>Indri Retrieval</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Results</cell><cell></cell><cell>Results</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Combination</cell></row><row><cell>Fig. 2. Result name</cell><cell cols="3">MnAP AP@10 AP@20 AP@100</cell></row><row><cell>Title</cell><cell cols="2">0.0802 0.0136 0.0376</cell><cell>0.1651</cell></row><row><cell>Image(all 3)</cell><cell cols="2">0.0763 0.0123 0.0320</cell><cell>0.1439</cell></row><row><cell>Image(first not null)</cell><cell cols="2">0.0711 0.0135 0.0241</cell><cell>0.1255</cell></row><row><cell>Title&amp;Image(all 3)</cell><cell cols="2">0.0852 0.0137 0.0262</cell><cell>0.1635</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="10,147.36,116.88,320.62,116.72"><head>Table 8 .</head><label>8</label><figDesc>Concept Retrieval with User Tags as Features(With Wiki Expansion)</figDesc><table coords="10,164.76,140.84,285.86,92.75"><row><cell>Result name</cell><cell cols="2">MnAP AP@10 AP@20 AP@100</cell></row><row><cell>WE Title</cell><cell>0.0852 0.0187 0.0383</cell><cell>0.1721</cell></row><row><cell>WE Image(all 3)</cell><cell>0.0736 0.0119 0.0212</cell><cell>0.1414</cell></row><row><cell>WE Image(first not null)</cell><cell>0.0786 0.0133 0.0260</cell><cell>0.1311</cell></row><row><cell>WE Title&amp;Image(all 3)</cell><cell>0.0933 0.0187 0.0338</cell><cell>0.1715</cell></row><row><cell>WE Combine Title&amp;Image(all 3)</cell><cell>0.0799 0.0141 0.0372</cell><cell>0.1638</cell></row><row><cell>at Result Level</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="10,173.28,248.52,268.71,46.64"><head>Table 9 .</head><label>9</label><figDesc>Concept Retrieval with Bag-of-Visual Words as Features</figDesc><table coords="10,201.72,272.00,211.94,23.15"><row><cell cols="2">Result name MnAP AP@10 AP@20 AP@100</cell></row><row><cell>BoV Retrieval 0.0045 0.0030 0.0064</cell><cell>0.0316</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="3,144.72,657.74,126.39,8.27"><p>http://www.lemurproject.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="5,144.72,657.74,173.43,8.27"><p>http://www.semanticmetadata.net/lire/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. We would like to thank <rs type="person">Prof. Jian-Yun Nie</rs> of <rs type="affiliation">Université de Montréal</rs> for his invaluable suggestions.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="11,142.88,143.04,337.67,8.96;11,151.56,153.96,328.06,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,342.48,143.04,138.07,8.96;11,151.56,153.96,140.09,8.96">The CLEF 2012 Photo Annotation and Concept-based Retrieval Tasks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Liebetrau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,313.08,153.96,107.53,8.96">CLEF 2012 Working Notes</title>
		<meeting><address><addrLine>Rome</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.88,164.88,337.67,8.96;11,151.56,175.92,303.22,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,280.08,164.88,200.47,8.96;11,151.56,175.92,66.12,8.96">A Survey of Automatic Query Expansion in Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Romano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,224.64,175.92,134.72,8.96">ACM Computing Surveys(CSUR)</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="50" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.88,186.84,337.54,8.96;11,151.56,197.76,328.99,8.96;11,151.56,208.80,257.02,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,384.36,186.84,96.06,8.96;11,151.56,197.76,139.21,8.96">YAWN: A Semantically Annotated Wikipedia XML Corpus</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schenkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gjergji</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kasneci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,311.16,197.76,169.39,8.96;11,151.56,208.80,138.05,8.96">Proceedings of Datenbanksysteme in Business, Technologie and Web (BTW)</title>
		<meeting>Datenbanksysteme in Business, Technologie and Web (BTW)<address><addrLine>Aachen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="277" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.88,219.72,337.74,8.96;11,151.56,230.64,307.90,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,357.48,219.72,123.14,8.96;11,151.56,230.64,21.13,8.96">Indri at TREC 2005: Terabyte Track</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,194.04,230.64,124.31,8.96">14th Text Retrieval Conference</title>
		<meeting><address><addrLine>Gaithersburg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="175" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.88,241.56,337.76,8.96;11,151.56,252.60,248.02,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,298.32,241.56,182.32,8.96;11,151.56,252.60,43.67,8.96">Features for image retrieval: an experimental comparison</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,202.80,252.60,92.76,8.96">Informational Retrieval</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="77" to="107" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.88,263.51,337.60,8.96;11,151.56,274.44,329.11,8.96;11,151.56,285.48,328.96,8.96;11,151.56,296.39,123.94,8.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,302.76,263.51,177.72,8.96;11,151.56,274.44,208.08,8.96">FCTH: Fuzzy Color and Texture Histogram -A Low Level Feature for Accurate Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Boutalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,380.40,274.44,100.27,8.96;11,151.56,285.48,325.11,8.96">Proceedings of the Ninth International Workshop on Image Analysis for Multimedia Interactive Services</title>
		<meeting>the Ninth International Workshop on Image Analysis for Multimedia Interactive Services<address><addrLine>Klagenfurt</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="191" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.88,307.31,337.76,8.96;11,151.56,318.36,328.99,8.96;11,151.56,329.27,204.10,8.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,282.96,307.31,197.68,8.96;11,151.56,318.36,53.88,8.96">Lire: Lucene Image Retrieval An Extensible Java CBIR Library</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chatzichristofis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,225.72,318.36,254.83,8.96;11,151.56,329.27,28.59,8.96">Proceedings of the 16th ACM International Conference on Multimedia</title>
		<meeting>the 16th ACM International Conference on Multimedia<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1085" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.88,340.19,337.67,8.96;11,151.56,351.24,328.94,8.96;11,151.56,362.15,325.30,8.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,364.56,340.19,115.99,8.96;11,151.56,351.24,116.37,8.96">A comparative study of evidence combination strategies</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Yavlinsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pickering</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Heesch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,290.76,351.24,189.74,8.96;11,151.56,362.15,173.68,8.96">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.88,373.07,337.79,8.96;11,151.56,384.11,241.06,8.96" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<title level="m" coord="11,348.72,373.07,131.95,8.96;11,151.56,384.11,24.39,8.96">Introduction to Information Retrieval</title>
		<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.54,395.03,338.12,8.96;11,151.56,405.95,328.96,8.96;11,151.56,416.99,75.10,8.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,272.40,395.03,208.27,8.96;11,151.56,405.95,121.68,8.96">Combining the Language Model and Inference Network Approaches to Retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,281.28,405.95,166.59,8.96">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="735" to="750" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.54,427.91,338.00,8.96;11,151.56,438.83,329.08,8.96;11,151.56,449.87,101.62,8.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,261.12,427.91,219.43,8.96;11,151.56,438.83,114.56,8.96">A study of smoothing methods for language models applied to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,272.40,438.83,203.65,8.96">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="214" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.54,460.79,338.00,8.96;11,151.56,471.71,329.07,8.96;11,151.56,482.75,150.10,8.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,330.84,460.79,112.15,8.96">The inquery retrieval system</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Harding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,463.08,460.79,17.47,8.96;11,151.56,471.71,329.07,8.96;11,151.56,482.75,48.88,8.96">Proceedings of the Third International Conference on Database and Expert Systems Applications</title>
		<meeting>the Third International Conference on Database and Expert Systems Applications<address><addrLine>Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.54,493.67,338.12,8.96;11,151.56,504.59,328.87,8.96;11,151.56,515.63,303.70,8.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,207.72,493.67,198.72,8.96">Combining Approaches to Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,414.72,493.67,65.95,8.96;11,151.56,504.59,328.87,8.96;11,151.56,515.63,34.92,8.96">Advances in Information Retrieval: Recent Research from the Center for Intelligent Information Retrieval</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</editor>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1" to="36" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
