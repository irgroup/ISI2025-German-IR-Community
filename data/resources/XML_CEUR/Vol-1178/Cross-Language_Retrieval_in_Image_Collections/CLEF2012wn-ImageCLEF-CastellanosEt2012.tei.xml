<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,125.78,152.67,343.48,12.64;1,137.78,170.67,319.78,12.64">Using Visual Concept Features in a Multimodal Retrieval System for the Medical collection at ImageCLEF2012</title>
				<funder ref="#_AX5wAfw">
					<orgName type="full">Regional Government of Madrid under Research Network MA2VIRMR</orgName>
				</funder>
				<funder ref="#_MBuePqE">
					<orgName type="full">Spanish Government</orgName>
				</funder>
				<funder ref="#_Pbxpje5">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,146.78,210.18,58.21,8.96"><forename type="first">A</forename><surname>Castellanos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universidad Nacional de Educación a Distancia</orgName>
								<orgName type="institution" key="instit2">UNED</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,213.05,210.18,46.57,8.96"><forename type="first">J</forename><surname>Benavent</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universitat de València</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,267.65,210.18,49.92,8.96"><forename type="first">X</forename><surname>Benavent</surname></persName>
							<email>xaro.benavent@uv.es</email>
							<affiliation key="aff1">
								<orgName type="institution">Universitat de València</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,325.61,210.18,73.04,8.96"><forename type="first">A</forename><surname>García-Serrano</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universidad Nacional de Educación a Distancia</orgName>
								<orgName type="institution" key="instit2">UNED</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,406.73,210.18,38.60,8.96"><forename type="first">E</forename><surname>De Ves</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universitat de València</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,125.78,152.67,343.48,12.64;1,137.78,170.67,319.78,12.64">Using Visual Concept Features in a Multimodal Retrieval System for the Medical collection at ImageCLEF2012</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EB7B0338D8988893AE8950F1967BF32D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multimedia Retrieval</term>
					<term>Concept Features</term>
					<term>Low-level features</term>
					<term>Logistic regression relevance feedback</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The main goal of this paper is to present our experiments in the classification modality and in the ad-hoc image retrieval tasks with the Medical collection at ImageCLEF 2012 Campaign. This edition we focus on applying new strategies for both the textual and the visual subsystems included in our multimodal retrieval system. The visual subsystem has focus on extending the low-level features vector with concept features. These concept features have been calculated by means of a logistic regression model. The textual subsystem has focus on applying a query reformulation to remove general and domain stop-words, trying to produce a query with only medical-related terms. We have not obtained the results as good as obtained at the Photo annotation retrieval subtask using similar techniques. Therefore, a deep analysis for the Medical collection will be done.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we present our experiments in ImageCLEF 2011 Campaign at Medical Image retrieval task [¡Error! No se encuentra el origen de la referencia.1]. In this campaign, we participate in two sub-tasks of the Medical Retrieval Tasks: Image Modality Classification and Ad-hoc Image Retrieval. The work done in this edition is building on the knowledge acquired in previous participations both at Medical Retrieval Task <ref type="bibr" coords="1,177.26,576.23,11.72,8.96" target="#b5">[6]</ref> and at Wikipedia Retrieval Task <ref type="bibr" coords="1,331.27,576.23,11.51,8.96" target="#b2">[3,</ref><ref type="bibr" coords="1,342.78,576.23,11.51,8.96" target="#b9">10]</ref>, using multimodal retrieval approaches.</p><p>Regarding the textual retrieval subsystem, we apply partially the successful technique tested last year <ref type="bibr" coords="1,222.29,612.23,16.76,8.96" target="#b9">[10]</ref> (2nd in textual category). This is based on the preprocessing of the query in order to delete common and domain stopwords (i.e generic terms not related to medical domain like image, photo and so on). Unlike the work presented in <ref type="bibr" coords="1,177.08,648.26,15.47,8.96" target="#b9">[10]</ref>, in this year we have decided not to use the modality classification of the images. This is due to that the possible improvements are highly dependent of the query type and query content; as was shown in our in-depth analysis of the results of last year, presented in <ref type="bibr" coords="1,225.01,684.26,10.81,8.96" target="#b6">[7]</ref>.</p><p>Concerning to the visual retrieval subsystem it uses the low-level features for image retrieval. This low-level information although gives quite enough results depending on the visual information of the query is not able to reduce the "semantic gap" in a semantic complex query. Our proposal <ref type="bibr" coords="2,288.51,186.18,11.97,8.96" target="#b3">[4]</ref> is to generate concept features extracted from the low-level features to obtain the probability of the presence of each trained category. We call this new vector, the expanded low-level concept vector that is calculated for each image of the collection and also for the example images of the query to process the retrieval task. A model for each category is trained using a logistic regression <ref type="bibr" coords="2,168.93,246.18,15.46,8.96" target="#b11">[12]</ref>. We use these regression models to extract the concept features from the low-level features and construct the expanded concept features vector for the retrieval process.</p><p>It is our first participation at the classification task with four visual runs submitted. We have adapted our regression model to act as a classifier for the classification task. A model for each of the categories have been trained and tested.</p><p>Section 2 describes the visual approach based on a regression model acting as a classifier for the modality classification subtask. Section 3 explains our multimodal retrieval system use for the ad-hoc image-based retrieval subtask. After that section 4 shows the submitted runs and the results obtained for modality classification and retrieval. Finally, in section 5 we extract conclusions and outlines possible future research lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Modality classification</head><p>We train a logistic regression model <ref type="bibr" coords="2,277.49,435.21,16.76,8.96" target="#b11">[12]</ref> for each of the 31 categories given by the 2012 medical classification subtask. Each trained model predicts the probability that a given image belongs to a certain category.</p><p>The medical classification task gives to the participants a training set, , for each of the categories. Being the training image set for each category (the relevant images), and the set that not belong to a certain category (non relevant images). The logistic regression analysis calculates the probability for a given image to belong to a certain category. Each image of the training set, is represented by a K-dimensional low-level features vector { }. The relevance probability for a certain category for a given image will be represented as ( ). A logistic regression model can estimate these probabilities. Let us consider for a binary Y, and k explanatory variables , the model for (x) = P(Y=1 X ) (probability ) for the x values [ ] , where logit ((x))=ln((x) / (1-(x)). The model parameters are obtained by maximizing the likelihood estimator (MLE) of the parameter vector β by using an iterative method.</p><p>We have a major difficulty when having to adjust an overall regression model in which we take the whole set of variables into account because the number of selected images (the number of positive plus negative images, k) is typically smaller than the number of characteristics (k &lt; p). In this case the adjusted regression model has as many parameters as the amount of data and many relevant variables could be not considered. In order to solve this problem our proposal is to adjust different smaller regression models: each model considers only a subset of variables consisting of semantically related characteristics of the image. Consequently each sub-model will associate a different relevance probability to a given image x and we have to combine them in order to rank the database according to the image probability or image score (Si).</p><p>The explanatory variables to train the model are the visual lowlevel features based on color and texture information that are calculated by our group. We have a low-level features vector of 293 components divided by five different visual information families. ─ The granulometric distribution functions <ref type="bibr" coords="3,313.73,375.21,10.69,8.96" target="#b1">[2]</ref>, using the coefficients that result in fitting the distribution function with a B-spline basis. We calculated for two different structuring elements: horizontal and vertical segment. We have 31 components for granulometric distribution with horizontal segment and 31 components for vertical segment. ─ The Spatial Size Distribution <ref type="bibr" coords="3,269.45,435.21,11.69,8.96" target="#b1">[2]</ref> using a horizontal segment as structuring element. We have a 9 components vector for the spatial size distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Ad-hoc image-based retrieval subtask</head><p>The overall system includes three main subsystems: the TBIR (Text-Based Image Retrieval), the CBIR (Content-Based Image Retrieval), and the Fusion subsystem (see Fig. <ref type="figure" coords="3,143.28,528.23,3.62,8.96">2</ref>). Both the textual (TBIR) and the visual subsystem (CBIR) obtain a ranked list of images based on similarity scores (St and Si) for a given query. Firstly, TBIR uses the textual information from the annotations (metadata and articles) to obtain these scores (St). This textual pre-filtered list is then used by the CBIR sub-system. It extracts the visual information from the given example images of the query and generates a similarity score (Si). The fusion sub-system is in charge of merging these two lists of results, taking into account the scores and rankings, in order to obtain the final result list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Text-based Index and Retrieval</head><p>This module is in charge of the textual-based indexing and retrieval, using the text associated with each image in the collection.</p><p>In order to be able to manage the textual information of the collection, a preprocessing step is carried out, before of the indexing. Later, it has been carried out the indexing of the images for their subsequent retrieval. To indexing the collection, Solr 1 , a search platform from Lucene 2 project, is used. The retrieval process is done through Solr too. The result of this retrieval process is a normalized image list for each query. Below, is explained in more detail each of the different stages performed by TBIR module:   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FUSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Content-Based Information and Visual Retrieval</head><p>The work of the CBIR subsystem is based on three main stages: Extraction of the low-level and the concept features of the images, and the calculation of the similarity (Si) of each of the images to the image examples given by a query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Extraction of low-level features:</head><p>The first step in the CBIR system is to extract the visual low-level and the concept features for all the images of the database as well as from the example images given in each question. The low-level features we use are calculated by our group and give color and texture information about the images. These features are the same that we have used for the modality classification task (see section 2.1 for more detailed information).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Calculating the Concept features vector:</head><p>The regression models trained for each of the concepts gives for each image on the database and for the example query the probability of the presence of each concept ( ). With this probability information for each concept, we extend the low-level features vector to m components, being m the number of concepts trained. Each image on the database is described by the extended vector ( ) } .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Similarity Module:</head><p>The similarity module instead of using the classical distance method to calculate the similarity of each of the images of the database to the example images for a given topic uses our own logistic regression relevance algorithm to get the probability of an image belonging to the query set. The sub-models regressions are set to five features inside each features family that are the number of example images given for each topic (see more details of the regression method at section 2.1.). The relevant images are the example images, and the non-relevant images are randomly taken from outside the pre-textual filtered list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">The fusion sub-system</head><p>The fusion subsystem is in charge of merging the two score result lists from the TBIR and the CBIR subsystem. In the present work we use the product fusion algorithm (Si*St). The two results lists are fused together to combine the relevance scores of both textual and visually retrieved images (St and Si). Both subsystems will have the same importance for the resulting list: the final relevance of the images will be calculated using the product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Modality classification experiments</head><p>In this our first participation on the medical modality classification subtask, we have only participated with visual modality runs. Our objective for this edition has been to test the behavior of our logistic regression model for the classification task, and to adjust the parameters for the regression model explained at the section 2. The parameters to be defined to model each of the categories are:</p><p> The automatic election for the relevant and non-relevant images for the model to train each of the categories (positive and negative images).</p><p>The organization gives a training set for each of the categories being these training sets the relevant images for our logistic regression model. The number of images for each category differs from 5 images at the lower range (DSEC and DSEM categories) to 49 images at the highest range (COMP, DRCT and GGEL categories). The non-relevant images are the nearest N image to the centroid images of the set of images of the other categories different to the one being trained. The number of non-relevant images will be the double of the number of relevant images. We present two approaches for the number of relevant images to be used: for the first approach all available images for each given category are taken as relevant images (runs 1, 3 and 4), and for the second approach we limit the number of relevant images to a MAX number of images. The MAX number chosen is 30 because is the average low-level features components for each visual information family (run 2).  The different subgroups to adjust smaller regression models.</p><p>As it has been explained above the number of positive plus negative images, k (5 + 5*2 for the minimum set of training image category is smaller than the number of characteristics p (292 low-level featured vector) (k &lt; p). We present four different approaches to group the low-level features: a regression model for each family low-level vector (run1), a regression model for each 30 components (run2) being 30 images the number of relevant images, a regression model for the lowest number of relevant images given that for this collection is 5 images (run3), and an adaptive regression model strategy different for each category depending on the minimum number of given relevant images or to the minimum number of components for the low-level featured family vector (run4). For all runs, the different submodels are merged by the average function. Table <ref type="table" coords="8,162.12,385.17,4.98,8.96" target="#tab_2">3</ref> shows the results obtained at the ad-hoc image-based retrieval subtask by means of the MAP (Mean Average Precision), bpref (binary preference) and the precisions at the first 10 and 30 image retrieved (P@10 and P@30 respectively). The textual baseline has very poor results with a MAP of 0.0039. As the multimodal approach relies on the textual pre-filtered list, the multimodal runs do not outperform the textual baseline as we have already tested in other collections <ref type="bibr" coords="8,388.99,445.17,15.43,8.96" target="#b10">[11]</ref>. This low MAP is due to a low recall value that means that an important set of the relevant images are not selected by the textual system and then are not processed by the visual system. The visual approaches that re-rank the final score list using only the visual score Si, marked as visual at table 3, get better results by means of MAP and precision @10 than runs using both textual and visual scores St*Si, runs marked as mixed at table 3. This behavior is opposite as other results in which the multimodal approaches outperform the textual baseline <ref type="bibr" coords="8,226.25,529.19,16.76,8.96" target="#b10">[11]</ref> due to the performance of the textual system.</p><p>Analyzing the multimodal experiments, we can observe that runs using the expanded conceptual vector, runs UNED_UV_04 and UNED_UV_05 obtain better results by means of MAP (0.0040 and 0.0036 respectively) than runs that only use the low-level features vector, run UNED_UV_02 (0.0034). These results confirm our idea that the expanded concept vector adds information about the categorization of the image for the retrieval process. About using a unique vector for the expanded concept features vector or two vectors, we can not extract a definitive conclusion so that the results obtains by the two runs are very close, and can be also mask by the noise introduced by the textual prefiltered approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remarks and Future Work</head><p>The textual retrieval approach we have proposed this time, based on a query reformatted process, which focuses on the semantic of the queries by try to use only medical terms, has not obtained the expected results. We will analyze this bad performance of the textual retrieval process at the Medical collection, given that this technique was successfully tested last year at the Wikipedia collection <ref type="bibr" coords="9,417.27,225.18,16.91,8.96" target="#b9">[10]</ref> (2nd in textual category).</p><p>For the multimodal approaches presented for the ad-hoc image-based retrieval subtask, our combination of the textual pre-filtered list as input to the visual system does not outperform the textual baseline, as it has already been tested in other ImageClef collections, Wikipedia <ref type="bibr" coords="9,220.27,285.21,12.21,8.96" target="#b2">[3,</ref><ref type="bibr" coords="9,232.48,285.21,12.21,8.96" target="#b9">10]</ref> due to the fact of the performance of the textual approaches. Focusing on the visual system, the expanded concept vector presented outperforms the use of the low-level features vector in the Medical collection as in the Flickr photo subtask <ref type="bibr" coords="9,208.85,321.21,10.68,8.96" target="#b4">[5]</ref>.</p><p>The results obtained at the classification modality subtask suffered from the fact that our visual approach is a retrieval approach adapted for the classification modality task. Nevertheless, the regression model system proposed as a modality classifier will be analyzed query-by-query to improve its classification performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,124.70,266.72,308.71,9.06;3,136.10,278.87,334.48,9.06;3,147.38,290.97,322.92,8.96;3,147.38,302.97,180.47,8.96;3,136.10,314.87,333.91,9.06;3,147.38,326.97,323.03,8.96;3,147.38,338.97,323.31,8.96;3,147.38,350.97,68.18,8.96;3,124.70,362.99,269.98,9.06"><head></head><label></label><figDesc>Color information: We calculate global and local histograms of the image. ─ Global color: It is a feature vector of 30 components represents the color information of the complete image. Each of these components represents a bin on a HS (hue-saturation) histogram of size 10 x 3. ─ Local color: Local histograms have been calculated by dividing the images into four fragments of the same size. A bi-dimensional HS histogram with 12x4 bins is computed for each patch, being 48 components for each patch, and a total of 192 components.  Texture information: Two types of texture feature are computed:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,165.14,396.02,264.72,8.10"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. -System Overview for the ad-hoc content image retrieval subtask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,130.34,518.47,334.51,88.62"><head>Table 1 .</head><label>1</label><figDesc>-Detailed information and results of the submitted visual modality class. runs.</figDesc><table coords="6,293.45,535.81,168.62,16.92"><row><cell>Regression parameters Color features</cell><cell>Texture features</cell><cell>Results</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,124.70,209.99,335.54,157.08"><head>Table 3 .</head><label>3</label><figDesc>Results for the submitted experiments at the ad-hoc image-base retrieval subtask.</figDesc><table coords="8,124.70,227.24,333.05,139.83"><row><cell>Run</cell><cell>Modality</cell><cell>MAP</cell><cell>bpref</cell><cell>P@10</cell><cell>P@30</cell></row><row><cell>UNED_UV_01_TXT_EN</cell><cell>Textual</cell><cell>0.0039</cell><cell>0.0055</cell><cell>0.0091</cell><cell>0.0076</cell></row><row><cell>UNED_UV_02_IMG_LOW_FEATURES</cell><cell>Visual</cell><cell>0.0034</cell><cell>0.0114</cell><cell>0.0455</cell><cell>0.0273</cell></row><row><cell>UNED_UV_03_TXTIMG_LOW_FEATURES</cell><cell>Mixed</cell><cell>0.0015</cell><cell>0.0037</cell><cell>0.0045</cell><cell>0.0061</cell></row><row><cell>UNED_UV_04_IMG_LOW_FEAT_2VECT</cell><cell>Visual</cell><cell>0.0400</cell><cell>0.0104</cell><cell>0.0409</cell><cell>0.0258</cell></row><row><cell>UNED_UV_05_IMG_EXPAND_FEAT_1VEC</cell><cell>Visual</cell><cell>0.0036</cell><cell>0.0111</cell><cell>0.0455</cell><cell>0.0303</cell></row><row><cell>UNED_UV_06_TXTIMG_EXPAND_FEAT_2VECT</cell><cell>Mixed</cell><cell>0.0013</cell><cell>0.0034</cell><cell>0.0091</cell><cell>0.0045</cell></row><row><cell>UNED_UV_07_TXTIMG_EXPAND_FEAT_1VECT</cell><cell>Mixed</cell><cell>0.0015</cell><cell>0.0036</cell><cell>0.0045</cell><cell>0.0061</cell></row><row><cell>UNED_UV_08_IMG_CONCEPT_FEAT</cell><cell>Visual</cell><cell>0.0033</cell><cell>0.0104</cell><cell>0.0227</cell><cell>0.0197</cell></row><row><cell>UNED_UV_09_TXTIMG_CONCEPT_FEAT</cell><cell>Mixed</cell><cell>0.0021</cell><cell>0.0050</cell><cell>0.0091</cell><cell>0.0061</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work has been partially supported for <rs type="funder">Regional Government of Madrid under Research Network MA2VIRMR</rs> (<rs type="grantNumber">S2009/TIC-1542</rs>), for <rs type="funder">Spanish Government</rs> by project <rs type="projectName">BUSCAMEDIA</rs> (<rs type="grantNumber">CEN-20091026</rs>) and by project <rs type="grantNumber">MCYT TEC2009-12980</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_AX5wAfw">
					<idno type="grant-number">S2009/TIC-1542</idno>
				</org>
				<org type="funded-project" xml:id="_MBuePqE">
					<idno type="grant-number">CEN-20091026</idno>
					<orgName type="project" subtype="full">BUSCAMEDIA</orgName>
				</org>
				<org type="funding" xml:id="_Pbxpje5">
					<idno type="grant-number">MCYT TEC2009-12980</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table <ref type="table" coords="7,161.64,150.18,4.98,8.96">1</ref> shows the detail information of the submitted runs and the results obtained by means of the percentage of correctly image classified. Our results for the test set at classification task are much lower than the results we get at the training set. We must study the query-by-query results to determine how to improve the performance of the regression model as a classifier. Analyzing the four different tuning parameters for the regression method (see Table <ref type="table" coords="7,245.76,210.18,3.65,8.96">1</ref>), the one that better performs is the adaptive model to the minimum of the number of relevant images or the number of components of the vector family features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ad-hoc image-based retrieval experiments</head><p>The multimodal experiments (runs 2 to 9) have been designed to test the behavior of the expanded concept features vector. The runs marked as visual at the modality column at </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,132.67,480.04,337.43,8.10;9,141.74,491.08,328.91,8.10;9,141.74,502.12,220.04,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,430.77,480.04,39.33,8.10;9,141.74,491.08,59.81,8.10;9,232.05,491.08,238.60,8.10;9,141.74,502.12,83.78,8.10">Evaluation of fusion techniques for multimodal content-based medical image retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Alpokocak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Ozturkmenoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Berber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">H</forename><surname>Vahid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">G</forename><surname>Hamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,241.01,502.12,96.76,8.10">CLEF 2011 Working Notes</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
	<note>DEMIR at ImageCLEFmed</note>
</biblStruct>

<biblStruct coords="9,132.67,513.04,337.78,8.10;9,141.74,524.08,328.59,8.10;9,141.74,535.12,88.54,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,235.50,513.04,234.95,8.10;9,141.74,524.08,29.94,8.10">Spatial Size Distributions. Applications to Shape and Texture Analysis</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ayala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Domingo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,177.95,524.08,234.20,8.10">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1430" to="1442" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,546.04,337.81,8.10;9,141.74,557.08,328.63,8.10;9,141.74,568.12,329.00,8.10;9,141.74,579.04,63.25,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,411.36,546.04,59.12,8.10;9,141.74,557.08,294.82,8.10">Experimentes at ImageCLEF 2010 using CBIR and TBIR Mixing Information Approaches</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Granados</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia-Serrano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,263.57,568.12,120.94,8.10">CLEF 2010 LABs and Workshops</title>
		<title level="s" coord="9,391.24,568.12,60.77,8.10">Notebook Papers</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Pianta</surname></persName>
		</editor>
		<meeting><address><addrLine>Padoua, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,590.08,338.07,8.10;9,141.74,601.12,329.00,8.10;9,141.74,612.04,194.91,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,283.15,590.08,187.58,8.10;9,141.74,601.12,79.46,8.10">Recuperación de Información visual utilizando descriptores conceptuales</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>De Ves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,238.49,601.12,232.25,8.10;9,141.74,612.04,131.88,8.10">Conference Proceedings of the Conferencia Española de Recuperación de Información, CERI 2012</title>
		<meeting><address><addrLine>Valencia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,620.08,337.99,11.10;9,141.74,634.12,329.00,8.10;9,141.74,645.04,302.05,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,415.03,623.08,55.63,8.10;9,141.74,634.12,329.00,8.10;9,141.74,645.04,162.24,8.10">Visual Concept Features and Textual Expansion in a Multimodal System for concept annotation and retrieval with Flickr photos at ImageCLEF2012</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">De</forename><surname>Ves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana</forename><surname>García-Serrano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,320.80,645.04,96.68,8.10">CLEF 2012 Working Notes</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,132.67,656.11,338.07,8.10;9,141.74,667.15,256.80,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,372.23,656.11,98.50,8.10;9,141.74,667.15,117.57,8.10">UNED-UV at Medical Retrieval Task of ImageCLEF 2011</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia-Serrano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,275.57,667.15,96.65,8.10">CLEF 2011 Working Notes</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,149.99,337.99,8.10;10,141.74,161.03,329.00,8.10;10,141.74,172.07,328.86,8.10;10,141.74,182.99,20.35,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,376.26,149.99,94.40,8.10;10,141.74,161.03,213.74,8.10">Multimedia Retrieval in a Medical Image Collection: Results Using Modality Classes</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García-Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cigarrán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,372.07,161.03,98.67,8.10;10,141.74,172.07,186.49,8.10">Workshop of Medical Content-based Retrieval for Clinical Decision Support</title>
		<imprint>
			<publisher>MCBR-CDS</publisher>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
	<note>To be published</note>
</biblStruct>

<biblStruct coords="10,132.67,194.03,337.62,8.10;10,141.74,205.07,328.85,8.10;10,141.74,215.99,45.33,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,288.92,194.03,181.37,8.10;10,141.74,205.07,214.91,8.10">XRCE&apos;s participation at medical image modality classification and ad-hoc retrieval task of ImageCLEFmed</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jacquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,393.55,205.07,77.04,8.10;10,141.74,215.99,18.98,8.10">CLEF 2011 Working Notes</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.67,227.03,338.19,8.10;10,141.74,238.07,329.00,8.10;10,141.74,248.99,219.80,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,251.79,227.03,219.06,8.10;10,141.74,238.07,56.45,8.10">Fusion techniques for combining textual and visual information retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Depeursinge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,216.41,238.07,42.08,8.10">ImageCLEF</title>
		<title level="s" coord="10,265.59,238.07,205.15,8.10;10,141.74,248.99,6.20,8.10">The springer international series on information retrieval</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="95" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,259.79,338.34,8.10;10,141.74,271.10,328.90,8.10;10,141.74,282.02,78.58,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,404.59,259.79,66.15,8.10;10,141.74,271.10,264.75,8.10">Multimodal Information Approaches for the Wikipedia Collection at ImageCLEF 2011</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Granados</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia-Serrano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,426.67,271.10,43.97,8.10;10,141.74,282.02,52.26,8.10">CLEF 2011 Working Notes</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,293.06,338.13,8.10;10,141.74,304.10,328.43,8.10;10,141.74,315.02,289.22,8.10" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,286.68,304.10,183.49,8.10;10,141.74,315.02,116.18,8.10">Overview of the ImageCLEF 2012 medical image retrieval and classification tasks</title>
		<author>
			<persName coords=""><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alba</forename><surname>Garcia Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jayashree</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dina</forename><surname>Demner Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sameer</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ivan</forename><surname>Eggel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,263.56,315.02,94.39,8.10">CLEF 2012 working notes</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,326.06,338.12,8.10;10,141.74,337.10,324.96,8.10" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,357.42,326.06,113.10,8.10;10,141.74,337.10,163.76,8.10">Applying logistic regression to relevance feedback in image retrieval systems</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zuccarello</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Ayala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>De Ves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Domingo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,311.81,337.10,71.21,8.10">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">2621</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,348.02,338.25,8.10;10,141.74,359.06,328.94,8.10;10,141.74,370.10,98.86,8.10" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,154.58,359.06,277.79,8.10">Overview of the CLEF 2011 medical image classification and retrieval tasks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García-Seco De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,448.75,359.06,21.93,8.10;10,141.74,370.10,72.53,8.10">CLEF 2011 Working Notes</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,381.02,338.26,8.10;10,141.74,392.06,328.93,8.10;10,141.74,403.10,201.98,8.10" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,327.64,381.02,143.02,8.10;10,141.74,392.06,222.98,8.10">Methods for Combining Content-Based and Textual-Based Approaches in Medical Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Torjmen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Pinel-Sauvagant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Boughanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,384.07,392.06,86.60,8.10;10,141.74,403.10,175.47,8.10">Evaluating Systems for Multilingual and Multimodal Information Access</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,132.40,414.02,337.73,8.10;10,141.74,425.06,204.57,8.10" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,277.45,414.02,192.68,8.10;10,141.74,425.06,62.93,8.10">Overview of the Wikipedia Image Retrieval Task at ImageCLEF 2011</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kludas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,223.37,425.06,96.73,8.10">CLEF 2011 Working Notes</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
