<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.61,116.95,332.15,12.62;1,140.97,134.89,333.41,12.62;1,222.75,152.82,169.86,12.62">A Plant Identification System using Shape and Morphological Features on Segmented Leaflets: Team IITK, CLEF 2012</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,138.94,190.49,52.36,8.74"><forename type="first">Akhil</forename><surname>Arora</surname></persName>
							<email>aarora@cse.iitk.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Kanpur</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,201.86,190.49,55.98,8.74"><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
							<email>angupta@ufl.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<settlement>Gainesville</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.38,190.49,65.54,8.74"><forename type="first">Nitesh</forename><surname>Bagmar</surname></persName>
							<email>nitesh@cse.iitk.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Kanpur</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,344.47,190.49,74.58,8.74"><forename type="first">Shashwat</forename><surname>Mishra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Kanpur</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,448.99,190.49,27.42,8.74;1,276.03,202.44,58.83,8.74"><forename type="first">Arnab</forename><surname>Bhattacharya</surname></persName>
							<email>arnabb@cse.iitk.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Kanpur</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.61,116.95,332.15,12.62;1,140.97,134.89,333.41,12.62;1,222.75,152.82,169.86,12.62">A Plant Identification System using Shape and Morphological Features on Segmented Leaflets: Team IITK, CLEF 2012</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7553B73197972BD6E310990751342A98</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>plant identification</term>
					<term>leaflet segmentation</term>
					<term>shadow correction</term>
					<term>petiole removal</term>
					<term>complex network features</term>
					<term>tooth features</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic plant identification tasks have witnessed increased interest from the machine learning community in recent years. This paper describes our (team IITK's) participation in the Plant Identification Task, CLEF 2012, organized by the Combined Lab Evaluation Forum (CLEF) where the challenge was to identify plant species based on leaf images. We first categorize the different types of images and then use a variety of novel preprocessing methods such as shadow and background correction, petiole removal and automatic leaflet segmentation for identifying the leaf blobs. We next use complex network framework along with novel tooth detection method and morphological operations to compute several useful features. Finally, we use a random forest for classification. Based on the proposed approach, we achieved 2 nd rank on the overall score in the competition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic plant identification tasks have gained recent popularity due to its use in quick characterization of plant species without requiring the expertise of botanists. Leaf-based features are preferred over flowers, fruits, etc. due to the seasonal nature of the later and also the abundance of leaves (except may be for the winter season). The Combined Lab Evaluation Forum (CLEF) hosts an annual competition on classifying plant species based on images of leaves. While there are some other important publicly available leaf image datasets such as the Flavia Dataset <ref type="bibr" coords="1,222.59,633.20,14.61,8.74" target="#b11">[12]</ref>, the SmithSonian Leaf Dataset <ref type="bibr" coords="1,384.61,633.20,9.96,8.74" target="#b2">[3]</ref>, and Swedish Leaf Dataset <ref type="bibr" coords="1,171.47,645.16,14.61,8.74" target="#b10">[11]</ref>, the ImageCLEF dataset <ref type="bibr" coords="1,300.29,645.16,10.52,8.74" target="#b6">[7]</ref> provided by CLEF is more challenging due to the difficulty of automatically segmenting the leaves in the images. Apart from containing scanned images and images taken in a controlled setup (pseudoscan), the dataset also contains natural photographs of plant species. Thus, the performance achieved on the CLEF dataset is a more realistic benchmark of the current state-of-the-art in this domain. Fig. <ref type="figure" coords="2,343.16,276.92,4.98,8.74" target="#fig_0">1</ref> shows certain example images from the dataset.</p><p>This paper describes our (team IITK) approach for the ImageCLEF Plant Identification Task, CLEF 2012. Our focus for this endeavor was on two main points: (i) providing good recognition accuracy for natural images and (ii) automating the process for the controlled setup images. We have been able to achieve both our targets satisfactorily as corroborated by the fact that one of our submitted runs achieved the 2 nd position overall in the competition.</p><p>Our contributions in this paper are as follows:</p><p>1. We propose novel pre-processing strategies for shadow removal and background noise correction. 2. We propose a fully automatic leaflet extraction approach for compound leaves. 3. We propose the use of tooth features, that provide a second level of discrimination for leaves with similar shape. 4. We also incorporate the use of an effective feedback based image segmentation interface for natural photographs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Approach</head><p>In this section, we present our proposed approach in detail. We begin with a description of the dataset followed by the preprocessing techniques. Next, we discuss the image features used and conclude with the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The ImageCLEF Pl@ntLeaves II Dataset</head><p>The ImageCLEF Pl@ntLeaves II dataset consists of a total of 11572 images from 126 tree species in the French Mediterranean area. The dataset is subdivided into three different types based on the acquisition methodology used: scans (57%), scan-like photos, i.e., pseudo-scans (24%) and natural photographs (19%). The entire dataset is divided into a training and a test set as specified in Table <ref type="table" coords="2,472.85,657.11,3.87,8.74" target="#tab_0">1</ref>. Associated with each image are metadata fields that include acquisition type, GPS coordinates of the observation, name of the author of the picture. The training images also contain the taxon name of the leaf species, and the task is to predict this field for the test images.</p><p>To classify an image, we first need to segment the leaf from the image. The process of segmentation, however, is not straightforward at all owing to the presence of several bottlenecks such as shadow, occlusion and complex leaves (Fig. <ref type="figure" coords="3,159.01,272.91,4.98,8.74" target="#fig_0">1</ref> highlights some of these). While some of the roadblocks such as shadow removal, petiole removal, etc. are common to most of the images, a quick glance at the dataset suggests that no common segmentation scheme can be applied to all the images. Images having a single leaf have different issues than compound leaf images. It is, therefore, useful to group the images with similar issues into a category and address each category separately. Based on this observation, the dataset was each divided into three categories as follows -Category 1: Scan + Pseudo-scan, Single Leaf -Category 2: Scan + Pseudo-scan, Compound Leaf -Category 3: Natural Photographs All natural photographs (type 3) were put in a single category. The remaining images were put in two separate groups depending on whether they contained single or compound leaves.</p><p>Fig. <ref type="figure" coords="3,169.58,439.28,4.98,8.74">2</ref> shows the overview of our system. Based on the category of the image, we follow different paths. We next discuss the image preprocessing techniques for each category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image Preprocessing Techniques</head><p>The image preprocessing involved steps such as basic segmentation, petiole removal, shadow removal, background noise removal, etc. which collectively aid the extraction of the leaf part from the image. The procedure is fully automatic for category 1 and category 2 images while it is semi-automatic (interactive) for category 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category 1 Images:</head><p>This category is composed of scan and pseudo-scan images of single leaf species. Fig. <ref type="figure" coords="3,191.43,616.97,17.71,8.74" target="#fig_2">3(a)</ref> shows one such image. We first perform OTSU thresholding 3 Fig. <ref type="figure" coords="4,237.59,218.05,4.13,7.89">2</ref>. Plant identification system overview.</p><p>[9] on the grayscale image. For many cases, the output is not as expected due to severe background color variation, confusion of shadow regions as leaf, etc. The aim of the pre-processing step is to handle these. We use the following three-stage process to obtain the correct bitmaps for the images in this category:</p><p>1. Binarization: The image I is converted to grayscale and OTSU thresholding is performed to obtain a "distorted" bitmap, I o . We use the term distorted as the output is easily affected by shadow and noise in the background. were part of the output of the previous step and, therefore, were (falsely) detected as being part of the leaf. Since petioles can adversely affect the shape characteristics of the leaf if their length is comparable to that of the leaf, it is needed to deselect them. This is achieved by searching for abrupt surges in the thickness as we scan each row from top to bottom. Rows whose thickness fell below a certain threshold (as a ratio of the maximum thickness of the leaf) were identified as petiole sections and were removed from the bitmap I ad to obtain the final bitmap I f which is used for feature vector computation. Fig. <ref type="figure" coords="4,230.95,589.83,18.27,8.74" target="#fig_2">3(d)</ref> shows the bitmap after petiole removal from Fig. <ref type="figure" coords="4,460.67,589.83,15.94,8.74" target="#fig_2">3(c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category 2 Images:</head><p>This category is composed of scan and pseudo-scan images of compound leaf species. Fig. <ref type="figure" coords="4,192.13,657.11,4.43,8.74" target="#fig_3">4</ref>(a) shows one such example. Such species contains a main stalk and several leaflets that branch out from the main stalk. Using shape-descriptors on the entire leaf does not capture the characteristics of the different compound leaf species. Thus, it is necessary to perform all analysis at the leaflet level. The challenge then is to segment a single leaflet from the compound leaf image. Our system undertakes the following steps to achieve the same:</p><p>1. Binarization and Shadow and Noise Removal: Since these two steps do not involve the intricacies of leaf structure, we follow the exact same procedure as in Category 1 images. Fig. <ref type="figure" coords="5,328.87,391.14,4.43,8.74" target="#fig_3">4</ref>(a) shows such an example. 2. Main Stalk Elimination: Since the ultimate aim is to extract a single leaflet, first the main stalk needs to be identified and removed from the image. A simple erosion operation does not work as the thickness of the main stalk can vary quite largely. We fit a curve of order 4 to approximate the main stalk (using the 'polyfit' operator from Octave <ref type="bibr" coords="5,413.26,451.91,10.30,8.74" target="#b5">[6]</ref>). The curve so obtained is thickened over neighboring pixels to ensure the formation of multiple connected components (blobs) in the binary image. Fig. <ref type="figure" coords="5,462.32,475.82,18.27,8.74" target="#fig_3">4(b)</ref> shows the approximated main stalk and Fig. <ref type="figure" coords="5,350.57,487.77,4.29,8.74" target="#fig_3">4</ref>(c) shows the image after its elimination. 3. Ellipse based Blob Ranking: The previous step outputs multiple blobs that need to be ranked according to their relevance, i.e., how closely they resemble a leaflet. We use the simple assumption that the shape of a leaflet can be approximated by an ellipse, and thus proceed to figure out how much does a blob resemble an ellipse. For each blob, a "relevance score" is computed that measures the area of the blob as a ratio of the area of the minimum bounding ellipse (MBE) around it. Higher this score, higher is the blob likely to be an ellipse. We retain the top three blobs according to this scoring function and this is output to the next stage for further refinement. 4. GrabCut Segmentation: The minimum bounding ellipses of the top three contenders are now used as inputs to the GrabCut algorithm <ref type="bibr" coords="5,424.33,633.20,15.50,8.74" target="#b9">[10]</ref> which in turn returns the images containing the leaflets. We observe that the images thus obtained are not always perfect and may contain background noise and petiole fragments around the leaflet. We resolve these in the next steps. Fig. <ref type="figure" coords="6,172.04,329.69,4.57,8.74" target="#fig_3">4</ref>(d) shows the three extracted candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Noise Removal:</head><p>We use a simple range filter to address the background fragments. We first compute the average RGB values for the background by traversing around the periphery of the original image. We then account for the background noise pixels by examining each pixel in the periphery and deselecting it (from the binary mask) if it falls within a particular range of the average background color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Stalk Removal:</head><p>The strategy used for stalk removal for category 1 images cannot be applied to the leaflets of compound leaves. This is because, unlike the single leaf images, the leaflets are not vertically oriented and upright.</p><p>Hence, an erosion-based scheme is used to eliminate the stalk from the leaflet bitmap. The inherent assumption in this scheme is that the erosion operation does not affect the shape and margin of the leaflets. Although this step did not completely solve the problem, it did manage to reduce the stalk content (if present) in the image. Fig. <ref type="figure" coords="6,286.95,503.26,4.29,8.74" target="#fig_3">4</ref>(e) shows the three candidate leaflets after noise and stalk removal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Best Candidate Selection:</head><p>The output of the last step produces three candidate leaflets of which we need to choose only one. While it is easy for a human being to choose the "best" among the three, we wanted to automate the entire process. Hence, we use the following simple method for the same. For each candidate, we compute a 50-dimensional feature vector using complex network shape descriptors (see Sec. 2.3 for a description of the features). We then compute the Euclidean distances between each pair of candidates and find the maximum pairwise distance. The candidate that does not feature in this is in some sense in the middle and was, thus, chosen as the "best" candidate. Category 3 Images:</p><p>Category 3 images consist of natural photographs. There is no distinction between single and compound leaves in this category. Fig. <ref type="figure" coords="7,387.16,252.42,4.98,8.74" target="#fig_4">5</ref> illustrates a few images in this category. The images suffer from multiple issues including occlusion, out of focus problem, etc. It is difficult to design an automated segmentation system that is robust to all these issues. We, therefore, resort to a semi-automated, feedback-based segmentation scheme using the GrabCut algorithm <ref type="bibr" coords="7,428.60,300.24,14.61,8.74" target="#b9">[10]</ref>. We develop an interface that iteratively seeks input from the user and makes corrections to the segmented image. The user "corrects" the segmentation output by undertaking either of the two following actions:</p><p>-Specifying missing regions in the current segmentation result (Red) -Specifying extraneous regions in the current segmentation result (Blue) Fig. <ref type="figure" coords="7,171.10,384.85,4.98,8.74" target="#fig_5">6</ref> shows the output at various stages for segmentation of leaf from a sample category 3 image. The top row denotes the user interactions and the bottom row denotes the output for the corresponding user actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Feature Extraction</head><p>The preprocessing phase, as described in Sec. 2.2, results in an image containing a single leaflet which acts as input to the feature extraction phase. The feature extraction phase results in a 90-dimensional feature vector comprising of geometrical, shape and texture features for each image. On a coarse level of granularity, the feature vector can be divided into three categories: 50 complex network, 28 tooth and 12 morphological features. These are described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complex Network Features:</head><p>The motivation behind using complex network features <ref type="bibr" coords="7,392.98,561.47,10.52,8.74" target="#b0">[1]</ref> as shape descriptors lies in the fact that the data contains noise (even after preprocessing) and these features are robust, noise tolerant, and scale and rotation invariant. The complex network construction procedure requires as input the contour of an image represented as a collection of pixels on the 2D plane. A graph G = (V, E) is built where each pixel of the contour collectively forms the vertex set V and undirected edges between each pair of vertices form the edge set E. The Euclidean distance between the points of each such pair defines the weight of an edge (normalized within the interval [0, 1]). The graph thus obtained has connections between every pair of vertices and, therefore, cannot be considered as a complex network. We accomplish the transformation of this regular network into a complex network by thresholding the edge weights. We consider views of the network at multiple resolutions (thresholds), each containing different number of edges. An edge is retained if its weight is less than the threshold at that level and discarded otherwise.</p><p>The complex network thus constructed provides five different features that can be broadly categorized as degree descriptors and joint-degree descriptors.</p><p>1. Degree Descriptors: The degrees of the vertices are first normalized into [0, 1] by dividing by the total number of vertices in the network. Then, the maximum degree and mean degree are realized as the two degree descriptors:</p><formula xml:id="formula_0" coords="8,224.79,471.42,181.62,30.32">M aximum = N max i=1 d i , M ean = N i=1 d i /N</formula><p>where d i denotes the degree of vertex v i and N is the total number of vertices. 2. Joint-Degree Descriptors: The joint-degree descriptors rely on the correlations between the degrees of vertices connected by an edge. The correlation for a degree d i is captured by the probability P (d i ) that two neighboring vertices have the same degree d i . This probability is computed empirically as the ratio of the number of all connected vertices with the same degree d i to the total number of edges. Using this, the following three descriptors are realized:</p><formula xml:id="formula_1" coords="8,151.70,615.74,341.82,30.32">Entropy = D i=0 [P (d i ) log 2 P (d i )], Energy = D i=1 [P (d i )] 2 , Sum = D i=1 [P (d i )]</formula><p>where D is the maximum degree in the network. We realize the network at 10 different resolutions ranging uniformly from 0.05 to 0.50. At each threshold, we calculate 2 degree descriptors and 3 joint-degree descriptors, thus producing a total of 50 complex network features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tooth Features:</head><p>We next describe the tooth features. A tooth point is a pixel on the contour that has a high curvature, i.e., it is a peak. To determine whether a point P i on the contour is a tooth point or we examine the angle θ subtended at P i by its neighbors P i-k and P i+k (where k is a threshold). Fig. <ref type="figure" coords="9,391.24,342.33,4.98,8.74" target="#fig_6">7</ref> shows an example. If the angle θ is within a particular range, then P i is a tooth; otherwise, it is not. The upper bound of θ is π/2, i.e., the right angle. The lower bound was determined by manual observations. The value that gave the best result was sin -1 0.8, i.e., the sine of the angle was constrained to be 0.8 ≤ sin θ ≤ 1. The feature value for a leaf at a particular threshold k is the number of tooth points on its contour. Fig. <ref type="figure" coords="9,169.49,427.67,4.98,8.74" target="#fig_7">8</ref> shows the output of the tooth point detection method on a leaf for two different thresholds. As the threshold, k, is increased from 5 to 45, the number of contour points detected as tooth points decreases from 25 to 11.</p><p>Since it is possible for two different types of leaves to have nearly the same number of teeth at a particular threshold, we compute the tooth-based features at multiple increasing values of k. While no single threshold may be good enough to distinguish different leaf margins, they should be sufficiently separated when multiple thresholds are used. We use 28 thresholds from 3 to 30.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Morphological Features:</head><p>For morphological features, we use the same ones as described in <ref type="bibr" coords="9,437.27,564.13,15.50,8.74" target="#b11">[12]</ref> <ref type="foot" coords="9,452.78,562.55,3.97,6.12" target="#foot_1">4</ref> . The automated process, however, expects the image to be aligned along the vertical axis with only a small allowable error. This is to facilitate the curve-fitting operation approximate the upper-most and lower-most points of the main vein of the leaf. Vertical orientation was performed using principal component analysis (PCA). It was observed that one of the two principal components generated by PCA is along the direction of the main vein. This works due to the fact that most of the leaflets are either ellipsoidal or palmately lobed in shape. The ellipsoidal leaflets have the main vein along the major axis of the ellipse that fits the leaflet. For palmately lobed leaflets, the two sides of the leaflet along the main vein are almost symmetric, and hence, the main vein is along one of the principal components.</p><p>To determine whether a leaflet is ellipsoidal in nature, we first compare its area with the minimum enclosing ellipse. If they match well, we consider the leaflet to be elliptical in shape. To vertically orient the leaflet along the main vein, we then rotate the image along the first principal component of the ellipse.</p><p>For palmately lobed leaflets, on the other hand, we associate a symmetry measure with each principal component. This measure checks the similarity between the two sides of the leaflet along the axis represented by that principal component. The one with a higher symmetry measure is considered to be along the main vein and the leaflet is rotated along that direction.</p><p>This vertical orientation process renders the task of marking endpoints of the main vein automatic, which in turn, makes computing the set of 12 morphological features as mentioned in <ref type="bibr" coords="10,242.59,497.97,15.50,8.74" target="#b11">[12]</ref> automatic as well. The features are computed from a set of 5 basic geometric parameters (Fig. <ref type="figure" coords="10,324.44,509.92,4.43,8.74" target="#fig_8">9</ref>) described next.</p><p>1. Diameter, D: The diameter of the leaf is the longest distance between any two points on the closed contour defined by the leaf. 2. Physiological Length, L: The physiological length refers to the length of the line connecting the two terminal points of the main vein in the leaf. 3. Physiological Width, W : The physiological width refers to the distance between the two endpoints of the longest line segment perpendicular to the physiological length. 4. Leaf Area, A: The leaf area is the number of pixels in the final preprocessed (binary) image constituting the leaf part. 5. Leaf Perimeter, P : The leaf perimeter is the number of pixels along the closed contour of the leaf. These geometric parameters yield the following 12 morphological features:</p><p>1. Smooth Factor: This is defined as the ratio of the area of the image smoothened by a 5×5 averaging filter to that smoothened by a 2×2 averaging filter. 2. Aspect Ratio: This is defined as the ratio of physiological length to physiological width, i.e., L/W . 3. Form Factor: This measures the "roundness" of the leaf and is computed as 4πA/P 2 . 4. Rectangularity: This measures how rectangular the leaf is and is computed as L.W/A. 5. Narrow Factor: This measures the "narrowness" of the leaf and is defined as D/L. 6. Perimeter Ratio of Diameter: This is defined as the ratio of the perimeter of the leaf to its diameter of the leaf, i.e., P/D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Perimeter Ratio of Physiological Length and Physiological Width:</head><p>This is defined as the ratio of the perimeter of the leaf to the sum of its physiological length and physiological width, i.e., P/(L + W ). 8. Vein Features: The skeletal structure of a leaf is defined by the vein patterns which play an important role in distinguishing the leaves that have similar shape. The standard and most widely used procedure for computing the vein features is to perform a morphological opening operation on the grayscale image. A flat, disk shaped structuring element of radius r is used and the resultant image is then subtracted from the contour of the leaf. The output resembles the vein structure of a leaf on which we compute the following 5 features: A 1 /A, A 2 /A, A 3 /A, A 4 /A, A 4 /A 1 where A r denotes the area of the remaining leaf obtained using a structuring element of radius r, and A is the area of the original leaf as mentioned earlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complex Images:</head><p>The proposed preprocessing techniques failed to successfully segment some of the images in the dataset (see Fig. <ref type="figure" coords="11,309.93,645.16,9.96,8.74" target="#fig_9">10</ref> for examples). These leaves are, in a way, compound of compound leaves -each branch of a leaf is analogous to a compound leaf. This distinction of "complex leaves" is done by examining the number of connected components calculated after performing the erosion operation on category 2 images. The leaves with high number of connected blobs (≈ 30) are termed as complex, thereby marking the others as normal compound leaves. Since a successful leaf segmentation is critical to the computation of the proposed features, we bypass the usual feature extraction process for the complex leaves, and instead, use a bag of features model <ref type="bibr" coords="12,345.33,356.13,9.96,8.74" target="#b7">[8]</ref>.</p><p>We first compute the SURF descriptors <ref type="bibr" coords="12,319.05,368.27,10.52,8.74" target="#b1">[2]</ref> of many randomly sampled patches from all the complex images. We next cluster the descriptors into 8 groups using k-means clustering. We use these clusters as a code-book <ref type="bibr" coords="12,386.21,392.18,10.52,8.74" target="#b7">[8]</ref> of size 8.</p><p>Given a complex image, we then extract all its SURF points, and denote each SURF descriptor by the nearest cluster center (equivalently, the code corresponding to it). This results in a histogram of size 8 for every image where bin i represents the number of SURF points that correspond to that particular code. The histogram represents the feature vector of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Classifier Selection</head><p>We tested many classifiers for predicting the type of species, and after extensive experimentation, chose random forests <ref type="bibr" coords="12,306.09,515.78,10.52,8.74" target="#b3">[4]</ref> as the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>We submitted four different runs for the CLEF 2012 plant identification task. All of them used the same preprocessing and classification methodologies and differed on the basis of (i) inclusion or exclusion of the author id metadata field, and (ii) choice between two different parameter sets for the random forest classifier. The two different parameter sets differ on the basis of number of trees in the random forest and the number of attributes (features) considered at each node of the trees to make a split. Each run was scored based on a scoring function that essentially measures the mean average precision per photographer per species. Mathematically, it is denoted as:</p><formula xml:id="formula_2" coords="13,216.47,267.86,182.41,33.41">S = 1 U . U u=1   1 P u . Pu p=1   1 N u,p . Nu,p n=1 S u,p,n    </formula><p>where U denotes the number of photographers that have submitted at least one test image, P u denotes the total number of images taken by the u th photographer, N u,p denotes the number of images of the p th plant taken by the u th photographer, and S u,p,n denotes a value between 0 and 1 explained next.</p><p>For each test image, multiple guesses about its correct class were allowed. The term S u,p,n measures how good the guess (for the n th image of the p th plant taken by the u th photographer) is, and is inversely proportional to the rank of the correct guess. The later the rank, the lower is the score, and the function decreases rapidly.</p><p>Table <ref type="table" coords="13,178.21,422.24,4.98,8.74" target="#tab_2">3</ref> shows the details and final standings of our four submitted runs. Our best submission, Run 3, was the only one that did not use any metadata information. For the other runs, we used the photographer id metadata field as the 91 st feature as we observed that it improved the classification accuracy significantly for a 10-fold cross validation evaluation. we utilized SMOTE <ref type="bibr" coords="13,470.08,470.06,10.52,8.74" target="#b4">[5]</ref> to address the class imbalance in the dataset for only Run 4. Consequently, it ranked higher than Run 1 and Run 2. For a complete description of competition results, we refer the reader to <ref type="bibr" coords="13,266.74,505.93,9.96,8.74" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>In this paper, we have tackled the problem of plant identification using leafbased features. We used the Pl@ntLeaves II dataset by CLEF as the benchmark to support our results. We proposed novel preprocessing strategies for shadow removal and background noise correction. We also proposed an automated leaflet segmentation procedure for compound leaves. We introduced the use of tooth features to discriminate leaves with similar shapes but different margins. We also implemented an important improvement on the calculation of morphological features by automating the process of detecting the endpoints of the main vein. We finally used a combination of shape, morphological and tooth features with random forests for classification. Our approach helped us achieve an overall rank of 2 nd in the Plant Identification Challenge, CLEF 2012.</p><p>In future, we would like to achieve effective automatic segmentation of natural photographs as well. We would also like to work on optimal fusion of different types of features as well as classifier boosting to improve the results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,168.62,219.93,278.11,7.89;2,209.44,116.83,193.40,94.00"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Sample images for the Plant Identification Task, CLEF 2012.</figDesc><graphic coords="2,209.44,116.83,193.40,94.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,188.26,327.80,261.39,8.74;4,138.97,339.23,341.62,8.77;4,151.70,351.22,328.89,8.74;4,151.70,363.17,328.89,8.74;4,151.70,375.13,328.89,8.74;4,151.70,387.09,328.89,9.65;4,151.70,399.04,328.89,8.74;4,151.70,411.00,328.89,8.74;4,151.70,422.95,328.89,9.65;4,151.70,434.91,328.89,8.74;4,151.70,446.86,328.89,9.65;4,151.70,458.82,328.88,9.65;4,151.70,470.77,326.70,9.65;4,138.97,482.21,341.62,8.77"><head></head><label></label><figDesc>Fig. 3(b) shows the output for the example image Fig. 3(a). 2. Shadow and Noise Removal: Since both scan and pseudo-scan images were taken against a plain background (low saturation), we observed that the falsely detected problematic background regions almost always had a lower saturation value than the true leaf region. We leverage this information to identify the problematic regions in the OTSU thresholded I o by transforming it into the HSV color space and then deselecting the low saturation regions. More formally, we performed OTSU thresholding on the saturation space of I o to obtain I s . We subtract I s from I o to get a mask, I n , that contains the noise regions. Since some leaf regions with low saturation value may also be sometimes present in I n , we erode I n to deselect such regions and invert the resultant to obtain I nf . A logical AND operation of I nf and I o gives the shadow-and noise-free bitmap I ad . Fig. 3(c) shows the result for Fig. 3(b). 3. Petiole Removal: Several images contained very long petiole sections which</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,134.77,245.79,345.82,7.89;5,134.77,256.77,345.83,7.86;5,134.77,267.73,198.42,7.86;5,172.31,126.80,60.00,80.00"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Category 1 preprocessing: Fig. 3(a) shows the original image, Fig. 3(b) shows the bitmap prior to shadow removal, Fig. 3(c) shows the bitmap after shadow removal, Fig. 3(d) shows the bitmap after petiole removal.</figDesc><graphic coords="5,172.31,126.80,60.00,80.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,134.77,255.78,345.83,7.89;6,134.77,266.77,345.83,7.86;6,134.77,277.72,345.83,7.86;6,134.77,288.68,208.46,7.86;6,139.76,126.91,59.16,89.88"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Category 2 preprocessing: Fig. 4(a) shows the original bitmap image, Fig. 4(b) shows the image with polyfit curve marked in red, Fig. 4(c) shows the bitmap after stalk elimination, Fig. 4(d) shows the three extracted leaflets prior to noise and stalk removal, Fig. 4(e) shows the final extracted leaflets.</figDesc><graphic coords="6,139.76,126.91,59.16,89.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,238.78,195.90,137.78,7.89;7,187.68,116.83,240.00,60.00"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Sample category 3 images.</figDesc><graphic coords="7,187.68,116.83,240.00,60.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,134.77,286.18,345.82,7.89;8,134.77,297.17,178.91,7.86;8,177.33,126.79,79.80,120.40"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Category 3 user feedback based segmentation: Fig. 6(a) through Fig. 6(c) represent the iterations 1, 2, and 3 respectively.</figDesc><graphic coords="8,177.33,126.79,79.80,120.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="9,263.02,205.73,89.33,7.89;9,251.77,116.83,108.75,79.80"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. A tooth point.</figDesc><graphic coords="9,251.77,116.83,108.75,79.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="10,204.32,257.20,206.73,7.89;10,234.22,127.51,66.48,91.20"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Tooth detection at two different thresholds.</figDesc><graphic coords="10,234.22,127.51,66.48,91.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="11,209.43,237.25,196.50,7.89"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The five basic morphological parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="12,230.42,267.31,154.51,7.89;12,199.30,126.79,59.85,106.65"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Examples of complex images.</figDesc><graphic coords="12,199.30,126.79,59.85,106.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,212.62,118.75,190.12,59.16"><head>Table 1 .</head><label>1</label><figDesc>Statistics of images in the dataset.</figDesc><table coords="3,212.62,118.75,190.12,43.95"><row><cell cols="5">Type Scan Pseudo-scan Natural Total</cell></row><row><cell cols="2">Training 4870</cell><cell>1819</cell><cell>1733</cell><cell>8422</cell></row><row><cell>Test</cell><cell>1760</cell><cell>907</cell><cell>483</cell><cell>3150</cell></row><row><cell>Total</cell><cell>6630</cell><cell>2726</cell><cell>2216</cell><cell>11572</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="12,134.77,645.16,345.83,20.69"><head>Table 2 .</head><label>2</label><figDesc>Table 2 provides a complete description of the classifier parameters. Description of random forest classifier parameters.</figDesc><table coords="13,165.51,118.75,281.26,66.67"><row><cell>Run ID</cell><cell cols="4">Single Leaf # Trees # Split Features # Trees # Split Features Compound Leaf</cell></row><row><cell>1</cell><cell>350</cell><cell>7</cell><cell>400</cell><cell>7</cell></row><row><cell>2</cell><cell>350</cell><cell>30</cell><cell>400</cell><cell>25</cell></row><row><cell>3</cell><cell>500</cell><cell>7</cell><cell>400</cell><cell>7</cell></row><row><cell>4</cell><cell>350</cell><cell>7</cell><cell>400</cell><cell>7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="14,147.08,118.75,321.19,78.32"><head>Table 3 .</head><label>3</label><figDesc>Run ID Scorescan Score pseudo Score natural Scoreaverage Rank Final standings of our submitted runs (Run 3 achieved the 2 nd rank).</figDesc><table coords="14,184.46,132.53,253.80,41.94"><row><cell>1</cell><cell>0.37</cell><cell>0.34</cell><cell>0.43</cell><cell>0.38</cell><cell>10</cell></row><row><cell>2</cell><cell>0.30</cell><cell>0.25</cell><cell>0.24</cell><cell>0.27</cell><cell>16</cell></row><row><cell>3</cell><cell>0.43</cell><cell>0.40</cell><cell>0.49</cell><cell>0.44</cell><cell>2</cell></row><row><cell>4</cell><cell>0.37</cell><cell>0.35</cell><cell>0.43</cell><cell>0.38</cell><cell>8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="3,144.73,635.88,335.87,7.86;3,144.73,646.84,335.87,7.86;3,144.73,657.79,111.96,7.86"><p>OTSU performs binarization by selecting an optimum threshold to separate the foreground and background regions of the image such that their combined (intraregion) variance is minimal.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1" coords="9,144.73,635.88,335.87,7.86;9,144.73,646.84,335.86,7.86;9,144.73,657.79,230.18,7.86"><p>The authors of<ref type="bibr" coords="9,209.38,635.88,14.34,7.86" target="#b11">[12]</ref> were generous enough to provide the code for human assisted extraction of several morphological features. We automate this process and subsequently use the system to compute the required features.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank <rs type="person">Aditya Nigam</rs>, <rs type="person">Tejas Gandhi</rs> and <rs type="person">Manash Pal</rs> for their guidance and feedback during the course of the project. We would also like to thank our department for giving us the resources and the freedom to pursue such non-curriculum academic interests.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="14,142.96,392.55,337.63,7.86;14,151.52,403.48,272.24,7.89" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,312.70,392.55,167.89,7.86;14,151.52,403.51,102.44,7.86">A Complex Network-based Approach for Boundary Shape Analysis</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bruno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,262.09,403.51,81.43,7.86">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="54" to="67" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,413.85,337.64,7.86;14,151.52,424.80,60.92,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,295.12,413.85,135.89,7.86">Surf: Speeded Up Robust Features</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,451.54,413.85,29.05,7.86">ECCV</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="404" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,435.14,337.63,7.86;14,151.52,446.10,329.07,7.86;14,151.52,457.06,319.12,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,351.55,446.10,129.04,7.86;14,151.52,457.06,203.97,7.86">Searching The Worlds Herbaria: A System for Visual Identification of Plant Species</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kress</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sheorey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,377.60,457.06,29.05,7.86">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="116" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,467.37,277.87,7.89" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,206.32,467.39,61.96,7.86">Random forests</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,275.93,467.39,69.14,7.86">Machine learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,477.73,337.63,7.86;14,151.52,488.66,328.71,7.89" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,353.95,477.73,126.64,7.86;14,151.52,488.69,75.83,7.86">Smote: Synthetic minority oversampling technique</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,235.53,488.69,167.03,7.86">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,499.02,273.22,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="14,195.29,499.02,83.18,7.86">GNU Octave Manual</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Eaton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Network Theory Limited</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,509.36,337.63,7.86;14,151.52,520.32,329.07,7.86;14,151.52,531.28,144.57,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,205.01,520.32,199.23,7.86">The ImageCLEF 2012 Plant Identification Task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Barthelemy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Molino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,432.88,520.32,47.71,7.86;14,151.52,531.28,58.30,7.86">CLEF 2012 Working Notes</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,541.61,337.63,7.86;14,151.52,552.57,167.12,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,289.66,541.61,190.93,7.86;14,151.52,552.57,52.18,7.86">Sampling Strategies for Bag-of-Features Image Classification</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,225.59,552.57,29.05,7.86">ECCV</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="490" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.96,562.91,337.64,7.86;14,151.52,573.84,203.62,7.89" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="14,191.11,562.91,232.01,7.86">A Threshold Selection Method from Gray-Level Histogram</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,430.60,562.91,49.99,7.86;14,151.52,573.86,140.47,7.86">IEEE Trans. on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.61,584.20,337.97,7.86;14,151.52,595.13,319.45,7.89" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,323.46,584.20,157.13,7.86;14,151.52,595.16,140.04,7.86">GrabCut&quot;: Interactive Foreground Extraction using Iterated Graph Cuts</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,299.96,595.16,81.55,7.86">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,142.61,605.49,337.98,7.86;14,151.52,616.45,181.67,7.86" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="14,220.99,605.49,255.50,7.86">Computer Vision Classification of Leaves from Swedish Trees</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Söderkvist</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>Linkoping University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s Thesis</note>
</biblStruct>

<biblStruct coords="14,142.61,626.79,337.97,7.86;14,151.52,637.75,329.07,7.86;14,151.52,648.71,57.34,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,400.75,626.79,79.84,7.86;14,151.52,637.75,275.58,7.86">A Leaf Recognition Algorithm for Plant Classification using Probabilistic Neural Network</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,450.78,637.75,29.81,7.86;14,151.52,648.71,2.56,7.86">ISSPIT</title>
		<imprint>
			<biblScope unit="page" from="11" to="16" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
