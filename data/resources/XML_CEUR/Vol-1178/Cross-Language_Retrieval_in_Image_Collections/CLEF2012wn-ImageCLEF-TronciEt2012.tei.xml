<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,149.65,116.95,316.06,12.62;1,232.76,134.89,149.85,12.62">Image Hunter at ImageCLEF 2012 Personal Photo Retrieval Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,148.20,173.08,66.61,8.74"><forename type="first">Roberto</forename><surname>Tronci</surname></persName>
							<email>roberto.tronci@diee.unica.it</email>
							<affiliation key="aff0">
								<orgName type="department">DIEE -Department of Electric and Electronic Engineering</orgName>
								<orgName type="institution">University of Cagliari</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">AmILAB -Laboratorio Intelligenza d&apos;Ambiente Ricerche</orgName>
								<address>
									<settlement>Sardegna</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,231.70,173.08,46.86,8.74"><forename type="first">Luca</forename><surname>Piras</surname></persName>
							<email>luca.piras@diee.unica.it</email>
							<affiliation key="aff0">
								<orgName type="department">DIEE -Department of Electric and Electronic Engineering</orgName>
								<orgName type="institution">University of Cagliari</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,289.11,173.08,71.25,8.74"><forename type="first">Gabriele</forename><surname>Murgia</surname></persName>
							<email>gabriele.murgia@sardegnaricerche.it</email>
							<affiliation key="aff1">
								<orgName type="institution">AmILAB -Laboratorio Intelligenza d&apos;Ambiente Ricerche</orgName>
								<address>
									<settlement>Sardegna</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,390.30,173.08,72.39,8.74"><forename type="first">Giorgio</forename><surname>Giacinto</surname></persName>
							<email>giacinto@diee.unica.it</email>
							<affiliation key="aff0">
								<orgName type="department">DIEE -Department of Electric and Electronic Engineering</orgName>
								<orgName type="institution">University of Cagliari</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,149.65,116.95,316.06,12.62;1,232.76,134.89,149.85,12.62">Image Hunter at ImageCLEF 2012 Personal Photo Retrieval Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A0CE58DBF93CC2E0F00D3EB8D6750C85</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>photo retrieval</term>
					<term>content based image retrieval</term>
					<term>relevance feedback</term>
					<term>SVM</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the participation of the Pattern Recognition and Application Group (PRA Group) and the Ambient Intelligence (AmILAB) in the ImageCLEF 2012 Personal Photo Retrieval Pilot Task. This is a pilot task that aims to provide a test bed for QBE-based retrieval scenarios in the scope of personal information retrieval based on a collection of 5,555 personal images plus rich meta-data. For this challenge we used Image Hunter, a content based image retrieval tool with relevance feedback previously developed by ourselves. The results show that we obtained good results by taking into account that we used only visual data, moreover we were the only one that used relevance feedback.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The personal photo retrieval is a pilot task introduced in the ImageCLEF 2012 competition. This pilot task provides a test-bed for query by example (QBE) image retrieval scenarios in the scope of personal information retrieval. In fact, instead of using images downloaded from Flickr or other similar web resources, the dataset proposed reflects an amalgamated personal image collection that has been taken by 19 photographers. The aim of this pilot task is to create an image retrieval scenario where a normal person (i.e., not expert in image retrieval tasks) searches in its personal photo collections some "relevant" images, i.e. the search for similar images or images depicting a similar event, e.g. a rock concert. This pilot task is divided into two tasks: retrieval of visual concepts and retrieval of events. A detailed overview of the dataset and the task can be found in <ref type="bibr" coords="1,449.89,596.82,14.61,8.74" target="#b15">[16]</ref>.</p><p>We took part only to the Task 1 of this competition. In this task we have a set of visual concepts with five QBE associated to be used in the retrieval process to retrieve relevant images. To perform the task we used Image Hunter, a content based image retrieval tool with relevance feedback previously developed at the AmILAB <ref type="bibr" coords="1,178.21,657.11,14.61,8.74" target="#b14">[15]</ref>. With the aim of building a practical application to show the potentialities of Content Based Image Retrieval tools with Relevance Feedback, we developed Image Hunter. Image Hunter is a prototype that shows the capabilities of an Image Retrieval engine where the search is started by an image provided by the user, and the system returns the most visually similar images. The system is enriched by Relevance Feedback capabilities that let the user specify which results match the desired concepts through an easy user interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Architectural description</head><p>This tool is entirely written in JAVA, so that the tool is machine independent. For its development, we partially took inspiration from the LIRE library <ref type="bibr" coords="2,465.09,471.58,15.50,8.74" target="#b10">[11]</ref> (that is just a feature extraction library). In addition, we chose Apache Lucene for building the index of the extracted data.</p><p>The main core of Image Hunter is a full independent module, thus allowing the development of a personalized user interface. A schema of the whole system can be seen in Figure <ref type="figure" coords="2,231.29,531.56,3.87,8.74" target="#fig_0">1</ref>.</p><p>The core of the system is subdivided into three main parts:</p><p>-Indexing and Lucene interface for data storing; -Feature extraction interface; -Image Retrieval and Relevance Feedback.</p><p>The Indexing part has the role of extracting the visual features and other informations from the images. The visual features and other descriptors of the images are then stored in a particular structure defined inside Image Hunter. The tool can index different types of image formats and it can be built in an incremental way. Lucene turned out to be well suited for the storage needs of Image Hunter. The core of Lucene's logical architecture is a series of document containing text fields, where we have associated different features (fields) to each image (document).</p><p>As we said before, for the feature extraction we took inspiration from the LIRE library that it is used as an external feature extraction library. We expanded and modified its functionalities by implementing or reimplementing in Image Hunter some extractors. The Feature extraction interface allows to extract different visual features based on different characteristics: color, texture and shape. They are:</p><p>-Scalable Color <ref type="bibr" coords="3,217.84,234.21,9.96,8.74" target="#b2">[3]</ref>, a color histogram extracted from the HSV color space; -Color Layout <ref type="bibr" coords="3,213.01,245.90,9.96,8.74" target="#b2">[3]</ref>, that characterizes the spatial distribution of colors; -RGB-Histogram and HSV-Histogram <ref type="bibr" coords="3,316.66,257.58,14.61,8.74" target="#b10">[11]</ref>, based on RGB and HSV components of the image respectively; -Fuzzy Color <ref type="bibr" coords="3,207.16,281.22,14.61,8.74" target="#b10">[11]</ref>, that considers the color similarity between the pixel of the image; -JPEG Histogram <ref type="bibr" coords="3,229.74,304.87,14.61,8.74" target="#b10">[11]</ref>, a JPEG coefficient histogram; -Edge Histogram <ref type="bibr" coords="3,224.31,316.55,9.96,8.74" target="#b2">[3]</ref>, that captures the spatial distribution of edges; -Tamura <ref type="bibr" coords="3,189.31,328.24,14.61,8.74" target="#b11">[12]</ref>, that captures different characteristic of the images like coarseness, contrast, directionality, regularity, roughness; -Gabor <ref type="bibr" coords="3,178.94,351.88,10.52,8.74" target="#b6">[7]</ref> that allows the edge detection; -CEDD (Color and Edge Directivity Descriptor) <ref type="bibr" coords="3,362.86,363.57,9.96,8.74" target="#b3">[4]</ref>; -FCTH (Fuzzy Color and Texture Histogram) <ref type="bibr" coords="3,353.10,375.25,9.96,8.74" target="#b4">[5]</ref>.</p><p>One of Image Hunter's greatest strengths is its flexibility: in fact, its structure was built in a way that it is possible to add any other image descriptor. The choice of the above mentioned set is due to the "real time" nature of the system with large database. In fact even if some local features such as SIFT or SURF could improve the retrieval performance for some particular kind of searches, on the other hand they are more time expensive in the evaluation of the similarity between images.</p><p>The core adopts three relevance feedback techniques <ref type="bibr" coords="3,384.58,477.79,14.60,8.74" target="#b14">[15]</ref>. Two of them are based on the nearest-neighbor paradigm (NN), while one of them is based on Support Vector Machines (SVM). The use of the nearest-neighbor paradigm has been driven by its use in a number of different pattern recognition fields, where it is difficult to produce a high-level generalization of a class of objects, but where neighborhood information is available <ref type="bibr" coords="3,303.22,537.56,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="3,315.40,537.56,7.01,8.74" target="#b7">8]</ref>. In particular, nearest-neighbor approaches have proven to be effective in outliers detection, and one-class classification tasks <ref type="bibr" coords="3,190.65,561.47,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="3,202.83,561.47,11.62,8.74" target="#b12">13]</ref>. Support Vector Machines are used because they are one of the most popular learning algorithm when dealing with high dimensional spaces as in the case of CBIR <ref type="bibr" coords="3,236.71,585.38,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="3,248.89,585.38,11.62,8.74" target="#b13">14]</ref>.</p><p>The user interface is structured to provide just the functionalities that are strictly related with the user interaction (e.g., the list of relevant images found by the user).</p><p>Image Hunter employs a web-based interface that can be viewed and experienced at the address http://prag.diee.unica.it/amilab/WIH. This version is a web application built for the Apache Tomcat web container by using a mixture of JSP and java Servlet. The graphic interface is based on the jQuery framework, and has been tested for the Mozilla Firefox and Google Chrome browsers. The Image Hunter homepage let the user choose the picture from which starting the search. The picture can be chosen either within those of the proposed galleries or among the images from the user hard disk. In order to make intuitive and easy the features offered by the application, the graphical interface has been designed relying on the Drag and Drop approach. From the result page, the user can drag the images that her deems relevant to her search in a special box-cart, and then submit the feedback. Then the feedback is processed by the system, and a new set of images is proposed to the user. The user can iterate the feedback process as many times he/she wants. Figure <ref type="figure" coords="4,298.31,239.54,4.98,8.74" target="#fig_1">2</ref> summarizes the typical user interaction within Image Hunter. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Relevance Feedback techniques implemented in Image Hunter</head><p>In this section we briefly describe the two relevance feedback techniques implemented in the core that we have used in this competition. The use of the nearest-neighbor paradigm is motivated by its use in a number of different pattern recognition fields, where it is difficult to produce a high-level generalization of a class of objects, but where neighborhood information is available <ref type="bibr" coords="4,445.04,633.20,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="4,457.21,633.20,7.01,8.74" target="#b7">8]</ref>. In particular, nearest-neighbor approaches have proven to be effective in outliers detection, and one-class classification tasks <ref type="bibr" coords="4,323.67,657.11,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="4,335.85,657.11,11.62,8.74" target="#b12">13]</ref>. Support Vector Machines are used because they are one of the most popular learning algorithm when dealing with high dimensional spaces as in CBIR <ref type="bibr" coords="5,317.35,131.95,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="5,329.53,131.95,11.62,8.74" target="#b13">14]</ref>.</p><p>k-NN Relevance Feedback In this work we resort to a technique proposed by some of the authors in <ref type="bibr" coords="5,235.30,175.53,10.52,8.74" target="#b8">[9]</ref> where a score is assigned to each image of a database according to its distance from the nearest image belonging to the target class, and the distance from the nearest image belonging to a different class. This score is further combined to a score related to the distance of the image from the region of relevant images. The combined score is computed as follows:</p><formula xml:id="formula_0" coords="5,188.10,244.65,292.50,22.31">rel(I) = n/t 1 + n/t • rel BQS (I) + 1 1 + n/t • rel N N (I)<label>(1)</label></formula><p>where n and t are the number of non-relevant images and the whole number of images retrieved after the latter iteration, respectively. The two terms rel N N and rel BQS are computed as follows:</p><formula xml:id="formula_1" coords="5,209.69,323.75,270.90,23.89">rel N N (I) = I -N N nr (I) I -N N r (I) + I -N N nr (I)<label>(2)</label></formula><p>where N N r (I) and N N nr (I) denote the relevant and the non relevant Nearest Neighbor of I, respectively, and • is the metric defined in the feature space at hand,</p><formula xml:id="formula_2" coords="5,212.82,395.88,267.77,36.66">rel BQS (I) = 1 -e 1-d BQS (I) max i d BQS (I i ) 1 -e<label>(3)</label></formula><p>where e is the Euler's number, i is the index of all images in the database and d BQS is the distance of image I from a reference vector computed according to the Bayes decision theory (Bayes Query Shifting, BQS) <ref type="bibr" coords="5,384.80,463.48,14.61,8.74" target="#b9">[10]</ref>. The aim of BQS approach is to "move" the query along the visual spaces by taking into account images marked as relevant and not-relevant within the visual concept searched to look for new images. The BQS query is computed as follows:</p><formula xml:id="formula_3" coords="5,165.35,520.64,315.24,23.22">Q BQS = m R + σ m R -m N • 1 - k R -k N max{k R , k N } • (m R -m N )<label>(4)</label></formula><p>where m R and m N are the mean vectors of relevant and not-relevant images respectively, σ is the standard deviation of the images belonging to the neighborhood of the original query and k R and k N are the number of relevant and not-relevant images, respectively. If we are using F feature spaces, we have different scores rel(I) for each f feature space. Thus the following combination is performed to obtain a "single" score:</p><formula xml:id="formula_4" coords="5,256.71,637.35,223.88,30.55">rel(I) = F f =1 w f • rel f (I) (5)</formula><p>where the w f is the weight associated to the f -space.</p><formula xml:id="formula_5" coords="6,222.85,137.11,257.74,49.49">w f = i∈R d f min (I i , R) i∈R d f min (I i , R) + i∈R d f min (I i , N )<label>(6)</label></formula><p>SVM based Relevance Feedback Support Vector Machines are used to find a decision boundary in each feature space f ∈ F . The SVM is very handy for this kind of task because, in the case of image retrieval, we deal with high dimensional feature spaces and two "classes" (i.e. relevant and not-relevant). For each feature space f , a SVM is trained using the feedback given by the user. The results of the SVMs in terms of distances from the hyperplane of separation are then combined into to a relevance score through the Mean rule as follows</p><formula xml:id="formula_6" coords="6,241.90,290.32,238.69,30.55">rel SV M (I) = 1 F F f =1 rel f SV M (I)<label>(7)</label></formula><p>3 Image Hunter at ImageCLEF</p><p>For the participation at the ImageCLEF competition we mainly used Image Hunter as it is. This means that as visual features we have used only those listed in the previous section, that are partially part of those provide for the competition <ref type="bibr" coords="6,189.84,394.93,14.61,8.74" target="#b15">[16]</ref>.</p><p>We took part only to the task 1 (retrieval of visual concepts). In the task different visual concepts are provided and to each concept five QBE are associated. However Image Hunter is designed to trigger the image retrieval process only with one QBE. Thus, instead of performing five different runs for each concept starting with a different QBE and averaging the results, we slightly modified Image Hunter at the first interaction step (i.e., the first content based image retrieval before the relevance feedback steps) to take into account the five QBE. In this case we adopted two different techniques: the "mean" of the QBEs and a mixed multi query approach. In the first case, the query used to trigger Image Hunter is the mean vector of the five QBE in each visual feature space:</p><formula xml:id="formula_7" coords="6,272.35,531.97,208.25,9.68">Q mean = m QBE<label>(8)</label></formula><p>that is the case of BQS presented in Equation ( <ref type="formula" coords="6,343.99,549.52,4.24,8.74" target="#formula_3">4</ref>) when there are only relevant images. In the second case we performed one content based image retrieval for each QBE, and then we mixed the results by assigning to each retrieved image the minimum distance from the five query images. After this first automatic interaction, the tool is ready to interact with real users by using one of the relevance feedback methodologies above. The interaction with real users was performed by 10 different people. The only constraint that we gave to each person was in the minimum number of interactions (i.e., 3), but letting them free to choose when stop the retrieval process. In Figure <ref type="figure" coords="6,216.16,657.11,4.98,8.74" target="#fig_2">3</ref> some snapshots of the tool in action are reported.  <ref type="table" coords="9,164.23,156.77,4.13,7.89">2</ref>. Performance in terms of precision after N docs retrieved. In bold the best results, in italics the second best results per measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head><p>Run ID P5 P10 P15 P20 P30 P100 KIDS IBMA0 0,8333 0,7833 0,7222 0,6896 0,6347 0,4379 KIDS OBOA0 0,8000 0,7292 0,6667 0,6354 0,6083 0,4117 KIDS IOMA0 0,7667 0,6583 0,6222 0,6104 0,5639 0,3925 KIDS OBMA0 0,6500 0,6500 0,6083 0,5771 0,5611 0,3925 REGIM run4 0,9000 0,8375 0,7917 0,7333 0,6292 0,3992 REGIM run2 0,9000 0,8417 0,7917 0,7292 0,6278 0,3975 REGIM run1 0,9000 0,8417 0,7889 0,7292 0,6278 0,3967 REGIM run5 0,9000 0,8458 0,7889 0,7292 0,6278 0,3971 REGIM run3 0,9000 0,8458 0,7889 0,7292 0,6278 0,3975 Image Hunter -Lpiras Run12 0,7917 0,7667 0,7361 0,6938 0,6083 0,3417 KIDS IOOA4 0,6750 0,6125 0,5778 0,5354 0,4486 0,3054 Image Hunter -Lpiras Run11 0,8000 0,7083 0,6222 0,5646 0,4903 0,2825 Image Hunter -Lpiras Run32 0,6583 0,5667 0,4667 0,3958 0,2972 0,1425 Table <ref type="table" coords="9,163.81,429.07,4.13,7.89">3</ref>. Performance in terms of normalized discounted cumulative gain (ndcg) and mean average precision (map) after N docs retrieved. In bold the best results, in italics the second best results per measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head><p>Run ID ndcg5 ndcg10 ndcg15 ndcg20 ndcg30 ndcg100 map30 map100 KIDS IBMA0 0,6405 0,6017 0,5658 0,5459 0,5213 0,4436 0,1026 0,1777 KIDS OBOA0 0,5858 0,5348 0,5028 0,4836 0,4728 0,4144 0,0952 0,1589 KIDS IOMA0 0,5800 0,5184 0,4951 0,4872 0,4615 0,3979 0,0906 0,1558 KIDS OBMA0 0,4073 0,4268 0,4123 0,4066 0,4046 0,3717 0,0854 0,1518 REGIM run4 0,4896 0,4703 0,4667 0,4563 0,4274 0,3572 0,0810 0,1224 REGIM run2 0,4926 0,4722 0,4687 0,4561 0,4271 0,3566 0,0811 0,1224 REGIM run1 0,4908 0,4730 0,4680 0,4560 0,4271 0,3561 0,0811 0,1222 REGIM run5 0,4899 0,4742 0,4681 0,4551 0,4264 0,3563 0,0811 0,1222 REGIM run3 0,4899 0,4742 0,4681 0,4551 0,4264 0,3564 0,0811 0,1222 IH -Lpiras Run12 0,6122 0,5777 0,5656 0,5457 0,5017 0,3700 0,0991 0,1216 KIDS IOOA4 0,5701 0,5062 0,4798 0,4545 0,4016 0,3303 0,0632 0,0930 IH -Lpiras Run11 0,6265 0,5642 0,5165 0,4835 0,4342 0,3087 0,0692 0,0852 IH -Lpiras Run32 0,4857 0,4404 0,3917 0,3466 0,2853 0,1795 0,0319 0,0363</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In our participation to the personal photo retrieval pilot task of ImageCLEF, we tested the efficiency of our previous tool Image Hunter. As our intention was to benchmark this tool on this task, we did not make any modification. The only modification made was about the use of five QBE instead of one. The results obtained are encouraging, especially if we think that the results were obtained using only visual features. Future improvements of the tool will focus on the use of combination of the meta-data features with the visual ones, and on the improvement of our ranking system that is not actually designed for scientific evaluation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,201.03,277.01,213.29,7.89;2,264.66,115.81,138.19,145.11"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Image Hunter: Web Application Architecture</figDesc><graphic coords="2,264.66,115.81,138.19,145.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,177.11,507.66,260.16,7.89;4,135.76,383.74,137.78,92.35"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example of a typical user interaction with Image Hunter</figDesc><graphic coords="4,135.76,383.74,137.78,92.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,158.14,613.25,299.07,7.89;7,152.06,387.79,311.23,210.69"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Image Hunter in action with the Personal Photo Retrieval dataset</figDesc><graphic coords="7,152.06,387.79,311.23,210.69" type="bitmap" /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>We submitted four runs to the competition combining the two methodologies for the first step and the relevance feedback methods used: Q mean + kNN (Run11 in tables), multi query + kNN (Run12 ), Q mean + SVM (Run31 ), multi query + SVM (Run32 ). Unfortunately, Run31 was not evaluated due some duplicates in the final file. In each run 100 retrieved documents are reported. In our case they are sorted with the relevance score obtained at the last step, these means that in some cases the first entry is not the first relevant image retrieved. This fact, derives from the methodologies used for relevance feedback. In particular the kNN, by means of the BQS, "moves" the query in the visual spaces, and with respect to the last BQS query the first relevant images retrieved could be far than other images.</p><p>In Table <ref type="table" coords="8,188.72,279.65,4.98,8.74">1</ref> the methodologies used by the competitors in the competition are presented. Among all the competitors we were the only one that used only the visual features for all the runs, and the only one that used a real user interaction relevance feedback. In Table <ref type="table" coords="8,188.79,548.81,4.98,8.74">2</ref> the precision after N docs retrieved is reported. REGIM obtained the bet results until 20 retrieved documents, after the best results are obtained by the run IBMA0 of KIDS. Instead, our results are generally good if we take into account the kNN retrieval (i.e., Run11 and Run12 ), and are mostly better than the only other method based only to visual features.</p><p>In Table <ref type="table" coords="8,187.56,609.29,4.98,8.74">3</ref> normalized discounted cumulative gain and mean average precision after N docs retrieved is reported. For these measures our run Run11 obtained better results than those from REGIM (that were the best in the previous table), and we can claim that it is the best second run if we look at the overall of the measures presented in this table.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,295.62,337.64,7.86;10,151.52,306.58,112.27,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,302.34,295.62,137.75,7.86">Instance-based learning algorithms</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Aha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kibler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">K</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,446.80,295.62,33.80,7.86;10,151.52,306.58,35.48,7.86">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="66" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,317.58,337.63,7.86;10,151.52,328.54,329.07,7.86;10,151.52,339.50,133.14,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,356.34,317.58,124.25,7.86;10,151.52,328.54,50.12,7.86">LOF: Identifying density-based local outliers</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="10,420.11,328.54,60.48,7.86;10,151.52,339.50,26.68,7.86">SIGMOD Conference</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Naughton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Bernstein</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="93" to="104" />
			<date type="published" when="2000">2000</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,350.50,337.63,7.86;10,151.52,361.46,110.71,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,289.17,350.50,133.45,7.86">Overview of the mpeg-7 standard</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Puri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,429.94,350.50,50.64,7.86;10,151.52,361.46,110.71,7.86">IEEE Trans. Circuits Syst. Video Techn</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,372.46,337.63,7.86;10,151.52,383.42,329.07,7.86;10,151.52,394.38,329.07,7.86;10,151.52,405.33,100.39,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,303.51,372.46,177.08,7.86;10,151.52,383.42,216.01,7.86">Cedd: Color and edge directivity descriptor: A compact descriptor for image indexing and retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="10,250.00,394.38,169.20,7.86">ICVS. Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Gasteratos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vincze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">5008</biblScope>
			<biblScope unit="page" from="312" to="322" />
			<date type="published" when="2008">2008</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,416.33,337.64,7.86;10,151.52,427.29,329.07,7.86;10,151.52,438.25,188.37,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,303.68,416.33,176.92,7.86;10,151.52,427.29,181.18,7.86">Fcth: Fuzzy color and texture histogram -a low level feature for accurate image retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,354.37,427.29,126.22,7.86;10,151.52,438.25,76.89,7.86">Image Analysis for Multimedia Interactive Services</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="191" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,449.25,337.63,7.86;10,151.52,460.21,302.21,7.86" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="10,284.67,449.25,195.92,7.86;10,151.52,460.21,153.32,7.86">An Introduction to Support Vector Machines and Other Kernel-based Learning Methods</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,471.21,337.64,7.86;10,151.52,482.17,174.81,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,298.34,471.21,182.26,7.86;10,151.52,482.17,43.82,7.86">Features for image retrieval: an experimental comparison</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,202.79,482.17,37.53,7.86">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="77" to="107" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,493.17,337.64,7.86;10,151.52,504.13,89.36,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="10,301.21,493.17,85.55,7.86">Pattern Classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>John Wiley and Sons, Inc</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,515.13,337.64,7.86;10,151.52,526.09,329.07,7.86;10,151.52,537.04,314.93,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,206.64,515.13,273.95,7.86;10,151.52,526.09,57.83,7.86">A nearest-neighbor approach to relevance feedback in content based image retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,228.33,526.09,252.26,7.86;10,151.52,537.04,114.50,7.86">CIVR &apos;07: Proceedings of the 6th ACM international conference on Image and video retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="456" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,548.04,337.97,7.86;10,151.52,559.00,212.26,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,247.71,548.04,232.88,7.86;10,151.52,559.00,24.44,7.86">Bayesian relevance feedback for content-based image retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,182.52,559.00,81.43,7.86">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1499" to="1508" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,570.00,337.98,7.86;10,151.52,580.96,329.07,7.86;10,151.52,591.92,254.98,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,285.64,570.00,194.95,7.86;10,151.52,580.96,43.46,7.86">Lire: lucene image retrieval: an extensible java cbir library</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,215.48,580.96,265.11,7.86;10,151.52,591.92,44.45,7.86">MM &apos;08: Proceeding of the 16th ACM international conference on Multimedia</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1085" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,602.92,337.98,7.86;10,151.52,613.88,329.07,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,310.17,602.92,170.42,7.86;10,151.52,613.88,40.75,7.86">Textural features corresponding to visual perception</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Yamawaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,198.84,613.88,175.90,7.86">IEEE Trans. Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="460" to="473" />
			<date type="published" when="1978-06">June 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,624.88,337.98,7.86;10,151.52,635.84,143.95,7.86" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="10,199.72,624.88,91.33,7.86">One-class classification</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Tax</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001-06">June 2001</date>
			<pubPlace>Delft, The Netherlands</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Delft University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct coords="10,142.62,646.84,337.97,7.86;10,151.52,657.79,289.92,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,239.07,646.84,237.95,7.86">Support vector machine active learning for image retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,165.60,657.79,191.53,7.86">Proc. of the 9th ACM Intl Conf. on Multimedia</title>
		<meeting>of the 9th ACM Intl Conf. on Multimedia</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="107" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,120.67,337.98,7.86;11,151.52,131.63,329.07,7.86;11,151.52,142.59,329.07,7.86;11,151.52,153.55,48.15,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,377.92,120.67,102.67,7.86;11,151.52,131.63,220.59,7.86">Imagehunter: a novel tool for relevance feedback in content based image retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tronci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Murgia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pili</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,393.20,131.63,87.40,7.86;11,151.52,142.59,324.87,7.86">5th Int. Workshop on New Challenges in Distributed Information Filtering and Retrieval -DART 2011</title>
		<imprint>
			<publisher>Ceur</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.62,164.51,337.98,7.86;11,151.52,175.46,285.40,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="11,206.37,164.51,270.02,7.86">Overview of the personal photo retrieval pilot task at imageclef 2012</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zellhöfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,197.12,175.46,103.78,7.86">CLEF 2012 working notes</title>
		<meeting><address><addrLine>Rome, Italy, Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
