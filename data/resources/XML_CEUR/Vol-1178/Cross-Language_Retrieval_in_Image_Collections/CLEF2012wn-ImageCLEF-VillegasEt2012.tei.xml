<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.70,115.76,339.95,12.93;1,224.57,133.69,166.20,12.93">Overview of the ImageCLEF 2012 Scalable Web Image Annotation Task</title>
				<funder ref="#_GCFZDd8">
					<orgName type="full">Generalitat Valenciana</orgName>
				</funder>
				<funder ref="#_TSW28Xk">
					<orgName type="full">Spanish MICINN</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,222.21,170.89,75.90,9.96"><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
							<email>mvillegas@iti.upv.es</email>
							<affiliation key="aff0">
								<orgName type="department">Institut Tecnològic d&apos;Informàtica</orgName>
								<orgName type="institution">Universitat Politècnica de València</orgName>
								<address>
									<addrLine>Camí de Vera s/n</addrLine>
									<postCode>46022</postCode>
									<settlement>València</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,320.81,170.89,72.33,9.96"><forename type="first">Roberto</forename><surname>Paredes</surname></persName>
							<email>rparedes@iti.upv.es</email>
							<affiliation key="aff0">
								<orgName type="department">Institut Tecnològic d&apos;Informàtica</orgName>
								<orgName type="institution">Universitat Politècnica de València</orgName>
								<address>
									<addrLine>Camí de Vera s/n</addrLine>
									<postCode>46022</postCode>
									<settlement>València</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.70,115.76,339.95,12.93;1,224.57,133.69,166.20,12.93">Overview of the ImageCLEF 2012 Scalable Web Image Annotation Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C4A7631DFC26E6D4BE01877C21A4A4D2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ImageCLEF 2012 Scalable Image Annotation Using General Web Data Task proposed a challenge, in which as training data instead of relying only on a set of manually annotated images, the objective was to make use of automatically gathered Web data, with the aim of developing more scalable image annotation systems. To this end, the participants were provided with a new dataset, composed of 250,000 images for training, which included various visual feature types, and textual features obtained from the websites in which the images appeared. Two subtasks were defined. The first subtask employed the same test set as the ImageCLEF 2012 Flickr Photo Annotation subtask, with the particularity that both the Flickr and Web training sets had to be used. The idea was to determine if the Web data could help to enhance the annotation performance in comparison to using only manually annotated data. The second subtask consisted in using only automatically gathered Web data to develop an image annotation system. For this, we provided a development and test sets of 1,000 and 2,000 images, respectively, both manually annotated for 95 and 105 concepts, respectively. The participants of the first subtask were not able to take advantage of the Web data to enhance the annotation performance. On the contrary, in the second subtask interesting results were obtained. As expected, the overall performance of the systems is worse than using manually annotated data, nonetheless, the results are promising when analyzing per concept. For some concepts the performance is relatively good, confirming that the Web data can in fact be quite useful. Moreover, due to the low participation and the relatively simple techniques used, it is believed that there is considerable room for improvement on both subtasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The rapidly increasing amount of digital information that people have to deal with every day, has created huge interest in developing automatic indexing systems, so that information needs can be easily and efficiently fulfilled. In the case of images and video, this indexing can be addressed by means of an automatic image annotation system, in which images are associated with one or more concepts. The research on image concept detection has generally relied on training data that have been manually, and thus reliably labeled, an expensive and laborious endeavor that cannot easily scale. Because of this, it has become common in past image annotation benchmark campaigns <ref type="bibr" coords="2,392.99,142.24,11.62,9.96">[6,</ref><ref type="bibr" coords="2,404.61,142.24,11.62,9.96" target="#b9">10]</ref> to use crowdsoursing approaches such as the Amazon Mechanical Turk<ref type="foot" coords="2,392.75,152.82,3.97,6.97" target="#foot_0">1</ref> (MTurk), in order to label a large amount of images. Still, crowdsoursing is expensive and difficult to scale to a very large amount of concepts, thus it is advisable to explore possible alternatives.</p><p>With the advance of multimedia technology and the Internet, we have at our disposal billions of images available online. Furthermore, the images are found on webpages surrounded with text which might have a direct relationship with the content of the image. Even though this surrounding unsupervised text is noisy and sometimes unrelated to the image, it potentially has useful information, and furthermore it can be cheaply gathered and be obtained for practically any topic. Thus, determining whether this kind of data can be used for reliably annotating images is important. Previous research indicates that this information is useful, being the work of Torralba et al. <ref type="bibr" coords="2,331.95,297.68,15.49,9.96" target="#b10">[11]</ref> an example of this, in which almost 80 million tiny images were effectively used for several tasks such as person detection. More closely related, in the work of Weston et al. <ref type="bibr" coords="2,423.79,321.59,15.49,9.96" target="#b15">[16]</ref> an image annotation learning method is proposed that scales to millions of images and thousands of possible annotations. Another related work is the Arista project <ref type="bibr" coords="2,465.09,345.50,15.49,9.96" target="#b14">[15]</ref> in which accurate tags can be generated for popular Web images that have nearduplicates included in their Web image database of billions of images.</p><p>This paper presents an overview of the ImageCLEF 2012 Scalable Image Annotation Using General Web Data Task, a benchmark campaign oriented at using automatically gathered Web data for image annotation. The paper is organized as follows. Section 2 describes the generation of the dataset that was created specifically for this evaluation. Followed by this, the two subtasks that were defined are presented in section 3. Then, section 4 presents the results submitted by the participants and a discussion of these. Finally, section 5 is the conclusion of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>Creation of the Dataset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Web Crawling</head><p>Among the objectives for the dataset being created <ref type="bibr" coords="2,370.86,541.12,14.60,9.96" target="#b13">[14]</ref>, was to have a wide variety of images with a relatively small amount of images (not billions). Thus in order to obtain a good set of image URLs, we opted to use the same crawling strategy as in <ref type="bibr" coords="2,201.68,576.99,14.61,9.96" target="#b10">[11]</ref>, where the image URLs (and the corresponding URLs of the webpages that contain the images) are obtained by querying popular image search engines. We selected Google, Bing and Yahoo, and queried them using words from the English dictionary that comes with the aspell spelling checker. The next step in the crawling process was to download the images and corresponding webpages, and store a snapshot of these. At the end, in total we obtained over 31 million of both images and webpages. In order to avoid duplicate images, several precautions were taken. First the URLs were normalized to prevent different versions of the same URL to be downloaded several times. However, there was also the possibility that the same image was found under different URLs. To account for this, the images were stored using a unique code or image identifier, composed of: part of the MD5 checksum of an 864-bit image signature (in some aspects similar to the one presented in <ref type="bibr" coords="3,389.20,190.06,15.49,9.96" target="#b16">[17]</ref>) and, part of the MD5 checksum of the file. This scheme guaranties storing exactly the same file only once and easily identifying duplicates or near duplicates (accounting for images in various formats, at different resolutions and with minor modifications such as some watermarks). The final image identifiers are 16 base64url digits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image Subset Selection</head><p>Even though the set of downloaded images was obtained using all of the words of the English dictionary, and therefore it contains images from practically any topic, a subset of the images was selected for practical reasons. Basically selecting a subset permitted to provide smaller data files that would not be so prohibitive for the participants to download and handle. Furthermore, since the test sets had to be manually labeled and this could be done only for a relatively small list of concepts, we could select only the images indexed with words related to the list of concepts. The size of the training set was chosen to be of 250,000 images, which results in feature vector sets of moderate size that can be easily handled on current personal computers.</p><p>Another reason for selecting a subset, was to discard some types of images. Even though the URLs were obtained from trustworthy search engines, inevitably there is a certain amount of problematic images that we decided to remove. Among the problematic images are for instance a message saying "Image removed", or dummy images some servers send specifically to web crawlers. Removing this type images is in itself a difficult problem, however we noticed that most of these tended to have many different URLs linking to them or be images that appeared in a large amount of webpages. So the approach to remove most of these was simply not to include images that had more than N URLs linking to them or that appeared in more than M webpages. The values of N and M were set manually from a quick look at the images being considered for removal.</p><p>When crawling the Web, another problem encountered was that an image could be still reachable, although the webpage where it appeared has changed and no longer includes the image, or has been removed and does not supply the proper HTTP 404 code. Resolving this issue was simple due to the requirements of the dataset. For each image in the dataset there was supposed to be at least one webpage that contained the image, thus we verified the location of the images within the webpages. Any image not having a corresponding webpage was obviously not considered for inclusion.</p><p>The image identifier codes guarantied storing exactly the same image file only once. However, for this subset selection we also employed a very simple near im-age duplicate removal scheme. This was done using the same image signature mentioned in the previous section. To reduce the amount computation required for duplicate detection, the 864-bit image signatures were first reduced to 128 bits using PCA followed by a random rotation and thresholding <ref type="bibr" coords="4,417.86,154.19,9.96,9.96">[3]</ref>. The duplicate removal scheme was not including images that had a normalized hamming distance lower than 0.1 to the other images already in the subset.</p><p>The final selection of the 250,000 images was based on a list of 158 concepts that were manually defined. This list included all of the concepts of the test sets of both subtasks described on section 3. A small set of 3,000 images was first manually labeled using 115 of the concepts. These are the same images and ground truth labels used as development and test sets in subtask 2. Then for each concept (defined by the concept words and synonyms of these) and cooccurrences of concepts in the labeled set, we retrieved ranked lists of images, using the query results from the search engines, and also querying our own image index generated from the downloaded webpages. The lists were sorted by rank and the first 250,000 images were the ones selected for the final dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Available Data</head><p>This dataset was made available under a Creative Commons license, however, since the data was gathered from the Internet, and their original copyright conditions are difficult to determine automatically, only the feature vectors were distributed <ref type="foot" coords="4,170.22,384.09,3.97,6.97" target="#foot_1">2</ref> . Nonetheless, as it is commonly done on image search engines, thumbnails of the images could be obtained from a web server by using the image identifiers 3 . For each of the 250,000 training images, both the textual and visual features described next were available. For the additional 3,000 images used in subtask 2 that were manually labeled, and the 25,000 flickr images used in subtask 1, only the visual features were included.</p><p>Textual Features: Four sets of textual features were extracted. First is the list of the words used to find the image when querying the search engines, along with the rank position of the image in the respective query and the search engine it was found on. The second textual features were the image URLs as referenced in the webpages they appeared in. In many cases the image URLs tend to be formed with words that relate to the content of the image, this is why they can also be useful as textual features. The other two textual features available correspond to text extracted from the webpages near the position of the image. The difference between these two feature sets was the amount of preprocessing.</p><p>To extract the text near the image, we first converted the webpages to valid XML to ease processing and removed the script and style elements. The text considered close, was the webpage title and all the terms that are closer than 600 in word distance to the image, not including the HTML tags and attributes.</p><p>The first level of processing of the features included this raw text, although some types of terms were converted to a special symbol, such as words with non-latin characters. In these features, the position of the image was also indicated and the words replaced by a special symbol serve to preserve word distances.</p><p>For the next level of processing, a weight s(t n ) was assigned to each of the words near the image, defined as</p><formula xml:id="formula_0" coords="5,210.23,198.13,270.35,27.94">s(t n ) = 1 ∀t∈T s(t) ∀tn,m∈T F n,m sigm(d n,m ) ,<label>(1)</label></formula><p>where t n,m are each of the appearances of the term t n in the document Visual Features: As features extracted from the images, we made available seven types. As preprocessing, we filtered the images and resized them so that the width and height had at most 240 pixels while preserving the original aspect ratio. The first feature set were 576-dimensional color histograms extracted using our own implementation. The second set of features were the GIST <ref type="bibr" coords="5,422.47,357.95,9.96,9.96">[7]</ref>. The other four features were obtained using the colorDescriptors software <ref type="bibr" coords="5,405.96,369.91,9.96,9.96" target="#b7">[8]</ref>. We computed features for SIFT, C-SIFT, RGB-SIFT and OPPONENT-SIFT. As configuration we used dense sampling with default parameters, and a hard assignment 1,000 and 10,000 codebooks using a spatial pyramid of 1 × 1 and 2 × 2 <ref type="bibr" coords="5,439.90,405.77,9.96,9.96" target="#b4">[5]</ref>. Since the vectors of the spatial pyramid were concatenated, this resulted in 5,000dimensional and 50,000-dimensional feature vectors, respectively. Keeping only the first fifth of the dimensions would be like not using the spatial pyramid. The codebooks were generated using 1.25 million randomly selected features and the k-means algorithm.</p><p>Individual features (i.e. without using a codebook) were also made available for SIFT, C-SIFT, RGB-SIFT, OPPONENT-SIFT, and the seventh feature type SURF, extracted using the TOP-SURF software <ref type="bibr" coords="5,347.08,501.65,9.96,9.96">[9]</ref>. In this case the preprocessing was a little different since these were extracted exactly the same as for the ImageCLEF 2012 Flickr Photo Annotation and Retrieval Task <ref type="bibr" coords="5,410.85,525.56,15.49,9.96" target="#b9">[10]</ref> to ease participation in both tasks. The images were filtered with the catrom filter and resized to 256 × 256 pixels ignoring the original aspect ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Description</head><p>As commented before, a very large amount of images can be cheaply gathered from the Web, and furthermore, from the webpages that contain the images, text associated with them can be obtained. However, the degree of relationship between the surrounding text and the image, varies greatly, thus this data can be considered to be very noisy. Moreover, the webpages can be of any language  or even a mixture of languages, and they tend to have many writing mistakes. The goal of this task is to evaluate different strategies to deal with noisy data so that it can be reliably used for annotating images from practically any topic.</p><p>To illustrate the objective of the task, consider for example that we searched for the word "rainbow" in a popular image search engine. It would be expected that many results be of landscapes in which in the sky a rainbow is visible. However, other types of images will also appear, see Figure <ref type="figure" coords="6,395.93,436.46,8.48,9.96" target="#fig_1">1a</ref>. The images will be related to the query in different senses, and there might even be images that do not have any apparent relationship. In the example of Figure <ref type="figure" coords="6,420.45,460.37,8.48,9.96" target="#fig_1">1a</ref>, one image is a text page of a poem about a rainbow, and another is a photograph of an old cave painting of a rainbow serpent. See Figure <ref type="figure" coords="6,359.42,484.28,10.51,9.96" target="#fig_1">1b</ref> for a similar example on the query "sun". As can be observed, the data is noisy, although it does have the advantage that this data can also handle the possible different senses that a word can have.</p><p>Based on these observations, an interesting research topic would be: how to use and handle the automatically retrieved noisy Web data to complement the manually labeled training data and obtain a better performing annotation system than when using the manually labeled data alone. On the other hand, since the Web data can easily be obtained for any topic, another research topic would be: how to use the noisy Web data to develop an annotation system with a somewhat unbounded list of concepts, using only automatically retrieved image and textual Web data.</p><p>Both of the research topics just mentioned have been addressed in two separate subtasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Subtask 1: Complementing Manually Annotated Data</head><p>In this subtask the list of concepts and the test samples were exactly the same as the ones used in the ImageCLEF 2012 Flickr Photo Annotation subtask <ref type="bibr" coords="7,462.32,153.65,14.61,9.96" target="#b9">[10]</ref>. The ImageCLEF 2012 Flickr dataset consisted of a training and test sets of 15,000 and 10,000 images, respectively, that were manually labeled by means of crowdsoursing using a list of 94 concepts. For further details on this dataset, the reader should refer to the overview paper of the ImageCLEF 2012 Flickr Photo Annotation and Retrieval Task <ref type="bibr" coords="7,273.35,213.43,14.61,9.96" target="#b9">[10]</ref>.</p><p>In this subtask, the participants had available for developing their annotation systems, both the Flickr and Web training datasets. The objective was to develop techniques to take advantage of the Web data, trying to obtain better concept annotation performance in comparison to using only the Flickr manually annotated data. The participants had to submit results using as training only the Flickr dataset, and using both the Flickr and Web datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Subtask 2: Scalable Concept Image Annotation</head><p>In this subtask, the objective was to develop systems that could easily change or scale the list of concepts used for image annotation. In other words, the list of concepts is also considered to be an input to the system. Thus, the system when given an input image and a list of concepts, its job is to give a score to each of the concepts in the list and decide how many and which of them assign as annotations. To observe this scalable characteristic of the systems, the list of concepts was different for the development and test sets, and the participants only had available the ground truth annotations for the development set.</p><p>The idea was that the participants use the 250,000 images of the Web training set, including the visual and textual features (see Section 2), to develop and estimate the models for image annotation. It was not permitted to use any manually annotated data, such as the Flickr training set. However, the use of other additional language resources, such as language models, language detectors, stemmers, WordNet <ref type="bibr" coords="7,245.81,498.66,9.96,9.96" target="#b1">[2]</ref>, spell checkers, etc., was permitted and encouraged.</p><p>The development set consisted of 1,000 images annotated for 95 concepts, and the test set consisted of 2,000 images for 105 concepts, among which 85 were common to the development set, i.e. 10 concepts were removed and 20 were added. The list of concepts and the number of images for each can be observed in Table <ref type="table" coords="7,214.13,559.12,3.87,9.96" target="#tab_1">1</ref>. So that the Web training would be the same as for subtask 1, for this first edition of the task, the list of concepts overlapped considerably with the concepts of the Flickr annotation task.</p><p>For this subtask, so that there could be a reference performance and also serve as a starting point, a toolkit was supplied to the participants. This toolkit included software that computed the evaluation measures (see Section 4.1), and the implementations of two baselines. The first baseline was a simple random, which is important since any system which gets worse performance than the random baseline means that this system is doing nothing. The other baseline, referred to as Co-occurrence Baseline, was a very basic technique for this image annotation task, which obviously gives better performance than random, although it was simple enough to give the participants a wide margin for improvement. In this technique when given an input image, its nearest K = 32 images from the training set are obtained, using only the 1,000 bag-of-words C-SIFT visual features and the L1 norm. Then, the textual features corresponding to these K nearest images are used to derive a score for each of the concepts. This is done by using a concept-word co-occurrence matrix estimated from all of the training set textual features. In order to make the vocabulary size more manageable, the textual features are first processed keeping only the words from the English dictionary. Finally for the selection of concepts for annotation, for all input images, the first 5 ranked concepts are always chosen as annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Performance Measures</head><p>The participants were asked to submit the results in the following way. For each image to annotate, a score had to be given for every one of the concepts in the list and also indicate which concepts had finally been selected as annotations. Two basic performance measures have been used for comparing the results of the different submissions. These basic measures are the Average Precision (AP) and the F-measure (F 1 ). The AP only takes into account the scores assigned to the concepts and ignores the decisions of the selected annotations. On the other hand, the F 1 only considers the the selected annotations.</p><p>The AP is algebraically defined as</p><formula xml:id="formula_1" coords="9,256.47,277.05,219.87,31.87">AP = 1 |K| |K| k=1 k rank(k) , (<label>2</label></formula><formula xml:id="formula_2" coords="9,476.34,288.12,4.24,9.96">)</formula><p>where K is the ordered set of the ground truth annotations, being the order induced by the annotation scores, and rank(k) is the order position of the k-th ground truth annotation. The fraction k/ rank(k) is actually the precision at the k-th ground truth annotation, and has been written like this to be explicit on the way it is computed. In the cases that there are ties in the scores, a random permutation is applied within the ties.</p><p>In the context of image annotation, the AP can be estimated from two different perspectives, one being concept-based and the other example-based. In the former, one AP is computed for each concept, and in the latter one AP is computed for each image to annotate. Which of these is more correct to use actually depends on exactly what the scores are. If the scores for example relate to the probability that the concept is present for a given image, and the comparison between scores for different images is not clearly defined, then the concept-based AP does not make sense and will probably not be a good indicator of the performance of the system. On the other hand, in this case the example-based AP will be a good indicator of the performance of the system. In the instructions given to the participants, this was not clearly explained, however, for all of the submissions, the scores seemed to be image based. Therefore, in this paper we present results only for the example-based AP. Finally, to obtain a global performance measure of the systems, we have taken the arithmetic mean, in which case it is known as the Mean Average Precision (MAP).</p><p>The other performance measure used, the F 1 , is defined as</p><formula xml:id="formula_3" coords="9,277.51,588.12,198.83,23.54">F 1 = 2P R P + R , (<label>3</label></formula><formula xml:id="formula_4" coords="9,476.34,594.86,4.24,9.96">)</formula><p>where P is the precision and R is the recall. Again this measure can also be estimated from the concept-based and the example-based perspectives. In this case both approaches are adequate and serve to analyze different aspects. For the example-based F 1 , as a global system performance measure the arithmetic mean is used, thus obtaining a mean F-measure (MF 1 ). On the other hand, the concept-based F 1 is used to analyze the behavior for different concepts. Other performance measures were computed and analyzed, however, for the received submissions they do not give any important details that are not already observed with the previously mentioned measures. Therefore for simplicity, we are not including them in this paper. These other measures were, the AP using the geometric mean and the interpolated versions, i.e. the Geometric Mean Average Precision (GMAP) the Interpolated Average Precision (IAP), the Mean Interpolated Average Precision (MIAP) and the Geometric Mean Interpolated Average Precision (GMIAP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Participation</head><p>In total, 47 groups registered for the task and signed the license agreement, and therefore had access for downloading the datasets. Unfortunately in the end, the participation was considerably low. For subtask 1 we received 15 runs from three groups, and for subtask 2 we received 10 runs from one group. Also, one of the groups that submitted results for subtask 1 said that they made a mistake and did not intend to participate in the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KIDS-NUTN:</head><p>The Knowledge, Information, and Database System Laboratory (KIDS-NUTN), from the National University of Tainan <ref type="bibr" coords="10,411.17,367.83,10.51,9.96" target="#b0">[1]</ref> submitted in total 9 runs. All of the runs were for subtask 1, being 5 using only the Flickr training set and the other 4 using both the Flickr and Web training sets. They used a combination of several visual feature types, namely AutoColorCorrelogram, ColorLayout, FCTH, Gabor, GIST, and ROI background, and as textual features they used the EXIF data. For the annotation, they used Random Forests and for comparison the also tried as a baseline the Multiple Bernoulli Relevance Models (MBRM). For further details, please refer to <ref type="bibr" coords="10,366.01,451.51,9.96,9.96" target="#b0">[1]</ref>.</p><p>ISI: The Intelligent Systems and Informatics Laboratory (ISI), from the University of Tokyo <ref type="bibr" coords="10,226.06,476.12,15.49,9.96" target="#b12">[13]</ref> submitted in total 20 runs. Half of the runs were for subtask 1, being 6 using only the Flickr training set and the other 4 using both the Flickr and Web training sets. For the other 10 runs for subtask 2, half correspond to the development set, and the other half to the test set. Their effort was targeted at making the system scalable, so for annotation they used the Passive-Aggressive with Averaged Pairwise Loss (PAAPL) <ref type="bibr" coords="10,409.91,535.90,14.61,9.96" target="#b11">[12]</ref>, which is an online learning method they propose for multiclass multilabel classification using a linear model. As visual features, they used the provided *SIFT features, and to tackle the Web data, they artificially labeled it by looking at the textual features and if a word that defined a concept appeared, then that concept was assumed to be present. The images that did not have any concept were simply discarded. In subtask 1, they tried first to learn models for the Flickr and Web data separately, and combine the results, and second they tried learning the models by merging all of the data. All of their submissions were using the latter approach, since during development it was the one that performed best. The difference between submissions is simply the combination of visual features. In subtask 2, again they artificially labeled the training data for learning, and the submissions differ in the combination of visual features. For further details, please refer to <ref type="bibr" coords="11,450.77,391.92,14.61,9.96" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and Discussion for Subtask 1</head><p>The results for subtask 1, given by the example-based MAP and the MF 1 , are presented in Tables <ref type="table" coords="11,224.08,452.21,9.96,9.96" target="#tab_2">2a</ref> and<ref type="table" coords="11,257.89,452.21,8.85,9.96" target="#tab_2">2b</ref>. The first table includes the best result of each group when using as training only the Flickr manually annotated data, and a baseline, which is randomly assigning scores to the concepts and selecting randomly the top N as annotations. The second table includes results for all of the submissions using both the Flickr and Web training data. As can be observed in the tables, all of the results using both the Flickr and Web training data have a worse performance than when using only Flickr data. In the case of the KIDS-NUTN, the difference between using or not using Web data is not so high. When we inquired them about these results, they answered that the textual data of the Web dataset did not help, but they did not give us a clearer explanation on how they arrived to this conclusion or exactly to what the submissions correspond. Moreover, they said that they were not able to dedicate much time on the problem.</p><p>Regarding the results of ISI, the difference between using or not using Web data is considerably high. In fact it seems that during development <ref type="bibr" coords="11,441.72,619.59,15.49,9.96" target="#b12">[13]</ref> they obtained better results, and these did not generalize to the test set. For the Flickr task, they obtained an MF 1 higher than 0.5 both during development and during test. However, using both Flickr and Web training data they obtained an MF 1 in the order of 0.48 during development in contrast to the 0.18 they obtain for the test set. This suggests that possibly there was some mistake or something was different in the models used for annotating the test set. In fact in subtask 2 (see Section 4.4), they obtain better results even though they have used the same technique, and the problem is harder since only Web data can be used as training and the random baseline is lower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.4</head><p>Results and Discussion for Subtask 2</p><p>Tables 3a and 3b present the results for the example-based MAP and MF 1 for the development and test sets, respectively. The tables include the results for the submitted runs and the two baselines, assigning random scores and selecting the random top N concepts per image, and the co-occurrence baseline as described in Section 3.2. The first thing to note is that the results for the development set, generalizes well to the test set, unlike what was observed in the ISI results for subtask 1. The second thing to note is that the submitted runs have a considerably better performance than the supplied co-occurrence baseline. This is a great achievement, even though the co-occurrence baseline is considerably simple. Unfortunately, there was only one participant, so there was no much competition, and definitely it is not possible to say that this level of performance is more or less what can be achieved using Web data as training, so that it could be compared to using labeled data as training. Furthermore, the ISI system can also be considered to be a relatively simple technique. It does seem that their proposed PAAPL technique is able to learn from the data despite the large amount of noise that it has. However, their use of the textual features is extremely simple, only searching exactly for the words that define the concepts. They have not used synonym information, stemming, WordNet, or any other resources that could be quite useful, and surely the performance could be improved.</p><p>Even though the test sets of subtask 1 and 2, differ in difficulty, image quality, number of concepts, etc., if we dare compare the results of image annotation using manually labeled data (see Table <ref type="table" coords="13,311.76,607.63,9.22,9.96" target="#tab_2">2a</ref>) with using automatically gathered data (see Table <ref type="table" coords="13,206.12,619.59,8.58,9.96" target="#tab_3">3b</ref>), the performance is lower for the latter. This is something expected since learning with Web data is considerably more challenging. The real objective is to observe how much can be achieved using the Web data. Ultimately in practice one or the other or a combination of both approaches will be better in a certain circumstance. Thinking about the problem, it would be understandable that an annotation system will work better for some concepts than for others.</p><p>In Tables <ref type="table" coords="14,178.04,376.99,9.96,9.96" target="#tab_4">4a</ref> and<ref type="table" coords="14,210.20,376.99,8.85,9.96" target="#tab_4">4b</ref>, there are lists of concepts categorized by the range of their concept-based F 1 , for the best system in subtask 1 and 2, respectively. Here, it can be observed that for some concepts, the Web data performs rather well, and in general it does not look too bad with respect to the results using manually labeled data. The same thing can be observed in Tables <ref type="table" coords="14,381.49,424.81,9.96,9.96" target="#tab_5">5a</ref> and<ref type="table" coords="14,414.13,424.81,8.85,9.96" target="#tab_5">5b</ref>, which show the top performing concepts according to the relative improvement<ref type="foot" coords="14,420.89,435.39,3.97,6.97" target="#foot_2">4</ref> with respect to the random baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>The ImageCLEF 2012 Scalable Image Annotation Using General Web Data proposed two subtasks. The overall objective was to take advantage of automatically gathered image and textual Web data for training, in order to develop more scalable image annotation systems. In the first subtask, the participants could use for developing their annotation systems, both manually labeled data, and automatically gathered Web data. In this subtask, none of the participants were able to use the Web data to obtain a better performance than when using only manually labeled data. The participation was extremely low, there being only three groups, and it seemed that they were not able to invest much time in the problem. Due to this, few conclusions can be drawn from the results. Although it certainly cannot be stated that the Web data is simply not useful, since in subtask 2 the results were somewhat positive, suggesting that in subtask 1 also good results could be achieved. Subtask 2 consisted in using only automatically gathered Web data, and possibly additional external language resources, to develop a more scalable image annotation system. A special characteristic was that the list of concepts was different for development than for test. In this subtask the participation also low, having participated only one group. However, the obtained results were specially interesting. The submissions obtained a considerably better performance than the two provided baselines, and the results generalized well to the test set, despite the change of concept list. Furthermore, the system of the participant was specially targeted at scalability, by using an online learning method adequate for this type of problem, thus it fulfills the initial objective. On the other hand, the processing of the textual data could only be considered to be very basic, thus suggesting that a much better performance could be achieved.</p><p>Another interesting aspect of the results of subtask 2 was that when analyzing on a per concept basis, in some cases the performance was comparable to good annotation systems learned using manually labeled data. Therefore, for some concepts the Web data is considerably effective. This also suggests that one possible way to address what was proposed in subtask 1, is to do some type of fusion per concept.</p><p>Since the participation was low, and there were positive results, it would be interesting to repeat this benchmark, but making a greater effort to get more groups to participate. However, even though it is believed that in subtask 1, good results could be achieved, it is not so interesting from a scalability point of view. For a future edition it could be changed slightly. For example it could be that for the concepts where there is manually labeled data available, the annotation systems would use a combination of manual and automatically gathered data, otherwise only automatically gathered data is used.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,204.52,189.93,206.31,8.97;6,213.23,285.00,188.91,8.97"><head>( a )</head><label>a</label><figDesc>Images from a Web search query of "rainbow". (b) Images from a Web search query of "sun".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,142.12,305.92,328.04,8.97;6,146.50,210.90,69.17,69.17"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Example of images retrieved by a Web search engine for different queries.</figDesc><graphic coords="6,146.50,210.90,69.17,69.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.77,238.16,345.83,69.74"><head></head><label></label><figDesc>T , F n,m is a factor depending on the DOM (e.g. title, alt, etc.) similar to what is done in the work of La Cascia et al. [4], and d n,m is the word distance from t n,m to the image. The sigmoid function was centered at 35, had a slope of 0.15 and minimum and maximum values of 1 and 10 respectively. The resulting features include for each image at most the 100 word-score pairs with the highest scores.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,134.77,126.74,347.40,339.92"><head>Table 1 :</head><label>1</label><figDesc>The number of images for the ground truth annotations per concept for the development and test sets of subtask 2.</figDesc><table coords="8,139.24,148.83,342.93,317.84"><row><cell>Concept</cell><cell cols="2">Dev. Test</cell><cell>Concept</cell><cell cols="2">Dev. Test</cell><cell>Concept</cell><cell cols="2">Dev. Test</cell></row><row><cell>aerial</cell><cell>30</cell><cell>56</cell><cell>indoor</cell><cell cols="2">37 104</cell><cell>traffic</cell><cell>24</cell><cell>40</cell></row><row><cell cols="2">airplane/helicopter 21</cell><cell>34</cell><cell>lake</cell><cell>26</cell><cell>45</cell><cell>train/tram/metro</cell><cell>31</cell><cell>27</cell></row><row><cell>baby</cell><cell>9</cell><cell>31</cell><cell>lightning</cell><cell>9</cell><cell>16</cell><cell>tree</cell><cell cols="2">183 287</cell></row><row><cell>beach</cell><cell>34</cell><cell>45</cell><cell>logo</cell><cell>12</cell><cell>30</cell><cell>truck</cell><cell>19</cell><cell>33</cell></row><row><cell>bicycle/tricycle</cell><cell>20</cell><cell>24</cell><cell>moon</cell><cell>6</cell><cell>29</cell><cell>underwater</cell><cell>22</cell><cell>54</cell></row><row><cell>bird</cell><cell>21</cell><cell>24</cell><cell>motorcycle</cell><cell>10</cell><cell>17</cell><cell>unpaved</cell><cell>12</cell><cell>22</cell></row><row><cell>boat</cell><cell>50</cell><cell>75</cell><cell>mountain</cell><cell cols="2">85 148</cell><cell>water</cell><cell cols="2">177 280</cell></row><row><cell>book</cell><cell>20</cell><cell>20</cell><cell cols="2">music+instrument 31</cell><cell>56</cell><cell>clouds</cell><cell>138</cell><cell>-</cell></row><row><cell>bridge</cell><cell>32</cell><cell>43</cell><cell>newspaper</cell><cell>7</cell><cell>10</cell><cell cols="2">computer+generated 18</cell><cell>-</cell></row><row><cell>building</cell><cell cols="2">124 199</cell><cell>nighttime</cell><cell>25</cell><cell>36</cell><cell>drums</cell><cell>9</cell><cell>-</cell></row><row><cell>car</cell><cell>30</cell><cell>71</cell><cell>outdoor</cell><cell cols="2">135 254</cell><cell>fog</cell><cell>13</cell><cell>-</cell></row><row><cell>cartoon</cell><cell>21</cell><cell>51</cell><cell>overcast</cell><cell>24</cell><cell>27</cell><cell>highway</cell><cell>14</cell><cell>-</cell></row><row><cell>castle</cell><cell>17</cell><cell>21</cell><cell>painting</cell><cell>23</cell><cell>59</cell><cell>lying</cell><cell>5</cell><cell>-</cell></row><row><cell>cat</cell><cell>11</cell><cell>20</cell><cell>person/people</cell><cell cols="2">178 427</cell><cell>portrait</cell><cell>8</cell><cell>-</cell></row><row><cell>child</cell><cell>18</cell><cell>48</cell><cell>plant</cell><cell cols="2">64 110</cell><cell>standing</cell><cell>4</cell><cell>-</cell></row><row><cell>church</cell><cell>12</cell><cell>14</cell><cell>poster</cell><cell>6</cell><cell>12</cell><cell>stream</cell><cell>18</cell><cell>-</cell></row><row><cell>cityscape</cell><cell>58</cell><cell>79</cell><cell>protest</cell><cell>9</cell><cell>19</cell><cell>vehicle</cell><cell>12</cell><cell>-</cell></row><row><cell>daytime</cell><cell cols="2">77 191</cell><cell>rain</cell><cell>10</cell><cell>26</cell><cell>bottle</cell><cell>-</cell><cell>26</cell></row><row><cell>desert</cell><cell>16</cell><cell>23</cell><cell>rainbow</cell><cell>9</cell><cell>14</cell><cell>bus</cell><cell>-</cell><cell>38</cell></row><row><cell>dirt</cell><cell>16</cell><cell>46</cell><cell>reflection</cell><cell>52</cell><cell>60</cell><cell>chair</cell><cell>-</cell><cell>39</cell></row><row><cell>dog</cell><cell>27</cell><cell>31</cell><cell>river</cell><cell>55</cell><cell>77</cell><cell>drink</cell><cell>-</cell><cell>38</cell></row><row><cell>drawing/diagram</cell><cell cols="2">71 155</cell><cell>road</cell><cell cols="2">108 189</cell><cell>galaxy</cell><cell>-</cell><cell>16</cell></row><row><cell>droplets</cell><cell>14</cell><cell>25</cell><cell>rural</cell><cell>36</cell><cell>61</cell><cell>glass</cell><cell>-</cell><cell>75</cell></row><row><cell>elder</cell><cell>10</cell><cell>28</cell><cell>sand</cell><cell>29</cell><cell>65</cell><cell>glasses</cell><cell>-</cell><cell>31</cell></row><row><cell>embroidery</cell><cell>9</cell><cell>13</cell><cell>sculpture</cell><cell>22</cell><cell>54</cell><cell>hat</cell><cell>-</cell><cell>39</cell></row><row><cell>fire</cell><cell>28</cell><cell>31</cell><cell>sea</cell><cell>72</cell><cell>94</cell><cell>insect</cell><cell>-</cell><cell>51</cell></row><row><cell>fireworks</cell><cell>11</cell><cell>20</cell><cell>shadow</cell><cell>29</cell><cell>48</cell><cell>nebula</cell><cell>-</cell><cell>13</cell></row><row><cell>fish</cell><cell>16</cell><cell>31</cell><cell>sign</cell><cell>51</cell><cell>76</cell><cell>pencil</cell><cell>-</cell><cell>34</cell></row><row><cell>flower</cell><cell>43</cell><cell>98</cell><cell>silhouette</cell><cell>12</cell><cell>25</cell><cell>phone</cell><cell>-</cell><cell>23</cell></row><row><cell>food</cell><cell>19</cell><cell>55</cell><cell>sitting</cell><cell>8</cell><cell>18</cell><cell>pool</cell><cell>-</cell><cell>30</cell></row><row><cell>footwear</cell><cell>10</cell><cell>23</cell><cell>sky</cell><cell cols="2">197 325</cell><cell>reptile</cell><cell>-</cell><cell>32</cell></row><row><cell>forest</cell><cell>62</cell><cell>84</cell><cell>smoke</cell><cell>15</cell><cell>14</cell><cell>rodent</cell><cell>-</cell><cell>44</cell></row><row><cell>furniture</cell><cell>39</cell><cell>97</cell><cell>snow</cell><cell>43</cell><cell>74</cell><cell>space</cell><cell>-</cell><cell>72</cell></row><row><cell>garden/park</cell><cell>39</cell><cell>62</cell><cell>sports</cell><cell>25</cell><cell>86</cell><cell>submarine</cell><cell>-</cell><cell>24</cell></row><row><cell>graffiti</cell><cell>14</cell><cell>10</cell><cell>stars</cell><cell>3</cell><cell>47</cell><cell>table</cell><cell>-</cell><cell>33</cell></row><row><cell>grass</cell><cell cols="2">99 175</cell><cell>sun</cell><cell>26</cell><cell>68</cell><cell>violin</cell><cell>-</cell><cell>21</cell></row><row><cell>guitar</cell><cell>6</cell><cell>12</cell><cell>sunrise/sunset</cell><cell>33</cell><cell>45</cell><cell>wagon</cell><cell>-</cell><cell>24</cell></row><row><cell>harbor/port</cell><cell>19</cell><cell>33</cell><cell>teenager</cell><cell>12</cell><cell>22</cell><cell></cell><cell></cell><cell></cell></row><row><cell>horse</cell><cell>18</cell><cell>46</cell><cell>toy</cell><cell>21</cell><cell>27</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,134.77,126.74,345.82,217.39"><head>Table 2 :</head><label>2</label><figDesc>Results for subtask 1, (2a) best result for each group for the submissions that only used Flickr training data, and the random baseline, and (2b) all of the submissions for each group using both Flickr and Web training data.</figDesc><table coords="11,217.74,160.07,174.41,184.06"><row><cell>(a)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>MAP</cell><cell>MF1</cell></row><row><cell>Random Baseline</cell><cell>0.103</cell><cell>0.100</cell></row><row><cell>ISI 1424</cell><cell>0.708</cell><cell>0.553</cell></row><row><cell>KIDS-NUTN 1451</cell><cell>0.579</cell><cell>0.454</cell></row><row><cell>(b)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>MAP</cell><cell>MF1</cell></row><row><cell>ISI 1393</cell><cell>0.250</cell><cell>0.182</cell></row><row><cell>ISI 1398</cell><cell>0.247</cell><cell>0.181</cell></row><row><cell>ISI 1399</cell><cell>0.245</cell><cell>0.178</cell></row><row><cell>ISI 1400</cell><cell>0.241</cell><cell>0.175</cell></row><row><cell>KIDS-NUTN 1369</cell><cell>0.521</cell><cell>0.399</cell></row><row><cell>KIDS-NUTN 1370</cell><cell>0.538</cell><cell>0.397</cell></row><row><cell>KIDS-NUTN 1371</cell><cell>0.493</cell><cell>0.331</cell></row><row><cell>KIDS-NUTN 1372</cell><cell>0.528</cell><cell>0.400</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="12,134.77,126.74,345.82,240.50"><head>Table 3 :</head><label>3</label><figDesc>Annotation results for all of the submissions for subtask 2 and the Random and Co-occurrence baselines for (3a) the development and (3b) the test sets.</figDesc><table coords="12,208.34,149.11,183.81,218.13"><row><cell>(a)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>MAP</cell><cell>MF1</cell></row><row><cell>Random Baseline</cell><cell>0.084</cell><cell>0.063</cell></row><row><cell>Co-occurrence Baseline</cell><cell>0.222</cell><cell>0.168</cell></row><row><cell>ISI 1406</cell><cell>0.331</cell><cell>0.262</cell></row><row><cell>ISI 1409</cell><cell>0.336</cell><cell>0.260</cell></row><row><cell>ISI 1410</cell><cell>0.340</cell><cell>0.267</cell></row><row><cell>ISI 1413</cell><cell>0.338</cell><cell>0.266</cell></row><row><cell>ISI 1414</cell><cell>0.333</cell><cell>0.264</cell></row><row><cell>(b)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>MAP</cell><cell>MF1</cell></row><row><cell>Random Baseline</cell><cell>0.067</cell><cell>0.055</cell></row><row><cell>Co-occurrence Baseline</cell><cell>0.221</cell><cell>0.171</cell></row><row><cell>ISI 1407</cell><cell>0.315</cell><cell>0.246</cell></row><row><cell>ISI 1408</cell><cell>0.322</cell><cell>0.251</cell></row><row><cell>ISI 1411</cell><cell>0.324</cell><cell>0.252</cell></row><row><cell>ISI 1412</cell><cell>0.323</cell><cell>0.254</cell></row><row><cell>ISI 1415</cell><cell>0.321</cell><cell>0.249</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="13,134.77,126.74,345.81,338.83"><head>Table 4 :</head><label>4</label><figDesc>F1 ranges per concept when training with (4a) manually annotated data (ISI 1424), and (4b) automatically gathered Web data (ISI 1411).</figDesc><table coords="13,139.63,149.11,339.10,316.46"><row><cell>(a)</cell><cell></cell></row><row><cell>Concepts</cell><cell>F1 range</cell></row><row><cell>none, noblur, dog, fireworks, flower, partialblur, fooddrink, adult</cell><cell>0.6 ≤ F1 &lt; 1.0</cell></row><row><cell>female, outdoor, tree, bird, coast, citylife, day</cell><cell>0.5 ≤ F1 &lt; 0.6</cell></row><row><cell>male, stars, moon, car, graffiti, baby</cell><cell>0.4 ≤ F1 &lt; 0.5</cell></row><row><cell>insect, closeupmacro, cycle, fish, indoor, silhouette, cat</cell><cell>0.3 ≤ F1 &lt; 0.4</cell></row><row><cell>clearsky, rainbow, flames, lenseffect, sun, circularwarp, partylife, calm, homelife,</cell><cell>0.2 ≤ F1 &lt; 0.3</cell></row><row><cell>grass, sunrisesunset, air</cell><cell></cell></row><row><cell cols="2">child, forestpark, underwater, portrait, fogmist, horse, inactive, smoke, overlay 0.1 ≤ F1 &lt; 0.2</cell></row><row><cell>rail, reflection, overcastsky, other, motionblur, shadow, seaocean, water, big-</cell><cell>0.0 ≤ F1 &lt; 0.1</cell></row><row><cell>group, familyfriends, plant, rural, pictureinpicture, artifacts, sportsrecreation,</cell><cell></cell></row><row><cell>completeblur, happy, riverstream, lightning, mountainhill, graycolor, melan-</cell><cell></cell></row><row><cell>cholic, active, unpleasant, elderly, teenager, amphibianreptile, spider, small-</cell><cell></cell></row><row><cell>group, three, two, coworkers, strangers, desert, euphoric, scary, truckbus, lake,</cell><cell></cell></row><row><cell>snowice</cell><cell></cell></row><row><cell>(b)</cell><cell></cell></row><row><cell>Concepts</cell><cell>F1 range</cell></row><row><cell>fireworks, pencil, stars</cell><cell>0.6 ≤ F1 &lt; 1.0</cell></row><row><cell>drawing/diagram, galaxy</cell><cell>0.5 ≤ F1 &lt; 0.6</cell></row><row><cell>water, plant, desert, furniture, airplane/helicopter, beach, sun, food, guitar,</cell><cell>0.2 ≤ F1 &lt; 0.3</cell></row><row><cell>flower, train/tram/metro, boat, rainbow, silhouette, sand, glass, harbor/port</cell><cell></cell></row><row><cell>river, bus, truck, car, cat, dog, castle, fish, baby, book, chair, embroidery, sports,</cell><cell>0.1 ≤ F1 &lt; 0.2</cell></row><row><cell>child, phone, toy, garden/park</cell><cell></cell></row><row><cell>motorcycle, wagon, bottle, poster, bird, rain, sculpture, table, outdoor,</cell><cell>0.0 ≤ F1 &lt; 0.1</cell></row><row><cell>cityscape, daytime, dirt, drink, droplets, elder, glasses, graffiti, hat, indoor, in-</cell><cell></cell></row><row><cell>sect, music+instrument, nighttime, overcast, reflection, reptile, rodent, rural,</cell><cell></cell></row><row><cell>shadow, sitting, smoke, submarine, teenager, traffic, unpaved, violin</cell><cell></cell></row></table><note coords="13,139.63,363.86,339.10,8.97;13,139.63,376.71,274.49,6.97;13,139.63,384.68,155.45,6.97;13,420.44,375.21,58.28,8.97"><p>newspaper, lightning, forest, pool, fire, aerial, horse, bicycle/tricycle, protest 0.4 ≤ F1 &lt; 0.5 sky, building, nebula, mountain, cartoon, church, footwear, logo, lake, moon, grass, road, underwater, tree, snow, painting 0.3 ≤ F1 &lt; 0.4</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="14,134.76,126.74,347.35,189.49"><head>Table 5 :</head><label>5</label><figDesc>Top performing concepts according to F1 relative improvement with respect to random when training with (5a) manually annotated data (ISI 1424), and (5b) automatically gathered Web data (ISI 1411).</figDesc><table coords="14,149.63,160.07,332.49,156.16"><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell></row><row><cell>Concept</cell><cell>F1</cell><cell>R. Imp. (%)</cell><cell>Concept</cell><cell>F1</cell><cell>R. Imp. (%)</cell></row><row><cell>none</cell><cell>0.872</cell><cell>85.8</cell><cell>fireworks</cell><cell>0.703</cell><cell>69.7</cell></row><row><cell>noblur</cell><cell>0.827</cell><cell>80.8</cell><cell>pencil</cell><cell>0.692</cell><cell>68.0</cell></row><row><cell>dog</cell><cell>0.722</cell><cell>70.7</cell><cell>stars</cell><cell>0.646</cell><cell>62.8</cell></row><row><cell>fireworks</cell><cell>0.667</cell><cell>66.6</cell><cell>sunrise/sunset</cell><cell>0.565</cell><cell>54.4</cell></row><row><cell>flower</cell><cell>0.662</cell><cell>64.9</cell><cell>galaxy</cell><cell>0.500</cell><cell>49.0</cell></row><row><cell>partialblur</cell><cell>0.648</cell><cell>61.5</cell><cell cols="2">drawing/diagram 0.564</cell><cell>48.5</cell></row><row><cell>fooddrink</cell><cell>0.623</cell><cell>59.9</cell><cell>newspaper</cell><cell>0.462</cell><cell>45.8</cell></row><row><cell>adult</cell><cell>0.612</cell><cell>57.7</cell><cell>space</cell><cell>0.483</cell><cell>44.5</cell></row><row><cell>one</cell><cell>0.593</cell><cell>55.5</cell><cell>lightning</cell><cell>0.452</cell><cell>44.3</cell></row><row><cell>female</cell><cell>0.589</cell><cell>55.4</cell><cell>pool</cell><cell>0.426</cell><cell>40.8</cell></row><row><cell>outdoor</cell><cell>0.588</cell><cell>55.2</cell><cell>fire</cell><cell>0.424</cell><cell>40.6</cell></row><row><cell>bird</cell><cell>0.561</cell><cell>54.8</cell><cell>protest</cell><cell>0.400</cell><cell>39.1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,657.32,61.20,7.65"><p>www.mturk.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,144.73,646.36,169.47,7.65"><p>http://risenet.iti.upv.es/webupv250k</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="14,144.73,645.24,335.86,8.97;14,144.73,656.20,310.70,8.97"><p>Relative improvement defined as the absolute improvement divided by the difference between the baseline performance and perfect performance which for F1 is 1.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank the CLEF campaign for supporting the ImageCLEF initiative. Work supported by the <rs type="funder">Spanish MICINN</rs> under the <rs type="programName">MIPRCV Consolider Ingenio 2010 program</rs> (<rs type="grantNumber">CSD2007-00018</rs>) and by the <rs type="funder">Generalitat Valenciana</rs> under grant <rs type="grantNumber">Prometeo/2009/014</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_TSW28Xk">
					<idno type="grant-number">CSD2007-00018</idno>
					<orgName type="program" subtype="full">MIPRCV Consolider Ingenio 2010 program</orgName>
				</org>
				<org type="funding" xml:id="_GCFZDd8">
					<idno type="grant-number">Prometeo/2009/014</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="15,142.95,580.42,337.63,8.97;15,151.52,591.37,329.06,8.97;15,151.52,602.33,151.21,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="15,454.87,580.42,25.72,8.97;15,151.52,591.37,262.65,8.97">KIDS-NUTN at ImageCLEF 2012 Photo Annotation and Retrieval Task</title>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">C</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">J</forename><surname>Gaou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">W</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,434.81,591.37,45.78,8.97;15,151.52,602.33,54.50,8.97">CLEF 2012 working notes</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.95,612.83,337.64,8.97;15,151.52,623.79,150.15,8.97" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="15,234.71,612.83,170.43,8.97">WordNet An Electronic Lexical Database</title>
		<editor>Fellbaum, C.</editor>
		<imprint>
			<date type="published" when="1998-05">May 1998</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA; London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,142.95,634.28,337.63,8.97;15,151.52,645.24,329.06,8.97;15,151.52,656.20,162.83,8.97" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,246.96,634.28,233.63,8.97;15,151.52,645.24,49.37,8.97">Iterative quantization: A procrustean approach to learning binary codes</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,222.33,645.24,206.13,8.97;15,458.46,645.24,22.13,8.97;15,151.52,656.20,44.32,8.97">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011-06">2011. june 2011</date>
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
	<note>IEEE Conference</note>
</biblStruct>

<biblStruct coords="16,142.95,119.07,337.64,8.97;16,151.52,130.03,329.07,8.97;16,151.52,140.99,323.41,8.97" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,295.97,119.07,184.62,8.97;16,151.52,130.03,181.88,8.97">Combining textual and visual cues for contentbased image retrieval on the World Wide Web</title>
		<author>
			<persName coords=""><forename type="first">La</forename><surname>Cascia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,354.94,130.03,125.65,8.97;16,151.52,140.99,78.58,8.97;16,261.05,140.99,119.17,8.97">Content-Based Access of Image and Video Libraries</title>
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings. IEEE Workshop</note>
</biblStruct>

<biblStruct coords="16,142.95,151.95,337.63,8.97;16,151.52,162.91,329.06,8.97;16,151.52,173.87,329.06,8.97;16,151.52,184.83,329.06,8.97;16,151.52,195.79,232.12,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="16,304.47,151.95,176.12,8.97;16,151.52,162.91,209.26,8.97">Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2006.68</idno>
		<ptr target="http://dx.doi.org/10.1109/CVPR.2006.68" />
	</analytic>
	<monogr>
		<title level="m" coord="16,383.06,162.91,97.53,8.97;16,151.52,173.87,329.06,8.97;16,262.90,184.83,40.19,8.97">Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
	<note>CVPR &apos;06</note>
</biblStruct>

<biblStruct coords="16,142.95,206.74,337.63,8.97;16,151.52,217.70,329.07,8.97;16,151.52,228.66,329.07,8.97;16,151.52,239.62,130.39,8.97" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="16,311.08,206.74,169.50,8.97;16,151.52,217.70,125.50,8.97">The CLEF 2011 Photo Annotation and Concept-based Retrieval Tasks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Liebetrau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,151.52,228.66,204.87,8.97">CLEF 2011 Labs and Workshop, Notebook Papers</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-09">September 2011. 2011</date>
			<biblScope unit="page" from="19" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.95,250.58,337.63,8.97;16,151.52,261.54,329.07,8.97;16,151.52,272.50,178.15,8.97" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="16,245.35,250.58,235.23,8.97;16,151.52,261.54,91.62,8.97">Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1011139631724</idno>
		<ptr target="http://dx.doi.org/10.1023/A:10111396317245" />
	</analytic>
	<monogr>
		<title level="j" coord="16,250.32,261.54,90.87,8.97">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001-05">May 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.95,283.46,337.63,8.97;16,151.52,294.42,329.07,8.97;16,151.52,305.38,133.63,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="16,332.92,283.46,147.66,8.97;16,151.52,294.42,107.29,8.97">Evaluating Color Descriptors for Object and Scene Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,266.09,294.42,214.50,8.97;16,151.52,305.38,45.57,8.97">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1582" to="1596" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.95,316.33,337.63,8.97;16,151.52,327.29,329.06,8.97;16,151.52,338.25,203.15,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="16,317.18,316.33,144.48,8.97">TOP-SURF: a visual words toolkit</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,151.52,327.29,282.35,8.97">Proceedings of the 18th International Conference on Multimedia 2010</title>
		<meeting>the 18th International Conference on Multimedia 2010<address><addrLine>Firenze, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">October 25-29 2010</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.61,349.21,337.97,8.97;16,151.52,360.17,329.07,8.97;16,151.52,371.13,4.60,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="16,259.08,349.21,221.50,8.97;16,151.52,360.17,101.02,8.97">Overview of the ImageCLEF 2012 Flickr Photo Annotation and Retrieval Task</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,273.61,360.17,103.17,8.97">CLEF 2012 working notes</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.61,382.09,337.97,8.97;16,151.52,393.05,329.07,8.97;16,151.52,404.01,270.81,8.97" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="16,310.72,382.09,169.87,8.97;16,151.52,393.05,198.57,8.97">80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,357.58,393.05,123.02,8.97;16,151.52,404.01,140.27,8.97">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2008-11">nov 2008</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

<biblStruct coords="16,142.61,414.96,337.97,8.97;16,151.52,425.92,311.25,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="16,311.39,414.96,169.19,8.97;16,151.52,425.92,81.05,8.97">Efficient Image Annotation for Automatic Sentence Generation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kuniyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,254.02,425.92,69.28,8.97">ACM Multimedia</title>
		<meeting><address><addrLine>Nara, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>accepted 10</note>
</biblStruct>

<biblStruct coords="16,142.61,436.88,337.98,8.97;16,151.52,447.84,329.07,8.97;16,151.52,458.80,322.36,8.97" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="16,315.38,447.84,165.21,8.97;16,151.52,458.80,85.47,8.97">ISI at ImageCLEF 2012: Scalable System for Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muraoka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Fujisawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yasumoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Gunji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Higuchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kuniyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,258.54,458.80,103.78,8.97">CLEF 2012 working notes</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.61,469.76,337.97,8.97;16,151.52,480.72,329.06,8.97;16,151.52,491.68,329.06,8.97;16,151.52,502.63,174.69,8.97" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="16,259.48,469.76,221.10,8.97;16,151.52,480.72,53.91,8.97">Image-Text Dataset Generation for Image Annotation and Retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,355.91,480.72,124.67,8.97;16,151.52,491.68,145.24,8.97">II Congreso Español de Recuperación de Información, CERI 2012</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Berlanga</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Rosso</surname></persName>
		</editor>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">June 18-19 2012</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
		<respStmt>
			<orgName>Universidad Politécnica de Valencia</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.61,513.59,337.97,8.97;16,151.52,524.55,329.06,8.97;16,151.52,535.51,250.00,8.97" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="16,369.13,513.59,111.46,8.97;16,151.52,524.55,146.67,8.97">ARISTA -image search to annotation on billions of web photos</title>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,305.68,524.55,170.71,8.97">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.61,546.47,337.98,8.97;16,151.52,557.43,329.07,8.97;16,151.52,568.39,179.64,8.97" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="16,298.22,546.47,182.36,8.97;16,151.52,557.43,135.76,8.97">Large scale image annotation: learningtorank withjoint word-image embeddings</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-010-5198-3</idno>
		<ptr target="http://dx.doi.org/10.1007/s10994-010-5198-32" />
	</analytic>
	<monogr>
		<title level="j" coord="16,295.74,557.43,73.20,8.97">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="21" to="35" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,142.61,579.35,337.97,8.97;16,151.52,590.31,320.92,8.97" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="16,300.65,579.35,162.10,8.97">An image signature for any kind of image</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,151.52,590.31,218.33,8.97">Proc. of International Conference on Image Processing</title>
		<meeting>of International Conference on Image essing</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="409" to="412" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
