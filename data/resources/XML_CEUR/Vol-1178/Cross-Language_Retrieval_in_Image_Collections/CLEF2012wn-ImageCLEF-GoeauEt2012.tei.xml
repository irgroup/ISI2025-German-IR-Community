<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.58,115.96,334.20,12.62">The ImageCLEF 2012 Plant identification Task</title>
				<funder>
					<orgName type="full">Agropolis</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,153.76,153.65,56.55,8.74"><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">IMEDIA &amp; ZENITH teams</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,220.87,153.65,60.95,8.74"><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
							<email>pierre.bonnet@cirad.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.36,153.65,48.08,8.74"><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">IMEDIA &amp; ZENITH teams</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,350.99,153.65,65.62,8.74"><forename type="first">Itheri</forename><surname>Yahiaoui</surname></persName>
							<email>itheri.yahiaoui@univ-reims.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">IMEDIA &amp; ZENITH teams</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Laboratoire CReSTIC</orgName>
								<orgName type="institution">Université de Reims</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,433.50,153.65,28.09,8.74;1,176.52,165.61,50.25,8.74"><forename type="first">Daniel</forename><surname>Barthelemy</surname></persName>
							<email>daniel.barthelemy@cirad.fr</email>
							<affiliation key="aff2">
								<orgName type="department">BIOS Direction and INRA</orgName>
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<postCode>F-34398</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,237.33,165.61,74.03,8.74"><forename type="first">Nozha</forename><surname>Boujemaa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">IMEDIA &amp; ZENITH teams</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,341.30,165.61,93.06,8.74"><forename type="first">Jean-François</forename><surname>Molino</surname></persName>
							<email>jean-francois.molino@ird.fr</email>
							<affiliation key="aff3">
								<orgName type="laboratory">IRD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.58,115.96,334.20,12.62">The ImageCLEF 2012 Plant identification Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0C7C0F25612767328A67EB477BCC14BC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>plant</term>
					<term>leaves</term>
					<term>images</term>
					<term>collection</term>
					<term>identification</term>
					<term>classification</term>
					<term>evaluation</term>
					<term>benchmark</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ImageCLEF's plant identification task provides a testbed for the system-oriented evaluation of plant identification, more precisely on the 126 tree species identification based on leaf images. Three types of image content are considered: Scan, Scan-like (leaf photographs with a white uniform background), and Photograph (unconstrained leaf with natural background). The main originality of this data is that it was specifically built through a citizen sciences initiative conducted by Tela Botanica, a French social network of amateur and expert botanists. This makes the task closer to the conditions of a real-world application. This overview presents more precisely the resources and assessments of task, summarizes the retrieval approaches employed by the participating groups, and provides an analysis of the main evaluation results. With a total of eleven groups from eight countries and with a total of 30 runs submitted, involving distinct and original methods, this second year pilot task confirms Image Retrieval community interest for biodiversity and botany, and highlights further challenging studies in plant identification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convergence of multidisciplinary research is more and more considered as the next big thing to answer profound challenges of humanity related to health, biodiversity or sustainable energy. The integration of life sciences and computer sciences has a major role to play towards managing and analyzing cross-disciplinary scientific data at a global scale. More specifically, building accurate knowledge of the identity, geographic distribution and uses of plants is essential if agricultural development is to be successful and biodiversity is to be conserved. Unfortunately, such basic information is often only partially available for professional stakeholders, teachers, scientists and citizens, and often incomplete for ecosystems that possess the highest plant diversity. A noticeable consequence, expressed as the taxonomic gap, is that identifying plant species is usually impossible for the general public, and often a difficult task for professionals, such as farmers or wood exploiters and even for the botanists themselves. The only way to overcome this problem is to speed up the collection and integration of raw observation data, while simultaneously providing to potential users an easy and efficient access to this botanical knowledge. In this context, content-based visual identification of plant's images is considered as one of the most promising solution to help bridging the taxonomic gap. Evaluating recent advances of the IR community on this challenging task is therefore an important issue. This paper presents the plant identification task that was organized for the second year running within ImageCLEF 2012<ref type="foot" coords="2,325.83,236.97,3.97,6.12" target="#foot_0">6</ref> dedicated to the system-oriented evaluation of visual based plant identification. The task was again focused on tree species identification based on leaf images, but with more species (126 instead of 70) which is an important step towards covering the entire flora of a given region. The task was more related to to a retrieval task instead of a pure classification task in order to consider a ranked list of retrieved species rather than a single brute determination. Visual content was being the main available information but with additional information including contextual meta-data (author, date, locality name and geotag, names at different taxonomic ranks) and some EXIF data. Three types of image content were considered: leaf scans, leaf photographs with a white uniform background (referred as scan-like pictures) and unconstrained leaf's photographs acquired on trees with natural background. The main originality of this data is that it was specifically built through a citizen sciences initiative conducted by Telabotanica<ref type="foot" coords="2,353.80,392.39,3.97,6.12" target="#foot_1">7</ref> , a French social network of amateur and expert botanists. This makes the task closer to the conditions of a real-world application: (i) leaves of the same species are coming from distinct trees living in distinct areas (ii) pictures and scans are taken by different users that might not used the same protocol to collect the leaves and/or acquire the images (iii) pictures and scans are taken at different periods in the year.</p><p>2 Task resources 2.1 The Pl@ntLeaves dataset Building effective computer vision and machine learning techniques is not the only side of the taxonomic gap problem. Speeding-up the collection of raw observation data is clearly another crucial one. The most promising approach in that way is to build real-world collaborative systems allowing any user to enrich the global visual botanical knowledge <ref type="bibr" coords="2,302.74,578.45,14.61,8.74" target="#b13">[15]</ref>. To build the evaluation data of Im-ageCLEF plant identification task, we therefore set up a citizen science project around the identification of common woody species covering the Metropolitan French territory. This was done in collaboration with TelaBotanica social network and with researchers specialized in computational botany.</p><p>Technically, images and associated tags were collected through a crowd-sourcing web application <ref type="bibr" coords="3,207.54,130.95,15.50,8.74" target="#b13">[15]</ref> and were all validated by expert botanists. Several cycles of such collaborative data collection and taxonomical validation occurred.</p><p>Scans of leaves were first collected over the two summers 2009 and 2010 thanks to the work of active contributors from TelaBotanica social network. The idea of collecting only scans during these two seasons periods was to initialize training data with limited noisy background and to focus on plant variability rather than mixed plant and view conditions variability. This allowed to collect 2228 scans over 55 species. A public version of the web application<ref type="foot" coords="3,433.00,213.06,3.97,6.12" target="#foot_2">8</ref> was then opened in October 2010 and additional data were collected up to March 2011. The new collected images were either scans, or photographs with uniform background (referred as scan-like photos), or unconstrained photographs with natural background. They involved besides 15 new species from the previous set of 55 species. Since April 2011 botanists from TelaBotanica contribute regularly every month on more and more tree species (174 on leaves at the time of writing), on more localities all over France and neighbouring countries, with more growing stages with spring leaves, mature summer leaves, dry autumn leaves and evergreen leaves. This nonstop data collecting over the months introduce slowly visual and morphological variabilities.</p><p>The Pl@ntLeaves dataset used within ImageCLEF2012 finally contained 11572 images: 6630 scans, 2726 scan-like photos and 2216 photographs (see Figure <ref type="figure" coords="3,465.95,358.10,3.87,8.74">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scan Scan-like Photograph</head><p>Fig. <ref type="figure" coords="3,169.94,496.69,4.13,7.89">1</ref>. The 3 image types illustrated with the same species Celtis australis L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pl@ntLeaves metadata</head><p>Each image of Pl@ntLeaves dataset is associated with the following meta-data:</p><p>-Date date and time of shot or scan -Type (acquisition type: Scan, Scan-like or Photograph) -Content type (see figure3):</p><p>• Leaf (tagged by default for all images, supposing that a picture is focused one single leaf),</p><p>• Picked leaf most of the time mature leaf -not dry -on the floor,</p><p>• Upper side and Lower side involving generally 2 images from one leaf,</p><p>• Branch for some pictures containing enumerable leaves,</p><p>• Leafage. -Taxon full taxon name according the botanical database[2](Regnum, Class, Subclass, Superorder, Order, Family, Genus, Species) -VernacularNames English commun name -Author name of the author of the picture -Organization name of the organization of the author -Locality locality name (a district or a country division or a region) -GPSLocality GPS coordinates of the observation These meta-data are stored in independent xml files, one for each image. Figure <ref type="figure" coords="4,134.77,268.56,4.98,8.74">2</ref> displays an example image with its associated xml data. Additional but partial meta-data information can be found in the image's EXIF, and might include the camera or the scanner model, the image resolution and dimension, the optical parameters, the white balance, the light measures, etc. Fig. <ref type="figure" coords="4,186.26,488.23,4.13,7.89">2</ref>. An image of Pl@ntLeaves dataset and its associated metadata</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pl@ntLeaves variability</head><p>The main originality of Pl@ntLeaves compared to previous leaf datasets, such as the Swedish dataset <ref type="bibr" coords="4,224.47,572.43,14.61,8.74" target="#b20">[22]</ref>, the ICL dataset <ref type="bibr" coords="4,318.00,572.43,10.52,8.74" target="#b0">[1]</ref> or the Smithsonian one <ref type="bibr" coords="4,435.94,572.43,9.96,8.74" target="#b1">[3]</ref>, is that it was built in a collaborative manner through a citizen sciences initiative. This makes it closer to the conditions of a real-world application: (i) leaves of the same species are coming from distinct trees living in distinct areas (ii) pictures and scans are taken by different users that might not used the same protocol to collect the leaves and/or acquire the images (iii) pictures and scans are taken at different periods in the year. Intra-species visual variability and view conditions variability are therefore more stressed-out which makes the identification more realistic but more complex. Figures 4 to 11 provide illustrations of the intraspecies visual variability over several criteria including growing stages, colors, global shape, margin appearance, number and relative positions of leaflets, compound leaf structures and lobe variations. On the other side, Figure <ref type="figure" coords="5,425.33,308.63,9.96,8.74">12</ref> illustrates the light reflection and shadows variations of Scan-like photos. It shows that this acquisition protocol is actually very different than pure scans. Both share the property of a limited noisy background but Scan-like photos are more complex due to the lighting conditions variability (flash, sunny weather, etc.) and the unflatness of leaves. Finally, the variability of unconstrained photographs acquired on the tree and with natural background is definitely a much more challenging issue as illustrated in Figure <ref type="figure" coords="5,261.65,392.32,8.49,8.74">13</ref>. 3 Task description</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training and Test data</head><p>The task was evaluated as a supervised classification problem with tree species used as class labels. A part of Pl@ntLeaves dataset was provided as training data whereas the remaining part was used later as test data. The two subsets were created with the respect to the individual plants: pictures of leaves belonging to the same individual tree cannot be split across training and test data. This prevents identifying the species of a given tree thanks to its own leaves   and that makes the task more realistic. In a real world application, it is indeed much unlikely that a user tries to identify a tree that is already present in the training data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Task objective and evaluation metric</head><p>The goal of the task was to associate the correct tree species to each test image. Each participant was allowed to submit up to 3 runs built from different methods. Compared to last year the task was more related to plant species retrieval than a pure classification problem. This year evaluation metric was slightly modified in order to consider a ranked list of retrieved species rather than a single brute determination. Each test image was attributed with a score between 1 and 0 equals to the inverse of the rank of the correct species. An average normalized score is then computed on all test images. A simple mean on all test images would indeed introduce some bias with regard to a real world identification system. Indeed, we remind that the Pl@ntLeaves dataset was built in a collaborative manner. So that few contributors might have provided much more pictures than many other contributors who provided few. Since we want to evaluate the ability of a system to provide correct answers to all users, we rather measure the mean of the average classification rate per author. Furthermore, some authors sometimes provided many pictures of the same individual plant (to enrich training data with less efforts). Since we want to evaluate the ability of a system to provide the correct answer based on a single plant observation, we also decided to average the classification rate on each individual plant. Finally, our primary metric was defined as the following average classification score S:</p><formula xml:id="formula_0" coords="8,234.63,592.62,245.97,31.28">S = 1 U U u=1 1 P u Pu p=1 1 N u,p Nu,p n=1 s u,p,n<label>(1)</label></formula><p>U : number of users (who have at least one image in the test data) P u : number of individual plants observed by the u-th user N u,p : number of pictures taken from the p-th plant observed by the u-th user s u,p,n : score between 1 and 0 equals to the inverse of the rank of the correct species for the n-th picture taken from the p-th plant observed by the u-th user</p><p>It is important to notice that while making the task more realistic, the normalized classification score also makes it more difficult. Indeed, it works as if a bias was introduced between the statistics of the training data and the one of the test data. It highlights the fact that bias-robust machine learning and computer vision methods should be preferred to train such real-world collaborative data. Finally, to isolate and evaluate the impact of the image acquisition type (Scan, Scan-like, Photogragh), a normalized classification score S was computed for each type separately. Participants were therefore allowed to train distinct classifiers, use different training subsets or use distinct methods for each data type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Participants and techniques</head><p>A total of 11 groups submitted 30 runs, which confirms the successful participation rate of the previous year task on a new topic. Participants are mainly academics, specialized in computer vision and multimedia information retrieval, coming from all around the world: Australia (1), Brazil (1), China (1), France (3), Germany (2), India (1), Italy (1) and Turkey <ref type="bibr" coords="9,332.15,356.98,11.62,8.74" target="#b0">(1)</ref>. We list below the participants and give a brief overview of the techniques they used to run the plant identification task. We remind here that ImageCLEF benchmark is a system-oriented evaluation and not a formal evaluation of the underlying methods. Readers interested by the scientific and technical details of any of these methods should refer to the ImageCLEF 2012 working note of each participant.</p><p>ArTe-Lab (1 run) <ref type="bibr" coords="9,230.68,440.89,17.82,8.77" target="#b12">[14]</ref> In their preliminary evaluations, these participants experiment several features and classification methods. They were guided by the wish to design a mobile application and thus aimed to have a good compromise between accuracy and computing time and cost. They retained the Pyramid of Histograms of Orientation Gradients (PHOG) and a variant of the HAAR descriptor evaluated as the most stable and robust features in their preliminary tests. They chose a multi-class probability estimation based on Support Vector Machine (SVM) classifiers with a one-vs-one, or pairwise comparisons, strategy. They choose to train a single SVM model for all 3 categories of images, without distinguish Scan, Scan-like and Photograph.</p><p>Brainsignals (1 run) <ref type="bibr" coords="9,243.88,572.40,17.82,8.77" target="#b14">[16]</ref> The aim of these participants was to focus less on possible computer-vision related techniques (especially without costly image preprocessing and feature extraction), and the much as possible to deal with machine learning techniques. First, they binarized the images using the Otsu algorithm, in order to extract two features: global shape features (from lateral projections), and local features (histograms of content types of small rectangular patches of bits, with and without sub-sampling). A Random Forest Classifier <ref type="bibr" coords="9,469.07,644.16,11.53,8.74" target="#b6">[8]</ref> was then used with 100 trees trained with several cross-validation steps on the whole training data without distinguish Scan, Scan-like and Photograph categories.</p><p>BTU DBIS (4 runs) <ref type="bibr" coords="10,240.04,154.83,17.82,8.77" target="#b8">[10]</ref> This team used a rather non-typical approach not relying on classification techniques. Logical combinations of low-level features are expressed in a query language, the Commuting Quantum Query Language <ref type="bibr" coords="10,457.54,178.77,18.44,8.74" target="#b19">[21]</ref>, and used to assess a document's similarity to a species. The principle is to combine similarity predicates as found in information retrieval and relational predicates common in databases. They tested 4 distinct methods without distinguishing the 3 categories Scan, Scan-like and Photograph. Their preliminary evaluations suggested to use MPEG7 Color Structure Descriptor eventually combined with GPS information. The first two runs explored two Query By Example based approaches, with or without GPS information. As a third approach, they combined the second approach with a Top-k Clustering (or "K-methoid") techniques which exploits relationships within the top-k results. The last run used a pure clustering image approach with GPS information, including the whole train and test datasets.</p><p>IFSC USP (3 runs) <ref type="bibr" coords="10,235.46,334.16,17.82,8.77" target="#b9">[11]</ref> Mainly focused on automatic shape boundary analysis, these participants adapted their approach according the image content and obtained the best results for the Photograph with a semi-automatic approach. Full-automatic approaches were used for Scan and Scan-like, but also on Photograph for one run. They started by an automatic leaf contour extraction with Otsu's method on Scan and Scan-like, while they used a k-means clustering algorithm in the RGB colorspace for Photograph. The semi-automatic approach on Photograph needs that a user marks leaf and background on regions automatically detected by a Mean Shift algorithm. Then, a merging process labelled gradually all content of the picture with these 2 parts. Contour were described with complex network <ref type="bibr" coords="10,225.88,453.74,13.06,8.74" target="#b3">[5]</ref>, volumetric fractal dimension [?] and geometric parameters. Gabor filters, and local binary patterns were used too. They included for each run the GPS information. Classification was performed by using Linear Discriminant Analysis. The first run used all samples from the 3 images categories for the training stage, which paid off comparing to the second run where only Scan and Scan-like were used. Only Gabor and GPS features were used for the automatic run on Photograph because contour extraction were sufficiently reliable.</p><p>Inria IMEDIA (3 runs) <ref type="bibr" coords="10,254.69,561.31,12.09,8.77" target="#b4">[6]</ref> These participants used distinct approaches. For Scan and Scan-like, they applied a late fusion of an approach based on shape boundary features and a large-scale matching approach. The second run used an advanced shape context descriptor combining boundary shape information and local features within a matching approach. These two runs used a top-K decision rule as classifier, while the third used a multi-class SVM technique on contour based descriptors with a one-vs-one schema and a linear kernel. For Photograph, local features around constrained Harris points in order to reduce the impact of the background, but automatic segmentation with a rejection criterion was attempted in order to extract shape features when possible. Two first runs used matching and top-K classifier. Last run used a method were Harris points were associated to bounding boxes detected as images' interesting zones; each box proposed a classification based on a multi-class SVM and a voting scheme unifying their responses. All preliminary evaluations used cross validation without splitting images from a same individual plants.</p><p>LIRIS ReVes (3 runs) <ref type="bibr" coords="11,247.92,214.61,17.82,8.77" target="#b10">[12]</ref> The method used by these participants is clearly oriented mobile application and is designed to cope with the challenges of complex natural images and to enable a didactic interaction with the user. The system first performs a two-step segmentation of the image, fully automatic on plain background images, and only guided by a rough coloring on photographs. It relies on the evaluation of a leaf model representing the global morphology of the leaf of interest, which is then used to guide an active contour towards the actual leaf margin. High-level geometric descriptors, inspired by the criteria used by botanists, are extracted on the obtained contour, along with more generic shape features, making a semantic interpretation possible. All these descriptors are then combined in a Random Forest classification algorithm <ref type="bibr" coords="11,405.23,334.19,12.78,8.74" target="#b6">[8]</ref>.</p><p>LSIS DYNI (3 runs) <ref type="bibr" coords="11,241.63,358.07,17.82,8.77" target="#b17">[19]</ref> These participant used a modern computer vision framework involving feature extraction coupled with Spatial Pyramidal Matching for local analysis and large-scale supervised classification based on linear SVM with the 1-vs-all multi-class strategy. For all submitted runs, they do not consider the three image categories. They notably obtained good results on Scan and Scan-like without any segmentation and/or specific pre-processing, and they obtained the 3 best results for Photograph among all full-automatic approaches with a significant difference. The differences between the runs concerns the choice of features and their combination (in late fusion schema). They used a Multiscale and Color extension of the Local Phase Quantization <ref type="bibr" coords="11,384.06,465.69,17.65,8.74" target="#b15">[17]</ref>, dense Multiscale (Color) Improved Local Binary Patterns with Sparse Coding of patches <ref type="bibr" coords="11,440.48,477.65,16.90,8.74" target="#b22">[24]</ref>, and dense SIFT feature with Sparse Coding too. More they combined the descriptors, more they obtained better performances.</p><p>Sabanci-Okan (2 runs) <ref type="bibr" coords="11,254.27,525.44,17.82,8.77" target="#b23">[25]</ref> Mainly focused by automatic shape boundary analysis, these participants adapted their approach according the image content and obtained the best results for Scan and Scan-like. They used a balanced combination of 2 kinds of classifications, a Local Features Matcher and SVM classifiers. Only Scan and Scan-like images were used as a training dataset. They used two distinct segmentation methods: Otsu's method for Scan, Scan-like, and two methods based on watershed transform for Photograph (full or semiautomatic). The Local Feature Matcher exploited local features (shape context, HOG, local curvature) from salient points on the boundary of the leaf. The SVM classifiers used texture and optionally shape features if test image were from the category Scan, or Scan-like or interactively segmented Photograph. For automat-ically segmented Photograph test images, the leaf boundary extractions were not sufficiently reliable for using shape information, and thus they used texture features for re-training the classifiers. These participants are among the rare ones to mention that they used cross validation without splitting images from a same individual plants in order to avoid over fitting problems.</p><p>The Who IITK (4 runs) <ref type="bibr" coords="12,265.65,190.69,12.09,8.77" target="#b2">[4]</ref> One originality is that these participants redefined 3 categories taking into account morphological properties of leaves: Simple leaf in Scan or Scan-like, Compound leaf in Scan or Scan-like and Photograph. For the "Simple" category, they developed an automatic segmentation method based on Otsu working on two steps in order to remove shadows, noisy background and the petiole of the leaf since it can affect the shape. For the "Compound" category, they used a combination of polynomial curve fitting and minimum-bounding-ellipse scores in order to extract the contour of the "most" representative leaflet. For Photograph, they developed a GrabCut <ref type="bibr" coords="12,422.30,286.37,15.50,8.74" target="#b18">[20]</ref> based interactive segmentation technique. Shapes were then described with complex network <ref type="bibr" coords="12,158.52,310.28,10.52,8.74" target="#b3">[5]</ref> and geometric parameters, and extracted specialized features on margin and venation. In order to fully automatise the process, they designed a Simple/Compound leaf classifier for test images. Note that on some "complex" photos where contour was too difficult to extract, like bipinnate compound leaves, they chose to use SURF with a bag-of-word technique. Random Forest Classification <ref type="bibr" coords="12,164.77,370.05,10.52,8.74" target="#b6">[8]</ref> for each category was finally used for the species prediction. The run giving the best result used in addition the Smote algorithm <ref type="bibr" coords="12,392.79,382.01,15.50,8.74" target="#b11">[13]</ref> designed to deal with unbalanced classification problems.</p><p>ZhaoHFUT (3 runs) <ref type="bibr" coords="12,245.35,417.84,17.82,8.77" target="#b24">[26]</ref> These participants investigated a linear Spatial Pyramidal Matching using Sparse coding (ScSPM) since this kind of approach achieve very good performance in object recognition. Indeed, SPM extends popular bag-of-features approach by taking account the spatial order of local appearances. They used the same structure in <ref type="bibr" coords="12,329.78,465.69,15.50,8.74" target="#b22">[24]</ref> and they tested thought the 3 runs the spatial encoding of dense SIFT, dense flip-SIFT descriptors (a SIFT extension considering leaf symmetry), and a late fusion of the two approaches. The sparse encoded features are pooled among several scales as spatial pyramid histogram representations, and these last representations are used as input of SVM classifiers tuned by cross-validations. The species predictions are formed by normalizing the degrees of attribution in different SVMs.</p><p>Table <ref type="table" coords="12,162.90,561.34,4.98,8.74" target="#tab_2">2</ref> attempts to summarize the methods used at different stages (feature, classification, subset selection,...) in order to highlight the main choices of participants. This table should be used in next section on result analysis, in order to see if there is some common techniques which tend to lead to good performances.   <ref type="table" coords="14,182.32,201.32,4.98,8.74">3</ref> finally presents the same results but with detailed numerical values (in that case a run is labelled as automatic only if all image types were processed fully automatically). Best runs: Although scores are more uniform across the three image types, there is still no run this year achieving the best performances on the three types. There is even no run in the top-3 of each image type. Sabanci-Okan runs achieved very good results on both Scan (1st) and Scan-like (2nd). TheWho runs achieve good results across the 3 categories but do not obtain the best performances on any of them. IFSC USP run1 obtained the best performance on Photograph category using semi-supervised segmentation but gets modest performances on Scan. Similarly, INRIA run 2 obtained the best performance on Scan-like but gets mitigated performances on Scan and Photograph. Finally, the 3rd run of LSIS DYNI is interesting even if it is positioned only at the 9th rank when looking at the average score across all categories: it actually obtained the best fully automatic performances on Photograph category which is actually the most challenging task. A normalized recognition rate of 32% on that  <ref type="table" coords="15,164.69,505.45,4.13,7.89">3</ref>. Normalized classification scores for each run and each image type. HA = humanly assisted, Auto=Full automatic. Top results per image type are highlighted in bold category makes an automatic real-world application more realistic (whereas last year's automatic runs on photographs did not exceed 20% and with less species).</p><p>Impact of the number of species: A first general remark is that the scores are globally lower than the one obtained during the 2011 campaign whereas the new evaluation metric of the 2012 campaign is more advantageous for participants (only the top-1 specie contributed to the score within the old metric whereas the new one allows retrieving the right species at larger ranks). That means that the difficulty of the task increased, and that the technological progress achieved by the participants did not compensate the increased difficulty. The main change between the two campaigns being the number of species covered by the task (from 70 to 126 species), it is very likely that this growth explains the overall lower performances. More classes actually involve more confusion. From a botanical point of view, the 2012 dataset actually contains more pairs of morphologically similar species. The species Betula pendula Roth was for instance quite well recognized in many runs during the 2011 campaign but is this year often confused with other species, probably Betula pubescens Ehrh. or Betula utilis var. jacquemontii sharing several morphological attributes.</p><p>Impact of the image type: Another global remark is that the scores of the 2012 campaigns are more uniform across the different image types than the year before. So that it is not true anymore that the performances are degrading with the complexity of the acquisition image type. This might be due to several reasons: (i) many species in the training set are relatively to other types more populated with Scan-like images than last year, (ii) several participants did use semi-supervised approaches to extract leaf contours in the Photograph category, (iii) participants may have work more on improving their methods for Scan-like and Photograph (since scans were identified more easily in the 2011 campaign).</p><p>About full automatic approaches and using semi-supervised segmentation: Every runs were performed with full automatic methods on Scan and Scan-like, with or without automatic image segmentation, revelling that all teams conceive that the problem is solvable in a long term. But with a max of 0.58, and average and a median around 0, 32 for Scan, there is still an important room of improvement especially when all tree species from a given flora (like around the 500 to 600 tree species in France) will be considered. Concerning Photograph, results demonstrated that a quite accurate leaf contour extraction in all training dataset performed with semi-supervised segmentation approaches enable to tend towards equivalent performances for Scan and Scan-like like for the IFSC-USP or TheWho teams. However it is not the case for Sabanci-Okan semi-automatic run, maybe because they performed semisupervised segmentation on test-images only while learning models on Scan and Scan-like. In the other side, IFSC-USP team obtained lower results on Scan and Scan-like than on Photograph which maybe highlights that accurate leaf contour is required. Moreover, it is difficult to conceive an approach needing tens or even hundreds thousands of interactive leaf segmentations of a dataset on a entire flora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>About matching and shape boundary approaches:</head><p>The most frequently used class of methods is shape boundary analysis (16 runs among 30 used), exclusively or not) which is not surprising since state-of-the-art methods addressing leaf-based identification in the literature are mostly based on leaf segmentation and shape boundary features <ref type="bibr" coords="17,260.43,585.25,10.52,8.74" target="#b5">[7,</ref><ref type="bibr" coords="17,272.60,585.25,12.73,8.74" target="#b16">18,</ref><ref type="bibr" coords="17,287.00,585.25,7.75,8.74" target="#b7">9,</ref><ref type="bibr" coords="17,296.41,585.25,12.73,8.74" target="#b21">23,</ref><ref type="bibr" coords="17,310.80,585.25,7.01,8.74" target="#b3">5]</ref>. Last year, one important remark was that more generic image retrieval approaches like matching could provide better results especially on Scan. But, rather than opposing these two approaches, several participants combined them with success. This suggests that modelling leaves as part-based rigid and textured objects might be complement to shape boundary approaches that do not characterize well margin details, ribs or limb texture.</p><p>Impact of the training subsets and cross-validations: Considering test images of Scan and Scan-like categories, we can say it was preferable to exclude Photograph images from training dataset and to not distinguish Scan and Scanlike during preliminary evaluations. But for Photograph test images, this rule seems not to be confirmed if we look the best performances obtained automatically by LSIS DYNI who used all images in the training dataset. Moreover, one can note that Sabanci-Okan and Inria teams seem to have take benefit from the respect of individual-plant during their cross-validation. Indeed, they mentionned that they do not split images from one same individual plant, in order to avoid overfitting problems, because images from a one same plants (and thus one same event) are very similar. This approach could be one key of the success of these runs independently to the relevance of the methods. About using metadata: Using geo-tags to help the identification has been tried again this year by some participants (IFSC USP, THEWHO and DBIS). Unfortunately, this year again, the results show that adding GPS information in the identification process is likely to degrade the performances. Best runs of TheWho and DBIS are actually the ones that did not use any additional metadata complementary to their baseline visual-based technique. The main reason is probably that the geographic spread of the data is limited (French Mediterranean area). So that most species of the dataset might be identically and uniformly distributed in the covered area. Geo-tags would be for sure more useful at a global scale (continent, countries). But at a local scale, the geographical distribution of plants is much more complex. It usually depends on localized environmental factors such as sun exposition or water proximity that would require much more data to be modelled. We have to note that some potentially relevant information were not or very few explored. The content tags (Leaf, Picked leaf, Leafage) for instance were only used by Inria Team. Date could have been used in order to identify evergreen plants and improve performances on these species. None of teams explored either the hierarchical taxonomy structure which could be a source of improvement. But some participants like The Who and ReVes LIRIS teams, created their own metadata about the leaf organisation (Simple or Compound ) in order to treat differently these two categories of leaves, which seems to have paid off for improving performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performances per species</head><p>This section aims to analyse, considering the results of the various approaches, how and which part of the task were difficult. We analyse which species, images and kind of leaves according to morphological attributes, were easy or very hard to identify. We attempt here to evaluate which species are more difficult to identify than others. We focus only on Scan in order to limit view conditions bias. We computed some basic statistics (mean, median, max) on the score by species over the runs of all participants. Figure <ref type="figure" coords="19,292.49,142.90,9.96,8.74">17</ref> shows these statistics by species with a decreasing order of the max score values. We can note that few species enable to obtain good performances over all approaches. For around one third there is a least one approach which performed perfectly on all images of theirs associated species. Most of them were species yet contained in the previous year dataset, and thus most of the time with more individual-plants and images. Figure <ref type="figure" coords="19,470.63,202.68,9.96,8.74" target="#fig_8">18</ref> shows in addition how are related max scores to the number of individual plant by species and it indicates clearly how more a species has some individual plants (as existing species previous year) more the identification of the species is reachable by most of the methods. For species with a small number of individual plants it is more difficult to understand precisely the score variations. They can be due to morphological variations but also to different view conditions (we will see it next subsections). Coming back to figure <ref type="figure" coords="19,234.37,298.32,8.49,8.74">17</ref>, for the other two-thirds of species, none of the tested methods perform perfectly. However, by comparing max and mean scores, we can say that there is each time on method performing distinctly better than the other. It is encourageing for all teams, beacuse it means there is for all these species a room of improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performances per image</head><p>To qualitatively assess which kind of images causes good results and which one makes all methods failed, we sorted all test pictures by the rate of succesfull runs in which they were correctly identified (where the correct species was retrieved at first rank). Figure <ref type="figure" coords="19,213.33,439.41,9.96,8.74" target="#fig_9">19</ref> attempts to represent that for each image category and with a decreasing order. First we can note that that Scan and Scan-like have more or less the same distribution relatively to their number of images, confirming that these two categories have a same level of difficulty in spite of the great variety of the methods experimented through the run. Moreover, the figure confirms that Photograph is definitely the most difficult category: a handful of photographs has a rate of successful runs over 0.5 while at the same time around one third of Scan and Scan-like images have it. 260 images were not correctly identified by any run with a majority of unconstrained photos (63 Scan, 27 Scan-like, 168 Photograph.</p><p>Figure20 displays 4 very well identified images with at least a rate of 0.8 successful run). They all are very standard leaf images similar to the one found in books illustrations.</p><p>Concerning Photograph, the three images with the higher rate of successful runs are showed in first line in figure21. One explication of these relative good rates can be that the associated species contains after 3 years numerous images and individual plants now, but also that these pictures were well framed, with a close-up on the whole leaf. In comparison photos from the same species in second line of figure21 have a very low, even 0, rate of successful runs: we can note that  these pictures are not well framed, too small, too dark, or with a part of the leaf out of the frame. . . However, we have to be careful with the "easy" or "difficult" images ccording to the methods used in the runs. We notice that numerous of the "easiest" images can be considered "too much easy" because on some species we can have images from a same "event" in the training and the test dataset. Indeed, we remind that we split the Pl@ntLeaves II dataset with the respect of individual plants, i.e. pictures of leaves belonging to the same individual tree cannot be split across training and test data. This prevents identifying the species of a given tree thanks to its own leaves and that makes the task more realistic. In a real world application, it is indeed much unlikely that a user tries to identify a tree that is already present in the training data. However, some individual plants from a same species are closely related by one same "event" because they were observed the same day, by the same contributor, and images were collected with the same device, with similar pose, framing and conditions. Moreover, the individual plants from a same "event" are observed at the same place and can be parent tree and/or present the same morphological attributes in function of the environment. In other words, these individual plants can be considered as one same individual-plant, and Since we not split the dataset with the respect of the notion of "event", it still have some bias between test and training datasets, as as it can be observed in figure <ref type="figure" coords="23,281.39,202.68,8.49,8.74" target="#fig_11">22</ref>. This notion of "event", instead of "individual-plant", should be considered in a next plant identification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper presented the overview and the results of ImageCLEF 2012 plant identification testbed following the pilot one in 2011. The number of participants increased from 8 to 11 groups showing an increasing interest in applying multimedia search technologies to environmental challenges (in 8 distinct countries). The main change between the two campaigns was the number of species covered by the task which was increased from 70 to 126 species. The technological progress achieved by the participants did unfortunately not compensate this increased difficulty since the overall performances degrade. This shows that scaling state-of-the-art plant recognition technologies to a real-world application with thousands of species might still be a difficult task. On the other side, a positive point was that the performances obtained on unconstrained pictures of leaves with non-uniform background were better than last year. The use of semi-supervised segmentation techniques has notably been a key element towards boosting performances showing that interactivity might play an important role towards successful applications, typically on mobile phones. But, the best automatic method also provided very honourable results showing that the door is not closed for a fully automatic identification. Interestingly this method was a generic visual classification technique without any specificity related to plants. Regarding the other image types, the best method on scans used a combination of many leaf boundary shape and texture features trained with a SVM classifier. The best run on pseudo-scans (photographs with uniform backgrounds) was achieved by another technology purely based on shape-context features that are already known to provide good performances for leaf recognition in the literature. Finally, using GPS data in the recognition process was still not beneficial probably because the geographic spread of the data is limited. A possible evolution for a new plant identification task in 2013 is to extend the task to multiple organs including flowers, trunks and fruits.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,134.77,514.13,345.83,7.89;5,134.77,525.12,146.84,7.86;5,351.06,424.86,83.00,69.17"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Four leaves from distinct plants of one same species (Platanus x hispanica Wild) collected at different growing stages.</figDesc><graphic coords="5,351.06,424.86,83.00,69.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,162.94,195.56,289.48,7.89;6,172.03,115.84,271.30,69.16"><head>Fig. 5 .Fig. 6 .Fig. 7 .Fig. 8 .Fig. 9 .</head><label>56789</label><figDesc>Fig. 5. Color variation of Cotinus coggygria Scop. (Eurasian smoketree)</figDesc><graphic coords="6,172.03,115.84,271.30,69.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="7,134.77,195.11,345.82,7.89;7,134.77,206.10,168.03,7.86;7,289.01,231.89,53.49,69.17"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Two compound leaves organisations of one same plant of Gleditsia triacanthos L. (Honey locust): pinnate and bipinnate.</figDesc><graphic coords="7,289.01,231.89,53.49,69.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,134.77,311.67,345.83,7.89;7,134.77,322.65,104.29,7.86;7,345.29,231.89,63.17,69.17"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Lobes variation of Broussonetia papyrifera (L.) L'Hér. ex Vent. (Paper mulberry) on one same plant.</figDesc><graphic coords="7,345.29,231.89,63.17,69.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,134.77,428.18,345.82,7.89;7,134.77,439.16,124.44,7.86;7,239.69,348.45,135.98,69.16"><head>Fig. 12 .Fig. 13 .</head><label>1213</label><figDesc>Fig. 12. Light reflection and shadows variation of scan-like photos of Magnolia Grandiflora L. (Southern Magnolia)</figDesc><graphic coords="7,239.69,348.45,135.98,69.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="14,134.77,165.45,345.82,8.74;14,134.77,177.41,345.82,8.74;14,134.77,189.37,345.82,8.74;14,134.77,201.32,345.83,8.74;14,134.77,213.28,345.82,8.74;14,134.77,225.23,88.83,8.74;14,134.77,267.19,345.80,212.83"><head>Figures 14 ,</head><label>14</label><figDesc>Figures 14,<ref type="bibr" coords="14,185.94,165.45,9.96,8.74" target="#b13">15</ref> and 16 present the normalized classification scores of the 30 submitted runs for each of the three image types. Note that two colors are used in the graphs to distinguish fully automatic methods from humanly assisted methods. Table3finally presents the same results but with detailed numerical values (in that case a run is labelled as automatic only if all image types were processed fully automatically).</figDesc><graphic coords="14,134.77,267.19,345.80,212.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="14,243.49,476.41,128.39,7.89"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Scores for Scan images</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="16,235.03,325.05,145.29,7.89;16,134.77,115.83,345.80,212.83"><head>Fig. 15 .Fig. 16 .</head><label>1516</label><figDesc>Fig. 15. Scores for Scan-like images</figDesc><graphic coords="16,134.77,115.83,345.80,212.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="21,134.77,330.65,345.83,7.89;21,134.77,341.63,71.53,7.86;21,152.06,115.84,311.24,190.08"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. Max score over the runs by number of individual-plants for each species (scan test images only).</figDesc><graphic coords="21,152.06,115.84,311.24,190.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="21,134.77,572.75,345.83,7.89;21,134.77,583.73,236.25,7.86;21,136.49,363.84,342.37,184.18"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. Distribution of the rates of successful runs for each test-image, re-arranged by the three image categories and with a decreasing order.</figDesc><graphic coords="21,136.49,363.84,342.37,184.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="22,171.25,227.08,272.86,7.89;22,293.79,251.30,67.44,89.91"><head>Fig. 20 .Fig. 21 .</head><label>2021</label><figDesc>Fig. 20. 4 well identified images (more than 24/30 successful runs).</figDesc><graphic coords="22,293.79,251.30,67.44,89.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="23,134.77,363.48,345.83,7.89;23,134.77,374.46,345.83,7.86;23,134.77,385.42,345.82,7.86;23,134.77,396.38,324.74,7.86;23,136.16,235.45,63.78,85.04"><head>Fig. 22 .</head><label>22</label><figDesc>Fig. 22. One test image identified by a high rate of 0.9 successful runs, but, which can be considered "too easy" because the only individual plants of the associated species Rhus coriaria L. in the training dataset were observed by a same contributor (DB), the same day (12/10/11), and probably at the same place in Monferrier-sur-Lez.</figDesc><graphic coords="23,136.16,235.45,63.78,85.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,134.77,608.30,345.82,56.56"><head>Table 1 .</head><label>1</label><figDesc>The training subset was built first by integrating the whole Im-ageCLEF2011 plant task dataset. New pictures were then added to the training and test data, by selecting randomly new individual plants, in a such way to reach so far as possible a ratio around 2 individual plants in the training data for 1 individual plant in the test data. Detailed statistics of the composition of the training and test data are provided in 1. Statistics of the composition of the training and test data</figDesc><table coords="8,173.02,162.13,269.31,99.15"><row><cell></cell><cell></cell><cell cols="3">Pictures Individual plants Contributors</cell></row><row><cell>Scan</cell><cell>Train Test</cell><cell>4870 1760</cell><cell>310 157</cell><cell>22 10</cell></row><row><cell>Scan-like</cell><cell>Train Test</cell><cell>1819 907</cell><cell>118 85</cell><cell>8 10</cell></row><row><cell>Photograph</cell><cell>Train Test</cell><cell>1733 483</cell><cell>253 213</cell><cell>22 25</cell></row><row><cell>All</cell><cell>Train Test</cell><cell>8422 3150</cell><cell>681 455</cell><cell>29 38</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="13,134.77,127.53,345.83,517.32"><head>Table 2 .</head><label>2</label><figDesc>Approaches used by participants. In training subset column All means that the 3 categories where not distinguished, while AllScans means Scan and Scan-like.</figDesc><table coords="13,134.77,127.53,345.83,517.32"><row><cell>S/C IP</cell><cell></cell><cell cols="2">× ×</cell><cell></cell><cell cols="2">× ×</cell><cell></cell><cell cols="2">× ×</cell><cell></cell><cell></cell><cell cols="2">× ×</cell><cell></cell><cell></cell><cell cols="3">× √</cell><cell></cell><cell></cell><cell cols="2">√ ×</cell><cell></cell><cell></cell><cell>× ×</cell><cell>× √</cell><cell>√ ×</cell><cell>× ×</cell></row><row><cell>Classification Training</cell><cell>subset(s)</cell><cell cols="2">SVMs, one-vs-one All</cell><cell></cell><cell cols="2">Random Forests All</cell><cell></cell><cell cols="2">Query By Example All</cell><cell></cell><cell cols="4">Linear Discrimi-All AllScans nant Analysis</cell><cell>Large scale match-</cell><cell cols="2">ing All</cell><cell cols="2">+knn classifier AllScans</cell><cell cols="2">SVM one-vs-one Scan</cell><cell>Random Forests Scan-like</cell><cell>Photo.</cell><cell></cell><cell>Linear SVM 1-vs-all All</cell><cell>multi-class strategy</cell><cell>SVM classifiers AllScans</cell><cell>Random Forest AS-Simple, AS-Compound, (+Smote) Photo.</cell><cell>Linear SVM 1-vs-all All</cell><cell>multi-class strategy</cell></row><row><cell>Features</cell><cell></cell><cell>PHOG</cell><cell cols="2">HAAR-like</cell><cell>Lateral proj. shape</cell><cell cols="2">Histo. of local binary content</cell><cell>MPEG7 CSD</cell><cell cols="2">(+GPS)</cell><cell cols="2">Numerous shape and texture,</cell><cell cols="2">GPS</cell><cell cols="2">Harrislike+constraints+SURF</cell><cell cols="2">+Local shape and texture</cell><cell cols="2">Shape context, DFH shape</cell><cell>Geometric model parameters,</cell><cell>Numerous shape and texture,</cell><cell>Morphological features</cell><cell>(Multiscale local (color) tex-</cell><cell>ture, SIFT) + Sparse coding</cell><cell>Spatial Pyramidal Matching</cell><cell>Numerous shape and texture,</cell><cell>Morphological features</cell><cell>Numerous shape and texture,</cell><cell>Morphological features,</cell><cell>SURF</cell><cell>(flip-)SIFT + Sparse coding</cell><cell>Spatial Pyramidal Matching</cell></row><row><cell>Segmentation</cell><cell></cell><cell cols="2">×</cell><cell></cell><cell cols="2">Otsu-like</cell><cell></cell><cell cols="2">×</cell><cell></cell><cell>AllScans: Otsu</cell><cell cols="2">Photo: Auto. k-Means</cell><cell>Photo: Interactive MeanShift</cell><cell></cell><cell cols="2">AllScans: Otsu</cell><cell cols="2">Photo: Otsu+abort criterion</cell><cell></cell><cell></cell><cell>Guided active contour</cell><cell></cell><cell></cell><cell>×</cell><cell>AllScans: Otsu</cell><cell>Photo: Semi-auto watershed</cell><cell>AllScans: Otsu</cell><cell>Photo: interactive GrabCut</cell><cell>×</cell></row><row><cell>Team</cell><cell></cell><cell cols="2">ArTe-</cell><cell>Lab</cell><cell cols="2">Brain-</cell><cell>signals</cell><cell cols="2">BTU</cell><cell>DBIS</cell><cell></cell><cell cols="2">IFSC</cell><cell>USP</cell><cell></cell><cell></cell><cell cols="2">Inria</cell><cell cols="2">IMEDIA</cell><cell></cell><cell>LIRIS</cell><cell>ReVes</cell><cell></cell><cell>LSIS</cell><cell>DYNI</cell><cell>Sabanci</cell><cell>Okan</cell><cell>TheWho</cell><cell>IITK</cell><cell>Zhao-</cell><cell>HFUT</cell></row><row><cell cols="26">Column S/C indicates if participants treated distinctively Simple and Compound. Col-</cell></row><row><cell cols="26">umn IP indicates if participants avoid to split images from a same Individual Plant</cell></row><row><cell cols="16">during evaluation on training dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_0" coords="2,144.73,645.84,126.99,7.86"><p>http://www.imageclef.org/2012</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_1" coords="2,144.73,656.80,123.92,7.86"><p>http://www.tela-botanica.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_2" coords="3,144.73,645.84,335.86,7.86;3,144.73,656.80,122.51,7.86"><p>it is closed now, but a newer a application can be found at http://identify.plantnetproject.org/en/base/plantscan</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was funded by the <rs type="funder">Agropolis</rs> fundation through the project Pl@ntNet (http://www.plantnet-project.org/) and the EU through the CHORUS+ Coordination action (http://avmediasearch.eu/). Thanks to all participants. Thanks to <rs type="person">Violette Roche</rs>, <rs type="person">Jennifer Carré</rs> and all contributors from <rs type="person">Tela Botanica</rs>. Thanks to <rs type="person">Vera Bakic</rs> and <rs type="person">Souheil Selmi</rs> from <rs type="affiliation">Inria</rs> and <rs type="person">Julien Barbe</rs> from <rs type="affiliation">Amap</rs> for their help for checking the datasets and developing the crowd-sourcing applications.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="24,142.96,440.13,317.07,7.86;24,139.37,450.77,3.58,7.86" xml:id="b0">
	<monogr>
		<ptr target="http://www.intelengine.cn/English/dataset2" />
		<title level="m" coord="24,151.53,440.13,126.64,7.86">The icl plant leaf image dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.96,461.42,337.63,7.86;24,151.52,472.38,329.07,7.86;24,151.52,483.34,329.07,7.86;24,151.52,494.29,60.92,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="24,213.12,483.34,218.97,7.86">First steps toward an electronic field guide for plants</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Kress</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">B</forename><surname>Dixit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Shirdhonkar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="24,439.83,483.34,25.08,7.86">Taxon</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="597" to="610" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.96,504.94,337.64,7.86;24,151.52,515.90,329.07,7.86;24,151.52,526.86,205.53,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="24,418.59,504.94,62.00,7.86;24,151.52,515.90,325.35,7.86">A Plant Identification System using Shape and Morphological Features on Segmented Leaflets</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Bagmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,165.60,526.86,162.78,7.86">Working notes of CLEF 2012 conference</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.96,537.50,337.64,7.86;24,151.52,548.46,273.59,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="24,323.70,537.50,156.89,7.86;24,151.52,548.46,97.75,7.86">A complex network-based approach for boundary shape analysis</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">M</forename><surname>Bruno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="24,256.14,548.46,81.42,7.86">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="54" to="67" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.96,559.11,337.63,7.86;24,151.52,570.07,329.07,7.86;24,151.52,581.03,328.71,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="24,291.22,570.07,189.38,7.86;24,151.52,581.03,116.26,7.86">Inria IMEDIA2&apos;s participation at ImageCLEF 2012 plant identification task</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Bakić</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mouine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ouertani-Litayem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Ouertani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Verroust-Blondet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,288.78,581.03,162.79,7.86">Working notes of CLEF 2012 conference</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.96,591.67,337.63,7.86;24,151.52,602.63,329.07,7.86;24,151.52,613.59,327.05,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="24,353.78,602.63,126.81,7.86;24,151.52,613.59,197.22,7.86">Searching the world4s herbaria: A system for visual identification of plant species</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kress</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sheorey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="24,369.65,613.59,23.24,7.86">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="116" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.96,624.23,337.64,7.86;24,151.52,635.19,177.93,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="24,217.75,624.23,68.19,7.86">Random forests</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1010933404324</idno>
		<ptr target="http://dx.doi.org/10.1023/A:1010933404324" />
	</analytic>
	<monogr>
		<title level="j" coord="24,298.80,624.23,59.62,7.86">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001-10">Oct 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,142.96,645.84,337.63,7.86;24,151.52,656.80,324.16,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="24,408.54,645.84,72.05,7.86;24,151.52,656.80,118.69,7.86">Fractal dimension applied to plant identification</title>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">M</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>De Oliveira Plotze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Falvo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>De Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="24,276.96,656.80,83.53,7.86">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2722" to="2733" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,142.62,119.67,337.97,7.86;25,151.52,130.63,329.07,7.86;25,151.52,141.59,25.60,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="25,374.62,119.67,105.97,7.86;25,151.52,130.63,139.66,7.86">BTU DBIS&apos; Plant Identification Runs at ImageCLEF 2012</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Böttcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zellhöfer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schmitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,314.04,130.63,166.56,7.86">Working notes of CLEF 2012 conference</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,142.62,152.55,337.98,7.86;25,151.52,163.51,329.07,7.86;25,151.52,174.47,70.42,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="25,436.57,152.55,44.03,7.86;25,151.52,163.51,186.42,7.86">IFSC/USP at ImageCLEF 2012: Plant identification task</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Batista Florindo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">N</forename><surname>Gonçalves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">M</forename><surname>Bruno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,360.17,163.51,120.41,7.86;25,151.52,174.47,41.76,7.86">Working notes of CLEF 2012 conference</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,142.62,185.43,337.98,7.86;25,151.52,196.39,329.07,7.86;25,151.52,207.34,286.49,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="25,167.24,196.39,313.35,7.86;25,151.52,207.34,73.84,7.86">ReVeS Participation -Tree Species Classification using Random Forests and Botanical Features</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cerutti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Antoine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tougne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Valet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Coquin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vacavant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,246.57,207.34,162.78,7.86">Working notes of CLEF 2012 conference</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,142.62,218.30,337.97,7.86;25,151.52,229.26,329.07,7.86;25,151.52,240.22,60.92,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="25,410.48,218.30,70.11,7.86;25,151.52,229.26,134.73,7.86">Smote: Synthetic minority over-sampling technique</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="25,294.30,229.26,170.56,7.86">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,142.62,251.18,337.98,7.86;25,151.52,262.14,329.07,7.86;25,151.52,273.10,46.08,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="25,318.66,251.18,161.93,7.86;25,151.52,262.14,160.59,7.86">Fast Tree Leaf Image Retrieval using a Probabilistic Multi-class Support Vector</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zamberletti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Albertini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,333.01,262.14,147.58,7.86;25,151.52,273.10,17.41,7.86">Working notes of CLEF 2012 conference</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,142.62,284.06,337.98,7.86;25,151.52,295.02,329.07,7.86;25,151.52,305.98,80.38,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="25,429.55,284.06,51.05,7.86;25,151.52,295.02,203.72,7.86">Visual-based plant species identification from crowdsourced data</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Selmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mouysset</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Joyeux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,375.86,295.02,104.74,7.86;25,151.52,305.98,51.71,7.86">Proceedings of ACM Multimedia 2011</title>
		<meeting>ACM Multimedia 2011</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,142.62,316.93,337.97,7.86;25,151.52,327.89,229.59,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="25,201.36,316.93,279.23,7.86;25,151.52,327.89,16.79,7.86">Brainsignals Submission to Plant Identification Task at ImageCLEF 2012</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Grozea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,189.66,327.89,162.78,7.86">Working notes of CLEF 2012 conference</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,142.62,338.85,337.97,7.86;25,151.52,349.81,329.07,7.86;25,151.52,360.77,329.07,7.86;25,151.52,371.73,294.83,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="25,305.08,338.85,175.51,7.86;25,151.52,349.81,97.38,7.86">Improved blur insensitivity for decorrelated local phase quantization</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Heikkila</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Ojansivu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICPR.2010.206</idno>
		<ptr target="http://dx.doi.org/10.1109/ICPR.2010.206" />
	</analytic>
	<monogr>
		<title level="m" coord="25,271.41,349.81,209.19,7.86;25,151.52,360.77,116.68,7.86;25,332.81,360.77,38.06,7.86">Proceedings of the 2010 20th International Conference on Pattern Recognition</title>
		<meeting>the 2010 20th International Conference on Pattern Recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="818" to="821" />
		</imprint>
	</monogr>
	<note>ICPR &apos;10</note>
</biblStruct>

<biblStruct coords="25,142.62,382.69,337.97,7.86;25,151.52,393.65,329.07,7.86;25,151.52,404.61,93.69,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="25,370.25,382.69,110.34,7.86;25,151.52,393.65,155.20,7.86">Plant species identification using elliptic fourier leaf shape analysis</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">C</forename><surname>Neto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">E</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">D</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Samal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="25,313.19,393.65,167.40,7.86">Computers and Electronics in Agriculture</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="134" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,142.62,415.56,337.98,7.86;25,151.52,426.52,329.07,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="25,284.44,415.56,196.16,7.86;25,151.52,426.52,121.50,7.86">Participation of LSIS/DYNI to ImageCLEF 2012 plant images classification task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Halkias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,292.80,426.52,159.73,7.86">Working notes of CLEF 2012 conference</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,142.62,437.48,337.98,7.86;25,151.52,448.44,265.50,7.86" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="25,308.72,437.48,171.87,7.86;25,151.52,448.44,99.77,7.86">grabcut&quot;: interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="25,258.16,448.44,79.00,7.86">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,142.62,459.40,337.97,7.86;25,151.52,470.36,208.95,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="25,200.91,459.40,117.67,7.86">Qql: A db&amp;ir query language</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Schmitt</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00778-007-0070-1</idno>
		<ptr target="http://dx.doi.org/10.1007/s00778-007-0070-1" />
	</analytic>
	<monogr>
		<title level="j" coord="25,326.24,459.40,79.36,7.86">The VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="56" />
			<date type="published" when="2008-01">Jan 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,142.62,481.32,337.98,7.86;25,151.52,492.28,329.07,7.86;25,151.52,503.24,102.51,7.86" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="25,231.51,481.32,245.10,7.86">Computer Vision Classification of Leaves from Swedish Trees</title>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">J O</forename><surname>Söderkvist</surname></persName>
		</author>
		<idno>liTH-ISY-EX-3132</idno>
		<imprint>
			<date type="published" when="2001-09">September 2001</date>
			<pubPlace>Linköping, Sweden</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Linköping University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct coords="25,142.62,514.19,337.98,7.86;25,151.52,525.15,329.07,7.86;25,151.52,536.11,100.85,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="25,313.78,514.19,166.81,7.86;25,151.52,525.15,40.41,7.86">Shape-based image retrieval in botanical collections</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Yahiaoui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Hervé</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Boujemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,213.04,525.15,245.80,7.86">Advances in Multimedia Information Processing -PCM 2006</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4261</biblScope>
			<biblScope unit="page" from="357" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,142.62,547.07,337.97,7.86;25,151.52,558.03,329.07,7.86;25,151.52,568.99,158.22,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="25,320.27,547.07,160.32,7.86;25,151.52,558.03,146.11,7.86">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,327.88,558.03,152.71,7.86;25,151.52,568.99,129.55,7.86">IEEE Conference on Computer Vision and Pattern Recognition(CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,142.62,579.95,337.98,7.86;25,151.52,590.91,283.00,7.86" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yanikoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Aptoula</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Tirkaz</surname></persName>
		</author>
		<title level="m" coord="25,313.44,579.95,167.16,7.86;25,151.52,590.91,254.32,7.86">Sabanci-Okan at ImageCLEF 2012 Plant Identification Competition: Combining Features and Classifiers</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,142.62,601.87,337.98,7.86;25,151.52,612.82,257.80,7.86" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="25,290.17,601.87,190.42,7.86;25,151.52,612.82,44.89,7.86">ZhaoHFUT at ImageCLEF 2012 Plant Identification Task</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="25,217.87,612.82,162.78,7.86">Working notes of CLEF 2012 conference</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
