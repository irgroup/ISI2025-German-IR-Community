<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,139.74,115.90,335.89,12.90;1,253.29,133.83,108.79,12.90">Brainsignals Submission to Plant Identification Task at ImageCLEF 2012</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,274.04,171.88,62.80,8.64"><forename type="first">Cristian</forename><surname>Grozea</surname></persName>
							<email>cristian.grozea@brainsignals.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Fraunhofer FOKUS</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,139.74,115.90,335.89,12.90;1,253.29,133.83,108.79,12.90">Brainsignals Submission to Plant Identification Task at ImageCLEF 2012</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6BCA0FF15B339211E5DB6684773B82AA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes our first participation to the plant identification task of ImageCLEF. We have approached it as intended with the least possible computer-vision related techniques (especially without costly image preprocessing and feature extraction) and as much as possible machine learning techniques (random forests). Our method is purely visual and entirely automatic, using only the image information. One should mention that the total time spent with preparing this submission was only about one week. The results were accordingly fairly poor, but probably usable for mobile applications, as the whole process of preprocessing and classification is very fast. We propose a slight modification to the Otsu algorithm, which makes it better suitable for the scan-like images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Our intention with this participation to ImageCLEF, the plant identification task, was to test our hypothesis that good machine learning techniques paired with rudimentary computer vision preprocessing could lead to fair/good results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preprocessing</head><p>The images are converted to grayscale, resized to 30% and then binarized with a slightly modified Otsu algorithm. Our algorithm gives unequal weights to the variances of the two classes when looking for the best split in the grays values space. We used that to tune the split towards being more inclusive with the leaf pixels class, which we have empirically found it works better than the standard Otsu algorithm on the scan-like images. This choice seemed to have paid back, as in the final results in the competition the performance of our algoritm degrades less on the scan-like images versus the one on the scan images than that of the algorithms of some other teams -which has the effect of pushing it to higher ranks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features Two types of features are extracted (for a total of 344 features):</head><p>-Local features: histograms of content types of small rectangular patches of bits, with and without sub-sampling.</p><p>-Global shape features from lateral projections: histograms of the number of b/w alternations on every pixel line/column.</p><p>The features we extract are thought to be extremely quick to compute and still give a good description of the content of the binarized image.</p><p>While the second class of features is self-explanatory, the first class needs perhaps some more detail. Let us assume a small window, say a square of 2 by 2 pixels. When this window is moved over the image, it contains several patches of 2x2 pixels. The content of those patches can be linearized as a 4 bit vector and has only 16 possible types of content, which are the binary representations of the numbers from 0 to 15. It is then natural and easy to compute the distribution (histogram) of those numbers. The same can be repeated for non-square windows, say of size 2x3, 3x2 or 3x3 and even generalized to binary masks where the bits collected are farther away from each other, disposed or not in a regular-step grid. Attempting to capture local and non-local topologies, we have used masks with distance between pixels of up to 30 pixels, but always few possible values for the selected content (sparse masks).</p><p>The discriminative power of those features, at least for some of the classes, can be observed in a plot of the data after dimensionality reduction by PCA in Figure <ref type="figure" coords="2,448.41,318.16,3.74,8.64" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Classification</head><p>A random forest classifier <ref type="bibr" coords="2,240.97,367.16,10.58,8.64" target="#b0">[1]</ref>, with 100 trees and standard number of selected features (1 + âˆš m) is trained on the training data, where m is the total number of features. The model obtained is then applied on the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>The 5-fold cross-validated classification accuracy on the training data set was 70%, which is very good for so many classes (126). It could be that the performance can be increased even more by using more trees in the random forest and/or by using weighting or balancing techniques to balance the samples similar to the scoring function.</p><p>Although the final test performance, as measured with the complicated balanced error function was certainly not up to the best (scans: 0.25, scans-like: 0.22, photographs: 0.05, average: 0,17), the method seemed to perform fine on the training data, failing most of the time only on images for which, after resizing and binarization, it was difficult for a human as well to spot that the class of the image should be different than the class of the random member of the wrongly misclassified-to class, as shown in Figure <ref type="figure" coords="2,475.61,553.64,4.98,8.64" target="#fig_1">2</ref> 4 Conclusion While our solution was not performing up to the best ones in this competition, it is fast, accurate enough for non-critical applications and has potential for improvement. We intend to proceed developing this solution further and plan a mobile phone implementation of it.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,195.58,441.81,221.96,8.12"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Class clustering in the space of the selected features</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,134.77,472.73,345.82,8.12;4,134.77,484.04,36.77,7.77;4,152.06,288.79,311.23,169.21"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Example of mis-classification; humans have a hard time as well trying to distinguish those two.</figDesc><graphic coords="4,152.06,288.79,311.23,169.21" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="3,138.13,499.66,250.39,7.77" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="3,194.28,499.66,55.51,7.77">Random forests</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="3,255.30,499.66,63.00,7.77">Machine learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
