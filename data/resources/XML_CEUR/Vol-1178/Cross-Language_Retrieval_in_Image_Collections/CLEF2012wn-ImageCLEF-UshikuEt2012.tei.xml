<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,221.26,116.95,172.83,12.62;1,171.66,134.89,272.03,12.62">ISI at ImageCLEF 2012: Scalable System for Image Annotation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,165.50,172.56,74.23,8.74"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Systems and Informatics Lab</orgName>
								<orgName type="institution">the University of Tokyo {ushiku</orgName>
								<address>
									<addrLine>muraoka</addrLine>
									<settlement>inaba</settlement>
									<region>fujisawa</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,247.81,172.56,70.54,8.74"><forename type="first">Hiroshi</forename><surname>Muraoka</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Systems and Informatics Lab</orgName>
								<orgName type="institution">the University of Tokyo {ushiku</orgName>
								<address>
									<addrLine>muraoka</addrLine>
									<settlement>inaba</settlement>
									<region>fujisawa</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,326.80,172.56,42.20,8.74"><forename type="first">Sho</forename><surname>Inaba</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Systems and Informatics Lab</orgName>
								<orgName type="institution">the University of Tokyo {ushiku</orgName>
								<address>
									<addrLine>muraoka</addrLine>
									<settlement>inaba</settlement>
									<region>fujisawa</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,376.88,172.56,68.51,8.74"><forename type="first">Teppei</forename><surname>Fujisawa</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Systems and Informatics Lab</orgName>
								<orgName type="institution">the University of Tokyo {ushiku</orgName>
								<address>
									<addrLine>muraoka</addrLine>
									<settlement>inaba</settlement>
									<region>fujisawa</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,170.06,184.51,64.97,8.74"><forename type="first">Koki</forename><surname>Yasumoto</surname></persName>
							<email>k-yasumoto@isi.imi.i.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Systems and Informatics Lab</orgName>
								<orgName type="institution">the University of Tokyo {ushiku</orgName>
								<address>
									<addrLine>muraoka</addrLine>
									<settlement>inaba</settlement>
									<region>fujisawa</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,243.45,184.51,62.19,8.74"><forename type="first">Naoyuki</forename><surname>Gunji</surname></persName>
							<email>gunji@isi.imi.i.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Systems and Informatics Lab</orgName>
								<orgName type="institution">the University of Tokyo {ushiku</orgName>
								<address>
									<addrLine>muraoka</addrLine>
									<settlement>inaba</settlement>
									<region>fujisawa</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,313.54,184.51,74.39,8.74"><forename type="first">Takayuki</forename><surname>Higuchi</surname></persName>
							<email>higuchi@isi.imi.i.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Systems and Informatics Lab</orgName>
								<orgName type="institution">the University of Tokyo {ushiku</orgName>
								<address>
									<addrLine>muraoka</addrLine>
									<settlement>inaba</settlement>
									<region>fujisawa</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,395.74,184.51,44.74,8.74"><forename type="first">Yuko</forename><surname>Hara</surname></persName>
							<email>y-hara@isi.imi.i.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Systems and Informatics Lab</orgName>
								<orgName type="institution">the University of Tokyo {ushiku</orgName>
								<address>
									<addrLine>muraoka</addrLine>
									<settlement>inaba</settlement>
									<region>fujisawa</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,223.45,196.47,67.64,8.74"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
							<email>harada@isi.imi.i.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Systems and Informatics Lab</orgName>
								<orgName type="institution">the University of Tokyo {ushiku</orgName>
								<address>
									<addrLine>muraoka</addrLine>
									<settlement>inaba</settlement>
									<region>fujisawa</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.74,196.47,73.18,8.74"><forename type="first">Yasuo</forename><surname>Kuniyoshi</surname></persName>
							<email>kuniyosh@isi.imi.i.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Systems and Informatics Lab</orgName>
								<orgName type="institution">the University of Tokyo {ushiku</orgName>
								<address>
									<addrLine>muraoka</addrLine>
									<settlement>inaba</settlement>
									<region>fujisawa</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,221.26,116.95,172.83,12.62;1,171.66,134.89,272.03,12.62">ISI at ImageCLEF 2012: Scalable System for Image Annotation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8C8316E082AC5788871474ED5D8F1DCE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We participate in the ImageCLEF 2012 Photo Annotation Tasks. We devote our attention to make our system scalable for the data amount. Therefore we train linear classifiers with our online multilabel learning. For Flickr Photo task, we extract visual Fisher Vectors (FVs) from some kinds of local descriptors and used the provided Flickr-tags for textual features. For Web Photo tasks, we just use the provided Bagof-Visual-Words (BoVW) of some kinds of SIFT descriptors. A linear classifier for each label is obtained with an online multilabel learning, Passive-Aggressive with Averaged Pairwise Loss (PAAPL). The results have shown that our scalable system achieves pretty good performances in all tasks we take part in.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we describe our method for the ImageCLEF 2012 Photo Annotation tasks. In particular, we attack three tasks: concept annotation using Flickr photos <ref type="bibr" coords="1,166.82,489.75,9.96,8.74" target="#b7">[8]</ref>, improving performance in Flickr concept annotation task using Web photos <ref type="bibr" coords="1,166.92,501.70,14.61,8.74" target="#b9">[10]</ref>, and scalable concept image annotation using Web photos <ref type="bibr" coords="1,441.62,501.70,14.61,8.74" target="#b9">[10]</ref>.</p><p>Especially, we pay our attention to the scalability for the data amount. In this literature, many techniques are developed to improve the performance of object recognition. Though some of them have succeeded by introducing a complicated classifier such as the multiple kernel SVM, the complexity for leaning and annotating is a problem. Because many kinds of labels require a large amount of training data, the scalability for the data amount is important for generic object recognition.</p><p>Consequently, our objective is to investigate scalable methods for feature extraction, for learning, and for annotation. Recent studies for large scale image classification adopt online learning for linear classification. In <ref type="bibr" coords="1,431.49,621.25,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="1,443.67,621.25,7.01,8.74" target="#b3">4]</ref>, highdimensional features in <ref type="bibr" coords="1,242.69,633.21,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="1,254.86,633.21,12.73,8.74" target="#b10">11]</ref> are used for learning 1000 classes from over a million images. In fact, Fisher Vectors (FVs) and linear SVM won the Image-CLEF 2010 as described in <ref type="bibr" coords="1,255.97,657.12,9.96,8.74" target="#b4">[5]</ref>.</p><p>Our main contribution is the investigation of our novel online learning for multilabel problem. Because batch learning by loading all training samples is impossible, usage of an online learning is a promising method in order to realize scalability. In <ref type="bibr" coords="2,197.32,155.86,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="2,209.49,155.86,7.01,8.74" target="#b3">4]</ref>, online SVM learning with Stochastic Gradient Descent <ref type="bibr" coords="2,470.09,155.86,10.52,8.74" target="#b0">[1]</ref> (SGD-SVM) is applied with a one-vs.-the-rest manner. The classifier for a label is obtained by regarding images associated with the label as positive samples and the rest images as negative samples. Furthermore, labels are output according to the scores from the binary classifiers. Nevertheless, no guarantee exists that the output of SVMs for different classifiers will have appropriate scales. Thus we investigate a multiclass learning Passive-Aggressive algorithm <ref type="bibr" coords="2,411.57,227.59,10.52,8.74" target="#b1">[2]</ref> to solve this problem. In <ref type="bibr" coords="2,190.53,239.55,9.96,8.74" target="#b8">[9]</ref>, we have proposed Passive-Aggressive with Averaged Pairwise Loss (PAAPL) for which multiple labels are attached to one sample. At first, we use an averaged pairwise loss instead of the hinge-loss of PA. Secondly, we randomly select these pairs at every learning. These two improvements make PAAPL can converge faster than PA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Feature Extraction</head><p>In this section, we describe the features we use in three tasks. For the Flickr Photo task, we extract FVs as visual features and some kinds of BoW of Flickrtags as textual features. For the Web Photo tasks, we use the provided Bag-of-Visual-Words only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual Features</head><p>Bag-of-Visual-Words. Bag-of-Visual-Words (BoVW) is quite a popular approach for image classification, because it achieves good performance in spite of its simplicity. The main idea is that images are treated as loose collections of K codewords, representative local descriptors, and that each key-point patch, in which a local descriptor is extracted, is sampled independently. The BoVW feature is obtained by making a histogram of the number of local descriptors assigned to each codeword. The dictionary, which consists of K codewords, has to be generated by unsupervised clustering of training samples in advance and each local descriptor is assigned to the nearest codeword in the dictionary. BoVW vector is therefore K-dimensional.</p><p>Fisher Vectors. Fisher Vectors (FV), which is regarded as an extension of the BoVW representation, is a standard approach to the large-scale image recognition. BoVW utilizes 0-order statistics of the distribution of local descriptors, whereas FV utilizes 1-and 2-order statistics. The distribution of local descriptors is fitted to the mixture model of K Gaussians, and the gradients of the likelihood in the parameter space are computed. The gradients describe which direction the model parameters are to be modified to get a better description of the image. The dimensions are then whitened by multiplying the square root of the Fisher information matrix.</p><p>We denote T local descriptors by X = {x 1 , x 2 , • • • , x T }, and the mixture weight, mean, covariance matrix of i-th Gaussian by w i , µ i , and σ i , respectively. Since the covariance matrices are assumed to be diagonal, we denote the variance vector by σ 2 . The FV representation is thus given as,</p><formula xml:id="formula_0" coords="3,217.81,179.44,258.54,34.51">G X µ,i = 1 T √ w i T ∑ t=1 γ t (i) ( x t -µ i σ i ) , (<label>1</label></formula><formula xml:id="formula_1" coords="3,476.35,194.15,4.24,8.74">)</formula><formula xml:id="formula_2" coords="3,218.22,214.27,258.13,34.51">G X σ,i = 1 T √ 2w i T ∑ t=1 γ t (i) [ (x t -µ i ) 2 σ 2 i -1 ] , (<label>2</label></formula><formula xml:id="formula_3" coords="3,476.35,228.98,4.24,8.74">)</formula><p>where γ t (i) is the soft assignment of x t to i-th Gaussian as follows,</p><formula xml:id="formula_4" coords="3,255.63,275.30,220.72,24.77">γ t (i) = w i u i (x t ) Σ K j=1 w j u j (x t ) . (<label>3</label></formula><formula xml:id="formula_5" coords="3,476.35,282.04,4.24,8.74">)</formula><p>Then, we obtain the 2KD-Dimensional vector by concatenating G X µ,i and G X σ,i . To enhance the performance, a power normalization is proven to be effective in <ref type="bibr" coords="3,134.76,333.68,9.96,8.74" target="#b5">[6]</ref>. The vectors are normalized with L 2 norms after the power normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Text Features</head><p>We use Bag-of-Words (BoW) representation, which is based on the idea that each word in a text appears independently. BoW is obtained by counting appearance of words in a text. In our method, the feature is converted in following two ways, TF-IDF and L 2 -normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TF-IDF weight</head><p>We regard the typicality of each Flickr-tag as a clue to how the tag relates to the image's contents. Therefore, we use TF-IDF value for each element of a BoW vector. L 2 -normalization To reduce the effect of different numbers of tags among images, we simply L 2 -normalize the BoW vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Online Multilabel Learning</head><p>To learn the models for each label from various images, requirements are not only compatibility of scalability for the data amount and accuracy for label estimation, but also tolerability of noise.</p><p>Given the t-th training sample</p><formula xml:id="formula_6" coords="3,134.76,591.47,345.82,60.65">x t ∈ R d associated with a label set Y t , a subset of Y = {1, . . . , n y }, it is classified with the present weight vector µ yi t (i = 1, . . . , n y ) 1 as, ŷt = arg max yi µ yi t • x t . (<label>4</label></formula><formula xml:id="formula_7" coords="3,476.35,636.04,4.24,8.74">)</formula><p>If necessary, multiple labels are estimated in score order. Multilabeling for one sample is applicable by defining n y &gt; 1. Here, hinge-loss ℓ is given as,</p><formula xml:id="formula_8" coords="4,148.24,155.94,328.11,35.87">ℓ(µ rt t , µ st t ; (x t , Y t )) = { 0 µ rt t • x t -µ st t • x t ≥ 1 1 -(µ rt t • x t -µ st t • x t ) otherwise . (<label>5</label></formula><formula xml:id="formula_9" coords="4,476.35,173.65,4.24,8.74">)</formula><p>where</p><formula xml:id="formula_10" coords="4,163.57,201.81,198.64,18.95">r t = arg min r∈Yt µ r t • x t and s t = arg max s / ∈Yt µ s t • x t .</formula><p>PA is an online learning method for binary and multiclass classification, regression, uniclass estimation and structure estimation. The biggest benefit of PA is that the update coefficient is analytically calculated according to the loss. In contrast, SGD based methods and traditional perceptron require designing the coefficient.</p><p>Here we seek to decrease the hinge-loss of multi-classification and not to change the weight radically. Consequently, we obtain the following formulation.</p><formula xml:id="formula_11" coords="4,183.33,312.79,297.26,33.77">µ rt t+1 , µ st t+1 = arg min µ r t ,µ r t ||µ rt -µ rt t || 2 + ||µ st -µ st t || 2 + Cξ 2 , (6) s.t. ℓ(µ rt , µ st ; (x t , Y t )) ≤ ξ and ξ ≥ 0.<label>(7)</label></formula><p>Therein, ξ denotes a slack variable representing the bound of the loss. C signifies a parameter to reduce the negative influence of noisy labels. It can be derived using Lagrange's method of undetermined multipliers. Therefore we obtain,</p><formula xml:id="formula_12" coords="4,218.56,401.08,257.79,13.21">µ rt t+1 = µ rt t + τ t • x t , µ st t+1 = µ st t -τ t • x t , (<label>8</label></formula><formula xml:id="formula_13" coords="4,476.35,403.47,4.24,8.74">)</formula><formula xml:id="formula_14" coords="4,218.56,417.16,257.79,13.01">τ t = min{C, ℓ(µ rt t , µ st t ; (x t , Y t ))/(2x 2 t )}. (<label>9</label></formula><formula xml:id="formula_15" coords="4,476.35,419.55,4.24,8.74">)</formula><p>This PA is called PA-II in <ref type="bibr" coords="4,263.48,440.88,9.96,8.74" target="#b1">[2]</ref>. PA and SGD-SVM have a closed form. Indeed, PA for binary classification and SGD-SVM without L 2 regularization have the same update rule. Differences between SGD-SVM and PA here are (1) binary or multi-class, (2) regularization form, and (3) the number of parameter to be tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Passive-Aggressive with Averaged Pairwise Loss</head><p>PA is online learning methods for classification, but it presents no problem if a sample is associated with multiple labels. Indeed, the Passive-Aggressive Model for Image Retrieval (PAMIR) <ref type="bibr" coords="4,272.21,561.48,10.52,8.74" target="#b2">[3]</ref> is proposed by application of PA to image retrieval.</p><p>However, they treat only one relevant label and one irrelevant label. Apparently, models of some labels are not well updated and that convergence becomes delayed.</p><p>Therefore, we have proposed a novel online learning algorithm for which multiple labels are attached to one sample in <ref type="bibr" coords="4,320.97,633.21,9.96,8.74" target="#b8">[9]</ref>. General online learning methods consist of two steps: classification of the t-th sample, and update of the t-th models. Given the d-dimensional weight vectors µ for all n y labels, the complexity for Additionally, we investigate a way to reduce the complexity O(dn y ) for the classification step. In <ref type="bibr" coords="5,226.83,510.38,14.61,8.74" target="#b11">[12]</ref>, the approximation of a loss function by the random selection of labels is an important step for online learning when using less powerful computers. Although random selection may miss incorrectly-classified labels at a higher rate, it was experimentally verified that correct models can be obtained eventually. Therefore, we also adopted random selection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>In this section, we describe the details of methods we use for Flickr Photo annotion task, Web Photo subtask1, and Web Photo subtask2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Photo Flickr</head><p>In our experiment, we extracted 5 kinds of visual descriptors from each image, SIFT and LBP in five patch sizes, and color-SIFTs (C-SIFT, RGB-SIFT, Op-ponentSIFT) in three patch sizes. As pre-processing, the images were resized into at most 300 × 300 pixels, of which aspect ratio were maintained. To calculate SIFT and LBP, the images were rendered in gray scale, in contrast to color-SIFTs which utilized color information. Then, each descriptor was densesampled on regular grids (every six pixels). The dimensionalities of SIFT, LBP, and color-SIFTs were 128, 1024, and 384 respectively. All of these descriptors were reduced to 64 dimensions with PCA, and then coded into two state-of-theart global feature representations. (5 × 2 = 10 visual features in total) One is FV, explained in the previous section. At first, we trained the mixture model of 256 Gaussians using standard EM-algorithm. To embed spatial information, FVs were calculated respectively over 1 × 1, 2 × 2, and 3 × 1 cells. In this way, we obtained FVs whose dimensionality was 64 × 256 × 8 × 2 = 262, 144. The other is Locality-constrained Linear Coding (LLC) <ref type="bibr" coords="6,365.66,380.63,14.61,8.74" target="#b10">[11]</ref>, which describes each local descriptor by a linear weighted sum of a few nearest codewords. In our experiment, 4,096 codewords were generated with k-means algorithm, and then each local descriptor was approximated using 3-NN of the descriptor. The images were divided into 1 × 1, 2 × 2 and 3 × 3 spatial grids differently from FV, so the dimensionality was 4096 × 14 = 57, 344. As text features, BoW vectors were extracted from Flickr-tags, and then we also prepared the one whose dimensions were removed if the corresponding words appeared 24 times or less. Furthermore, all combinations of 2 kinds of processing (TF-IDF, L 2 -normalization) were done, so 2 × 2 2 = 8 text features were generated in total. Finally, the classifiers of 10 visual features and 8 text features were trained separately using PAAPL. The trade-off parameter was set to C = 10 5 . All the experiments in the following sub-section were implemented using a workstation with CPUs of 12-core and 96GB RAM.</p><p>The validation set consisted of one-third of the training images, and validation was done only two times for the lack of time.</p><p>The size of the visual feature such as FV or LLC tends to be large. It is known to be effective to quantize the vector with Product Quantization (PQ) as described in <ref type="bibr" coords="6,191.16,597.34,10.52,8.74" target="#b6">[7,</ref><ref type="bibr" coords="6,203.33,597.34,7.01,8.74" target="#b3">4]</ref>. At first, we investigated the performance effect of PQ using FV-SIFT. The parameter of PQ was decided empirically, b = 1, G = 8. We iterated PAAPL learning 15 times. As a result, FV-SIFT achieved F1-measure (F1) 0.5604 with PQ while it achieved 0.5632 without PQ. Because the drop of performance was actually not significant, we quantized visual features of training samples for saving the RAM.</p><p>Since the number of runs which could be submitted was limited, we examined which combinations of visual features were effective. Then we investigated which text feature should be added to achieve the best performance through the next experiment.</p><p>The Table <ref type="table" coords="7,195.38,170.03,4.98,8.74">1</ref> shows the top six F1-measures of the combinations of 2 10 = 1, 024 visual features. These features were all quantized with PQ. Note that all LLCs are shown to be inefficient here. In this way, we chosen to extract FVs from SIFT, from C-SIFT, and from LBP, which achieved the best performance.</p><formula xml:id="formula_16" coords="7,181.37,246.26,243.56,107.60">FV-SIFT ✓ ✓ ✓ ✓ ✓ ✓ FV-LBP ✓ ✓ ✓ ✓ ✓ ✓ FV-OpponentSIFT - - - ✓ - ✓ FV-cSIFT ✓ ✓ - - ✓ ✓ FV-rgbSIFT - ✓ ✓ - ✓ - LLC-SIFT - - - - - - LLC-LBP - - - - - - LLC-OpponentSIFT - - - - - - LLC-cSIFT - - - - - - LLC-rgbSIFT - - - - ✓ -</formula><p>F1-measure 0.5715 0.5707 0.5703 0.5693 0.5693 0.5688 Table <ref type="table" coords="7,243.54,384.41,4.13,7.89">1</ref>. Top combinations of visual features.</p><p>The Table <ref type="table" coords="7,196.10,432.58,4.98,8.74" target="#tab_2">2</ref> shows the F1-measures of eight text features. As a result, thresholding w.r.t. the number of corresponding images is not effective for the performance. Therefore, we chosen only four text features which were not thresholded. Finally, we present the top six F1-measures (F1) of the combinations of three visual features and for text features in the Table <ref type="table" coords="7,347.17,607.73,3.87,8.74" target="#tab_3">3</ref>.<ref type="foot" coords="7,357.95,606.15,3.97,6.12" target="#foot_1">2</ref> Following these results, we submitted our runs described in Table <ref type="table" coords="7,304.44,619.68,4.98,8.74" target="#tab_4">4</ref> and got the scores also shown in Table <ref type="table" coords="7,134.76,631.64,3.87,8.74" target="#tab_4">4</ref>. Note that all of these visual features from test images were not quantized. </p><formula xml:id="formula_17" coords="8,140.50,121.97,334.36,90.08">Visual (FV) Textual (BoW) F1 FV-SIFT FV-LBP FV-cSIFT histgram TF-IDF histgram (L2) TF-IDF (L2) ✓ ✓ ✓ ✓ - - - 0.5798 ✓ ✓ - ✓ - - - 0.5792 ✓ ✓ - - - ✓ - 0.5770 ✓ ✓ ✓ - - ✓ - 0.5763 ✓ ✓ - - ✓ - - 0.5760 ✓ ✓ ✓ ✓ - ✓ - 0.5759</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Photo Web</head><p>For Photo Web tasks, we could not extract FVs because there were no images in the provided dataset. Hence we use the provide BoVWs from some kinds of SIFTs.</p><p>Moreover, our PAAPL requires labels for each training sample. We investigated a simple way to define the labels for each training sample. In particular, we extracted words whichever are concept words from the surrounding texts for each image. Images around which any concepts do not exist are just discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subtask 1: Improving performance in Flickr concept annotation task</head><p>For Flickr and Web image representation, we made use of four provided BoVWs, which were computed respectively from SIFT, C-SIFT, OpponentSIFT and RGB-SIFT. Unlike Flickr data, Web data was not annotated. Therefore we needed to estimate labels of web data for using it as training data in supervised learning. In order to do this, we sought for concepts in surrounding texts. If any concept labels exist in a textual feature, then we consider that the corresponding image has that label.</p><p>To annotate images, we use PAAPL with regularization parameter from 10 4 , 10 5 and 10 6 . As a result, C = 10 6 achieves the best performance in almost all cases where a classifier is trained on different type descriptors. The number of training iteration is 25, same as the Flickr Photo task.</p><p>We have two ideas on how to utilize web data for improvement of annotation performance. One idea is that we use Web data and Flickr data independently. At first, we trained eight classifiers using four types BoVWs from either Web data or Flickr data. Then, we summed the scores form the eight classifiers when we annotated the test images. To find best combinations of descriptors, we used 10000 Flickr images or 10000 Web images for training, and 5000 Flickr data for validation. We computed F1-measure as a measure of effectiveness of a combination. Results are shown in Table <ref type="table" coords="9,292.07,203.68,3.87,8.74" target="#tab_5">5</ref> obtained by the other idea, so we do not adopt this idea.</p><p>The other idea is that we merge 15000 Flickr data and 250000 web data , and unified 265000 data is used for training. Classifiers are trained respectively for each type of BoVW computed from different descriptor, so we have four classifiers. Scores for each label are computed by summing scores from each different classifiers. A number of ways of combining classifiers is ∑ 4 i=1 4 C i = 15. To find best combinations of four different classifiers, we use 10000 Flickr images and 10000 Web images for training, and 5000 Flickr data for validation. Then we computed F1-measure shown in Table <ref type="table" coords="9,318.24,456.43,3.87,8.74">6</ref>.</p><p>Therefore we submitted the following combinations of BoVWs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this working note, we describe our method to annotate images in ImageCLEF 2012 Photo Annotation and Retrieval tasks. We pay our attention to make our method scalable for a large amount of images. Consequently, we use FVs and BoWs in Photo Flickr task, and use the provided BoVW in Photo Web tasks. Annotation itself is achieved using a novel online learning PAAPL, which has been already proposed in <ref type="bibr" coords="11,247.51,556.56,10.52,8.74" target="#b8">[9]</ref> for multilabel problem.</p><p>For Photo Flickr task, we have achieved the top scores among all teams although our system is scalable and simple. Not only FVs from SIFTs but also FV from LBP is shown to be useful for annotation. Moreover, simple tag information with BoW improves the performance. For Photo Web tasks, there are few teams that have submitted at least one run. In Subtask 1, there are no teams that have improved the performance with Web data. The result of Subtask 2 also indicates that the Web Photoes are difficult to be extracted their proper concepts from their Web pages. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.76,119.01,345.84,362.64"><head></head><label></label><figDesc>Randomly select one relevant label r t from Y t and one irrelevant label s t from Ȳt . 3. Based on a hinge-loss between r t and s t , 1 -(µ rt</figDesc><table coords="5,173.23,119.01,268.89,155.38"><row><cell>Hinge-loss</cell><cell>Averaged Pairwise Loss</cell></row><row><cell>Score</cell><cell>Score</cell></row><row><cell>correct labels</cell><cell>correct labels</cell></row><row><cell>update</cell><cell>update</cell></row><row><cell>Score</cell><cell>Score</cell></row><row><cell>correct labels</cell><cell>correct labels</cell></row><row><cell cols="2">Fig. 1. Comparison between hinge-loss and averaged pairwise loss.</cell></row></table><note coords="5,134.76,303.95,345.83,9.66;5,134.76,315.91,345.83,9.65;5,134.76,327.86,345.83,9.66;5,134.76,339.82,345.84,8.74;5,134.76,351.77,345.83,9.66;5,134.76,363.73,345.84,8.74;5,134.76,375.68,345.84,8.74;5,134.76,387.64,157.06,8.74;5,138.97,410.63,341.62,12.17;5,151.70,425.10,91.16,8.74;5,138.97,437.06,7.75,8.74;5,367.61,465.46,3.01,6.12;5,377.86,460.94,35.89,9.68;5,413.75,458.58,6.57,6.12;5,413.75,465.46,3.01,6.12;5,424.07,460.94,56.52,9.68;5,151.70,472.92,106.78,8.74"><p><p><p><p>classification of a sample is O(dn y ), while update of a model is O(d). If we update all models with given labels Y t , its complexity becomes O(d|Y t |). In image annotation and especially sentence generation, we can assume n y ≫ |Y t |. Therefore, since classification is the rate-controlling step, total computation time remains much the same whether we update one model or |Y t | models. Fig.</p>1</p>shows the conceptual difference between hinge-loss and the loss used in proposed method. Thus the proposed PAAPL achieves efficiency by averaging all pairwise loss between relevant and irrelevant labels.</p>1. Given a t-th image, define label set Ȳt of n y labels by selecting highly scored and irrelevant labels. 2. t • x t -µ st t • x t ), update models according to PA.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,138.97,583.71,341.62,68.52"><head></head><label></label><figDesc>1. Randomly select one relevant label r t from Y t . 2. Define irrelevant label s t with random selection from Y t and compute the hinge-loss 1 -(µ rt t • x t -µ st t • x t ). Continue selecting s t until the loss becomes positive. 3. If the hinge-loss becomes positive, update models for r t and s t according to PA; otherwise move on to next training sample.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,214.65,493.88,186.05,61.61"><head>Table 2 .</head><label>2</label><figDesc>Top combinations of visual features.</figDesc><table coords="7,232.18,493.88,150.99,41.64"><row><cell>threshold</cell><cell>histgram L2</cell><cell>TF-IDF L2</cell></row><row><cell>-</cell><cell cols="2">0.5157 0.5156 0.5135 0.5121</cell></row><row><cell>✓</cell><cell cols="2">0.5114 0.5109 0.5020 0.4990</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,134.76,226.64,345.82,112.65"><head>Table 3 .</head><label>3</label><figDesc>Top combinations of visual and textual features. L2 means the vector is normalized according to its L2 norm.</figDesc><table coords="8,151.34,271.60,312.67,67.69"><row><cell>method</cell><cell>MiAP GMiAP F1</cell></row><row><cell>FV-SIFT + FV-LBP</cell><cell>0.3243 0.2590 0.5451</cell></row><row><cell>FV-SIFT + FV-LBP + text</cell><cell>0.4046 0.3436 0.5559</cell></row><row><cell>FV-SIFT + FV-LBP + text(TF-IDF)</cell><cell>0.4029 0.3462 0.5597</cell></row><row><cell>FV-SIFT + FV-LBP + FV-C-SIFT + text</cell><cell>0.4136 0.3540 0.5574</cell></row><row><cell cols="2">FV-SIFT + FV-LBP + FV-C-SIFT + text(TF-IDF) 0.4131 0.3580 0.5583</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,134.76,354.39,345.82,18.84"><head>Table 4 .</head><label>4</label><figDesc>All five submissions and their scores on the Flickr Photo task. MiAP and GMiAP stand for (Geometric) Mean interpolated Average Precision</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,152.06,203.68,328.54,116.46"><head>Table 5 .</head><label>5</label><figDesc>. The best F1-measure is worse than that Results of descriptor combinations. O-SIFT means OpponentSIFT.</figDesc><table coords="9,209.33,236.61,196.81,63.05"><row><cell></cell><cell></cell><cell>Flickr</cell></row><row><cell></cell><cell></cell><cell cols="2">SIFT C-SIFT O-SIFT RGB-SIFT</cell></row><row><cell></cell><cell>SIFT</cell><cell>0.2086 0.2195 0.2162</cell><cell>0.2119</cell></row><row><cell>Web</cell><cell cols="3">C-SIFT 0.2158 0.2207 0.2220 0.2195 O-SIFT 0.2190 0.2276 0.2252 0.2227</cell></row><row><cell></cell><cell cols="2">RGB-SIFT 0.2009 0.2112 0.2062</cell><cell>0.2031</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,134.76,492.11,345.85,173.75"><head></head><label></label><figDesc>we use only Flickr photos. This means that improving performance in Flickr concept annotation task using Web photos is not successful. Although improving performance in Web concept annotation task using Flickr photos seems to be meaningful, Web images are too noisy with such a simple way to combine. Results of grid search on randomly selected 10k train data from web set and test data from development set as a function of iteration number and mean F1-measure (F1) for the four BoVW features.</figDesc><table coords="9,134.76,492.11,345.84,173.75"><row><cell></cell><cell>0.21</cell><cell></cell><cell></cell><cell>C C-SIFT =10 4</cell></row><row><cell></cell><cell>0.20</cell><cell></cell><cell></cell><cell>C C-SIFT =10 5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>C C-SIFT =10 6</cell></row><row><cell></cell><cell>0.19</cell><cell></cell><cell></cell><cell>C O-SIFT =10 4</cell></row><row><cell></cell><cell>0.18</cell><cell></cell><cell></cell><cell>=10 5 C O-SIFT C O-SIFT =10 6</cell></row><row><cell></cell><cell>0.17</cell><cell></cell><cell></cell><cell>C R-SIFT =10 4</cell></row><row><cell>F1</cell><cell>0.16</cell><cell></cell><cell></cell><cell>C R-SIFT =10 5 C R-SIFT =10 6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>C SIFT =10 4</cell></row><row><cell></cell><cell>0.15</cell><cell></cell><cell></cell><cell>C SIFT =10 5</cell></row><row><cell></cell><cell>0.14</cell><cell></cell><cell></cell><cell>C SIFT =10</cell></row><row><cell></cell><cell>0.13</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.12</cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell></row><row><cell></cell><cell></cell><cell cols="2">iteration number</cell></row><row><cell cols="2">1. SIFT + C-SIFT</cell><cell></cell><cell></cell></row><row><cell cols="3">2. SIFT + C-SIFT + Opponent SIFT</cell><cell></cell></row><row><cell cols="3">3. SIFT + C-SIFT + RGB-SIFT</cell><cell></cell></row><row><cell cols="4">4. SIFT + C-SIFT + Opponent SIFT + RGB-SIFT</cell></row><row><cell cols="5">In combination of Web and Flickr photos, we obtained MiAP 0.264, GMiAP</cell></row><row><cell cols="5">0.217 and F1 0.182. However, we obtained MiAP 0.719, GMiAP 0.689 and F1</cell></row><row><cell cols="5">0.553 when Subtask 2: Scalable concept image annotation Before we achieved the</cell></row><row><cell cols="3">results, we took three steps as follows.</cell><cell></cell></row></table><note coords="11,438.14,270.91,2.64,3.75;11,134.76,382.29,27.89,7.89"><p>6 Fig. 2.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="12,134.76,121.94,345.82,201.26"><head>Table 7 .</head><label>7</label><figDesc>SIFT C-SIFT O-SIFT RGB-SIFT F1 Results of summing up the combinations of the scores from each BoVW feature on train data from all web set and test data from all development set.</figDesc><table coords="12,232.94,137.33,156.30,162.39"><row><cell>✓</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.237</cell></row><row><cell>-</cell><cell>✓</cell><cell>-</cell><cell>-</cell><cell>0.258</cell></row><row><cell>-</cell><cell>-</cell><cell>✓</cell><cell>-</cell><cell>0.251</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>✓</cell><cell>0.244</cell></row><row><cell>✓</cell><cell>✓</cell><cell>-</cell><cell>-</cell><cell>0.257</cell></row><row><cell>✓</cell><cell>-</cell><cell>✓</cell><cell>-</cell><cell>0.256</cell></row><row><cell>✓</cell><cell>-</cell><cell>-</cell><cell>✓</cell><cell>0.247</cell></row><row><cell>-</cell><cell>✓</cell><cell>✓</cell><cell>-</cell><cell>0.267</cell></row><row><cell>-</cell><cell>✓</cell><cell>-</cell><cell>✓</cell><cell>0.260</cell></row><row><cell>-</cell><cell>-</cell><cell>✓</cell><cell>✓</cell><cell>0.262</cell></row><row><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>-</cell><cell>0.256</cell></row><row><cell>✓</cell><cell>✓</cell><cell>-</cell><cell>✓</cell><cell>0.258</cell></row><row><cell>✓</cell><cell>-</cell><cell>✓</cell><cell>✓</cell><cell>0.257</cell></row><row><cell>-</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>0.266</cell></row><row><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>0.264</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,657.77,129.63,7.89;3,274.36,662.08,2.85,5.24;3,280.78,657.77,17.84,7.89;3,298.62,656.03,5.76,5.58;3,298.62,662.08,2.85,5.24;3,307.43,657.80,20.86,8.37;3,328.29,656.03,5.76,5.58;3,328.29,662.08,2.85,5.24;3,334.54,657.77,75.08,7.89;3,409.62,656.03,5.76,5.58;3,409.62,657.80,29.23,8.86;3,438.85,656.03,5.76,5.58;3,438.85,657.80,17.52,8.86"><p>Here, the bias b is included inµ t as µ ⊤ t ← [µ ⊤ t , b] by redefining x ⊤ t ← [x ⊤ t , 1]</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="7,144.73,657.80,149.07,7.86"><p>FV from SIFT is not quantized here.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Authors Suppressed Due to Excessive Length SIFT C-SIFT O-SIFT RGB-SIFT F1</head><p>First, in order to assign concepts to each image, we compared the concepts to the raw text extracted near each image. If the raw text contains a concept, the concept is assigned as one of the concepts of the image.</p><p>Second, using randomly sampled 10000 training data from web and test data from all development set, we made grid search as for the two parameters in PA, that is C = {10 4 , 10 5 , 10 6 } and the iteration number N = {10, 15, 20, 25}. These candidates are empirically selected. As a result, shown in Fig. <ref type="figure" coords="10,431.54,428.49,4.13,8.74">2</ref> Finally, utilizing the parameters stated above, we trained the weight vectors µ corresponding to each feature from all 250k web data. After training, we examined all combinations (2 4 = 16) calculated by summing up the candidates from the four dot products of weight vectors and BoVW feature vectors. We assigned three concepts which had highest combined scores to each image from development set. We submitted five runs from 16 combinations as the results of the development set by calculating mean F1-measure (shown in Table <ref type="table" coords="10,451.51,536.08,4.43,8.74">7</ref>) and used the same five combinations in order to achieve the results of the test set.</p><p>Therefore we submitted the following combinations of BoVWs. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.95,388.97,337.63,7.86;12,151.52,399.93,81.01,7.86" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="12,200.21,388.97,261.52,7.86">Large-Scale Machine Learning with Stochastic Gradient Descent</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>COMPSTAT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,410.49,337.64,7.86;12,151.52,421.44,195.89,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,419.37,410.49,61.21,7.86;12,151.52,421.44,88.55,7.86">Online Passive-Aggressive Algorithms</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Keshet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,247.45,421.44,25.72,7.86">JMLR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="551" to="585" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,432.00,337.63,7.86;12,151.52,442.96,190.59,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,300.88,432.00,179.71,7.86;12,151.52,442.96,113.54,7.86">A Discriminative Approach for the Retrieval of Images from Text Queries</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Monay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,286.32,442.96,27.13,7.86">ECML</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,453.52,337.62,7.86;12,151.52,464.48,329.06,7.86;12,151.52,475.44,25.60,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,454.83,453.52,25.75,7.86;12,151.52,464.48,281.39,7.86">Largescale Image Classification: Fast Feature Extraction and SVM Training</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Cour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,453.96,464.48,26.62,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,486.00,337.63,7.86;12,151.52,496.96,329.06,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,418.21,486.00,62.37,7.86;12,151.52,496.96,178.26,7.86">Lear and xrce&apos;s participation to visual concept detection task</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,349.44,496.96,103.14,7.86">CLEF 2010 working notes</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,507.52,337.63,7.86;12,151.52,518.48,179.16,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,319.71,507.52,160.88,7.86;12,151.52,518.48,103.12,7.86">Improving the Fisher Kernel for Large-Scale Image Classification</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,275.52,518.48,26.49,7.86">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,529.04,337.63,7.86;12,151.52,540.00,179.29,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,265.84,529.04,214.74,7.86;12,151.52,540.00,103.12,7.86">High-Dimensional Signature Compression for Large-Scale Image Classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,275.52,540.00,26.62,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,550.56,337.62,7.86;12,151.52,561.52,224.28,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,258.28,550.56,222.30,7.86;12,151.52,561.52,69.46,7.86">Overview of the imageclef 2012 flickr photo annotation and retrieval task</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,241.98,561.52,105.15,7.86">CLEF 2012 working notes</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.95,572.08,337.63,7.86;12,151.52,583.03,213.33,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="12,311.38,572.08,169.20,7.86;12,151.52,583.03,81.05,7.86">Efficient Image Annotation for Automatic Sentence Generation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kuniyoshi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>ACM MM</publisher>
		</imprint>
	</monogr>
	<note>accepted</note>
</biblStruct>

<biblStruct coords="12,142.61,593.59,337.97,7.86;12,151.52,604.55,207.07,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,257.84,593.59,222.74,7.86;12,151.52,604.55,52.25,7.86">Overview of the imageclef 2012 scalable web image annotation task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,224.77,604.55,105.16,7.86">CLEF 2012 working notes</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,615.11,337.98,7.86;12,151.52,626.07,230.65,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,398.72,615.11,81.87,7.86;12,151.52,626.07,154.49,7.86">Locality-constrained Linear Coding for Image Classification</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,326.88,626.07,26.62,7.86">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,636.63,337.97,7.86;12,151.52,647.59,146.54,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,302.17,636.63,178.41,7.86;12,151.52,647.59,71.37,7.86">WSABIE: Scaling Up To Large Vocabulary Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,244.43,647.59,24.95,7.86">IJCAI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
