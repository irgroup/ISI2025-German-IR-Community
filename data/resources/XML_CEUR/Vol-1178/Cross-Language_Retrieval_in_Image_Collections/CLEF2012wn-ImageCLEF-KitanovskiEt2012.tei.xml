<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.98,151.67,322.75,12.54;1,227.33,169.07,140.72,12.54">FCSE at ImageCLEF 2012: Evaluating techniques for medical image retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,186.41,207.32,62.31,9.05"><forename type="first">Ivan</forename><surname>Kitanovski</surname></persName>
							<email>ivan.kitanovski@finki.ukim.mk</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science and Engineering</orgName>
								<address>
									<settlement>Skopje</settlement>
									<country key="MK">Macedonia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,255.46,207.32,70.76,9.05"><forename type="first">Ivica</forename><surname>Dimitrovski</surname></persName>
							<email>ivica.dimitrovski@finki.ukim.mk</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science and Engineering</orgName>
								<address>
									<settlement>Skopje</settlement>
									<country key="MK">Macedonia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,345.62,207.32,74.56,9.05"><forename type="first">Suzana</forename><surname>Loskovska</surname></persName>
							<email>suzana.loshkovska@finki.ukim.mk</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science and Engineering</orgName>
								<address>
									<settlement>Skopje</settlement>
									<country key="MK">Macedonia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.98,151.67,322.75,12.54;1,227.33,169.07,140.72,12.54">FCSE at ImageCLEF 2012: Evaluating techniques for medical image retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9FA2F172EACA251330971F3995B03574</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Information Retrieval</term>
					<term>Medical Imaging</term>
					<term>Content-based Image Retrieval</term>
					<term>Medical Image Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the details of the participation of FCSE (Faculty of Computer Science and Engineering) research team in ImageCLEF 2012 medical retrieval task. We investigated by evaluating different weighting models for text retrieval. In the case of the visual retrieval, we focused on extracting low-level features and examining their performance. For, the multimodal retrieval we used late fusion to combine the best text and visual results. We found that the choice of weighting model for text retrieval dramatically influences the outcome of the multimodal retrieval. We tested the multimodal retrieval on data from ImageCLEF 2011 medical task and based on that we submitted new experiments for ImageCLEF 2012. The results show that fusing different modalities in the retrieval can improve the overall retrieval performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we present the experiments performed by the Faculty of Computer Science and Engineering (FSCE) team for the medical retrieval task at ImageCLEF <ref type="bibr" coords="1,124.82,520.21,11.69,9.05" target="#b0">[1]</ref> 2012.</p><p>The task of medical image retrieval consists of retrieving the most relevant images to a given query from a database of images. Medical image retrieval from medical image databases does not aim to replace the physician by predicting the disease of a particular case but to assist him/her in diagnosis. By consulting the output of a medical image retrieval system, the physician can gain more confidence in his/her decision or even consider other possibilities.</p><p>There are two forms of medical image retrieval: text-based (textual) and contentbased (visual) <ref type="bibr" coords="1,190.88,612.25,10.74,9.05" target="#b0">[1]</ref>. In text-based image retrieval images are usually manually annotated with keywords or a short caption, which describe their content, or in the case of medical images the keywords are related to modality of the image, the present body part, the disease or anomaly depicted. In the latter stage, the user provides textual queries and the retrieval is performed using traditional text retrieval techniques. In visual retrieval the images are represented using descriptors (automatically generated) which describe the visual content of the images. Descriptors are usually numerical by nature and are represented as vectors of numbers <ref type="bibr" coords="2,429.51,149.48,10.91,9.05" target="#b1">[2]</ref>. In the retrieval phase, the user provides visual queries (query images) and the retrieval is performed by comparing descriptors of the query images to those of all images in the database <ref type="bibr" coords="2,161.78,184.04,10.69,9.05" target="#b2">[3]</ref>.</p><p>Recently, multimodal image retrieval arises as an active research topic <ref type="bibr" coords="2,430.84,195.56,11.78,9.05" target="#b3">[4]</ref> and is also part of medical task of ImageCLEF. Multimodal image retrieval is the process of using both text-based and visual-based techniques for retrieval. In multimodal retrieval the user provides textual queries and query images and retrieval should provide an ordered set of images related to that complex query. The authors of <ref type="bibr" coords="2,442.62,241.52,12.22,9.05" target="#b4">[5]</ref> use late fusion to combine the results from text-based and visual-based retrieval. For the text-based retrieval, they use a bag-of-words representation on the image captions and DFR-BM25 model for the retrieval. In the visual-based retrieval, they describe the images using a low-level feature called CEDD, and the retrieval is performed using Img(Rummager). Then, using late fusion they combine the results. The most efficient strategy was a linear combination scheme. In <ref type="bibr" coords="2,322.63,310.55,10.87,9.05" target="#b5">[6]</ref>, the authors use Late Semantic Combination for multimodal retrieval. They represent each image caption with a bagof-words representation and for the retrieval they compare several models: Dirichlet Smoothed Language (DIR), Log-logistic Information-Based Model (LGD), Smoothed Power Law Information-Based Model (SPL) and Lexical Entailment based IR Model (AX). In the visual retrieval, the images are described with ORH and COL features and they use dot product as similarity measure for the visual retrieval. In <ref type="bibr" coords="2,442.40,379.55,11.74,9.05" target="#b6">[7]</ref> the authors use linear late fusion for multimodal retrieval. The text-based retrieval is performed using Lucene and the visual-based using Lira. The multimodal retrieval is performed by linear combination of scores from the text-based and visual-based retrieval and re-ranking. The authors of <ref type="bibr" coords="2,289.06,425.51,11.79,9.05" target="#b7">[8]</ref> first perform text-based retrieval and use those results as a filter for the visual-based retrieval. They use low-level texture and color features (CEDD) for the visual-based retrieval. Text-based retrieval is performed using a Lucene.</p><p>The paper is organized as follows: Section 2 presents the feature set which is used for the text-based and visual-based retrieval. Multimodal retrieval techniques are presented in section 3. The evaluation of our used features is described in section 4. Section 5 provides the submitted runs. The concluding remarks are given in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Feature Set</head><p>Feature selection is a very important part in every information retrieval system, since it directly influences the performance. In this paper we analyze text features for the text-based retrieval and visual features for visual-based retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Textual Features</head><p>Text-based retrieval is needed when we have text describing the image content i.e. image caption. From the related work we can conclude that regarding the text-based retrieval a traditional bag-of-words representation can be used for the image caption.</p><p>The image captions are first pre-processed. Pre-processing includes stemming and stop words removal <ref type="bibr" coords="3,205.95,161.00,10.79,9.05" target="#b4">[5]</ref>, which is needed so we can extract only the vital information.</p><p>The choice of a weighting model may crucially affect the retrieval hence we evaluated the positive and negative sides of different weighting models. We evaluated the following models: PL2 <ref type="bibr" coords="3,239.91,195.56,10.88,9.05" target="#b8">[9]</ref>, BM25 <ref type="bibr" coords="3,287.90,195.56,10.64,9.05" target="#b8">[9]</ref>, DFR-BM2 <ref type="bibr" coords="3,353.21,195.56,10.69,9.05" target="#b8">[9]</ref>, BB2 <ref type="bibr" coords="3,393.85,195.56,11.66,9.05" target="#b8">[9]</ref> and one of the most popular TF-IDF <ref type="bibr" coords="3,214.82,206.96,15.45,9.05" target="#b9">[10]</ref>. We choose these models as one the most commonly used in real practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual Features</head><p>Related work shows that low-level features are typically used in content-based image retrieval systems, since they typically deal with large image databases. These features are called low-level because they have little or nothing with human perception. We decided to use the following features:  Color and Edge Directivity Descriptor (CEDD) combines EHD feature <ref type="bibr" coords="3,432.10,321.23,17.23,9.05" target="#b10">[11]</ref> with color histogram information. This descriptor is limited in size to 54 bytes per image, which makes it appropriate for large image databases. Important attribute of the CEDD is the low computational power needed for its extraction, in comparison to the needs of the most MPEG-7 descriptors.  Fuzzy Color and Texture Histogram (FCTH) is a fuzzy version of CEDD feature which contains fuzzy set of color and texture histogram <ref type="bibr" coords="3,382.61,390.95,15.64,9.05" target="#b11">[12]</ref>. FCTH contains results from the combination of three fuzzy systems including histogram, color and texture information. This feature is limited in size to 72 bytes per image and that makes it suitable for use in large image databases.  The Scalable Fuzzy Brightness and Texture Directionality Histogram (BTDH), was originally created for representing radiology images <ref type="bibr" coords="3,349.51,449.15,15.43,9.05" target="#b12">[13]</ref>. BTDH is very similar to FCTH feature. The main difference from FCTH feature is using brightness instead of color histogram. It combines brightness and texture characteristics and their spatial distribution in one compact vector by using a two-unit fuzzy system. This feature does not contain color data, since it was meant for grayscale images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Fusion Techniques for Multimodal Retrieval</head><p>Multimodal information retrieval refers to the task of using multiple media to perform a retrieval task. Multimodal retrieval is usually done by fusing multiple modalities. Fusing multiple modalities can improve the overall accuracy in the decision making process <ref type="bibr" coords="3,157.30,594.97,15.37,9.05" target="#b13">[14]</ref>.</p><p>The fusion of multiple modalities can be performed at feature level or decision level. In fusion at feature level, also known as early fusion, various features extracted from the input data are combined in some fashion and then that newly created feature is sent as input to the module that performs the analysis task. In fusion at decision level, also known as late fusion, the analysis units first provide the local decisions that are obtained based on individual features. Afterwards a decision fusion unit combines local decisions to create a new fused decision vector which is analyzed to provide a final decision about the task. To utilize the merits of both approaches, researchers have attempted to create hybrid techniques which are a combination of both feature and decision level techniques.</p><p>Related work shows that the late fusion strategy is frequently used. Late fusion has many advantages over early fusion. The decisions usually have the same representation. For instance, the result of both text-based and visual-based retrieval is an ordered list of images. Hence, the implementation of fusion techniques becomes much easier. Furthermore, late fusion allows for modularity and scalability in terms of modalities used in the fusion process, which is quite difficult to achieve with early fusion techniques. Additionally, late fusion allows us to use the optimal analyzing methods for each modality separately which cannot be done with early fusion techniques.</p><p>Because of these merits we used late fusion strategy in our experiments. We turned to Linear Weighed Fusion strategy, one of the simplest and most widely used methods. We applied this strategy to results obtained from the separate text-based and visual-based retrievals. Each, retrieval contains an ordered list of images with computed similarity scores. The weighted average function is applied by multiplying each individual similarity with a weight value. The weight assignment to individual scores defines the importance of each modality in the decision making process. If a modality has a high weight it will have significant impact on the final results and vice versa. In this way we can control the influence of individual modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluating Features on ImageCLEF2011</head><p>The evaluation consists of three kinds of retrieval: text-based, visual-based and multimodal retrieval. We use the text-based and visual-based retrieval to find the best weighting model and descriptor, which we latter use in the multimodal retrieval.</p><p>The data for the evaluation is provided from the collection of the ImageCLEF 2011 medical task <ref type="bibr" coords="4,178.25,491.53,10.69,9.05" target="#b0">[1]</ref>. The collection contains textual and visual information. It consists of 230088 images, each described with a short text (image caption). The queries which we used for testing are the same which were provided for the medical retrieval task. Participants were given a set of 30 textual queries with 2-3 sample images for each query. The queries are classified into textual, visual or mixed (multimodal) based on the data and techniques used. For the text-based retrieval, we only use the text queries. On the other hand, for the visual-based retrieval we use the images provided for each query. Finally, for the multimodal retrieval we use both text and image data provided for every query.</p><p>The text-based retrieval was performed using Terrier IR Platform <ref type="bibr" coords="4,400.68,595.09,15.51,9.05" target="#b14">[15]</ref>, open source search engine written in Java which is developed at School of Computer Science, University of Glasgow. For stemming we used Porter stemmer <ref type="bibr" coords="4,380.26,618.01,16.96,9.05" target="#b15">[16]</ref> for English, since the image captions and text queries are in English. Terrier also has a predefined stop words list, which we use in the preprocessing stage. All weighting models which we analyze are integrated in Terrier. The results from these evaluations in presented on Table <ref type="table" coords="4,150.13,664.12,3.76,9.05" target="#tab_0">1</ref>. The visual-based retrieval was performed with the aid of the Img(Rummager) application <ref type="bibr" coords="5,171.98,310.79,15.46,9.05" target="#b16">[17]</ref>, developed in the Automatic Control Systems &amp; Robotics Laboratory at the Democritus University of Thrace-Greece. CEDD, FCTH and BDTH features are implemented in the application. The retrieval stage greatly relies on the distance/similarity function used to quantitatively compare the images. We compute the similarity score based on Tanimoto distance <ref type="bibr" coords="5,323.75,356.75,15.59,9.05" target="#b10">[11]</ref>, since it is one most frequently used methods for the visual features which we use to describe the images. Since, there were multiple images per query we used an averaging technique in this stage <ref type="bibr" coords="5,434.62,379.79,10.69,9.05" target="#b4">[5]</ref>. Here we calculated a mean descriptor from all images in a query, thus creating one new feature vector which will be passed as a query. Visual results are presented at Table <ref type="table" coords="5,461.81,402.71,3.76,9.05" target="#tab_1">2</ref> The multimodal retrieval is performed using late fusion strategy. In this stage we pick the best weighting model for text-based retrieval and the best performing descriptor for visual-based retrieval. Then, we combine the results from the separate retrievals using linear combination. The formula by which we calculated the score for each image in the retrieval is the following:</p><formula xml:id="formula_0" coords="5,177.05,598.56,293.77,9.87">( text_score * w 1 + visual_score * w 2 ) / 100= score (1)</formula><p>After comparing different studies <ref type="bibr" coords="5,274.75,627.61,16.97,9.05" target="#b17">[18]</ref> and experimenting with various parameters we determined to multiply the text score with 85 and the visual score with 15, thus giving a greater influence to the text-based component.</p><p>Before we combine the score we need to normalize them to get more valid and accurate results since different modalities calculate different ranges of values in the similarity score. Here we apply the most common used method for normalization i.e.</p><p>Min-Max normalization <ref type="bibr" coords="6,224.33,149.48,15.43,9.05" target="#b18">[19]</ref>. This normalization ensures that the values of the scores are in the range from 0 to 1. The lowest value is set to 0 and the highest value is set to 1. This allows us to compare values that are measured using different scales. After normalization takes place, we turn to linear combination of the modified retrieval scores.</p><p>In this case, we make three types of experiment to assess the change in retrieval performance. First, we make linear combination of the text-based and visual-based retrieval. The second approach slightly modifies the text-based retrieval with query expansion, since the text-based retrieval has the crucial impact on final result. The third approach uses query expansion and word weighting. This approach assigns weights to special words, in our case image modalities (i.e. MRI, CT, X-RAY etc.). We added a weight of 2.5 to these words using query language of Terrier. The results (Table <ref type="table" coords="6,154.30,287.51,4.17,9.05" target="#tab_2">3</ref>) show that there is an improvement of the retrieval compared to text-based retrieval in every multimodal experiment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Submitted Results on ImageCLEF 2012</head><p>After evaluating the performance of different visual features, weighting models and linear combination parameters we could evaluate which performs best under which conditions. We made another experiment, only now using ImageCLEF 2012 data and submitted the results only from the best performing techniques. For text-based retrieval we submitted the run using BM25 weighting model using query expansion and word weights and for the visual-based retrieval we submitted the run using CEDD descriptor. Finally, for the multimodal retrieval we submitted the linear combination of the two previous modalities. The results from our runs on ImgeCLEF 2012 are presented on Table <ref type="table" coords="6,203.45,554.17,3.71,9.05" target="#tab_3">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we explained in detail our participation in ImageCLEF 2012. We examined the effects of different weighting models for text-based retrieval and concluded that the choice of a weighting model dramatically influences retrieval. In the case of visual-based retrieval, we compared several low level visual features and found that CEDD descriptor to be the best suited for this type of task. Additionally, we investigated in late fusion for multimodal retrieval. We used linear combination for late fusion of the text-based and visual-based retrieval results. The obtained results show that by combining the two modalities the overall retrieval performance can be improved.</p><p>Medical image retrieval is a crucial task which can aid the work of medical practitioners. It is a very complex which can be improved in many aspects from improving current weighting models to developing or modifying features to describe the image content and creating different techniques to combine these two modalities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,144.50,149.30,306.47,108.36"><head>Table 1 .</head><label>1</label><figDesc>Comparison of weighting models in text-based retrieval</figDesc><table coords="5,144.50,173.22,306.47,84.44"><row><cell>Model</cell><cell>MAP</cell><cell>P10</cell><cell>P20</cell><cell>Rprec</cell><cell># of rel. docs</cell></row><row><cell>BB2</cell><cell>0.2059</cell><cell>0.3700</cell><cell>0.3100</cell><cell>0.2425</cell><cell>1472</cell></row><row><cell>BM25</cell><cell>0.2144</cell><cell>0.3633</cell><cell>0.3200</cell><cell>0.2449</cell><cell>1504</cell></row><row><cell>DFR-BM25</cell><cell>0.2054</cell><cell>0.3533</cell><cell>0.2967</cell><cell>0.2426</cell><cell>1494</cell></row><row><cell>PL2</cell><cell>0.1970</cell><cell>0.3533</cell><cell>0.2967</cell><cell>0.2413</cell><cell>1474</cell></row><row><cell>TF-IDF</cell><cell>0.2048</cell><cell>0.3533</cell><cell>0.3033</cell><cell>0.2398</cell><cell>1482</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,152.30,402.71,317.03,101.24"><head>Table 2 .</head><label>2</label><figDesc>. Comparison of features in visual-based retrieval</figDesc><table coords="5,152.30,449.37,294.32,54.58"><row><cell>Feature</cell><cell>MAP</cell><cell>P10</cell><cell>P20</cell><cell>Rprec</cell><cell># of rel. docs</cell></row><row><cell>CEDD</cell><cell>0.0142</cell><cell>0.0867</cell><cell>0.0733</cell><cell>0.0401</cell><cell>552</cell></row><row><cell>FCTH</cell><cell>0.0134</cell><cell>0.0633</cell><cell>0.0483</cell><cell>0.0342</cell><cell>621</cell></row><row><cell>BDTH</cell><cell>0.0053</cell><cell>0.0419</cell><cell>0.0372</cell><cell>0.0216</cell><cell>217</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,143.66,321.89,311.03,78.36"><head>Table 3 .</head><label>3</label><figDesc>Results of the multimodal retrieval experiments</figDesc><table coords="6,143.66,345.69,311.03,54.56"><row><cell>Mode</cell><cell>MAP</cell><cell>P10</cell><cell>P20</cell><cell>Rprec</cell><cell># of rel. docs</cell></row><row><cell>mixed</cell><cell>0.2148</cell><cell>0.3600</cell><cell>0.3233</cell><cell>0.2579</cell><cell>1531</cell></row><row><cell>mixed + qe</cell><cell>0.2179</cell><cell>0.3833</cell><cell>0.3433</cell><cell>0.2577</cell><cell>1483</cell></row><row><cell>mixed + ww</cell><cell>0.2232</cell><cell>0.3933</cell><cell>0.3467</cell><cell>0.2568</cell><cell>1458</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,151.22,577.03,295.80,78.36"><head>Table 4 .</head><label>4</label><figDesc>Runs of FCSE group in ImageCLEFMed 2012</figDesc><table coords="6,151.22,600.83,295.80,54.56"><row><cell>Type</cell><cell>MAP</cell><cell>GM-MAP</cell><cell>bpref</cell><cell>P10</cell><cell>P30</cell></row><row><cell>text</cell><cell>0.1763</cell><cell>0.0498</cell><cell>0.1773</cell><cell>0.2909</cell><cell>0.1864</cell></row><row><cell>visual</cell><cell>0.0041</cell><cell>0.0003</cell><cell>0.0105</cell><cell>0.0318</cell><cell>0.0364</cell></row><row><cell>mixed</cell><cell>0.1794</cell><cell>0.049</cell><cell>0.1851</cell><cell>0.3</cell><cell>0.1894</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,128.23,391.97,342.49,8.18;7,136.22,402.29,314.33,8.18" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="7,148.70,402.29,275.17,8.18">Overview of the CLEF 2011 medical image classification and retrieval tasks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy--Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">G S</forename><surname>De Herrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tsikrika</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.23,412.61,342.61,8.18;7,136.22,423.05,163.04,8.18" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,290.69,412.61,176.64,8.18">Image processing, analysis, and machine vision</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sonka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Hlavac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Boyle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>PWS publishing Pacific</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>Grove, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.23,433.37,342.57,8.18;7,136.22,443.69,334.57,8.18;7,136.22,454.01,53.35,8.18" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,221.69,433.37,223.93,8.18">An overview of content-based image retrieval techniques</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,454.78,433.37,16.01,8.18;7,136.22,443.69,304.85,8.18">th International Conference on Advanced Information Networking and Applications</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="59" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.23,464.45,342.42,8.18;7,136.22,474.79,334.58,8.18;7,136.22,485.11,24.09,8.18" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,280.01,464.45,190.63,8.18;7,136.22,474.79,16.78,8.18">Overview of the CLEF 2009 medical image retrieval track</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy--Cramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,159.26,474.79,263.65,8.18">Multilingual Information Access Evaluation II. Multimedia Experiments</title>
		<imprint>
			<biblScope unit="page" from="72" to="84" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.23,495.43,342.60,8.18;7,136.22,505.87,334.58,8.18;7,136.22,516.19,334.32,8.18;7,136.22,526.51,116.40,8.18" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,430.54,495.43,40.28,8.18;7,136.22,505.87,60.87,8.18;7,226.62,505.87,244.18,8.18;7,136.22,516.19,88.27,8.18">Evaluation of Fusion Techniques for Multimodal Content-based Medical Image Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Alpkocak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Ozturkmenoglu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Berber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">H</forename><surname>Vahid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">G</forename><surname>Hamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,230.33,516.19,235.90,8.18">12th Workshop of the Cross-Language Evaluation Forum (CLEF)</title>
		<meeting><address><addrLine>Amsterdam, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
	<note>DEMIR at ImageCLEFMed</note>
</biblStruct>

<biblStruct coords="7,128.23,536.83,342.45,8.18;7,136.22,547.27,255.64,8.18" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="7,284.33,536.83,186.35,8.18;7,136.22,547.27,229.18,8.18">XRCE&apos;s Participation at Medical Image Modality Classification and Ad-hoc Retrieval Tasks of ImageCLEF 2011</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jacquet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.23,557.59,342.49,8.18;7,136.22,567.91,115.56,8.18" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,292.13,557.59,174.61,8.18">IPL at ImageCLEF 2011 Medical Retrieval Task</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gkoufas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Morou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kalamboukis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,136.22,567.91,89.05,8.18">Working Notes of CLEF</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.23,578.23,342.58,8.18;7,136.22,588.55,135.49,8.18" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia-Serrano</surname></persName>
		</author>
		<title level="m" coord="7,385.39,578.23,85.41,8.18;7,136.22,588.55,109.11,8.18">UNED-UV at Medical Retrieval Task of ImageCLEF</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,128.23,598.99,342.32,8.18;7,136.22,609.31,334.37,8.18;7,136.22,619.63,116.86,8.18" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,265.25,598.99,205.30,8.18;7,136.22,609.31,162.34,8.18">Probabilistic models of information retrieval based on measuring the divergence from randomness</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,306.67,609.31,163.92,8.18;7,136.22,619.63,24.15,8.18">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="357" to="389" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,132.40,629.95,338.25,8.18;7,136.22,640.39,268.93,8.18" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,190.61,629.95,280.04,8.18;7,136.22,640.39,28.92,8.18">A probabilistic justification for using tf-idf term weighting in information retrieval</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,170.66,640.39,146.78,8.18">International Journal on Digital Libraries</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="131" to="139" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,131.87,650.71,338.67,8.18;7,136.22,661.06,330.52,8.18" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,262.25,650.71,208.29,8.18;7,136.22,661.06,153.14,8.18">Cedd: Color and edge directivity descriptor: A compact descriptor for image indexing and retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Boutalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,294.91,661.06,92.67,8.18">Computer Vision Systems</title>
		<imprint>
			<biblScope unit="page" from="313" to="322" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,131.87,149.30,338.70,8.18;8,136.22,159.74,334.43,8.18;8,136.22,170.06,192.83,8.18" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,277.37,149.30,193.21,8.18;8,136.22,159.74,129.96,8.18">Fcth: Fuzzy color and texture histogram-a low level feature for accurate image retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,272.81,159.74,197.84,8.18;8,136.22,170.06,113.97,8.18">Ninth International Workshop on Image Analysis for Multimedia Interactive Services</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="191" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,131.87,180.38,338.68,8.18;8,136.22,190.70,334.64,8.18;8,136.22,201.14,39.80,8.18" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,274.49,180.38,196.07,8.18;8,136.22,190.70,146.27,8.18">Content based radiology image retrieval using a fuzzy rule based scalable composite descriptor</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,288.65,190.70,128.30,8.18">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="493" to="519" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.36,211.46,338.39,8.18;8,136.22,221.78,276.96,8.18" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="8,385.75,211.46,85.00,8.18;8,136.22,221.78,105.57,8.18">Multimodal fusion for multimedia analysis: a survey</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">K</forename><surname>Atrey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>El Saddik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,247.85,221.78,72.53,8.18">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="345" to="379" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.63,232.10,338.00,8.18;8,136.22,242.54,315.03,8.18" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="8,445.78,232.10,24.85,8.18;8,136.22,242.54,106.47,8.18">Terrier information retrieval platform</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,248.69,242.54,123.91,8.18">Advances in Information Retrieval</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="517" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.40,252.86,199.57,8.18" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="8,187.25,252.86,118.38,8.18">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,131.87,263.18,338.83,8.18;8,136.22,273.50,334.49,8.18;8,136.22,283.85,121.07,8.18" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="8,318.79,263.18,151.91,8.18;8,136.22,273.50,108.42,8.18">Img (rummager): An interactive content based image retrieval system</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,252.77,273.50,217.94,8.18;8,136.22,283.85,44.64,8.18">Second International Workshop on Similarity Search and Applications</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="151" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,132.40,294.29,338.31,8.18;8,136.22,304.61,93.83,8.18" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="8,192.17,294.29,177.12,8.18">Combining approaches to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,376.99,294.29,93.72,8.18;8,136.22,304.61,28.92,8.18">Advances in information retrieval</title>
		<imprint>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,131.83,314.93,338.88,8.18;8,136.22,325.25,168.57,8.18" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="8,271.85,314.93,195.01,8.18">Score normalization in multimodal biometric systems</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nandakumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,136.22,325.25,67.55,8.18">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2270" to="2285" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
