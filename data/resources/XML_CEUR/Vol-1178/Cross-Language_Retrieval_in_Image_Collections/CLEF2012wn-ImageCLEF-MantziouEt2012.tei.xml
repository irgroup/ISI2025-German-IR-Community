<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,138.78,115.96,337.80,12.62;1,220.05,133.89,175.26,12.62">CERTH&apos;s participation at the photo annotation task of ImageCLEF 2012</title>
				<funder ref="#_Punbf5u">
					<orgName type="full">European Commission</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,142.86,171.56,64.46,8.74"><forename type="first">Eleni</forename><surname>Mantziou</surname></persName>
							<email>lmantziou@iti.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Information Technologies Institute Centre for Research and Technology Hellas</orgName>
								<address>
									<settlement>Thessaloniki</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,215.51,171.56,68.08,8.74"><forename type="first">Georgios</forename><surname>Petkos</surname></persName>
							<email>gpetkos@iti.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Information Technologies Institute Centre for Research and Technology Hellas</orgName>
								<address>
									<settlement>Thessaloniki</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,291.40,171.56,95.78,8.74"><forename type="first">Symeon</forename><surname>Papadopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Technologies Institute Centre for Research and Technology Hellas</orgName>
								<address>
									<settlement>Thessaloniki</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,395.37,171.56,72.42,8.74"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
							<email>sagonas@iti.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Information Technologies Institute Centre for Research and Technology Hellas</orgName>
								<address>
									<settlement>Thessaloniki</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.36,183.51,94.01,8.74"><forename type="first">Yiannis</forename><surname>Kompatsiaris</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Technologies Institute Centre for Research and Technology Hellas</orgName>
								<address>
									<settlement>Thessaloniki</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,138.78,115.96,337.80,12.62;1,220.05,133.89,175.26,12.62">CERTH&apos;s participation at the photo annotation task of ImageCLEF 2012</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0C1228CB907DA6DF6AD3B94BBFA8FACC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the approaches and experimental settings of the five runs submitted by CERTH at the photo annotation task of ImageCLEF 2012. Two different approaches were used, the first using the Laplacian Eigenmaps of an image similarity graph for learning, and the second using a "same class" learning model. Four runs were submitted using the first, and one using the second approach. A multitude of textual and visual features were employed, making use of different aggregation (BoW, VLAD) and post-processing schemes (WordNet, pLSA). The best performance scores in the test set was achieved by Run 3 (first approach using all features), which amounted to 0.321 in terms of MiAP and 0.2547 in terms of GMiAP (7th out of 18 compteting teams), and Run 5 which led to an F-ex score of 0.495 (6th out of 18 teams).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This document describes the participation of CERTH at the photo annotation task of the 2012 ImageCLEF competition <ref type="bibr" coords="1,317.04,444.60,9.96,8.74" target="#b0">[1]</ref>. CERTH submitted five runs using two different approaches. The first approach, to be described in subsection 2.1, computes the similarity between test images and train images, constructs an image similarity graph, and trains concept detectors by using the graph Laplacian Eigenmaps (LE) <ref type="bibr" coords="1,211.49,492.42,10.52,8.74" target="#b6">[7]</ref> as features. This is done for each modality and the final result is obtained by performing late fusion using a linear classifier. The second approach, to be detailed in subsection 2.2, utilizes the concept of a "same class" model that takes as input the set of distances (as many as the number of used features) between the image to be annotated and a reference item that represents a target concept, and predicts whether the image belongs to the target concept. Section 3 outlines each of the submitted runs and presents the obtained test results. Section 4 presents some general remarks and conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Concept detection using image similarity graphs</head><p>The first approach used by CERTH is based on the construction of a similarity graph between the images. This graph is used to obtain a low-dimensional feature representation: we use the first eigenvectors of the graph Laplacian as features. These features correspond well to semantically coherent groups of images, and are thus used to train concept classifiers.</p><p>The idea of utilizing the implicit relational structure that can be derived by computing similarities between the images of a collection has been proposed before. In <ref type="bibr" coords="2,169.94,179.63,9.96,8.74" target="#b7">[8]</ref>, an extended similarity measure is proposed that takes into account the local neighbourhood structure of images, i.e, the content and label information (if available) of images that are similar to the input image. The aforementioned measure is used in combination with two well-known semi-supervised learning methods <ref type="bibr" coords="2,213.97,227.45,15.50,8.74" target="#b15">[16]</ref> and is shown to improve their performance both in synthetic experiments and in benchmark video annotation task. Our work is mostly related to <ref type="bibr" coords="2,181.01,251.36,10.52,8.74" target="#b5">[6]</ref> that introduces the concept of "social dimensions", i.e. the top-k eigenvectors of a graph Laplacian, as an alternative to tackling the relational classification problem, <ref type="bibr" coords="2,234.46,275.27,14.61,8.74" target="#b9">[10]</ref>, i.e. the classification of a graph node by taking into account information from neighbouring nodes. Here, we adopt a similar representation for graph structure features.</p><p>Method overview: Given a set of K target concepts Y = {Y 1 ....Y K } and an annotated set L = {(x i , y i )} l i=1 of training samples, where x i ∈ D stands for the feature vector extracted from content item i and y i ∈ {0, 1} K for the corresponding concept indicator vector, a transductive learning algorithm attempts to predict concepts associated with a set of unknown items U = {x j } l+u j=l+1 , by processing together sets L and U. Based on the features of the input items, a graph G = (V,E) is constructed that represents the similarities between all pairs of items. The nodes of the graph include the items of both sets of media items (L and U), i.e. V = V L ∪ V U with |V | = n. There are different options for constructing such a graph. We adopt the kNN graph in which an edge is inserted between items i and j as long as one of them belongs to the set of top-k most similar items of the other. Similarity can be computed by means of different schemes, e.g. inner product or heat kernel (Equation <ref type="formula" coords="2,326.52,455.46,3.87,8.74" target="#formula_0">1</ref>).</p><formula xml:id="formula_0" coords="2,254.24,478.38,226.35,23.89">w ij = exp - |x i -x j | 2 t<label>(1)</label></formula><p>The basic variants of this scheme are symmetric and asymmetric, depending on whether both items to be linked need to belong to the set of top-k most similar items of each other or not. Having constructed the similarity graph between the input items, our approach proceeds with mapping the graph nodes to feature vectors that represent the associations of nodes with latent groups of nodes forming densely connected clusters. To extract such features, we first construct the normalized graph Laplacian:</p><formula xml:id="formula_1" coords="2,223.50,609.18,257.09,11.26">L = D -1/2 LD -1/2 = I -D -1/2 AD -1/2 (2)</formula><p>where D and A are the degree and adjacency matrix of the graph respectively, and L = D-A is the graph Laplacian. Computing the eigenvectors of L corresponding to the C D smallest non-zero eigenvalues of the matrix results in a set of n vectors with C D dimensions, which are then stacked to form the input matrix S ∈ nxC D , each row of which is denoted as S i ∈ C D and constitutes the graph structure feature vector for media item i. These features are also known as Laplacian Eigenmaps (LE) <ref type="bibr" coords="3,267.16,154.86,9.96,8.74" target="#b6">[7]</ref>.</p><p>Training and performance tuning: We approximately optimise the values from the top-k most similar items, the LEs and the c parameter from the SVM linear classifier in order to construct sets of parameters per concept for each initial feature. In practice, we choose six different top-k [100, 200, 500, 1000, 1500, 2000] nearest neighbours values and for each one we compute LE vectors for six different values <ref type="bibr" coords="3,203.54,226.59,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="3,222.26,226.59,12.73,8.74">50,</ref><ref type="bibr" coords="3,238.20,226.59,17.71,8.74">100,</ref><ref type="bibr" coords="3,259.13,226.59,17.71,8.74">200,</ref><ref type="bibr" coords="3,280.05,226.59,17.71,8.74">400,</ref><ref type="bibr" coords="3,300.98,226.59,17.71,8.74">500]</ref> for C D using spectral clustering. For the parameter c we investigate the performance of the SVM classifier by doing cross validation and to decide which of the five different values [0.1, 1, 5, 50] yields the best performance. In most cases the best classification was achieved for c = 5 and, thus, we set this as the default value. This procedure was done for every feature and every concept in order to choose the best parameter set (top-k, C D ) for each concept-feature configuration. A late fusion step would then output an overall prediction score. This simple late fusion technique is implemented by simple LE vector concatenation and an optional feature normalization step after the final step was evaluated, but led to marginally lower performance, thus it was not used for preparing the final submission. In the final step, a linear classifier is trained using the structure feature vectors of the labelled items as input. In our implementation, we opted for the use of SVM. Apart from classification performance considerations, it is important for retrieval applications that the classifier produces real-valued prediction scores for unlabelled items, so that they can be ranked per concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Concept detection using a same class model</head><p>A very large variety of features can be extracted from an image. For detecting different concepts, the use of specific features or modalities may be more appropriate than others. That is, it could be that for some concept, similarity according to some feature or modality between an image and some set of images that belong to a specific concept is a very strong indicator that the image belongs to that concept; whereas for other concepts similarity according to some other set of features may be more indicative. The second approach attempts to deal with this issue; i.e. to learn in an automatic manner which modalities should be used for the detection of specific concepts. It uses what is termed the "same class" model. A "same-class" model takes as input the set of pairwise dissimilarities between two images according to the set of features and modalities that are used and predicts if these two images belong to the same class.</p><p>There are two options for training and predicting with such a model. In the first, all images that belong to the target concept are used. Pairs of samples from these images are generated in order to come up with the positive examples of the classifier. Additionally, a set of images that do not belong to the target concept are selected and pairs of images consisting of an image that does and an image that does not include the concept are generated in order to come up with the negative examples. For a new image, the pairwise distances between it and the set of reference images that belong to that class would be computed and fed into the classifier that would output a set of scores, each of which is a prediction if the new image belongs to the same class as each image in the reference set for that concept. A final fusion step would then output an overall prediction score. This approach is depicted in Figure <ref type="figure" coords="4,293.36,178.77,3.87,8.74">1</ref>.</p><p>Fig. <ref type="figure" coords="4,154.40,399.45,4.13,7.89">1</ref>. A vector of dissimilarities for the set of used features is computed between the image to be annotated and the each of example the images that belong to the concept. These vectors are fed to the "same class" model and the predictions are fused to obtain a final prediction for the membership of the test image to the particular concept.</p><p>The first option essentially represents each concept by the set of images that belong to that concept and requires a final fusion step. The other option is to represent each concept using a mock average image, e.g. for each feature the average value for the images that belong to that class is computed and the set of all average values is used to build a prototype feature set for the items that belong to that class. The rest of the procedure is similar as in the first scenario: a set of positive examples is generated by computing the multimodal distances between images that belong to the target concept and the prototype representation of the concept. A set of negative examples is generated in a similar manner. When a new image is being annotated, the vector of distances between it and the prototype image is computed and fed into the classifier. Contrary to the previous case, there is no need for a final fusion step, as the classifier provides a single "same-class" prediction. This approach is depicted in Figure <ref type="figure" coords="4,437.52,608.30,3.87,8.74">2</ref>.</p><p>Compared to the first option, the second is more crude, in the sense that information from individual images that may be important for concept detection may be lost during averaging. On the other hand, the second option is computationally more efficient and does not require a final fusion step. Pursuing the Fig. <ref type="figure" coords="5,154.40,225.49,4.13,7.89">2</ref>. A vector of dissimilarities for the set of used features is computed between the image to be annotated and the aggregate image that represents the concept. The vector of dissimilarities is fed to the same class model in order to obtain a final prediction for the membership of the test image to the particular concept.</p><p>second option, the hope is that the important parts of features will be so prevalent for each concept, that the averaging procedure will manage to maintain them in this generic prototype representation. In practice, the first option did not perform well in preliminary tests and the second option was eventually used.</p><p>The same class approach has been applied before for dealing with multimodal problems in a clustering task <ref type="bibr" coords="5,292.43,352.05,9.96,8.74" target="#b8">[9]</ref>. In that work, it is recognized that when attempting to cluster items that may be represented by multiple modalities or features, different clustering results that correspond to different conceptual organizations of the data may result by putting emphasis on different modalities (i.e. by following different fusion strategies). Instead of looking for appropriate fusion strategies, it was deemed interesting to allow an example clustering to guide the clustering procedure. The example clustering was used to obtain the "same-class" model, which in turn was used to group together items that had similar "same-class" relationships to the rest of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Description of Runs and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Runs Description</head><p>This section describes the experimental settings of our submissions. Runs 1, 2, 3 and 5 are based on the first approach and Run 4 is based on the second. Runs 1, 2, 3, and 5 were performed on two quad core machines (Intel Quad Core i7-950 @3.07Ghz, 12G RAM and Intel Quad Core Q6600 @2.4Ghz, 8G RAM) and coded in Matlab. Run 4 was performed on a dual core machine (Intel Dual Core Q900 @2.5Ghz, 4G RAM ) and coded in Java. Run 1 uses textual, Run 2 visual, while Runs 3-5 make use of both visual and textual features.</p><p>Run 1 (Approach 1, textual only): The total Mean interpolated Average Precision (MiAP) in this run was 0.2913 in training and 0.2311 in testing. In this run we used seven textual descriptors (Table <ref type="table" coords="5,337.78,644.16,3.87,8.74" target="#tab_0">1</ref>). The TOP-TAGS feature was created using the 5000 most frequent tags. The TAGS-BOW textual feature was extracted using the 5000-dimensional bag of words (BoW) representation following the approach of <ref type="bibr" coords="6,250.47,130.95,14.61,8.74" target="#b12">[13]</ref>. We took the union of raw tags of all images in the training set and applied stemming and stop word removal. This led to a vocabulary of approximately 32000 stems. Then, we applied feature selection to select the most important features using the χ 2 max criterion and finally selected the top 5000 features. The next three features were extracted using WordNet. The TAGS-WNET-TOP500 uses BoW representation using a codebook of 500 words. In order to define the codebook the full set of tags accompanying the ImageCLEF images was pre-processed by removing stop words and words not recognized by WordNet <ref type="bibr" coords="6,241.75,226.59,9.96,8.74" target="#b2">[3]</ref>. Then, the 500 most frequent tags were selected to compose the codebook. Finally, every image in the dataset was expressed as the occurrence count histogram of the codebook words in its set of tags, resulting in 500-dimensional feature vectors. As above, the TAGS-WNET-TOP5712 feature was extracted by selecting 5712 distinct tags instead of 500 to compose the codebook. The resulting feature vectors were 5712-dimensional. The last feature (TAGS-WNET-KRN-TOP500) was extracted using WordNet-based kernel similarities to enhance the semantic information enclosed by the BoW representation <ref type="bibr" coords="6,199.93,322.23,14.61,8.74" target="#b10">[11]</ref>, by measuring the semantic relatedness of every word in the codebook with all other members of the codebook. Subsequently, the resulting matrix was multiplied with the original 500-dimensional BoW representation, to generate a new feature space with 500 dimensions. The last three features were extracted by applying probabilistic Latent Semantic Analysis (pLSA), a technique that considers a single document as a mixture of topics and learns the conditional distribution of features (words) given that some topic is present in the document <ref type="bibr" coords="6,198.12,405.92,9.96,8.74" target="#b3">[4]</ref>. According to this, the PLSA-TOP10000TAGS was extracted by applying pLSA on top 10000 tags feature vectors using 100 latent topics and the PLSA-TOPTAGS by applying pLSA on the top 10000 tags feature vectors using 100 latent topics respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 2 (Approach 1, visual only):</head><p>We achieved a MiAP of 0.3118 in training and 0.2628 in testing. In both training and testing, visual features were found to yield higher scores than textual. We used Dense and Harris Laplace sampling to extract keypoints. For local feature aggregation, hard assignment was used only in the TOPSURF+BOW descriptor, while Vector of Locally Aggregating Descriptors (VL) <ref type="bibr" coords="6,240.72,524.61,10.52,8.74" target="#b4">[5]</ref> was used for the rest. Two of the used visual features include the GIST and TOPSURF+BOW descriptors made available by the Im-ageCLEF organizers. The SURF features were extracted from all training images and codebooks of sizes k = 64, 128 and 256 were learned using the k-means algorithm (code provided by the authors of <ref type="bibr" coords="6,323.57,572.43,14.76,8.74" target="#b11">[12]</ref>). This process led to three sets of SURF+VL features with dimensionalities 64x64 (4096), 64x128 (8192) and 64x256 (16384). The final vectors where power (a=0.5) and L2 normalized. The SIFT(D)+VL features, were computed in the same way as SURF+VL using codebooks of k=64 visual words, with dimensionalities 64x128 (8192) computed on a dense multi-scale grid. The HUESIFT(D)+VL feature where computed in the same way as SURF+VL using codebooks of k=64 visual words, with dimensionalities 64*165 (10560) computed on a dense multi-scale grid. The RG-BSIFT(D)+VL, OPPONENTSIFT(D)+VL, RGSIFT(D)+VL, CSIFT(D)+VL and HSVSIFT(D)+VL where computed in the same way as SURF+VL using codebooks of k=64 visual words, with dimensionalities 64x384 (24576) computed on a dense multi-scale grid. The SIFT(H)+VL, RGBSIFT(H)+VL, RGSIFT(H) +VL and HUESIFT(H)+VL were computed in the same way as SURF+VL using codebooks of k=64 visual words, with dimensionalities 64x128 (8192, SIFT) and 64x384 (24576) where regions found with Harris Laplace keypoint detector. In the end, we used the GIST-PLSA by applying pLSA on the GIST feature vectors using 100 latent topics. In total, we combined 17 different visual features.</p><p>Run 3 (Approach 1, multimodal): In this run, MiAP was 0.3894 in training and 0.3210 in testing. This was the best MiAP performance achieved by CERTH. Figure <ref type="figure" coords="7,167.82,266.15,4.98,8.74" target="#fig_0">3</ref> illustrates the MiAP for each concept for this run. In this run all aforementioned features and also the hybrid feature which combines the GIST and TOP-TAGS descriptors by applying pLSA were used. More specifically, the pLSA model was applied independently in both the GIST and TOP-TAGS features resulting in the 100-dimensional GIST-PLSA and PLSA-TOPTAGS. Motivated by the fact that both feature spaces refer to latent semantic spaces and express probabilities (i.e., the degree to which a certain topic exists in the image), we assume that the topics obtained from both modalities are homogeneous and can be indiscriminately considered as the words of a common Topic Word Vocabulary. Based on this assumption we applied a second level pLSA model that operates on the feature space generated by concatenating the GIST-PLSA and PLSA-TOPTAGS (i.e. 100 + 100 = 200-dimensions). In total we combined 25 visual and textual features.</p><p>Run 4 (Approach 2, multimodal): In this run, MiAP was 0.3014 in the training set and 0.2887 in the test set. Figure <ref type="figure" coords="7,295.88,449.18,4.98,8.74" target="#fig_1">4</ref> illustrates the MiAP for each concept for this run. SVM was used in order to learn the same class model. The following set of features were used: textual using only the tags (no stemming and stop word removal was applied) and a bag of words representation, SURF using a bag of words representation, SURF using a VLAD aggregation scheme with 2048 dimensions and GIST. For each concept, a separate same class model was used.</p><p>The positive examples for each model were obtained by selecting all items that belong to the concept and computing the set of distances between them and the prototype of the concept. The negative examples were obtained by randomly sampling a number of images that do not belong to the concept. The number of negative examples was equal to the number of positive examples.</p><p>Run 5 (Approach 1, multimodal): In the final run, MiAP was 0.3769 and 0.3012 in the training and test set respectively. MiAP was not better than Run 3, to which the features were similar, but we managed to achieve higher F-measure (0.495) in the test set. In this run all features of Run 3 were used except the ones that were pre-processed with pLSA and extracted using WordNet (Table <ref type="table" coords="7,134.77,656.12,3.87,8.74" target="#tab_0">1</ref>, features 1, 3-19).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation</head><p>Feature comparison: Table <ref type="table" coords="8,264.75,644.16,4.98,8.74" target="#tab_0">1</ref>  SIFT(H)+VL and textual features TOP-TAGS, TAGS-BOW and TAGS-WNET-TOP5712 achieved the best MiAP scores compared to the rest.</p><p>Approach 1 vs approach 2: Figure <ref type="figure" coords="9,319.63,450.17,4.98,8.74">5</ref> illustrates the MiAP score for each run comparing the performance we achieved in the training set with the one in the test set. Apparently, Runs 3 and 5 suffer from overfitting, while Run 4 appears to generalize better. Furthermore, in some concepts one approach does better than the other. Run 3, based on the first approach is slightly better than Run 4 in the majority of concepts separately (50 concepts). Run 3 does much better in concepts celestial stars (6, Figure <ref type="figure" coords="9,334.09,521.90,3.87,8.74" target="#fig_0">3</ref>), weather clearsky (7), weather rainbow <ref type="bibr" coords="9,173.63,533.86,16.38,8.74" target="#b9">(10)</ref>, flora grass (36) and quality partialblur (63), while Run 4 does much better in concepts water underwater (28, Figure <ref type="figure" coords="9,374.07,545.81,3.87,8.74" target="#fig_1">4</ref>), fauna horse (39) and fauna amphibianreptile (44).</p><p>Comparison to competing teams: Comparing per concept our best performance (Run 3) to other competitors, good performance was achieved (in terms of MiAP) in eight concepts and relatively low performance in six concepts. Specifically, our approach yields good performance in concepts weather rainbow, combustion fireworks, flora plant, fauna spider, sentiment euphoric, combustion smoke, style graycolor and transport truckbus, while it yields low performance in concepts water other, fauna amphibianreptile, quantity two, quantity three, age elderly and sentiment unpleasant. Finally, Tables <ref type="table" coords="9,357.88,656.12,4.98,8.74" target="#tab_1">2</ref> and<ref type="table" coords="9,387.66,656.12,4.98,8.74" target="#tab_2">3</ref> provide an impres-sion of the standing of CERTH's performance against competing teams. Table <ref type="table" coords="10,134.77,130.95,4.98,8.74" target="#tab_1">2</ref> presents the rank of CERTH's best submission both at run-level (80 runs in total) and at team level (18 competing teams) in terms of the three performance measures. Table <ref type="table" coords="10,208.95,154.86,4.98,8.74" target="#tab_2">3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>According to the obtained results, CERTH's performance ranks a bit higher than median. This leaves much room for improving performance in the future. An obvious option to achieve this is to use enhanced features. According to Table 3 particular emphasis should be placed on visual features. A second option for improving the performance of the first approach is to avoid overfitting by devising a more robust training process. A further option for improving performance stems from the fact that each image may be related to more than one concepts. For the same class approach, this implies that the average feature for each concept captures not only characteristics of the concept but also some of the characteristics of other concepts frequently co-occuring with it. This could lead to false positives for images not related to the concept but carrying these characteristics due to their relevance to these related concepts. Moreover, in some cases, when these characteristics are very prevalent they may even dominate the representation of the concept, leading to false negatives.</p><p>There is a lot of space for improvement considering the fact that we are dealing with a multi-label classification problem <ref type="bibr" coords="11,332.98,324.37,14.61,8.74" target="#b14">[15]</ref>. That is, from a probabilistic point of view, the occurrence of many concepts is not independent of the occurrence of other concepts and therefore, the estimates about the occurrence of a concept could be refined using the estimates about the occurrence of other concepts. There have been many approaches for dealing with this problem, for instance <ref type="bibr" coords="11,174.77,384.14,9.96,8.74" target="#b1">[2]</ref>, which builds a chain of binary classifiers (one for each concept) where the input space of each classifier is augmented by the decisions of previous classifiers and <ref type="bibr" coords="11,227.40,408.05,15.50,8.74" target="#b13">[14]</ref> where a set of meta-classifiers are stacked upon the decisions of independent binary classifiers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,234.20,384.40,146.96,7.89"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. MiAP per concept for Run 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,234.20,384.40,146.96,7.89"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. MiAP per concept for Run 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,134.77,407.14,348.88,177.40"><head>Table 1 .</head><label>1</label><figDesc>The MiAP scores for each descriptor, D stands for Dense grid and H stands for Harris Laplace keypoint Detector, and VL stands for VLAD<ref type="bibr" coords="8,396.69,576.68,9.22,7.86" target="#b4">[5]</ref>.</figDesc><table coords="8,136.16,407.14,347.49,155.54"><row><cell># Descriptor</cell><cell>Dims MiAP # Descriptor</cell><cell>Dims MiAP</cell></row><row><cell>1 GIST</cell><cell>480 0.21026 13 HSVSIFT(D)+VL</cell><cell>24576 0.26629</cell></row><row><cell>2 GIST-PLSA</cell><cell>100 0.21604 14 SIFT(H)+VL</cell><cell>8192 0.2561</cell></row><row><cell cols="2">3 TOPSURF+BOW 200k 0.1469 15 RGBSIFT(H)+VL</cell><cell>24576 0.27177</cell></row><row><cell>4 SURF+VL</cell><cell>4096 0.23483 16 RGSIFT(H)+VL</cell><cell>24576 0.25235</cell></row><row><cell>5 SURF+VL</cell><cell>8192 0.23722 17 HUESIFT(H)+VL</cell><cell>10560 0.24541</cell></row><row><cell>6 SURF+VL</cell><cell>16384 0.23667 18 TOP-TAGS</cell><cell>500 0.2739</cell></row><row><cell>7 SIFT(D)+VL</cell><cell>8192 0.26377 19 TAGS-BOW</cell><cell>5000 0.29369</cell></row><row><cell cols="2">8 HUESIFT(D)+VL 10560 0.25883 20 PLSA-TOPTAGS</cell><cell>100 0.22639</cell></row><row><cell cols="2">9 RGBSIFT(D)+VL 24576 0.27672 21 PLSA-GIST-TOPTAGS</cell><cell>200 0.24506</cell></row><row><cell cols="2">10 OPP-SIFT(D)+VL 24576 0.27718 22 PLSA-TOP10000TAGS</cell><cell>100 0.20751</cell></row><row><cell cols="2">11 RGSIFT(D)+VL 24576 0.25368 23 TAGS-WNet-TOP500</cell><cell>500 0.27691</cell></row><row><cell>12 CSIFT(D)+VL</cell><cell>24576 0.27214 24 TAGS-WNET-TOP5712</cell><cell>5712 0.28025</cell></row><row><cell cols="3">13 HSVSIFT(D)+VL 24576 0.26629 25 TAGS-WNET-KRN-TOP500 500 0.22877</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,134.77,154.86,345.82,462.53"><head>Table 2 .</head><label>2</label><figDesc>presents the ranks of all CERTH runs compared to runs of the same type of features (textual, visual, multimodal). The test scores from ImageCLEF competition and the best rank</figDesc><table coords="10,170.72,212.83,273.91,404.55"><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">mIAP train</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.35</cell><cell cols="2">mIAP test</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.15</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>Run1</cell><cell>Run2</cell><cell>Run3</cell><cell>Run4</cell><cell>Run5</cell></row><row><cell></cell><cell></cell><cell cols="4">Fig. 5. MiAP for all Runs</cell></row><row><cell></cell><cell>measures</cell><cell cols="5">score Run-Level Rank Best-Run Rank</cell></row><row><cell></cell><cell>MiAP</cell><cell>0.3210</cell><cell></cell><cell>28/80</cell><cell></cell><cell>7/18</cell></row><row><cell></cell><cell>GMiAP</cell><cell>0.2547</cell><cell></cell><cell>29/80</cell><cell></cell><cell>7/18</cell></row><row><cell></cell><cell>F-ex</cell><cell>0.4950</cell><cell></cell><cell>27/80</cell><cell></cell><cell>6/18</cell></row><row><cell cols="2">Runs features</cell><cell></cell><cell cols="2">MiAP</cell><cell cols="2">GMiAP</cell><cell>F-ex</cell></row><row><cell>1</cell><cell>textual</cell><cell></cell><cell cols="4">0.2311 (5/17) 0.1669 (7/17) 0.3946 (7/17)</cell></row><row><cell>2</cell><cell>visual</cell><cell cols="5">0.2628 (13/28) 0.1904 (13/28) 0.4838 (10/28)</cell></row><row><cell>3</cell><cell cols="6">Multimodal All 0.3210 (15/35) 0.2547 (15/35) 0.4899 (18/35)</cell></row><row><cell>4</cell><cell cols="6">Multimodal gp 0.2887 (18/35) 0.2314 (18/35) 0.2234 (32/35)</cell></row><row><cell>5</cell><cell cols="6">Multimodal l 0.3012 (17/35) 0.2286 (19/35) 0.4950 (17/35)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="10,159.01,620.40,297.35,7.89"><head>Table 3 .</head><label>3</label><figDesc>The test scores from ImageCLEF competition and the level run</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements This work was supported by the <rs type="projectName">SocialSensor</rs> project, partially funded by the <rs type="funder">European Commission</rs>, under contract number <rs type="grantNumber">FP7-287975</rs>. We also thank <rs type="person">Eleftherios Spyromitros-Xioufis</rs> for providing us with the VLAD-based features and the features in [13], and <rs type="person">Spiros Nikolopoulos</rs> for providing us with the pLSA-based features.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_Punbf5u">
					<idno type="grant-number">FP7-287975</idno>
					<orgName type="project" subtype="full">SocialSensor</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,142.96,557.10,337.64,7.86;11,151.52,568.06,329.07,7.86;11,151.52,579.02,73.59,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,296.18,557.10,184.42,7.86;11,151.52,568.06,88.96,7.86">Overview of the clef 2012 flickr photo annotation and retrieval task</title>
		<author>
			<persName coords=""><forename type="first">Thomee</forename><surname>Bart</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,258.53,568.06,217.56,7.86">the working notes for the clef 2012 labs and workshop</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,590.33,337.64,7.86;11,151.52,601.29,329.07,7.86;11,151.52,612.25,294.73,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,422.12,590.33,58.48,7.86;11,151.52,601.29,226.54,7.86">Bayes optimal multilabel classification via probabilistic classifier chains</title>
		<author>
			<persName coords=""><forename type="first">Krzysztof</forename><surname>Dembczynski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weiwei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eyke</forename><surname>Hüllermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,284.19,612.25,21.38,7.86">ICML</title>
		<editor>
			<persName><forename type="first">Johannes</forename><surname>Fürnkranz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</editor>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="279" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,623.57,337.64,7.86;11,151.52,634.52,179.10,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="11,235.58,623.57,245.01,7.86;11,151.52,634.52,83.06,7.86">WordNet: An Electronic Lexical Database (Language, Speech, and Communication)</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.96,645.84,337.64,7.86;11,151.52,656.80,202.38,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,233.97,645.84,145.72,7.86">Probabilistic latent semantic analysis</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,399.64,645.84,80.95,7.86;11,151.52,656.80,94.75,7.86">Proc. of Uncertainty in Artificial Intelligence</title>
		<meeting>of Uncertainty in Artificial Intelligence<address><addrLine>Stockholm</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>UAI99</note>
</biblStruct>

<biblStruct coords="12,142.96,119.67,337.63,7.86;12,151.52,130.63,329.07,7.86;12,151.52,141.59,271.86,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,344.12,119.67,136.47,7.86;12,151.52,130.63,130.50,7.86">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,306.34,130.63,174.25,7.86;12,151.52,141.59,143.64,7.86">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3304" to="3311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,152.55,337.63,7.86;12,151.52,163.51,193.46,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,232.09,152.55,200.03,7.86">Leveraging social media networks for classification</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,439.15,152.55,41.44,7.86;12,151.52,163.51,59.68,7.86">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="478" />
			<date type="published" when="2011-11">Nov 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,174.47,337.64,7.86;12,151.52,185.43,302.91,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,258.68,174.47,221.91,7.86;12,151.52,185.43,76.76,7.86">Laplacian eignemaps for dimensionality reduction and data representation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,236.34,185.43,71.46,7.86">Neural Computing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003-06">June 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,196.39,337.63,7.86;12,151.52,207.34,329.07,7.86;12,151.52,218.30,96.78,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,257.41,196.39,223.18,7.86;12,151.52,207.34,144.66,7.86">Beyond distance measurement: Constructing neighborhood similarity for video annotation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,391.38,207.34,89.21,7.86;12,151.52,218.30,44.45,7.86">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="465" to="476" />
			<date type="published" when="2009-04">April 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,229.26,337.63,7.86;12,151.52,240.22,329.07,7.86;12,151.52,251.18,329.07,7.86;12,151.52,262.14,106.54,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,432.16,229.26,48.43,7.86;12,151.52,240.22,292.85,7.86">Social event detection using multimodal clustering and integrating supervisory signals</title>
		<author>
			<persName coords=""><forename type="first">Georgios</forename><surname>Petkos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Symeon</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiannis</forename><surname>Kompatsiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,462.94,240.22,17.66,7.86;12,151.52,251.18,329.07,7.86;12,151.52,262.14,10.75,7.86">Proceedings of the 2nd ACM International Conference on Multimedia Retrieval, ICMR &apos;12</title>
		<meeting>the 2nd ACM International Conference on Multimedia Retrieval, ICMR &apos;12</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,273.10,337.98,7.86;12,151.52,284.06,234.11,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,282.30,273.10,198.29,7.86;12,151.52,284.06,82.89,7.86">Classification in networked data: A toolkit and a univariate case study</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Macskassy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,242.52,284.06,22.50,7.86">JMLR</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="935" to="983" />
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,295.02,337.98,7.86;12,151.52,305.98,314.40,7.86" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="12,253.22,295.02,227.37,7.86;12,151.52,305.98,187.88,7.86">Incorporating dictionary and corpus information into a context vector measure of semantic relatedness</title>
		<author>
			<persName coords=""><forename type="first">Patwardhan</forename><surname>Siddharth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003-08">August 2003</date>
		</imprint>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct coords="12,142.62,316.93,337.97,7.86;12,151.52,327.89,329.07,7.86;12,151.52,338.85,329.08,7.86;12,151.52,349.81,300.72,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,207.31,327.89,273.28,7.86;12,151.52,338.85,97.75,7.86">An empirical study on the combination of surf features with vlad vectors for image search</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Spyromitros-Xioufis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Kompatsiaris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,271.70,338.85,208.90,7.86;12,151.52,349.81,41.70,7.86;12,223.83,349.81,127.83,7.86">Image Analysis for Multimedia Interactive Services (WIAMIS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
	<note>13th International Workshop on</note>
</biblStruct>

<biblStruct coords="12,142.62,360.77,337.97,7.86;12,151.52,371.73,329.07,7.86;12,151.52,382.69,329.07,7.86;12,151.52,393.65,129.80,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,433.80,360.77,46.79,7.86;12,151.52,371.73,312.57,7.86">Mlkd&apos;s participation at the clef 2011 photo annotation and concept-based retrieval tasks</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Spyromitros-Xioufis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sechidis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,382.69,329.07,7.86;12,151.52,393.65,101.60,7.86">ImageClef Lab of CLEF 2011 Conference on Multilingual and Multimodal Information Access Evaluation</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,404.61,337.98,7.86;12,151.52,415.56,329.07,7.86;12,151.52,426.52,329.07,7.86;12,151.52,437.48,223.22,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,220.16,415.56,260.44,7.86;12,151.52,426.52,77.03,7.86">Correlation-based pruning of stacked binary relevance models for multi-label learning</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dimou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Spyromitros-Xioufis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Mezaris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Kompatsiaris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,246.62,426.52,233.98,7.86;12,151.52,437.48,131.32,7.86">Proceedings of the 1st International Workshop on Learning from Multi-Label Data (MLD&apos;09)</title>
		<meeting>the 1st International Workshop on Learning from Multi-Label Data (MLD&apos;09)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="101" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,448.44,337.98,7.86;12,151.52,459.40,281.94,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,271.67,448.44,149.59,7.86">Multi-label classification: An overview</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Katakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,428.00,448.44,52.60,7.86;12,151.52,459.40,209.55,7.86">International Journal of Data Warehousing and Mining (IJDWM)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.61,470.36,337.98,7.86" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="12,183.72,470.36,144.70,7.86">semi-supervised learning with graphs</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>Pittsburgh, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
