<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.64,116.56,302.07,13.46;1,207.46,134.49,200.43,13.46">Overview of the ImageCLEF 2012 Flickr Photo Annotation and Retrieval Task</title>
				<funder>
					<orgName type="full">European Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,228.66,171.99,56.43,9.58"><forename type="first">Bart</forename><surname>Thomee</surname></persName>
							<email>bthomee@yahoo-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Yahoo! Research</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,311.41,171.99,70.81,9.58"><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
							<email>adrian.popescu@cea.fr</email>
							<affiliation key="aff1">
								<orgName type="department">Vision &amp; Content Engineering Laboratory</orgName>
								<orgName type="institution">CEA LIST</orgName>
								<address>
									<settlement>Fontenay-aux-Roses</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.64,116.56,302.07,13.46;1,207.46,134.49,200.43,13.46">Overview of the ImageCLEF 2012 Flickr Photo Annotation and Retrieval Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8894731D44A859DCFDE3E460DD7BF508</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ImageCLEF's Flickr Photo Annotation and Retrieval task aims to advance the state of the art in multimedia research by providing a challenging benchmark for visual concept detection, annotation and retrieval in the context of a diverse collection of Flickr photos. The benchmark consisted of two separate but closely connected subtasks, where the objective of the first subtask was to accurately detect a wide range of semantic concepts for the purpose of automatic image annotation, while the objective of second subtask was to correctly retrieve relevant images for conceptoriented queries inspired by what people actually search for on the internet. This paper presents an overview of the benchmark, summarizes the annotation and retrieval techniques proposed by the participating teams and evaluates their performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="33" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="34" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic recognition of photographic content is useful in a wide range of domains, ranging from specialized application, such as medical imagery, to large public applications, such as web content structuring and retrieval. Although considerable research efforts have been devoted to concept detection in public images, this task remains difficult because the number of possible concepts that can be depicted is boundless, where the visual aspect of each concept additionally can vary along numerous dimensions.</p><p>The Flickr Photo Annotation and Retrieval task we present in this paper is a multi-label classification challenge that offers a benchmark for testing novel visual concept detection, annotation and retrieval algorithms on a public collection containing photos gathered from the social sharing website Flickr <ref type="foot" coords="1,454.92,552.21,3.49,6.71" target="#foot_0">1</ref> . The aim is to analyze the images in terms of their visual and/or textual features in order to detect the presence of one or more semantic concepts. The detected concepts are then to be used for the purpose of automatically annotating the images or for retrieving the best matching images to a given concept-oriented query. The concepts are very diverse and range across categories such as people (e.g. teenager, female), scenery (e.g. lake, desert), weather (e.g. rainbow, fog) and even impressions (e.g. unpleasant, euphoric). This task has a longstanding tradition at ImageCLEF. Since 2009 the task has been based upon various subsets of the MIRFLICKR collection <ref type="bibr" coords="2,429.18,130.98,10.79,9.58" target="#b0">[1,</ref><ref type="bibr" coords="2,439.97,130.98,7.19,9.58" target="#b1">2]</ref>, where every year the list of concepts to detect was updated in order to cover a wider selection of concept types and to make the task more challenging. Last year a concept-based retrieval subtask was added to exploit the concept annotations in the context of image retrieval. In contrast with the closely related Scalable Web Image Annotation task <ref type="bibr" coords="2,236.53,190.75,10.58,9.58" target="#b2">[3]</ref>, also held this year at ImageCLEF, our task involves a smaller collection of images but has been fully and manually annotated within the chosen concept space, a characteristic that favors experiment reproducibility. To this end, the annotations associated with the collection will be released after the campaign. The related PASCAL Visual Object Classes challenge <ref type="bibr" coords="2,451.87,238.58,11.62,9.58" target="#b3">[4]</ref> has as aim to accurately detect the bounding boxes and labels of objects in a set of images, whereas our focus is on both visual and textual information instead of visual information only and furthermore we offer a larger range of concepts to detect. Yet another related benchmark is the ImageNet Large Scale Visual Recognition Challenge<ref type="foot" coords="2,235.68,296.77,3.49,6.71" target="#foot_1">2</ref> , which is run over a larger dataset and with a larger number of concepts, but without focus on multi-label classification.</p><p>The remainder of this paper is organized as follows. First, in Section 2 we describe the dataset upon which the task is based, in Section 3 we present the concepts and the concept-oriented queries, and in Section 4 we discuss how we collected the ground truth. We then introduce the participating teams in Section 5 and evaluate the performance of their techniques on the annotation and retrieval subtasks in Sections 6 and 7 respectively. Finally, in Section 8 we offer outlooks for the future of visual concept detection, annotation and retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>The annotation and retrieval subtasks are based on the MIRFLICKR collection <ref type="bibr" coords="2,154.83,459.06,10.79,9.58" target="#b0">[1,</ref><ref type="bibr" coords="2,165.62,459.06,7.19,9.58" target="#b1">2]</ref>. The entire collection contains 1 million images from the social photo sharing website Flickr and was created by downloading up to a thousand photos per day in the period 2008-2010 that at that moment were deemed to be the most interesting according to Flickr. All photos in this collection were released by their photographers under a Creative Commons license, allowing them to be freely used for research purposes. The annotation subtask was based on the first 25 thousand images of the MIRFLICKR collection, whereas the retrieval subtask involved a subset of 200 thousand images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Textual features</head><p>Each of the images used in both subtasks was accompanied by descriptive metadata, within which we can distinguish the following textual features:</p><p>User tags: These are the tags that the users assigned to the photos they uploaded to Flickr.</p><p>User, photo and license information: These features contain details about the Flickr user that took the photo, the photo itself and the Creative Commons license associated with the photo.</p><p>EXIF metadata: If available, the EXIF metadata contains information about the camera that took the photo and the parameters used.</p><p>In Figure <ref type="figure" coords="3,177.41,191.40,4.98,9.58">1</ref> we show an example photo and its associated textual features.</p><p>Fig. <ref type="figure" coords="3,152.90,352.01,3.74,9.58">1</ref>: An example photo from the MIRFLICKR collection and its associated user tags, user information, photo information, license information and EXIF metadata. Due to space considerations we only show part of the metadata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual features</head><p>We noticed that often similar types of visual features were used by the participants in previous editions of the photo annotation task, in particular descriptors based on interest points and bag-of-words were popular. To allow the participants to direct their attention on the actual concept detection instead of having to compute common features, we extracted a number of descriptors for the participants beforehand and released them together with the dataset. We additionally gave the participants some pointers to toolkits that would allow them to extract the descriptors with a different set of parameters or to extract other related descriptors. Each of the images used in the annotation task was accompanied by the following visual features:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SURF [5]:</head><p>The SURF technique uses a Hessian matrix-based measure for the detection of interest points and a distribution of Haar wavelet responses within the interest point neighborhood as descriptor. An image is analyzed at several scales, so interest points can be extracted from both global ('coarse') and local ('fine') image details. Additionally, the dominant orientation of each of the interest points is determined to support rotation-invariant matching. We used the OpenSURF toolkit <ref type="foot" coords="3,215.53,629.97,3.49,6.71" target="#foot_2">3</ref> to extract this descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TOP-SURF [6]:</head><p>A bag-of-words technique <ref type="bibr" coords="4,323.03,119.02,11.62,9.58" target="#b6">[7]</ref> was used to cluster the SURF interest points extracted from a representative collection of photographic images into a number of visual words. The interest points present in each Flickr image are then converted to their most closely matching visual words, after which a histogram is formed consisting of only the top few most dominant visual words. We used the TOP-SURF toolkit <ref type="foot" coords="4,303.23,177.22,3.49,6.71" target="#foot_3">4</ref> to extract this descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SIFT [8], C-SIFT [9], RGB-SIFT [10], OPPONENT-SIFT [10]:</head><p>The SIFT descriptor describes the local shape of an image region using edge orientation histograms. The other three descriptors are variations that represent the image in different color spaces before computing the SIFT descriptor. We used the ISIS Color Descriptors toolkit <ref type="foot" coords="4,244.03,242.75,3.49,6.71" target="#foot_4">5</ref> to extract all these descriptors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GIST [11]:</head><p>The GIST descriptor is based on a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. To capture this image structure, oriented edge responses are aggregated at multiple scales into very coarse bins. We used the LabelMe toolkit <ref type="foot" coords="4,204.41,308.27,3.49,6.71" target="#foot_5">6</ref> to extract this descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Concepts and queries</head><p>Defining a compact yet representative list of concepts to annotate or queries to propose in a search engine are not trivial tasks, because the spaces to choose from and the user needs are virtually infinite. While in the past concepts and queries were chosen to represent different ontological fields and to be of variable difficulty, this year an usage-oriented constraint was added with the exploitation of image search query logs in order to define and/or refine the concepts and queries. We additionally took their 'textualness' and 'visualness' into account in order to offer a set of concepts and queries with varying difficulty and to accommodate for both the textual and visual techniques the participants may propose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Concept definition</head><p>In this edition of the photo annotation and retrieval task we continued along the same lines as previous years in terms of concepts, where in total we defined a set of 94 concepts referring to nature, people, image quality and so on. In comparison with last year's benchmark we removed a few of the concepts that were not sufficiently present in the dataset or ambiguously defined, based on feedback given by former participants. We furthermore introduced several new concepts that were inspired by popular queries issued to the Yahoo! image search engine 7 in order to provide a more realistic context for the task. We show an overview of the concepts in Table <ref type="table" coords="5,326.25,119.02,3.74,9.58" target="#tab_0">1</ref>, where we grouped them into related categories. The exact descriptions we used to represent each concept are listed in Appendix A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Query definition</head><p>Similar to how we defined the concepts for the annotation subtask, we removed a few of the queries of last year's retrieval subtask and introduced several new ones that were also inspired by the most popular queries we found in the image search logs. In total we defined 42 concept-oriented queries, where many of them can be considered as linear combinations of the concepts used in this task, whereas others are formed by involving additional constraints. We show an overview of the queries in Table <ref type="table" coords="5,278.21,644.19,3.74,9.58" target="#tab_1">2</ref>. The exact descriptions we used to represent each query are listed in Appendix B. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Ground truth collection</head><p>We acquired the ground truth relevance annotations for the newly defined concepts and queries, as well as for the concepts and queries reused from last year, through crowd sourcing. We enlisted the help of many anonymous workers on Amazon's Mechanical Turk<ref type="foot" coords="6,257.59,470.23,3.49,6.71" target="#foot_7">8</ref> , which is an online marketplace for distributing small jobs to be performed by interested people for a small fee. Due to the presence of workers that do not have a genuine interest in performing the requested service, where such a worker may either be a real person or an automated service that pretends to be human, it is necessary to validate the quality of the performed work. To this end we used the intermediary service of CrowdFlower<ref type="foot" coords="6,476.61,530.01,3.49,6.71" target="#foot_8">9</ref> to obtain the relevance judgments, because this service automatically performs the filtering of the workers based on the quality of the work they perform by validating it against specific examples for which the correct answer is known. Such examples are commonly referred to as gold and need to be supplied in addition to the job. Ultimately, the CrowdFlower service guarantees that each unit, which in our case refers to a concept-image or query-image combination, ends up being assessed by at least three workers that exhibited good annotation behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Concept relevance assessment</head><p>The concept annotation jobs we created showed the workers a photo accompanied by a short list of related concepts and asked them to indicate all concepts that were clearly present in the image, see Figure <ref type="figure" coords="7,363.54,159.06,4.98,9.58">2</ref> for an example. We created separate tasks for the different concept subcategories, because intuitively we felt it would be easier for a worker to perform relevance judgments faster and more accurately for related concepts (e.g. cat, dog, fish) than for arbitrary concepts (e.g. cat, reflection, city life).</p><p>Fig. <ref type="figure" coords="7,152.90,477.18,3.74,9.58">2</ref>: An example crowd sourcing job for the concept relevance assessment, showing an image and a list of concepts the worker can annotate the image with. Here, we have marked the correct answer in yellow.</p><p>Before starting a job, a worker was presented with a set of instructions, three example images for each concept included in the job description and three example images not containing any of the sought-for concepts. The gold we used for validating the performance of the workers was annotated by ourselves and either clearly exhibited a particular concept or it clearly did not exhibit it; to not mistakenly mark a worker as a bad annotator we did not include images as gold that ambiguously contained a concept. Due to the subjective nature of many of the concepts it was certainly possible for good annotators to disagree with each other about the presence of a concept in an image and therefore we applied the majority voting rule to the relevance judgments to make the final decisions. Using the earlier example, in Figure <ref type="figure" coords="7,339.81,644.19,4.98,9.58" target="#fig_0">3</ref> we show which concepts were considered to be present by the workers in the image of the cake. After the ground truth annotations were collected, we divided the photo collection of 25 thousand images into a training set of 15 thousand images and a testing set of 10 thousand images. We ensured that the number of images assigned to each concept in both sets was roughly proportional to the quantity in which they were present in the entire collection, e.g. if concept A was present in a total of 250 images, then we aimed to assign 150 of these images to the training set and the remaining 100 to the testing set. We considered it to be of paramount importance to assure that concepts with few images were sufficiently present in both sets and in balance with each other, in effect mitigating the small sample size problem. This is also in response to the feedback received of some of last years' participants, who indicated that previously some concepts were underrepresented in the training set and overrepresented in the testing set, or vice versa. To this end, we used a greedy approach to perform the image assignments, where the algorithm iteratively distributed the images starting with the least represented concepts and ending with the most represented concepts. In Appendix A we have listed the number of times each concept is represented in both the training set and the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Query relevance assessment</head><p>For the query relevance assessment we used a similar setup as with the concept annotations, see Figure <ref type="figure" coords="8,263.08,536.60,4.98,9.58">4</ref> for an example. We presented a worker with a set of instructions and with three example images for each of the 42 conceptrelated queries before they were able to start a job. The set of images that needed to be judged was formed by aggregating the top 100 images of all ranked retrieval results that were submitted by each participating team for each query. The gold we used for validating the performance of the workers was annotated by several trained editors of the National Institute of Standards and Technology (NIST), which we had access to through a collaboration with the TREC Crowdsourcing track. For the same reasons as for the concept relevance judgments, we applied the majority voting rule to the relevance judgments to determine whether or not an image was ultimately relevant to a particular query. Fig. <ref type="figure" coords="9,152.90,282.88,3.74,9.58">4</ref>: An example crowd sourcing job for the query relevance assessment, where the worker has to indicate whether the image is relevant to the query. Here, we have marked the correct answer in yellow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Participation</head><p>This year, in total 100 teams registered for the benchmark and signed the license agreement to access the collections. Eventually, 18 teams submitted a total of 80 runs to the annotation subtask, where the maximum number of runs per team was limited to 5. For the retrieval subtask, 7 teams submitted a total of 47 runs to the retrieval subtask, where the maximum number of runs per team was limited to 10. In this section we will introduce the participating teams in alphabetical order and highlight the techniques they used to perform the image annotation and/or retrieval. We refer to their working notes with a superscript 'a' if the team participated in the annotation subtask and with a superscript 'r' if they participated in the retrieval subtask. In case of encountering unfamiliar acronyms in the technique summaries, please refer to the respective working notes, if available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BUUA AUDR a [12]:</head><p>This team proposed textual and multimodal runs. Bags of visual words based on SIFT, coupled with soft assignment, were used to represent visual content. Frequent tags were selected in order to form a textual vocabulary that was used to map visual concepts. SVM classifiers were then used to predict potentially relevant concepts for each image. Annotation refinement that accounts for concept correlation was introduced to improve results obtained with textual or visual schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CEA LIST a [13]:</head><p>This team concentrated on the combination of textual and visual image. Textual models were built by combining semantic and contextual information, respectively derived from WordNet and Flickr, that were consequently processed using pooling strategies. For visual information, a computationally efficient bag of multimedia words strategy was tested against classical bag of visual words approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CERTH a [14]:</head><p>This team tested two approaches for image annotation. The first was based on Laplacian Eigenmaps of an image similarity graph model and the second on a "same class" model. They tested different multimedia fusion schemes and reported that best results were obtained when both textual and visual information were combined using the first learning scheme. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DMS-SZTAKI a [16]:</head><p>This team presented only multimodal runs. A fixed length visual descriptor with different similarity measures was devised and it allowed the early combination of textual and visual features. Gaussian Mixture Models were trained to define low-level features. Both global and local features were extracted from the images and a biclustering approach was adopted in order to represent Flickr tags. A three step fusion approach that included a transformation, a feature aggregation and a selection step was adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IMU a,r [17]:</head><p>This team focused on textual information modeling. Concept annotation was performed using maximum conditional probability to assess the probability of occurrence of a concept in an image based on already existing tags. Concept-based retrieval was performed using a classical language modeling technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IL a [18]:</head><p>This team focused on the exploitation of textual features associated to images in the test dataset. They tackled tag noise and incompleteness by creating tag-concept co-occurrence models in a two phase procedure, which first removed noisy tags from the model and then enriched existing tags with related ones that were not filtered out during the first phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ISI a [19]:</head><p>This team presented an approach that focused on scalability. Fisher Vectors and bag of visual words based on SIFTs were used to represent visual content, while classical bag of words with TF-IDF weighting was used to represent textual content. To achieve this, an online multi-label learning approach called Passive-Aggressive with Averaged Pairwise Loss was adapted from authors' earlier work. Reported results showed that a combination of different visual features was beneficial for the overall performance of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KIDS NUTN a,r [20]:</head><p>This team proposed multimedia fusion techniques that exploited textual and visual features, whose processing was done using dimensionality reduction, random forest classifiers and semi-supervised learning strategies. They reported that simple visual feature combination did not improve results over the use of single visual descriptors and that semi-supervised learning did not outperform supervised learning. For the retrieval subtask, results were based on the annotation results and the best results were also obtained with a combination of textual and visual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LIRIS a [21]:</head><p>This team modeled both textual and visual information and introduced a competitive fusion of the two modalities. A histogram of textual concepts that relied on semantic similarity between user tags and a concept dictionary was used to represent tags associated to Flickr images. Different global and local visual features were considered to model visual content. A Selective Weighted Late Fusion that iteratively selected and weighted the best features to use for each concept was introduced to combine textual and visual modalities, resulting in a significant improvement over monomodal runs.</p><p>MSATL a,r : This team used keyword and document representations of the concepts as textual features to match against the textual descriptions of the images for the purpose of annotation. In addition, they incorporated a random forest into which their visual features were embedded. For the retrieval subtask, they focused on textual features only and retrieved images based on the title of the query and a combination of descriptions and keywords associated with the query's concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLKD a,r [22]:</head><p>This team proposed visual, textual and multimodal runs for the annotation subtask. Different visual representations, such as BOVW, VLAD and VLAT were tested on top of SURF, SIFT and color SIFT. Text was modeled using standard bag of words techniques with TF-IDF weighting. Multimodal combination was realized either by averaging or by selecting the best model for each concept. For retrieval, they introduced two methods, where the first involved the production of co-occurrence models from Flickr to score concepts, while the second was a standard vector space model for text retrieval.</p><p>NII a : This team used a combination of local visual features and global visual features to address the annotation subtask, where the local features included dense SIFT, color SIFT and PHOW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NPDILIP6 a [23]:</head><p>This team focused on visual processing and introduced Bossa Nova, a mid-level image representation, that enriched the classical bag-of-words image representations by adding a histogram of distances between the descriptors of the image and those in the codebook. This compact and efficient representation was a useful addition to Fisher Vector representations and the results were improved by combining these two techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PRA a [24]:</head><p>This team submitted only visual runs that combined different visual descriptors and furthermore proposed a dynamic fusion of visual classifiers. A combination of SVMs was used to obtain annotations, where the final decisions were obtained using the mean rule, through majority voting or according to a dynamic score selection approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RedCAD r [25]:</head><p>This team used Latent Dirichlet Allocation to produce topic models and use the Jensen-Shannon Divergence measure for topic similarity to retrieve similar images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REGIM r [26]:</head><p>This team presented an approach that deals with query analysis and relevance-based ranking, two central problems in image retrieval. Their query analysis exploited both the textual and visual information provided with the proposed queries. Ranking was performed by choosing an appropriate similarity measure and enhancing the results with a random walk with restart algorithm.</p><p>UAIC a [27]: This team exploited both textual and visual features of the images to annotate. Tags were processed to extract most frequent elements and then processed using a linear kernel. Used visual features include both local ones, such as TOP-SURF, and global ones, such as Profile Entropy or Color Moments. SVM were used to fuse modalities and a post-processing step was added to check the consistency of predicted labels. In addition, face detection was used to increase the accuracy of person-related concepts.</p><p>UNED a,r [28]: This team presented a system that exploited textual cues to prefilter results before applying image processing techniques in a setting inspired from information retrieval algorithms. With minor adaptations the same system was used for both image annotation and retrieval. Logistic regression was used in order to predict the probability of occurrence of a concept in a given photo. Tag expansion techniques were used to improve image description prior to the annotation and retrieval processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>URJCyUNED a [29]:</head><p>This team used multiple visual features to represented lowlevel image content and WordNet to derive similarities between user tags and the concepts to annotate. Fusion strategies that selected either textual or visual features were tested and the reported results showed that such strategies were superior to the use of both modalities. Nonetheless, their textual approach outperformed both fusion strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Photo annotation evaluation</head><p>The runs submitted by the participants for the annotation subtask contained the relevance assessments for each concept-image combination, where a binary decision was made whether or not the concept was considered to be present in the image and additionally a real-valued score was supplied expressing the confidence of that decision. While the confidence scores are not comparable between runs of different teams or not even necessarily between runs of the same team, they can be seen as indicating a relative ordering of how the images are assigned to the concepts, allowing us to also apply evaluation measures to the confidence scores that are typically applied in the context of information retrieval. In this section we present only an evaluation for a selection of the runs, whereas detailed information on all runs can be found on the Photo Annotation subtask website <ref type="foot" coords="12,204.72,578.13,6.97,6.71" target="#foot_9">10</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Evaluation measures</head><p>To assess the performance of the runs submitted by the teams, we used the following evaluation measures:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean Average Precision (MAP):</head><p>This evaluation measure first ranks the images by their confidence scores, from high to low, for each concept separately. The images are inspected one by one and each time a relevant image is encountered the precision and recall values are computed. In case of ties we consider all the images with the same confidence score together at once and produce only a single precision and recall value for them using a tie-aware ranking approach <ref type="bibr" coords="13,168.44,190.75,15.27,9.58" target="#b29">[30]</ref>. We then interpolate the values so the recall measurements range from 0.0 to 1.0 with steps of 0.1; the precisions at these recall levels are obtained by taking the maximum precision obtained at any non-interpolated recall level equal or greater to the interpolated recall step level under consideration. To obtain the overall non-interpolated MAP (MnAP) value we average the non-interpolated precisions for each concept and then average these averages, whereas to obtain the overall interpolated MAP (MiAP) we instead average the average interpolated precisions over all concepts. In the analysis of the annotation runs we focus on the interpolated MAP, although for completeness we also report the non-interpolated MAP values in the detailed results available on the website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Geometric Mean Average Precision (GMAP):</head><p>This evaluation measure is an extension to MAP. When comparing runs with each other the GMAP specifically highlights improvements obtained on relatively difficult concepts, e.g. increasing the average precision of a concept from 0.05 to 0.10 has a larger impact in its contribution to the GMAP than increasing the average precision from 0.25 to 0.30. To compute the non-interpolated GMAP (GMnAP) and the interpolated GMAP (GMiAP), we follow the same procedure as with MnAP and MiAP, but we instead average the logs of the average precision for each concept, after which we exponentiate the resulting average back to obtain the GMAP. To avoid taking the log of an average precision of zero we add a very small epsilon value to each average precision before computing its log, which we remove again after exponentiating the averages of these logs; when the epsilon value is very small its effect on the final GMAP is negligible. In the analysis of the annotation runs we focus on the interpolated GMAP, although for completeness we also report the non-interpolated GMAP values in the detailed results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F1:</head><p>The F1-measure uses the provided binary scores to determine how well the annotations are. We have computed the instance-averaged, micro-averaged and macro-averaged F1 scores for the photos as well as for the concepts. The instance-F1 for the photos is computed by determining the number of true positives, false positives, true negatives and false negatives in terms of detected concepts and using this to compute the F1-score for each individual photo, after which the F1-scores are averaged over all photos. The micro-F1 for the photos is computed by averaging the precision and recall scores for each individual photo and then computing the F1-score from these averages. The macro-F1 for the photos is computed by aggregating the number of true positives, false positives, true negatives and false negatives over all photos and then computing the F1-score based on these numbers. The micro-F1 and macro-F1 for the concepts are computed in a similar fashion, swapping the roles of the photos and con-cepts. In the analysis of the annotation runs we focus on the micro-F1 scores, although for completeness we also report all instance-F1 and macro-F1 values in the detailed results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>The underlying techniques of the participants could use one of three possible configurations, namely textual features only, visual features only or a multimodal combination of both. For this subtask, 18 teams submitted in total 80 runs, of which 17 runs exclusively used textual features, 28 runs exclusively used visual features and 35 runs used a multimodal approach. We present the overall evaluation results according to the MiAP, GMiAP and micro-F1 in Table 3 to get an understanding of the best results irrespective of the features used, where in the Feature column the letter T refers to the textual configuration, V to the visual configuration and M to the multimodal configuration. In the tables, the ranks indicate the position at which the best run appeared in the results. To compare only runs using the same configuration we present separate results for the textual features in Table <ref type="table" coords="14,257.36,325.46,3.74,9.58">4</ref>, the visual features in Table <ref type="table" coords="14,386.05,325.46,4.98,9.58" target="#tab_3">5</ref> and the multimodal features in Table <ref type="table" coords="14,209.36,337.41,3.74,9.58">6</ref>.</p><p>Table <ref type="table" coords="14,160.92,362.00,3.74,9.58">3</ref>: Summary of the annotation results for the evaluation per concept and image for the best overall run per team per evaluation measure. As we can see, the subtask was best solved with a MiAP of 0.4367 by LIRIS with the three runners up DMS-SZTAKI, CEA LIST and ISI all scoring above a MiAP of 0.4. The same ordering can be found when considering the GMiAP evaluations. As for the micro-F1 score, LIRIS performs best once again although its best run is closely followed by the runs of five other teams. If we look at the rank at which the best run of a team was placed, then the majority of the runs submitted by the teams that took the top 4 positions occupy the places 1-15, indicating that these teams submitted several variations of their runs that were all performing rather well. Table <ref type="table" coords="15,160.92,127.39,3.74,9.58">4</ref>: Summary of the annotation results for the evaluation per concept and image for the best textual run per team per evaluation measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team Rank MiAP</head><p>Team Rank GMiAP Table <ref type="table" coords="15,160.92,471.99,3.74,9.58">6</ref>: Summary of the annotation results for the evaluation per concept and image for the best multimodal run per team per evaluation measure. Inspecting the results for the different feature configurations, we can see that the multimodal configuration was predominantly used in the best performing runs, where moreover the MiAP and GMiAP of the multimodal runs tended to be higher than those of the textual and visual runs. Nonetheless, the micro-F1 scores of the better performing visual runs come close to those of the multimodal runs, which means that they both were able to roughly equivalently well annotate the images with the correct concepts without including too many concepts that were incorrect. Overall, we can see across the different tables that that roughly the same ordering of the teams is maintained, suggesting that the better performing teams had a good underlying strategy that performed well irrespective of the features used. Note that the MLKD team discovered a bug in their textual runs that consequently also affected their multimodal runs. As this discovery happened past the submission deadline, we did not include their updated runs in the results out of fairness to the other teams. Nonetheless, please refer to their working notes <ref type="bibr" coords="16,263.00,286.40,16.60,9.58" target="#b21">[22]</ref> for the updated results, since the fixed runs yielded a substantial improvement over the original runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Team Rank MiAP Team Rank GMiAP</head><p>If we analyze the performance on the individual concepts instead of the performance over all concepts combined, as is shown in Figure <ref type="figure" coords="16,405.94,324.15,3.74,9.58" target="#fig_2">5</ref>, we can clearly observe that the concepts were of varying difficulty. In particular, the concepts quantity_none and quality_infocus appeared relatively easy to detect, with an average accuracy of around 0.8. For many concepts in the categories natural elements and environment the maximum attained accuracy exceeded 0.7, yet at the same time the minimum accuracy came close to or equaled zero, indicating that some of the runs were quite capable of detecting the concepts, whereas other runs were simply unable to detect the concepts at all. The concepts in the impression subcategory proved rather difficult, presumably due to their highly subjective nature, although the two concepts impression_happy and impression_calm were easiest to detect amongst them.  The runs submitted by the participants for the retrieval subtask contained for each query a list of images they believed were the most relevant, where the list was ranked based on their included relevance scores. The number of images that could be returned for a query was restricted to be at most 1000, although eventually we only focused on the top 100 most relevant images returned by each team for each query. Due to the large collection of 200 thousand images within which the participants were to find all relevant images for a particular query, we did not obtain ground truth relevance assessments for each queryimage combination beforehand. As already briefly mentioned in Section 4.2, we formed a pool for each query by aggregating all images that at least one team considered to be relevant for that query. For these pools we then obtained the relevance assessments using crowdsourcing, which we then used to evaluate the runs of each team. In this section we present only an evaluation for a selection of the runs, whereas detailed information on all runs can be found on the Photo Retrieval subtask website<ref type="foot" coords="17,291.74,308.91,6.97,6.71" target="#foot_10">11</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Evaluation measures</head><p>To assess the performance of the runs submitted by the teams, we used the following evaluation measures:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean Average Precision (MAP):</head><p>This evaluation measure is the same as earlier defined in Section 6.1. In the analysis of the retrieval runs we focus on the noninterpolated MAP, although for completeness we also report the interpolated MAP values in the detailed results available on the website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Geometric Mean Average Precision (GMAP):</head><p>This evaluation measure is the same as earlier defined in Section 6.1. In the analysis of the retrieval runs we do not focus on GMAP at all, although we report both the interpolated and the non-interpolated GMAP values in the detailed results.</p><p>AP@X: This evaluation measure reports the average precision obtained once a certain number of images has been encountered. We have computed the scores for values ranging from 10 to 100 in steps of 10, although in the analysis of the retrieval runs we focus only on AP@10, AP@20 and AP@100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Results</head><p>As with the annotation subtask, the techniques of the participants could use one of three possible configurations, namely textual features only, visual features only or a multimodal combination of both. For this subtask, 7 teams submitted in total 47 runs, of which 21 runs exclusively used textual features, 4 runs exclusively used visual features and 22 runs used a multimodal approach.</p><p>In addition, none of the runs this year included a manual intervention in the query generation step, such as explicitly specifying a boolean connection between concepts or using relevance feedback, and instead all retrieved the images in a completely automated fashion. We present the overall evaluation results according to the MnAP, AP@10, AP@20 and AP@100 in Table <ref type="table" coords="18,433.74,166.84,4.98,9.58">7</ref> to get an understanding of the best results indiscriminate of the features used, where as before in the Feature column the letter T refers to the textual configuration, V to the visual configuration and M to the multimodal configuration, while in the Type column the letter M refers to the manual query specification and the letter A to the automatic query specification. In the tables, the ranks indicate the position at which the best run appeared in the results based on MnAP. To compare the runs using the same configuration we present separate results for the textual features in Table <ref type="table" coords="18,257.36,262.49,3.74,9.58">8</ref>, the visual features in Table <ref type="table" coords="18,386.05,262.49,4.98,9.58">9</ref> and the multimodal features in Table <ref type="table" coords="18,209.36,274.44,8.30,9.58" target="#tab_0">10</ref>.</p><p>Table <ref type="table" coords="18,161.42,300.21,3.74,9.58">7</ref>: Summary of the retrieval results for the evaluation per query for the best overall run per team. From the results we can immediately see that 9 runs of the IMU team were better than any run of the other teams, where all these runs addressed the subtask using textual features only. The runner-up MLKD was close in terms of performance to IMU and their 9 mainly multimodal teams were better than those of the remaining teams. Regarding performance, we recognize that the retrieval subtask was difficult, considering the highest MnAP was less than 0.1 and thus on average for every 10 images retrieved only at most one of them would be relevant to the query. However, if we look at the results from a user perspective, where search results are typically shown by the search engine in individual pages containing 10 or 20 images each, then based on the average precision at recall results we would have to conclude that in most instances the first two pages of results usually would not contain a single relevant image.</p><p>One of the most demanding aspects of the queries was that they were not composed of just a linear combination of individual concepts, but often had additional nuances associated with them that could be explained in multiple ways and constraints that required additional modeling. For example, the search query close-up red roses implicitly required the detection of the concepts view_closeupmacro, quality_selectivefocus and flora_flower, where additional constraints were placed on the shape (i.e. rose) and color (i.e. red) of the flower. Also, the query flying airplane had the remark that the airplanes should not be taking off or landing, where photos of planes with their landing gear down but depicted in the middle of the sky were relevant or not was a point of contention amongst the annotators.</p><p>If we compare the runs per configuration we can see that even the best visual run did not come close to the performance of most of the textual or multimodal runs, indicating that textual features played an important role in addressing the query-based retrieval. Whereas the visual features could detect the individual concepts reasonably well, as judged by the results of the annotation subtask, they proved to be inadequately able to deal with additional nuances and constraints associated with the queries. Table <ref type="table" coords="19,161.42,258.38,3.74,9.58">8</ref>: Summary of the retrieval results for the evaluation per query for the best textual run per team.</p><p>Team Rank MnAP AP@10 AP@20 AP@100 Type IMU 1 0.0933 0.0187 0.0338 0.1715 A MLKD 10 0.0534 0.0111 0.0222 0.1335 A UNED 12 0.0250 0.0004 0.0019 0.0729 A MSATL 14 0.0138 0.0044 0.0077 0.0547 A ReDCAD 15 0.0129 0.0003 0.0042 0.0475 A REGIM 18 0.0025 0.0009 0.0024 0.0169 A Table <ref type="table" coords="19,161.42,380.63,3.74,9.58">9</ref>: Summary of the retrieval results for the evaluation per query for the best visual run per team.</p><p>Team Rank MnAP AP@10 AP@20 AP@100 Type MLKD 1 0.0244 0.0098 0.0176 0.0751 A IMU 2 0.0045 0.0030 0.0064 0.0316 A REGIM 3 0.0031 0.0022 0.0039 0.0164 A Table 10: Summary of the retrieval results for the evaluation per query for the best multimodal run per team.</p><p>Team Rank MnAP AP@10 AP@20 AP@100 Type MLKD 1 0.0702 0.0214 0.0342 0.1495 A KIDS NUTN 8 0.0313 0.0051 0.0077 0.0729 A UNED 9 0.0295 0.0125 0.0206 0.0848 A REGIM 17 0.0020 0.0005 0.0019 0.0154 A Analyzing the individual queries, as is shown in Figure <ref type="figure" coords="19,401.45,572.46,3.74,9.58">6</ref>, we can see that the average accuracy was highest for the query skyline fireworks with an average MnAP of 0.13, followed by horse riding and full moon. The highest accuracy was obtained for the query hot air balloon with a MnAP of 0.66. Yet, all queries had a minimum accuracy of zero, indicating that for each query at least one of the submitted runs was not able to retrieve any relevant images at all. Overall, whereas the average performance is quite low, the highest accuracies are much better although still far away from perfect. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>ImageCLEF's Flickr Photo Annotation and Retrieval task is a multi-label classification challenge that offered a benchmark for testing novel visual concept detection, annotation and retrieval algorithms on a public collection containing photos gathered from the social sharing website Flickr. The task could be addressed by analyzing the textual features and/or visual features of the images in the dataset. The aim of the annotation subtask was to automatically annotate the images with one or more semantic concepts, whereas for the retrieval subtask the goal was to retrieve the most relevant images for concept-oriented search queries. The concepts and queries were to a certain extent similar to those used last year, although we removed several of them based on feedback from former participants and added new or refined existing ones based on an inspection of image search logs to provide a more realistic context for the task. This year a total of 100 teams registered for the task, while eventually 18 teams together submitted 80 annotation runs and 7 teams together submitted 47 retrieval runs. The results indicate that the annotation subtask, like previous year, could be solved reasonably well, with the top runs achieving a MiAP of over 0.4 using multimodal features and a micro-F1 score of over 0.55 using visual or multimodal features. All in all, if we were to pick the best technique for each individual concept, i.e. we would only look at the maximum obtained accuracy of each concept, then for the majority of concepts an accuracy between 0.6 and 0.8 was achieved, which can certainly be called promising in light of the challenging set of concepts and the ambiguity involved in evaluating them. The retrieval subtask proved to be more difficult compared to last year, with the best run using textual features and achieving a MnAP of just under 0.1. None of the runs involved explicit manual intervention and thus the search queries were all automatically executed, which is somewhat surprising considering that last year the manual runs performed generally better than the automatic runs. In comparison with the annotation subtask, the multiple concepts, nuances and constraints that were embedded into the queries made the retrieval subtask notably harder to solve.</p><p>Even though this year we had placed more emphasis on collecting more reliable ground truth annotations, the performance of crowdsource workers does not reach the same level as that of professionally trained editors, and as such the annotations may still have contained inaccuracies. In terms of evaluating the annotations, it is not necessarily an optimal strategy to apply the majority vote rule to the crowdsourced relevance assessments due to the subjective nature of many concepts, where the truth of whether a concept is present in an image or not is flexible and may depend on the viewer. Furthermore, it would be worthwhile looking into 'gamifying' the collection of the relevance assessment <ref type="bibr" coords="21,159.87,274.44,16.60,9.58" target="#b30">[31]</ref> to further boost their quality.</p><p>For next year's task we plan to work more closely together with the participants, while at the same time reaching further out to what the world is really searching for, in order to redefine the aims of the annotation and retrieval subtasks and optimize the ground truth collection. This may mean breaking away from the set of concepts that have been the focus of this task during the previous years, and instead coming up with more realistic scenarios that can instantly be applied to the grand challenges our research community faces and have instant worldwide impact. Nonetheless, whereas several hurdles still will need to be crossed to get closer to a perfect precision, the contributions of the participants to both subtasks have raised the bar and advanced the state of the art in visual concept detection, annotation and retrieval. The picture shows smoke emitted from a fire source, such as smoke coming out of chimneys, cigarettes and airplanes.</p><p>15 combustion_fireworks (54, <ref type="bibr" coords="25,238.95,155.43,8.72,6.71" target="#b17">18)</ref> The picture shows exploding fireworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="16">lighting_shadow (861,576)</head><p>The picture shows a sharp and clearly visible shadow of something in the scene. This means that we are not looking for soft shadows, vague shadows and small shadows, and we are also not looking for shadows of something not visible in the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17">lighting_reflection (448,273)</head><p>The picture shows a sharp and clearly visible reflection of something in the scene. This means that we are not looking for soft reflections, vague reflections and small reflections, and we are also not looking for reflections of something not visible in the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18">lighting_silhouette (475,314)</head><p>The picture shows a silhouette of something in the scene. A silhouette refers to the dark shape and outline of someone or something visible against a lighter background. You should not be able to see any details of the shape due to it being so dark. For a silhouette to be present there must be some kind of background light visible. Pay attention with black and white images, because you may easily confuse actual dark shapes with silhouettes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19">lighting_lenseffect (530,344)</head><p>The picture shows that the light sources visible in the image have been affected in some way. We are particularly looking for lens flares, where circular lighting effects are visible in the image, halos, where spiky lighting effects are visible around the light source, and bokeh, where the light sources are severely blurred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20">scape_mountainhill (295,218)</head><p>The picture shows a mountain or hill.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21">scape_desert (73,36)</head><p>The picture shows a desert, containing sandy or rocky plains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22">scape_forestpark (451,303)</head><p>The picture shows a forest or park, typically containing many trees and/or grass. We are not interested in people's gardens/backyards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="23">scape_coast (766,436)</head><p>The picture shows a coast, where the sea meets land. This includes photos showing cliffs, rocks sticking out of the sea close to land and the beach. In principle, it needs to be clear the image was taken at the coast or of the coast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="24">scape_rural (361,237)</head><p>The picture shows a landscape of the countryside, typically showing an open view of a rural or agricultural environment with at most a few man-made objects like cottages and small roads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="25">scape_city (906,572)</head><p>The picture shows a view of the city from the inside or the outside. We are interested in photos that let you get a clear impression of the city, so this means we are not looking for photos focusing on specific things in the city like shops, people and cars; rather, the scene will typically be taken with a zoomed out lens to capture as much of the scene as possible. The picture shows a large piece of graffiti that is the dominant feature of the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="27">water_underwater (53,44)</head><p>The picture shows an underwater scene.</p><p>28 water_seaocean <ref type="bibr" coords="26,204.16,172.53,15.17,6.71">(369,</ref><ref type="bibr" coords="26,219.33,172.53,12.14,6.71">197)</ref> The picture shows a sea or ocean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="29">water_lake (135,75)</head><p>The picture shows a lake.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="30">water_riverstream (181,115)</head><p>The picture shows a river or stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="31">water_other (399,255)</head><p>The picture shows liquid water not belonging to the other water categories. It can appear in all shapes and forms, such as a glass of water, water droplets, or a puddle of rain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="32">flora_tree (2129,1343)</head><p>The picture shows a tree or a close-up of a tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="33">flora_plant (419,262)</head><p>The picture shows an indoor or outdoor plant, which typically has many leaves and small or no flowers. This includes cacti and close-ups of plants. As a rule of thumb, if you yourself would put what you see in a vase, then it is a flower. If you would put it in a pot, then it is a plant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="34">flora_flower (719,508)</head><p>The picture shows an indoor or outdoor plant, which typically has small or no leaves and a big flower. This includes close-ups of flowers. As a rule of thumb, if you yourself would put what you see in a vase, then it is a flower. If you would put it in a pot, then it is a plant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="35">flora_grass (858,548)</head><p>The picture shows a field of grass or a close-up of grass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="36">fauna_cat (106,72)</head><p>The picture shows a cat-like animal. This includes wild cats such as lions and cheetahs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="37">fauna_dog (361,267)</head><p>The picture shows a dog-like animal. This includes wild dogs such as wolves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="38">fauna_horse (64,40)</head><p>The picture shows a horse-like animal. This includes animals such as donkeys.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="39">fauna_fish (49,39)</head><p>The picture shows a fish-like animal. This includes animals such as sharks. Photos showing underwater creatures that do not look like typical fish, such as jellyfish, seahorses, tortoises, etc. should not be marked with this concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="40">fauna_bird (352,219)</head><p>The picture shows a bird-like animal. This includes animals such as pelicans, flamingos, ducks, geese and swans. 41 fauna_insect <ref type="bibr" coords="26,194.21,641.05,15.17,6.71">(137,</ref><ref type="bibr" coords="26,209.38,641.05,12.14,6.71">114)</ref> The picture shows an insect-like animal. This includes animals such as flies, wasps, bees, butterflies and moths. The picture shows a scene of which most, if not all, of the content is in focus. As a rule of thumb, the photographer that took the photo wanted to capture everything in the scene and did not focus on anything specifically. 62 quality_selectivefocus <ref type="bibr" coords="28,226.92,167.49,18.70,6.71">(3549,</ref><ref type="bibr" coords="28,245.62,167.49,15.58,6.71">2293)</ref> The picture shows a scene that partly is very much in focus and partly very much out of focus. You can clearly distinguish between an area in the image that is the 'foreground' and another area that is the 'background'. As a rule of thumb, the photographer that took the photo wanted to capture one particular part of the scene in particular, which is the part that is in focus, whereas the other part is out of focus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="63">quality_outfocus (100,83)</head><p>The picture shows a scene of which most, if not all, of the content is out of focus. As a rule of thumb, the photographer that took the photo made a mistake and did not properly set the lens focus, so the scene is a bit or very much blurred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="64">quality_motionblur (287,176)</head><p>The picture shows a scene of which part or all is blurred as a result of the camera moving or part of the scene moving while the photo was taken. This includes long exposure photos resulting in light trails. Typically when the camera was moved the entire scene looks streaky, whereas if part of the scene moved then only that part looks streaky.</p><p>65 quality_noisyblocky <ref type="bibr" coords="28,220.44,350.14,15.17,6.71">(318,</ref><ref type="bibr" coords="28,235.61,350.14,12.14,6.71">199)</ref> The picture is of low quality and very noisy or blocky. This is not because the scene is out of focus, but because the image was shot at a very low resolution or it was compressed afterwards. It may also be taken in low light conditions, introducing lots of tiny specks in the image. 66 style_pictureinpicture <ref type="bibr" coords="28,227.93,411.02,14.89,6.71">(113,</ref><ref type="bibr" coords="28,242.82,411.02,8.93,6.71">64)</ref> The picture is divided into multiple different photos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="67">style_circularwarp (167,141)</head><p>The picture shows a scene that is distorted/warped, so that straight lines in the scene look curved/round in the image and give a circular effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="68">style_graycolor (306,219)</head><p>The picture shows a scene where most of the content is shown in black-and-white (grayscale), but only a small part is shown in its original color(s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="69">style_overlay (567,371)</head><p>The picture contains a piece of text or a logo that the photographer has added to the photo after it was taken, for example a copyright statement. Thus the text or logo was not present in the original scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="70">view_portrait (1533,1069)</head><p>The picture shows a scene where one or more persons are the center of attention, typically facing the camera and aware that a photo is being taken of them. The photo normally captures at least their entire face, although a small part of it may be missing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="71">view_closeupmacro (2340,1589)</head><p>The picture shows a close-up of objects, where a lot of zoom has been used by the photographer, and includes macro shots, where things are shown much larger than they normally are. In contrast with a portrait a close-up can be of anything and not just people, although a photo showing only a face would be called a portrait. The picture shows an indoor scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="73">view_outdoor (4856,3259)</head><p>The picture shows an outdoor scene.</p><p>74 setting_citylife <ref type="bibr" coords="29,202.18,174.89,18.70,6.71">(1676,</ref><ref type="bibr" coords="29,220.88,174.89,15.58,6.71">1128)</ref> The picture shows a typical scene from life in the city or town, showing for instance streets, shops, restaurants or bars. The picture may also show people doing their regular things such as shopping, talking and commuting. The image must clearly show it was taken in a city or town.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="75">setting_partylife (368,256)</head><p>The picture shows scenes related to parties or celebrations, where people for instance are dancing, drinking or chatting, a music band is performing on stage or it may even show party-related equipment. Photos showing parties are typically taken during night time, although this does not always have to be the case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="76">setting_homelife (945,645)</head><p>The picture shows scenes of things taking place in or around one's home, such as having dinner, watching television, lying in bed, reading a book in the garden or even playing with the cat. The image must clearly show it was taken in or around someone's home.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="77">setting_sportsrecreation (506,283)</head><p>The picture shows a scene where people are doing or watching sports or are enjoying themselves in a relaxed way, such as lying on the beach. Note that recreation is not the same as having or being at a party; a party is typically an organized event for dancing or celebrating, whereas recreation is more the opposite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="78">setting_fooddrink (626,430)</head><p>The picture shows a scene where food and/or drink play an important role. A photo showing only a little bit of food or drink, which does not have particular focus in the scene, should not be marked with this concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="79">impression_happy (1146,840)</head><p>The picture shows a scene that looks happy and/or gives you a happy and warm feeling. Other words you can associate with this concept are joy and pleasure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="80">impression_calm (2119,1441)</head><p>The picture shows a scene that looks calm, quiet, peaceful and/or relaxed. Other words you can associate with this concept are soothing and comforting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="81">impression_inactive (1262,877)</head><p>The picture shows a scene where nothing exiting happens and it expresses a sense of timelessness. Other words you can associate with this concept are boring and passive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="82">impression_melancholic (880,594)</head><p>The picture shows a scene that looks gloomy and depressed, giving you a sense of sadness and darkness. An example would for instance be the feeling you get when you miss someone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="83">impression_unpleasant (623,447)</head><p>The picture shows a scene that feels uncomfortable, troublesome or nasty, giving you a general sense of unpleasantness. Other words you can associate with this concept are displeasing and dreadful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="84">impression_scary (377,278)</head><p>The picture shows a scene that is sinister or terrifying. Other words you can associate with this concept are spooky and horrifying.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="85">impression_active (1087,735)</head><p>The picture shows a scene that is dynamic, colorful and gives you energy. Other words you can associate with this concept are uplifting, lively, sporty and busy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="86">impression_euphoric (189,140)</head><p>The picture shows a scene that bursts with energy and joy, giving you the impression of being on top of the world. Other words you can associate with this concept are excited, ecstatic and blissful. 87 impression_funny (765,557)</p><p>The picture shows a scene looks comical and makes you laugh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="88">transport_cycle (220,142)</head><p>The picture shows an unmotorized or motorized bike. This includes bikes such as pushbikes, scooters and motorbikes. A bike typically has two wheels, but can have three wheels or even have a sidecar attached. 89 transport_car (500,321)</p><p>The picture shows a car. This includes sedans, convertibles, vans, landrovers, suburbans, pick-up trucks, race cars and even taxis. In principle, vehicles that belong to this category have four wheels and have as main purpose the transport of only a small number of people or personal materials. Note that we are not looking for photos taken from within the vehicle, but only from the outside.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="90">transport_truckbus (69,44)</head><p>The picture shows a truck or bus. In principle, vehicles that belong to this category have six wheels or more and have as main purpose the transport of many people or goods. Note that we are not looking for photos taken from within the vehicle, but only from the outside. 91 transport_rail <ref type="bibr" coords="30,199.02,434.37,11.62,6.71">(93,</ref><ref type="bibr" coords="30,210.64,434.37,8.72,6.71">61)</ref> The picture shows a rail vehicle, such as a train, tram and metro. Note that we are not looking for photos taken from within the vehicle, but only from the outside.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="92">transport_water (187,127)</head><p>The picture shows a water vehicle, such as a rowing boat, sailing boat, yacht, navy ship and freighter. Note that we are not looking for photos taken from within the vehicle, but only from the outside.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="93">transport_air (89,50)</head><p>The picture shows an air vehicle, such as an airplane and helicopter. Note that we are not looking for photos taken from within the vehicle, but only from the outside.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Query descriptions</head><p>In this appendix we list all query definitions, which were part of the instructions we gave to the crowdsourcing workers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="0">flying airplane</head><p>The user is looking for photos showing one or more airplanes flying in the sky. He is not looking for photos that show airplanes on the ground, taking off or landing, nor for pictures of airplanes from the inside.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">horse riding</head><p>The user is looking for photos showing one or more persons riding horses, which includes people riding horses at a rodeo. He is not interested in pictures of people on the ground next to a horse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">mountain coast</head><p>The user is looking for photos showing mountains or hills right besides the coast or lake.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">single performer live music</head><p>The user is looking for photos showing a single person performing live on stage, so only one person should be visible that is singing, mixing, or playing an instrument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">snowy trees</head><p>The user is looking for photos of trees that are covered in snow or frost, where the scene contains more than one tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">hot air balloon</head><p>The user is looking for photos showing one or more hot air balloons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">beach sunset sunrise</head><p>The user is looking for photos taken at the beach during sunset or sunrise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">old men</head><p>The user is looking for photos showing only one or more elderly men, so no other people should be additionally visible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">train station</head><p>The user is looking for photos showing a train stopping at or traveling through a station. He is not looking for photos showing the train from the inside. Pictures of metros and trams are also fine. 9 sad dogs The user is looking for photos showing one or more dogs that look sad. He is particularly looking for pictures exhibiting selective focus, where the dogs are in focus and the background is out of focus.</p><p>10 silence before the storm The user is looking for photos of an overcast or fogged over day at sea, where it seems it could start storming any moment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">smooth water flow</head><p>The user is looking for photos showing a stream of water. The picture should be taken with a long exposure time, so that the flowing water has become motion blurred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12">birds in a tree</head><p>The user is looking for photos of one or more birds sitting on a tree branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13">person silhouette</head><p>The user is looking for photos showing the silhouette of a person, where the silhouette captures all, or almost all, of the entire person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14">traffic light trails</head><p>The user is looking for photos showing light trails made by traffic at night. The picture effectively is taken with a long exposure time and thus the trails form straight or smoothly curving lines following the flow of traffic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15">city reflections by day</head><p>The user is looking for photos taken during the day showing part of a city and its reflection in a large body of still water. The water surface ideally is like a mirror to give a crystal clear reflection, although some diffusion in the reflection is acceptable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="16">close-up red roses</head><p>The user is looking for photos showing one or more red roses. The picture should have selective focus, where the focus is on the roses and the background is out of focus. The roses shown should only be red and not have other colors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17">double rainbow</head><p>The user is looking for photos showing a double rainbow. Pictures where the second rainbow is faintly but still distinctly visible are fine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18">fast car</head><p>The user is looking for photos of a fast road car, such as a Porsche, Lambourghini, Ferrari etc. He is also interested in images of sports cars on a circuit. The picture may show motion blur, although this does not necessarily have to be the case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19">autumn park leaves</head><p>The user is looking for photos of a park in autumn, where the trees have yellow/reddish leaves. Ideally the ground is covered in those leaves as well, although this is not necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20">close-up cupcakes</head><p>The user is looking for close-up photos of one or more cupcakes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21">halloween costumes</head><p>The user is looking for inspiration what to wear for Halloween and wants to find photos showing one or more people wearing a costume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22">surf swim</head><p>The user is looking for photos showing a sea or lake where one or more persons are doing sports, such as surfing or swimming, or are relaxing, such as simply floating or bathing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="23">flower field</head><p>The user is looking for photos showing a field of flowers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="24">foggy forest</head><p>The user is looking for photos showing a park or forest where fog is present. He is not interested in seeing smoke.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="25">grass field recreation</head><p>The user is looking for photos showing one or more people on a grass field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="26">woman short hairstyles</head><p>The user is looking for portraits showing a single teenage or adult woman with short hair. The user is looking for night time photos showing a city skyline with exploding fireworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="28">full moon</head><p>The user is looking for photos showing a full moon. He is interested in both pictures showing a close-up of the moon as well as the moon being in the background of a scene, as long as the moon is clearly visible and practically in the full moon stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="29">sleeping baby</head><p>The user is looking for photos showing a sleeping (calm, quiet) baby.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="30">graffiti artist</head><p>The user is looking for photos showing a person and a graffiti artwork.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="31">fish tank</head><p>The user is looking for photos showing one or more fish in a tank or bowl.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="32">people dancing at party</head><p>The user is looking for photos showing a large group of people dancing to music.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="33">beautiful sceneries</head><p>The user is looking for photos showing a scenery where no people are visible. The scenery, for example a landscape, cityscape, seascape or mountainscape, should be in focus and not a close-up of anything. To ensure a good photographer took the picture, there should be a small copyright notice visible in one of the corners of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="34">underwater sea life no divers</head><p>The user is looking for photos showing sea life under the surface, so at least one aquatic animal should be visible. He is not interested in seeing photos containing divers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="35">euphoric people</head><p>The user is looking for photos showing one or more persons that are exceptionally happy or euphoric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="36">fire without smoke</head><p>The user is looking for photos showing a fire or explosion where no smoke is visible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="37">high speed cycling</head><p>The user is looking for photos showing one or more people cycling through the city at high speeds, so the picture should show high amounts of motion blur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="38">water drops</head><p>The user is looking for photos showing one or more water drops in selective focus, where the water drops may be accompanied by a splash or stream of water, as long as the water drops are clearly recognizable. The picture does not necessarily have to be a close-up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="39">dark clothing</head><p>The user is looking for photos showing one or more persons wearing mostly black or dark gray clothes, so there should be no people visible wearing other colored clothing. Small parts may be colored, such as shoes for instance, but the persons outer layers should be made of dark fabric.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,134.77,235.33,345.82,9.58;8,134.77,247.29,345.82,9.58;8,134.77,259.24,308.63,9.58;8,134.77,105.87,345.85,117.90"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: The concepts associated with the example image for which at least one worker indicated they were present. The figure further includes the relative agreement between the workers and the outcome of the majority vote.</figDesc><graphic coords="8,134.77,105.87,345.85,117.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="10,134.77,184.01,32.10,9.34;10,169.62,182.41,3.49,6.54;10,173.60,183.93,306.99,9.58;10,134.77,195.89,345.82,9.58;10,134.77,207.84,345.82,9.58;10,134.77,219.80,345.82,9.58;10,134.77,231.75,345.82,9.58;10,134.77,243.71,170.18,9.58"><head>DBRIS a [ 15 ]:</head><label>15</label><figDesc>This team focused on low-level image descriptions that combine different SIFT based features. Two image representations were compared: the first was based on spatial pyramids and the second on visual phrases. Results show that visual phrases clearly outperformed spatial pyramids and classical bags of visual words. Visual phrases alone also outperformed their combination with the other representation schemes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="16,134.77,622.67,345.82,9.58;16,134.77,634.63,345.82,9.58;16,134.77,646.58,345.82,9.58;16,134.77,658.54,251.13,9.58"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Summary of the per concept annotation accuracy for all runs combined. The black bar lengths represent the standard deviations in accuracy, whereas the red bar lengths represent the minimum and maximum accuracy achieved. The concepts are list in order as specified in Appendix A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="7,134.77,227.28,345.83,238.34"><head></head><label></label><figDesc></figDesc><graphic coords="7,134.77,227.28,345.83,238.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="9,134.77,105.87,345.83,165.45"><head></head><label></label><figDesc></figDesc><graphic coords="9,134.77,105.87,345.83,165.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.77,174.57,350.01,358.29"><head>Table 1 :</head><label>1</label><figDesc>Overview of the concepts used in the photo annotation subtask, hierarchically grouped into categories.</figDesc><table coords="5,136.16,204.51,348.61,63.75"><row><cell cols="2">Natural elements</cell></row><row><cell>time of day</cell><cell>day, night, sunrise/sunset</cell></row><row><cell cols="2">celestial bodies sun, moon, stars</cell></row><row><cell>weather</cell><cell>clear sky, overcast sky, cloudy sky, rainbow, lightning, fog/mist, snow/ice</cell></row><row><cell>combustion</cell><cell>flames, smoke, fireworks</cell></row><row><cell cols="2">lighting effects shadow, reflection, silhouette, lens effects</cell></row></table><note coords="5,136.16,281.62,53.80,8.41;5,136.16,292.79,27.07,8.55;5,195.64,292.90,277.56,8.63;5,136.16,303.75,20.42,8.55;5,195.64,303.86,193.35,8.63;5,136.16,314.70,16.34,8.55;5,195.64,314.82,94.58,8.63;5,136.16,325.66,20.43,8.55;5,195.64,325.78,262.53,8.63;5,136.16,347.77,27.89,8.41;5,136.16,358.94,31.05,8.55;5,195.64,359.05,207.17,8.63;5,136.16,369.90,11.95,8.55;5,195.64,370.01,138.31,8.63;5,136.16,380.86,24.42,8.55;5,195.64,380.97,50.37,8.63;5,136.16,391.81,42.68,8.55;5,195.64,391.93,148.91,8.63;5,136.16,413.92,64.50,8.41;5,136.16,425.09,25.57,8.55;5,195.64,425.21,252.83,8.63;5,136.16,436.05,16.94,8.55;5,195.64,436.17,206.46,8.63;5,136.16,447.01,16.94,8.55;5,195.64,447.12,165.22,8.63;5,136.16,457.97,15.44,8.55;5,195.64,458.08,235.07,8.63;5,136.16,468.93,39.20,8.55;5,195.64,469.04,289.13,8.63;5,195.64,480.00,23.81,8.63;5,136.16,501.99,69.97,8.41;5,136.16,513.16,348.61,8.74;5,195.64,524.23,64.14,8.63"><p>Environment scenery mountain/hill, desert, forest/park, coast, landscape, cityscape, graffiti water underwater, sea/ocean, lake, river/stream, other flora tree, plant, flower, grass fauna cat, dog, horse, fish, bird, insect, spider, amphibian/reptile, rodent People quantity none, zero, one, two, three, small group, large group age baby, child, teenager, adult, elderly gender male, female relationship family/friends, co-workers, strangers Image elements quality in focus, selective focus, out of focus, motion blur, noisy/blocky style picture-in-picture, circular warp, gray-color, overlay view portrait, close-up/macro, indoor, outdoor type city life, party life, home life, sports/recreation, food/drink impression happy, calm, inactive, melancholic, unpleasant, scary, active, euphoric, funny Human elements transportation bicycle/motorcycle, car/van/pick-up, truck/bus, rail vehicle, water vehicle, air vehicle</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,145.44,127.39,321.03,257.07"><head>Table 2 :</head><label>2</label><figDesc>Concept-oriented queries used in the photo retrieval subtask.</figDesc><table coords="6,145.44,145.38,321.03,239.09"><row><cell>Query</cell><cell>Title</cell><cell>Query</cell><cell>Title</cell></row><row><cell>0</cell><cell>flying airplane</cell><cell>21</cell><cell>halloween costumes</cell></row><row><cell>1</cell><cell>horse riding</cell><cell>22</cell><cell>surf swim</cell></row><row><cell>2</cell><cell>mountain coast</cell><cell>23</cell><cell>flower field</cell></row><row><cell>3</cell><cell>single performer live music</cell><cell>24</cell><cell>foggy forest</cell></row><row><cell>4</cell><cell>snowy trees</cell><cell>25</cell><cell>grass field recreation</cell></row><row><cell>5</cell><cell>hot air balloon</cell><cell>26</cell><cell>woman short hairstyles</cell></row><row><cell>6</cell><cell>beach sunset sunrise</cell><cell>27</cell><cell>skyline fireworks</cell></row><row><cell>7</cell><cell>old men</cell><cell>28</cell><cell>full moon</cell></row><row><cell>8</cell><cell>train station</cell><cell>29</cell><cell>sleeping baby</cell></row><row><cell>9</cell><cell>sad dogs</cell><cell>30</cell><cell>graffiti artist</cell></row><row><cell>10</cell><cell>silence before the storm</cell><cell>31</cell><cell>fish tank</cell></row><row><cell>11</cell><cell>smooth water flow</cell><cell>32</cell><cell>people dancing at party</cell></row><row><cell>12</cell><cell>birds in a tree</cell><cell>33</cell><cell>beautiful sceneries</cell></row><row><cell>13</cell><cell>person silhouette</cell><cell>34</cell><cell>underwater sea life no divers</cell></row><row><cell>14</cell><cell>traffic light trails</cell><cell>35</cell><cell>euphoric people</cell></row><row><cell>15</cell><cell>city reflections by day</cell><cell>36</cell><cell>fire without smoke</cell></row><row><cell>16</cell><cell>close-up red roses</cell><cell>37</cell><cell>high speed cycling</cell></row><row><cell>17</cell><cell>double rainbow</cell><cell>38</cell><cell>water drops</cell></row><row><cell>18</cell><cell>fast car</cell><cell>39</cell><cell>dark clothing</cell></row><row><cell>19</cell><cell>autumn park leaves</cell><cell>40</cell><cell>above the clouds</cell></row><row><cell>20</cell><cell>close-up cupcakes</cell><cell>41</cell><cell>bride</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="15,134.77,156.63,349.23,290.84"><head>Table 5 :</head><label>5</label><figDesc>Summary of the annotation results for the evaluation per concept and image for the best visual run per team per evaluation measure.</figDesc><table coords="15,398.72,156.63,85.28,8.41"><row><cell>Team Rank micro-F1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="20,134.77,114.61,345.82,203.55"><head></head><label></label><figDesc>Summary of the per query retrieval accuracy for all runs combined. The black bar lengths represent the standard deviations in accuracy, whereas the red bar lengths represent the minimum and maximum accuracy achieved. The queries are list in order as specified in Appendix B.</figDesc><table coords="20,134.77,114.61,331.58,167.68"><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MnAP</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>flying airplane</cell><cell>horse riding</cell><cell>mountain coast</cell><cell>single performer live music</cell><cell>snowy trees</cell><cell>hot air balloon</cell><cell>beach sunset sunrise</cell><cell>old men</cell><cell>train station</cell><cell>sad dogs</cell><cell>silence before the storm</cell><cell>smooth water flow</cell><cell>birds in a tree</cell><cell>person silhouette</cell><cell>traffic light trails</cell><cell>city reflections by day</cell><cell>close-up red roses</cell><cell>double rainbow</cell><cell>fast car</cell><cell>autumn park leaves Queries close-up cupcakes halloween costumes surf swim</cell><cell>flower field</cell><cell>foggy forest</cell><cell>grass field recreation</cell><cell>woman short hairstyles</cell><cell>skyline fireworks</cell><cell>full moon</cell><cell>sleeping baby</cell><cell>graffiti artist</cell><cell>fish tank</cell><cell>people dancing at party</cell><cell>beautiful sceneries</cell><cell>underwater sea life no divers</cell><cell>euphoric people</cell><cell>fire without smoke</cell><cell>high speed cycling</cell><cell>water drops</cell><cell>dark clothing</cell><cell>above the clouds</cell><cell>bride</cell></row><row><cell cols="2">Fig. 6:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,657.93,112.98,6.31"><p>http://www.flickr.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.73,657.93,280.25,6.31"><p>http://www.image-net.org/challenges/LSVRC/2012/index</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,144.73,657.93,313.03,6.31"><p>http://www.chrisevansdev.com/computer-vision-opensurf.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,144.73,624.21,252.85,6.31"><p>http://press.liacs.nl/researchdownloads/topsurf</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,144.73,635.45,166.77,6.31"><p>http://www.colordescriptors.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="4,144.73,646.69,156.01,6.31"><p>http://labelme.csail.mit.edu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="4,144.73,657.93,123.74,6.31"><p>http://images.yahoo.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="6,144.73,646.69,107.60,6.31"><p>http://www.mturk.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="6,144.73,657.93,139.88,6.31"><p>http://www.crowdflower.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9" coords="12,144.73,657.93,269.49,6.31"><p>http://imageclef.org/2012/photo-flickr/annotation/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10" coords="17,144.73,657.93,264.11,6.31"><p>http://imageclef.org/2012/photo-flickr/retrieval/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to express our deepest gratitude to the <rs type="funder">European Science Foundation</rs> for their financial support, which made the collection of the ground truth possible. Furthermore, we would like to thank the editors of the <rs type="institution">National Institute of Standards and Technology</rs> for assisting with the gold standard creation for the retrieval subtask. We were able to use their services through a successful collaboration with the TREC crowdsourcing track, kindly arranged by <rs type="person">Gabriella Kazai</rs> from <rs type="affiliation">Microsoft Research</rs>. Finally, we are very grateful to the many crowdsource workers who performed the actual relevance assessments, as well as to the Flickr users whose photos we used in this task.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Concept descriptions</head><p>In this appendix we list all concept definitions, which were part of the instructions we gave to the crowdsourcing workers. Between parenthesis we list the number of images in which a particular concept was considered to be present in the training and testing sets, respectively, based on applying the majority vote rule to the relevance assessments of the workers. 0 timeofday_day <ref type="bibr" coords="24,196.52,218.36,18.70,6.71">(4897,</ref><ref type="bibr" coords="24,215.22,218.36,15.58,6.71">3325)</ref> The picture shows that it was taken during the day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">timeofday_night (685,431)</head><p>The picture shows that it was taken during the night.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">timeofday_sunrisesunset (508,348)</head><p>The picture shows that it was taken during the transition from night to day or from day to night, i.e. during sunrise, sunset, dusk, dawn or twilight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">celestial_sun (368,224)</head><p>The picture shows the sun. If only the effects of the sun are visible, e.g. clouds are lit up by the sunshine but the sun itself is not visible, then you should not mark the image as showing the sun.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">celestial_moon (101,68)</head><p>The picture shows the moon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">celestial_stars (44,25)</head><p>The picture shows the stars in the sky.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">weather_clearsky (1105,705)</head><p>The picture shows a sky that is completely clear of clouds, although a small amount of white puffs of water vapor in a clear sky is still acceptable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">weather_overcastsky (694,433)</head><p>The picture shows a sky that is completely covered in densely packed clouds, although a small amount of visible sky is acceptable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">weather_cloudysky (1196,812)</head><p>The picture shows a sky containing one or more large solid clouds. In principle it is the middle ground between a clear sky and an overcast sky. 9 weather_rainbow (33, <ref type="bibr" coords="24,217.77,522.02,8.72,6.71" target="#b17">18)</ref> The picture shows a rainbow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">weather_lightning (167,125)</head><p>The picture shows a lightning strike.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">weather_fogmist (168,100)</head><p>The picture shows a cloud of water, dust or sand particles suspended in the atmosphere at or near the earth's surface that obscures or restricts visibility. Note that this at times may look similar to smoke, so please do not confuse the two concepts.</p><p>12 weather_snowice (100,91)</p><p>The picture shows snow or ice. This also includes whiteness as a result of frost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13">combustion_flames (68,35)</head><p>The picture shows flames emitted from a fire source. The picture shows a spider-like animal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="43">fauna_amphibianreptile (40,27)</head><p>The picture shows an amphibian-like or reptile-like animal. This includes animals such as lizards, chameleons, frogs and crocodiles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="44">fauna_rodent (59,46)</head><p>The picture shows a rodent-like animal. This includes animals such as squirrels, hamsters, mice and rats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="45">quantity_none (10335,6989)</head><p>The picture shows no people.</p><p>46 quantity_one <ref type="bibr" coords="27,196.20,252.10,18.70,6.71">(3084,</ref><ref type="bibr" coords="27,214.90,252.10,15.58,6.71">1990)</ref> The picture shows one person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="47">quantity_two (682,432)</head><p>The picture shows two people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="48">quantity_three (203,127)</head><p>The picture shows three people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="49">quantity_smallgroup (313,239)</head><p>The picture shows a small group of people (4-9 persons).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="50">quantity_largegroup (383,223)</head><p>The picture shows a large group of people (10+ persons).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="51">age_baby (81,81)</head><p>The picture shows a baby (0-2 years of age).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="52">age_child (400,256)</head><p>The picture shows a child (2-10 years of age).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="53">age_teenager (313,220)</head><p>The picture shows a teenager (10-18 years of age).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="54">age_adult (3536,2306)</head><p>The picture shows an adult (18-65 years of age).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="55">age_elderly (225,127)</head><p>The picture shows an elderly person (65+ years of age) 56 gender_male <ref type="bibr" coords="27,194.05,536.01,18.70,6.71">(2484,</ref><ref type="bibr" coords="27,212.75,536.01,15.58,6.71">1660)</ref> The picture shows a male person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="57">gender_female (2619,1721)</head><p>The picture shows a female person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="58">relation_familyfriends (816,563)</head><p>The picture shows people that are likely friends or family of each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="59">relation_coworkers (239,136)</head><p>The picture shows people that are likely co-workers of each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="60">relation_strangers (335,212)</head><p>The picture shows people that likely do not know each other. The user is looking for photos that were taken at high altitude, above cloud level. These pictures can be taken from space or from airplanes for instance. The picture should clearly show that the clouds were below the photographer when he or she took the photo. If there are multiple layers of clouds present in the image the user is satisfied if at least one layer of clouds is shown below the vantage point. The photo may contain other objects such as the engine or wings of a plane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="41">bride</head><p>The user is looking for photos showing a bride. No other people should be visible in the picture, unless it is the groom.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="21,142.61,599.57,337.98,8.74;21,150.95,610.53,329.64,8.74;21,150.95,621.61,71.48,8.63" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="21,267.47,599.69,140.09,8.63">The MIR Flickr retrieval evaluation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,427.91,599.57,52.69,8.55;21,150.95,610.53,228.56,8.55">Proceedings of the 10th ACM Conference on Multimedia Information Retrieval</title>
		<meeting>the 10th ACM Conference on Multimedia Information Retrieval<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="39" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="21,142.61,634.91,337.98,8.63;21,150.95,645.75,329.64,8.74;21,150.95,656.83,175.33,8.63" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="21,310.55,634.91,170.04,8.63;21,150.95,645.87,25.45,8.63">New trends and ideas in visual concept detection</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="21,193.34,645.75,283.74,8.55">Proceedings of the 11th ACM Conference on Multimedia Information Retrieval</title>
		<meeting>the 11th ACM Conference on Multimedia Information Retrieval<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="527" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.61,119.70,337.98,8.63;22,150.95,130.54,329.64,8.74;22,150.95,141.50,98.16,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="22,264.07,119.70,216.52,8.63;22,150.95,130.66,64.44,8.63">Overview of the ImageCLEF 2012 Scalable Web Image Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,236.06,130.54,244.53,8.55;22,150.95,141.50,22.21,8.55">Working Notes of the 2012 Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.61,156.97,337.98,8.63;22,150.95,167.81,329.64,8.74;22,150.95,178.89,77.70,8.63" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="22,446.34,156.97,34.25,8.63;22,150.95,167.93,167.45,8.63">The Pascal Visual Object Classes (VOC) Challenge</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,327.07,167.81,149.88,8.55">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.61,194.24,337.98,8.63;22,150.95,205.08,245.92,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="22,337.58,194.24,138.60,8.63">Speeded-up robust features (SURF)</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,150.95,205.08,157.38,8.55">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="359" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.61,220.55,337.98,8.63;22,150.95,231.39,329.64,8.74;22,150.95,242.47,75.43,8.63" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="22,320.04,220.55,140.97,8.63">TOP-SURF: A visual words toolkit</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,150.95,231.39,255.15,8.55">Proceedings of the 18th ACM International Conference on Multimedia</title>
		<meeting>the 18th ACM International Conference on Multimedia<address><addrLine>Firenze, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1473" to="1476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.61,257.82,337.98,8.63;22,150.95,268.66,329.64,8.74;22,150.95,279.62,318.37,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="22,377.31,257.82,103.28,8.63;22,150.95,268.78,150.28,8.63">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,319.69,268.66,160.90,8.55;22,150.95,279.62,151.36,8.55">Proceedings of the 2007 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2007 IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.61,294.97,337.98,8.74;22,150.95,305.93,219.50,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="22,198.89,295.09,227.19,8.63">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,435.01,294.97,45.58,8.55;22,150.95,305.93,140.40,8.55">Springer International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.61,321.40,337.98,8.63;22,150.95,332.24,258.85,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="22,302.10,321.40,178.49,8.63;22,150.95,332.36,14.93,8.63">Performance evaluation of local color invariants</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J</forename><surname>Burghouts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Geusebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,172.85,332.24,157.38,8.55">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="62" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.24,347.71,338.35,8.63;22,150.95,358.55,329.64,8.74;22,150.95,369.51,133.17,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="22,366.73,347.71,113.86,8.63;22,150.95,358.67,125.67,8.63">Evaluating color descriptors for object and scene recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,285.43,358.55,195.16,8.55;22,150.95,369.51,40.85,8.55">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1582" to="1596" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.24,384.98,338.35,8.63;22,150.95,395.82,329.64,8.74;22,150.95,406.90,38.11,8.63" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="22,254.04,384.98,226.55,8.63;22,150.95,395.94,89.06,8.63">Modeling the shape of the scene: a holistic representation of the spatial envelope</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="22,249.27,395.82,185.29,8.55">Springer International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.24,422.25,338.35,8.63;22,150.95,433.09,329.64,8.74;22,150.95,444.17,20.17,8.63" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="22,243.57,422.25,232.98,8.63">BUAA AUDR at ImageCLEF 2012 Photo Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,161.69,433.09,264.85,8.55">Working Notes of the 2012 Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.24,459.52,338.35,8.63;22,150.95,470.36,329.64,8.74;22,150.95,481.32,238.81,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="22,379.32,459.52,101.27,8.63;22,150.95,470.48,208.68,8.63">CEA LIST&apos;s participation to the Concept Annotation Task of ImageCLEF 2012</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Znaidia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Shabou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">Le</forename><surname>Borgne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,380.59,470.36,100.00,8.55;22,150.95,481.32,162.86,8.55">Working Notes of the 2012 Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.24,496.79,338.35,8.63;22,150.95,507.63,329.64,8.74;22,150.95,518.59,272.18,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="22,443.85,496.79,36.74,8.63;22,150.95,507.75,245.66,8.63">CERTH&apos;s participation at the photo annotation task of ImageCLEF 2012</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mantziou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Petkos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kompatsiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,416.16,507.63,64.43,8.55;22,150.95,518.59,196.23,8.55">Working Notes of the 2012 Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.24,534.06,338.35,8.63;22,150.95,544.90,329.63,8.74;22,150.95,555.98,20.17,8.63" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="22,263.24,534.06,200.08,8.63">DBRIS at ImageCLEF 2012 Photo Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Rischka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Conrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,150.95,544.90,273.94,8.55">Working Notes of the 2012 Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.24,571.33,338.35,8.63;22,150.95,582.17,329.64,8.74;22,150.95,593.24,69.27,8.63" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="22,319.58,571.33,161.01,8.63;22,150.95,582.29,43.28,8.63">DMS-SZTAKI @ ImageCLEF 2012 Photo Annotation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Daróczy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Siklosi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Benczúr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,212.77,582.17,263.37,8.55">Working Notes of the 2012 Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.24,608.48,338.36,8.74;22,150.95,619.44,258.98,8.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="22,290.13,608.60,95.31,8.63">IMU @ ImageCLEF 2012</title>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,403.56,608.48,77.04,8.55;22,150.95,619.44,183.03,8.55">Working Notes of the 2012 Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.24,634.91,338.35,8.63;22,150.95,645.75,329.64,8.74;22,150.95,656.82,69.27,8.63" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="22,204.93,634.91,275.66,8.63;22,150.95,645.87,51.15,8.63">The participation of IntermidiaLab at the ImageCLEF 2012 Photo Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Manzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,218.57,645.75,257.58,8.55">Working Notes of the 2012 Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.24,119.70,338.35,8.63;23,150.95,130.66,329.64,8.63;23,150.95,141.50,329.64,8.74;23,150.95,152.46,98.16,8.74" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="23,306.70,130.66,173.89,8.63;23,150.95,141.62,70.47,8.63">ISI at ImageCLEF 2012: Scalable System for Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Muraoka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Fujisawa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Yasumoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Gunji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Higuchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kuniyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,240.51,141.50,240.08,8.55;23,150.95,152.46,22.21,8.55">Working Notes of the 2012 Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.24,166.61,338.35,8.63;23,150.95,177.45,329.64,8.74;23,150.95,188.41,280.90,8.74" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="23,456.43,166.61,24.16,8.63;23,150.95,177.57,255.64,8.63">KIDS-NUTN at ImageCLEF 2012 Photo Annotation and Retrieval Task</title>
		<author>
			<persName coords=""><forename type="first">B.-C</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G.-B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L.-J</forename><surname>Gaou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-W</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S.-E</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,425.47,177.45,55.12,8.55;23,150.95,188.41,204.95,8.55">Working Notes of the 2012 Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.24,202.55,338.35,8.63;23,150.95,213.40,329.64,8.74;23,150.95,224.35,304.06,8.74" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="23,193.77,213.51,233.30,8.63">LIRIS-Imagine at ImageCLEF 2012 Photo Annotation task</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Trus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-E</forename><surname>Bichot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bres</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Tellez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,448.90,213.40,31.70,8.55;23,150.95,224.35,228.11,8.55">Working Notes of the 2012 Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.24,238.50,338.35,8.63;23,150.95,249.46,329.64,8.63;23,150.95,260.30,329.64,8.74;23,150.95,271.26,162.16,8.74" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="23,381.91,238.50,98.68,8.63;23,150.95,249.46,329.64,8.63;23,150.95,260.42,137.10,8.63">MLKD&apos;s Participation at Imageof the 2012 Conference and Labs of the Evaluation Forum Photo Annotation and Concept-based Retrieval Tasks</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Spyromitros-Xioufis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,306.23,260.30,174.37,8.55;23,150.95,271.26,86.21,8.55">Working Notes of the 2012 Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.24,285.41,338.35,8.63;23,150.95,296.25,329.64,8.74;23,150.95,307.21,162.16,8.74" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="23,375.49,285.41,105.10,8.63;23,150.95,296.36,135.47,8.63">BossaNova at ImageCLEF 2012 Flickr Photo Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Araújo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,305.24,296.25,175.36,8.55;23,150.95,307.21,86.21,8.55">Working Notes of the 2012 Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.24,321.35,338.35,8.63;23,150.95,332.19,329.64,8.74;23,150.95,343.15,162.16,8.74" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="23,332.99,321.35,147.60,8.63;23,150.95,332.31,135.47,8.63">The PRA and AmILAB at ImageCLEF 2012 Photo Flickr Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Piras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tronci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Murgia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,305.24,332.19,175.36,8.55;23,150.95,343.15,86.21,8.55">Working Notes of the 2012 Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.24,357.30,338.35,8.63;23,150.95,368.14,329.64,8.74;23,150.95,379.10,98.16,8.74" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="23,378.47,357.30,102.12,8.63;23,150.95,368.26,75.92,8.63">Applying LDA in contextual image retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hatem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Torjmen</forename><surname>Khemakhem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">Ben</forename><surname>Jemaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,243.86,368.14,236.73,8.55;23,150.95,379.10,22.21,8.55">Working Notes of the 2012 Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.24,393.25,338.35,8.63;23,150.95,404.20,329.64,8.63;23,150.95,415.05,329.64,8.74;23,150.95,426.01,181.01,8.74" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="23,393.24,393.25,87.35,8.63;23,150.95,404.20,329.64,8.63;23,150.95,415.16,155.15,8.63">REGIMvid at Imageof the 2012 Conference and Labs of the Evaluation Forum: Concept-based Query Refinement and Relevance-based Ranking</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Rim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Ghada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ksibi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ben Ammar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">Ben</forename><surname>Amar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,324.93,415.05,155.67,8.55;23,150.95,426.01,105.06,8.55">Working Notes of the 2012 Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.24,440.15,338.35,8.63;23,150.95,450.99,329.64,8.74;23,150.95,461.95,98.16,8.74" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="23,296.05,440.15,184.54,8.63;23,150.95,451.11,64.44,8.63">UAIC participation at ImageCLEF 2012 Photo Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pitu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Grijincu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,236.06,450.99,244.53,8.55;23,150.95,461.95,22.21,8.55">Working Notes of the 2012 Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.24,476.10,338.35,8.63;23,150.95,487.06,329.64,8.63;23,150.95,498.02,329.64,8.63;23,150.95,508.86,329.64,8.74;23,150.95,519.82,140.25,8.74" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="23,150.95,487.06,329.64,8.63;23,150.95,498.02,329.64,8.63;23,150.95,508.97,115.35,8.63">Visual Concept Features and Textual Expansion in a Multimodal System for Concept Annotation and Retrieval with Flickr Photos at Imageof the 2012 Conference and Labs of the Evaluation Forum</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Benavent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Garcia-Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>De Ves Cuenca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,284.92,508.86,195.67,8.55;23,150.95,519.82,64.30,8.55">Working Notes of the 2012 Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.24,533.96,338.35,8.63;23,150.95,544.92,329.64,8.63;23,150.95,555.76,329.64,8.74;23,150.95,566.84,41.68,8.63" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="23,268.86,544.92,211.73,8.63;23,150.95,555.88,14.75,8.63">URJCyUNED at ImageCLEF 2012 Photo Annotation task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sánchez-Oro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Montalvo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">S</forename><surname>Montemayor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Cabido</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Pantrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Fresno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Martínez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,183.99,555.76,264.37,8.55">Working Notes of the 2012 Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.24,580.87,338.35,8.63;23,150.95,591.71,329.64,8.74;23,150.95,602.67,295.02,8.74" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="23,271.16,580.87,209.43,8.63;23,150.95,591.83,183.18,8.63">Computing information retrieval performance measures efficiently in the presence of tied scores</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,355.76,591.71,124.83,8.55;23,150.95,602.67,130.50,8.55">Proceedings of the 30th European Conference on Information Retrieval</title>
		<meeting>the 30th European Conference on Information Retrieval<address><addrLine>Glasgow, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="414" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.24,616.81,338.35,8.63;23,150.95,627.66,329.64,8.74;23,150.95,638.62,329.64,8.55;23,150.95,649.58,188.84,8.74" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="23,393.46,616.81,87.13,8.63;23,150.95,627.77,256.00,8.63">Quality through flow and immersion: gamifying crowdsourced relevance assessments</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,427.83,627.66,52.77,8.55;23,150.95,638.62,329.64,8.55;23,150.95,649.58,22.45,8.55">Proceedings of the 35th ACM International Conference on Research and Development in Information Retrieval</title>
		<meeting>the 35th ACM International Conference on Research and Development in Information Retrieval<address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="871" to="880" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
