<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.56,115.96,312.24,12.62;1,222.95,133.89,169.45,12.62">Baseline Multimodal Place Classifier for the 2012 Robot Vision Task</title>
				<funder ref="#_JrXtHXn">
					<orgName type="full">Spanish &quot;Junta de Comunidades de Castilla-La Mancha&quot;</orgName>
				</funder>
				<funder ref="#_VhKmax3">
					<orgName type="full">SNSF</orgName>
				</funder>
				<funder>
					<orgName type="full">European Social Fund (FEDER)</orgName>
				</funder>
				<funder ref="#_TuSPUam #_RFFVrsY #_2tYXRxn">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">Spanish Ministerio de Ciencia e Innovacion (MICINN)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,150.30,171.87,98.58,8.74"><forename type="first">Jesus</forename><surname>Martinez-Gomez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Castilla-La Mancha Albacete</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,259.45,171.87,88.46,8.74"><forename type="first">Ismael</forename><surname>Garcia-Varea</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Castilla-La Mancha Albacete</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,377.84,171.87,70.76,8.74"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Idiap Research Institute</orgName>
								<address>
									<addrLine>Centre Du Parc, Rue Marconi 19</addrLine>
									<postBox>P.O. Box 592</postBox>
									<postCode>CH-1920</postCode>
									<settlement>Martigny</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,151.56,115.96,312.24,12.62;1,222.95,133.89,169.45,12.62">Baseline Multimodal Place Classifier for the 2012 Robot Vision Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">75831130B944C83E31CD31E761462006</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The objective of this article is reporting the participation of the SIMD-IDIAP group in the RobotVision@ImageCLEF 2012 challenge. This challenge addresses the problem of multimodal place classification, and the 2012 edition has been organized by the members of the SIMD-IDIAP team. One of the main novelties in the 2012 edition of the task has been the proposal of several techniques for features extraction and cue integration. This paper details how to use all these techniques for developing a multimodal place classifier and describes the results obtained with it. Our approach ranked 7th for task 1 and 4th for task 2. The complete results for all the participants of the 2012 RobotVision task are also reported in this article.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This article describes the participation of the SIMD-IDIAP team at the fourth edition of the Robot Vision task <ref type="bibr" coords="1,278.60,472.91,9.96,8.74" target="#b6">[7]</ref>. This competition addresses the problem of multimodal place localization for mobile robots in indoor environments. Since its release in 2009, the information provided by the organizers consisted on visual images acquired with a mobile robot while moving within indoor environments. However, the 2012 edition has introduced the use of range images as well as proposes useful techniques for the development of the approaches. The SIMD-IDIAP team has developed a multimodal place classifier based on the use of Support Vector Machines (SVMs) <ref type="bibr" coords="1,328.42,556.60,14.61,8.74" target="#b12">[13]</ref>. The classifier has been trained using a combination of visual and depth features extracted from sequences of images. The selected features, as well as the classifier, are those proposed by the organizers of the task, members of the SIMD-IDIAP team. Therefore, the results achieved by the method presented in this article can be considered as baseline results that any participant is expected to improve. The rest of the paper is organized as follows: Section 2 describes the 2012 edition of the RobotVision task. Section 3 gives an overview of the SIMD-IDIAP proposal, while the feature extraction and classification techniques are described in Section 4 and Section 5 respectively. We report the results obtained in Section 6, and finally, in Section 7, conclusions are drawn and future work are outlined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The RobotVision Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Description</head><p>The fourth edition of the RobotVision challenge is focused on the problem of multi-modal place classification. Participants are asked to classify functional areas on the basis of image sequences, captured by a perspective camera and a kinect mounted on a mobile robot (see Fig. <ref type="figure" coords="2,327.05,304.39,4.43,8.74" target="#fig_0">1</ref>) within an office environment. Participants have available visual images and range images that can be used to generate 3D cloud point files. The difference between visual images, range images and 3D point cloud files can be observed in Figure <ref type="figure" coords="2,393.33,584.25,3.87,8.74">2</ref>. Training and test sequences have been acquired within the same building and floor but with some variations in the lighting conditions or the acquisition procedure (clockwise and counter clockwise).</p><p>Two different tasks are considered in the RobotVision challenge: task 1 and task 2. For both tasks, participants should be able to answer the question "'where are you?"' when presented with a test sequence imaging a room category already seen during training. The difference between both tasks is the presence (or lack) of kidnappings in the final test sequence, and the availability on the use of the temporal continuity of the sequence. The kidnappings (only task 2) affect to the room changes. Room changes in sequences without kidnappings are usually represented by a small number of images showing a smooth transition. On the other side, room changes with kidnappings are represented by a drastic change for frames, as can be observed in Figure <ref type="figure" coords="3,177.83,569.30,3.87,8.74" target="#fig_1">3</ref>.</p><p>The Data Three different sequences of frames are provided for training and two additional ones for the final experiment. All training frames are labelled with the name of the room they were acquired from. There are 9 different categories of rooms:</p><formula xml:id="formula_0" coords="3,140.99,656.09,47.88,8.77">-Corridor -Elevator Area -Printer Room -Lounge Area -Professor Office -Student Office -Visio Conference -Technical Room -Toilet</formula><p>Figure <ref type="figure" coords="4,181.78,225.56,4.98,8.74" target="#fig_2">4</ref> shows an exemplar visual image for each one of the 9 room categories.  Task 1 This task is mandatory and the test sequence has to be classified without using the temporal continuity of the sequence. Therefore, the order of the test frames cannot be taken into account. Moreover, there are not kidnappings in the final test sequence.</p><p>Task 2 This task is optional and participants can take advantage of the temporal continuity of the test sequence. There are kidnapping in the final test sequences that allow participants to obtain additional points when they are managed correctly</p><p>Performance Evaluation The proposals of the participants are compared using the score obtained by their submissions. These submissions are the classes or room categories assigned to the frames of the test sequences, and the score is calculated using the rules that are shown in the Table <ref type="table" coords="5,386.16,154.86,3.87,8.74">1</ref>. Due to wrong classifications obtain negative points, participants are allowed to not classify test frames.</p><p>Table <ref type="table" coords="5,219.65,208.73,4.13,7.89">1</ref>. Rules used to calculate the final score for a run.</p><p>Each correctly classified frame +1 points Each misclassified frame -1 points Each frame that was not classified +0 points (Task 2) All the 4 frames correctly classified after a kidnapping +1 points (additional)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Information provided by the organizers</head><p>The organizers of the 2012 RobotVision task propose the use of several techniques for features extraction and cue integration (classifier). Thanks to the use of these techniques, participants can focus on the development of new features while using the proposed method for cue integration or vice versa. The organizers also provide information as the point cloud library <ref type="bibr" coords="5,435.55,387.42,15.50,8.74" target="#b10">[11]</ref> and a basic technique for taking advantage of the temporal continuity <ref type="foot" coords="5,408.32,397.80,3.97,6.12" target="#foot_0">1</ref> . All these techniques have been used for generating the multimodal place classifier proposed in this article and are explained in Section 4 and Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overall Description</head><p>The SIMD-IDIAP proposal for the 2012 RobotVision task can be split into two steps: training and classification. The first step is performed by generating a SVM classifier <ref type="bibr" coords="5,202.55,505.02,15.50,8.74" target="#b12">[13]</ref> with a combination of features extracted from visual and depth images. We opted for the visual and depth features proposed by the task organizers and a SVM classifier. Both features are then concatenated for generating a single feature. The complete training process is shown in Figure <ref type="figure" coords="5,450.78,540.88,3.87,8.74" target="#fig_3">5</ref>.</p><p>The second step corresponds with the classification of test frames. Before classifying a test frame, it is necessary to extract the same features extracted in the training step. After that, the features are classified using the SVM previously generated, which obtains a decision margin for each class. All these margins are then processed to decide whether to classify the frame or not, depending on the level of confidence of the decision. Low confidence frames will not be classified in order to avoid obtaining negative points. The complete process for classification and post-processing will be explained in Section 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Feature Extraction</head><p>As features to extract from the sequences of frames, we have chosen the Pyramid Histogram of Orientated Gradients (PHOG) <ref type="bibr" coords="6,330.39,390.69,10.52,8.74" target="#b1">[2]</ref> and the Normal Aligned Radial Feature (NARF). These descriptors can be extracted from visual images and 3D point cloud files respectively, and they are the features proposed by the task organizers. PHOG features are histogram-based global features that combine structural and statistical approaches. Other descriptors similar to PHOG that could also be used are: Sift-based Pyramid Histogram Of visual Words (PHOW) <ref type="bibr" coords="6,426.75,462.42,9.96,8.74" target="#b0">[1]</ref>, Pyramid histogram of Local Binary Patterns (PLBP) <ref type="bibr" coords="6,337.11,474.38,9.96,8.74" target="#b7">[8]</ref>, Self-Similarity-based PHOW (SS-PHOW) <ref type="bibr" coords="6,192.18,486.33,14.61,8.74" target="#b11">[12]</ref>, and Compose Receptive Field Histogram (CRFH) <ref type="bibr" coords="6,435.03,486.33,9.96,8.74" target="#b4">[5]</ref>. NARF features is a novel descriptor technique that has been included in the point cloud library <ref type="bibr" coords="6,218.54,510.24,14.61,8.74" target="#b10">[11]</ref>. The number of descriptors that can be extracted from a range image is not fixed, in the same manner as SIFT points. In order to extract descriptors with the same length, we have computed a new feature from the NARF points extracted from a 3D point cloud file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visual Features -PHOG</head><p>The Pyramid Histogram of Orientated Gradients (PHOG) is inspired by the pyramid representation presented in <ref type="bibr" coords="6,297.61,608.30,10.52,8.74" target="#b3">[4]</ref> and the Histogram of Gradient Orientation (HOG) described in <ref type="bibr" coords="6,256.28,620.25,9.96,8.74" target="#b2">[3]</ref>. This descriptor consists then of a histogram of orientation gradients over each image sub-region at each level. It represents local image shape and its spatial layout, both together with a spatial pyramid kernel. Two parameters should be fixed when extracting a PHOG descriptor from a visual image: the number of levels of the pyramid and the number of bins of the histogram. Each bin represents the number of edges with orientations within a certain angular range. In our approach, we opted for 20 bins, 3 levels (0, 1 and 2) and the range of orientations [0,360]. Using these parameters, for each visual image we obtain a 420 bytes descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Depth Features -NARF</head><p>The process to generate depth features from range images consists of three steps: a) convert the range image into a 3D point cloud file, b) extract NARF features from keypoints, and c) compute a descriptor pyramidly from the NARF features. The first step has been done by using a pyhton script provided by the task organizers. This script has been used due to the specific format of the RobotVision@ImageCLEF 2012 range images, but the step can be skipped when using the PCL software to register point cloud files directly from the kinect device. The second step extracts NARF features from the keypoints detected in a point cloud file. The keypoints are detected by using the neighbour cloud points and taking into account the borders. For each keypoint, a 36 bytes NARF feature is extracted, and we also store the ¡x,y,z¿ position of the 3D point. For the third step, we compute pyramidly a descriptor from the data generated in the previous step. This is done by following the pyramid representation presented in <ref type="bibr" coords="7,178.19,386.05,9.96,8.74" target="#b3">[4]</ref>. In a similar way as for the PHOG descriptor, we have to fix the number of bins of the histogram and the number of levels. We selected 100 bins and 2 levels (0 and 1), obtaining a 500 bytes descriptor. Both PHOG and NARF-based descriptors are directly concatenated to obtain a 920 bytes descriptor by frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Classification and post-processing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Classifier</head><p>The algorithm that was proposed by the organizers for cue integration was the Online-Batch Strongly Convex mUlti keRnel lEarning (OBSCURE) <ref type="bibr" coords="7,431.10,524.61,14.61,8.74" target="#b9">[10]</ref>. In this work, we have used a classifier similar to OBSCURE based on the use of Online Independent Support Vector Machines (OI-SVM) <ref type="bibr" coords="7,358.60,548.52,9.96,8.74" target="#b8">[9]</ref>. This classifier is named Memory Controlled OI-SVM <ref type="bibr" coords="7,264.73,560.48,10.52,8.74" target="#b5">[6]</ref> and allows to keep under control the memory growth while the algorithm learns incrementally. This is done by applying a forgetting strategy over the stored Training Samples (TSs), while preserving the stored Support Vectors and it approximates reasonably well the original optimal solution. The Memory Controlled OI-SVM is trained using the combination of visual and depth features described in the previous section. When a test frame is classified, the classifier obtains a decision margin for each class, and this output has to be processed to obtain the most feasible class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Low confidence detection</head><p>In order to detect low confidence classifications, we process the obtained outputs in the following way: (i) we normalize the margins by dividing all values by the maximum margin, and (ii) we test whether the normalized outputs pass two conditions. On the basis of the output margins M i n C i=1 , with C= number of classes, for each frame n, the two conditions used to detect challenging frames are:</p><formula xml:id="formula_1" coords="8,138.97,223.95,341.62,40.79">1. M i n &lt; M max C i=1 : for each of the possible classes C none obtains a high level of confidence; 2. |M i n -M j n | &lt; ∆ C i=1</formula><p>: there are at least two classes with high level of confidence, but the difference between them is too small to allow for a confident decision.</p><p>If a frame has been detected as challenging, the algorithm will not assign them any room category and it will not be classified. In all the experiments, we have used M max = 0.2 and ∆ = 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Temporal Continuity</head><p>For the task 2, the temporal continuity of the sequence can be taken into account. In order to take advantage of this situation, we used the solution proposed by the task organizers: detect prior classes and use them to classify challenging frames. Once a frame has been identified as challenging (and not classified), we use the classification results obtained for the last n frames to solve the ambiguity: if all the last n frames have been assigned to the same class C i , then we can conclude that all frames come from the same class C i , we consider it a prior class, and the label will be assigned accordingly. We have used n = 5 for all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>We only submitted two runs: one for the task 1 and one for the task 2. The process for generating both runs includes all the techniques that have been explained above. We extracted a PHOG and NARF-based feature descriptor for each one of the training and test frames. Both descriptors were concatenated to generate a 920 bytes descriptor and then, we trained a Memory Controlled OI-SVM using the training1, training2, and training3 sequences. Finally, we classified the test sequences (test 1 for task 1 and test 2 for task 2) using the MC-OI-SVM. For both tasks, we post-processed the output generated with the classifier to detect challenging frames. These challenging frames were not labelled. For task 2, we used the temporal continuity of the sequence for classifying the challenging fames when possible (a prior class using the last 5 frames has been detected).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Task 1</head><p>Eight different groups submitted runs for the task 1 and all the results can be observed in Table <ref type="table" coords="9,215.39,148.55,3.87,8.74" target="#tab_2">2</ref>. The maximum score that could be achieved was 2445 and the best group (CII UTN FRC) obtained 2071 points. Our proposal ranked 7th with 462 points and most of the teams, as expected, achieved better than us. The submitted run classified only 1526 frames (37.5% of the test frames were detected as challenging), with 994 hits and 532 fails. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Task 2</head><p>For the optional task, the maximum score was 4079 and only 4 groups submitted runs. Our submission ranked 4th with 1041 points and the winner for the task 2 was the CIII UTN FRC group with 3930 points. All the results can be seen in Table <ref type="table" coords="9,161.96,439.20,3.87,8.74" target="#tab_3">3</ref>. In this task, our algorithm discarded 1205 challenging frames but 97 of them were classified using a prior class detected with the temporal continuity.</p><p>Concretely, the final submission consisted of 2915 frames classified and 1108 (27.54%) frames that were not labelled. 7 Conclusions and Future Work</p><p>This article describes the participation of the SIMD-IDIAP group to the RobotVision task at ImageCLEF 2012. We developed an approach using the techniques for feature extraction, cue integration, and temporal continuity proposed by the task organizers. We submitted runs for task 1 (mandatory) and task 2 (optional) using the information extracted from visual and depth images. Due to such runs were generated using the techniques proposed by the organizers, these results can be considered as baseline results.</p><p>Our best runs in the two tracks ranked respectively seventh (task 1) and fourth (task 2), showing that most of the teams ranked better than the baseline results.</p><p>For future work, we have in mind to improve the NARF-based descriptor introduced in this article. We also have plans to evaluate the use of larger descriptors, which can be done by using a higher number of levels for the pyramid and bins for the histogram.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,194.71,526.83,225.94,7.89;2,276.56,336.64,62.25,175.42"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Mobile robot platform used for data acquisition.</figDesc><graphic coords="2,276.56,336.64,62.25,175.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,244.14,487.87,127.08,7.89;3,143.41,307.47,328.53,165.63"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Example of kidnapping.</figDesc><graphic coords="3,143.41,307.47,328.53,165.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,175.61,501.82,264.14,7.89;4,183.18,427.85,79.53,59.65"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Examples of images from the RobotVision 2012 database.</figDesc><graphic coords="4,183.18,427.85,79.53,59.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,160.98,320.80,293.39,7.89;6,143.41,115.83,328.54,190.20"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Overall overview of the training step for generating the classifier.</figDesc><graphic coords="6,143.41,115.83,328.54,190.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,166.28,227.26,282.79,116.73"><head>Table 2 .</head><label>2</label><figDesc>Ranking of the runs submitted by the groups for the task 1.</figDesc><table coords="9,208.95,248.06,194.40,95.93"><row><cell>Rank</cell><cell>Group Name</cell><cell>Best Score</cell></row><row><cell>1</cell><cell>CIII UTN FRC</cell><cell>2071</cell></row><row><cell>2</cell><cell>NUDT</cell><cell>1817</cell></row><row><cell>3</cell><cell>UAIC2012</cell><cell>1348</cell></row><row><cell>4</cell><cell>USUroom409</cell><cell>1225</cell></row><row><cell>5</cell><cell>SKB Kontur Labs</cell><cell>1028</cell></row><row><cell>6</cell><cell>CBIRITU</cell><cell>551</cell></row><row><cell cols="2">7 SIMD-IDIAP -baseline results</cell><cell>462</cell></row><row><cell>8</cell><cell>BuffaloVision</cell><cell>-70</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,166.28,505.95,282.79,72.90"><head>Table 3 .</head><label>3</label><figDesc>Ranking of the runs submitted by the groups for the task 2.</figDesc><table coords="9,208.95,526.75,194.40,52.10"><row><cell>Rank</cell><cell>Group Name</cell><cell>Best Score</cell></row><row><cell>1</cell><cell>CIII UTN FRC</cell><cell>3930</cell></row><row><cell>2</cell><cell>NUDT</cell><cell>3925</cell></row><row><cell>3</cell><cell>CBIRITU</cell><cell>3169</cell></row><row><cell cols="2">4 SIMD-IDIAP -baseline results</cell><cell>1041</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,144.73,656.80,130.86,7.86"><p>http://imageclef.org/2012/robot</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This work was supported by the <rs type="funder">SNSF</rs> project <rs type="projectName">MULTI</rs> (B. C.), and by the <rs type="funder">European Social Fund (FEDER)</rs>, the <rs type="funder">Spanish Ministerio de Ciencia e Innovacion (MICINN)</rs>, and the <rs type="funder">Spanish "Junta de Comunidades de Castilla-La Mancha"</rs> (<rs type="projectName">MIPRCV Consolider Ingenio</rs> <rs type="grantNumber">2010 CSD2007-00018</rs>, <rs type="grantNumber">TIN2010-20900-C04-03</rs>, <rs type="grantNumber">PBI08-0210-7127</rs> and <rs type="grantNumber">PPII11-0309-6935</rs> projects, J. M.-G. and I. G.-V.)</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_VhKmax3">
					<orgName type="project" subtype="full">MULTI</orgName>
				</org>
				<org type="funded-project" xml:id="_JrXtHXn">
					<idno type="grant-number">2010 CSD2007-00018</idno>
					<orgName type="project" subtype="full">MIPRCV Consolider Ingenio</orgName>
				</org>
				<org type="funding" xml:id="_TuSPUam">
					<idno type="grant-number">TIN2010-20900-C04-03</idno>
				</org>
				<org type="funding" xml:id="_RFFVrsY">
					<idno type="grant-number">PBI08-0210-7127</idno>
				</org>
				<org type="funding" xml:id="_2tYXRxn">
					<idno type="grant-number">PPII11-0309-6935</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,298.18,337.64,7.86;10,151.52,309.14,329.07,7.86;10,151.52,320.10,20.99,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,315.86,298.18,164.74,7.86;10,151.52,309.14,36.36,7.86">Image classification using random forests and ferns</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Munoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,208.09,309.14,185.11,7.86">International Conference on Computer Vision</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,330.19,337.64,7.86;10,151.52,341.15,329.07,7.86;10,151.52,352.11,127.90,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,311.26,330.19,169.34,7.86;10,151.52,341.15,22.84,7.86">Representing shape with a spatial pyramid kernel</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Munoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,192.05,341.15,288.54,7.86;10,151.52,352.11,31.94,7.86">Proceedings of the 6th ACM international conference on Image and video retrieval</title>
		<meeting>the 6th ACM international conference on Image and video retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page">408</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,362.20,337.63,7.86;10,151.52,373.16,329.07,7.86;10,151.52,384.12,241.93,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,256.65,362.20,220.00,7.86">Histograms of oriented gradients for human detection</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,162.69,373.16,167.37,7.86;10,361.72,373.16,118.87,7.86;10,151.52,384.12,87.45,7.86">CVPR 2005. IEEE Computer Society Conference on</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct coords="10,142.96,394.21,337.63,7.86;10,151.52,405.17,329.07,7.86;10,151.52,416.13,299.29,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,312.81,394.21,167.78,7.86;10,151.52,405.17,194.05,7.86">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,384.97,405.17,95.63,7.86;10,151.52,416.13,229.20,7.86">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,426.22,337.64,7.86;10,151.52,437.18,273.89,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,274.81,426.22,205.78,7.86;10,151.52,437.18,142.30,7.86">Object recognition using composed receptive field histograms of higher dimensionality</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Linde</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,313.49,437.18,83.98,7.86">Proc. ICPR. Citeseer</title>
		<meeting>ICPR. Citeseer</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,447.27,337.64,7.86;10,151.52,458.23,295.18,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,267.21,447.27,213.38,7.86;10,151.52,458.23,105.20,7.86">Towards semi-supervised learning of semantic spatial concepts for mobile robots</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,264.81,458.23,106.02,7.86">Journal of Physical Agents</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="19" to="31" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,468.32,337.64,7.86;10,151.52,479.28,295.73,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,431.78,468.32,48.82,7.86;10,151.52,479.28,144.24,7.86">Overview of the imageclef 2012 robot vision task</title>
		<author>
			<persName coords=""><forename type="first">Jesus</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ismael</forename><surname>Garcia-Varea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,315.22,479.28,104.06,7.86">CLEF 2012 working notes</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,489.37,337.64,7.86;10,151.52,500.33,329.07,7.86;10,151.52,511.29,84.02,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,341.38,489.37,139.21,7.86;10,151.52,500.33,193.68,7.86">Gray scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mäenpää</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,355.77,500.33,99.38,7.86">Computer Vision-ECCV</title>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
			<biblScope unit="page" from="404" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,521.38,337.64,7.86;10,151.52,532.34,329.07,7.86;10,151.52,543.30,20.99,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,397.14,521.38,83.45,7.86;10,151.52,532.34,215.61,7.86">Indoor place recognition using online independent support vector machines</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Castellini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sandini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,385.57,532.34,48.09,7.86">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,553.39,337.98,7.86;10,151.52,564.35,317.69,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,302.91,553.39,177.68,7.86;10,151.52,564.35,33.81,7.86">Online-Batch Strongly Convex Multi Kernel Learning</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,205.16,564.35,203.69,7.86">Proc. of Computer Vision and Pattern Recognition</title>
		<meeting>of Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,574.44,337.98,7.86;10,151.52,585.40,329.07,7.86;10,151.52,596.36,20.99,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,265.26,574.44,143.62,7.86">3d is here: Point cloud library (pcl)</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cousins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,429.17,574.44,51.42,7.86;10,151.52,585.40,250.75,7.86">Robotics and Automation (ICRA), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,606.45,337.97,7.86;10,151.52,617.41,329.07,7.86;10,151.52,628.37,109.84,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,276.68,606.45,203.91,7.86;10,151.52,617.41,23.52,7.86">Matching local self-similarities across images and videos</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,195.28,617.41,256.15,7.86;10,151.52,628.37,36.03,7.86">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR&apos;07</note>
</biblStruct>

<biblStruct coords="10,142.61,638.46,337.97,7.86;10,151.52,649.42,39.16,7.86" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m" coord="10,208.65,638.46,158.35,7.86">The nature of statistical learning theory</title>
		<imprint>
			<publisher>Springer-Verlag New York Inc</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
