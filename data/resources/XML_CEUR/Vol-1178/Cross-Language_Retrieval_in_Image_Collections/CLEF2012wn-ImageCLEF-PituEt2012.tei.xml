<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,173.76,142.44,263.75,11.96;1,258.24,158.88,94.92,11.96">UAIC participation at ImageCLEF 2012 Photo Annotation Task</title>
				<funder ref="#_UtQ7d2y">
					<orgName type="full">Sector Operational Program for Human Resources Development</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,217.56,195.04,38.68,8.48"><forename type="first">Mihai</forename><surname>Pîțu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,262.08,195.04,62.99,8.48"><forename type="first">Daniela</forename><surname>Grijincu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,343.20,195.04,50.47,8.48"><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,173.76,142.44,263.75,11.96;1,258.24,158.88,94.92,11.96">UAIC participation at ImageCLEF 2012 Photo Annotation Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">32EF811D87B48425111EB72777F40274</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ImageCLEF</term>
					<term>Image classification</term>
					<term>Photo annotation</term>
					<term>SVMs</term>
					<term>TopSurf</term>
					<term>Bag-of-Words model</term>
					<term>kernel methods</term>
					<term>PEF</term>
					<term>Color moments</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the participation of our group in the ImageCLEF 2012 Photo Annotation Task. Our approach is based on visual and textual features as we experiment with different strategies in order to extract the semantics inside an image. First, we construct a textual dictionary of tags using the most frequent words present in the user tag annotated images from the training data sets. A linear kernel is then developed based on this dictionary. To gather more information from the images we further extract local and global visual features using TopSurf and Profile Entropy Features as well as Color Moments technique. We then aggregate these features with Support Vector Machines classification algorithm and train separate SVM models for each concept. In the end, to improve our system's performance, we add a postprocessing step that verifies the consistency of the predicted concepts and also applies a face detection algorithm in order to increase the recognition accuracy of the person related concepts. Our submission consists of one visual-only and four multi-modal runs. We further give a more detailed perspective of our system and discuss our results and conclusions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>ImageCLEF 2012 <ref type="foot" coords="1,212.88,483.13,3.04,5.52" target="#foot_0">1</ref> Photo Annotation Task<ref type="foot" coords="1,311.88,483.13,3.04,5.52" target="#foot_1">2</ref> represents a competition that aims to improve the state of the art of the Computer Vision field by addressing the problem of automated image annotation <ref type="bibr" coords="1,252.72,506.68,10.03,8.48" target="#b0">[1]</ref>. The participants are asked to create systems that can automatically assign an image a subset of concepts from a list of 94 possible visual concepts.</p><p>In 2012, the organizers offered a database consisting of 15,000 training images annotated with the corresponding 94 binary labels and a set of 10,000 test images which were to be automatically annotated (see Figure <ref type="figure" coords="1,352.20,560.68,3.34,8.48" target="#fig_0">1</ref>). The images were extracted from Flickr 3 online photo sharing application and so each image had the associated EXIF data and Flickr user tags. Among the reasons that make this task of image annotation a difficult one are the diversity of the concepts simultaneously present in an image, the subjectivity of the existing annotations in the training set (especially regarding the feelings related concepts) and the fact that the training samples are unbalanced and so there may be more examples for a concept then for another. The system we propose combines different state of the art image processing techniques (TopSurf, PEF, Color Moments) with Support Vector Machines and Kernel functions we defined in an attempt to obtain good overall performances. This was our second participation in Photo Annotation task, after our contribution from 2009 <ref type="bibr" coords="2,164.16,411.16,9.94,8.48" target="#b1">[2]</ref>.</p><p>The rest of the article is organized as follows: Section 2 presents the visual and textual features we extracted to describe the images, Section 3 covers the classification and post processing modules of our system, Section 4 details our submitted runs and Section 5 outlines our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Visual and Textual Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Local Visual Features -TopSurf</head><p>TopSurf<ref type="foot" coords="2,174.36,542.41,3.04,5.52" target="#foot_3">4</ref>  <ref type="bibr" coords="2,180.36,544.48,10.91,8.48" target="#b2">[3]</ref> is a visual library that combines SURF interest points <ref type="bibr" coords="2,402.60,544.48,10.14,8.48" target="#b3">[4,</ref><ref type="bibr" coords="2,415.68,544.48,7.67,8.48" target="#b4">5]</ref> with visual words based on a large pre-computed codebook <ref type="bibr" coords="2,327.84,555.28,10.14,8.48" target="#b5">[6,</ref><ref type="bibr" coords="2,340.92,555.28,7.67,8.48" target="#b6">7]</ref> and returns the most important visual information in the image (based on assigned Tf-Idf scores <ref type="foot" coords="2,400.80,564.13,3.04,5.52" target="#foot_4">5</ref> ). SURF interest points and the associated descriptors provide (partial) invariance to affine transformations of objects in images, but the number of interest points may vary between 0 and a few thousands, depending of the size and details of a photo. Because to every SURF interest point corresponds a descriptor (a 64 dimensional array), the problem of matching such descriptors arises. As matching thousands of descriptors of a given image against a large database is highly time consuming and practical infeasible, TopSurf library assigns every SURF descriptor a visual word from the precomputed codebook and associates a limited number (the most important) of such visual words to the image. The time of the extraction process slightly increases (experiments <ref type="bibr" coords="3,194.64,194.92,11.03,8.48" target="#b2">[3]</ref> shows that for SURF interest point extraction is required on average 0.37s and 0.07s for the assignment of the visual words), but matching TopSurf descriptors improves the time complexity and quality of the overall process.</p><p>The TopSurf library assigns Tf-Idf scores <ref type="bibr" coords="3,312.72,227.44,10.91,8.48" target="#b7">[8]</ref> to every visual word in the image and returns the most important ones. In our system we use the cosine similarity to measure the distance (angle) between two given images described by their corresponding TopSurf descriptor:</p><formula xml:id="formula_0" coords="3,163.44,263.67,274.50,37.23">‫,1݀‪ሺ‬ݏܥ݉݅ݏ‬ ݀2ሻ = cosሺ߮ሻ = ݀1 * ݀2 ห|݀1|ห ห|݀2|ห = ∑ ݀1 ݀2 ே ୀଵ ඥ∑ ሺ݀1ሻ ଶ ே ୀଵ ඥ∑ ሺ݀2ሻ ଶ ே ୀଵ</formula><p>The similarity score will be between 0 and 1 (because the angle of the vectors d1 and d2 is smaller than 90 degrees), with 1 for identical descriptors and 0 for absolutely different ones. The time needed to compare these descriptors is, on average, 0.2 ms (with a database of 100,000 images).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Profile Entropy Features</head><p>Profile Entropy Features (PEF) <ref type="bibr" coords="3,265.32,391.48,10.91,8.48" target="#b8">[9]</ref> is a technique of extracting global visual features which combines the texture characteristics with the shapes present in a given image by computing the simple arithmetic mean in horizontal or vertical direction.</p><p>The PEF features are computed on an image I by using the normalized RGB channels: ‫ݎ‬ = </p><formula xml:id="formula_1" coords="3,178.92,551.87,77.35,81.31">‫ܨܧܲ‬ ሺ‫ܫ‬ሻ = ுሺФ ሺூሻሻ ୪୭ ே ‫ܨܧܲ‬ ሺ‫ܫ‬ሻ = ுሺФ ሺூሻሻ ୪୭ ே ‫ܨܧܲ‬ ሺ‫ܫ‬ሻ = ுሺௗሺூሻሻ ୪୭ ே</formula><p>The algorithm repeats for each of the 3 equal horizontal sub-images (see Figure <ref type="figure" coords="4,460.80,140.92,3.84,8.48" target="#fig_2">2</ref>) and on the whole image. The PEF descriptor is denoted by the concatenation of ‫ܨܧܲ‬ , ‫ܨܧܲ‬ , ‫ܨܧܲ‬ the mean and variance of the 3 channels, thus we have 4 regions × 5 features × 3 channels = 60 dimensions that describe the image I. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Color moments</head><p>Color moments represent a method that can be used to differentiate images based on their features of color. The main idea behind color moments is the assumption that the distribution of color can be interpreted as a probability distribution, which can be characterized by a number of moments (mean, variance, etc.). Stricker and Orengo <ref type="bibr" coords="4,143.04,444.40,15.47,8.48" target="#b10">[11]</ref> used three central moments of an image's color distribution: mean, standard deviation and skewness. The same authors showed that traditional methods like color histograms are sensitive to minor modifications in illumination or affine transformations.</p><p>A color can be abstractly represented by using color models like RGB (Red, Green, and Blue) or HSV (Hue, Saturation and Value). Thus, each of the three dimensions of the chosen color model is characterized by three moments of a color distribution, resulting in a nine dimension vector which will describe the color distribution in a given image.</p><formula xml:id="formula_2" coords="4,282.72,543.11,54.65,24.91">‫ܧ‬ = ଵ ே ∑ ‫‬ ே ୀଵ</formula><p>, ‫ܧ‬ is the mean or the average color value in the image, ‫‬ is the value of the j th pixel in the i th dimension of the color model and N is the number of pixels in the image.</p><formula xml:id="formula_3" coords="4,256.08,612.11,96.30,24.91">ߪ = ට ଵ ே ∑ ሺ‫‬ -‫ܧ‬ ሻ ଶ ே ୀଵ</formula><p>, ߪ is the standard deviation (the square root of the variance) of the distribution.</p><formula xml:id="formula_4" coords="5,255.96,157.91,99.42,24.79">‫ݏ‬ = ට ଵ ே ∑ ሺ‫‬ -‫ܧ‬ ሻ ଷ ே ୀଵ య ,</formula><p>‫ݏ‬ is the skewness of the distribution which is a measure of the degree of its asymmetry.</p><p>The similarity function ݀ can be used to adjust the weights ‫ݓ(‬ ) of each channel, because it makes sense that, for example, the hue of a color is more important than its intensity. The function is defined as the sum of the weighted differences between the moments of the two distributions:</p><formula xml:id="formula_5" coords="5,179.28,256.00,252.88,21.23">݀ ሺ‫ܫ‬ ଵ , ‫ܫ‬ ଶ ሻ ൌ ∑ ሺ‫ݓ‬ ‫ܧ|‬ ଵ െ ‫ܧ‬ ଶ | ‫ݓ‬ |ߪ ଵ െ ߪ ଶ | ‫ݓ‬ ‫ݏ|‬ ଵ െ ‫ݏ‬ ଶ |ሻ ଷ ୀଵ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Using Flickr user tags</head><p>In some situations, the visual information is not enough to give a semantic interpretation of an image and this is why we exploit user defined tags to improve the judgment of the whole system. The problems that arise with these approaches are the fact the number of user defined tags is relatively small (or 0), the tag can be in any language, some of them are irrelevant or they are a concatenation of words (see Figure <ref type="figure" coords="5,143.04,370.24,3.38,8.48" target="#fig_3">3</ref>). These problems make the traditional methods used in the field of natural language processing inapplicable in this situation. The authors in <ref type="bibr" coords="5,212.52,555.64,15.59,8.48" target="#b11">[12]</ref> propose a linear SVM kernel that uses the most frequent user tags from the training set, which proved to be a good method. The idea is to construct a dictionary with user tags that appear at least k times (in our system we used k = 16) in the associated images from the training set. This process eliminates irrelevant and rare user tags and limits the dictionary to a number of n tags. Prior to the construction of the dictionary, we used Bing Translator 6 on every associated user tag, in order to attempt translation in English and a stemming algorithm that will reduce inflected or derived words to their root. After the dictionary is computed, an n-dimensional binary vector will be assigned to each image, with the i th component 1 if the image is annotated with the ith user tag from the dictionary and 0, otherwise. The linear SVM kernel that classifies these vectors is:</p><formula xml:id="formula_6" coords="6,156.60,185.55,188.61,40.92">‫ܭ‬ ீ ൫‫ݐ‬ , ‫ݐ‬ ൯ = ‫ݐ‬ ் ‫ݐ‬ ‫ݐ‬ ் ‫ݐ‬</formula><p>is the dot product between the transposed binary vector ‫ݐ‬ and the ‫ݐ‬ vector.</p><p>The KG kernel counts the number of shared user tags between two associated images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Classification using SVMs</head><p>Support vector machines <ref type="bibr" coords="6,239.28,314.32,14.82,8.48" target="#b12">[13,</ref><ref type="bibr" coords="6,256.68,314.32,12.35,8.48" target="#b13">14]</ref> proved to be one of the best classification technique used to address image classification problems as it can be very flexible and work with large amounts of data. Because this task requires multi-label classification (an image can be annotated with more than one concept), we choose to train an SVM classifier for each of the 94 concepts proposed by the ImageCLEF organizers <ref type="bibr" coords="6,410.64,357.64,15.71,8.48" target="#b14">[15]</ref> (to train a classifier for a concept c, we choose as positive examples the images that are annotated with the c concept and as negative examples the rest of the training images). Also, because of the highly unbalanced classification problem (the positive examples are usually less than the negative examples), we implemented a sampling method <ref type="bibr" coords="6,173.52,411.76,14.35,8.48" target="#b15">[16]</ref>.</p><p>We propose a combined SVM kernel that makes use of all the features described above:</p><p>‫ܭ‬ ௗ ሺ‫,ݔ‬ ‫ݕ‬ሻ = ܿ ௧௦ ‫ܭ‬ ௧௦ ሺ‫,ݔ‬ ‫ݕ‬ሻ + ܿ ‫ܭ‬ ሺ‫,ݔ‬ ‫ݕ‬ሻ + ܿ ௨௧ ‫ܭ‬ ௨௧ ሺ‫,ݔ‬ ‫ݕ‬ሻ + ܿ ‫ܭ‬ ሺ‫,ݔ‬ ‫ݕ‬ሻ</p><p>Where ܿ ௧௦ , ܿ , ܿ ௨௧ , ܿ ∈ [0, 1], (such that ܿ ௧௦ + ܿ + ܿ ௨௧ + ܿ = 1) are weights for the following kernel functions:</p><p>• ‫ܭ‬ ௧௦ ሺ‫,ݔ‬ ‫ݕ‬ሻ = ‫݀‪ሺ‬ݏܥ݉݅ݏ‬ ௧௦ ሺ‫ݔ‬ሻ, ݀ ௧௦ ሺ‫ݕ‬ሻሻ is the cosine similarity defined in section 2.1 for the TopSurf library; • ‫ܭ‬ ሺ‫,ݔ‬ ‫ݕ‬ሻ = exp ሺ-ߛ||‫ݔ‬ -‫||ݕ‬ ଶ ) is the RBF kernel and it is used with PEF descriptors (section 2.2);</p><formula xml:id="formula_7" coords="6,159.96,540.76,96.30,13.54">• ‫ܭ‬ ௨௧ ‫,ݔ(‬ ‫)ݕ‬ = ௧(௫) ௧(௬)</formula><p>is the linear kernel defined in section 2.4 normalized by the number of tags in the dictionary; • ‫ܭ‬ ‫,ݔ(‬ ‫)ݕ‬ = exp (-ߛ ݀ ‫,ݔ(‬ ‫))ݕ‬ is the kernel that uses ݀ function for color moments (section 2.3) and ߛ is the regularization parameter.</p><p>These functions and K ୡ୭୫ୠ୧୬ୣୢ kernel satisfy Mercer's theorem <ref type="bibr" coords="6,402.48,600.88,15.47,8.48" target="#b12">[13]</ref> necessary to ensure SVMs convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Post processing</head><p>In the post processing module of our system we ensure that the classifications made by SVMs models are correct. For example, if an image is classified with quality_noblur and quality_partialblur at the same time, we adjust the concept's probabilities so they sum up to 1. We learn about mutual exclusive concepts ‫)ݔܧܥ(‬ from the training set. Let ܵ be the set of predictions made by SVMs, with ܿ ଵ , ܿ ଶ ∈ ‫ݔܧܥ‬ and ܿ ଵ , ܿ ଶ ∈ ܵ :</p><formula xml:id="formula_8" coords="7,215.16,238.51,181.22,10.39">ܵ = ܵ \ ሼܿ ଵ ∶ (ܿ ଵ , ܿ ଶ ) ∈ ‫,ݔܧܥ‬ ‫ܿ(‬ ଵ ) ൏ ‫ܿ(‬ ଶ )ሽ</formula><p>We also compute the Voila -Jones face detection algorithm <ref type="bibr" coords="7,384.72,262.96,14.35,8.48" target="#b16">[17]</ref>, in order to count the number of persons in a given image (the concepts regarding the number of persons in this year's competition are: quantity_none, quantity_one, quantity_two, quantity_three, quantity_smallgroup, quantity_largegroup) and to determine if view_portrait concept is present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submitted runs and results</head><p>Our system (Figure <ref type="figure" coords="7,218.76,367.60,3.90,8.48" target="#fig_4">4</ref>) has a modular and flexible structure and can easily be extended with some other feature extractors' algorithms:  Our best run was the one with Submission1 configuration and it was ranked 11 th of a total of 18 group participants <ref type="bibr" coords="8,261.12,457.84,9.94,8.48" target="#b0">[1]</ref>. The fact that our visual-only run achieved the best of our scores shows that local invariant visual features are more appropriate for this task than other type of features. Also, we noticed that using user tags for classifying some of the concepts is, in fact, misleading. For example, for concept weather_cloudysky the most frequent tags were: blue, Cannon, Nikon, clouds. All participants at ImageCLEF 2012 in Photo Annotation task have submitted several runs using not only visual strategies based on features extracted from the images but as well textual ones based on user defined tags that were given alongside the images. The best results however, as it can also be observed from the table above, were achieved by the systems that managed to combine both the visual and textual features together. What our system lacked was the fact that we did not find the best balance between feature extraction algorithms (with their contribution in the learning step) and also the fact that some of them should weight more or less depending on the concept that is being learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper we combined several different state of the art algorithms for image processing together with Support Vector Machines and kernel functions in order to approach the task of automated image annotation. As images can be annotated with more than one concept we tried to increase our system's performance by using not only local image feature descriptors (TopSurf), that for example, proved to be unpractical at detecting feelings in an image, but also try analyzing the colors (Color Moments) and the textures (Profile Entropy Features) in the image and even make use of the user defined tag semantics and face detection algorithms.</p><p>All experiments were made using the approach we presented in this paper and careful attention was given to the selection of the threshold parameters of the SVM kernel function that we used, ‫ܭ‬ ௗ , ܿ ௧௦ , ܿ , ܿ ௨௧ and ܿ .</p><p>As future work, we will try and set different values for these parameters taking into consideration the concept that the classifier is training for. For example, for concepts that express feelings, Color Moments technique should have the deciding weight, whereas for panoramic images a greater weight should be given to the texture descriptor (PEF).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,230.64,345.89,149.91,8.59"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of train/test images</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,201.72,428.75,4.29,14.59;3,206.16,437.20,2.34,8.48;3,214.80,430.23,15.38,20.88;3,232.92,428.75,4.27,14.59;3,237.48,437.20,2.34,8.48;3,246.12,430.23,58.02,20.88;3,310.56,437.20,22.87,8.48;3,339.72,430.23,12.86,20.88;3,355.20,428.75,23.27,14.59;3,364.92,439.07,3.78,14.59;3,378.48,437.20,89.83,8.48;3,143.04,454.00,263.27,8.48;3,409.56,447.03,6.69,20.88;3,416.52,440.91,10.38,20.88;3,418.56,452.19,6.03,20.88;3,430.08,454.00,38.35,8.48;3,143.04,471.04,70.05,8.48;3,213.24,457.95,10.38,20.88;3,215.52,469.23,5.63,20.88;3,223.80,471.04,244.75,8.48;3,143.04,485.20,134.20,8.48;3,281.88,478.23,34.96,20.88;3,321.48,485.20,7.67,8.48;3,333.72,478.23,34.24,20.88;3,372.60,485.20,25.99,8.48;3,403.08,478.23,17.68,20.88;3,425.40,485.20,43.24,8.48;3,143.04,496.12,50.51,8.48;3,198.84,489.15,16.84,20.88;3,217.80,496.12,250.55,8.48;3,143.04,508.48,42.77,8.48;3,187.92,501.51,12.70,20.88;3,202.68,508.48,15.59,8.48;3,221.40,501.51,64.48,20.88;3,289.08,508.48,179.34,8.48;3,143.04,522.52,49.76,8.48;3,196.68,515.55,7.28,20.88;3,204.00,509.43,10.38,20.88;3,206.04,520.71,6.03,20.88;3,214.44,515.19,50.50,21.24;3,265.32,509.43,10.26,20.88;3,267.36,520.71,6.03,20.88;3,275.64,515.55,3.88,20.88;3,283.56,522.52,184.91,8.48;3,143.04,536.32,102.92,8.48"><head>ோ 1 -.</head><label>1</label><figDesc>‫ݎ‬ -݃, where ݈ = ோାீା ଷ The profiles of the orthogonal projections of the pixels to the horizontal X axis is noted ߎ ‫‬ ܺ and to the vertical Y axis (ߎ ‫‬ ܻ ), where op is the projection operator (arithmetic or harmonic mean). The length of a profile is ܵ = ‫ܥ‬ሺ‫ܫ‬ሻ or ܵ = ‫ܮ‬ሺ‫ܫ‬ሻ (where ‫ܥ‬ሺ‫ܫ‬ሻ denotes I's columns and ‫ܮ‬ሺ‫ܫ‬ሻ denotes I's rows) and we estimate its probability distribution function ‫(‬ ݀ ݂ ) on ܰ = ‫‪݊݀ሺ√ܵሻ‬ݑݎ‬ bins [10]. Then for each channel and operator, we compute: Ф ‫‬ ܺ ሺ‫ܫ‬ሻ = ‫݂݀‬ሺߎ ‫‬ ܺ ሻ and we set PEF components to the normalized entropy of this distribution:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,235.68,348.53,139.99,8.59"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The 3 regions of the image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,156.72,522.77,297.90,8.59;5,268.68,533.56,74.03,8.48;5,228.00,397.08,172.56,115.20"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Flickr user tags: oldbook, rarebook, latin, greek, library, bornin1550, deadlanguage, libro</figDesc><graphic coords="5,228.00,397.08,172.56,115.20" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,264.96,644.24,81.45,7.76"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: UAIC system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,153.72,355.40,302.59,76.77"><head>Table 1 :</head><label>1</label><figDesc>Results of our submitted runs</figDesc><table coords="8,153.72,372.20,302.59,59.97"><row><cell>#Run</cell><cell>MiAP</cell><cell>GMiAP</cell><cell>F-ex</cell><cell>Features</cell></row><row><cell>1 1340348352281__submision1</cell><cell>0.2359</cell><cell>0.1685</cell><cell>0.4359</cell><cell>Visual</cell></row><row><cell>2 1340348434346__submision2</cell><cell>0.1863</cell><cell>0.1245</cell><cell cols="2">0.4354 Multimodal</cell></row><row><cell>3 1340348489605__submision3</cell><cell>0.1521</cell><cell>0.1017</cell><cell cols="2">0.4144 Multimodal</cell></row><row><cell>4 1340348583288__submision4</cell><cell>0.1504</cell><cell>0.1063</cell><cell cols="2">0.4206 Multimodal</cell></row><row><cell>5 1340348681456__submision5</cell><cell>0.1482</cell><cell>0.1000</cell><cell cols="2">0.4143 Multimodal</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,151.68,140.72,307.63,257.13"><head>Table 2 :</head><label>2</label><figDesc>Results of participants in Photo Annotation task at ImageCLEF 2012</figDesc><table coords="9,151.68,157.52,307.63,240.33"><row><cell>Group name</cell><cell>MiAP</cell><cell>GMiAP</cell><cell>F-ex</cell><cell>Features</cell></row><row><cell>1 DBRIS</cell><cell>0.0925</cell><cell>0.0445</cell><cell>0.9980</cell><cell>Visual</cell></row><row><cell>2 LIRIS ECL</cell><cell>0.4367</cell><cell>0.3877</cell><cell cols="2">0.5766 Multimodal</cell></row><row><cell>3 DMS, MTA SZTAKI</cell><cell>0.4258</cell><cell>0.3676</cell><cell cols="2">0.5731 Multimodal</cell></row><row><cell>4 National Institute of Informatics</cell><cell>0.3265</cell><cell>0.2650</cell><cell>0.5600</cell><cell>Visual</cell></row><row><cell>ISI</cell><cell>0.4131</cell><cell>0.3580</cell><cell cols="2">0.5597 Multimodal</cell></row><row><cell>6 CEA LIST</cell><cell>0.4159</cell><cell></cell><cell cols="2">0.5404 Multimodal</cell></row><row><cell>7 MLKD</cell><cell>0.3118</cell><cell>0.2516</cell><cell cols="2">0.5285 Multimodal</cell></row><row><cell>8 Multimedia Group of the Informatics</cell><cell>0.3012</cell><cell>0.2286</cell><cell cols="2">0.4950 Multimodal</cell></row><row><cell>and Telematics Institute Centre for</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Research and Technology Hellas</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>9 Feiyan</cell><cell>0.2368</cell><cell>0.1825</cell><cell>0.4685</cell><cell>Textual</cell></row><row><cell>10 KIDS NUTN</cell><cell>0.1717</cell><cell>0.0984</cell><cell cols="2">0.4406 Multimodal</cell></row><row><cell>11 UAIC2012</cell><cell>0.2359</cell><cell>0.1685</cell><cell>0.4359</cell><cell>Visual</cell></row><row><cell>12 NPDILIP6</cell><cell>0.3356</cell><cell>0.2688</cell><cell>0.4228</cell><cell>Visual</cell></row><row><cell>13 IntermidiaLab</cell><cell>0.1521</cell><cell>0.0894</cell><cell>0.3532</cell><cell>Textual</cell></row><row><cell>14 URJCyUNED</cell><cell>0.0622</cell><cell>0.0254</cell><cell>0.3527</cell><cell>Textual</cell></row><row><cell>15 Pattern Recognition and Applications</cell><cell>0.0857</cell><cell>0.0417</cell><cell>0.3331</cell><cell>Visual</cell></row><row><cell>Group</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>16 Microsoft Advanced Technology</cell><cell>0.2086</cell><cell>0.1534</cell><cell>0.2635</cell><cell>Textual</cell></row><row><cell>Labs Cairo</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>17 BUAA AUDR</cell><cell>0.1307</cell><cell>0.0558</cell><cell cols="2">0.2592 Multimodal</cell></row><row><cell>18 UNED</cell><cell>0.0873</cell><cell>0.0441</cell><cell>0.1360</cell><cell>Visual</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,148.08,625.62,169.82,7.66"><p>ImageCLEF2012: http://www.imageclef.org/2012</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,148.08,635.34,211.22,7.66"><p>Photo Annotation Task: http://www.imageclef.org/2012/photo</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="1,148.08,645.06,102.67,7.66"><p>Flickr: http://www.flickr.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,148.08,635.34,192.31,7.66"><p>TopSurf: http://press.liacs.nl/researchdownloads/topsurf/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="2,148.08,645.06,78.79,7.66"><p>Tf-Idf: http://tfidf.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="5,148.08,645.06,166.63,7.66"><p>Bing Translator: http://www.bing.com/translator/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. The research presented in this paper was funded by the <rs type="funder">Sector Operational Program for Human Resources Development</rs> through the project "<rs type="projectName">Development of the innovation capacity and increasing of the research impact through post</rs><rs type="programName">-doctoral programs</rs>" <rs type="grantNumber">POSDRU/89/1.5/S/49944</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_UtQ7d2y">
					<idno type="grant-number">POSDRU/89/1.5/S/49944</idno>
					<orgName type="project" subtype="full">Development of the innovation capacity and increasing of the research impact through post</orgName>
					<orgName type="program" subtype="full">-doctoral programs</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,146.19,333.42,322.31,7.66;10,155.76,343.02,214.73,7.66" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,247.08,333.42,221.42,7.66;10,155.76,343.02,47.84,7.66">Overview of the ImageCLEF 2012 Flickr Photo Annotation and Retrieval Task</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,209.40,343.02,88.90,7.66">CLEF 2012 working notes</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,146.20,352.74,322.31,7.66;10,155.76,362.46,312.52,7.66;10,155.76,372.30,298.13,7.66" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,283.20,352.74,172.16,7.66">UAIC at ImageCLEF 2009 Photo Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Vamanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Croitoru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,235.80,362.46,38.89,7.66">CLEF 2009</title>
		<title level="s" coord="10,356.62,362.46,111.67,7.66;10,155.76,372.30,36.62,7.66;10,211.44,372.30,89.59,7.66">Multilingual Information Access Evaluation</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6242</biblScope>
			<biblScope unit="page" from="283" to="286" />
		</imprint>
	</monogr>
	<note>II Multimedia Experiments</note>
</biblStruct>

<biblStruct coords="10,146.20,382.02,322.21,7.66;10,155.76,391.74,312.67,7.66;10,155.76,401.34,22.49,7.66" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,295.20,382.02,117.29,7.66">TOP-SURF: a visual words toolkit</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,427.20,382.02,41.20,7.66;10,155.76,391.74,201.97,7.66">Proceedings of the 18th ACM International Conference on Multimedia</title>
		<meeting>the 18th ACM International Conference on Multimedia<address><addrLine>Firenze, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1473" to="1476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,146.20,411.06,322.31,7.66;10,155.76,420.90,43.61,7.66" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,194.04,411.06,113.03,7.66">Notes on the OpenSURF Library</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<idno>CSTR-09-001</idno>
		<imprint>
			<date type="published" when="2009-01">January 2009. 2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Bristol</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,146.20,430.62,322.24,7.66;10,155.76,440.34,312.74,7.66;10,155.76,450.06,160.73,7.66" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,337.80,430.62,130.63,7.66;10,155.76,440.34,151.88,7.66">Speeded-up robust features (SURF). Computer Vision and Image Understanding</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,314.88,440.34,153.62,7.66;10,155.76,450.06,24.23,7.66">Computer Vision and Image Understanding (CVIU)</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="359" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,146.20,459.78,322.21,7.66;10,155.76,469.50,294.17,7.66" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,357.48,459.78,110.92,7.66;10,155.76,469.50,42.33,7.66">Visual Categorization with Bags of Keypoints</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,213.00,469.50,181.84,7.66">Workshop on Statistical Learning in Computer Vision</title>
		<imprint>
			<publisher>ECCV</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,146.20,479.22,322.31,7.66;10,155.76,488.94,312.63,7.66;10,155.76,498.78,179.81,7.66" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,430.56,479.22,37.94,7.66;10,155.76,488.94,184.17,7.66">Comparing Compact Codebooks for Visual Categorization</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Geusebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,352.20,488.94,116.19,7.66;10,155.76,498.78,48.20,7.66">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="450" to="462" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,146.20,508.38,318.62,7.66" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="10,237.00,508.38,149.92,7.66">Introduction to modern information retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>McGraw-Hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,146.20,518.10,322.31,7.66;10,155.76,527.82,312.74,7.66;10,155.76,537.66,247.25,7.66" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,282.72,518.10,185.78,7.66;10,155.76,527.82,92.70,7.66">Efficient image concept indexing by harmonic and arithmetic profiles entropy</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,269.64,527.82,198.86,7.66;10,155.76,537.66,100.98,7.66">Proceedings of 2009 IEEE International Conference on Image Processing (ICIP 2009)</title>
		<meeting>2009 IEEE International Conference on Image Processing (ICIP 2009)<address><addrLine>Cairo, Egypt</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">November 7-11, 14. (2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,149.97,547.38,318.44,7.66;10,155.76,557.10,274.37,7.66" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,224.76,547.38,243.64,7.66;10,155.76,557.10,41.18,7.66">On estimation of entropy and mutual information of continuous distributions</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Moddemeijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,202.20,557.10,58.96,7.66">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="246" />
			<date type="published" when="1989-03">March 1989. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,149.97,566.70,318.54,7.66;10,155.76,576.54,203.69,7.66" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,248.64,566.70,90.06,7.66">Similarity of color images</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Stricker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Orengo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,355.92,566.70,112.59,7.66;10,155.76,576.54,112.68,7.66">Storage and Retrieval for Image and Video Databases</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">2420</biblScope>
			<biblScope unit="page" from="381" to="392" />
		</imprint>
	</monogr>
	<note>Proc. SPIE</note>
</biblStruct>

<biblStruct coords="10,149.97,586.26,318.54,7.66;10,155.76,595.98,312.79,7.66;10,155.76,605.70,85.49,7.66" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,303.00,586.26,165.51,7.66;10,155.76,595.98,43.52,7.66">Multimodal semi-supervised learning for image classification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,205.32,595.98,211.72,7.66">IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<meeting><address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="902" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,149.97,615.54,318.56,7.66;10,155.76,625.14,288.41,7.66" xml:id="b12">
	<monogr>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<title level="m" coord="10,274.80,615.54,193.73,7.66;10,155.76,625.14,102.06,7.66">An Introduction to Support Vector Machines and other kernel-based learning methods</title>
		<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,149.97,634.86,318.46,7.66;10,155.76,644.58,145.97,7.66" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,230.16,634.86,202.37,7.66">Support vector machine active learning for image retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,450.84,634.86,17.59,7.66;10,155.76,644.58,59.29,7.66">Proc. ACM Multimedia</title>
		<meeting>ACM Multimedia<address><addrLine>Ottawa, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,149.97,140.82,318.54,7.66;11,155.76,150.54,164.66,7.66;11,320.28,148.76,4.49,5.11;11,329.16,150.54,139.39,7.66;11,155.76,160.26,93.05,7.66" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,287.88,140.82,180.63,7.66;11,155.76,150.54,68.78,7.66">Image Classification Using SVMs: One-against-One Vs. One-against-All</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gidudu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hulley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Marwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,243.72,150.54,76.70,7.66;11,320.28,148.76,4.49,5.11;11,329.16,150.54,135.78,7.66">Proceeding of the 28 th Asian Conference on Remote Sensing</title>
		<meeting>eeding of the 28 th Asian Conference on Remote Sensing<address><addrLine>Malaysia, CD-Rom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,149.97,169.98,318.53,7.66;11,155.76,179.70,223.01,7.66" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="11,273.00,169.98,195.50,7.66;11,155.76,179.70,37.26,7.66">Data Mining -Practical Machine Learning Tools and Techniques</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Morgan Kaufmann</publisher>
			<biblScope unit="volume">629</biblScope>
		</imprint>
	</monogr>
	<note>Third Edition</note>
</biblStruct>

<biblStruct coords="11,149.97,189.42,318.46,7.66;11,155.76,199.14,312.67,7.66;11,155.76,208.98,151.73,7.66" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,234.72,189.42,230.52,7.66">Rapid object detection using boosted cascade of simple features</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,155.76,199.14,292.83,7.66">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Kauai, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
