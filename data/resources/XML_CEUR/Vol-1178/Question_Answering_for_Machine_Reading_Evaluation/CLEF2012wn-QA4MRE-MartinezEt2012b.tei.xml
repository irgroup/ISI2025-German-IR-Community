<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,155.61,115.96,304.14,12.62;1,204.45,133.89,206.46,12.62">Simple similarity-based question answering strategies for biomedical text</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,155.02,171.56,68.24,8.74"><forename type="first">David</forename><surname>Martinez</surname></persName>
							<email>david.martinez@nicta.com.au</email>
							<affiliation key="aff0">
								<orgName type="department">Victoria Research Lab</orgName>
								<orgName type="institution">National ICT Australia</orgName>
								<address>
									<postCode>3010</postCode>
									<settlement>Parkville</settlement>
									<region>VIC</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,233.82,171.56,84.72,8.74"><forename type="first">Andrew</forename><surname>Mackinlay</surname></persName>
							<email>andrew.mackinlay@nicta.com.au</email>
							<affiliation key="aff0">
								<orgName type="department">Victoria Research Lab</orgName>
								<orgName type="institution">National ICT Australia</orgName>
								<address>
									<postCode>3010</postCode>
									<settlement>Parkville</settlement>
									<region>VIC</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,329.09,171.56,79.84,8.74"><forename type="first">Diego</forename><surname>Molla-Aliod</surname></persName>
							<email>diego.molla-aliod@mq.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">Macquarie University North Ryde</orgName>
								<address>
									<region>NSW Australia</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,419.49,171.56,40.85,8.74;1,238.24,183.51,37.36,8.74"><forename type="first">Lawrence</forename><surname>Cavedon</surname></persName>
							<email>lawrence.cavedon@nicta.com.au</email>
							<affiliation key="aff0">
								<orgName type="department">Victoria Research Lab</orgName>
								<orgName type="institution">National ICT Australia</orgName>
								<address>
									<postCode>3010</postCode>
									<settlement>Parkville</settlement>
									<region>VIC</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,305.54,183.51,67.11,8.74"><forename type="first">Karin</forename><surname>Verspoor</surname></persName>
							<email>karin.verspoor@nicta.com.au</email>
							<affiliation key="aff0">
								<orgName type="department">Victoria Research Lab</orgName>
								<orgName type="institution">National ICT Australia</orgName>
								<address>
									<postCode>3010</postCode>
									<settlement>Parkville</settlement>
									<region>VIC</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,155.61,115.96,304.14,12.62;1,204.45,133.89,206.46,12.62">Simple similarity-based question answering strategies for biomedical text</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BDF2BD8F8E765FC004396150191DEC39</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>question answering, biomedical natural language processing</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce an approach to question answering in the biomedical domain that utilises similarity matching of question/answer pairs in a document, or a set of background documents, to select the best answer to a multiple-choice question. We explored a range of possible similarity matching methods, ranging from simple word overlap, to dependency graph matching, to feature-based vector similarity models that incorporate lexical, syntactic and/or semantic features. We found that while these methods performed reasonably well on a small training set, they did not generalise well to the final test data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A combined NICTA/Macquarie team participated in the CLEF2012 QA4MRE pilot task on "Machine Reading of Biomedical Texts about Alzheimer". This task addresses the goal of obtaining a detailed understanding of the content of a text, in this case a text on the topic of Alzheimer's disease. A system's ability to interpret a text is measured practically through the performance of that system on a series of multiple choice questions about the text. Each question had five possible answers; the goal of the system was to select the correct answer from among those five.</p><p>We experimented with several simple approaches to this task, each based on the similarity of a candidate query constructed from the question plus a candidate answer to the information available in text. The approaches varied in the details of how similarity was measured, and what text sources were used to assess the relevance of the candidate answer. We will describe these details, and provide an assessment of the performance of each system variant, in the remainder of the paper.</p><p>Question answering is a natural language processing task that has quite a long history of research. It was revived in the late 1990s with the Question Answering track of the Text REtrieval Conference (TREC) <ref type="bibr" coords="2,357.98,166.67,14.61,8.74" target="#b22">[23]</ref>, with other evaluationbased competitions following suit. Systems typically focus on factoid questionanswering where the answer is a specific fact such as a location, person, etc.</p><p>They find the answer by first determining the answer type during a question classification step, retrieving a set of candidate documents or passages using standard information retrieval techniques, and then extracting the answer from those candidates. Answer extraction generally has involved techniques such as (a combination of) pattern matching <ref type="bibr" coords="2,296.80,250.35,14.61,8.74" target="#b19">[20]</ref>, similarity matching with the question using simple word-based features <ref type="bibr" coords="2,283.23,262.31,15.50,8.74" target="#b12">[13]</ref> or Bayesian techniques <ref type="bibr" coords="2,406.24,262.31,9.96,8.74" target="#b3">[4]</ref>, measurement of answer redundancy e.g. on the Web <ref type="bibr" coords="2,301.86,274.26,9.96,8.74" target="#b1">[2]</ref>, and even methods based on logic <ref type="bibr" coords="2,462.33,274.26,14.61,8.74" target="#b16">[17]</ref>.</p><p>Research on question answering methods specifically for biomedical text is a relatively new topic and systems attempt to find answers that are more complex than simple facts. Thus, MedQA <ref type="bibr" coords="2,276.32,310.13,15.50,8.74" target="#b23">[24]</ref> and AskHERMES <ref type="bibr" coords="2,374.69,310.13,10.52,8.74" target="#b2">[3]</ref> incorporate summarisation and clustering techniques. Other approaches such as Demner-Fushman et al.'s <ref type="bibr" coords="2,152.10,334.04,10.40,8.74" target="#b5">[6]</ref> system and EPoCare <ref type="bibr" coords="2,260.47,334.04,15.50,8.74" target="#b15">[16]</ref> use information extraction techniques to identify specific types of information relevant to biomedical research queries.</p><p>Research in multiple-choice question answering is less active though it is related to the task of answer validation, where question answering systems use techniques such as answer redundancy with the support of large corpora or the Web <ref type="bibr" coords="2,157.95,393.81,14.61,8.74" target="#b11">[12]</ref>, or methods based on logical proving <ref type="bibr" coords="2,343.88,393.81,15.50,8.74" target="#b16">[17]</ref> and textual entailment <ref type="bibr" coords="2,467.31,393.81,9.96,8.74" target="#b7">[8]</ref>. Answer validation is the central task of the series of Answer Validation Exercises (AVE) at CLEF <ref type="bibr" coords="2,208.66,417.72,14.61,8.74" target="#b17">[18]</ref>.</p><p>A similar approach to our methods is taken by <ref type="bibr" coords="2,369.76,429.68,9.96,8.74" target="#b6">[7]</ref>, where three types of sentence similarity methods are explored: tree-distance, sequence similarity, and order invariant methods. Their empirical evaluation shows that the method to choose depends heavily on the testbed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Each method that we experimented with is based on selecting the most likely answer from a set of multiple-choice candidate answers to a question, through an evaluation of the similarity of a candidate answer to the text in a given document or set of documents. The high-level process for each system variant was:</p><p>1. Construct a candidate query based on the combination of a question and a candidate answer. 2. Search the relevant text for sentences matching each candidate query. 3. Select the candidate query with the best match/most similar sentence in the text as the correct answer to the question.</p><p>The system variants differed on the matching algorithm employed, the matching criteria applied, and the text that was searched.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing</head><p>For the primary article as well as the background articles, we worked from the pre-parsed sentences provided by the task organisers, since these were the cleanest source of data available, stripping the supplied dependency labels while preserving sentence boundaries and tokenisation. We normalised greek characters and numbers to address inconsistencies between the source article and the text in the questions and answers. Greek characters such as α were converted to the equivalent version spelled out using Latin characters (alpha), while spelled out numbers such as three were rewritten using digits <ref type="bibr" coords="3,377.61,222.61,13.64,8.74" target="#b2">(3 )</ref>. We parsed all text with ClearParser <ref type="bibr" coords="3,210.80,234.56,9.96,8.74" target="#b4">[5]</ref>, a dependency-parser which has been demonstrated to have state-of-the-art performance on biomedical text <ref type="bibr" coords="3,341.04,246.52,14.61,8.74" target="#b21">[22]</ref>, and which has a pre-trained biomedical parsing model available for download.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Construction of candidate queries using question/answer pairs</head><p>The starting point for each method is to construct a query based on a combination of the question and a candidate answer. The aim of this step is to produce a succinct statement of the information represented by a candidate answer, with the question providing appropriate context. For instance, a question/answer pair "Q: Which protein is known to remove Aβ from the brain? A: IDE" would be merged to form the query "IDE is known to remove Aβ from the brain". This process was applied for each candidate answer within the multiple-choice question. Thus we constructed 5 queries for each question.</p><p>Bag-of-words queries The simplest system we developed utilised simple word overlap as its matching algorithm. For this system, a correspondingly simple method was used to construct the query ("bag of words"), since the candidate sentences do not need to be grammatically sound. Each query was constructed from all question words excluding the initial question word, plus all of the words in the candidate answer.</p><p>Merging Question and Answer Graphs For some experiments, we used the outputs of a dependency parser to evaluate the similarity of question and answer sentences, to provide a semantically-richer method of comparison. This requires a distinct graph corresponding to each answer which also integrates the dependencies of the question to compare against the graphs from the evidence sentences. We achieve this by using a custom algorithm to insert the answer subgraph into the question graph. This merged graph also formed the starting point for the vector space model methods described in Section 3.4. The aim of this step is to produce a merged graph that looks very similar to the graph of a declarative sentence in which the particular answer to that question is stated. For example, consider the example question in (1) taken from the sample data and the corresponding answer in <ref type="bibr" coords="3,278.53,643.03,11.62,8.74" target="#b1">(2)</ref>. Ideally, from the dependency graph of this question shown in Figure <ref type="figure" coords="3,246.66,654.98,3.87,8.74" target="#fig_0">1</ref>, and that of the answer fragment, we would create a similar dependency graph to what would be obtained by parsing <ref type="bibr" coords="4,418.97,268.89,11.62,8.74" target="#b2">(3)</ref>. We would like to replace the dependency nodes corresponding to Which and enzyme with a node for Aromatase derived from the answer subgraph.</p><p>(1) Which enzyme is responsible for the transformation of testosterone into estrogen?</p><p>(2) Aromatase (3) Aromatase is responsible for the transformation of testosterone into estrogen.</p><p>The procedure for this which we use for most questions is as follows:</p><p>1. Find the "question node" -a single node within the question graph which has as its lemma any item from hand-created list of eight Wh-question lemmas, such as what, how and where. In Figure <ref type="figure" coords="4,322.96,404.38,3.87,8.74" target="#fig_0">1</ref>, this is node 1, for the token which. 2. Find the "root target node" in the question graph. This is the node linked by a dependency with label ROOT, generally corresponding to the main verb in a complete sentence. For Figure <ref type="figure" coords="4,305.40,440.24,3.87,8.74" target="#fig_0">1</ref>, this is node 3. 3. Find the subgraph corresponding to the path from the root target node to the question node, noting the label of link on the path closest to the root target node, which we denote the "question link label". Then delete this entire subgraph. This removes the nodes corresponding to the question phrase; in Figure <ref type="figure" coords="4,231.13,500.02,3.87,8.74" target="#fig_0">1</ref>, we would remove nodes 1 and 2 corresponding to the subject of the question sentence, which enzyme. In addition, if the question node is how, delete any connected nodes with lemma many (thus treating how many as a multi-token question word). 4. Find the root target node of the answer graph (which generally corresponds to the head of a noun phrase). Insert this node, along with all linked nodes (the complete graph apart from the root node) into the modified question graph, adding a link from the root target node of the question graph, with the label set to the question link label from above. The intuition here is that the answer should occupy the grammatical slot (most often SBJ) which was formerly filled by the question.</p><p>After applying this procedure to the graph in Figure <ref type="figure" coords="4,379.34,639.50,4.98,8.74" target="#fig_0">1</ref> and the answer graph for the noun phrase aromatase from (2), we obtain the graph shown in Figure <ref type="figure" coords="4,472.85,651.45,3.87,8.74" target="#fig_1">2</ref>.</p><p>Upon examination of the test set, we found that further optimisation of these rules was required. In particular, these rules produced suboptimal results for questions phrased as full or reduced relative clauses such as ( <ref type="formula" coords="5,402.19,142.90,4.24,8.74">4</ref>) and <ref type="bibr" coords="5,433.36,142.90,11.62,8.74" target="#b4">(5)</ref>. (4) What is the major protease produced by microglia responsible for degrading A? <ref type="bibr" coords="5,134.77,343.54,13.24,8.74" target="#b4">(5)</ref> What are the sst receptors that are expressed on rat astrocytes?</p><p>To handle these cases, we introduced a new set of rules to replace step 4 above, which we do not explain in detail here, but which attempt to directly attach the candidate answer graph to the main verb in the body of the relative clause or the omitted copular verb in the case of a restrictive relative such as (4). For the above examples, we would be attempting to produce combined dependency graphs which could be obtained by parsing sentences such as ( <ref type="formula" coords="5,407.83,425.24,4.24,8.74">6</ref>) and ( <ref type="formula" coords="5,442.88,425.24,3.87,8.74">7</ref>).</p><p>(6) IDE is produced by microglia responsible for degrading A <ref type="bibr" coords="5,134.77,459.11,13.24,8.74" target="#b6">(7)</ref> Microglia are expressed on rat astrocytes</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Word Overlap</head><p>The simplest algorithm that we experimented with considered the lexical overlap between the query "bag of words" described above, and the sentences in the reference corpus. We measured the number of overlapping words (where a word is defined as a single token), and assigned the candidate answer with highest word overlap with some sentence in the reference as the system's response. As reference corpus we experimented with different variants (reference document, background collection, and in-house background collection) as explained in Section 3.6. In case of ties, during the development we returned all tied answers, and assigned partial credit in the evaluation. This system had poor performance on the training data; we chose not to submit runs with this system for the test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Vector Space Model (VSM)</head><p>For this method, we measured the similarity of each query sentence to each sentence in the reference text, using cosine similarity of feature vectors representing the sentences, consisting of lexical, syntactic, and semantic information. This method, known as the vector space model (VSM) has been widely used in Information Retrieval to compare documents and queries <ref type="bibr" coords="6,398.20,186.24,9.96,8.74" target="#b8">[9]</ref>. We utilised the merged question/answer queries from Section 3.2 as the starting point. The query-reference sentence pair that has the highest similarity score is selected as the answer. In this case ties are rare, and we choose the answer randomly when that happens. Again, our experiments relied on different background collections, described in Section 3.6.</p><p>We made use of the following feature types:</p><p>-Lexical (LEX): We lemmatised the text using the Genia tagger <ref type="bibr" coords="6,427.91,277.28,15.50,8.74" target="#b20">[21]</ref> in order to use lemmas as well as word-forms in the feature vectors. We also tested the effects of removing function words over the development data. Our final version of this feature type used lemmas, function word removal, and special features for NUMBER and DATE classes. -Syntactic (SYN): We extracted all possible triples from the dependency parser output (cf. Section 3.5), and each of these relationships was used as a feature of the vector (e.g. "introduce-OBJ-morphometry"). -Semantic (SEM): We used MetaMap <ref type="bibr" coords="6,312.79,372.67,10.52,8.74" target="#b0">[1]</ref> to obtain phrases and concepts that occur in the UMLS metathesaurus <ref type="bibr" coords="6,301.33,384.63,14.61,8.74" target="#b9">[10]</ref>. The mapped concepts can be marked as negative by the built-in Negex tool, and in these cases we added a "NEG " prefix to the feature. Each concept is also associated to one of the 73 Semantic Types that form the high level ontology of UMLS (e.g. "Enzyme"). We extracted three types of features from the MetaMap output: Concept identifiers (CUI), Semantic Types (ST), and hypernyms of the original concepts (HYP). We tested different combinations of these features.</p><p>In order to build the feature vectors, we tested using both raw frequencies, and tf-idf scores. We also incorporated a thresholded pre-filter to compare background sentences to answer candidate strings only, in order to remove sentences that have large overlap with the query but little with the possible answers. Finally, we explored combining the outputs of different VSM configurations, by choosing the candidate with the highest cosine similarity value from any of a set of underlying systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Graph Overlap</head><p>For the dependency-graph approach, we built on prior work from the AnswerFinder QA system <ref type="bibr" coords="6,186.83,608.30,14.61,8.74" target="#b13">[14]</ref>. The idea behind this was to create a rough dependency-based analog of the word-overlap method described above. For the dependency matching, we first apply the graph-merging algorithm described in Section 3.2. We then compare the merged question-answer graph for a candidate answer with the dependency graph for every supporting sentence. The intuition was that, for the correct answer, there should be some corresponding declarative sentence in the text denoting the answer, and the graph of this should show a high degree of overlap with the merged question-answer graph. The supporting graph with the highest similarity score is likely to have the most similar declarative content to the candidate question-answer graph, providing evidence that the potential answer may be correct. We repeat this process for each candidate answer, and the answer with most closely matching (highest-scoring) supporting sentence is marked as correct.</p><p>Dependency Parsing The pre-processing described in Section 3.1 was applied to all background text, as well as questions and answers. For the reference corpora, the POS-tags from preprocessed files supplied by the organisers was preserved; for questions and answers, no preprocessed version was provided, so we POS-tagged the questions using the biomedical POS-tagging model of ClearParser. A qualitative analysis showed that this performed poorly over questions due to differences in the tagging model, so we added a subsequent post-correction phase. If any token among the first three corresponded to a whquestion word such as which, what or how, the tag was explicitly set to be the correct tag for the token to operate as a question word, ensuring, for example, that which and what are tagged correctly (according to <ref type="bibr" coords="7,382.47,352.12,15.50,8.74" target="#b18">[19]</ref>) as WDT, <ref type="foot" coords="7,445.86,350.55,3.97,6.12" target="#foot_0">3</ref> rather than IN, the tag they were (surprisingly) assigned more frequently.</p><p>For parsing, we used the biomedical model over the sample data, where we found it gave acceptable accuracy after the POS-tags were corrected. However, over the questions in the test data, a manual inspection of the parser outputs revealed a large number of parsing errors probably due, as in POS-tagging, to the parsing model having very few question instances in its training data from which to learn parsing features. We switched instead to the pre-trained 'Medical' model which includes clinical questions in its training data, and observed a qualitative improvement in parsing accuracy.</p><p>Scoring Graphs for Similarity After merging candidate answers with question subgraphs, we have a set of distinct dependency graphs which can be compared against the graphs obtained by parsing the reference corpus as described above. The comparison method we used was based on the graph comparison techniques from AnswerFinder <ref type="bibr" coords="7,268.07,537.42,15.50,8.74" target="#b14">[15,</ref><ref type="bibr" coords="7,285.23,537.42,11.62,8.74" target="#b13">14]</ref>. Specifically, we converted the ClearParser outputs to the Logical Graphs of <ref type="bibr" coords="7,275.82,549.38,14.61,8.74" target="#b14">[15]</ref>, requiring some minor extensions to the An-swerFinder codebase, and then compared the logical graphs from the questionanswer parses to those of the reference sentences using the implementation of the MCS algorithm for graph comparison included with AnswerFinder. Unlike <ref type="bibr" coords="7,134.77,597.20,14.61,8.74" target="#b13">[14]</ref>, this work here did not have a stage of learning QA-rules from training data, due to a lack of readily available in-domain data.</p><p>Each candidate merged query graph was compared against every reference sentence. The raw overlap score for a given pair is the size of the largest overlapping subgraph. This score was either kept as a raw count or normalised by the lengths of the respective sentences, to avoid a bias towards longer sentences. The score for a given answer was set to the maximum similarity found from any pairing involving the answer graph. The question/answer graph with the highest value for this maximum similarity was assumed to be most likely to contain the correct answer, and the corresponding answer would then be marked as correct.</p><p>We varied the reference corpus in our experiments. In some cases, we limited the comparison to the canonical main article supplied with the question data, while in others we used text from the supplied background document collections. Due to the computationally intensive nature of the graph comparison process, as well as the limited time available for experimentation over the test set, exhaustive comparison of each candidate answer sentence with all background sentences would have been infeasible. In cases where we used the background collection, this was filtered by thresholding against the word-vector similarity score for the sentence-pair from Section 3.4. This reduction in collection size would be unlikely to erroneously omit documents, since documents with a low word overlap are unlikely to have a high degree of graph overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Resources</head><p>The only external knowledge resource that we used was the Unified Medical Language System (UMLS) <ref type="bibr" coords="8,255.32,395.96,14.61,8.74" target="#b9">[10]</ref>. We applied the MetaMap system <ref type="bibr" coords="8,427.53,395.96,10.52,8.74" target="#b0">[1]</ref> to recognise UMLS concepts in the texts. In several of the runs we submitted, we took advantage of the hierarchical structure of the UMLS in order to generalise observed concepts to their hypernyms. These hypernyms were used as features in some vector space model runs.</p><p>Runs varied in terms of which texts were used as the reference corpus for the query similarity matching. In some runs, only the source document associated with the questions was utilised. Other runs considered the full background set of documents that we were provided at the outset of the experiment by the organisers. Finally, we also constructed an extra background collection of 63,000 abstracts built by querying PUBMED with the terms "Alzheimer's disease". We experimented with various combinations of these collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We submitted 10 runs for the test set in the evaluation. The results we obtained for each system variant on both the training set and the evaluation test set are shown in Table <ref type="table" coords="8,225.87,607.16,3.87,8.74">1</ref>. 'VS' refers to the vector space model, 'WO' indicates word overlap, and 'GM' refers to the graph-matching approach. The 'Wtd Acc' columns refer to the 'weighted accuracy', where if multiple values tie for the highest rank, the system receives only partial credit -the reciprocal of the number of tied answers, instead of one. For runs using 'GM', ties were broken by arbitrarily choosing the numerically-lower answer.</p><p>The 'Backgrd' column refers to supporting documents added from the background material which were used as evidence. This could be no documents, the complete collection, or a filtered subset thresholded on the basis of vector-space bag-of-words similarity ('BW'), in which case the threshold is shown.  <ref type="table" coords="9,162.68,388.79,4.13,7.89">1</ref>. Results of the NICTA/Macquarie team runs on the 2012 QA4MRE Alzheimer task. Italicised results (with no run ID) were not submitted as official runs to the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>The performance of our methods on the test set was significantly worse than the performance of the small sample data set we had been provided with. While several methods showed high accuracy over the sample data, the results over the test set were in general not significantly better than random, if at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Word overlap and VSM</head><p>The results of both the word overlap system and VSM were well above the random baseline over the development data. Moreover, with rich semantic features, such as hypernyms, the VSM model reached 100% accuracy (without parameter tuning). These results suggested that VSM could perform well in the challenge, however the results over the test set were at the level of the random baseline.</p><p>An analysis of the outputs of the VSM on the test data showed that there was large variability on the answers given by the different configurations, despite the similar low performance. However, even if we had an oracle system over the outputs of these 8 systems, it would only achieve 62.5% accuracy. The systems performed particularly badly over the first document (22506010), with an oracle accuracy of only 30%. We manually analysed those errors. We found that one of the main sources of error seemed to be the selection of distractors, which were often terms with high frequency in the reference document; this misled our naive classifiers, which have minimal awareness of structure, and led them to retrieve large numbers of sentences with high similarity scores.</p><p>Manual analysis also showed that relevant sentences would usually appear towards the top of the ranking, but below other candidates with higher weight. This did not happen over development data, where one of the candidate sentences usually stood out. This suggests that this approach may be useful where the questions are more straightforward, like the ones given in the development set, or even as a initial filter in a harder challenge such as represented by the test data; but it is not effective as a complete solution. These results are consistent with <ref type="bibr" coords="10,158.37,286.37,9.96,8.74" target="#b6">[7]</ref>, where VSM is the best performing system for one of the two target tasks (TREC 11 QAD, built semi-automatically), but failed to perform well for the other (GNU Library Manual, built manually).</p><p>Examining the different variants of VSM, adding the background collection clearly increased the confusion; the best results were obtained using the reference document only. Comparing the performance of different feature sets, it is difficult to make strong conclusions. Syntactic and semantic features seem to be generally useful, though inclusion of hypernyms reduced performance on the test data. As mentioned before, ties are rare, and choosing not to answer in these cases has minimal effect on the final scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Graph Matching</head><p>Over the sample data, the graph-matching approach performed appreciably better than random in at least some configurations, although the figures were much less promising than the VSM methods. As with the word-based methods, the accuracy dropped noticeably over the larger test set, although the magnitude of this drop was smaller, largely because the performance over the sample set was not as high to begin with. While the best accuracy was obtained over a graph matching run, much of this may be attributable to chance. An error analysis examining the eight questions which were answered correctly in the 'nicta12102' run but not in the VSM 'nicta12053' run showed that only two had genuinely selected a sentence which provided good evidence for a single postulated answer, while the remainder had the question correct by chance, either because a spurious sentence match lead to the correct answer anyway, or because there was a tie between two or more sentences and the best answer was arbitrarily selected.</p><p>There were other interesting differences between the sample data and the test set. In post-submission experiments, we found that the normalisation for sentence length had a positive effect over the test data, even though it was detrimental over the small set of sample data. Similarly adding in the filtered background collection caused changes in different directions over the test and development sets. There were some easily repairable deficiencies in the graph-matching approach which were made obvious during more detailed analysis. The handling of numbers in our system was suboptimal, which is particularly a problem when the answers to a question are mostly numeric. The logical graphs which we check for overlaps use the lemma as the node identifier, which is a sensible approach for most words. However, the lemmas produced by ClearParser convert all numbers and contiguous sequences of digits to a single digit '0'. So '10' and '283.0' would both be mapped to '0', and the string 'P436Q' would be converted to 'p0q', in all cases losing potentially valuable information from a QA perspective. In future work, we will experiment with a different tool, e.g. the BioLemmatizer <ref type="bibr" coords="11,445.65,226.59,14.61,8.74" target="#b10">[11]</ref>.</p><p>Another deficiency of the graph-matching approach was that there was often insufficient distinction between the different answers, if it happened that the answers were not part of a matching subgraph for any of the evidence sentences. In these cases, a fallback strategy (such as using a VSM approach) could have helped somewhat. Another option would have been to not postulate analyses for cases where there were multi-way ties. In post-hoc analysis, we investigated what the outcome would have been if we had refused to pick an answer when there were three or more answers tied for the highest score. For the graph matching variant with the background collection, this would have meant only answering 17 questions, but the c@1 score would have decreased from 0.30 to 0.23 (with a similar drop over the sample data), indicating that we are probably benefiting from chance co-occurrences in the displayed test score.</p><p>While the graph matching algorithm itself was fairly well-developed, the overall pipeline was fairly simplistic. It is possible that it would have been beneficial to to use a more sophisticated approach such as the rule-learning approach of <ref type="bibr" coords="11,134.77,417.87,14.61,8.74" target="#b13">[14]</ref>, but this was difficult due to a lack of in-domain training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we explored the application of similarity-based methods to a multiple-choice question answering task in the biomedical domain. The somewhat unusual multiple-choice nature of this question answering task meant that we were able to attempt interesting transformations of questions by inserting answer graphs into the question graphs, and compare those merged graphs to sentences in a reference document or collection. Our initial experiments on the development data showed promise particularly for the similarity measurements utilising the vector space model with a combination of lexical and semantic features; however these results did not generalise to the test data, where we saw consistently lower performance and no evidence that one feature type or similarity matching strategy is consistently better than another. The inclusion of the background document collection did not in general seem to help, except possibly when filtered to identify the most likely relevant subset of that background. Together, our results suggest that the test set of questions and answers required a more sophisticated solution for answer selection than we developed on the basis of the sample data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,181.84,206.94,251.67,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The dependency graph of (1) produced by ClearParser</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,134.77,265.98,345.82,7.89;5,134.77,276.96,345.83,7.86;5,134.77,287.92,129.37,7.86"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. The dependency graph from Figure1with the graph of the candidate answer aromatase inserted according to the method described, corresponding roughly with the parse we would obtain from (3).</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0" coords="7,144.73,628.41,335.86,7.86;7,144.73,639.37,318.48,7.86"><p>In some cases, what as a question word should be tagged as WP, when it acts as the the head of the noun phrase<ref type="bibr" coords="7,261.28,639.37,13.52,7.86" target="#b18">[19]</ref>, but we did not allow for this possibility here.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,139.77,337.63,7.86;12,151.52,150.73,329.07,7.86;12,151.52,161.69,93.10,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,214.34,139.77,266.24,7.86;12,151.52,150.73,92.33,7.86">Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Aronson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,268.19,150.73,161.00,7.86">AMIA Annual Symposium Proceedings</title>
		<meeting><address><addrLine>Washington DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,172.09,337.64,7.86;12,151.52,183.05,313.16,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,353.28,172.09,127.31,7.86;12,151.52,183.05,11.14,7.86">Data-intensive question answering</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,337.00,183.05,99.01,7.86">Proceedings TREC 2001</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>TREC 2001</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,193.44,337.64,7.86;12,151.52,204.40,329.07,7.86;12,151.52,215.36,329.07,7.86;12,151.52,226.32,196.34,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,212.03,204.40,268.56,7.86;12,151.52,215.36,68.47,7.86">AskHERMES: An online question answering system for complex clinical questions</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Antieau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Cimino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Ely</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/21256977" />
	</analytic>
	<monogr>
		<title level="j" coord="12,228.46,215.36,139.15,7.86">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="277" to="288" />
			<date type="published" when="2011-04">Apr 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,236.72,337.64,7.86;12,151.52,247.68,329.07,7.86;12,151.52,258.63,329.07,7.86;12,151.52,269.59,174.86,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,335.04,236.72,145.55,7.86;12,151.52,247.68,77.07,7.86">LDA based similarity modeling for question answering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Tur</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1867768" />
	</analytic>
	<monogr>
		<title level="m" coord="12,254.63,247.68,225.97,7.86;12,151.52,258.63,67.08,7.86">Proceedings of the NAACL HLT 2010 Workshop on Semantic Search</title>
		<meeting>the NAACL HLT 2010 Workshop on Semantic Search</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,279.99,337.63,7.86;12,151.52,290.95,329.07,7.86;12,151.52,301.91,329.07,7.86;12,151.52,312.87,257.79,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,247.28,279.99,233.30,7.86;12,151.52,290.95,11.14,7.86">Getting the most out of transition-based dependency parsing</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,184.98,290.95,295.61,7.86;12,151.52,301.91,207.17,7.86">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06">June 2011</date>
			<biblScope unit="page" from="687" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,323.26,337.63,7.86;12,151.52,334.22,288.63,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,279.36,323.26,201.22,7.86;12,151.52,334.22,100.54,7.86">Answering clinical questions with knowledge-based and statistical techniques</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,259.19,334.22,106.72,7.86">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="63" to="103" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,344.62,337.64,7.86;12,151.52,355.58,329.07,7.86;12,151.52,366.53,329.07,7.86;12,151.52,377.49,209.67,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,202.89,344.62,235.32,7.86">Variants of tree similarity in a question answering task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Emms</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1641976.1641989" />
	</analytic>
	<monogr>
		<title level="m" coord="12,463.04,344.62,17.56,7.86;12,151.52,355.58,222.13,7.86;12,446.44,355.58,30.57,7.86">Proceedings of the Workshop on Linguistic Distances</title>
		<meeting>the Workshop on Linguistic Distances<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="100" to="108" />
		</imprint>
	</monogr>
	<note>LD &apos;06</note>
</biblStruct>

<biblStruct coords="12,142.96,387.89,337.64,7.86;12,151.52,398.85,209.97,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,320.64,387.89,159.96,7.86;12,151.52,398.85,38.40,7.86">TE4AV: Textual entailment for answer validation</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Ferrández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palomar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,210.91,398.85,83.73,7.86">Proceedings NLP-KE</title>
		<meeting>NLP-KE</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,409.24,337.64,7.86;12,151.52,420.20,229.92,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,292.96,409.24,183.56,7.86">A vector space model for automatic indexing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W</forename><surname>Gerard Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,165.60,420.20,115.05,7.86">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="613" to="620" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,430.60,337.97,7.86;12,151.52,441.56,329.07,7.86;12,151.52,452.52,262.91,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,336.24,430.60,144.35,7.86;12,151.52,441.56,13.44,7.86">The unified medical language system</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lindberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mccray</surname></persName>
		</author>
		<ptr target="http://www.nlm.nih.gov/cgi/request" />
	</analytic>
	<monogr>
		<title level="j" coord="12,175.31,441.56,75.52,7.86">Methods Inf Med</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="281" to="291" />
			<date type="published" when="1993-06-20">1993. June 20, 2007</date>
		</imprint>
	</monogr>
	<note>retreived file d2007</note>
</biblStruct>

<biblStruct coords="12,142.62,462.91,337.97,7.86;12,151.52,473.87,329.07,7.86;12,151.52,484.83,137.39,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,407.83,462.91,72.76,7.86;12,151.52,473.87,278.55,7.86">BioLemmatizer: a lemmatization tool for morphological processing of biomedical text</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">A</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Verspoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,438.31,473.87,42.28,7.86;12,151.52,484.83,87.73,7.86">Journal of Biomedical Semantics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,495.23,337.97,7.86;12,151.52,506.19,325.30,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,347.31,495.23,133.28,7.86;12,151.52,506.19,151.67,7.86">Is it the right answer? exploiting web redundancy for answer validation</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Prevete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Tanev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,324.18,506.19,67.37,7.86">Proceedings ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="425" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,516.58,337.97,7.86;12,151.52,527.54,329.07,8.37;12,151.52,538.50,139.51,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,187.21,527.54,160.10,8.37">Lasso: A tool for surfing the answer net</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">I</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Sca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Goodrum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,177.18,538.50,77.50,7.86">Proceedings TREC</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,548.90,337.97,7.86;12,151.52,559.86,329.07,7.86;12,151.52,570.81,103.52,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,196.73,548.90,202.21,7.86">Learning of graph-based question answering rules</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mollá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,421.24,548.90,59.35,7.86;12,151.52,559.86,329.07,7.86;12,151.52,570.81,26.52,7.86">Proceedings of HLT/NAACL 2006 Workshop on Graph Algorithms for Natural Language Processing</title>
		<meeting>HLT/NAACL 2006 Workshop on Graph Algorithms for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="37" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,581.21,337.98,7.86;12,151.52,592.17,311.56,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,268.94,581.21,192.31,7.86">Learning of graph rules for question answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mollá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Van Zaanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,592.17,282.88,7.86">Proceedings of the 2005 Australasian Language Technology Workshop</title>
		<meeting>the 2005 Australasian Language Technology Workshop</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,602.57,337.97,7.86;12,151.52,613.52,329.07,7.86;12,151.52,624.48,282.77,7.86" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,383.66,602.57,96.93,7.86;12,151.52,613.52,112.02,7.86">Answering clinical questions with role identification</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Mcarthur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Rodriguez-Gianolli</surname></persName>
		</author>
		<ptr target="http://citeseer.ist.psu.edu/581532.html" />
	</analytic>
	<monogr>
		<title level="m" coord="12,283.79,613.52,196.80,7.86;12,151.52,624.48,90.60,7.86">Proc. ACL, Workshop on Natural Language Processing in Biomedicine</title>
		<meeting>ACL, Workshop on Natural Language essing in Biomedicine</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,634.88,337.97,7.86;12,151.52,645.84,329.07,7.86;12,151.52,656.80,178.91,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,312.44,634.88,168.15,7.86;12,151.52,645.84,11.14,7.86">High performance question answering</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Paşca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">M</forename><surname>Harabagiu</surname></persName>
		</author>
		<ptr target="http://citeseer.ist.psu.edu/pasca01high.html" />
	</analytic>
	<monogr>
		<title level="m" coord="12,195.76,645.84,66.25,7.86">Proc. SIGIR&apos;01</title>
		<meeting>SIGIR&apos;01<address><addrLine>New Orleans, Luisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,119.67,337.98,7.86;13,151.52,130.63,329.07,7.86;13,151.52,141.59,231.34,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,262.34,119.67,218.25,7.86;13,151.52,130.63,61.11,7.86">Testing the reasoning for question answering validation (draft)</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
		<ptr target="http://logcom.oxfordjournals.org/content/18/3/459.short" />
	</analytic>
	<monogr>
		<title level="j" coord="13,223.49,130.63,156.21,7.86">Journal of Logic and Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">459474</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,152.55,337.98,7.86;13,151.52,163.51,242.12,7.86" xml:id="b18">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
		<title level="m" coord="13,207.86,152.55,272.73,7.86;13,151.52,163.51,33.47,7.86">Part-of-speech tagging guidelines for the penn treebank project (3rd revision)</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">570</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct coords="13,142.62,174.47,337.98,7.86;13,151.52,185.43,329.07,7.86;13,151.52,196.39,25.60,7.86" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="13,228.31,174.47,252.29,7.86;13,151.52,185.43,29.45,7.86">Patterns of potential answer expression as clues to the right answers</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Soubbotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,354.49,185.43,97.00,7.86">Proceedings TREC 2001</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>TREC 2001</meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,207.34,337.98,7.86;13,151.52,218.30,329.07,7.86;13,151.52,229.26,329.07,7.86;13,151.52,240.22,83.08,7.86" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="13,194.19,218.30,249.64,7.86">Developing a robust part-of-speech tagger for biomedical text</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Tateishi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mcnaught</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,465.49,218.30,15.10,7.86;13,151.52,229.26,270.83,7.86">Advances in Informatics -10th Panhellenic Conference on Informatics</title>
		<meeting><address><addrLine>Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Volas</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="382" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,251.18,337.98,7.86;13,151.52,262.14,329.07,7.86;13,151.52,273.10,329.07,7.86;13,151.52,284.06,329.07,7.86;13,151.52,295.02,222.31,7.86" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="13,263.91,273.10,216.68,7.86;13,151.52,284.06,329.07,7.86;13,151.52,295.02,63.67,7.86">A corpus of full-text journal articles is a robust evaluation tool for revealing differences in performance of biomedical natural language processing tools</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lanfranchi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Warner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">L</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Roeder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Funk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Malenkiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">A B</forename><surname>Jr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">E</forename><surname>Hunter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,261.90,295.02,83.27,7.86">BMC Bioinformatics</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct coords="13,142.62,305.98,337.98,7.86;13,151.52,316.93,86.53,7.86" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="13,217.36,305.98,144.23,7.86">The TREC question answering track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,367.80,305.98,112.79,7.86;13,151.52,316.93,12.29,7.86">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="361" to="378" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,327.89,337.98,7.86;13,151.52,338.85,329.07,7.86;13,151.52,349.81,329.07,7.86;13,151.52,360.77,235.70,7.86" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="13,254.94,327.89,225.65,7.86;13,151.52,338.85,115.62,7.86">Automatically extracting information needs from ad hoc clinical questions</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">G</forename><surname>Cao</surname></persName>
		</author>
		<ptr target="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2655957/" />
	</analytic>
	<monogr>
		<title level="m" coord="13,300.70,338.85,175.67,7.86">AMIA Annual Symposium Proceedings</title>
		<imprint>
			<publisher>American Medical Informatics Association</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page">96</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
