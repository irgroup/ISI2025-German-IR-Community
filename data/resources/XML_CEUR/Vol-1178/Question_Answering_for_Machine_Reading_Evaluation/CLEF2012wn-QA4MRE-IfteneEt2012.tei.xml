<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,135.86,151.67,323.59,12.54;1,164.66,169.07,266.15,12.54">Enhancing a Question Answering system with Textual Entailment for Machine Reading Evaluation</title>
				<funder ref="#_vHNVAZT">
					<orgName type="full">Sector Operational Program for Human Resources Development</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,139.58,207.32,53.66,9.05"><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,201.53,207.32,103.38,9.05"><forename type="first">Alexandru-Lucian</forename><surname>Gînscă</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,313.25,207.32,48.65,9.05"><forename type="first">Alex</forename><surname>Moruz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">Romanian Academy Iasi Branch</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,376.63,207.32,66.97,9.05"><forename type="first">Diana</forename><surname>Trandabăț</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computer Science</orgName>
								<orgName type="institution">Romanian Academy Iasi Branch</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,232.25,218.84,53.11,9.05"><forename type="first">Maria</forename><surname>Moruz</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Center of Biblical-Philological Studies Monumenta linguae Dacoromanorum</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,293.67,218.84,66.32,9.05"><forename type="first">Emanuela</forename><surname>Boroș</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,135.86,151.67,323.59,12.54;1,164.66,169.07,266.15,12.54">Enhancing a Question Answering system with Textual Entailment for Machine Reading Evaluation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5B54D306A3A8659931F1050A7B383DAF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Question Answering for Machine Reading Evaluation</term>
					<term>Information Retrieval</term>
					<term>Textual Entailment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes UAIC 1 's Question Answering for Machine Reading Evaluation systems participating in the QA4MRE 2012 evaluation task. We submitted two types of runs, first type of runs based on our system from 2011 edition of QA4MRE, and second type of runs based on Textual Entailment system. For second types of runs, we construct the Text and the Hypothesis, asked by Textual Entailment system from initial test data (the &lt;documents&gt; tag was used to build the Text and the &lt;question&gt; and &lt;answer&gt; tags were used to build the Hypothesis). The results offered by organizer showed that second type of runs were better than first type of runs for English.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.32" lry="842.04"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As in the 2011 campaign, the Question Answering for Machine Reading Evaluation (QA4MRE<ref type="foot" coords="1,169.22,509.03,3.00,5.45" target="#foot_1">2</ref> ) task in 2012 intends to cross-evaluate the ability of systems to read and understand texts. The task focuses on reading a document and identifying the correct answer from a set of five multiple choice answers, using inferences and previously acquired background knowledge. The test data and background knowledge are related to four topics: AIDS, Climate Change, Music and Society (the same topics adopted last year <ref type="bibr" coords="1,164.54,566.89,11.31,9.05" target="#b0">[1]</ref>) and a new one i.e. Alzheimer <ref type="bibr" coords="1,312.29,566.89,10.69,9.05" target="#b1">[2]</ref>. An important note is that, for all involved languages (English, Spanish, German, Italian and Romanian), the test data was the same (parallel translations) and the background knowledge was available to all participants.</p><p>For UAIC's participation in the QA4MRE task in 2012, we used as base the system built for the 2011 QA4MRE edition <ref type="bibr" coords="1,310.66,624.40,10.74,9.05" target="#b2">[3]</ref>, which was, at its turn, an updated version of our previous systems from the 2009 and 2010 QA@CLEF editions <ref type="bibr" coords="1,439.75,635.92,10.68,9.05" target="#b3">[4]</ref>, <ref type="bibr" coords="1,456.70,635.92,10.60,9.05" target="#b4">[5]</ref>.</p><p>The base system was further improved by adapting a Textual Entailment component for the Question Answering module, similar to the approach in <ref type="bibr" coords="2,378.55,161.24,10.69,9.05" target="#b5">[6]</ref>.</p><p>The rest of the paper is structured as follows: Section 2 details the general architecture of our Question Answering system for Machine Reading Evaluation and the new textual entailment module, Section 3 presents the results and an error analysis, while the last Section discusses the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System components</head><p>In QA4MRE 2012, UAIC submitted two types of runs for Romanian and English. For the first type of runs, we use the system from the previous edition of QA4MRE 2011 <ref type="bibr" coords="2,124.82,295.67,10.68,9.05" target="#b2">[3]</ref>, consisting in modules specialized for test data processing, background knowledge indexing, snippet extraction and identification of the correct answer. For the second type of runs, we use the Romanian and the English textual entailment systems <ref type="bibr" coords="2,445.99,318.59,10.89,9.05" target="#b6">[7,</ref><ref type="bibr" coords="2,459.94,318.59,7.26,9.05" target="#b7">8]</ref>, similar to the approach detailed in <ref type="bibr" coords="2,265.25,330.11,10.60,9.05" target="#b8">[9]</ref>. The English system is similar to the Romanian system, with the difference that a part of the modules presented in subsections 2.1 were only partially used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The base architecture</head><p>In 2012, the Romanian background knowledge consisted of a collection of 184,263 documents in text format (28,826 correspond to the AIDS topic, 57,160 to Climate Change topic, 88,687 to Music and Society topic and 9,590 to Alzheimer topic). The test data consists in an XML file with 16 test documents (4 documents for each of the four topics), 10 questions for each document (160 questions in total) and 5 possible answers for each question (800 possible answers in total).</p><p>The base architecture is similar to the system used for the 2011 edition of the QA4MRE competition, presented in <ref type="bibr" coords="2,288.46,489.73,10.66,9.05" target="#b2">[3]</ref>. Thus, after indexing the background collection using Lucene<ref type="foot" coords="2,225.53,498.98,3.24,5.89" target="#foot_2">3</ref> libraries, the system processes the test data applying 3 operations: (a) extracting documents from the background knowledge, (b) analyzing the test questions and (c) processing possible answers. If the first step is performed using Lucene indexing of the background collection, for analyzing the question we used our question processing module <ref type="bibr" coords="2,286.13,547.21,11.58,9.05" target="#b0">[1]</ref> and the web services available both for Romanian and English from the Sentimatrix<ref type="foot" coords="2,309.89,558.47,3.00,5.45" target="#foot_3">4</ref> project <ref type="bibr" coords="2,348.84,558.73,16.72,9.05" target="#b9">[10]</ref> to eliminate stop words, perform lemmatization and identify the Named Entity in the question. Then, a Lucene query is build. For instance, in the case of the question with q_id = "8":</p><formula xml:id="formula_0" coords="2,124.82,604.87,345.78,17.44">Ro: Care dintre următoarele nu este o cauză a vulnerabilității</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>femeilor căsătorite față de infecțiile cu HIV?</head><p>En: Which of the following is not a cause of HIV infection for married women? the execution of the above steps has the following results:</p><p>-in the first step, the following stop words are eliminated: care, dintre, următoarele, o, a, de, cu (En: which, of, following, a, for); -in the next step, lemmas for the words cauză, vulnerabilității, femeilor, infecțiile (En: cause, vulnerability, women, infections) are identified; -in the third step, HIV is identified as a Named Entity; -in the last step, the Lucene query is build: "nu (cauză^2 cauza)</p><formula xml:id="formula_1" coords="3,154.22,254.30,316.45,19.36">(vulnerabilității^2 vulnerabilitate) (femeilor^2 femeie) (căsătorite^2 căsătorit) (infecțiile^2 infecție) HIV^3".</formula><p>From the above Lucene query, one can notice that we consider named entities to be of most relevance (hence receiving a boost of 3, expressed as using the ^ operator), while the inflected form of the words existing in the question receive a lower boost value (2 in the example above).</p><p>Another module analyzes the possible answers types and features, using the ontology presented in <ref type="bibr" coords="3,215.69,345.71,15.34,9.05" target="#b10">[11]</ref>, more specifically the relations between regions and cities and the relations between cities and countries, in order to eliminate the answers with low probability to be the required answer. For instance, for the question:</p><p>Ro: În ce stat american oamenii de știință universitari au calculat că pentru combaterea SIDA în Africa fiecare american trebuie să plătească un cost de 5 dolari anual?</p><p>En: In what American state did university scientists calculate the cost to each American of spending 5 dollars annually to combat AIDS in Africa?, we eliminate from the list of possible answers the answers with non-American states.</p><p>As presented in <ref type="bibr" coords="3,209.81,478.69,10.66,9.05" target="#b2">[3]</ref>, the index of background knowledge is queried, and all retrieved documents are placed in separate indexes. The results of this step are 160 separate indexes for every question from the initial test data. Then every index is searched for every answer, and a list of documents with Lucene relevance scores are returned, where Score(d, a) is the relevance score for document d when we search with the Lucene query associated to the answer a. Finally, a normalized value is computed for all answers associated to a question, and the answer with the highest value is selected as the most probable answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Enhancing the base architecture with Textual Entailment</head><p>The architecture of the components that used the Textual Entailment (TE) system is presented in Figure <ref type="figure" coords="3,206.05,626.80,3.84,9.05" target="#fig_0">1</ref>, being based on the system used in AVE exercises in 2007 and in 2008 <ref type="bibr" coords="3,159.06,638.32,11.79,9.05" target="#b8">[9]</ref> and being similar to the architecture of one of the best systems from the QA4MRE 2011 edition <ref type="bibr" coords="3,221.33,649.72,15.34,9.05" target="#b12">[13]</ref>.</p><p>The steps executed by our system are as follows:</p><p> We build a pattern with variables for every question according to the question type;</p><p> Using a pattern and all possible answers, we build a set with 5 hypotheses for each of the questions: H 1 , H 2 , H 3 , H 4 , H 5 ;  We assign to the document tag from the initial XML file the role of text T and we run the TE system for all obtained pairs: (T, H 1 ), (T, H 2 ), (T, H 3 ), (T, H 4 ), (T, H 5 ).</p><p>Finally, we consider the candidate from the hypothesis for which we obtain the greatest global fitness to be the correct answer for a current question. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pattern building</head><p>In order to use the TE system for ranking the possible answers in the QA4MRE task, all these questions are first transformed according to the algorithm presented in <ref type="bibr" coords="4,444.31,489.13,15.36,9.05" target="#b13">[14]</ref>.</p><p>For example, for the following question we have:</p><p>Question: What is the goal of the ABC strategy?</p><p>Our program generates the following pattern:</p><p>Pattern: ANSWER is the goal of the ABC strategy.</p><p>where ANSWER is the variable in this case. We generate specific patterns according to the following answer types: Measure (How many, How much), Person (Who, Name), Location (In what), Date (On what date, When) and Other. Following the building of the pattern, we proceed to constructing the corresponding hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothesis building</head><p>Using the pattern building mechanism above and the answers provided within the QA4MRE input XML data, we built the corresponding hypotheses. For example, for the question above, we built, according to the answers from the English test data, the following hypotheses: For each of these hypotheses, we consider that the corresponding text from the "document" tag as having the role of text T.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answers classification</head><p>We consider the pairs built above as input for our Textual Entailment system. After running the TE system, the global fitness values for the exemplified pairs are the following:</p><formula xml:id="formula_2" coords="5,136.22,354.35,136.95,63.86">GlobalFitness(H 1 , T) = 2.1854732 GlobalFitness(H 2 , T) = 1.3577608 GlobalFitness(H 3 , T) = 1.92097 GlobalFitness(H 4 , T) = 2.2404695 GlobalFitness(H 5 , T) = 2.2766914</formula><p>Since in the considered case the highest value is obtained for the answer 5 "promoting the prevention and treatment programs", we consider it as the most probable answer. The NOA answers were considered the pairs for which we have the maximum value for GlobalFitness very close to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Evaluation</head><p>For the QA4MRE 2012 task, our team submitted 10 runs, out of which 5 were for the Romanian-Romanian language pair and 5 for the English-English pair.</p><p>The evaluation of the results is done from two different perspectives in a similar manner as in the 2011 QA4MRE edition. The first one is equivalent to a traditional evaluation in which all the answers are gathered in a single set which is then compared to a gold standard, not taking into account the document associated with a particular answer. On the other hand, the reading perspective offers insight on how well the system "understands" a particular document. At first, the C@1 measures of each test comprising of 10 questions per document are taken into consideration. These results are then used to obtain statistical measures, such as the mean, average and standard deviation over values grouped by topic or as an overall view.</p><p>In the following 4 tables, we detail the result obtained by each of the 5 different configurations for Romanian and other 5 configurations for English. In each case, the first two configurations (C1 and C2) refer to the first architecture design. The difference between C1 and C2 represents the difference in choosing the threshold for providing the "NOA" response. Our intent was to evaluate the impact of a more permissive configuration, which gives less "NOA" answers versus a more restrictive one. The last three configurations represent runs in which the architecture involving Textual Entailment system was used. The difference between these three configurations resides, as in the case of the first two, in the different choice of threshold for the "NOA" answers. We tested a permissive, a moderate and a restrictive threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation at the question answering level</head><p>In Table <ref type="table" coords="6,160.99,274.79,3.76,9.05" target="#tab_1">1</ref>, we present the results for the 5 runs on Romanian and in Table <ref type="table" coords="6,424.83,274.79,3.76,9.05" target="#tab_2">2</ref>, the same results are provided for the 5 runs on English. As can be seen in Table <ref type="table" coords="6,233.91,440.41,3.76,9.05" target="#tab_1">1</ref>, the best result of our system in terms of C@1 measure is obtained for the run in which the first type of architecture was used together with a slightly more permissive threshold for the unanswered questions. Contrary to this, for English, two out of the three query reformulation runs outperform the best result of the first two configurations. This shift can be explained by the increased effectiveness of the patterns applied for query rewriting when working on the English language. We can observe the influence of the correctly unanswered questions in the C@1 measure when comparing the number of right answers for the best run for Romanian, with the best from the English runs. Although in the Ro-Ro run, a higher number of questions were correctly answered (38 right answers) than in the En-En run (34 right answers), the C@1 measure obtained for the English run (0.28) is higher than the one given by the best Romanian run (0.25). This is explained by the difference in the number of correctly unanswered questions.</p><p>A common denominator between the results for Romanian and those for English is that a balanced threshold provided the best results. This is best observed when comparing the last three configurations both for English and for Romanian. For example, in Table <ref type="table" coords="7,207.77,230.24,3.76,9.05" target="#tab_1">1</ref>, the C4 configuration in which there were 24 unanswered questions outperformed the C3 (13 unanswered questions) and C5 (72 unanswered questions). The same pattern is found in Table <ref type="table" coords="7,312.61,253.28,3.76,9.05" target="#tab_2">2</ref>, for the En-En runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation at the reading test level</head><p>In Table <ref type="table" coords="7,161.71,309.35,3.76,9.05" target="#tab_3">3</ref>, we present the median and mean for each of the 4 topics, Topic1 (AIDS), Topic2 (Climate Change), Topic3 (Music and society) and Topic4 (Alzheimer) and their overall values for the Ro-Ro runs. In Table <ref type="table" coords="7,325.95,332.27,3.76,9.05" target="#tab_4">4</ref>, the same results are provided for the En-En runs. These results in term of average and median are consistent with the trend introduced in Table <ref type="table" coords="8,207.85,161.24,4.98,9.05" target="#tab_1">1</ref> and<ref type="table" coords="8,233.83,161.24,29.65,9.05" target="#tab_2">Table 2</ref>. The best overall mean was obtained for the third configuration on English and the second one, on Romanian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Error analysis</head><p>In extension to the analysis carried out above, we have also performed an error analysis over the reported results. The analysis was carried out exclusively over the questions in topic 2 (the topic was arbitrarily chosen), and a report of the most relevant error sources is given below. In interpreting the analysis results, two important factors need to be taken into account:</p><p> Firstly, the submitted runs are grouped according to the basic philosophy regarding query generation. In the first case (the first three submitted runs), queries were generated on the basis of the question alone, and then the potential answer was searched in the top scoring results. In the second case (the last two submitted runs), we generated 5 queries for each question, one for each potential answer (the potential answer was included in the query). The textual entailment system was also used in the second case.  Secondly, the various runs for each case were obtained by tweaking the threshold at which the system decided to provide an answer. One of the first types of error we have encountered is the fact that the second type of runs has different queries than the first type. This is the case of the first question in the second topic, reading 5:</p><p>Ro: Care dintre următorii este un biocombustibil? En: Which of the following is a biofuel?</p><p>The first three runs provide the correct answers (using the method described above), while the last two run have wrong answers. This is due to the fact that the query (următorii^2 următor) biocombustibil etanol provides lower scoring snippets than the query (următorii^2 următor) biocombustibil carbon (the correct answer is "ethanol", but the provided answer is "carbon"). Upon examining the snippets returned by the five queries extracted for this particular question, we have discovered that the correct answer scored fourth overall, which practically excludes the correct answer from consideration. This type of error was also encountered for questions 4, 5 and 6, reading 5 in the 2 nd topic.</p><p>A more subtle type of error is the one which generated an incorrect answer for the question 2, reading 5, topic 2. In this case, regardless of the manner of creating the query, the chance of obtaining the correct answer is low because of the nature of the base text. The fault comes from the answer extraction module, which is unable to solve coreference, and therefore cannot extract the correct answer Ro: ... combustibilul lichid este atât de valoros. Până în prezent, este câştigătorul evident atunci când avem nevoie de energie pentru transport -în special transport aerian şi transport maritim greu pe distanţe mari -deoarece ne permite să înghesuim o grămadă de energie într-un spaţiu de stocare relativ mic şi să realimentăm cu ușurință... Some errors are caused by the query generation module, such as the case of question 3, reading test 5, topic 2. In this instance, none of the five submitted runs provided the correct answer, mainly because most of the query words are not found in the vicinity of the correct answer. The query generated by the question analysis module is:</p><p>Ro: (poate^2 putea) (mărită^2 mări) (cantitatea^2 cantitate) (culturi^2 cultură) (cultivate^2 cultivat) simultan bucată pământ to which the system then adds the query:</p><formula xml:id="formula_3" coords="9,124.82,317.45,345.92,17.53">Ro: (folosind^2 folosi) (culturi^2 cultură) (anuale^2 anual) (succesive^2 succesiv)</formula><p>for the expected answer. The text span which contains the correct answer is:</p><p>Ro: A doua premiză greșită din scenariile cele mai nefavorabile este aceea că de pe aceeași suprafață de teren nu se pot obține mai multe recolte. Amestecurile perene pentru biocombustibili celulozici ar putea fi, de fapt, cultivate alături de culturile anuale sau, pe acelaşi teren, între recoltare şi însămânțare... and we can see that some of the keywords of the query are not found within it. The only way in which the system could solve this is by using synonyms for the keywords (in this case, culturi in the question and recolte in the answer). The fact that most of the keywords in the query are not found in the supporting text is also highlighted by the fact that the system did not provide an answer for this question in run 5, because of the low score of all the retrieved snippets. The same type of error was observed in the case of question 9, reading 5, topic 2 and question 6, reading 6, topic 2. An extreme case of this error can be seen in question 1, reading 6, topic 2, where none of the runs gave any answer, although the extracted answer was correct, because of the los score of the supporting snippets.</p><p>In some cases, errors arise from the addition of the second query in the case of the first three runs. This can be seen for the answer generated for question 7, reading 5, topic 2, where the initial query provides high scoring snippets which contain the correct solution:</p><p>Ro: O parte din problemă provine din apetitul deosebit al porumbului pentru stimulente, cum ar fi îngrăşămintele. but these snippets are then penalized because of the second query (in this particular case, the secondary query (absoarbe^2 absorbi) (cantități^2 cantitate) (reduse^2 reduce) (gaze^2 gaz) efect seră introduces a far score for a different snippet, because of the high number of keywords compared to the correct solution query, nevoie (cantități^2 cantitate) (mari^2 mare) fertilizatori^2). This type of issue could be corrected to some extent by the use of synonymy, which would increase the score of the correct snippet. This issue can also be seen in the case of question 8, reading 5, topic 2 and questions 2 and 4, reading 6, topic 2.</p><p>A type of error that stems from the lack of sufficient background knowledge can be found for question 10, reading 5, topic 2:</p><p>Ro: Care este biocombustibilul a cărui producție reduce cel mai mult emisiile de gaze cu efect de seră?</p><p>En: Which is the biofuel which reduces greenhouse gas emissions most?)</p><p>The correct answer for this question, etanol celulozic (En: cellulose ethanol) cannot be found as such in the text, although it is referred in another form:</p><p>Ro: o versiune celulozică de etanol En: a cellulosic version of ethanol as can be seen in the snippet containing the correct answer:</p><p>Ro: Există o mulţime de moduri diferite de a face biocombustibili celulozici, inclusiv o versiune celulozică de etanol, şi ei reduc emisiile cu un procent enorm, între 82% și 87%.</p><p>This type of problem can only be corrected by the appropriate background knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This paper presents the updated Question Answering system developed by UAIC for the Machine Reading Evaluation task within CLEF 2012 labs. The presented systems were built starting from the main components of our QA systems (the question processing and information retrieval modules), but the multiple choice questions were addressed using a textual entailment component.</p><p>The evaluation shows a best overall median for all 4 topics of 0.29 for both the Romanian and English monolingual tasks. We can observe the influence of the correctly unanswered questions in the C@1 measure when comparing the number of right answers for the best run for Romanian, with the best from the English runs. Although in the Ro-Ro run, a higher number of questions were correctly answered (38 right answers) than in the En-En run (34 right answers), the C@1 measure obtained for the English run (0.28) is higher than the one given by the best Romanian run (0.25). This is explained by the difference in the number of correctly unanswered questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,144.98,442.69,305.51,9.05"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The second architecture based on Textual Entailment (TE) system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,175.22,309.65,245.61,114.86"><head>Table 1 :</head><label>1</label><figDesc>Results of UAIC's Ro-Ro runs at question answering level</figDesc><table coords="6,175.58,327.68,245.25,96.84"><row><cell></cell><cell>C1</cell><cell>C2</cell><cell>C3</cell><cell>C4</cell><cell>C5</cell></row><row><cell>Answered right</cell><cell>34</cell><cell>38</cell><cell>34</cell><cell>33</cell><cell>21</cell></row><row><cell>Answered wrong</cell><cell>114</cell><cell>111</cell><cell>113</cell><cell>104</cell><cell>67</cell></row><row><cell>Total answered</cell><cell>148</cell><cell>149</cell><cell>147</cell><cell>137</cell><cell>88</cell></row><row><cell>Unanswered right</cell><cell>3</cell><cell>3</cell><cell>4</cell><cell>0</cell><cell>12</cell></row><row><cell>Unanswered wrong</cell><cell>9</cell><cell>8</cell><cell>9</cell><cell>23</cell><cell>60</cell></row><row><cell>Total unanswered</cell><cell>12</cell><cell>11</cell><cell>13</cell><cell>23</cell><cell>72</cell></row><row><cell>Overall accuracy</cell><cell>0.21</cell><cell>0.24</cell><cell>0.21</cell><cell>0.21</cell><cell>0.13</cell></row><row><cell>C@1 measure</cell><cell>0.23</cell><cell>0.25</cell><cell>0.23</cell><cell>0.24</cell><cell>0.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,175.58,521.23,245.25,115.01"><head>Table 2 :</head><label>2</label><figDesc>Results of UAIC's En-En runs at question answering level</figDesc><table coords="6,175.58,539.26,245.25,96.99"><row><cell></cell><cell>C1</cell><cell>C2</cell><cell>C3</cell><cell>C4</cell><cell>C5</cell></row><row><cell>Answered right</cell><cell>34</cell><cell>23</cell><cell>34</cell><cell>37</cell><cell>25</cell></row><row><cell>Answered wrong</cell><cell>96</cell><cell>65</cell><cell>78</cell><cell>104</cell><cell>62</cell></row><row><cell>Total answered</cell><cell>130</cell><cell>88</cell><cell>112</cell><cell>141</cell><cell>87</cell></row><row><cell>Unanswered right</cell><cell>7</cell><cell>16</cell><cell>6</cell><cell>3</cell><cell>15</cell></row><row><cell>Unanswered wrong</cell><cell>23</cell><cell>54</cell><cell>42</cell><cell>16</cell><cell>58</cell></row><row><cell>Total unanswered</cell><cell>30</cell><cell>72</cell><cell>48</cell><cell>19</cell><cell>73</cell></row><row><cell>Overall accuracy</cell><cell>0.21</cell><cell>0.14</cell><cell>0.21</cell><cell>0.23</cell><cell>0.16</cell></row><row><cell>C@1 measure</cell><cell>0.25</cell><cell>0.21</cell><cell>0.28</cell><cell>0.26</cell><cell>0.23</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,180.14,367.13,240.69,137.20"><head>Table 3 :</head><label>3</label><figDesc>Results of UAIC's Ro-Ro runs at reading test level</figDesc><table coords="7,180.14,385.16,240.69,119.18"><row><cell></cell><cell>C1</cell><cell>C2</cell><cell>C3</cell><cell>C4</cell><cell>C5</cell></row><row><cell>Topic 1 median</cell><cell>0.16</cell><cell>0.26</cell><cell>0.26</cell><cell>0.24</cell><cell>0.18</cell></row><row><cell>Topic 2 median</cell><cell>0.31</cell><cell>0.31</cell><cell>0.31</cell><cell>0.20</cell><cell>0.21</cell></row><row><cell>Topic 3 median</cell><cell>0.20</cell><cell>0.20</cell><cell>0.15</cell><cell>0.18</cell><cell>0.07</cell></row><row><cell>Topic 4 average</cell><cell>0.29</cell><cell>0.29</cell><cell>0.21</cell><cell>0.28</cell><cell>0.21</cell></row><row><cell>Overall median</cell><cell>0.24</cell><cell>0.29</cell><cell>0.26</cell><cell>0.23</cell><cell>0.16</cell></row><row><cell>Topic 1 average</cell><cell>0.18</cell><cell>0.28</cell><cell>0.26</cell><cell>0.25</cell><cell>0.16</cell></row><row><cell>Topic 2 average</cell><cell>0.27</cell><cell>0.27</cell><cell>0.27</cell><cell>0.25</cell><cell>0.26</cell></row><row><cell>Topic 3 average</cell><cell>0.20</cell><cell>0.20</cell><cell>0.18</cell><cell>0.18</cell><cell>0.07</cell></row><row><cell>Topic 4 average</cell><cell>0.26</cell><cell>0.26</cell><cell>0.19</cell><cell>0.26</cell><cell>0.20</cell></row><row><cell>Overall average</cell><cell>0.23</cell><cell>0.25</cell><cell>0.22</cell><cell>0.23</cell><cell>0.17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,180.14,530.35,240.69,137.21"><head>Table 4 :</head><label>4</label><figDesc>Results of UAIC's En-En runs at reading test level</figDesc><table coords="7,180.14,548.38,240.69,119.19"><row><cell></cell><cell>C1</cell><cell>C2</cell><cell>C3</cell><cell>C4</cell><cell>C5</cell></row><row><cell>Topic 1 median</cell><cell>0.25</cell><cell>0.30</cell><cell>0.31</cell><cell>0.30</cell><cell>0.33</cell></row><row><cell>Topic 2 median</cell><cell>0.21</cell><cell>0.23</cell><cell>0.25</cell><cell>0.21</cell><cell>0.22</cell></row><row><cell>Topic 3 median</cell><cell>0.20</cell><cell>0.14</cell><cell>0.28</cell><cell>0.23</cell><cell>0.17</cell></row><row><cell>Topic 4 average</cell><cell>0.20</cell><cell>0.08</cell><cell>0.22</cell><cell>0.18</cell><cell>0.16</cell></row><row><cell>Overall median</cell><cell>0.21</cell><cell>0.16</cell><cell>0.29</cell><cell>0.26</cell><cell>0.22</cell></row><row><cell>Topic 1 average</cell><cell>0.28</cell><cell>0.29</cell><cell>0.34</cell><cell>0.34</cell><cell>0.32</cell></row><row><cell>Topic 2 average</cell><cell>0.22</cell><cell>0.22</cell><cell>0.24</cell><cell>0.24</cell><cell>0.20</cell></row><row><cell>Topic 3 average</cell><cell>0.23</cell><cell>0.17</cell><cell>0.25</cell><cell>0.24</cell><cell>0.18</cell></row><row><cell>Topic 4 average</cell><cell>0.22</cell><cell>0.08</cell><cell>0.22</cell><cell>0.19</cell><cell>0.16</cell></row><row><cell>Overall average</cell><cell>0.23</cell><cell>0.19</cell><cell>0.26</cell><cell>0.25</cell><cell>0.21</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,130.10,675.82,149.65,8.18"><p>University "Al. I. Cuza" of Iasi, Romania</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,130.10,686.38,182.95,8.18"><p>QA4MRE: http://celct.fbk.eu/QA4MRE/index.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,130.10,675.82,120.13,8.18"><p>Lucene: http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,130.10,686.38,143.74,8.18"><p>Sentimatrix: http://www.sentimatrix.eu/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. The research presented in this paper was funded by the <rs type="funder">Sector Operational Program for Human Resources Development</rs> through the project "<rs type="projectName">Development of the innovation capacity and increasing of the research impact through post</rs><rs type="programName">-doctoral programs</rs>" <rs type="grantNumber">POSDRU/89/1.5/S/49944</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_vHNVAZT">
					<idno type="grant-number">POSDRU/89/1.5/S/49944</idno>
					<orgName type="project" subtype="full">Development of the innovation capacity and increasing of the research impact through post</orgName>
					<orgName type="program" subtype="full">-doctoral programs</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="11,128.23,261.05,342.41,8.18;11,138.38,271.37,332.44,8.18;11,138.38,281.69,277.33,8.18" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,225.88,261.05,155.65,8.18">A Simple Measure to Assess Non-response</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,398.16,261.05,72.48,8.18;11,138.38,271.37,332.44,8.18;11,138.38,281.69,113.93,8.18">Proceedings of 49th Annual Meeting of the Association for Computational Linguistics -Human Language Technologies (ACL-HLT 2011)</title>
		<meeting>49th Annual Meeting of the Association for Computational Linguistics -Human Language Technologies (ACL-HLT 2011)<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">June 19-24. (2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,128.23,292.01,342.18,8.18;11,138.38,302.45,332.08,8.18;11,138.38,312.77,332.23,8.18;11,138.38,323.09,188.41,8.18" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,237.74,302.45,232.72,8.18;11,138.38,312.77,104.97,8.18">Overview of QA4MRE at CLEF 2012: Question Answering for Machine Reading Evaluation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Á</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sporleder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Forascu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Benajiba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Osenova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,250.11,312.77,220.50,8.18;11,138.38,323.09,22.50,8.18">CLEF 2012 Evaluation Labs and Workshop Working Notes Papers</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09-20">17-20 September, 2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,128.23,333.41,342.54,8.18;11,138.38,343.85,332.14,8.18;11,138.38,354.17,295.23,8.18" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,383.47,333.41,87.30,8.18;11,138.38,343.85,240.82,8.18">Question Answering for Machine Reading Evaluation on Romanian and English Languages</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Gînscă</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moruz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Trandabăţ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Husarciuc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,385.72,343.85,84.80,8.18;11,138.38,354.17,103.58,8.18">Notebook Paper for the CLEF 2011 LABs Workshop</title>
		<meeting><address><addrLine>Amsterdam, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-09-22">19-22 September. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,128.23,364.49,342.50,8.18;11,138.38,374.81,332.07,8.18;11,138.38,385.13,332.08,8.18;11,138.38,395.57,205.45,8.18" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,438.17,364.49,32.56,8.18;11,138.38,374.81,180.64,8.18">Question Answering on English and Romanian Languages</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Trandabăţ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moruz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Pistol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Husarciuc</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cristea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,424.81,374.81,41.63,8.18;11,409.33,385.13,61.14,8.18;11,138.38,395.57,43.01,8.18">I Text Retrieval Experiments</title>
		<title level="s" coord="11,220.21,385.13,164.62,8.18">Multilingual Information Access Evaluation</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6241</biblScope>
			<biblScope unit="page" from="229" to="236" />
		</imprint>
	</monogr>
	<note>CLEF 2009</note>
</biblStruct>

<biblStruct coords="11,128.23,405.89,342.32,8.18;11,138.38,416.21,332.60,8.18;11,138.38,426.53,125.61,8.18" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,339.75,405.89,130.80,8.18;11,138.38,416.21,111.31,8.18">Question Answering on Romanian, English and French Languages</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Trandabăţ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moruz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Husarciuc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,256.58,416.21,194.57,8.18">Notebook Paper for the CLEF 2010 LABs Workshop</title>
		<meeting><address><addrLine>Padua, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09-23">22-23 September. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,128.23,436.99,342.58,8.18;11,138.38,447.31,332.20,8.18;11,138.38,457.63,332.28,8.18;11,138.38,467.95,59.59,8.18" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,266.88,436.99,203.92,8.18;11,138.38,447.31,38.01,8.18">Improving a QA System for Romanian Using Textual Entailment</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Balahur-Dobrescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,201.09,447.31,269.49,8.18;11,138.38,457.63,164.44,8.18">Proceedings of RANLP workshop &quot;A Common Natural Language Processing Paradigm For Balkan Languages</title>
		<meeting>RANLP workshop &quot;A Common Natural Language Processing Paradigm For Balkan Languages<address><addrLine>Borovets, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09-26">September 26, 2007. 2007</date>
			<biblScope unit="page" from="7" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,128.23,478.39,342.54,8.18;11,138.38,488.71,240.41,8.18" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<title level="m" coord="11,179.37,478.39,70.38,8.18">Textual Entailment</title>
		<meeting><address><addrLine>Iasi, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-10">October, 2009. 2009</date>
		</imprint>
		<respStmt>
			<orgName>University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Al. I. Cuza</note>
</biblStruct>

<biblStruct coords="11,128.23,499.03,342.31,8.18;11,138.38,509.35,332.48,8.18;11,138.38,519.79,216.28,8.18" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="11,261.61,499.03,208.93,8.18;11,138.38,509.35,283.16,8.18">Textual Entailment on Romanian. The third Workshop on Romanian Linguistic Resources and Tools for Romanian Language Processing</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Balahur-Dobrescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-12">December. 2007</date>
			<biblScope unit="page" from="14" to="15" />
			<pubPlace>Iasi, Romania</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,128.23,530.11,342.35,8.18;11,138.38,540.43,332.09,8.18;11,138.38,550.75,332.44,8.18;11,138.38,561.19,332.35,8.18;11,138.38,571.51,58.55,8.18" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,234.77,530.11,219.28,8.18">Answer Validation on English and Romanian Languages</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Balahur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,138.38,540.43,332.09,8.18;11,138.38,550.75,189.72,8.18">Evaluating Systems for Multilingual and Multimodal Information Access, 9th Workshop of the Cross-Language Evaluation Forum, CLEF 2008</title>
		<title level="s" coord="11,257.56,561.19,131.09,8.18">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">September 17-19, 2008. 2009. 2009</date>
			<biblScope unit="volume">5706</biblScope>
			<biblScope unit="page" from="385" to="392" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="11,132.40,581.59,338.06,8.18;11,138.38,592.15,332.23,8.18;11,138.38,602.59,332.46,8.18;11,138.38,612.94,238.43,8.18" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,183.82,592.15,200.02,8.18">Sentimatrix -Multilingual Sentiment Analysis Service</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Gînscă</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Boroș</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Trandabăţ</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Toader</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Corîci</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">A</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cristea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,401.51,592.15,69.09,8.18;11,138.38,602.59,332.46,8.18;11,138.38,612.94,73.85,8.18">Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (ACL-WASSA2011)</title>
		<meeting>the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (ACL-WASSA2011)<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">June 19-24. (2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,623.26,338.23,8.18;11,138.38,633.58,332.60,8.18;11,138.38,644.02,136.67,8.18;11,124.82,654.34,7.58,8.18" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,269.99,623.26,184.73,8.18">Named Entity Relation Mining Using Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Balahur-Dobrescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,138.38,633.58,332.60,8.18;11,138.38,644.02,9.05,8.18">Proceedings of the Sixth International Language Resources and Evaluation (LREC&apos;08). 28-30</title>
		<meeting>the Sixth International Language Resources and Evaluation (LREC&apos;08). 28-30<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-05">May. 2008</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,654.34,173.98,8.18" xml:id="b11">
	<monogr>
		<ptr target="http://lucene.apache.org/java/docs/" />
		<title level="m" coord="11,138.38,654.34,32.57,8.18">LUCENE</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.40,664.66,338.47,8.18;11,138.38,674.98,332.06,8.18;12,138.38,149.54,332.38,8.18;12,138.38,159.98,118.48,8.18" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,464.37,664.66,6.50,8.18;11,138.38,674.98,332.06,8.18;12,138.38,149.54,36.63,8.18">A Hybrid Question Answering System based on Information Retrieval and Answer Validation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pakray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Chandra Pal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,183.84,149.54,210.28,8.18">Notebook Paper for the CLEF 2011 LABs Workshop</title>
		<meeting><address><addrLine>Amsterdam, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-09-22">19-22 September. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.40,170.30,338.15,8.18;12,138.38,180.62,332.16,8.18;12,138.38,190.94,332.09,8.18;12,138.38,201.38,24.09,8.18" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,138.38,180.62,243.73,8.18">The Second PASCAL Recognising Textual Entailment Challenge</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bar-Haim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Szpektor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,400.84,180.62,69.70,8.18;12,138.38,190.94,276.74,8.18">Proceedings of the Second PASCAL Challenges Workshop on Recognizing Textual Entailment</title>
		<meeting>the Second PASCAL Challenges Workshop on Recognizing Textual Entailment<address><addrLine>Venice. Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
