<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.58,115.96,312.20,12.62">Adaptation of LIMSI&apos;s QALC for QA4MRE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,154.72,153.63,59.21,8.74"><forename type="first">Brigitte</forename><surname>Grau</surname></persName>
						</author>
						<author>
							<persName coords="1,230.82,153.63,64.07,8.74"><forename type="first">Van-Minh</forename><surname>Pho</surname></persName>
						</author>
						<author>
							<persName coords="1,311.78,153.63,86.93,8.74"><forename type="first">Anne-Laure</forename><surname>Ligozat</surname></persName>
						</author>
						<author>
							<persName coords="1,415.60,153.63,45.03,8.74;1,182.06,165.58,32.65,8.74"><forename type="first">Asma</forename><forename type="middle">Ben</forename><surname>Abacha</surname></persName>
						</author>
						<author>
							<persName coords="1,225.27,165.58,88.75,8.74"><forename type="first">Pierre</forename><surname>Zweigenbaum</surname></persName>
						</author>
						<author>
							<persName coords="1,343.96,165.58,78.54,8.74"><forename type="first">Faisal</forename><surname>Chowdhury</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">LIMSI-CNRS</orgName>
								<address>
									<addrLine>rue John von Neumann</addrLine>
									<postCode>91403</postCode>
									<settlement>Orsay cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Université Paris-Sud</orgName>
								<address>
									<postCode>91400</postCode>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">ENSIIE</orgName>
								<address>
									<addrLine>1 square de la résistance</addrLine>
									<postCode>91000</postCode>
									<settlement>Evry</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">FBK</orgName>
								<address>
									<addrLine>Via S.Croce 77</addrLine>
									<postCode>38122</postCode>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,151.58,115.96,312.20,12.62">Adaptation of LIMSI&apos;s QALC for QA4MRE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4C25FC11E1A84A1DDDEE3F7A1BB49EB6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>question answering</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present LIMSI participation to one of the pilot tasks of QA4MRE at CLEF 2012: Machine Reading of Biomedical Texts about Alzheimer. For this exercise, we adapted an existing question answering (QA) system, QALC, by searching answers in the reading document. This basic version was used for the evaluation and obtains 0.2, which was increased to 0.325 after basic corrections. We developed then different methods for choosing an answer, based on the expected answer type and the comparison between question plus answer rewritten to form hypothesis compared with candidates sentences. We also conducted studies on relation extraction by using an existing system. The last version of our system obtains 0.375.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we present LIMSI participation to one of the pilot tasks of QA4MRE at CLEF 2012: Machine Reading of Biomedical Texts about Alzheimer. The objective was to select the correct answer from a list of five possible answers, according to a corresponding document, which was a biomedical text about Alzheimer disease. For this exercise, we adapted an existing question answering (QA) system, QALC <ref type="bibr" coords="1,231.41,512.66,9.96,8.74" target="#b0">[1]</ref>. We selected candidate sentences of the document of the reading test, depending on the question and answers terms present in the sentence, and their distance, as it was done in QALC. In order to improve answer selection, we enhanced our lexicons for variant recognition, and used an existing relation extraction module to detect the semantic relations between entities. We also added two criteria for selecting answers: verification of the expected answer type, and similarity measure between question + answer and candidate sentence, according to shallow or syntactic measures.</p><p>We used the background collection in order to collect lexicons and to verify answer expected types. We built a list of terms associated with their UMLS concept, and a list of definitions extracted with pattern on the annotated collection.</p><p>In the following, we will first present the adaptation of our QA system, and after the new modules we developed. We will then present results. As our results were very different on the development set and on the test set, we randomly selected 10 questions among the 4 reading tests in order to study our errors and to augment the training set. Our official results were 8 right answers on 40 questions. After the new developments, we gained 5 correct answers on the remaining 30 questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>2.1 Adaptation of the existing Question Answering system QALC As we already disposed of a question answering system for English <ref type="bibr" coords="2,429.76,246.86,9.96,8.74" target="#b0">[1]</ref>, we used it as a basis for this task. The architecture of this adapted system, named QALC4mre, is presented in Figure <ref type="figure" coords="2,293.83,270.77,3.87,8.74" target="#fig_0">1</ref>. We reused existing modules concerning variant recognition of terms and adapted the sentence weighting scheme for passage selection, in order to integrate the presence of an answer.</p><p>We only look for answers in the document of the reading test, in full text, and passages are made of one sentence. We normalized Greek letters, in order to standardize document, questions and answers. We now detail each step of the answering process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Question analysis</head><p>The objectives of the question analysis module are the following:</p><p>determine the expected answer type: for example enzyme in the question Which enzyme is responsible for the transformation of testosterone into estrogen? ; rewrite the question to form a hypothesis: for example, from the previous question and the candidate answer aromatase, the hypothesis should be aromatase is responsible for the transformation of testosterone into estrogen.</p><p>This module was developed for QA4MRE, as types of questions and answers are very different from factual question in open domain. Determination of the expected answer type is based on the question syntactic tree. The question is parsed with the Biomodel tool <ref type="bibr" coords="3,266.51,235.74,9.96,8.74" target="#b1">[2]</ref>. Tregex and Tsurgeon <ref type="bibr" coords="3,375.48,235.74,10.52,8.74" target="#b2">[3]</ref> are used to determine the expected type of the answer according to its position in the parse tree: we basically choose a common or proper noun, that is a son of the interrogative.</p><p>Then, the question is rewritten in the assertive form thanks to Tsurgeon rules, and the answer replaces the interrogative, leading to generate five sentences which we will name hypothesis. These hypothesis will be compared to selected sentences, in order to contribute to the choice of the answer All rules were developed on a separate corpus of nearly 300 medical questions extracted from the Journal of Family Practice<ref type="foot" coords="3,339.08,330.39,3.97,6.12" target="#foot_0">1</ref> . On this set of questions, 94% expected types are correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sentence selection</head><p>Sentence selection is based on recognition of the hypothesis terms in the sentences of the reading document: question terms and answer terms.</p><p>As in QAlC, we look for multiword terms and monoword terms, either their exact formulation or their variants, by using Fastr <ref type="bibr" coords="3,356.59,432.83,9.96,8.74" target="#b3">[4]</ref>. Fastr analyses these sentences and recognizes morphological, syntactical and semantic variants of terms by applying rewriting rules on tagged documents with POS tags. For example, if the hypothesis contains the word association, the word associated will be recognized as a morphological variant. Besides uni-terms, multiword term variants are also recognized:aged male mice is recognized as a variant for old mouse, patients with AD for AD patient or regulates IDE expression for expression regulation. We noted that in the development test, some verb variants were missing, such as convert/transformation. In order to add such variants to our lexicons, we added the variants contained in the FrameNet lexical units to our lexicon. This list contains 673 entries, with several variants for each entry (for example the name modification has the following variants: the verb change, the verb convert. . . ). It contains some useful variants for our task, such as convert/transformation, but also antonyms, such as successful/failed. Adding these variants to our lexicons enabled us to recognize more terms, but did not improve the overall performance of the system.</p><p>Fastr was applied on full text tagged documents and on normalized document with terms replaced by their UMLS concept, named CUI. A list of the different CUIs associated to terms was extracted from the annotated background collection in that purpose.</p><p>Sentence weighting Each recognized term is attributed a weight, corresponding to a combination of different criteria. The criteria that we retained are those of QALC and use the following features retrieved within the candidate sentence:</p><p>question words, weighted by their specificity degree, variants of question words, exact words of the question, mutual closeness of question words.</p><p>The main item is the specificity degree of the terms. This value depends on the inverse of the term relative frequency within a large corpus of newspapers. As the task domain is Alzheimer disease, there are specific terms which do not belong to the corpus, thus they receive a weight of 1, i.e. the max value. First we compute a basic weight of the sentence based on the presence of question words within the sentence, and then we add weights from the other criteria. The computation of the basic weight of a sentence is made from lemmas (or from words if the word is unknown for the tagger), and their specificity degree. Some words are not taken into account, i.e. determinants or prepositions, transparent nouns, and auxiliary verbs. A transparent noun is a noun whose complement is semantically more relevant that the noun itself. For instance, the word kind is transparent in a question as What kind of gliar cell ... , and cell is the semantically relevant noun. We made an a priori list of such words.</p><p>Thus, the basic weight of a sentence is given by: BasicWeight = (dr1 + ... + dri + ... + drm) / (dq1 + ... + drj + ... + dqn) with: dri: dri is a question term found in the sentence. If dri is a mono-term, the specificity degree of its lemma, 0.1 otherwise, dqj: drj is a question term. If dqj is a mono-term, the specificity degree of the lemma, 0.1 otherwise, m: number of lemmas found in the sentence, n: number of lemmas in the question. Each lemma can be taken into account each time it occurs in the same sentence or only once. If a word from the question is not found in the sentence, but a variant of it, half of the specificity degree of the word is added to the basic weight of the sentence. As the elementary weights belong to [0-1], the basic weight maximum is close to one. We bring it to 1000 for convenience. We subsequently add an additional weight to this basic weight for each additional criterion that is satisfied. Each additional weight cannot be higher than about 10% of the basic weight. The criterion of mutual closeness of question words aims at representing the fact that several words are used in the same way in the question and in the sentence. Thus, it is computed between single terms arranged in pairs in the sentence. Each pair which is separated by maximum a significant word receives a weight of 0.02. The last criterion represent the ratio of lemmas found in the sentence without variation.</p><p>Thus, the final weight W of a sentence S is: W(S)= BasicWeight*1000 + MutualCloseness*1000 + ExactLemmas*100 Candidate answers are also searched in the sentences, and they are weighted by the following scheme: W(a)= BasicWeight*1000 + ExactLemmas*100</p><p>Relation extraction Semantic relation extraction consists in determining the relations linking two given named entities. This task uses a predefined context containing the two entities (source and target of the relation) which may be a sentence, a paragraph or a whole document. This textual context can be exploited with diverse techniques (morpho-syntactic analysis, dependency relations, synonymy, etc.) for the identification of semantic relations.</p><p>In the biomedical domain, several approaches tackled relation extraction between (bio)medical entities such as the treatment relation between a treatment and a disease or protein-protein interactions. However, relation extraction in the context of precise information retrieval systems such as question-answering systems is not as widely covered in the literature.</p><p>Two main methods could be described for relation extraction:</p><p>-Pattern-based / keyword-based methods which use a list of patterns or keywords to identify the semantic relation. Table <ref type="table" coords="5,356.34,383.91,4.98,8.74">1</ref> presents some examples of keywords associated to the relation "inhibit". -Machine learning methods which allow to build automated classifiers with annotated corpora. This second category of approaches is the most scalable when a sufficient number of training examples is available for the targeted relation.</p><p>Wordnet inhibit, inhibitor, inhibition, limit, block, decrease Xiao and Rosner <ref type="bibr" coords="5,231.35,493.97,12.53,7.86" target="#b4">[5]</ref> suppress, restrict, reduce, prevent, restrain Table <ref type="table" coords="5,225.50,504.85,4.13,7.89">1</ref>. Keywords examples for the "inhibit" relation</p><p>In our current work, we used a machine learning method and trained a classifier for Protein-Protein Interaction (PPI) and the Regulatory Relation (RR). The annotations of biomedical entities were provided by the application of two systems on the reference corpora: GDep parser and ABNER tagger. Chunks and named entities are represented in the BIO format (B for Begin, I for Inside, and O for Outside). Five semantic classes of medical entities were annotated: DNA, RNA, cell line, cell type, and PROTEIN. We list three example annotations of the reference corpus showing, respectively, an entity recognized by the first tool, by the second tool, then by both tools: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Answer selection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answer type validation</head><p>Corpus validation Some questions contain the expected type of the answer: for the question What experimental technique was used specifically to purify the -secretase complex?, the answer must be an experimental technique. This information can be found in corpora, for example by searching for existing hyponymy patterns between the answer and the expected answer type. We used the annotated background collection for this purpose. We extracted all dependency relations of the NMOD type (noun modifier) between two nouns, since this was the most common way the hyponymy was expressed: for example, the noun phrase affinity chromatography technique can be found in the background collection, and validates affinity chromatography as a possible answer to the previous question. Then, for each possible answer, we searched for the presence of the head of the answer and the head of the expected answer type in the previous list of relations. We attribute a score to the instantiated patterns:</p><p>-3 if all the answer belong to the extracted definition -2 if the definition contains the head word of the answer, computed as its last word if it is a noun, -1 if there are other words than the head word.</p><p>In order to enhance the recall of this method, and because other kinds of extraction patterns are difficult to conceive, we also look for cooccurrences of question words with the expected type in the full text collection, by searching short passages with Lucene, and counting cooccurrences in excerpts of sentences without punctuation marks 9 which could indicate separate phrases. The score given by this method is the number of extracted cooccurrences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UMLS validation</head><p>Given the expected type of the answer and a candidate answer, we also check whether the former subsumes the latter in an external resource with a large coverage of the biomedical domain: the UMLS Metathesaurus <ref type="bibr" coords="7,467.31,142.90,9.96,8.74" target="#b5">[6]</ref>. We do this by first projecting them to UMLS concepts, then by testing concept pairs for subsumption.</p><p>Since the background collection was annotated with UMLS concepts (Concept Unique Identifiers, or CUIs), we collected all terms annotated with CUIs into a dictionary. We then used this dictionary to annotate the expected type of the answer and each candidate answer with one or more CUIs. In case of multiple CUIs, no disambiguation was attempted. After the official submission, we also added a CUI detection method based on exact match to any UMLS string.</p><p>The UMLS Metathesaurus includes more than a hundred source vocabularies covering various sub-domains of medicine. Most of these vocabularies have a hierarchical structure which is often based on the is-a relation, but can also mix it with part of or other relations. We focused on the hierarchies of six large vocabularies included in the UMLS Metathesaurus: the Systematized Nomenclature of Medicine (SNOMED CT), the Gene Ontology, the National Cancer Institute thesaurus, the International Classification of Diseases (ICD9-CM and ICD10-CM), the MeSH thesaurus, and the Medical Drug Regulatory Activity thesaurus. We queried these hierarchies through the pathsToRoot method of the UMLS::Interface Perl module <ref type="bibr" coords="7,264.96,358.10,9.96,8.74">[7]</ref>, which provides all ancestors of a given CUI. The subsumption test was considered successful for a candidate answer if the expected type of the answer was found among its ancestors in any of the six vocabularies. Version 2011AA of the UMLS was used for these tests.</p><p>Similarity between hypothesis and sentence For selecting the correct answer among the five candidates, we consider that it must be the one that produces an assertive form which will be the closest of the supporting sentence. Thus we compute the similarity between assertive forms of the answers, named hypothesis, and sentences selected by the system. We chose metrics used originally for the recognition of textual entailment. These metrics are based on two levels of textual representation: surface and syntactic forms.</p><p>The metric based on surface forms of the hypothesis and the sentence is TERp (Translation Edit Rate plus) <ref type="bibr" coords="7,296.92,518.46,9.96,8.74" target="#b7">[8]</ref>, which computes the edit distance between hypothesis and sentence. In addition to compute the minimal number of word insertions, deletions and substitutions, TERp includes phrasal shifts. In our case, we do not compare the hypothesis with the whole sentence: we keep the part of the sentence containing words of the hypothesis.</p><p>Most of the metrics we used are based on dependency trees of the hypothesis and the sentence. Here is the list of these metrics :</p><p>the ratio of common dependencies between hypothesis and sentence to the number of hypothesis dependencies. Two dependencies are common if the father node, the relation and the child node are the same. For example, the following dependencies are equal :</p><p>• be SUB NEP</p><p>• be SUB mouse NMOD level PMOD NEP the tree edit distance between the sentence and the hypothesis. We compute the minimal cost of operations to transform the dependency tree of the sentence into the dependency tree of the hypothesis. For this, we implemented Zhang and Shasha's algorithm <ref type="bibr" coords="8,287.44,170.17,9.96,8.74" target="#b8">[9]</ref>; the ratio of common subtrees between hypothesis and sentence and the subtrees of the hypothesis. For this, we compute the tree kernel between both utterances <ref type="bibr" coords="8,199.93,207.15,14.61,8.74" target="#b9">[10]</ref>.</p><p>We also compute the common subtrees between the constituent trees of hypothesis and sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Official results</head><p>The system version used for the official evaluation was the adaptation of QALC, applied on the full text documents, tagged with TreeTagger <ref type="bibr" coords="8,410.62,346.83,15.50,8.74" target="#b10">[11]</ref> and on the documents where terms annotated with an UMLS concept in the background collection were replaced by their concept identifier. In this test, questions and answers were also annotated in the same way. The scoring scheme for sentences counted all the occurrences of a same term within them.</p><p>Two scoring measures were implemented for selecting an answer:</p><p>max sentence: selects the answer with the highest weight from the sentence which has the highest weight, most frequent: selects the most frequent answer from the N most important sentences (N=5).</p><p>The different measures produced the same scores of 8 correct answers on the test set (40 questions), while they produced 5 or 6 answers on the development set (10 questions). For studying errors, we randomly chose 10 answers among the four reading tests. Errors come from:</p><p>-Non recognized variants: synonyms, abbreviations, as familial forms of Alzheimer's disease vs FAD, and collocations, as medical condition vs disease, -Ambiguity among several answers inside the correct sentence, -Problem in the normalisation process (characters not encoded in utf-8, remaining Greek letters).</p><p>We then realized new tests, on the remaining 30 questions, after correction of the normalization of documents, and by adding type verification and similarity measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Type checking</head><p>UMLS subsumption could be checked on the test set when UMLS concept identifiers (CUIs) were detected for both a candidate answer and the expected answer type. This happened for 17 {answer, expected answer type} pairs, and the subsumption test was positive for 3 of these. For instance, for question Which hormone can control the expression of CLU isoforms?, the expected answer type was hormone (CUI C0019932), and only one answer (androgen, CUI C0002844 or C0919646, which is the correct answer) had a CUI which was subsumed by C0019932.</p><p>Type verification by computing cooccurrences was launched on 15 passages. We integrated type verification in the background collection in QALC4mre by computing two weights for the answers:</p><p>cooc: Number of occurrences * 10 + pattern score, pat: pattern score * 100 + occurrence number.</p><p>The score cooc gives priority to the robust method, while pat favors precision. These scores were used in place of the weight computed according to the found terms. We computed two measures, by considering only not null scores for both scores, cooc0 and pat0, or not, cooc and pat. Results are shown in table <ref type="table" coords="9,458.88,346.71,3.87,8.74" target="#tab_0">2</ref>.</p><p>Sentences were ordered by the same weighting scheme of the official evaluation, sent weight all, and by considering only once multiple question terms in the sentence, sent weight 1. In place of tagging documents of the reading tests, we used their annotated version given by the organizers. The correction of the documents and the use of the annotated reading documents lead to improve the system which was used for the official evaluation: 9 and 12 answers on 30 questions, column sent weight all, lines weight and most frequent. On the 10 questions removed, we only find 1 answer. Thus our new score is 0.325 on the initial test set with the evaluation version of QALC4mre corrected.</p><p>We can see that the best sentence weighting is obtained by counting only once question terms in sentences. It was the initial weighting scheme of QALC; however, on the development set, it seemed better to account for all the occurrences, as sentences were often long. In the test set, as many question terms are not recognized, the waiting scheme sent weight all overweights sentences with multiple occurrences of few question words, bypassing sentences with more different terms.</p><p>The measure which selects most frequent answers in the top 5 sentences shows the better results on the test set. Concerning the type checking scores, we can see it is better to keep an answer even if one of the two scores is null, and giving the priority when definition patterns apply seems more reliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Relation extraction</head><p>We applied the Hyrex system <ref type="bibr" coords="10,274.87,266.74,15.50,8.74" target="#b11">[12]</ref> for relation extraction, which uses SVM-LIGHT-TK 10 . Hyrex is currently the state-of-the-art system for the extraction of PPI.</p><p>First Approach. In this first attempt, we consider that a sentence that contains the same relation expressed in the question is more likely to contain the required answer. The application of the Hyrex system allowed to retrieve PPI and RR relations in the annotated corpora. However, very few relations were retrieved from the questions (e.g. only one PPI relation was retrieved in the 40 questions). This is mainly due to the fact that not all biomedical entities were retrieved by GDep parser and ABNER tagger. For example, in the question "With which particular protein does amyloid-beta interact? " only amyloid-beta was recognized, and the implicit entity referred to by "particular protein" was not detected. This first results led us to think of an adaptation consisting in using the declarative forms associated to the questions for the detection of biomedical entities.</p><p>Second Approach. In a second attempt we used the declarative forms associated to the questions. In a first step, we used the answers provided for each question to associate 5 declarative sentences to each question. For example, the following declarative sentences were associated to the question "Which hormone is able to inhibit the transcription of BACE1? ":</p><p>-PB1P-A is able to inhibit the transcription of BACE1 .</p><p>-APP is able to inhibit the transcription of BACE1 .</p><p>-Testosterone is able to inhibit the transcription of BACE1 .</p><p>-IDE is able to inhibit the transcription of BACE1 .</p><p>-NEP is able to inhibit the transcription of BACE1 .</p><p>In a second step, we launched relation extraction with the Hyrex system. We evaluated only 10 questions.</p><p>With relation extraction only, we retrieved answers for 4 questions of the initial 10. Only one of these answers is correct: "knock-out of BACE1 gene" the answer of the question "What experimental approach was successful to inhibit in vivo the production of amyloid beta?" (precision value of 25%). However, these results must be taken with caution as relations were retrieved for only 6 declarative sentences among the 50 declarative sentences associated to the targeted 10 questions. Thus, the number of extracted relations was not sufficiently important to integrate the relation extraction module in the final system. However, this approach could increase the weight of the answers when relations are extracted from them.</p><p>We think that the second approach is very interesting, in particular if the affirmative forms associated to the questions are well constructed. What remains to be improved is: (i) to target more relation types and (ii) to use other annotated corpora to increase recall and precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Similarity between hypothesis and sentences</head><p>Similarity between hypothesis and sentences allows to select the most likely answer, for each question of the corpus. We evaluate our system with each metric plus the weight between hypothesis and sentences. We use evaluation functions described in the introduction of this section. We use two formulas to select the correct answer:</p><p>max sentence: selects the hypothesis with the highest similarity from the sentences which have the highest weight. max: selects the hypothesis with the highest similarity score, regardless of sentence weights.</p><p>Table <ref type="table" coords="11,177.08,410.26,4.98,8.74" target="#tab_1">3</ref> gives the results of this evaluation. The best results bypass the best result given by the most frequent function (43 % correct answers in table 2). Although computation of tree edit distance between hypothesis and sentences gives the best results (whathever the evaluation function), other measures provide correct answers not selected by tree edit distance. We can see that similarity based on syntactic criteria lead to the best scores and seem to be worth to develop. When evaluated on the 40 questions of the initial test set, the system finds 15 correct answers and obtain an accuracy of 0.375. There are several reasons to select a wrong answer:</p><p>the sentence justifying the good answer has a weak weight;</p><p>the most relevant sentences do not contain the good answer; a wrong hypothesis has a better similarity score than the correct one; hypothesis of the most relevant sentences have the same similarity score. In this case, the first hypothesis is chosen.</p><p>We have to find other criteria or other similarity measures in order to answer the third kind of problem. For the last case, a possibility could consist in making a fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>The system we developed for QA4MRE is an adaptation of a QA system in open domain. The scoring scheme of sentences reveals to be adapted in this new task. The system found 13 correct answers for 30 questions, after basic correction. We studied different methods for selecting the right answer among candidate sentences: verification of the expected answer type by semantic verification in the UMLS and corpus verification, similarity measures between each hypothesis and the candidate sentences, based on surface or syntactic features. We also studied relation extraction in order to improve answer and sentence selection. All of these methods show interesting results, and we have to study how to integrate them. However, an important remaining problem is that many variations of question terms are still not recognized. We will study an integration of different lexicons and methods for bypassing absence of knowledge, as searching for paraphases in corpus.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,237.06,569.06,141.23,7.89;2,134.77,362.83,255.11,191.45"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Architecture of QALC4mre</figDesc><graphic coords="2,134.77,362.83,255.11,191.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,140.99,118.96,306.91,8.77;6,140.99,130.68,293.40,8.77;6,140.99,142.40,311.00,8.77;6,149.71,161.41,330.87,8.74;6,138.97,180.12,151.82,8.77;6,157.93,191.60,40.87,8.77;6,198.80,190.06,3.97,6.12;6,203.27,191.63,41.40,8.74;6,244.67,190.06,3.97,6.12;6,249.14,191.63,45.24,8.74;6,294.39,190.06,3.97,6.12;6,298.86,191.63,29.89,8.74;6,328.75,190.06,3.97,6.12;6,333.22,191.63,24.76,8.74;6,357.98,190.06,3.97,6.12;6,362.45,191.63,2.77,8.74;6,138.97,203.08,110.59,8.77;6,249.56,201.54,3.97,6.12;6,254.03,203.11,209.74,8.74;6,463.77,201.54,3.97,6.12;6,468.24,203.11,2.77,8.74"><head>-</head><label></label><figDesc>glycoprotein glycoprotein I-NP NN B-protein 4 SUB B-C0017968 O -KEYWORD KEYWORD B-NP NN O 0 ROOT O B-PROTEIN -APP APP B-NP NN B-protein 14 PMOD B-C0085151 B-PROTEIN We used the following annotated corpora for the extraction of PPI and RR: 1. PPI: 5 PPI benchmark corpora -AIMed 2 , BioInfer 3 , HPRD50 4 , IEPA 5 , LLL 6 , 2. RR: BioNLP-ST 2011 7 . Other annotated corpora exist for this relation 8 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="9,181.95,428.02,251.45,108.44"><head>Table 2 .</head><label>2</label><figDesc>Evaluation on the new test set made of 30 questions</figDesc><table coords="9,181.95,428.02,251.45,97.93"><row><cell></cell><cell cols="3">sent weight all sent weight 1 best accuracy</cell></row><row><cell></cell><cell>#</cell><cell>#</cell><cell>(30 questions)</cell></row><row><cell>weight</cell><cell>9</cell><cell>13</cell><cell>0.43</cell></row><row><cell>pat</cell><cell>8</cell><cell>12</cell><cell>0.40</cell></row><row><cell>pat0</cell><cell>7</cell><cell>8</cell><cell>0.26</cell></row><row><cell>cooc</cell><cell>8</cell><cell>11</cell><cell>0.36</cell></row><row><cell>cooc0</cell><cell>5</cell><cell>7</cell><cell>0.23</cell></row><row><cell>most frequent</cell><cell>12</cell><cell>13</cell><cell>0.43</cell></row><row><cell>N=5</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="11,162.56,526.74,290.24,75.56"><head>Table 3 .</head><label>3</label><figDesc>Evaluation of the similarity between hypothesis and sentences</figDesc><table coords="11,207.86,526.74,199.63,64.65"><row><cell>Metric</cell><cell cols="2">max sentence max</cell></row><row><cell>TERp</cell><cell>0.43</cell><cell>0.43</cell></row><row><cell>Common dependencies</cell><cell>0.43</cell><cell>0.40</cell></row><row><cell>Common constituent subtrees</cell><cell>0.40</cell><cell>0.30</cell></row><row><cell>Common dependency subtrees</cell><cell>0.37</cell><cell>0.33</cell></row><row><cell>Tree edit distance</cell><cell>0.43</cell><cell>0.47</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,656.80,76.54,7.86"><p>www.jfponline.com</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,467.65,337.64,7.86;12,151.52,478.61,329.07,7.86;12,151.52,489.57,329.07,7.86;12,151.52,500.53,253.02,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,406.17,478.61,74.43,7.86;12,151.52,489.57,307.26,7.86">The Question Answering System QALC at LIMSI, Experiments in Using Web and WordNet</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>De Chalendar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dalmas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Elkateb-Gara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Ferret</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hurault-Plantet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Illouz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Monceaux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Robba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Vilnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,151.52,500.53,35.58,7.86">TREC11</title>
		<imprint>
			<publisher>NIST SPECIAL PUBLICATION SP</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="457" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,512.06,337.64,7.86;12,151.52,523.01,227.45,7.86" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="12,213.65,512.06,266.95,7.86;12,151.52,523.01,70.22,7.86">Any Domain Parsing: Automatic Domain Adaptation for Natural Language Parsing</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Mcclosky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>Brown University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct coords="12,142.96,534.54,337.63,7.86;12,151.52,545.50,329.07,7.86;12,151.52,556.46,317.99,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,243.96,534.54,236.63,7.86;12,151.52,545.50,78.06,7.86">Tregex and Tsurgeon: tools for querying and manipulating tree data structures</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,297.48,545.50,183.11,7.86;12,151.52,556.46,201.33,7.86">Proceedings Fifth international conference on Language Resources and Evaluation (LREC 2006)</title>
		<editor>
			<persName><surname>Elda</surname></persName>
		</editor>
		<meeting>Fifth international conference on Language Resources and Evaluation (LREC 2006)<address><addrLine>Genoa, Italy, ELDA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,567.99,337.63,7.86;12,151.52,578.95,238.15,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,217.13,567.99,259.64,7.86">Syntagmatic and paradigmatic representations of term variation</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Jacquemin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,165.60,578.95,189.99,7.86">Proceedings of the 37th annual meeting of ACL</title>
		<meeting>the 37th annual meeting of ACL</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,590.48,337.63,7.86;12,151.52,601.43,329.07,7.86;12,151.52,612.39,329.07,7.86;12,151.52,623.35,60.92,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,236.04,590.48,244.55,7.86;12,151.52,601.43,229.85,7.86">Finding high-frequent synonyms of a domain-specific verb in english sub-language of medline abstracts using wordnet</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Rsner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,406.94,601.43,73.65,7.86;12,151.52,612.39,325.01,7.86">GWC 2004 -Proceedings of the 2nd International Conference of the Global WordNet Association</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="242" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,634.88,337.63,7.86;12,151.52,645.81,329.07,7.89" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,228.52,634.88,252.07,7.86;12,151.52,645.84,91.39,7.86">The Unified Medical Language System (UMLS): Integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,250.78,645.84,93.24,7.86">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">267</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>Database issue</note>
</biblStruct>

<biblStruct coords="13,151.53,119.67,329.06,7.86;13,151.52,130.63,329.07,7.86;13,151.52,141.59,329.07,7.86;13,151.52,152.55,128.53,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,328.08,119.67,152.50,7.86;13,151.52,130.63,271.58,7.86">UMLS-Interface and UMLS-Similarity : Open source software for measuring paths and semantic similarity</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">V</forename><surname>Pakhomov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,445.37,130.63,35.22,7.86;13,151.52,141.59,305.51,7.86">Proceedings of the American Medical Informatics Association (AMIA) Symposium</title>
		<meeting>the American Medical Informatics Association (AMIA) Symposium<address><addrLine>San Fransisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-11">November 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,163.51,337.64,7.86;13,151.52,174.44,329.07,7.89;13,151.52,185.43,60.92,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="13,354.34,163.51,126.25,7.86;13,151.52,174.47,212.13,7.86">Ter-plus: paraphrase, semantic, and alignment enhancements to translation edit rate</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,371.99,174.47,82.95,7.86">Machine Translation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="127" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.96,196.39,337.64,7.86;13,151.52,207.32,285.06,7.89" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="13,252.40,196.39,228.19,7.86;13,151.52,207.34,105.41,7.86">Simple fast algorithms for the editing distance between trees and related problems</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Shasha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="13,265.35,207.34,72.57,7.86">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1245" to="1262" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,218.30,337.97,7.86;13,151.52,229.26,329.07,7.86;13,151.52,240.22,329.07,7.86;13,151.52,251.18,124.45,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="13,287.56,218.30,193.03,7.86;13,151.52,229.26,202.45,7.86">A syntactic tree matching approach to finding similar questions in community-based qa services</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,380.26,229.26,100.33,7.86;13,151.52,240.22,329.07,7.86;13,151.52,251.18,32.07,7.86">Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 32nd international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,262.14,337.98,7.86;13,151.52,273.10,329.07,7.86;13,151.52,284.06,51.70,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="13,202.06,262.14,222.19,7.86">Probabilistic part-of-speech tagging using decision trees</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,445.37,262.14,35.22,7.86;13,151.52,273.10,324.98,7.86">Proceedings of the International Conference on New Methods in Language Processing</title>
		<meeting>the International Conference on New Methods in Language Processing</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="44" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,295.02,337.98,7.86;13,151.52,305.98,283.76,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="13,296.20,295.02,184.40,7.86;13,151.52,305.98,169.77,7.86">Combining tree structures, flat features and patterns for biomedical relation extraction</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lavelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,343.39,305.98,27.90,7.86">EACL</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
