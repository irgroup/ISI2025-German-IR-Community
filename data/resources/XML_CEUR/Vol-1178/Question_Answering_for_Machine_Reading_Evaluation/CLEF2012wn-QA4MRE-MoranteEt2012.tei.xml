<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.65,115.96,314.06,12.62;1,237.45,133.89,140.45,12.62">Machine Reading of Biomedical Texts about Alzheimer&apos;s Disease</title>
				<funder ref="#_PyjafBZ">
					<orgName type="full">University of Antwerp</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,181.13,171.56,62.31,8.74"><forename type="first">Roser</forename><surname>Morante</surname></persName>
							<email>roser.morante@ua.ac.be</email>
							<affiliation key="aff0">
								<orgName type="department">CLiPS</orgName>
								<orgName type="institution">University of Antwerp</orgName>
								<address>
									<addrLine>Prinsstraat 13</addrLine>
									<postCode>B-2000</postCode>
									<settlement>Antwerpen</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">CNIO</orgName>
								<address>
									<addrLine>Melchor Fernández Almagro 3</addrLine>
									<postCode>28029</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,256.14,171.56,75.83,8.74"><forename type="first">Martin</forename><surname>Krallinger</surname></persName>
							<email>mkrallinger@cnio.es</email>
							<affiliation key="aff1">
								<orgName type="institution">CNIO</orgName>
								<address>
									<addrLine>Melchor Fernández Almagro 3</addrLine>
									<postCode>28029</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,343.99,171.56,70.96,8.74"><forename type="first">Alfonso</forename><surname>Valencia</surname></persName>
							<email>avalencia@cnio.es</email>
							<affiliation key="aff0">
								<orgName type="department">CLiPS</orgName>
								<orgName type="institution">University of Antwerp</orgName>
								<address>
									<addrLine>Prinsstraat 13</addrLine>
									<postCode>B-2000</postCode>
									<settlement>Antwerpen</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,270.09,183.51,79.64,8.74"><forename type="first">Walter</forename><surname>Daelemans</surname></persName>
							<email>walter.daelemans@ua.ac.be</email>
						</author>
						<title level="a" type="main" coord="1,150.65,115.96,314.06,12.62;1,237.45,133.89,140.45,12.62">Machine Reading of Biomedical Texts about Alzheimer&apos;s Disease</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5A92C0AF4EF07393392D84D8263DBAA3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report describes the task Machine reading of biomedical texts about Alzheimer's disease, which is a pilot task of the Question Answering for Machine Reading Evaluation (QA4MRE) Lab at CLEF 2012. The task aims at exploring the ability of a machine reading system to answer questions about a scientific topic, namely Alzheimer's disease. As in the QA4MRE task, participant systems were asked to read a document and identify the answers to a set of questions about information that is stated or implied in the text. A background collection was provided for systems to acquire background knowledge. The background collection is a corpus newly compiled for this task, the Alzheimer's Disease Literature Corpus. Seven teams participated in the task submitting a total of 43 runs. The highest score obtained by a team was 0.55 c@1, which is clearly above baseline.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This report describes the task Machine reading of biomedical texts about Alzheimer 's disease, which is a pilot task of the Question Answering for Machine Reading Evaluation (QA4MRE) <ref type="foot" coords="1,236.21,481.78,3.97,6.12" target="#foot_0">1</ref> Lab at CLEF 2012. The task follows the same set up and principles as the QA4MRE task <ref type="bibr" coords="1,300.08,495.31,23.28,8.74">(7; 8)</ref>, with the difference that it focuses on the biomedical domain.</p><p>This pilot task aims at exploring the ability of a machine reading system <ref type="bibr" coords="1,134.77,531.18,28.65,8.74">(4; 12)</ref> to answer questions about a scientific topic, namely the Alzheimer's disease (AD). AD has been chosen as the focus of the task because there is a particular interest in more efficient processing of Alzheimer-related literature, as this condition constitutes a considerable health challenge for an aging population (Citron 2010). The increasing importance of AD is reflected in the recently approved US National Alzheimer's Project Act, 2 which will result in considerable funding being made available for research on this disease and for financing better data infrastructure resources. Currently, the illness is being analyzed from various perspectives in a growing number of scientific studies (5; 1; 2). The report is organised as follows. The task is described in Section 2, Section 3 provides information about the Alzheimer's Disease Literature Corpus and Section 4 about the test data. Section 5 explains the process followed to annotated the data. Section 6 deals with the design of questions. In Section 7 the evaluation process is explained and in Section 8 details about the number of participating systems and runs are presented as well as their results. Finally, Section 9 closes the paper with some conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task description</head><p>As in the QA4MRE task, participant systems were asked to read a document and identify the answers to a set of questions about information that is stated or implied in the text. Questions are in the form of multiple choice, each having five options, and only one correct answer. The detection of correct answers is specifically designed to require various kinds of inference and the consideration of previously acquired background knowledge. Knowledge acquisition can be performed from a document collection called the background collection provided by the organization. Although the additional knowledge obtained through the background collection may be used to assist with answering the questions, the principal answer is to be found among the facts contained in the test documents. Participants were provided with a collection of texts about Alzheimer's Disease called the Alzheimer's Disease Literature Corpus (ADLC corpus), which was compiled for this task. The evaluation is performed on four reading tests with ten multiple choice questions each.</p><p>To solve the task, participants could make use of existing resources, such as ontologies or databases, and tools, such as named entity taggers, event extractors, parsers, etc. In order to keep the task reasonably simple for systems, the task organizers provided the texts of the background collection and the test documents processed at several levels of linguistic analysis (lemmas, part-of-speech, named entities, chunking, dependency parsing) with publicly available state of the art tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background collection: the Alzheimer's Disease Literature Corpus</head><p>The background collection is a collection of texts about Alzheimer's disease called the Alzheimer's Disease Literature Corpus (ADLC corpus). Systems could use it to acquire reading capabilities and to obtain knowledge about Alzheimer's disease that could help in answering the questions about the test documents. The texts have been carefully selected to be as specific as possible for this topic and the corpus should constitute a comprehensive resource for this task in particular and for text mining efforts tailored to the Alzheimer's disease field in general. Although the use of the background collection is recommended, it is not mandatory. The background collection is released subject to signing a license agreement. <ref type="foot" coords="3,182.12,129.37,3.97,6.12" target="#foot_2">3</ref> It contains the following sets of documents:</p><p>PubMed abstracts. 66,222 abstracts obtained by performing in PubMed the search provided in Figure <ref type="figure" coords="3,268.42,166.64,3.87,8.74" target="#fig_2">1</ref>. The abstracts were provided in XML format, and with the annotations described in Section 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Test data</head><p>The test set is composed of 4 reading tests, each consisting of 10 questions about 1 document, with 5 answer choices per question. So, there were in total 40 questions and 200 choices/options. Participating systems were required to answer these 40 questions by choosing in each case one answer from the five alternatives. Systems could leave questions unanswered. The test documents were selected from a list of bibliographic records provided by professor Tim Clark from the Massachusetts Alzheimer's Disease Research Center, USA. The records were compiled in 2011 and represent 45 core hypotheses in Alzheimer's disease.</p><p>The test documents were provided in text format. They were first converted automatically from PDF into text format and then the text version was corrected manually, paying attention to symbols that express relevant information about Alzheimer's disease. The captions of figures and tables were also included, but the figures and tables not. Participants were not expected to process the contents of tables and figures. A sample of a test document with questions can be downloaded from the QA4MRE website. 5 The test documents and the questions were provided also with annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Data annotation</head><p>The documents in the background collection, the test documents, and the questions were provided with annotations in a column format as shown in Figure <ref type="figure" coords="4,472.84,486.48,3.87,8.74" target="#fig_0">2</ref>.  The annotations were obtained automatically with the dependency parser GDep <ref type="bibr" coords="5,163.15,130.95,16.38,8.74" target="#b9">(10)</ref>, a UMLS (3) based NE tagger developed at CLiPS, and the ABNER NE tagger <ref type="bibr" coords="5,182.81,142.90,16.38,8.74" target="#b10">(11)</ref>. The content of the columns is specified in Table <ref type="table" coords="5,419.03,142.90,3.87,8.74">1</ref>. Table <ref type="table" coords="5,268.84,308.09,4.13,7.89">1</ref>. Annotated information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Question design</head><p>As in the QA4MRE task, questions are in multiple choice format and focus on testing the comprehension of one single document. The questions posed for this task should address aspects that are of biomedical relevance and that have been proven to be of importance in the context of previous efforts such as BioCreative <ref type="foot" coords="5,155.80,438.83,3.97,6.12" target="#foot_4">6</ref> , Genomics TREC track<ref type="foot" coords="5,263.89,438.83,3.97,6.12" target="#foot_5">7</ref> or the BioNLP shared tasks. <ref type="foot" coords="5,392.74,438.83,3.97,6.12" target="#foot_6">8</ref> This should enable participants to make use of resources developed for these competitions and will establish a link between this pilot task and previous efforts. Additionally, since machine reading of biomedical texts is a new task, it seemed more appropriate to restrict the types of questions somehow. Therefore a restricted set of named entity types associated to the questions was defined, as well as a list of question types. The expected answer types for the multiple choice answers depend on allowed entity types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Named entities</head><p>The categories of named entities considered for this task are the following: In order to identify the named entities above, the following lexico-semantic resources and tools can be used (among others): ABNER, BANNER, Genia Tagger, BioThesaurus, BioLexicon,UMLS, LINNAEUS tagger, OrganismTagger, MeSH, Gene Ontology (and other ontologies from OBO), etc... . The test documents were processed with UMLS and the BANNER tagger before making the questions, so that questions would refer only to entities that can be automatically identified with existing resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Question types</head><p>Based on examination of the relationships between the various entity types we compiled the following collection of biomedically relevant question types:</p><p>Experimental evidence/qualifier. This question type refers to experimental techniques, methods or models used to generate or validate a given discovery.</p><p>Examples include animal models used for a given in vivo study, interaction detection methods used to detect protein interactions, imaging techniques for visualization or localization of a particular protein. Protein-protein interaction. This question type refers to the detection of an interaction partner of a given protein. Examples include physical binding of two proteins in a protein-protein complex or more transient interaction in phosphorylation of one protein by another. Gene synonymy relation. This question type tries to establish relations between two entity mentions of genes or proteins that refer actually to the same biological entity. For instance this relation exists between 'APP' and 'amyloid beta (A4) precursor protein'. Here alternative aliases of a gene name or symbol are included, as well as typographical variants and acronyms and their corresponding expanded forms. Organism source relation. This question type refers to the actual organism source for a given protein or gene. An example would be the genes encoded in the human genome or expressed in humans. Regulatory relation. This question type refers to gene regulatory relationships between two bio-entities (protein and gene), i.e. whether one bio-entity affects the gene expression of another entity (e.g. transcription factor target gene relation).</p><p>Increase (improvement, higher expression). This is a more specific question type of the regulatory relation. It refers to cases where one bio-entity causes the upregulation (increased expression) of another bio-entity. Decrease (depletion, reduction). This is a more specific question type of the regulatory relation. It refers to cases where one bio-entity causes the downregulation (decreased expression) of another bio-entity. Inhibition/disruption/impaired. This question type refers to cases were one bio-entity blocks or inhibits another bio-entity. Examples include drugs blocking a given protein or enzyme, or proteins that inhibit a particular biological process or pathway.</p><p>We found that some types of questions defined initially based on an example test document did not occur frequently in the document tests used for evaluation. For a future edition of the task we would like to select entity types that are guaranteed to occur in all documents. Document selection could also be performed depending on the most frequent entity types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Degrees of difficulty</head><p>Questions can be assigned a degree of difficulty: simple, medium and complex.</p><p>Simple. Factual questions that can be answered using information from the target document and whose textual evidence is contained multiple times in the paper, e.g. several text snippets are supporting the correct answer. The answer is found almost verbatim in the paper. Medium. The correct answer is phrased in a way that requires the use of lexicosemantic dictionaries and name alias recognition capabilities to be able to handle lexico-semantic alienations of keywords and entities. Complex. Reasoning must be applied to answer this question. Choosing the correct answer requires combining pieces of evidence. Such questions might need ad hoc axiomatic knowledge and abductive processes.</p><p>A collection of criteria for question difficulty classification was followed. Aspects that influence question difficulty include:</p><p>-Are the ontological relations encoded in the question? If they are encoded the question should be easier. -If keyword-based indexing and conceptual indexing are required the question is less easy. -Script like questions such as 'how is an anatomical structure assembled?' should be more difficult since answering them requires combining several units of information. -Template questions about successive temporal events (biological processes, disease stages) should be more difficult since it also requires several units of information.</p><p>-Is it necessary to process morphological alternations such as phosphorylate lexicalized as the nominalization phosphorylation? In this case the degree of difficulty should be simple/medium, depending on other characteristics of the question. -Is it necessary to process lexical alternations? The usage of synonyms or semantically related terms derived from ontologies is necessary to increase the recall. -Is it necessary to process semantic alternations and paraphrases? This involves finding relations between multi-term paraphrases and single terms, textual patterns, or complex examination between word building terms within the ontology. -Is it necessary to process terminological variants and high level indexes comprising terms and their variants for retrieval? A variant recognition module is required as well as weighting of matching between questions and documents. -How big is the paragraph window size of the evidence text? Is it a continuous span of text? The bigger the window size, the more difficult is the question. Non continuous spans are more difficult to process than continuous.</p><p>Assigning degrees of difficulty to the questions proved to be a difficult task because several factors interact. For a future edition of the task we would like to define a protocol in order to facilitate the assignment of degrees of difficulty, which can also be used to perform error analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Answers</head><p>As in the main task, systems are not required to answer every question, since the c@1 measure (6) was used for evaluation. This measure encourages systems to reduce the number of incorrect answers while maintaining the number of correct ones by leaving some questions unanswered. Systems were asked to choose the right answer among five choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation</head><p>As in the main task, participants were allowed to submit a maximum of 10 runs. Each run should be categorized as one of the following types, depending on the resources that have been used to assist in asnwering the questions:</p><p>1. No external resource was used (only the test document). 2. Only the test document and the associated background collection was used. 3. The test document and other resources were used, but not the background collection. 4. The test document together with the background collection and other resources were used.</p><p>Evaluation was performed automatically following the same procedure as in the QA4MRE task. Each question received one (and only one) of the three following assessments:</p><p>-Correct if the system selected the correct answer among the five candidate ones of the given question. -Incorrect if the system selected one of the wrong answers.</p><p>-NoA if the system chose not to answer the question.</p><p>The main evaluation measure used was c@1 (6), which takes into account the option of not answering certain questions. The formulation of c@1 is given in <ref type="bibr" coords="9,146.29,199.27,11.62,8.74" target="#b0">(1)</ref>. The overall c@1 is calculated over the 40 questions of the test collection.</p><formula xml:id="formula_0" coords="9,258.33,218.64,222.26,22.31">c@1 = 1 n (n R + n U n R n )<label>(1)</label></formula><p>where n R : number of questions correctly answered. n U : number of questions unanswered n: total number of questions As a secondary measure systems are evaluated on accuracy, which is the traditional measure applied to question answering evaluations that do not distinguish between answered and unanswered questions. The formulation of accuracy is given in <ref type="bibr" coords="9,184.17,353.09,11.62,8.74" target="#b1">(2)</ref>. The overall accuracy is calculated over the 40 questions of the test collection.</p><formula xml:id="formula_1" coords="9,258.56,383.81,222.03,22.31">accuracy = n R + n U R n<label>(2)</label></formula><p>where n R : number of questions correctly answered. n U R : number of unanswered questions whose candidate answer was correct. n: total number of questions More information about the evaluation procedure can be found in the article that describes the QA4MRE task (8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Participation and results</head><p>Out of the 23 groups that had previously registered and signed the license agreement to download the background collection, a total of 7 groups participated submitting 43 runs. Table <ref type="table" coords="9,250.86,572.43,4.98,8.74">2</ref> shows the list of participating teams and the reference to their reports. Table <ref type="table" coords="9,178.00,596.34,4.98,8.74" target="#tab_3">3</ref> provides information about the number of runs per team and the scores of the best run in terms of c@1. A random baseline is calculated, assuming that a system answers all questions. This baseline has five possibilities when trying to answer a question: it can select the correct answer to the question, or it can select one of the four incorrect answers. In this case, the overall result is 0,20. One of the participating systems scores as the baseline, whereas the team Table <ref type="table" coords="10,213.25,227.69,4.13,7.89">2</ref>. Participating teams with reference to their reports.</p><p>that obtained the best results is clearly above baseline. Given the fact that four of the seven teams are only some points above baseline, we consider that the task is complex and the questions put were not easy to answer. However, given that two systems are clearly above baseline, we consider that the difficulty of the questions provided was suitable for the task. The team that obtained the highest scores, Pisa, approaches the task using the index expansion technique, adding variants of terms and relations to a specialized sentence retrieval engine. Indexes are enriched with information extracted from linguistic analysis of the documents, including the documents from the background collection. The merk team applies two strategies, information retrieval (IR) and semantic web-based. For some IR-based runs they follow a two step retrieval approach. For one run they use a hypothesis generation technique. In another run they follow a majority voting scheme for selecting the correct answer from a pool of runs. In the semantic approach they use various named entity recognition techniques followed by shallow linguistic similarity, semantic similarity and network analysis approaches to identify the correct answers. The best performing strategy uses a combination of query processing followed by IR on the background collection. The kule team applies two different similarity-based strategies using only the input text, the question string, and the multiple choice answers, and do not rely on any external resources or the background collection. The nict team experiments also with several approaches that assess the similarity of a candidate query constructed from the question plus a candidate answer to the information available in a document, or a set of background documents, to select the best answer to a multiple-choice question. They explored a range of possible similarity matching methods, ranging from simple word overlap, to dependency graph matching, to feature-based vector similarity models that incorporate lexical, syntactic and/or semantic features. The only external knowledge resource used was UMLS. The iirg team applies rule-based pattern matching techniques. The lims team adapted an existing question answering system for English named QALC, and the ntnu team applies TF and TF-IDF weighting schemes as well as the OMIM terms about Alzheimer for query expansion with background collections to help for machine reading comprehension. More details about the approaches applied are available in the corresponding articles in this volume.</p><p>Table <ref type="table" coords="11,176.55,298.32,4.98,8.74" target="#tab_4">4</ref> illustrates the mean c@1 scores for each of the 4 reading tests considering all systems. This shows the difficulty of each particular test. Test 1 at 0,11 appears to be a very hard test, whereas Test 2 at 0.34 seems to be somewhat easier. The scores per run are provided in Table <ref type="table" coords="11,333.45,433.08,4.98,8.74">5</ref> in terms of overall c@1, median and standard deviation of c@1, and overall accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions</head><p>This report presented the task Machine Reading of Biomedical Texts about Alzheimer's Disease, which was organised as a pilot task of the QA4MRE Lab at CLEF 2012. The task focused on biomedical texts about Alzheimer's disease in English. Participating systems should answer readability tests about the test documents provided. Each readability test consisted on 10 multiple choice questions about a document. A new corpus has been compiled for participating systems to acquire background knowledge, the Alzheimer's Disease Literature Corpus. The fact that 7 teams participated in the task with 43 runs shows that there was enough interest in machine reading exercises from research teams working on biomedical texts. The best system obtained a c@1 score of 0,55, which is certainly above baseline.</p><p>For future editions of the task we will work on improving the question design and selection of test documents. We found that some types of questions defined initially based on an example test document did not occur frequently in the document tests used for evaluation. We will perform a deeper analysis in order to define a more stable typology of questions. Additionally, we will explore additional strategies for sampling suitable documents for the test set collections in order to select test documents that are similar in terms of contents and readability complexity. From the average scores per document we observe that some documents are more difficult than others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>Overall c@1 Median c@1 St. Dev Overall accuracy Pisa12013enen 0,55 0,55 0,13 0,55 merk12062enen 0,47 0,52 0,17 0,43 merk12022enen 0,40 0,37 0,16 0,38 merk12052enen 0,39 0,47 0,27 0,35 merk12012enen 0,36 0,33 0,28 0,30 merk12072enen 0,35 0,40 0,24 0,30 kule12061enen 0,30 0,30 0,08 0,30 kule12101enen 0,30 0,30 0,08 0,30 nict12102enen 0,28 0,30 0,05 0,28 merk12042enen 0,26 0,26 0,12 0,25 iirg12021enen 0,25 0,25 0,13 0,25 iirg12041enen 0,25 0,25 0,13 0, 25 kule12041enen 0,25 0,20 0,10 0,25 kule12051enen 0,25 0,30 0,10 0,25 kule12091enen 0,25 0,20 0,19 0,25 merk12032enen 0,25 0,30 0,17 0,25 iirg12021enen 0,23 0,20 0,22 0,23 iirg12031enen 0,23 0,20 0,22 0,23 iirg12051enen 0,23 0,20 0,22 0,23 iirg12061enen 0,23 0,25 0,17 0,23 kule12031enen 0,23 0,25 0,10 0,23 nict12031enen 0,23 0,25 0,21 0,23 nict12041enen 0,23 0,25 0,17 0,23 nict12053enen 0,23 0,20 0,15 0,23 nict12063enen 0,23 0,25 0,10 0,23 nict12074enen 0,23 0,20 0,13 0,23 kule12011enen 0,21 0,19 0,12 0,18 lims12013enen 0,21 0,21 0,16 0,20 lims12024enen 0,21 0,20 0,12 0,20 lims12043enen 0,21 0,27 0,14 0,20 kule12071enen 0,20 0,20 0,08 0,20 kule12081enen 0,20 0,20 0,12 0,20 nict12091enen 0,20 0,20 0,08 0,20 ntnu12032enen 0,20 0,19 0,17 0,18 ntnu12054enen 0,20 0,20 0,16 0,20 iirg12011enen 0,18 0,10 0,15 0,18 kule12021enen 0,18 0,19 0,15 0,15 ntnu12012enen 0,18 0,15 0,17 0,18 ntnu12044enen 0,18 0,10 0,24 0,18 ntnu12022enen 0,17 0,13 0,17 0,15 nict12012enen 0,15 0,15 0,13 0,15 nict12024enen 0,15 0,15 0,13 0,15 lims12034enen 0,14 0,18 0,12 0,13 Table <ref type="table" coords="13,163.86,617.20,4.13,7.89">5</ref>. Results per run. 'st. dev' stands for standard deviation, which is calculated over c@1 of all 4 reading tests.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,221.58,637.51,172.20,7.89;4,134.77,518.34,362.25,104.40"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example of an annotated sentence.</figDesc><graphic coords="4,134.77,518.34,362.25,104.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,137.50,655.03,3.65,5.24;4,144.73,657.44,287.15,7.47"><head>5</head><label></label><figDesc>http://celct.fbk.eu/QA4MRE/index.php?page=Pages/downloads.php</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,164.84,176.66,126.84,7.86;5,164.84,187.62,184.58,7.86;5,164.84,198.58,167.99,7.86;5,164.84,209.54,130.32,7.86;5,164.84,220.50,138.11,7.86;5,164.84,231.46,150.78,7.86;5,164.84,242.42,181.70,7.86;5,164.84,253.38,163.08,7.86;5,164.84,264.33,285.68,7.86;5,164.84,275.29,209.08,7.86;5,164.84,286.25,212.46,7.86;5,164.84,297.21,173.43,7.86"><head>Column 1</head><label>1</label><figDesc>Document identifier Column 2 Sentence number in the document Column 3 Token number in the sentence Column 4 Word (GDep parser) Column 5 Lemma (GDep parser) Column 6 Chunk tag (GDep parser) Column 7 Part-of-speech tag (GDep parser) Column 8 Named entity (GDep parser) Column 9 Parent node in the dependency syntaxt tree (GDep parser) Column 10 Dependency syntax label (GDep parser) Column 11 UMLS named entity (CLiPS NE Tagger) Column 12 Named entity (ABNER tagger)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,196.67,355.90,222.02,107.27"><head>Table 3 .</head><label>3</label><figDesc>Number of runs and highest scores per team.</figDesc><table coords="10,224.38,355.90,166.61,96.36"><row><cell cols="3">Team # of runs Highest c@1 score</cell></row><row><cell>Pisa</cell><cell>1</cell><cell>0,55</cell></row><row><cell>merk</cell><cell>7</cell><cell>0,47</cell></row><row><cell>kule</cell><cell>10</cell><cell>0,30</cell></row><row><cell>nict</cell><cell>9</cell><cell>0,28</cell></row><row><cell>iirg</cell><cell>7</cell><cell>0,25</cell></row><row><cell>lims</cell><cell>4</cell><cell>0,21</cell></row><row><cell>ntnu</cell><cell>5</cell><cell>0,20</cell></row><row><cell>baseline</cell><cell>-</cell><cell>0,20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,209.50,365.56,196.35,29.76"><head>Table 4 .</head><label>4</label><figDesc>Mean c@1 scores for each reading test.</figDesc><table coords="11,246.46,365.56,122.44,18.85"><row><cell cols="4">Test 1 Test 2 Test 3 Test 4</cell></row><row><cell>0,11</cell><cell>0,34</cell><cell>0,22</cell><cell>0,28</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,646.48,127.10,7.47"><p>http://celct.fbk.eu/QA4MRE/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,144.73,657.44,174.17,7.47"><p>http://aspe.hhs.gov/daltcp/napa/#NAPA</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,144.73,634.88,335.87,8.12;3,144.73,646.48,273.03,7.47"><p>The ADLC corpus can be downloaded from the following link: http://celct.fbk. eu/ResPubliQA/index.php?page=Pages/bg_collection_pilot.php</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,144.73,656.80,279.75,8.12"><p>LA-PDFText is available at http://code.google.com/p/lapdftext/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="5,144.73,635.53,122.39,7.47"><p>http://www.biocreative.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="5,144.73,646.48,127.10,7.47"><p>http://ir.ohsu.edu/genomics</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6" coords="5,144.73,657.44,174.17,7.47"><p>http://sites.google.com/site/bionlpst</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was made possible through financial support from the <rs type="funder">University of Antwerp</rs> (<rs type="projectName">GOA</rs> project BIOGRAPH). We are grateful to the organizers of the <rs type="institution">QA4MRE Lab at CLEF</rs> 2012 for hosting the pilot task. <rs type="person">Vincent Van Asch</rs>, <rs type="person">Florian Geitner</rs>, <rs type="person">Cartic Ramakrishnan</rs>, <rs type="person">Gully A.P.C. Burns</rs>, <rs type="person">Pamela Forner</rs>, and <rs type="person">Giovanni Moretti</rs> provided technical support. Elsevier was kind enough to allow us to include some of their articles in the background collection. We are grateful to <rs type="person">Anita de Waard</rs> and <rs type="person">Antony Scerri</rs> for providing the Elsevier documents. Finallly, we also thank <rs type="person">Professor Tim Clark</rs> for providing the list of SWAN cited and derived articles.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_PyjafBZ">
					<orgName type="project" subtype="full">GOA</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="14,154.09,170.48,326.50,7.86;14,154.08,181.44,326.51,7.86;14,154.08,192.40,60.92,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,266.09,170.48,214.50,7.86;14,154.08,181.44,77.57,7.86">A new text mining approach for finding protein-todisease associations</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Al-Mubaid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,239.10,181.44,218.94,7.86">American Journal of Biochemistry and Biotechnology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="152" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.09,203.36,326.51,7.86;14,154.08,214.32,326.51,7.86;14,154.08,225.28,278.25,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,209.28,214.32,271.31,7.86;14,154.08,225.28,147.14,7.86">PESCADOR, a web-based tool to assist text-mining of biointeractions extracted from pubmed queries</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Barbosa-Silva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Fontaine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Donnard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Stussi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Andrade-Navarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,308.11,225.28,83.26,7.86">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.09,236.24,326.50,7.86;14,154.08,247.20,286.47,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,229.19,236.24,251.40,7.86;14,154.08,247.20,91.63,7.86">The unified medical language system (UMLS): Integrating biomedical terminology</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,252.92,247.20,93.70,7.86">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267D" to="270" />
		</imprint>
	</monogr>
	<note>Suppl.</note>
</biblStruct>

<biblStruct coords="14,252.54,258.15,228.05,7.86;14,154.08,269.11,326.51,7.86;14,154.08,280.07,85.96,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,318.69,258.15,65.34,7.86">Machine reading</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,405.68,258.15,74.92,7.86;14,154.08,269.11,199.45,7.86">Proceedings of the 21st National Conference on Artificial Intelligence</title>
		<meeting>the 21st National Conference on Artificial Intelligence<address><addrLine>Boston, Massachusetts</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1517" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.09,291.03,326.51,7.86;14,154.08,301.99,326.51,7.86;14,154.08,312.95,326.51,7.86;14,154.08,323.91,42.49,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,168.95,301.99,307.73,7.86">SWAN: A distributed knowledge infrastructure for alzheimerdisease research</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Seaborne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Cayzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,154.08,312.95,283.18,7.86">Web Semantics: Science, Services and Agents on the World Wide Web</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="222" to="228" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.09,334.87,326.51,7.86;14,154.08,345.83,326.51,7.86;14,154.08,356.78,288.93,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,249.27,334.87,175.69,7.86">A simple measure to assess the non-response</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,445.36,334.87,35.23,7.86;14,154.08,345.83,326.51,7.86;14,154.08,356.78,195.75,7.86">Proceedings of 49th Annual Meeting of the Association for Computational Linguistics -Human Language Technologies (ACL-HLT 2011)</title>
		<meeting>49th Annual Meeting of the Association for Computational Linguistics -Human Language Technologies (ACL-HLT 2011)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1415" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.09,365.48,326.51,10.13;14,154.08,378.70,326.51,7.86;14,154.08,389.66,319.05,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,212.57,378.70,268.01,7.86;14,154.08,389.66,96.39,7.86">Overview of QA4MRE at CLEF 2011: Question answering for machine reading evaluation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Á</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F E</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Forascu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sporleder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,271.60,389.66,168.12,7.86">CLEF (Notebook Papers/Labs/Workshop</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.09,398.35,326.50,10.13;14,154.08,411.58,326.51,7.86;14,154.08,422.54,326.51,7.86;14,154.08,433.50,208.64,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="14,320.98,411.58,159.62,7.86;14,154.08,422.54,209.10,7.86">Overview of QA4MRE at CLEF 2012: Question answering for machine reading evaluation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Á</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F E</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sporleder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Forascu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Benajiba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Osenova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,386.12,422.54,94.48,7.86;14,154.08,433.50,179.97,7.86">CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.09,444.46,326.50,7.86;14,154.08,455.41,326.51,7.86;14,154.08,466.37,33.27,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="14,363.84,444.46,116.75,7.86;14,154.08,455.41,148.72,7.86">Layout-aware text extraction from full-text pdf of scientific articles</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Patnia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Burns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,309.19,455.41,149.54,7.86">Source code for biology and medicine</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.08,477.33,326.52,7.86;14,154.08,488.29,326.51,7.86;14,154.08,499.25,326.51,7.86;14,154.08,510.21,326.51,7.86;14,154.08,521.17,91.01,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,242.90,477.33,237.70,7.86;14,154.08,488.29,97.26,7.86">Dependency parsing and domain adaptation with lr models and parser ensembles</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,272.81,488.29,207.78,7.86;14,154.08,499.25,326.51,7.86;14,154.08,510.21,223.85,7.86">Proceedings of the CoNLL 2007 Shared Task. Joint Conferences on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL&apos;07)</title>
		<meeting>the CoNLL 2007 Shared Task. Joint Conferences on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL&apos;07)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1044" to="1050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.08,532.13,326.51,7.86;14,154.08,543.09,295.75,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="14,200.01,532.13,280.58,7.86;14,154.08,543.09,125.95,7.86">ABNER: an open source tool for automatically tagging genes, proteins and other entity names in texts</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,286.82,543.09,58.56,7.86">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="3191" to="3192" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,154.08,554.04,326.52,7.86;14,154.08,565.00,326.51,7.86;14,154.08,575.96,326.51,7.86;14,154.08,586.92,326.51,7.86;14,154.08,597.88,139.90,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="14,264.27,565.00,216.32,7.86;14,154.08,575.96,256.08,7.86">The DARPA machine reading program -encouraging linguistic and reasoning research with a series of reading tasks</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Herr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Keesing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Oblinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schrag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,432.52,575.96,48.07,7.86;14,154.08,586.92,326.51,7.86;14,154.08,597.88,42.28,7.86">Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC&apos;10)</title>
		<meeting>the Seventh International Conference on Language Resources and Evaluation (LREC&apos;10)<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
