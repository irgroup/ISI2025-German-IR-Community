<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,140.42,152.67,314.22,12.64;1,265.37,170.67,64.58,12.64">Index Expansion for Machine Reading and Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,211.61,210.18,67.14,8.96"><forename type="first">Giuseppe</forename><surname>Attardi</surname></persName>
							<email>attardi@di.unipi.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica</orgName>
								<orgName type="institution">Università di Pisa</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,285.17,210.18,46.54,8.96"><forename type="first">Luca</forename><surname>Atzori</surname></persName>
							<email>atzoril@di.unipi.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica</orgName>
								<orgName type="institution">Università di Pisa</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,338.35,210.18,45.08,8.96"><forename type="first">Maria</forename><surname>Simi</surname></persName>
							<email>simi@di.unipi.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica</orgName>
								<orgName type="institution">Università di Pisa</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,140.42,152.67,314.22,12.64;1,265.37,170.67,64.58,12.64">Index Expansion for Machine Reading and Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3225919505B1035513368D61D6489E65</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>index expansion</term>
					<term>question answering</term>
					<term>machine reading</term>
					<term>information retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The paper reports our experiments in tackling the CLEF 2012 Pilot Task on Machine Reading for Question Answering. We introduce the technique of index expansion, which relies on building a search index enriched with information gathered from a linguistic analysis of texts. The index provides a highly tangled representation of the sentences where each word is directly connected to others representing both meaning and relations. Instead of keeping the knowledge base separate, the relevant knowledge gets embedded within the text. We can hence use efficient indexing techniques to represent such knowledge and query it very effectively. We explain how index expansion was used in the task and describe the experiments that we performed. The results achieved are quite positive and a final error analysis shows how the technique can be further improved.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="697.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The paper reports our experiments in tackling the CLEF 2012 Pilot Task on Machine Reading for Question Answering. The task aimed at exploring the ability of a machine reading system to answer questions about a scientific topic, in particular about the Alzheimer disease, using knowledge extracted from biomedical texts. The evaluation has the format of traditional Multiple Choice Reading Comprehension tests: it involves reading a single scientific article at a time and answering a set of questions regarding information that is stated or implied in the text. Multiple choice answers are provided for each question, each having five options with only one being correct. A background collection of reference articles about the topic is provided by the organizers, which may be exploited for learning knowledge useful in answering the questions. Nonetheless the principal answer is to be found among the facts presented in the given test document.</p><p>The rationale of the task <ref type="bibr" coords="1,240.98,638.27,16.91,8.96" target="#b12">[13]</ref> is to concentrate on the step of Answer Validation which is the last one in a traditional QA pipeline (Question Analysis, Retrieval, Answer Extraction, Answer Selection/Validation).</p><p>We approach the task using a technique that we call "index expansion", which is the dual to query expansion. Query expansion adds terms to the query in order to achieve better recall, but this often results in poorer precision, since, because of term polysemy, the terms introduced may have also quite different meaning and may match irrelevant documents, introducing a lot of noise in the results.</p><p>Index expansion instead keeps the original query terms but adds variants of terms to the index, for example synonyms and hyperrnyms. This introduces much less noise, since if the variant term is used in the query, then it is relevant and the match will be successful. On the other hand, inappropriate terms will not be used in the queries and hence their presence in the index will not affect negatively the results. The only drawback of the approach is that the size of the index increases, but this is a minor issue, since the index is compressed and disk space is cheap. In other words we trade space for speed and accuracy, which is akin to the technique of "database denormalization".</p><p>The approach extends the one that we proposed and applied successfully to the task of Blog Opinion Mining at TREC 2006 <ref type="bibr" coords="2,285.02,318.21,10.71,8.96" target="#b2">[3]</ref>.</p><p>The index is enriched with information extracted from linguistic analysis of the documents. The index in fact can be seen as a multilayer index, where each layer represents one kind of annotation. The layers are:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>─ form</head><p>the original text ─ lemma the lemma of each word ─ POS the POS of each word ─ head the governor of each word ─ deprel the dependency relation of each word with its governor ─ lemma the lemmas of each word ─ synonyms list of synonyms for each word ─ hypernyms list of hypernyms for each word</p><p>The process of answering a question relies on a similar analysis of text: questions are parsed and from the parse tree a basic query in the DeepSearch query language is generated. To each of these queries one of the multiple possible answers is added. Those queries which obtain an answer with the highest score are considered to include the correct answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Document Analysis</head><p>The collection of background documents about Alzheimer's Disease provided for the task included: around 66,000 abstracts from PubMed 1 ; around 8,000 Open Access full articles from Central 2 ; full articles about the key hypotheses in Alzheimer Disease published by Elsevier. The documents were provided also in a preprocessed format, split into sentences and tokens, each one annotated with lemma, POS, dependency parsing annotations from the dependency parser GDep <ref type="bibr" coords="3,266.61,150.18,15.49,8.96" target="#b14">[15]</ref>, and two types of Named Entities, one from a UMLS-based NE tagger developed at CLiPS, and one from the ABNER NE tagger <ref type="bibr" coords="3,124.70,174.18,15.43,8.96" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Index Expansion</head><p>The idea of index expansion is to create a highly tangled representation of the sentences where each word is directly connected to others representing both meaning and relations. Instead of keeping the knowledge base separate, the relevant knowledge gets embedded within the text. We can hence use efficient indexing techniques to represent such knowledge and query it very effectively with suitably modified techniques of information retrieval. The first step of the process is to analyze the sentences and annotate them with syntactic and semantic tags. Despite the fact that documents were provided preprocessed with the GDep parser, we preferred to parse them with a parser that could produce Stanford Dependencies, which provide more refined analysis with respect to standard parsers and whose annotation is closer to represent semantic roles. In particular we will make use of the ability to distinguish apposition.</p><p>Besides syntactic information, we looked for some kind of semantic information, in particular grouping syntactic variants and identifying synonyms and hypernyms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Synonym expansion</head><p>We explored using the Unsupervised Semantic Parser by <ref type="bibr" coords="3,357.18,437.25,15.54,8.96" target="#b13">[14]</ref>, in order to obtain a set of synonyms or related terms to annotate the documents. USP transforms dependency trees in the Stanford Dependencies notation <ref type="bibr" coords="3,295.37,461.72,11.08,8.53" target="#b8">[9]</ref> into quasi-logical forms and clusters them to abstract away syntactic variations of the same meaning. The approach seemed promising, in particular since USP had been trained on the Genia Corpus. Results on the Genia corpus showed the USP ability to discover similar terms, albeit sometimes with opposite polarity, and alternative phrasing for the same concept. For example, here is a sample of the top clusters from the Genia corpus: USP requires the input annotated with Stanford Dependencies <ref type="bibr" coords="4,393.22,218.22,10.93,8.96" target="#b8">[9]</ref>. These dependencies are typically produced using the Stanford Parser which produces constituency trees that are then converted to dependency trees.</p><p>This process turned out to be too slow to handle large collections. Therefore we decided to train the DeSR dependency parser <ref type="bibr" coords="4,312.61,266.22,11.76,8.96" target="#b0">[1]</ref> on a version of the Penn TreeBank annotated with basic Stanford Dependencies. This produced a native parser for Stanford dependencies which outperforms the Stanford parser itself in accuracy<ref type="foot" coords="4,427.99,288.06,3.24,5.83" target="#foot_2">3</ref> (91.18 % of Unlabeled Attachment Score), but most importantly in performance, reducing significantly the parsing. This is due to the fact that DeSR algorithm has linear complexity and parsing a sentence takes in the order of hundredth of a seconds, while the Stanford parser is cubic and the time grows to dozen of minutes for long sentences.</p><p>We applied USP to the Elsevier corpus. We had to overcome a number of problems with the implementation, in particular: ─ the handling of apostrophes, which the parser introduces to denote copy nodes and therefore confuse USP ─ the handling of underscores, whose presence in words also confused USP ─ infinite loops due to the presence of cycles in the dependencies After fixing these problems, we had to partition the Elsevier collection into four smaller subsets because the parser was too slow to handle it as a whole. Unfortunately the clusters that we were able to obtain were not very significant. Here are the top clusters from the first subset: ─ furthermore, however ─ rinse, wash ─ moreover, thus ─ recent, previous ─ feature, manifestation ─ nacl, hepes ─ experimental, procedures ─ the (dep body), the (amod body), the (nn body), the (num body), the (nsubj body), the (advmod body), the (poss body), the (nsubjpass body), … ─ phosphorylation (prep_at s45), phosphorylation (prep_of s45), phosphorylation (amod s45), …</p><p>Except for the first few, all the remaining ones were cases of alternative parses of the same phrase, e.g. 'body' with different dependency relations: 'amod', 'num' or 'nsubj'. Therefore the clusters turned out not to be very useful for our purposes.</p><p>Hence as an alternative we resorted to use WordNet <ref type="bibr" coords="5,355.15,150.18,11.72,8.96" target="#b8">[9]</ref> to extract synonyms and hypernyms. Two layers of annotation were added to represent them for each term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Appositions and Acronyms</head><p>We exploit the dependency annotations as appositions, to add the apposition at the position of the head. For example, "Abeta" occurs as an apposition for "amyloid beta": the term "Abeta" is then added as a synonym in correspondence of "beta" and hence it will inherit any relations that the phrase has.</p><p>Similarly the expansion of acronyms can be added as variants of terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Syntactic Variants</head><p>Syntactic variants are also added to the expanded index. For example to deal with alternative passive forms, when a term has the dependency "nsubj_pass", to the index is also added the same term with a tag as "dobj", while the corresponding "agent " is annotated also as "nsubj".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Indexing</head><p>The search engine that we used in our experiments is called DeepSearch and is built by means of IXE <ref type="bibr" coords="5,196.06,408.21,10.73,8.96" target="#b0">[1]</ref>, an Open Source search engine library in C++ that we have been developing along several years. DeepSearch provides facilities for passage retrieval, so that it can return individual passages, in our case sentences, matching queries. Scoring is based on a relevance metric for the match within one sentence or within a number of adjacent sentences.</p><p>For this task we exploited the capability of IXE of dealing with multiple layers in documents. A layer is an overlay of different terms on the same document. Each layer has its own full-text index and can be queried independently. However the layers can be considered as stacked and querying performed across layers, for instance one can find a word with a given POS by searching for the word in the text layer that has the given POS in the same position in the POS layer. Also proximity and phrase searches that rely on term positions can exploit this.</p><p>Besides having multiple layers, a layer can have multiple terms in the same position. This feature will be exploited to deal with variants, like synonyms and hypernyms.</p><p>Notice that in among both synonyms and hypernyms we include the lemma itself, hence there is no loss in generality when querying on these layers.</p><p>Finally, the column for dependencies is dealt specially in order to enable searching for terms that are syntactically related. IXE exploits the fact that heads of terms represents positions, and list of positions are already represented in an inverted index. Therefore a special posting list with position is created for the heads column. The technique used for search is based on the Small Adaptive Set Intersection <ref type="bibr" coords="5,444.43,660.26,10.89,8.96" target="#b0">[1,</ref><ref type="bibr" coords="5,459.82,660.26,7.26,8.96" target="#b7">8]</ref>, which relies on cursors, which are scanned in parallel, until a match is found at the same document position.</p><p>The SASI algorithm has been extended to handle dependencies queries, which look for two related terms d and h, where h is the head of d. A special dependency cursor uses three inner cursors, cursor C d on the posting list of d, cursor C h for that of h and a cursor H d on the postings of the heads of d. It first uses C d and C h to find matches for d and h. Whenever such a match is found, it checks that the value stored in the posting list of H at the position of d corresponds to the position of h. Having a single posting list for all terms, rather than having one for each term, allows repeating the last step and checking whether there is a transitive dependency. Despite the fact that the posting lists for heads becomes very long, the algorithm is still quite fast since it exploits skip lists to quickly scan such list.</p><p>To illustrate what goes into the index, we show here the annotated parse tree for one sentence from one of the reading documents (with slight corrections of parser mistakes) that we will also use as an example for the process of answer selection: Each column in the table represents a layer, which we represent here vertically for easier readability. Most layers contain a single term. The terms in the last two layers are to be considered as variants; hence they appear in the index as if they had the same position. For eample the sentence will match a query for both form "Semagatestac" and synonym "check". Notice that the presence of a possibly irrelevant synonym like "run" would not affect any search that does not use it explicitly, which will happen instead if one performs query expansion with synonyms.</p><formula xml:id="formula_0" coords="6,141.62,323.60,15.28,7.24">form</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Query Language</head><p>The DeepSearch query language allows specifying conditions occurring simultaneously at the same position in different layers. Each layer is identified by its name, so for example: ne:protein matches the term protein in the "ne" (Named Entity) layer.</p><p>dep:nsubj matches the term "nsubj" in the "dep" (Dependency Relation) layer. The cooccurrence of these condition at the same position, i.e. looking for a term which is both a protein and the subject of a verb, can be specified using the align operator "|": ne:protein|dep:nsubj Moreover one can specify the presence of a syntactic dependency, either direct or indirect, between two terms, like in this example:</p><p>(ne:protein|dep:nsubj &lt;-lemma:test) Indirect dependencies are useful since quite often terms are connected though intermediate prepositions.</p><p>Dependencies can be chained, as in: (phase &lt;-lemma:trial &lt;-lemma:test) These queries can be tested in our online demo accessible at: http://semawiki.di.unipi.it/alzheimer/ 4 Question Answering</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Query generation</head><p>Questions are processed similarly to documents, using the parsers DeSR and then USP, in order to obtain a parse tree with semantic annotations. A script analyzes the annotated parse tree and generates a query for the enriched index.</p><p>Here is an example of the processing of one of the questions in the evaluation:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What candidate drug that blocks the γ-secretase is now tested in clinical trials?</head><p>The sentence is parsed with Stanford dependencies and then expanded with synonyms, hypernyms and syntactic variants, obtaining the following layers (omitting some of the synonyms, hypernyms): Among the hypernyms for the word "candidate" appear terms like "politician". It is fairly clear that if these terms were used for expanding the query, quite confusing results might be retrieved, unless one does some sophisticated kind of word sense disambiguation to discard those terms. In our case though, the last three columns of the analyzed sentence are discarded during query generation.</p><p>The query generator creates a basic DeepSearch query, which includes relevant terms and both syntactic and semantic features from the query. A list of clauses is produce which are combined in a Boolean disjunctive query. A few heuristics are used in producing such clauses, for instance the possible answer is not included if it is already present in another clause from the question.</p><p>For the above example the query generator produces this base DeepSearch query: candidate OR syn:candidate OR drug OR syn:drug OR γ-secretase OR syn:γ-secretase OR clinical OR syn:clinical OR lemma:candidate OR (ne:protein &lt;-lemma:test) OR syn:drug Five variants of this query are submitted by adding to each, one of the possible multiple answers. The one with the addition of "Semagacestat" returns two results, with the highest score of 19.35. The query with the addition of "LPR1" returns one result with a score of -1.64; the addition of "biochemical" returns 21 results with the best score of 4.24; the addition of "AD" obtains 9 results, with the best score of 9.18; the addition of "PSEN1" returns 21 results, the highest with a score of 5.51. Hence "Semagacestat" is selected as the correct answer, as it is indeed. The sentence retrieved with the highest score is the one presented in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Answer Selection</head><p>Most Question Answering system perform sophisticated processing on the candidate answers, in order to determine which one is the most appropriate one. This processing sometimes involves complex reasoning based on theorem proving techniques <ref type="bibr" coords="9,451.27,308.25,15.58,8.96" target="#b10">[11]</ref>: the answer is transformed in some form of first order logic formula and an attempt is made to prove that the formula entails the question either directly or by abduction.</p><p>Our system instead relies only on the ranking provided by the DeepSearch engine. If the engine does not return any answer for a given query, the candidate solution is discarded. When more than one query has answers, the one is chosen whose first answer has the highest rank.</p><p>One limitation of the approach is due to the fact we employ a passage retrieval engine, which splits documents at the sentence level and returns sentences that match the query. We had planned to apply the anaphora resolution tool <ref type="bibr" coords="9,392.23,416.25,11.72,8.96" target="#b4">[5]</ref> that we had developed for SemEval 2011 to add an anaphora layer to our expanded index, but time limitation prevented us from doing it. The problem is somewhat alleviated by exploiting feature of the passage retrieval engine, which considers matches occurring also in adjacent sentences, albeit with a lower score. This is controlled by parameter Con-secutivePassage in DeepSearch, which was set to 2 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>The metric for the evaluation is c@1 <ref type="bibr" coords="9,280.85,533.39,15.43,8.96" target="#b11">[12]</ref>, which is based on the number of correct single answers but takes into account the option of not answering certain questions.</p><p>We submitted a single run for evaluation, which obtained a cumulative c@1 score of 0.55, with the following breakdown on the four documents: We briefly investigate the reason for the failures.</p><formula xml:id="formula_1" coords="9,186.02,594.83,201.19,9.77">Reading Test n n R n U c@</formula><p>For the 8 th question the query generated contained the phrase "affinity chromatography", but neither the term "affinity" nor "chromatography" appear in the document.</p><p>For question 7, the query with "EM" did not have the highest score. The answer was hard to find, because the relevant sentence was twisted, saying that "protocols for the purification of … γ-secretase … allowed the reconstitution of 3D structures … by EM". Some form of splitting and rewriting of the sentence might help with cases like this.</p><p>Finally, in question 9 "wild-type" prevailed over "P436Q", because the generated query did not constrain the term "wild-type" to be connected with "mutation".</p><p>These errors appear that could be reduced by improving some aspects of index expansion and query generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>The approach of index expansion relies on enriching the index with information which is gathered from a linguistic analysis of texts. Differently from traditional approaches to Question Answering, where an abstract separate formal representation of texts is produced and then queried through some logical reasoning process, our approach keeps the information in the text itself.</p><p>The idea is to create a highly tangled representation of the sentences where each word is directly connected to others representing both meaning and relations. Instead of keeping the knowledge base separate, the relevant knowledge gets embedded within the text. We can hence use efficient indexing techniques to represent such knowledge and query it very effectively with suitably modified techniques of information retrieval.</p><p>The approach proved fairly effective for the Pilot task of Machine Reading for Question Answering. In the experiments we incurred into some limitations of the tools we had planned to use. In particular it is important for the approach to be able to identify alternative forms of expressions, in particular synonyms and hypernyms, which are specific to the domain. We had some success in doing this by using dependency relations produced by a statistical dependency parser. In particular the apposition relation allowed identifying synonyms and the recognition of passive forms allowed normalizing them. We also tried some recent tools for semantic analysis that in principle could have provided additional versions of linguistic variants, more domain specific. This did not work as expected and we had to resort to general linguistic knowledge provided by WordNet. We also did not have time to incorporate our tool for coreference resolution.</p><p>We hope that exploiting better or tuned versions of the tools for syntactic and semantic analysis of text, the index expansion approach can provide an effective solution to answer validation in the context of question answering.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,124.94,541.19,345.64,153.11"><head></head><label></label><figDesc>Items no. 5, 7 and 9 clearly include opposites, but the remaining ones are indeed either syntactic variants or have similar meaning.</figDesc><table coords="3,124.94,541.19,345.64,153.11"><row><cell>13. srf, e2f, nfkappab, nf-kappab, hgata-3, nf-kb</cell></row><row><cell>14. susceptibility, sensitivity</cell></row><row><cell>15. govern, effect, control, specify, regulate, modulate, mediate</cell></row><row><cell>1. overexpression, over-expression</cell></row><row><cell>2. sustain, have</cell></row><row><cell>3. expression, co-expression, accumulation</cell></row><row><cell>4. receptor-alpha, receptor</cell></row><row><cell>5. low, highest</cell></row><row><cell>6. display, exhibit</cell></row><row><cell>7. htlv-i-infected, human, lipopolysaccharide-stimulated, uninfected</cell></row><row><cell>8. greater, higher, lower</cell></row><row><cell>9. emphasize, support</cell></row><row><cell>10. alteration, change</cell></row><row><cell>11. novel, many, other, new, known, latter, various, multiple, individual, several, cer-</cell></row><row><cell>tain, respective, additional</cell></row><row><cell>12. previously, originally</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,136.10,675.19,138.33,8.10"><p>http://www.ncbi.nlm.nih.gov/pubmed/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,136.10,686.23,124.89,8.10"><p>http://www.ncbi.nlm.nih.gov/pmc/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,136.10,686.23,281.39,8.10"><p>https://sites.google.com/site/desrparser/Announcements/stanforddependencies</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments.</head><p><rs type="person">Mihai Surdeanu</rs> provided us access to a version of the Penn Treebank annotated with basic Stanford Dependencies and assisted us in the training of the DeSR parser.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="11,132.67,549.04,338.07,8.10;11,141.74,560.08,242.81,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,186.86,549.04,118.95,8.10">IXE at the TREC Terabyte Task</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Attardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,326.11,549.04,144.63,8.10;11,141.74,560.08,116.24,8.10">Proceedings of The Forteenth Text Retrieval Conference (TREC 2005)</title>
		<meeting>The Forteenth Text Retrieval Conference (TREC 2005)<address><addrLine>Gaithersburg (MD)</addrLine></address></meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.67,571.12,338.05,8.10;11,141.74,582.04,315.92,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,189.02,571.12,263.99,8.10">Experiments with a Multilanguage Non-Projective Dependency Parser</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Attardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,141.74,582.04,222.28,8.10">Proc. of the Tenth Conference on Natural Language Learning</title>
		<meeting>of the Tenth Conference on Natural Language Learning<address><addrLine>New York, (NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.67,593.08,338.19,8.10;11,141.74,604.12,296.67,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,221.81,593.08,153.41,8.10">Blog Mining Through Opinionated Words</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Attardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Simi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,382.63,593.08,88.23,8.10;11,141.74,604.12,167.68,8.10">Proceedings of The Fifteenth Text Retrieval Conference (TREC 2006)</title>
		<meeting>The Fifteenth Text Retrieval Conference (TREC 2006)<address><addrLine>Gaithersburg (MD)</addrLine></address></meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.67,615.04,338.19,8.10;11,141.74,626.08,191.65,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,273.78,615.04,63.34,8.10">The Tanl Pipeline</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Attardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dei Rossi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Simi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,355.75,615.04,115.11,8.10;11,141.74,626.08,137.35,8.10">Proc. of Workshop on Web Services and Processing Pipelines in HLT</title>
		<meeting>of Workshop on Web Services and essing Pipelines in HLT<address><addrLine>Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.67,637.12,337.82,8.10;11,141.74,648.07,251.04,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,278.50,637.12,191.99,8.10;11,141.74,648.07,89.61,8.10">TANL-1: Coreference Resolution by Parse Analysis and Similarity Clustering</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Attardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dei Rossi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Simi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,249.65,648.07,81.12,8.10">Proc. of SemEval 2010</title>
		<meeting>of SemEval 2010<address><addrLine>Uppsala</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,132.67,659.11,338.06,8.10;11,141.74,670.15,204.61,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,272.93,659.11,180.98,8.10">Tuning DeSR for Dependency Parsing of Italian</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Attardi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Simi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zanelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,141.74,670.15,75.84,8.10">Proc. of Evalita 2011</title>
		<meeting>of Evalita 2011</meeting>
		<imprint>
			<publisher>Springer LNCS</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="11,132.67,681.07,322.36,8.10" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="11,217.13,681.07,150.18,8.10">Natural Language Processing with Python</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Looper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>O&apos;Reilly Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.67,149.99,338.07,8.10;12,141.74,161.03,328.86,8.10;12,141.74,172.07,92.83,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,311.84,149.99,158.90,8.10;12,141.74,161.03,28.57,8.10">Adaptive set intersections, unions, and differences</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">D</forename><surname>Demaine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>López-Ortiz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">I</forename><surname>Munro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,191.90,161.03,278.70,8.10;12,141.74,172.07,27.94,8.10">Proc. of the 11th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)</title>
		<meeting>of the 11th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="743" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.67,182.99,338.06,8.10;12,141.74,194.03,328.84,8.10;12,141.74,205.07,24.09,8.10" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,279.05,182.99,175.22,8.10">The Stanford typed dependencies representation</title>
		<author>
			<persName coords=""><forename type="first">M.-C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,141.74,194.03,328.84,8.10">COLING 2008 Workshop on Cross-framework and Cross-domain Parser Evaluation</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.40,215.99,338.31,8.10;12,141.74,227.03,108.13,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="12,193.89,215.99,157.63,8.10">WordNet: A Lexical Database for English</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,359.23,215.99,106.23,8.10">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.40,238.07,338.31,8.10;12,141.74,248.99,186.73,8.10" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,221.93,238.07,183.47,8.10">COGEX: A Logic Prover for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,425.83,238.07,44.88,8.10;12,141.74,248.99,75.35,8.10">Proceedings of HLT-NAACL 2003</title>
		<meeting>HLT-NAACL 2003<address><addrLine>Edmonton</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="87" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.40,260.03,338.46,8.10;12,141.74,271.07,328.97,8.10;12,141.74,282.02,328.96,8.10;12,141.74,293.06,233.39,8.10" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,203.11,260.03,267.75,8.10;12,141.74,271.07,66.45,8.10">Overview of ResPubliQA 2009: Question Answering Evaluation over European Legislation</title>
		<author>
			<persName coords=""><forename type="middle">A</forename><surname>Peñas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,308.21,271.07,158.70,8.10;12,168.38,282.02,302.32,8.10;12,141.74,293.06,40.50,8.10">Text Retrieval Experiments. Workshop of the Cross-Language Evaluation Forum. CLEF 2009</title>
		<title level="s" coord="12,188.54,293.06,21.54,8.10">LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6241</biblScope>
		</imprint>
	</monogr>
	<note>Multilingual Information Access Evaluation</note>
</biblStruct>

<biblStruct coords="12,132.40,304.10,338.13,8.10;12,141.74,315.02,231.79,8.10" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,206.45,304.10,264.08,8.10;12,141.74,315.02,69.81,8.10">Overview of QA4MRE at CLEF 2011: Question Answering for Machine Reading Evaluation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,227.33,315.02,71.75,8.10">Proc. of CLEF 2011</title>
		<meeting>of CLEF 2011<address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.40,326.06,338.16,8.10;12,141.74,337.10,319.36,8.10" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,224.93,326.06,111.27,8.10">Unsupervised semantic parsing</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Domingo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,352.39,326.06,118.17,8.10;12,141.74,337.10,187.80,8.10">Proc. of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>of the 2009 Conference on Empirical Methods in Natural Language essing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.40,348.02,338.34,8.10;12,141.74,359.06,320.33,8.10" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,218.09,348.02,252.65,8.10;12,141.74,359.06,48.42,8.10">Dependency parsing and domain adaptation with LR models and parser ensembles</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,206.33,359.06,136.65,8.10">Proc. of the CoNLL 2007 Shared Task</title>
		<meeting>of the CoNLL 2007 Shared Task<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,132.40,370.10,338.01,8.10;12,141.74,381.02,247.32,8.10" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,185.78,370.10,284.63,8.10;12,141.74,381.02,90.50,8.10">ABNER: an open source tool for automatically tagging genes, proteins, and other entity names in text</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,237.77,381.02,52.05,8.10">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="3191" to="3192" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
