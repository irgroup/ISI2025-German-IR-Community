<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,164.05,116.95,287.25,12.62;1,262.21,134.89,90.94,12.62">A Text Mining Approach as Baseline for QA4MRE&apos;12</title>
				<funder ref="#_KDCrrrc #_gRFsXrK">
					<orgName type="full">Research Foundation Flanders</orgName>
				</funder>
				<funder ref="#_yuKmZa8 #_HYUXxAn">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,235.05,172.56,73.14,8.74"><forename type="first">Mathias</forename><surname>Verbeke</surname></persName>
							<email>mathias.verbeke@cs.kuleuven.be</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Katholieke Universiteit Leuven</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,330.89,172.56,49.42,8.74"><forename type="first">Jesse</forename><surname>Davis</surname></persName>
							<email>jesse.davis@cs.kuleuven.be</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Katholieke Universiteit Leuven</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,164.05,116.95,287.25,12.62;1,262.21,134.89,90.94,12.62">A Text Mining Approach as Baseline for QA4MRE&apos;12</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">67DA318D6245C7E3271421BBC4CED75D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Question Answering</term>
					<term>Machine Reading</term>
					<term>Text Mining</term>
					<term>Baseline</term>
					<term>Biomedical Natural Language Processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the KU Leuven DTAI team in the pilot task on machine reading of biomedical texts about the Alzheimer disease, which is part of the 2012 Question Answering for Machine Reading Evaluation campaign (QA4MRE'12). The main objective of our research was to develop a text mining system as a strong baseline for the task. Based on the outcome of this system, we want to investigate which types of questions can be answered based solely on the input text and the question string, and for which ones we need more advanced techniques that also consider the previously acquired background knowledge from the reference document collection. Furthermore this should give us some insights into the system behavior for specific question types and background information for the development of a tailored inference algorithm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine Reading (MR) or Natural Language Understanding <ref type="bibr" coords="1,396.45,458.41,10.52,8.74" target="#b0">[1]</ref> is one of the core objectives since the emergence of Artificial Intelligence and Natural Language Processing. Its main goal is to automatically extract knowledge from unstructured text and to use this knowledge to make decisions or answer questions. The latter is also the focus of the Question Answering for Machine Reading Evaluation (QA4MRE <ref type="bibr" coords="1,206.56,518.19,10.79,8.74" target="#b1">[2]</ref>) campaign, where the task is to read single documents and identify the answers to a set of multiple-choice questions about information that appears explicitly of implicitly in the text.</p><p>The pilot task on machine reading of biomedical texts aims at exploring the ability of systems to answer questions about a specific scientific topic, namely the Alzheimer disease. Question answering in this domain poses additional challenges for natural language processing which mainly arise because domain knowledge is essential for achieving deep understanding in this setting. Consider the following example for the QA4MRE pilot task on MR for biomedical texts:</p><p>Text: [...] Additionally, no estrogen could be detected in the APP23/Ar / mice (data not shown), suggesting that aromatase gene knock-out prevented the conversion of endogenous testosterone into estrogen. [...]</p><p>Question: What experimental approach is useful to create an in vivo system where conversion of testosterone into estrogen is blocked? Candidate Answers:</p><p>1. ELISA analysis 2. hole-board memory task 3. NEP activity assay 4. Western blot 5. knock-out of the aromatase gene</p><p>In this case, the machine reading system should identify 'knock-out of the aromatase gene" as the correct answer.</p><p>In order to evaluate the system behavior for specific question types and investigate the necessity of using and reasoning about the background knowledge contained in the reference document collection, our approach is based on basic text mining techniques and does not make use of any existing resources. It is developed to provide some insights towards a tailored inference algorithm and can be seen as a strong baseline for the task.</p><p>The paper is organized as follows. The methodology of the system built for QA4MRE'12 is presented in section 2. Section 3 discusses the results and analyses them by means of a detailed error analysis. Finally, section 4, concludes and presents ideas for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>The main goal of our approach is to investigate the importance of background knowledge for MR of biomedical texts. Therefore our system is solely based on basic text mining techniques, and does not rely on the reference document collection or any other external resources. Since the outcome should be able to serve as a baseline for the task, preprocessing is kept to a minimum. Sentence splitting is performed, for which we relied on the splits from the preprocessed files provided by the task organizers. For stopword removal, the English stop word corpus provided by NLTK <ref type="bibr" coords="2,277.01,498.78,10.52,8.74" target="#b2">[3]</ref> was used.</p><p>We propose two different strategies, that mainly differ in the order in which the text mining techniques are applied. Both approaches only use the input text, the question string, and the multiple choice answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Approach 1: Question Similarity</head><p>The first approach computes the similarity between each question in the reading test and every sentence in the input document and selects the top k most similar sentences. Next, each sentence votes on an answer by checking to see if it contains (part of) an answer. If it does, the respective answer's vote is incremented by either 1 or the normalized similarity value. The answer with the highest number of votes, i.e., the highest weight, is selected. The pseudocode (Algorithm 1) describes the question similarity approach.</p><p>Algorithm 1 Pseudocode of the question similarity algorithm 1: for all question q in reading test do 2:</p><p>q tok = wordTokenize(q) 3: for all sentence s in input document do 4:</p><p>s tok = wordTokenize(s) 5:</p><p>similarity(q tok, s tok) 6:</p><p>end for 7:</p><p>for all answer a in choice list do 8:</p><p>a tok = wordTokenize(a) 9:</p><p>for all top k sentence ts do 10:</p><p>if element of a tok in ts then 11: incrementVote(a) 12:</p><p>end if 13:</p><p>end for 14:</p><p>return top voted a 15:</p><p>end for 16: end for For the similarity calculation in line 5, we employed two different measures, namely the Jaccard similarity <ref type="bibr" coords="3,264.95,346.11,10.52,8.74" target="#b3">[4]</ref> (Equation <ref type="formula" coords="3,324.38,346.11,4.43,8.74" target="#formula_0">1</ref>) and the MASI distance <ref type="bibr" coords="3,437.66,346.11,10.52,8.74" target="#b4">[5]</ref> (Equation 2). Both measures operate on sets, so both the sentence from the input document as well as the question need to be tokenized to transform them into a set of words. Suppose S is the set of words from a sentence in the input document and Q is the set of words from the question. Then the two measures are defined as follows:</p><formula xml:id="formula_0" coords="3,253.37,427.52,227.22,22.31">Jaccard(S, Q) = |S ∩ Q| |S ∪ Q|<label>(1)</label></formula><p>and,</p><formula xml:id="formula_1" coords="3,234.38,480.34,246.21,22.31">M ASI(S, Q) = 1 - |S ∪ Q| max(|S|, |Q|)<label>(2)</label></formula><p>To convert the MASI distance into a similarity measure, we subtract it from 1. As can be seen from the formula, the MASI distance is a weighted version of the Jaccard similarity, that takes into account partial agreement between sets, by downweighting the Jaccard score when partial overlap between the sets exists.</p><p>For tokens that refer to numbers, we also check if the written form of numbers is contained in the sentence. In future work we also plan to integrate the reverse, i.e. if the numerical form of a written token appears in the sentence.</p><p>We also explored the following variations of algorithm 1:</p><p>Answer concatenation Instead of first calculating the similarity between the question and the sentence, followed by checking if the answer appears in the sentence, the question and the answer can also be concatenated before the similarity calculation. Subsequently the concatenation of the question and every answer is compared with every sentence in the input document. Lastly, the average similarity of the k most similar sentences is calculated from which the best one is chosen.</p><p>Weighting by similarity Instead of giving the same weight to each answer when it appears in a top k sentence, the answers can also be weighted with the calculated similarity value. This sum of similarity values is then normalized by the number of times the answer was voted for, before it is used to select an answer based on majority voting.</p><p>Overall similarity If we drop the top k requirement when voting for the answer in the last step of the algorithm, all sentences that are not completely different from the question (i.e. have a MASI and Jaccard similarity different from 0) are taken into account in the voting process. The answer that is contained in the greatest number of the sentences is chosen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Approach 2: Answer Containment</head><p>The second approach reverses the procedure of the question similarity approach, and first checks if the answer appears in the sentence. If it does, the similarity is calculated and the sentence with the highest similarity is selected, which can be seen from the pseudocode in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2</head><p>Pseudocode of the answer containment algorithm</p><p>1: for all question q in reading test do 2: q tok = wordTokenize(q) 3: containingSentences = [ ] 4:</p><p>for all answer a in choice list do 5:</p><p>a tok = wordTokenize(a) 6: for all sentence s in input document do 7:</p><p>s tok = wordTokenize(s) 8:</p><p>if element of a tok in s tok then 9:</p><p>containingSentences.append(s tok) 10:</p><p>end if 11:</p><p>end for 12:</p><p>for all sentence cs in containingSentences do 13: cs tok = wordTokenize(cs) 14: similarity(q tok, cs tok) 15:</p><p>end for 16:</p><p>for all top k sentence ts do 17: incrementVote(a) 18:</p><p>end for 19:</p><p>return top voted a 20:</p><p>end for 21: end for For the similarity calculation, the weighting by similarity approach as described for Algorithm 1 is used, with MASI as the similarity measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Discussion</head><p>These two approaches and the described variations were tested on the 40 questions from the four reading tests. The scoring of the output for each reading test is done using c@1 <ref type="bibr" coords="5,214.83,219.28,9.96,8.74" target="#b5">[6]</ref>, an evaluation measure between 0 and 1 that rewards systems that, while maintaining the number of correct answers, are able to reduce the number of incorrect ones by leaving some questions unanswered. It can be formulated as follows:</p><formula xml:id="formula_2" coords="5,258.33,277.58,218.02,22.31">c@1 = 1 n (n R + n U n R n ) (<label>3</label></formula><formula xml:id="formula_3" coords="5,476.35,284.32,4.24,8.74">)</formula><p>where n R : the number of correctly answered questions n U : the number of unanswered questions n: the number of questions Furthermore, accuracy was measured.</p><p>In Table <ref type="table" coords="5,187.57,369.40,3.87,8.74" target="#tab_0">1</ref>, the question answering evaluation for each run is listed. It contains the number of answered and unanswered questions, together with the accuracy and c@1 measure over all readings tests. Furthermore, it shows the used algorithm, its parameter setting for k (i.e. the number of most similar sentences taken into account in the voting process), the employed similarity measures and the variations to the basic algorithm. Based on the results for the full dataset, the following observations can be made:</p><p>-For the standard version of the first algorithm, a higher k value has a positive influence on the results. This is caused by the fact that when k is set to 5, for some questions, none of the answers appear in the selected sentences, meaning they do not get answered (respectively 7 and 8 questions, in run 01 and 02), whereas with a higher k, the correct answer can be selected in some of the cases. During development, higher numbers of k were tried on the sample reading test, but this did not yield better results. -In algorithm 2, it is checked if the answer tokens appear in sentences of the input document at the start of the algorithm. Since for every question, one or more of the answers could be found verbatim in the input document, this gave no option for leaving the question unanswered, which seems to have a negative effect on the results. -The variation of algorithm 1, where the answers are weighted by similarity, gives on average the best results, with a score of 0.30 for both accuracy and c@1. Furthermore, this seems to be independent of the similarity measure used, with equal scores for MASI and Jaccard.</p><p>-Using the answer concatenation and overall similarity variants of algorithm 1 resulted in small improvements when compared to the standard version, where MASI was used as the similarity measure.</p><p>Table <ref type="table" coords="6,176.48,354.34,4.98,8.74" target="#tab_1">2</ref> lists the results for the four individual reading tests. For each reading test, the c@1 score is shown, whereas also the median, the average and the standard deviation over all the reading tests are given. From these results, the following observations can be made:</p><p>-On average over all runs, reading test 2 gave the best results (average c@1 of 0.317), followed by test 4 (0.259), test 3 (0.24) and test 1 (0.121). Although test 2 is also the one with the highest standard deviation, whereas task 4 has the lowest. -The best scoring runs, namely 06 and 10, which both use the weighted by similarity variation of algorithm 1, also have the lowest standard deviation, which could be an indication that this algorithm is more robust to different types of questions. -The highest individual reading test score of 0.50 is achieved by run 09 for test 2. The overall similarity approach used for this run highly resembles the weighting by similarity variant, except for the top k requirement. If the answer to some of the questions in reading test 2 was only mentioned in sentences that were more dissimilar than the k most similar ones, this could explain this result.</p><p>Also at the individual question level, we analyzed our results, with the following main observations:</p><p>-The following questions could be correctly answered in at least half of the runs (the number before the dot indicates the reading test, the number behind it the question, and the one between brackets the number of runs in which it was answered correctly): 1.  <ref type="bibr" coords="7,168.13,309.17,11.62,8.74" target="#b5">(6)</ref>. For all of these questions, the input document contained sentences that included one or multiple tokens from the question together with the answer. -The proposed approach does not perform an analysis of the question beforehand. Doing so could help in correctly answering a substantial number of questions. For example, in question 4.4, the experimental technique that was used to purify the γ-secretase complex is asked for. Since only 2 of the 5 questions deal with experimental techniques, the other answers could be discarded beforehand. -13 out of the 40 questions were incorrectly answered in all runs. A deeper analysis should confirm this, but we suppose answering these questions requires combining information from multiple sentences, performing some form of inference.</p><p>When the test set preparation procedure is made available, we would like to analyze to which level of question difficulty our system is able to answer questions correctly. In order to do this, it should be known which facts are explicitly stated in the text, which ones are present but are not explicitly related and for which facts inference is needed to connect them and form the answer. This will also clarify to what extent the reference background document collection is a prerequisite to answer certain (types of) questions. In addition, this will also give some insights on which kinds of textual inference and features are important for the task, i.e., lexical information (e.g., acronym, synonymy, hyperonymyhyponymy), syntactic structures (e.g., nominalization-verbalization, causative, paraphrase, active-passive) or the identification of discourse relations (e.g., coreference, anaphora ellipsis).</p><p>From our current system design, it is clear that only questions for which the answer is mentioned in the same sentence can be answered, since it does not combine information at the document level. Nor does it consult the background collection or perform inference to reason about facts stated in the text. In a more extensive error analysis, we will also analyze the influence of the similarity measure on the results.</p><p>Furthermore, the answer level needs to be analyzed, in order to determine which types of answers the system is able to distinguish. For example, can the system discriminate between different answers if they are all enzymes, or only answer questions that ask for an entity of which the type is unique in the answer list? Currently, the only preprocessing that is done at the answer level is the transformation of verbatim numbers to their numerical form, but other transformations may be necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>We presented the methodology of our system for machine reading of biomedical texts about the Alzheimer disease. Question answering about a scientific topic poses additional challenges to a MR system, which is mainly caused by the increased importance of the background knowledge. The main purpose of our approach was to investigate how far-reaching this influence was. Therefore we developed a system using basic text mining techniques, without considering any external resources.</p><p>The proposed approach can be seen as a strong baseline for the task. Futhermore, the error analysis of the results provides valuable insights for further improvement. In future work, we want adopt this knowledge to develop a tailored inference algorithm for this task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,96.10,464.07,423.10,143.71"><head>Table 1 .</head><label>1</label><figDesc>QA level evaluation for each run</figDesc><table coords="5,96.10,464.07,423.10,132.80"><row><cell></cell><cell></cell><cell>unanswered</cell><cell></cell><cell></cell><cell cols="2">answered</cell><cell>all</cell><cell cols="2">algorithm</cell></row><row><cell cols="9">Run # Right Wrong Empty # Right Wrong Acc. C@1 algor. k similarity</cell><cell>variation</cell></row><row><cell>01 7</cell><cell>0</cell><cell>0</cell><cell>7</cell><cell>33</cell><cell>7</cell><cell>26</cell><cell>0.18 0.21 alg. 1 5</cell><cell>MASI</cell><cell>n/a</cell></row><row><cell>02 8</cell><cell>0</cell><cell>0</cell><cell>8</cell><cell>32</cell><cell>6</cell><cell>26</cell><cell>0.15 0.18 alg. 1 5</cell><cell>Jaccard</cell><cell>n/a</cell></row><row><cell>03 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>40</cell><cell>9</cell><cell>31</cell><cell>0.23 0.23 alg. 1 10</cell><cell>MASI</cell><cell>n/a</cell></row><row><cell>04 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">40 10</cell><cell>30</cell><cell cols="2">0.25 0.25 alg. 1 10 Jaccard</cell><cell>n/a</cell></row><row><cell>05 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">40 10</cell><cell>30</cell><cell>0.25 0.25 alg. 1 5</cell><cell>MASI</cell><cell>Answer concat.</cell></row><row><cell>06 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">40 12</cell><cell>28</cell><cell cols="2">0.30 0.30 alg. 1 n/a MASI</cell><cell>Weighting</cell></row><row><cell>07 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>40</cell><cell>8</cell><cell>32</cell><cell>0.20 0.20 alg. 2 5</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell>08 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>40</cell><cell>8</cell><cell>32</cell><cell>0.20 0.20 alg. 2 10</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell>09 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">40 10</cell><cell>30</cell><cell>0.25 0.25 alg. 1 n/a</cell><cell>n/a</cell><cell>Overall similarity</cell></row><row><cell>10 0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="2">40 12</cell><cell>28</cell><cell cols="2">0.30 0.30 alg. 1 n/a Jaccard</cell><cell>Weighting</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,306.29,657.11,174.30,8.74"><head>Table 2 .</head><label>2</label><figDesc>Detailed QA level evaluation per reading test 4.6</figDesc><table coords="6,306.29,657.11,174.30,8.74"><row><cell>2 (5), 2.6 (9), 2.8 (7), 3.3 (5), 4.2 (8) and</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Acknowledgements</head><p><rs type="person">Mathias Verbeke</rs> is funded by the <rs type="funder">Research Foundation Flanders</rs> (<rs type="projectName">FWO</rs>-project <rs type="grantNumber">G.0478.10 -</rs><rs type="projectName">Statistical Relational Learning of Natural Language</rs>). <rs type="person">Jesse Davis</rs> is partially supported by the <rs type="funder">Research Foundation Flanders</rs> (<rs type="projectName">FWO</rs>-project <rs type="grantNumber">G.0356.12 -A</rs> <rs type="projectName">Synergestic Approach to Extraction, Learning and Reasoning for Machine Reading</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_KDCrrrc">
					<idno type="grant-number">G.0478.10 -</idno>
					<orgName type="project" subtype="full">FWO</orgName>
				</org>
				<org type="funded-project" xml:id="_gRFsXrK">
					<orgName type="project" subtype="full">Statistical Relational Learning of Natural Language</orgName>
				</org>
				<org type="funded-project" xml:id="_yuKmZa8">
					<idno type="grant-number">G.0356.12 -A</idno>
					<orgName type="project" subtype="full">FWO</orgName>
				</org>
				<org type="funded-project" xml:id="_HYUXxAn">
					<orgName type="project" subtype="full">Synergestic Approach to Extraction, Learning and Reasoning for Machine Reading</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.35,548.46,342.25,7.86;8,146.91,559.42,25.60,7.86" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allen</surname></persName>
		</author>
		<title level="m" coord="8,189.97,548.46,138.12,7.86">Natural Language Understanding</title>
		<imprint>
			<publisher>Benjamin/Cummings</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct coords="8,138.35,570.32,342.24,8.12;8,146.91,581.92,28.24,7.47" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="8,146.91,570.32,229.04,7.86">Question Answering for Machine Reading Evaluation</title>
		<ptr target="http://celct.fbk.eu/QA4MRE" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,592.17,342.24,7.86;8,146.91,603.13,89.73,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="8,299.14,592.17,176.86,7.86">Natural Language Processing with Python</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>O&apos;Reilly Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,611.76,342.24,10.13;8,146.91,624.98,333.68,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,196.86,611.76,283.73,10.13;8,146.91,624.98,42.08,7.86">Étude comparative de la distribution florale dans une portion des Alpes et des Jura</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jaccard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,195.64,624.98,218.12,7.86">Bulletin del la Société Vaudoise des Sciences Naturelles</title>
		<imprint>
			<biblScope unit="page" from="547" to="579" />
			<date type="published" when="1901">1901</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.35,635.88,342.24,7.86;8,146.91,646.84,333.68,7.86;8,146.91,657.79,168.44,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,213.16,635.88,267.43,7.86;8,146.91,646.84,84.91,7.86">Measuring agreement on set-valued items (MASI) for semantic and pragmatic annotation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Passonneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,252.13,646.84,228.46,7.86;8,146.91,657.79,139.76,7.86">Proceedings of the International Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>the International Conference on Language Resources and Evaluation (LREC)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,138.35,120.67,342.24,7.86;9,146.91,131.63,333.68,7.86;9,146.91,142.59,333.68,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,248.00,120.67,173.87,7.86">A Simple Measure to Assess Non-response</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,445.37,120.67,35.22,7.86;9,146.91,131.63,333.68,7.86;9,146.91,142.59,121.92,7.86">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>ACL, Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1415" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
