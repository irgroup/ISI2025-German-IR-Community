<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.74,152.67,293.67,12.64;1,267.65,170.67,59.95,12.64">An Entailment-Based Approach to the QA4MRE Challenge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,213.17,210.18,45.26,8.96"><forename type="first">Peter</forename><surname>Clark</surname></persName>
							<email>peterc@vulcan.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Vulcan Inc</orgName>
								<address>
									<postCode>98104</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,266.67,210.18,53.53,8.96"><forename type="first">Phil</forename><surname>Harrison</surname></persName>
							<email>philipha@vulcan.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Vulcan Inc</orgName>
								<address>
									<postCode>98104</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,328.47,210.18,50.17,8.96"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
							<email>xuchen@cs.jhu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Johns Hopkins Univ</orgName>
								<address>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>Maryland</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.74,152.67,293.67,12.64;1,267.65,170.67,59.95,12.64">An Entailment-Based Approach to the QA4MRE Challenge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1288D251F6B924B0EC69FAF1F1582BD4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our entry to the 2012 QA4MRE Main Task (English dataset). The QA4MRE task poses a significant challenge as the expression of knowledge in the question and answer (in the document) typically substantially differs. Ultimately, one would need a system that can perform full machine reading -creating an internal model of the document's meaningto achieve high performance. Our approach is a preliminary step toward this, based on estimating the likelihood of textual entailment between sentences in the text, and the question Q and each candidate answer A i . We first treat the question Q and each answer A i independently, and find sets of sentences SQ, SA that each plausibly entail (the target of) Q or one of the A i respectively. We then search for the closest (in the document) pair of sentences &lt;S Q SQ, S Ai SA&gt; in these sets, and conclude that the answer A i entailed by S Ai in the closest pair is the answer. This approach assumes coherent discourse, i.e., that sentences close together are usually "talking about the same thing", and thus conveying a single idea (namely an expression of the Q+A i pair).</p><p>In QA4MRE it is hard to "prove" entailment, as a candidate answer A may be expressed using a substantially different wording in the document, over multiple sentences, and only partially (as some aspects of A may be left implicit in the document, to be filled in by the reader). As a result, we instead estimate the likelihood of entailment (that a sentence S entails A) by look for evidence, namely entailment relationships between components of S and A such as words, bigrams, trigrams, and parse fragments. To identify these possible entailment relationships we use three knowledge resources, namely WordNet, ParaPara (a large paraphrase database from Johns Hopkins University), and the DIRT paraphrase database. Our best run scored 40% in the evaluation, and around 42% in additional (unsubmitted) runs afterwards. In ablation studies, we found that the majority of our score (approximately 38%) could be attributed to the basic algorithm, with the knowledge resources adding approximately 4% to this baseline score. Finally we critique our approach with respect to the broader goal of machine reading, and discuss what is needed to move closer to that goal.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine Reading remains one of the Grand Challenges of Artificial Intelligence, and also one of the most difficult. Machine Reading requires more than just parsing a text; it requires constructing a coherent internal model of the world that the text is describing. This goal is particularly challenging because typically only a fraction of that model is made explicit in text, requiring substantial background knowledge to fill in the gaps and resolve ambiguities <ref type="bibr" coords="2,276.34,174.18,116.37,8.96" target="#b13">(Schank and Abelson, 1977)</ref>. By one estimate, only one eighth of the knowledge conveyed by text is stated explicitly <ref type="bibr" coords="2,429.64,186.18,40.74,8.96;2,124.70,198.18,21.54,8.96" target="#b4">(Graesser, 1981)</ref>.</p><p>The QA4MRE task is a simpler version of the full Machine Reading challenge because it uses multiple-choice questions against a single document. However, QA4MRE is still formidable because the answer information is typically expressed in varied, imprecise, complex, and sometimes distributed ways in the document, and almost always requires background knowledge to bridge the gap to the original question and candidate answers. In the ideal case, a system would still build an internal model of the entire document, and decide which candidate answer that model entails.</p><p>In our system, we use a simpler and more impoverished approach, namely to look for entailment relationships between phrases or sentences in the document and the question Q and candidate answers A i , as a first step towards more complete model construction.</p><p>An overview of our approach is as follows: We first treat the question Q and each answer A i independently, and find sets of sentences SQ, SA that each plausibly entail (the target of) Q or one of the A i respectively. We then search for the closest (in the document) pair of sentences &lt;S Q SQ, S Ai SA&gt; in these sets, and conclude that the answer A i entailed by S Ai in the closest pair is the answer. The justification for this is that sentences close together are usually "talking about the same thing", and thus conveying a single idea (namely an expression of the Q+A i pair).</p><p>In the rest of this paper we describe our approach, present some examples, and then summarize our results. Our best run scored 40% in the evaluation, and around 42% in additional (unsubmitted) runs afterwards. In ablation studies, we found that the majority of our score (approximately 38%) could be attributed to the basic algorithm, with the knowledge resources adding approximately 4% to this baseline score. Finally we critique our approach with respect to the broader goal of machine reading, and discuss what is needed to move closer to that goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>One approach to QA4MRE would be to substitute a candidate answer A into a question Q to form a single sentence, called Q+A, and then assess whether the document semantically entails Q+A. However, in our early experiments with this approach, we had mediocre performance. We believe this was, in part, because this involves searching for several pieces of information at once (from within both Q and A), with some items of information confusing the search for others. As a result, we have found a different approach to be more effective:</p><p>1. find expressions of Q and A independently in the document 2. assess whether those two expressions indicate A is an answer to Q By searching for Q and A independently in the document, we avoid answer details confusing the search for the question statement in the document, and vice versa.</p><p>These two steps rely on two important tasks:</p><p>Task 1. Assessing how likely it is that a sentence<ref type="foot" coords="3,344.95,191.91,3.24,5.83" target="#foot_0">1</ref> is expressing the same information as (the target of) a question Q or a candidate answer A Task 2. Assessing how likely it is that an expression of Q and an expression of A imply that A is an answer to Q.</p><p>Task 1 can be viewed as a form of the Textual Entailment challenge:</p><p> Given sentence S and (the target of) a question Q, does S entail (the target of) Q?  Given sentence S and a candidate answer A, does S entail A?</p><p>By "target of the question", we mean the description of the item the question is asking for, typically a noun phrase. For instance in "Where is the epicenter of the AIDS pandemic?" the target is "the epicenter of the AIDS pandemic".</p><p>For example, given the Q+A pair and the sentence S37 below from the target document:</p><p>Question Q[3.11] Why were transistor radios a significant development? Answer A2. Young people could listen to pop outside. Sentence S27. In the 1960s, the introduction of inexpensive, portable transistor radios meant that teenagers could listen to music outside of the home.</p><p>the entailment questions our current system asks are:  How likely is that that S27 entails "transistor radios were a significant development" (Q)?  How likely is it that S27 entails "Young people could listen to pop outside" (A2)?</p><p>Task 2 can also be viewed as a Textual Entailment task: Given sentence S1 plausibly entails Q, and sentence S2 plausibly entails A, does S1+S2 entail Q+A? (where Q+A is a sentence created by substituting A into Q). To assess this, we significantly approximate this task by simply measuring how close S1 and S2 are in the document, the proximity being taken as a measure of likelihood that S1+S2 entails Q+A. The justification for this is an assumption of coherent discourse, i.e., that sentences close together are usually "talking about the same thing", and thus close sentences are often conveying a single coherent idea (e.g., the Q+A pair). Although this is a gross approximation, it is helpful because often the connection between Q+A is not explicit in the document. Rather, it is implied by pragmatic considerations such as context, sentence ordering, or subtle discourse words (as in the above example).</p><p>Although Q and A2 in the above example are complete sentences, we apply the same approach when the Q and A are phrases (as is more usually the case). We say that a sentence S "entails" a phrase P if the meaning of P forms part of the meaning of S. For example we say sentence S entails the answer phrase A below: S ("Text"): Because humanity has polluted so much surface water on the planet, we are now mining the groundwater far faster than it can be replaced by nature. A ("Hypothesis"): Because surface water is too polluted. because the notion that "surface water is too polluted" is part of the meaning of S.</p><p>We now describe Task 1 (Entailment Assessment) and Task 2 (Proximity Detection) in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task 1: Entailment Assessment</head><p>To determine if a sentence S entails a candidate answer A, one approach is to create a formal representation of S and A and then prove S implies A. However, reliably creating formal representations is extremely difficult. A weaker but more practical variant is to do reasoning at the textual level -the "natural logic" approach <ref type="bibr" coords="4,428.78,344.25,37.15,8.96;4,124.70,356.25,94.05,8.96" target="#b8">(MacCartney &amp; Manning, 2007;</ref><ref type="bibr" coords="4,222.22,356.25,78.87,8.96" target="#b7">MacCartney, 2009)</ref> -in which semantically valid (or plausible) rewrite rules are applied directly to the linguistic structure of the text. If S's parse can be validly transformed to A's parse, then A is, in a way, "proved" (entailed). However, even this is a high bar; often we cannot fully "prove" that S entails A by this method, either because we are missing background knowledge, or because some unstated context/assumptions are needed, or because in a strict sense A is not fully derivable from S due to some of the required knowledge being implicit (unstated). As a result, we relax the problem further and collect evidence of entailment (do pieces of S entail pieces of A?) to provide an overall assessment of how likely S entails A. This is a standard approach taken in most Recognizing Textual Entailment (RTE) systems (e.g., <ref type="bibr" coords="4,147.94,476.27,49.22,8.96">NIST, 2011)</ref>. The key question is which evidence should be used, and how that evidence should be combined.</p><p>Given S and A, our system looks for possible entailment relations between various fragments of S and A, namely words, bigrams, trigrams, and parse tree fragments. To assess entailment between these components, the system considers equality, synonymy, and entailment relationships drawn from three sources: WordNet (hypernyms), the DIRT paraphrase database, and the ParaPara paraphrase database from Johns Hopkins University (described shortly). Each relationship found is a piece of evidence that S entails A. Then, the various evidence is combined to provide an overall confidence in entailment. For this task, the absolute confidence number is not important, as our algorithm only cares about the relative entailment strength in order to find the sentences that most likely entail A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word and N-gram Matching:</head><p>The simplest entailment is word matching: a word in S matches (i.e., is identical to, taking into account morphological variants) a word in A, e.g., "produce"(in S) → "production"(in A); "suppporting"(S) → "supportive"(A). Word matches are scored according to the word's "importance", i.e, the extent to which it carries the semantic meaning of the phrase or sentence in which it occurs. For example, words like "HIV", "virus", "infection" (for the AIDS topic) carry more of the meaning than general words like "go", "move", etc. To capture this intuition we use two measures of "importance":</p><p> Salience: an Idf (inverse document frequency) measure of how unlikely a word is, with uncommon words being weighted higher than common words, defined as:</p><formula xml:id="formula_0" coords="5,160.10,262.38,233.38,8.96">salience(w) = max [ log (1/p(w|topical-documents)) -k, 0]</formula><p>k is a constant chosen such that a pool of very common words ("of", "the", "a") have a salience of 0. A word has high salience if it occurs infrequently, and the most common words have a salience of 0.</p><p> Topicality: A word that occurs unusually frequently for documents about a particular topic (relative to general text) is considered topical, and can be given more weight. We define it as:</p><formula xml:id="formula_1" coords="5,160.10,374.73,301.65,20.96">topicality(w) = max[ log (p(w|topical-documents)/p(w|general-documents)) -1, 0]</formula><p>A word has high topicality if it occurs unusually frequently in domain texts (relative to general texts), while a word that is no more/less frequent in domain texts than general texts (independent of its absolute frequency) has a topicality of 0. Topicality helps to distinguish between domain-general and domain-specific terms, allowing us to place more weight on domain-specific terms relative to equally infrequent domain-general terms (e.g., weight "virus" more than "primarily" for documents about AIDS).</p><p>The overall entailment strength is a weighted combination of these measures:</p><formula xml:id="formula_2" coords="5,160.10,530.65,192.23,9.06">weight(w) = λ.topicality(w) + (1-λ).salience(w)</formula><p>where λ controls for the relative weights of topicality and salience. p(w|topicaldocuments) is estimated from the QA4MRE background collection for the topic of the question (AIDS, climate change, music and society, Alzheimer), and p(w|generaldocuments) is estimated from the British National Corpus <ref type="bibr" coords="5,364.37,590.75,101.96,8.96">(BNC Consortium, 2001)</ref>.</p><p>For Ngrams, we add the weights of individual words in the Ngrams. We optimized for λ on the 2011 QAMRE data, producing an optimal value of λ = 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Use of Paraphrases to Identify Entailments:</head><p>In addition to looking for exact word/phrase matches, the system also looks for entailment relations using two paraphrase databases, namely ParaPara <ref type="bibr" coords="5,416.92,668.78,53.46,8.96;5,124.70,680.78,23.33,8.96" target="#b2">(Chan et al., 2011)</ref> and DIRT <ref type="bibr" coords="5,193.25,680.78,84.09,8.96" target="#b6">(Lin and Pantel, 2001</ref>):</p><p> The ParaPara paraphrases are of the form string 1 → string 2 (string equivalents), found using a combination of bilingual pivoting and monolingual distributional similarity. Bilingual pivoting uses aligned parallel corpora in multiple languages. If a phrase X is aligned with a foreign phrase Y, and that foreign phrase Y is aligned with a phrase Z, then X and Z may be paraphrases. The set of high confidence paraphrases found this way is then given a second score based on the distributional similarity of their surrounding words in the Google N-gram corpus. Finally, the two scores are combined together using an SVM trained on human-annotated training data. Some of the better examples that applied in QA4MRE are:</p><p>"total lack of" → "lack of" "increasingly" → "every day" "cause" → "make" "cooperation with" → "closely with" "finance" → "fund the" "against aid" → "fight aid" "to combat" → "to fight" "spend" → "devote" "one" → "member" "reason" → "factor" "is one of" → "among"</p><p> The DIRT paraphrases are of the form IF (X r 1 Y) THEN (X r 2 Y), where r 1 and r 2 are dependency paths between words X and Y, and are based on distributional similarity: (X r 1 Y) and (X r 2 Y) are paraphrases if the frequency distribution of the Xs with r 1 , and of the Xs with r 2 , are similar (combined with the same for the Ys). For our purposes here we simply use the IF (X ←subjverb 1 -obj→ Y) THEN (X ←subjverb 2 -obj→ Y) paraphrases as string equivalents (verb 1 → verb 2 ), although with more engineering we could also use longer paraphrases in the database and exploit the dependency structure more. In our earlier work, we found these simple verbal paraphrases to be the most reliable. Some of the better examples that applied in QA4MRE are:</p><formula xml:id="formula_3" coords="6,170.06,534.71,153.45,68.96">IF X decreases Y THEN X reduces Y IF X increases Y THEN X grows Y IF X offers Y THEN X includes Y IF X names Y THEN X calls Y IF X supports Y THEN X funds Y IF X causes Y THEN X affects Y</formula><p>If a paraphrase X → Y (from either ParaPara or DIRT) applies during the entailment assessment, then we score the entailment as weight(x)*confidence(paraphrase), where the confidence(paraphrase) is provided in the paraphrase databases (range 0-1). In other words, we downgrade the strength of an exact match (X → X) by confidence(paraphrase). If the paraphrase occurs in both databases we take the highest confidence for it. This formula was found to be the most effective in the trials with ran.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WordNet</head><p>We also use WordNe in a straightforward way to find word hypernyms and synonyms. As well as using the WordNet synsets, we also use WordNet's pertainym ("pertains to") links. Pertainyms cross part-of-speech boundaries and offer some useful additional synonyms to the synsets, for example the pertainyms of (senses) of "quick" are:</p><p>"quickly" "speedily" "quicker" "faster" "quickly" "quickest" "fastest" "prompt" "quick" "quickly" "agilely" "nimbly"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Syntactic Fragments</head><p>In addition to word and phrase-based matching, we also match parse tree fragments from parses of the two phrases/sentences. This allows some credit to be given if syntactic dependencies between words is preserved between the two sentences. We use (a modern version of) the SAPIR parser <ref type="bibr" coords="7,281.08,370.29,122.21,8.96" target="#b5">(Harrison and Maxwell, 1986)</ref>, a broad coverage chart parser, generate a dependency-like structure from the parse, and then "shred" the dependency tree into fragments, where each fragment denotes one dependency-style link. If a sentence S and answer A share a syntactic fragment, then this constitutes another piece of evidence that S entails A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.3</head><p>Task 2: Implication Assessment (Proximity Detection)</p><p>Given the system has identified sentences in the document that most likely express Q and A in the document, the second task is to assess how likely it is that the combination of these sentences imply that A is an answer to Q. Ideally we would find some appropriate syntactic connection between those two sentences (e.g., "Q-sentence because A-sentence"). However, this task is difficult because the connection may be indirect or simply not stated explicitly in the text. To deal with this, we perform task 2 in a crude way, by measuring the distance between the sentences, prefering the closest pair. In the case of a tie, we then prefer the pair that also most strongly entails Q and A, using the scores from Task 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithmically, we:</head><p>1. Find the N sentences SQ i that most strongly entail Q 3. Find the N sentences SA nj that most strongly entail A n , for each of the five candidate answers A n 4. Find the pair &lt;SQ i ,SA nj &gt; where the distance (number of sentences) between SQ i and SA nj in the document is smallest. For ties, prefer the pair where the entailments of SQ i → Q, and QA nj → A are strongest. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Example</head><p>As an example of the system's behavior consider the following question:</p><formula xml:id="formula_4" coords="8,136.10,563.51,26.91,8.96">Q[3.5]</formula><p>What is one of the MCP goals in Third World countries? A1: funding international organizations A2: separation of HIV-positive mothers from their infants A3: elimination of the contribution to the Global Fund A4: participation in USAID A5: separation of family planning from HIV prevention [CORRECT]</p><p>First the system finds the top three sentences most likely entailing Q and each answer A i , as illustrated in Table <ref type="table" coords="8,227.56,659.54,3.90,8.96">1</ref>:</p><p>Question Q / Candidate answer Ai 3 sentences that most strongly entail Q or Ai Q[3.5] What is one of the MCP goals in Third World countries? S7 S30 S52 A1: funding international organizations S2 S29 S35 A2: separation of HIV-positive mothers from their infants S2 S6 S10 A3: elimination of the contribution to the Global Fund S26 S29 S30 A4: participation in USAID S13 S29 S34 A5: separation of family planning from HIV prevention [CORRECT] S30 S31 S45 Table 1. First the system finds the 3 sentences that most likely entail Q, and for each A i the 3 sentences that most likely entail that A i For example, there is strong evidence that S30 likely entails Q due to the paraphrases "aimed at" → "goal" and "developing countries" → "Third World countries" (both from ParaPara): Q[3.5] What is one of the MCP goals in Third World countries? S30: ...U.S. funding...will be saddled by...restrictions aimed at separating family planning from HIV prevention in developing countries.</p><p>Let SQ be the set of 3 sentences most entailing Q, i.e., {S7,S30,S52}, and SA be the set of 5x3=15 sentences most entailing one of the A i , i.e., {S2,S6,…,S35,S45}. Next the system looks for &lt;S Q SQ, S Ai SA&gt; pairs from these sets with the mimimum (sentence number) distance between some entailing sentences (3x3x5 pairs to consider). There are two pairs where the distance is zero (the same sentence entails both Q and an A i ):</p><p>Q+A3, both plausibly entailed by S30 (see Table <ref type="table" coords="9,346.80,530.51,4.12,8.96">1</ref>) Q+A5, both plausibly entailed by S30 (see Table <ref type="table" coords="9,346.80,542.51,4.12,8.96">1</ref>)</p><p>To break the tie, the system looks at the strengths of the entailments. Using the scoring metric earlier, the scores are: For A3: 7.36 (S30→Q) + 10.85 (S30→A3) = 18.21 For A5: 7.36 (S30→Q) + 33.48 (S30→A5) = 40.84</p><p>Thus answer A5 ("separation of family planning from HIV prevention") is selected (in this case this is the right answer). The reason the entailment strength is so high (33.48) for this entailment is obvious, as S30 contains A5 almost verbatim within it: A5: separation of family planning from HIV prevention S30: ...U.S. funding...will be saddled by...restrictions aimed at separating family planning from HIV prevention in developing countries.</p><p>Note that the entailment S30 → Q+A5 is still only partial; for example the system did not find evidence related to MCP in the question, i.e., it did not prove that the goals in S30 were "MCP goals" (Q). More generally, there are typically elements of S and A that are left unentailed. However, for the QA4MRE task, we only need to know the relative entailment strengths in order to select the best answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>We submitted three runs with different parameters for scoring, the highest run achieving an accuracy of 0.40 (versus a baseline of random guessing of 0.20). The c@1 score is also 0.40, as the system always guesses an answer.</p><p>We also performed some ablation studies to see the effects of adding/removing different knowledge sources. The results are shown in Table <ref type="table" coords="10,360.41,351.21,3.72,8.96">2</ref>, using the current version of system: Subtractive ablations 42.5 Main system (all resources) 41.9 minus WordNet (only) 38.1 minus ParaPara (only) 41.9 minus DIRT (only) 38.1 baseline (none of the resources)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additive ablations</head><p>38.1 baseline (none of the resources) 41.9 add WordNet (only) 39.4 add ParaPara (only) 41.9 add DIRT (only) 42.5 Main system (all resources)</p><p>Table <ref type="table" coords="10,149.42,543.04,3.41,8.10">2</ref>. Precision (percent correct). The knowledge resources contribute approximately 4% to the system's accuracy (also c@1) on the 2012 QA4MRE data.</p><p>The patterns in the ablation results are somewhat varied, illustrating the interactions that can occur between the scores from different knowledge resources, and making it difficult to draw detailed conclusions about individual resources. However, the general picture from these studies is clear: the basic (resource-free) algorithm accounts for the majority of the score (38%), with the knowledge resources together contributing an additional 4% to the score, and with no single knowledge resource clearly dominating the other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>There are clearly many improvements that can be made. We summarize some of these here, before finally turning to the larger challenge of Machine Reading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Better question analysis</head><p>We did not make any attempt to analyze the questions beyond extracting words, bigrams, and parse fragments, although clearly knowing the question type would affect what kind of an answer is sought. In addition, in one case, there is no domainspecific content to the questions at all: Q[12.12]: Which of the following is true?</p><p>Here it is pointless to search for sentences entailing (the target of) question Q <ref type="bibr" coords="11,438.37,323.25,18.19,8.96">[12,</ref><ref type="bibr" coords="11,456.56,323.25,13.64,8.96">12]</ref> (as there is no target), and the results will be random. Better analysis of the questions, in particular identification of the question type, would help improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sentence-level entailments and anaphora resolution</head><p>Although we allow the content of the Q+A pair to be split over multiple sentences, we assume that the semantic content of Q alone, and A alone, is within a single sentence. In practice, this assumption does not always hold, for example a pronoun may refer back to previous sentences (our system does not currently do anaphora resolution). For instance, in this example: Q[3.9] Why did Burney decide not to write a biography of Dr Samuel Johnson? S44: At one time he thought of writing a life of his friend Dr Samuel Johnson, but he retired before the crowd of biographers who rushed into that field.</p><p>our system's inability to realize that "he" (in S44) is "Burney" (mentioned in earlier sentences) weakens the assessed entailment between S44 and Q. Our system gets this question wrong.</p><p>In addition, several questions and answers themselves use pronouns, e.g.,:</p><p>Q[9.9] How did he earn his degrees at Oxford? 1. He wrote an essay on comets 2. He produced an operetta at Drury Lane 3. He studied with Dr Arne 4. He composed various pieces 5. He sang in a choir Without identifying who "he" is (which our system does not do), our entailment reasoning is missing a critical piece of information to be entailed, again weakening its entailment reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3</head><p>Proximity as an Approximation for Capturing Q+A Together</p><p>Our method assumes that if two sentences S Q and S A expressing Q and A respectively are close, then the two sentences together likely expresses the combined meaning of Q+A. Although this is clearly a gross assumption, it appears to holds true surprisingly often. The larger problem we observe is that, as currently implemented, proximity always takes precidence over the strengths of entailments. A bad example is as follows: Q[2.8] What advantage does the Jatropha curcas offer? A2: it grows on semi-arid fields [actual correct] A4: it reduces pests of other crops [predicted correct]</p><p>The system selects A4 because a sentence S16 entails both Q and A4 with moderate strength. However, inspection of the data shows that two other sentences, S2 and S3, very strongly entail A and Q respectively: S2: It sounds too good to be true: a biofuel crop that grows on semi-arid lands and degraded soils... S3: That is what some are claiming for Jatropha curcas, the miracle biofuel crop.</p><p>Despite the relatively strong entailments, our system disprefers this option as the two sentences are further apart (distance 1, rather than distance 0), and sentence proximity currently takes absolute priority over entailment strengths once the top 3 entailing sentences for each answer have been selected. In future, we could consider a weighted combination of the distance and entailment strength metrics when selecting an answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Short answers</head><p>When an answer is short, e.g., a single word, there is very little to be "entailed" by the text. In the worst case, if the answer word is common then there is little basis to pick the best 3 sentences that "entail" that answer. Our system did notably worse on questions with single-word answers, for example: Q[2.5] Which is the biofuel whose production most reduces the emission of greenhouse effect gases? 1. bio-diesel 2. bio-oil 3. corn ethanol 4. cellulosic biofuel 5. gasoline</p><p>In the supporting document, 9 sentences contain the phrase "corn ethanol", and thus there is no basis (using our current algorithm) to select the 3 sentences that "most strongly entail" corn ethanol from them. Again, a modification of the algorithm to allow more entailing sentences in its set of candidates could overcome this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Towards Machine Reading</head><p>Finally we consider the larger goal of Machine Reading, and the architecture of our system within it. Although our system's performance was relatively respectable, there is still a long way to go. Most significantly, our system is largely relying on local entailment estimations, and does not make any attempt to construct an overall model of the text, resulting in sometimes brittle behavior, for example when there are many word-level entailments between the text T and the hypothesis H, but the relationships between those words in T and between those words in H are completely different. Clearly more work is needed to acquire lexical and world knowledge to help recognize such near-equivalences.</p><p>Concerning reasoning, text allows many weak entailments to be made, but the reasoning problem is how to combine all those entailments together into a coherent whole. This is something our system does not do; given some text, it can posit many weak entailments, many of which are contradictory, but does nothing to try and find a subset of those entailment which are together coherent. One can view this as the challenge of "reasoning with messy knowledge". Part of the challenge is devising a suitable method for reasoning with uncertainty, so that contradictions in the entailments can be best resolved. (A promising candidate for this is Markov Logic Networks <ref type="bibr" coords="13,124.70,675.26,137.79,8.96" target="#b11">(Richardson and Domingos, 2006)</ref>). However, simply ruling out inconsistencies may not be a sufficient constraint on the problem; When people read, they also bring largescale expectations about "the way the world might be", and coerce the fragmented evidence from the text to fit those expectations. Reproducing this kind of behavior computationally has long been a goal of AI <ref type="bibr" coords="14,314.38,186.18,63.53,8.96" target="#b9">(Minsky, 1974;</ref><ref type="bibr" coords="14,382.21,186.18,88.00,8.96;14,124.70,198.18,21.54,8.96" target="#b13">Schank and Abelson, 1977)</ref>, but still remains elusive, both in acquiring such expectations and using them to guide reading <ref type="bibr" coords="14,185.60,210.18,54.11,8.96">(Clark, 2010)</ref>. Recent work on learning event narratives ("scripts"), e.g., <ref type="bibr" coords="14,145.18,222.18,128.67,8.96" target="#b1">(Chambers and Jurafsky, 2008)</ref> and building proposition stores <ref type="bibr" coords="14,408.10,222.18,61.97,8.96;14,124.70,234.18,37.99,8.96" target="#b14">(Van Durme et al., 2009;</ref><ref type="bibr" coords="14,165.55,234.18,94.64,8.96">Penas and Hovy, 2010)</ref> offers some exciting new possibilities in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary</head><p>Our system for QA4MRE is based on assessing entailment likelihood, and breaks down the problem into two parts:</p><p>i. finding sentences that most entail the question Q and each answer A i ii. finding the closest pair of such sentences where one entails Q and the other A i .</p><p>The system's best run scored 40% correct. As just discussed, to progress further, the system needs to move from assessing local entailments to constructing some kind of "best coherent model", built from a subset of those (many) weak entailments. This requires both addressing the knowledge problem (of acquiring the knowedge to support that) and the reasoning problem (to construct such a model). The QA4MRE challenge itself, though difficult, is one that seems ideally suited to promoting research in these directions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="8,124.70,443.30,338.09,8.10;8,124.70,454.22,195.86,9.01;8,136.10,477.47,334.51,8.96;8,124.70,489.47,182.08,8.96;8,136.05,147.40,325.70,287.19"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The system finds the closest pair of sentences, one plausibly entailing Q, one plausibly entailing an answer A i , and concludes the answer is A i Experimentally, we used N = 3, as it achieved the highest performance on the 2011 training data. Figure 1 illustrates this process.</figDesc><graphic coords="8,136.05,147.40,325.70,287.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="13,124.70,315.21,346.00,224.98"><head>encourage the use of groundwater</head><label></label><figDesc>There are two major challenges to overcome to move closer towards machine reading: the knowledge problem, and the reasoning problem. The knowledge problem is that large amounts of world knowledge are needed to fully identify entailments, but our existing resources (e.g., paraphrase databases) are still limited. Although paraphrasing allows some simple entailments to be recognized, e.g. IF X contains Y THEN Y is inside X, there are many cases in the QA4MRE dataset where the gap between T and H is substantial. Some examples of semantically similar, but lexically different, phrasings are shown below requiring considerable knowledge to recognize the ap-S73 ...the UN has ...a program to give them access to groundwater sources.</figDesc><table coords="13,124.70,411.21,345.86,116.98"><row><cell>proximate equivalence:</cell></row><row><cell>Q[2.7] What is the external debt of all African countries?</cell></row><row><cell>S61 Africa owes foreign banks and governments about 350 billion.</cell></row><row><cell>Q[2.1] When did the rate of AIDS started to halve in Uganda?</cell></row><row><cell>S73 The rate of AIDS in Uganda is down to about 8, from a high of 16 in the</cell></row><row><cell>early 1990s.</cell></row><row><cell>A4[to Q7.9] to</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,129.98,675.19,340.37,8.10;3,136.10,686.23,229.88,8.10"><p>We assume that (the target of) a question or answer is expressed in a single sentence, although the expression of the two together may span multiple sentences.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="14,124.70,488.28,346.00,9.06;14,142.70,499.33,328.00,9.05;14,142.70,510.37,20.72,9.05" xml:id="b0">
	<monogr>
		<title level="m" coord="14,201.65,488.28,160.45,9.06">The British National Corpus, version 2</title>
		<imprint>
			<publisher>Oxford University Computing Services on behalf of the BNC Consortium</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>BNC Consortium</orgName>
		</respStmt>
	</monogr>
	<note>Distributed</note>
</biblStruct>

<biblStruct coords="14,124.70,521.29,345.72,9.05;14,142.70,532.33,80.12,9.05" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,245.77,521.29,204.45,9.05">Unsupervised Learning of Narrative Event Chains</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,142.70,532.33,50.12,9.05">Proc ACL&apos;08</title>
		<meeting>ACL&apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,124.70,544.31,346.12,8.96;14,142.70,556.31,327.80,8.96;14,142.70,568.31,52.80,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,312.89,544.31,157.93,8.96;14,142.70,556.31,230.79,8.96">Reranking Bilingually Extracted Paraphrases Using Monolingual Distributional Similarity</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,384.91,556.31,85.59,8.96;14,142.70,568.31,24.15,8.96">EMNLP Workshop: GEMS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,124.70,580.31,345.29,8.96;14,142.70,592.31,22.65,8.96" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="14,164.64,580.31,170.88,8.96">Do Scripts Solve NLP? Working Note 28</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Univ Texas at Austin</publisher>
		</imprint>
		<respStmt>
			<orgName>AI Lab</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="14,124.70,604.31,317.38,8.96" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Graesser</surname></persName>
		</author>
		<title level="m" coord="14,217.87,604.31,158.04,8.96">Prose Comprehension Beyond the Word</title>
		<meeting><address><addrLine>NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,124.70,616.31,345.56,8.96;14,142.70,628.31,160.25,8.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,243.44,616.31,133.24,8.96">A New Implementation of GPSG</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Maxwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,391.28,616.31,78.97,8.96;14,142.70,628.31,92.25,8.96">Proc. 6th Canadian Conf on AI (CSCSI&apos;86)</title>
		<meeting>6th Canadian Conf on AI (CSCSI&apos;86)</meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,124.70,640.31,346.00,8.96;14,142.70,652.34,204.65,8.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,221.13,640.31,214.54,8.96">Discovery of Inference Rules for Question Answering</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,447.61,640.31,23.08,8.96;14,142.70,652.34,107.02,8.96">Natural Lan-guage Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="343" to="360" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,124.70,664.34,345.53,8.96;14,142.70,676.34,40.82,8.96" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="14,192.90,664.34,108.71,8.96">Natural language inference</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Maccartney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-06">June 2009</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct coords="15,124.70,150.18,345.58,8.96;15,142.70,162.18,226.98,8.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="15,248.51,150.18,136.84,8.96">Natural logic for textual inference</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,392.44,150.18,77.84,8.96;15,142.70,162.18,146.61,8.96">ACL Workshop on Textual Entailment and Paraphrasing</title>
		<meeting><address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06">June 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,124.70,174.18,345.60,8.96;15,142.70,186.18,65.87,8.96" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="15,176.36,174.18,175.52,8.96">A Framework for Representing Knowledge</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Minsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974-06">June, 1974</date>
			<biblScope unit="volume">306</biblScope>
		</imprint>
		<respStmt>
			<orgName>MIT-AI Laboratory Memo</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="15,124.70,198.08,315.43,9.06" xml:id="b10">
	<monogr>
		<author>
			<orgName type="collaboration" coords="15,124.70,198.18,19.83,8.96">NIST</orgName>
		</author>
		<title level="m" coord="15,151.91,198.08,258.42,9.06">Proceedings of the fourth Text Analysis Conference (TAC&apos;2011)</title>
		<meeting>the fourth Text Analysis Conference (TAC&apos;2011)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,124.70,210.18,346.12,8.96;15,142.70,222.18,42.50,8.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="15,253.52,210.18,98.83,8.96">Markov Logic Networks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,360.66,210.18,74.75,8.96">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="107" to="136" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,124.70,234.18,345.88,8.96;15,142.70,246.18,327.92,8.96;15,142.70,258.18,22.65,8.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="15,204.17,234.18,215.72,8.96">Filling Knowledge Gaps in Text for Machine Reading</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,427.01,234.18,43.57,8.96;15,142.70,246.18,286.50,8.96">23rd International Conference on Computational Linguistics (COLING 2010)</title>
		<meeting><address><addrLine>Beijing</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,124.70,269.24,342.93,9.05" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Schank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Abelson</surname></persName>
		</author>
		<title level="m" coord="15,223.10,269.24,159.26,9.05">Scripts, Plans, Goals and Understanding</title>
		<meeting><address><addrLine>NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Erlbaum</publisher>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,124.70,281.25,345.92,8.96;15,142.70,293.25,327.58,8.96;15,142.70,305.25,226.19,8.96" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="15,313.69,281.25,156.92,8.96;15,142.70,293.25,142.21,8.96">Deriving generalized knowledge from corpora using WordNet abstraction</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Michalak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Schubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,296.07,293.25,174.21,8.96;15,142.70,305.25,196.22,8.96">12th Conf. of the European Chapter of the Assoc. for Computational Linguistics (EACL-09)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
