<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,156.67,115.96,302.02,12.62">The LogAnswer Project at QA4MRE 2012</title>
				<funder ref="#_DPWakuc">
					<orgName type="full">DFG</orgName>
				</funder>
				<funder ref="#_eDYN3Wd">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,234.09,153.77,61.26,8.74"><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
							<email>ingo.gloeckner@fernuni-hagen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Information and Communication Systems Group (IICS)</orgName>
								<orgName type="institution">University of Hagen</orgName>
								<address>
									<postCode>59084</postCode>
									<settlement>Hagen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,322.50,153.77,54.30,8.74"><forename type="first">Björn</forename><surname>Pelzer</surname></persName>
							<email>bpelzer@uni-koblenz.de</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Artificial Intelligence Research Group</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Koblenz-Landau</orgName>
								<address>
									<addrLine>Universitätsstr. 1</addrLine>
									<postCode>56070</postCode>
									<settlement>Koblenz</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,156.67,115.96,302.02,12.62">The LogAnswer Project at QA4MRE 2012</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4851CD002748147F501F8B4DDBD67BAF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For QA4MRE 2012, we have improved our prototype of a pure logic-based answer validation system that was originally developed for QA4MRE 2011. The system uses core functionality of the LogAnswer question answering (QA) system available on the web, which was combined and extended such as to best meet the demands of the QA4MRE task. In particular, coreference resolution is used in order to allow knowledge processing on the document level, and a fragment of OpenCyc has been integrated in order to allow more flexible reasoning. In addition to a general consolidation of the original prototype that took part in QA4MRE 2011, the current system was optimized for accuracy on nonrejected questions. While last year's system only achieved an accuracy of 0.24 for answered questions, the various measures taken to improve the rejection decision now yield an accuracy of 0.31 for answered questions in the QA4MRE 2012 task. Results show that these improvements were sufficient to raise the overall quality of answers to a c@1 score of 0.26, which is clearly above the c@1 baseline of 0.20 for random guessing. Due to the difficulty of the reading tests in QA4MRE, we consider this result promising, recalling the near-baseline scores of most QA4MRE participants in 2011.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The DFG-funded project LogAnswer investigates the use of deep linguistic processing and logical reasoning for QA, with a special emphasis on the issues of robustness and efficiency. An experimental QA system for German (also called LogAnswer) <ref type="bibr" coords="1,222.64,565.25,10.52,8.74" target="#b1">[2]</ref> that evolved from this research can be tested online (www.loganswer.de), a screenshot is shown in see Fig. <ref type="figure" coords="1,375.74,577.20,3.87,8.74">1</ref>. The system took part in the QA@CLEF and ResPubliQA QA system evaluations <ref type="bibr" coords="1,394.67,589.16,10.79,8.74" target="#b2">[3,</ref><ref type="bibr" coords="1,405.46,589.16,7.20,8.74" target="#b3">4,</ref><ref type="bibr" coords="1,412.66,589.16,7.20,8.74" target="#b4">5]</ref>. Recently, we have focused on the issue of providing automatic QA support for human Q&amp;A portals. Users of the German FragWikia! portal (http://frag.wikia.com/) can subscribe to the automatic answer service on the LogAnswer website. LogAnswer keeps track of new questions at FragWikia! and sends an answer email whenever How many people died when the MS Estonia sank? more than 900 people Fig. <ref type="figure" coords="2,181.00,288.65,4.13,7.89">1</ref>. Screenshot of the LogAnswer QA system, using QA@CLEF data it finds sufficiently good results for a question of a registered user. In this new use case for QA, we face the challenge that there is often few lexical overlap between question and answer. Moreover the system must validate answers in order to achieve better precision. This is because users do not necessarily expect the system to answer at all in this scenario. But if it does post a response, then the answer should show a high probability of correctness, in order to avoid users getting annoyed by the machine-generated answers. This scenario resembles the present QA4MRE machine reading exercise <ref type="bibr" coords="2,332.12,425.19,10.52,8.74" target="#b5">[6]</ref> in that there is often no close relationship between question and answers, and in the need to validate answers rather than always providing a ranked list of answer candidates.</p><p>Despite these similarities that make the QA4MRE participating appealing for our project, application of existing LogAnswer system (even with the precisionoriented refinements for QA portals) to the QA4MRE task does not make sense:</p><p>-LogAnswer normally finds answer from scratch, based on a candidate retrieval backend whose retrieval scores are considered in determining the final ranking. In QA4MRE, however, there is no retrieval score as one of the starting points for reranking. -LogAnswer usually works with a sentence-level segmentation of documents, while QA4MRE may demand a justification of answers from several parts of a document or even from several documents. -Since the validation features of LogAnswer depend on sentence-level segmentation and since the retrieval score as one of these features, the validation models of LogAnswer as determined from a learning-to-rank approach are not applicable. -The existing QA@CLEF-based training set for the machine learning (ML)</p><p>technique cannot be reused either because it shows strong effect of lexical overlap (unlike QA4MRE) and because it comprises sentence-level items only.</p><p>Owing to these difficulties that were already present in QA4MRE 2011, we decided to build a new LogAnswer prototype for the QA4MRE task in the last year <ref type="bibr" coords="3,156.73,173.94,9.96,8.74" target="#b6">[7]</ref>, using only those basic techniques from the regular LogAnswer system that seemed useful for the task:</p><p>-The core system for QA4MRE was designed to achieve logical answer validation on document level representations; -Existing LogAnswer functionality was reused whenever possible; -Instead of using the supplied "QA4MRE Background Collection" or a webbased validation approach, we decided to strengthen the background knowledge of LogAnswer by integrating part of OpenCyc; -Generation of explanations for a chosen answer was added.</p><p>For QA4MRE 2012, our goal was a general consolidation of the LogAnswer prototype built for QA4MRE and an improvement of its capabilities for rejecting wrong answers in particular.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>The LogAnswer prototype for QA4MRE relies on a deep linguistic analysis.</p><p>-The reading tests, questions and answer candidates are parsed by the WOCADI parser <ref type="bibr" coords="3,181.70,403.81,9.96,8.74" target="#b7">[8]</ref>. -WOCADI computes a meaning representation for each sentence in the form of a Multilayered Extended Semantic Network (MultiNet <ref type="bibr" coords="3,404.00,427.51,10.30,8.74" target="#b8">[9]</ref>). -The question is then subjected to a question classification, based on a system of 248 classification rules that operate on the generated semantics networks.</p><p>In QA4MRE 2011, the question classification was of minor importance compared to regular Q&amp;A here since no predictive annotation and no expected answer type check was used. For QA4MRE 2012, some effort was spent in utilizing the computed question category, specific answer types (as known from the former QA@CLEF and ResPubliQA exercises) and other information determined by the question classification rules in order to eliminate questions that the system is unable to answer:</p><p>-LogAnswer is designed for answering single-sentence questions. Therefore, questions that span more than one sentence are now discarded. -LogAnswer has no special means for validating list answers (e.g. answers involving a coordination such as 'London and Paris'). Therefore question that target at several answers (such as 'Which cities. . . ') are now recognized and discarded since they are not likely to be answered correctly. -Questions are now generally discarded if no expected answer type can be established, except if the question is classified into the 'DEFINITION' category (for definition questions).</p><p>In the following we sketch the construction of the logical representation on the document level.</p><p>-Coreference resolution is essential to the success of logic-based processing on whole documents: under the unique name assumption, the combined information about a discourse entity can only be utilized if all mentions of the entity in the text are represented by the same constant. We therefore use a coreference resolver, CORUDIS <ref type="bibr" coords="4,301.23,199.22,9.96,8.74" target="#b7">[8]</ref>, for handling anaphoric pronouns and other forms of coreference. -The coreference resolver is complemented with a simplistic solution to identifying the speaker from the manuscript of a talk which was activated in the QA4MRE runs. The method simply chooses the first name mentioned at the beginning of the manuscript. Knowing the speaker is necessary for resolving first person deixis 'Ich'/'I'). -In a process called assimilation, the text representation is constructed from all logical sentence representations, thereby using the results of coreference analysis and speaker recognition.</p><p>In order to capture more inferences, ca. 20,000 subsumption relationships were extracted from OpenCyc and translated into our concept system for German, see Section 3. For efficiency reasons, this subsumption hierarchy is not added to the background knowledge seen by the prover. Instead, the text representation is enriched by explicitly declaring each instance of a concept an instance of all of its superconcepts as well.</p><p>Example:</p><p>-A document mentions a tsunami.</p><p>-Logical representation contains statement like SUB(c24, tsunami.1.1), expressing that entity c24 is an instance of a tsunami (with HaGenLex word sense index 1.1, see <ref type="bibr" coords="4,239.04,468.17,14.76,8.74" target="#b9">[10]</ref>). -From the OpenCyc link, we know that every tsunami is a 'Naturkatastrophe' (natural disaster). -The OpenCyc expansion step then adds an assertion expressing that c24 is also an instance of a natural disaster, viz SUB(c24, naturkatastrophe.1.1).</p><p>Let us now turn to hypothesis construction. By a hypothesis, we mean a declarative statement formed from the question and the candidate answer. Consider the question 'What is Bono's attitude with respect to the digital age?' and the possible answer "enthusiasm". In this case, the textual hypothesis would be 'Bono's attitude with respect to the digital age is enthusiasm'. In our prototype, the template-based construction of a textual hypothesis from question and answer, followed by parsing, is only used as a fallback. This is because the textbased hypothesis construction is problematic for German due to mismatches of word forms with respect to syntactic case. Instead, our system normally constructs the logical hypothesis directly on the logical level, by combining the literal lists that represent question and answer.</p><p>Based on the logical hypothesis (as a list of literals) and the logical representation of the document for the given reading test, our system then uses robust logical processing for justifying or rejecting the hypothesis (and in turn, the answer candidate). This procedure follows the general principle of logical answer validation:</p><p>-Try to prove the logical hypothesis from the logical representation of the reading test document and from the general background knowledge of the system. -If such a proof succeeds, then the answer from which the hypothesis was constructed is considered correct.</p><p>In practice, linguistic analyses contain errors, coreference resolution can fail, and the background knowledge is incomplete. Therefore, the proof attempt may fail for correct hypotheses, and we cannot conclude from a failed proof that the hypothesis is indeed wrong. For ensuring sufficient robustness, we thus add a relaxation loop to the logical validation process:</p><p>-Remove problematic hypothesis literals one at a time until a proof of the remaining hypothesis fragment succeeds. -Impose a limit on the number of admissible relaxation steps in order to keep processing time within reasonable bounds.</p><p>The skipped literal count and other criteria related to the relaxation result are then used as features that affect answer scoring. QA4MRE 2011 required systems to provide one or more so-called provenance elements that justify the chosen answer. This requirement was simplified in this year's task, which only assumes general information of the resources involved to be provided. Still, we did not eliminate the extraction of detailled justifications from our system since it proved useful for computing validation scores, depending on the coherence of found justifications.</p><p>The computation of provenance information (i.e. of the individual justifying elements that together explain the answer) is based on prover results:</p><p>-All names of used axioms (implicative rules) are added as provenance elements, since axiom names in our system are chosen in a meaningful way. -Used facts from the background knowledge generally express lexical-semantic relations, such as CHPA(hoch.1.1, höhe.1.1) (i.e., property 'high' corresponds to attribute 'height'). Knowing the MultiNet documentation, such a fact is self-explanatory and can be added as a provenance element. -For literals from the text representation such as SUB(c24, tsunami.1.1), the system presents a witness sentence rather than the used fact itself. These witness sentences are also considered as provenence elements and thus included into the computed justification.</p><p>Since no sufficient training data was available for applying a machine-learning technique that automatically determines validation scores, we used an ad-hoc scoring metric that closely resembles the validation criterion already used in the last year. <ref type="foot" coords="6,174.67,117.42,3.97,6.12" target="#foot_0">2</ref> Many of the validation features normally used by LogAnswer had to be omitted since it was not clear how to utilize them in a simple hand-crafted scoring criterion.</p><p>As in the last year, we started from a basic scoring metric defined as the arithmetic mean of:</p><p>-1 = α #skipped-lits , where α = 0.7.<ref type="foot" coords="6,295.28,186.12,3.97,6.12" target="#foot_1">3</ref> -2 = α #skipped-lits • β #unknown-status-lits , where β = 0.8. <ref type="foot" coords="6,378.95,198.26,3.97,6.12" target="#foot_2">4</ref>-3 : optimistic proportion of provable question literals:</p><formula xml:id="formula_0" coords="6,266.67,232.18,102.91,16.39">3 = 1 - #skipped-q-lits</formula><p>#all-q-lits -4 : pessimistic proportion of proved question literals:</p><formula xml:id="formula_1" coords="6,277.34,293.07,14.98,9.65">4 =</formula><p>#proved-q-lits #all-q-lits -5 : optimistic proportion of provable answer literals:</p><formula xml:id="formula_2" coords="6,266.81,347.23,32.70,9.65">5 = 1 -</formula><p>#skipped-a-lits #all-a-lits -6 : pessimistic proportion of proved answer literals:</p><formula xml:id="formula_3" coords="6,277.48,394.65,81.29,22.31">6 = #proved-a-lits #all-a-lits</formula><p>This basic validation score is then combined with a criterion intended to capture the coherence of the justification of the answer as found by the system using a relaxation proof. In order to determine this coherence score, we organize the witness sentences into blocks of adjacent sentences. Let us say that a block of adjacent witness sentences is 'unconnected' to earlier blocks if it is not linked by coreference to these blocks. Let u be the number of unconnected blocks of witness sentences for the given answer a. The final answer selection score σ is then computed from the basic score and the unconnected sentence score u as</p><formula xml:id="formula_4" coords="6,281.28,532.52,52.30,10.81">σ = • γ u-1</formula><p>where γ = 0.7 in our experiments. The final step in solving an individual reading test item is the selection of the best answer, or alternatively the decision not to commit to any answer at all, leaving the question unanswered.</p><p>-The prototype generally refuses to answer if question analysis fails (i.e., if there is no full parse of the question or an empty question literal pattern). -An answer candidate is also rejected if hypothesis construction fails (e.g. if the constructed hypothesis contains no literals associated with the original answer).</p><p>As opposed to our system setup in QA4MRE 2011 described in <ref type="bibr" coords="7,406.07,186.63,9.96,8.74" target="#b6">[7]</ref>, the prototype no longer rejects answer candidates for which provenance generation fails. In this case that an empty justification for the answer is extracted, the system rather determines σ based on u = 1 (i.e. assuming that the candidate answer is justified by a single witness sentence). If all answers are rejected for one of the reasons mentioned, then the question is left unanswered. Otherwise the answer with maximum score is chosen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Resources Used</head><p>The background knowledge of LogAnswer comprises:</p><p>-A system of 49,000 synonym classes involving 112,000 lexemes, used for synonym normalization (concepts are replaced by a canonical representative); -About 10,500 facts, e.g.</p><p>-nominalization relationships ('to attack' vs. 'the attack') -adjective-attribute relationships ('high' vs. 'height') -gender-specific profession names ('Professor' vs. 'Professorin' [female professor] in German) -Approx. 120 rules for basic inferences (e.g. for making use of the lexicalsemantic relationships listed above).</p><p>In order to enhance the background knowledge, the system is further equipped with knowledge from OpenCyc (http://sw.opencyc.org/), To this end, the HaGenLex <ref type="bibr" coords="7,185.48,473.03,15.50,8.74" target="#b9">[10]</ref> concept system of German word senses was linked to a fragment of OpenCyc using a semi-automatic alignment technique. We started with automatic generation of hypotheses for HaGenLex-OpenCyc links:</p><p>-HaGenLex → wikipedia.de topic or redirect → DBPedia map (de-en) → wikipedia.en-OpenCyc link, or alternatively:</p><formula xml:id="formula_5" coords="7,140.99,540.61,354.96,20.72">-HaGenLex → HaEnLex → wikipedia.en topic/redirect → wikipedia.en-OpenCyc link,</formula><p>where HaEnLex is a core lexicon for English designed analogously to HaGenLex and interlinked with the German lexicon. The automatic generation of link hypotheses was followed by a manual validation, checking several ten thousands of links generated in this way for their actual correctness. The checked links are then used for knowledge extraction: the validated alignment lets us map synonyms, subsumptions, and disjoint declarations from OpenCyc into our HaGenLex concept system. A final refinement consists in a check for cyclic subsumptions and inconsistencies, which are found automatically but had to be fixed by hand. Moreover, there is an automatic elimination of redundancy (i.e., optimization of subsumption chains). This process resulted in 20,000 new redundancy-free subsumption relationships, 1,500 new synonyms, and 4,000 disjointness declarations, which substantially extend the background knowledge available to LogAnswer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>Two runs were submitted to QA4MRE 2012, using German as both the source and target language. These runs were based on the following system configurations:</p><p>-loga12011dede: only elementary synonyms (spelling variants, wrong spellings) and elementary background knowledge (e.g. simple temporal and spatial regularities), no OpenCyc integration. -loga12023dede: full background knowledge of LogAnswer including OpenCyc expansion.</p><p>The main evaluation results obtained for the 160 test questions of QA4MRE 2012 are shown in Table <ref type="table" coords="8,243.73,428.14,3.87,8.74" target="#tab_0">1</ref>. Here, 'aw' is the number of answered (non-rejected) questions, 'corr' is the number of answered questions with a correct choice of the answer, 'acc|aw' is the accuracy achieved for answered questions, 'acc' is overall accuracy, and 'c@1' is the c@1 score achieved by the given run. The system attempts to answer 94 questions, with 28 correct answers in the first run and 30 correct answers in the second run supported by the full background knowledge of LogAnswer. Thus, there is a slight positive effect of the added knowledge including the OpenCyc integration. The achieved c@1 score of 0.26 clearly exceeds the c@1 score of 0.20 for random guessing. The accuracy of 0.31 for non-requested questions in the second run is substantially higher than the corresponding value of 0.24 achieved in the last year <ref type="bibr" coords="8,362.57,547.69,9.96,8.74" target="#b6">[7]</ref>. Moreover the c@1 score has improved from 0.22 to the current 0.26. This indicates a positive effect of the consolidation of the system and of the refinements for recognizing questions that the system is unable to handle. Table <ref type="table" coords="8,178.07,595.93,4.98,8.74" target="#tab_1">2</ref> also shows the results by topics. T1. T2, T3 and T4 are the c@1 scores achieved for topics 1 (AIDS), 2 (Climate Change), 3 (Music and Society), and 4 (Alzheimer), respectively. 'all' is the overall c@1 score for comparison.</p><p>While the self-assessment of the system with respect to failed answer attempts has been improved, mainly by better exploiting the results of question classification, some problems of the prototype with the QA4MRE tasks remain: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-Poor document quality</head><p>One of the main problem for deep linguistic processing were missing blanks, resulting in adjacent words to be wrongly merged into single tokens. There was also no structuring of the documents by blank lines at all, so that headlines were not clearly separated from regular text. Due to parsing failure, this resulted in many sentences to become invisible for logic-based validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-Spelling errors in questions</head><p>The WOCADI parser has found a full parse for 132 out of the 160 questions.</p><p>Parsing problems in the remaining 28 questions were mostly due to spelling errors or wrong grammar. These problems made it impossible for the system to answer the corrupted questions. -Linguistic diversity of answers Due to the rich question types in the reading tests, answers were syntactically diverse and hard to parse (e.g. incomplete sentences).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-Missing background knowledge</head><p>The regularities needed to understand the correctness of an answer were complex and hard to capture by logical rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>We have described the LogAnswer prototype for QA4MRE 2012, which resulted from last year's system by a general consolidation and refinements for better recognizing questions that the system cannot answer. These improvements have raised the c@1 score of the system to 0.26, which is substantially better than the expected c@1 score of 0.20 for random guessing. There was also a slight positive effect of using background knowledge including the OpenCyc integration, compared to using no background knowledge at all. However, defects in reading test documents (missing blanks and structuring) still make it difficult to apply deep linguistic processing and subsequent logical reasoning for validating answers. No 'shallow' techniques were used as a fallback if the deep linguistic analysis fails, since lexical overlap and similar shallow linguistic criteria will not help too much in QA4MRE. It would have been best to complement logical answer validation with web-based validation or hit scores obtained from the QA4MRE Background Collection. The modest c@1 score achieved by LogAnswer can also be attributed to the fact that the QA4MRE background collections were not yet used by the system. Looking at concrete questions, a decomposition of coordinating answers such as 'London and Paris' would have been useful, but for the time being the system discards questions that target at such answers. From a more general perspective, we will continue working on techniques for increasing the precision of results in order to make our system more useful in the QA portals use case.</p><p>One concrete approach that we are currently working on is the use of techniques from Case-Based Reasoning (CBR). These techniques could help to utilize user feedback for continually improving validation quality.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,174.50,115.91,266.35,50.98"><head>Table 1 .</head><label>1</label><figDesc>Main results of LogAnswer prototype in QA4MRE 2012</figDesc><table coords="8,228.75,136.71,157.85,30.18"><row><cell>Run</cell><cell>aw corr acc|aw acc c@1</cell></row><row><cell cols="2">loga11011dede 96 28 0.29 0.18 0.25</cell></row><row><cell cols="2">loga12023dede 96 30 0.31 0.19 0.26</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="9,173.74,115.91,267.88,50.98"><head>Table 2 .</head><label>2</label><figDesc>Topic results of LogAnswer prototype in QA4MRE 2012</figDesc><table coords="9,230.82,136.71,153.72,30.18"><row><cell>Run</cell><cell>T1 T2 T3 T4 all</cell></row><row><cell cols="2">loga11011dede 0.26 0.07 0.42 0.23 0.25</cell></row><row><cell cols="2">loga12023dede 0.29 0.11 0.42 0.23 0.26</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="6,144.73,612.96,335.86,7.86;6,144.73,623.92,53.30,7.86"><p>The QA4MRE 2011 data for LogAnswer was considered too small for learning a useful model.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="6,144.73,634.88,307.12,7.86"><p>#skipped-lits is the number of skipped hypothesis literals due to relaxation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="6,144.73,645.84,335.86,7.86;6,144.73,656.80,192.86,7.86"><p>#unknown-status-lits is the number of non-skipped hypothesis literals not yet proved in the last proof attempt of the relaxation loop.</p></note>
		</body>
		<back>

			<div type="funding">
<div> 1  1  <p>LogAnswer is supported by the <rs type="funder">DFG</rs> with grants <rs type="grantNumber">FU 263/12-2</rs> and <rs type="grantNumber">GL 682/1-2</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_DPWakuc">
					<idno type="grant-number">FU 263/12-2</idno>
				</org>
				<org type="funding" xml:id="_eDYN3Wd">
					<idno type="grant-number">GL 682/1-2</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.96,210.33,337.63,7.86;10,151.52,221.29,329.07,7.86;10,151.52,232.25,329.07,7.86;10,151.52,243.21,315.30,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,258.35,221.29,222.24,7.86;10,151.52,232.25,296.63,7.86">Evaluating Systems for Multilingual and Multimodal Information Access: 9th Workshop of the Cross-Language Evaluation Forum</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kurimo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Petras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,455.89,232.25,24.70,7.86;10,151.52,243.21,16.79,7.86">CLEF 2008</title>
		<meeting><address><addrLine>Aarhus, Denmark; Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="10,142.96,254.17,337.64,7.86;10,151.52,265.10,329.07,7.89" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,309.22,254.17,171.38,7.86;10,151.52,265.13,144.56,7.86">An application of automated reasoning in natural language question answering</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Furbach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Glöckner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pelzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,303.92,265.13,80.40,7.86">AI Communications</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="241" to="265" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,276.09,337.63,7.86;10,151.52,287.05,32.64,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="10,248.34,276.09,232.25,7.86;10,151.52,287.05,18.39,7.86">Combining logic and machine learning for answering questions</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Glöckner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pelzer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,298.01,337.63,7.86;10,151.52,308.96,255.07,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,248.24,298.01,151.74,7.86">The LogAnswer project at CLEF 2009</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Glöckner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pelzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,421.09,298.01,59.50,7.86;10,151.52,308.96,117.31,7.86">Working Notes for the CLEF 2009 Workshop</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09">September 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,319.92,337.63,7.86;10,151.52,330.88,158.30,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,251.83,319.92,180.78,7.86">The LogAnswer project at ResPubliQA 2010</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Glöckner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pelzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,455.89,319.92,24.70,7.86;10,151.52,330.88,79.80,7.86">CLEF 2010 Working Notes</title>
		<imprint>
			<date type="published" when="2010-09">September 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,341.84,337.64,7.86;10,151.52,352.80,329.07,7.86;10,151.52,363.76,329.07,7.86;10,151.52,374.72,215.76,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,282.32,352.80,198.27,7.86;10,151.52,363.76,173.87,7.86">Overview of QA4MRE at CLEF 2012: Question Answering for Machine Reading Evaluation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sporleder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Forascu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Benajiba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Osenova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,347.77,363.76,132.82,7.86;10,151.52,374.72,132.00,7.86">CLEF 2012 Evaluation Labs and Workshop Working Notes Papers</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,385.68,337.63,7.86;10,151.52,396.64,186.08,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,292.00,385.68,169.37,7.86">The LogAnswer project at QA4MRE 2011</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Glöckner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pelzer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,151.52,396.64,107.57,7.86">CLEF 2011 Working Notes</title>
		<imprint>
			<date type="published" when="2011-09">September 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,407.59,337.63,7.86;10,151.52,418.55,145.58,7.86" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="10,213.21,407.59,213.80,7.86">Hybrid Disambiguation in Natural Language Analysis</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hartrumpf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Der Andere Verlag</publisher>
			<pubPlace>Osnabrück, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,429.51,337.64,7.86;10,151.52,440.47,92.61,7.86" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,202.18,429.51,273.85,7.86">Knowledge Representation and the Semantics of Natural Language</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Helbig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.61,451.43,337.98,7.86;10,151.52,462.36,281.00,7.89" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,314.52,451.43,166.07,7.86;10,151.52,462.39,40.22,7.86">The semantically based computer lexicon HaGenLex</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Hartrumpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Helbig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Osswald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,200.86,462.39,146.82,7.86">Traitement automatique des langues</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="81" to="105" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
