<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,186.92,116.90,241.52,12.90">Testing lexical approaches in QA4MRE</title>
				<funder>
					<orgName type="full">Fundo Europeu de Desenvolvimento Regional (EU)</orgName>
				</funder>
				<funder ref="#_G8fPC4P">
					<orgName type="full">AdI (Agência de Inovac ¸ão)</orgName>
				</funder>
				<funder>
					<orgName type="full">Quadro de Referência Estratégica Nacional)</orgName>
				</funder>
				<funder ref="#_UdNyemF">
					<orgName type="full">FCT Fundac ¸ão para a Ciência e a Tecnologia</orgName>
				</funder>
				<funder>
					<orgName type="full">QREN</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,199.35,155.11,63.73,8.64"><forename type="first">Hugo</forename><surname>Rodrigues</surname></persName>
							<email>hugo.rodrigues@l2f.inesc-id.pt</email>
						</author>
						<author>
							<persName coords="1,269.91,155.11,51.90,8.64"><forename type="first">Luísa</forename><surname>Coheur</surname></persName>
							<email>luisa.coheur@l2f.inesc-id.pt</email>
						</author>
						<author>
							<persName coords="1,328.85,155.11,82.30,8.64"><forename type="first">Ana</forename><forename type="middle">Cristina</forename><surname>Mendes</surname></persName>
							<email>ana.mendes@l2f.inesc-id.pt</email>
						</author>
						<author>
							<persName coords="1,216.33,167.06,62.36,8.64"><forename type="first">Ricardo</forename><surname>Ribeiro</surname></persName>
							<email>ricardo.ribeiro@l2f.inesc-id.pt</email>
						</author>
						<author>
							<persName coords="1,302.11,167.06,96.93,8.64"><forename type="first">David</forename><surname>Martins De Matos</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Spoken Language Systems Laboratory -L</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">INESC</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Instituto Superior Técnico</orgName>
								<orgName type="institution">Technical University of Lisbon R. Alves Redol</orgName>
								<address>
									<postCode>9 -1000-029</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,186.92,116.90,241.52,12.90">Testing lexical approaches in QA4MRE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">46DB9A14575BA88BE4EA54FCBC8B3613</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine Reading</term>
					<term>Word Proximity</term>
					<term>Distance Measures</term>
					<term>Similarity Measures</term>
					<term>QA4MRE</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe our strategy in the course of our participation in the 2012 QA4MRE main task. We follow a lexical approach, based on both Word Proximity and similarity measures. In the former, we implement a method that was successfully applied in the "Who Wants to be a Millionaire" contest; in the later we use the notion of "extent", that is, a passage that includes terms of the given questions or answers, and results from comparing the attained extents through widely known similarity measures such as Jaccard and Dice. Considering the 2011 QA4MRE competition, our results are promising, although still far from the ones attained by the winning system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine Reading (MR) aims at developing systems capable of "reading" and extracting knowledge from free text. However, in the same way computers do not play chess as humans do, systems do not interpret text as we do. We are able, for example, to quickly understand to which entity a pronoun refers, which is still not true for machines: in the sentences The friends sat on the chairs because they were tired and The friends sat on the chairs because they were cozy, the pronoun they refers, in the former, to the friends and in the second sentence to the chairs. We can disambiguate with no effort, but machines cannot.</p><p>In order to boost MR, a task dedicated to this topic -Question Answering for Machine Reading Evaluation (QA4MRE) -was introduced in the Cross-Language Evaluation Forum (CLEF), in 2011 <ref type="bibr" coords="1,265.52,573.58,15.27,8.64" target="#b9">[10]</ref>. Being given a text and several questions about it, competing systems had to choose the correct answer among five candidates to each question, showing in this way their level of "comprehension" of the text. The information needed to correctly choose between the different questions could be found in the given texts and in a collection of documents, called Background Collection.</p><p>Our main motivation to participate in this task is related with the FalaComigo project, where an agent poses multiple-choice tests to the audience. At the current moment these tests are manually crafted, although we have already implemented a system capable of generating questions <ref type="bibr" coords="2,261.38,120.31,11.62,8.64" target="#b3">[4]</ref> and distractors. Our approach to the QA4MRE tasks represents our efforts in developing a tool that selects an answer from a set of possible candidates, as our goal is to automatize all the multiple-choice tests generation.</p><p>In this paper we investigate a lexical approach to the QA4MRE task. We study the contribution of an algorithm previously applied to the "Who Wants to be a Millionaire?" contest, as its goal is also to choose the correct answer among several possible answers, and the usage of similarity measures to assess the likeness of the questions and answers.</p><p>This paper is organized as follows: in Section 2 we present related work, in Section 3 we detail our approaches and, in Section 4, we evaluate and discuss them. In Section 5 we present the main conclusions and point to some future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Twelve systems participated in the QA4MRE task in 2011; however, only eight working notes are available <ref type="bibr" coords="2,210.85,300.64,15.27,8.64" target="#b9">[10]</ref>. For all submitted runs (43 for english, 11 for german and 9 for romanian), nearly half reported a score below the baseline, which was of 20% accuracy (considering that each question has 5 different answers and that there is an uniform distribution of the different answers, the baseline is attained by always choosing the n-th answer). The winning system <ref type="bibr" coords="2,279.13,348.46,11.62,8.64" target="#b8">[9]</ref> achieved results of 0.57 considering the c@1 measure, as defined in Section 4 <ref type="bibr" coords="2,265.64,360.42,15.27,8.64" target="#b9">[10]</ref>.</p><p>Although very different approaches were followed by participating systems, several steps were common to many of them, and various resources were widely exploited. In fact, many systems performed pre-processing, namely: anaphora or co-reference resolution <ref type="bibr" coords="2,160.86,408.66,10.79,8.64" target="#b1">[2,</ref><ref type="bibr" coords="2,174.49,408.66,11.83,8.64" target="#b13">14]</ref>, stopword filtering <ref type="bibr" coords="2,267.31,408.66,16.60,8.64" target="#b12">[13]</ref> and Named Entity (NE) Recognition (NER) <ref type="bibr" coords="2,466.48,408.66,10.58,8.64" target="#b4">[5]</ref>. Regarding tools and resources, Lucene<ref type="foot" coords="2,287.20,418.68,3.69,6.39" target="#foot_0">1</ref> was used by many systems to index the texts (as described by Iftene et al. <ref type="bibr" coords="2,234.90,432.57,11.62,8.64" target="#b4">[5]</ref> and Martinez-Romo and Araujo <ref type="bibr" coords="2,378.48,432.57,11.20,8.64" target="#b6">[7]</ref>) and WordNet <ref type="bibr" coords="2,451.21,432.57,11.62,8.64" target="#b7">[8]</ref> was a constant presence (for instance Saias and Quaresma <ref type="bibr" coords="2,356.93,444.52,16.60,8.64" target="#b12">[13]</ref> report its use in synonym detection).</p><p>Nevertheless, different strategies were implemented, from information retrieval-to logic-based approaches. For instance, Verberne <ref type="bibr" coords="2,324.78,480.80,16.60,8.64" target="#b13">[14]</ref> took advantage of the BM25 function to rank passages from the Background Collection according to their similarity to a given text fragment, "expanding" it in this way. In the work described by Iftene et al. <ref type="bibr" coords="2,466.48,504.71,10.58,8.64" target="#b4">[5]</ref>, Lucene is used to index the texts. The built index is then queried using the questions, creating this way another index, built with the retrieved passages/documents. Then, based on this new index, answers are used as queries in a new retrieval step. The relevance scores from each retrieval step are then used to compute a final score for each answer. Babych et al. <ref type="bibr" coords="2,227.73,564.49,10.58,8.64" target="#b0">[1]</ref>, on the other hand, describes a system for German based in logical inferences. Here, given the candidate answer C, the input text T , and the Background Collection B, the system tries to infer if (T ∧ B) C. Text is parsed in a dependency graph, and hyponym and other relations are extracted from this graph.</p><p>Other strategies try to relate terms. Saias and Quaresma <ref type="bibr" coords="2,374.29,612.73,16.60,8.64" target="#b12">[13]</ref> report the use of rules to measure the distance between the key elements of the question and the answer. The system described by Cao et al. <ref type="bibr" coords="2,257.82,636.64,11.62,8.64" target="#b1">[2]</ref> tries to simulate the strategy applied by people when learning a new language and answering reading tests. According to these authors, people will first locate named entities in the passages related with the questions. Thus, their system performs NER to find related passages and, afterwards, compares the NEs between the question and the passages. Terms are also related by using WordNet relations, such as synonym and hypernym. Each type of relation has a weight associated, which contributes to the final score.</p><p>A completely different approach is reported by Martinez-Romo and Araujo <ref type="bibr" coords="3,451.60,194.29,10.79,8.64" target="#b6">[7]</ref>: the system links all nouns (proper and common) and verbs within a given document, establishing a co-ocurrence graph. This means that the terms appearing in a given document are related under the same topic. Then, WalkTrap <ref type="bibr" coords="3,332.24,230.16,16.60,8.64" target="#b11">[12]</ref> is used to automatically discover "communities", that is, clusters that gather terms belonging to the same topic. Then, each question is assigned to a community based on their similarity. Following this, each answer is also assigned to a community; the selected answer is the one with greater similarity to the question context (i.e., community). We should note that the authors do not specify what are the similarity measures used to compare questions with communities, answers with communities, and the communities themselves.</p><p>In what concerns the winning system <ref type="bibr" coords="3,304.75,316.10,10.58,8.64" target="#b8">[9]</ref>, it combines two different strategies: an Answer Validation (AV) approach and a Question Answering (QA) approach. The best results were accomplished by using the system as an hybrid between the two. The AV module is based on textual entailments: for each answer of a given question, an hypothesis H is generated, according to a set of patterns. These are then used to retrieve passages from the texts, which are indexed with Lucene. The topmost sentence, T , is paired with the corresponding hypothesis, resulting in the pair T-H. These pairs are then processed by a pipeline of different strategies to check if they are textual entailments. Among these strategies are the comparison of NEs, the number of co-occurring unigrams, bigrams and skip-bigrams between T and H, and the matching of question and answer types. Finally, the pair with greatest score from all strategies is chosen as correct answer. In what concerns the QA module, it starts by doing a similar task. Following some rules, each question is transformed into a pattern, where the wh-word is substituted by one of the candidate answers. From these patterns are also extracted stopwords, creating a keyword list. Then, each pattern will be compared against each sentence from the documents. If they do not match, the same is done between the respective keywords list and the sentences. Whichever matches, a score is assigned. Finally, the answer associated with the sentence with greater score is chosen as the correct answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Lexical approaches</head><p>Considering our participation in the 2012 QA4MRE task, we detach two of the submitted runs. The first employs Word Proximity, a strategy based on previous work to solve the "Who Wants to be a Millionaire?" contest <ref type="bibr" coords="3,338.79,621.57,10.58,8.64" target="#b5">[6]</ref>. The second is based in the same strategy, but also uses similarity measures to compare passages related to the question with passages related with the answer and use those measures to evaluate their similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Proximity</head><p>Word Proximity technique is based on the assumption that answers occur close to questions terms. Originally, the algorithm was applied to documents retrieved from the Web. In the present work, we will apply it to each reading test text. The algorithm calculates the distance between each candidate answers' term and the question terms in the surroundings. It weighs the distances, of a maximum radius<ref type="foot" coords="4,398.93,185.44,3.69,6.39" target="#foot_1">2</ref> , so that documents with too many references to an answer but not to the corresponding question terms worth less. The algorithm is presented in Algorithm 1. The parameter documentSplit represents an array where each position is a term in the document. The best value for radius is not trivial to obtain, but according to the authors it is about 40-50 [6, Figure <ref type="figure" coords="4,227.78,507.52,3.60,8.64">1</ref>]. Both answers and questions can be filtered from stopwords, from wh-words (who, how) to prepositions (a, from) or 'to be' forms (are, was).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Similarity Measures</head><p>Our second approach is based on similarity measures. For this we use the notion of extents, that is, a passage that includes each term of a given query at least once <ref type="bibr" coords="4,448.64,579.86,10.58,8.64" target="#b2">[3]</ref>. The used queries are simply the questions and answers, seen as bag of words. Thus, we will have an extent for the question (question extent) and other five extents, one for each answer (answer extents). As the original constraint is too strong (all terms in the query must appear in the extent), we created a different version of the concept. This is based on Part of Speech (POS) tagging. The idea is to have important words (read nouns and verbs present in the query) to contribute with some weight to the extent. The extent has a score threshold, which, if surpassed, defines the extent. Table <ref type="table" coords="5,405.81,144.22,4.98,8.64" target="#tab_1">1</ref> shows the scores attributed to each POS. The threshold is defined by two parameters: the tag threshold and the others threshold. The later is set, empirically, to 8.0, while the former is defined in function of the query, and is set to half the total present in the query. This way we can create extents that contain only parts of the query (thus, reducing their size), but that are still large enough to apply the similarity measures (for example, if the tag threshold is set to 12.0, and we find three Proper Nouns together (3 times 4.0), we still need to find other eight words (8 times 1.0) to complete the extent <ref type="foot" coords="5,344.95,225.98,3.69,6.39" target="#foot_2">3</ref> ). This strategy is called Extents Points. The extents are then compared against each other (question extent versus each one of the answer extents) and choose as correct answer the one with highest similarity value. The most widely used similarity measures that do not penalize word order are used in our experiments: Overlap, Jaccard and Dice, as defined in Equations 1 to 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>POS</head><formula xml:id="formula_0" coords="5,246.23,441.47,234.36,81.92">Overlap(X,Y ) = |X ∩Y | min(|X|, |Y |) (1) Jaccard(X,Y ) = |X ∩Y | |X ∪Y | (2) Dice(X,Y ) = 2 × |X ∩Y | |X| + |Y |<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>In this section we detail the experimental setup and we show the achieved results with the two most relevant submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Considering the QA4MRE evaluation in 2011, the given texts -TED talk transcriptions -dealt with three topics: "Aids", "Climate Change" and "Music and Society". In 2012 a new topic was considered: "Alzheimer". Each topic has four reading tests associated, with a text and ten questions each. Thus, the exercise comprises a total of 160 questions. Each question has five hypothesis of answer, from which only one was correct. The corpus characteristics can be consulted in Table <ref type="table" coords="6,328.41,156.17,3.74,8.64" target="#tab_2">2</ref>. The other source of knowledge, the Background Collection, was not used in our experiments. The evaluation in QA4MRE is done with c@1 <ref type="bibr" coords="6,337.15,292.65,15.27,8.64" target="#b9">[10]</ref>, which can be defined as:</p><formula xml:id="formula_1" coords="6,261.38,311.70,92.61,22.17">c@1 = 1 n (n R + n U n R n ),</formula><p>where n R is the number of correct answers and n U the number of unanswered questions, among n questions. The metric rewards systems that choose not to answer questions instead of doing it and getting it wrong. Note that c@1 ends up being accuracy if we answer all questions.</p><p>Our system only does not answer questions to which all candidate answers get no score. Also, if a question is negative, that is, contains a not, then the answer with the least score is chosen. The idea behind this option is that the given candidate answer is less related with the question and, thus, it is the less probable answer to the question due to the presence of the not <ref type="bibr" coords="6,254.74,438.79,10.58,8.64" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>As stated before, we detach in this paper two of the submitted runs (Run 3 and Run 4). These two runs were based in the evaluation done with the 2011 corpus and were the ones that attained the best results. The first is only based on Word Proximity, with a radius of 20 (value defined after some experiments with the 2011 corpus). The other run combines both described techniques. Here, Word Proximity used, once again, a value of 20 for the radius, and Extents Points was ran with Dice as similarity measure. Results are shown in Table <ref type="table" coords="6,244.12,560.07,3.74,8.64" target="#tab_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>The results accomplished are promising (approximately 0.34 in c@1, in both runs), if a comparison is made with the 2011 results. Although the corpus is fairly different (different tests, texts and questions), most of the participating systems had results bellow 0.25 in c@1. However, our results are still far away from the 0.57 attained by Pakray et al. <ref type="bibr" coords="6,156.62,657.43,10.58,8.64" target="#b8">[9]</ref>. The evaluation by topic shows that our system does not perform equally in different domains. This may have to do with a non purposeful difficulty increase (that is, tests are harder but were not meant to) or simply with the topic domain (i.e., more technical and, thus, requiring more synonyms knowledge). If we go deeper in the evaluation (by test), we see that this problem is even more patent, with scores ranging from 0.10 to 0.55 within the same topic.</p><p>Considering our techniques, previous results showed that Word Proximity performs better when using smaller values for radius (20). We also noticed that, with the current algorithm, a question term closer to the answer has more weight than two or three question terms in the extremes of the considered snippet. A problem arises: which one is better, proximity or quantity? The answer can be found by developing other algorithms.</p><p>In what respects similarity measures it was clear, on previous experiments, that Overlap distance is not accurate for this task, performing as good as the baseline, much because it will boost extents containing other extents. This is due to the fact that the ratio between the size of the intersection (and, because one extent encloses the other, this is the smallest extent) and the size of the smallest extent, following Overlap definition, ends up being 1.0 for all those candidate answers.</p><p>It is also important to note that we used no other resources, namely the Background Collection. We believe, thus, that results can be better when using such information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this paper, we explored the application of a lexical approach to QA4MRE, a Machine Reading task. In particular, we used the Word Proximity algorithm <ref type="bibr" coords="7,420.34,558.99,10.58,8.64" target="#b5">[6]</ref>, which had been previously employed in the "Who Wants to be a Millionaire?" contest. We also tested a set of similarity measures between passages of text containing terms of the question and passages containing the answer candidates.</p><p>Regarding future work, much can be done. Both approaches can be boosted, either by testing with other measures, or by pre-processing the texts (for instance, by using a stemmer or WordNet for synonyms). The use of the Background Collection, that was not considered in our experiments, may improve our results, as well as other combinations of the developed techniques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,134.77,256.99,345.83,210.49"><head></head><label></label><figDesc>Algorithm 1 Pseudocode for word proximity scoring algorithm, giving more weight to answers near question words, within radius words.</figDesc><table coords="4,143.73,284.43,222.69,183.05"><row><cell>DistanceScore(documentSplit, questWords, ansWords, radius)</cell></row><row><cell>score, ansFoundWords = 0</cell></row><row><cell>for i = 1 to ||documentSplit|| do</cell></row><row><cell>if documentSplited[i] ∈ ansWords then</cell></row><row><cell>ansFountWords += 1</cell></row><row><cell>for j = (i-radius) to (i+radius) do</cell></row><row><cell>if documentSplited[ j] ∈ questWords then</cell></row><row><cell>score += (radius -|i -j|)/radius</cell></row><row><cell>end if</cell></row><row><cell>end for</cell></row><row><cell>end if</cell></row><row><cell>end for</cell></row><row><cell>if ansFoundWords == 0 then</cell></row><row><cell>return 0</cell></row><row><cell>else</cell></row><row><cell>return score/ansFoundWords</cell></row><row><cell>end if</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,247.40,270.59,120.55,66.10"><head>Table 1 :</head><label>1</label><figDesc>Scores for each POS tag.</figDesc><table coords="5,267.69,270.59,79.98,54.75"><row><cell>Tag</cell><cell>Score</cell></row><row><cell cols="2">Proper Noun 4.0</cell></row><row><cell cols="2">Common Noun 2.5</cell></row><row><cell>Verb</cell><cell>1.5</cell></row><row><cell>Others</cell><cell>1.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,197.11,201.27,221.15,44.19"><head>Table 2 :</head><label>2</label><figDesc>Characteristics of 2012 QA4MRE corpus.</figDesc><table coords="6,197.11,201.27,221.15,32.83"><row><cell cols="6">Track Number Total Unique Longest Shortest Avg.</cell></row><row><cell></cell><cell cols="5">questions words words question question Length</cell></row><row><cell>2012</cell><cell>160</cell><cell>1762 672</cell><cell>23</cell><cell>3</cell><cell>11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,184.04,118.72,247.29,55.94"><head>Table 3 :</head><label>3</label><figDesc>Results by topic for the two submitted runs.</figDesc><table coords="7,184.04,118.72,247.29,44.59"><row><cell></cell><cell cols="2">Run 3</cell><cell></cell><cell></cell><cell cols="2">Run 4</cell><cell></cell></row><row><cell cols="8">Topic 1 Topic 2 Topic 3 Topic 4 Topic 1 Topic 2 Topic 3 Topic 4</cell></row><row><cell>0.41</cell><cell>0.38</cell><cell>0.29</cell><cell>0.26</cell><cell>0.44</cell><cell>0.35</cell><cell>0.32</cell><cell>0.28</cell></row><row><cell></cell><cell cols="2">0.33</cell><cell></cell><cell></cell><cell cols="2">0.34</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,144.73,658.08,88.49,7.77"><p>http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,144.73,647.12,335.86,7.77;4,144.73,658.08,143.46,7.77"><p>We use the term radius as it was introduced by Lam et al.<ref type="bibr" coords="4,355.02,647.12,9.52,7.77" target="#b5">[6]</ref>, instead of window. Notice that the size of a window is twice the radius.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,144.73,658.08,282.81,7.77"><p>Whenever necessary, this expansion is done evenly for both sides of the extent.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by national funds through <rs type="funder">FCT Fundac ¸ão para a Ciência e a Tecnologia</rs>, under project <rs type="grantNumber">PEst-OE/EEI/LA0021/2011</rs>. Thanks are also due to <rs type="funder">QREN</rs> (<rs type="funder">Quadro de Referência Estratégica Nacional)</rs>, <rs type="funder">Fundo Europeu de Desenvolvimento Regional (EU)</rs> and <rs type="funder">AdI (Agência de Inovac ¸ão)</rs> for financial support to <rs type="projectName">FalaComigo</rs> project (QREN number <rs type="grantNumber">13449</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_UdNyemF">
					<idno type="grant-number">PEst-OE/EEI/LA0021/2011</idno>
				</org>
				<org type="funded-project" xml:id="_G8fPC4P">
					<idno type="grant-number">13449</idno>
					<orgName type="project" subtype="full">FalaComigo</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,156.34,249.58,324.25,8.64;8,156.34,261.54,279.57,8.64" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="8,156.34,261.54,192.78,8.64">Dependency-based answer validation for german</title>
		<author>
			<persName coords=""><forename type="first">Svitlana</forename><surname>Babych</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Henn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jan</forename><surname>Pawellek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<editor>Petras et al.</editor>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,156.34,273.62,324.24,8.64;8,156.34,285.57,187.71,8.64" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="8,348.04,273.62,132.54,8.64;8,156.34,285.57,101.53,8.64">Question answering for machine reading with lexical chain</title>
		<author>
			<persName coords=""><forename type="first">Ling</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<editor>Petras et al.</editor>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,156.34,297.65,324.24,8.64;8,156.34,309.43,324.25,8.82;8,156.34,321.38,324.25,8.59;8,156.34,333.52,249.88,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,425.05,297.65,55.53,8.64;8,156.34,309.61,126.58,8.64">Exploiting redundancy in question answering</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">R</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,301.57,309.43,179.03,8.59;8,156.34,321.38,324.25,8.59;8,156.34,333.52,40.33,8.64">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;01</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;01<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="358" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,156.34,345.60,324.24,8.64;8,156.34,357.37,324.24,8.82;8,156.34,369.51,90.76,8.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,375.52,345.60,105.06,8.64;8,156.34,357.55,194.35,8.64">Question generation based on lexico-syntactic patterns learned from the web</title>
		<author>
			<persName coords=""><forename type="first">Sérgio</forename><surname>Curto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cristina</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luisa</forename><surname>Coheur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,359.00,357.37,95.44,8.59">Dialogue and Discourse</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="175" />
			<date type="published" when="2012-03">March 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,156.34,381.59,324.25,8.64;8,156.34,393.54,324.24,8.64;8,156.34,405.50,212.76,8.64" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="8,253.65,393.54,226.94,8.64;8,156.34,405.50,126.37,8.64">Question answering for machine reading evaluation on romanian and english languages</title>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandru-Lucian</forename><surname>Gînsca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Mihai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Moruz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Trandabat</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Husarciuc</surname></persName>
		</author>
		<editor>Petras et al.</editor>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,156.34,417.58,324.24,8.64;8,156.34,429.36,324.25,8.82;8,156.34,441.31,324.25,8.82;8,156.34,453.45,264.54,8.64" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,384.00,417.58,96.59,8.64;8,156.34,429.53,282.84,8.64">1 billion pages = 1 million dollars? mining the web to play &quot;who wants to be a millionaire?</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">K</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">M</forename><surname>Pennock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cosley</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lawrence</surname></persName>
		</author>
		<ptr target="http://www.grouplens.org/papers/pdf/1m-uai2003.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="8,465.10,429.36,15.49,8.59;8,156.34,441.31,176.24,8.59">Uncertainty in Artificial Intelligence (UAI2003)</title>
		<meeting><address><addrLine>Acapulco, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="337" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,156.34,465.53,324.24,8.64;8,156.34,477.48,304.91,8.64" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="8,331.97,465.53,148.61,8.64;8,156.34,477.48,219.38,8.64">Graph-based word clustering applied to question answering and reading comprehension tests</title>
		<author>
			<persName coords=""><forename type="first">Juan</forename><surname>Martinez-Romo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lourdes</forename><surname>Araujo</surname></persName>
		</author>
		<editor>Petras et al.</editor>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,156.34,489.38,324.24,8.82;8,156.34,501.52,169.87,8.64" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,233.86,489.56,156.55,8.64">Wordnet: a lexical database for english</title>
		<author>
			<persName coords=""><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,399.93,489.38,59.00,8.59">Commun. ACM</title>
		<idno type="ISSN">0001-0782</idno>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995-11">November 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,156.34,513.60,324.24,8.64;8,156.34,525.55,324.24,8.64;8,156.34,537.51,294.19,8.64" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="8,335.45,525.55,145.14,8.64;8,156.34,537.51,208.20,8.64">A hybrid question answering system based on information retrieval and answer validation</title>
		<author>
			<persName coords=""><forename type="first">Partha</forename><surname>Pakray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pinaki</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Somnath</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bidhan</forename><surname>Chandra Pal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><forename type="middle">F</forename><surname>Gelbukh</surname></persName>
		</author>
		<editor>Petras et al.</editor>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,156.34,547.42,324.25,10.81;8,156.34,561.54,324.24,8.64;8,156.34,573.50,315.47,8.64" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="8,367.46,561.54,113.12,8.64;8,156.34,573.50,229.35,8.64">Overview of qa4mre at clef 2011: Question answering for machine reading evaluation</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">E</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Corina</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Caroline</forename><surname>Forascu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sporleder</surname></persName>
		</author>
		<editor>Petras et al.</editor>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,156.34,585.40,324.25,8.82;8,156.34,597.35,324.24,8.59;8,156.34,609.31,48.99,8.82" xml:id="b10">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="8,409.19,585.40,71.40,8.59;8,156.34,597.35,54.35,8.59">CLEF 2011 Labs and Workshop</title>
		<title level="s" coord="8,217.46,597.35,65.85,8.59">Notebook Papers</title>
		<editor>
			<persName><forename type="first">Vivien</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-09-22">19-22 September 2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,156.34,621.57,324.24,8.64;8,156.34,633.52,324.24,8.64;8,158.28,643.17,322.31,10.95;8,156.34,657.43,217.36,8.64" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,295.16,621.57,185.42,8.64;8,156.34,633.52,69.84,8.64">Computing communities in large networks using random walks</title>
		<author>
			<persName coords=""><forename type="first">Pascal</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthieu</forename><surname>Latapy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,229.16,645.30,21.44,8.59">ISCIS</title>
		<title level="s" coord="8,330.06,645.30,146.42,8.59">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Pinar</forename><surname>Yolum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tunga</forename><surname>Güngör</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fikret</forename><forename type="middle">S</forename><surname>Gürgen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Can</forename><forename type="middle">C</forename><surname>Özturan</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3733</biblScope>
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,156.34,120.31,324.24,8.64;9,156.34,132.26,187.03,8.64" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="9,287.66,120.31,192.93,8.64;9,156.34,132.26,100.80,8.64">The di@ue&apos;s participation in qa4mre: from qa to multiple choice challenge</title>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Saias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><surname>Quaresma</surname></persName>
		</author>
		<editor>Petras et al.</editor>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,156.34,144.22,324.24,8.64;9,156.34,156.17,100.15,8.64" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="9,225.09,144.22,255.50,8.64;9,156.34,156.17,14.39,8.64">Retrieval-based question answering for machine reading evaluation</title>
		<author>
			<persName coords=""><forename type="first">Suzan</forename><surname>Verberne</surname></persName>
		</author>
		<editor>Petras et al.</editor>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
