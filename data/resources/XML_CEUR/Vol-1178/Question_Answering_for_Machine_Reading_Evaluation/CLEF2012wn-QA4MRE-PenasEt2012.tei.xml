<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,127.58,74.37,375.79,12.54;1,210.65,90.47,174.35,12.54">Overview of QA4MRE at CLEF 2012: Question Answering for Machine Reading Evaluation</title>
				<funder ref="#_9wqXrpj">
					<orgName type="full">Spanish Ministry of Science and Innovation</orgName>
				</funder>
				<funder ref="#_EmyCyut">
					<orgName type="full">PROMISE Network of Excellence</orgName>
				</funder>
				<funder ref="#_Uyk2VXK">
					<orgName type="full">Regional Government of Madrid</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,124.70,131.00,61.78,9.05"><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
							<email>anselmo@lsi.uned.es</email>
						</author>
						<author>
							<persName coords="1,194.76,131.00,53.62,9.05"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>hovy@isi.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Information Sciences Institute</orgName>
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,256.71,131.00,58.45,9.05"><forename type="first">Pamela</forename><surname>Forner</surname></persName>
							<email>forner@celct.it</email>
							<affiliation key="aff2">
								<orgName type="institution">CELCT</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,323.69,131.00,62.87,9.05"><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
							<email>alvarory@lsi.uned.es</email>
							<affiliation key="aff0">
								<orgName type="department">NLP&amp;IR group</orgName>
								<orgName type="institution">UNED</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,394.97,131.00,68.33,9.05"><forename type="first">Richard</forename><surname>Sutcliffe</surname></persName>
							<email>richard.sutcliffe@ul.ie</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Limerick</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,471.82,131.00,34.28,9.05;1,168.98,142.52,38.89,9.05"><forename type="first">Caroline</forename><surname>Sporleder</surname></persName>
							<email>csporled@coli.uni-sb.de</email>
							<affiliation key="aff4">
								<orgName type="institution">Saarland University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Al. I</orgName>
								<orgName type="institution">Cuza University of Iasi</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,216.05,142.52,61.20,9.05"><forename type="first">Corina</forename><surname>Forascu</surname></persName>
						</author>
						<author>
							<persName coords="1,285.53,142.52,69.66,9.05"><forename type="first">Yassine</forename><surname>Benajiba</surname></persName>
							<email>yassine.benajiba@philips.com</email>
							<affiliation key="aff6">
								<orgName type="institution">Philips Research North America</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,363.53,142.52,59.46,9.05"><forename type="first">Petya</forename><surname>Osenova</surname></persName>
							<affiliation key="aff7">
								<orgName type="institution">Bulgarian Academy of Sciences</orgName>
								<address>
									<country key="BG">Bulgaria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,127.58,74.37,375.79,12.54;1,210.65,90.47,174.35,12.54">Overview of QA4MRE at CLEF 2012: Question Answering for Machine Reading Evaluation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">45312D9DF4637DECFE1C60B5CF7D4A28</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the Question Answering for Machine Reading (QA4MRE) task at the 2012 Cross Language Evaluation Forum. In the main task, systems answered multiple-choice questions on documents concerned with four different topics. There were also two pilot tasks, Processing Modality and Negation for Machine Reading, and Machine Reading on Biomedical Texts about Alzheimer's disease. This paper describes the preparation of the data sets, the definition of the background collections, the metric used for the evaluation of the systems' submissions, and the results. Eleven groups participated in the task submitting a total of 43 runs in seven languages.</p><p>4. Inferences that require composing several answers, in particular answering one part of the question using the background collection and then, with its answer, answering the other part of the initial question (e.g., "Who is the wife of the person who won the Nobel Peace Prize in 1992?").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TASK DESCRIPTION</head><p>In 2012, we had a main task and two pilot exercises.</p><p>Main Task This remained the same for participants. Background collections, test documents and reading tests were available in Arabic, Bulgarian, English, German, Italian, Romanian, and Spanish. In addition to last year's topics (AIDS, Climate Change, Music and Society), we included a topic on Alzheimer's disease. This new topic is related to a new pilot on Biomedical texts. The difference is that the reference collection for the main task is built from general public sources and for the pilot the source is the PubMed repository.</p><p>Having these two parallel exercises about the same topic but in different domains opens the door to evaluate research approaching the challenges of domain and language adaptation, the use of knowledge in one domain captured in the other, the differences in the background knowledge acquired, the differences between questions and answers in each domain, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pilot on Processing Modality and Negation for Machine</head><p>Reading. This exercise is aimed at evaluating whether systems are able to understand extra-propositional aspects of meaning like modality and negation. Modality is a grammatical category for expressing the attitude of the speaker towards his/her statements, such as expressions of certainty, factuality, and evidentiality. Negation is a grammatical category that allows changing the truth value of a proposition. In the pilot, participants received some texs where they have to decide whether some events are Asserted, Negated, or Speculated. Our plan is to integrate modality and negation in the main task next year.</p><p>Machine Reading on Biomedical Texts about Alzheimer's disease. This exercise is aimed at setting questions in the Biomedical domain with a special focus on one disease, namely Alzheimer's. This pilot task explored the ability of a system to answer questions using scientific language. Texts were taken from PubMed Central related to Alzheimer's and from 66,222 Medline abstracts. In order to keep the task reasonably simple for systems, participants were given the background collection already processed with Tok, Lem, POS, NER, and dependency parsing.</p><p>The two pilot tasks are described in detail in dedicated papers in these proceedings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Reading Comprehension tests are routinely used to assess the degree to which people comprehend what they read, so we work with the hypothesis that it is reasonable to use these tests to assess the degree to which a machine "comprehends" what it is reading.</p><p>When reading a text, a human performs two processes, namely:</p><p>1. s/he partially/fully understands its immediate surface meaning; 2. if needed, s/he makes additional inferences from the text, i.e., performs some kind of reasoning, and solves the textual inferences (linguistic/lexical, co-reference), using previously acquired experience/knowledge of any type.</p><p>To assess the degree and types of understanding, we have the system answer questions about a given text.</p><p>While the desired answer is usually also present in the test document (albeit perhaps in some non-obvious form), it may not be, or the reader may require additional background information to know what to search for, such as explicit and implicit references to entities, events, dates, places, situations, etc. pertaining to the topic.</p><p>In general, more prior background knowledge makes understanding and question answering easier.</p><p>Computational resources such as wordnets, framenets, paraphrase lists, knowledge bases, etc., are aimed at making different kinds of prior knowledge available for the machine. In QA4MRE we add to these resources the possibility to acquire background knowledge from a large collection of related documents. The advantage is the opportunity to gather probability distributions linked to knowledge, and to explore distributional approaches to QA. We discuss background knowledge in Section 3.</p><p>The evaluation questions should be answerable by most humans without the need to explore a specific document of the background collection. Examples of inferences we allow are:</p><p>1. Linguistic inferences such as coreference, deictic references (like "then" and "here"), etc.); 2. Simple ontological inferences such as considering part-of relations or obtaining direct super-concepts for common objects; 3. Inferences considering causal relations or procedural steps in "life scripts" like visiting a restaurant or attending a concert;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Main Task</head><p>Tests were divided into: -4 topics, namely "Aids", "Climate change", "Music and Society" and "Alzheimer"; -Each topic had four reading tests; -Each reading test consisted of one single document, with 10 questions and a set of five choices per question.</p><p>Overall, the following evaluation setting was proposed:</p><p>-16 test documents (4 documents for each of the four topics), -160 questions (10 questions for each document) with , -800 choices/options (5 for each question).</p><p>Test documents and questions were made available in English, German, Italian, Romanian, and Spanish and newly this year also in Arabic and Bulgarian. These materials were exactly the same in all languages, created using parallel translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE BACKGROUND COLLECTIONS</head><p>This is a very important element of the evaluation setting. It connects the task also with the research in Information Retrieval. The goal of reference/background collections is to contextualize the reading of a single document related to the topic by collecting and fleshing out additional pertinent information. In the future this step may be done on the fly as a retrieval process once a single test text is provided. However, for now, we provide a carefully constructed background corpus for two main reasons: to allow more comparison among participant systems, and to focus on the Reading Comprehension problem. We believe it is important to develop a good methodology for building background collections for the evaluation task.</p><p>We define background knowledge in terms of the relation between the testing questions and answers, and the background collection. To determine the potential kinds of uses of the prior knowledge, we distinguish at least four main types of background knowledge (although in fact it's a continuum):</p><p>1. Very specific facts related to the document under study. For example, the relevant relation between two concrete people involved in a specific event. 2. General facts not specific to any particular event. For example, geographical knowledge, main players in international affairs, movie stars, world wars. Also acronyms, transformations between quantities and measures, etc. 3. General abstractions that humans use to interpret language, to generate hypotheses or to fill missing or implicit information. For example, abstractions such as the result of observing the same event with different players (e.g. petroleum companies drill wells, quarterbacks throw passes, etc.) 4. Linguistic knowledge. For example, synonyms, hypernyms, transformations such as active/passive or nominalizations. Also transformations from words to numbers, meronymy, and metonymy. Obviously this is not an exhaustive list. For example, we do not include ontological relations that enable temporal and spatial reasoning, or reasoning on quantities, which are also all relevant.</p><p>Ideally, the background collection should cover completely the corresponding topic. This is feasible sometimes and unrealistic at others. For example, in the case of the pilot on Biomedical documents about Alzheimer's disease, a set of experts built a query (a set of conjunctions and disjunctions over 18 terms) that approximates very much the retrieval of all relevant documents (more than 66,000) without introducing much noise. However, this is not so easy in more open domains (e.g., Climate Change) or cases with non-specialized sources of information. In these cases, we crawl the web using, for each language and topic a list of keywords and a list of sources. Keywords are translated into English and then translated into the rest of the languages. Documents may be crawled from a variety of sources: newspapers, blogs, Wikipedia, journals, magazines, etc. The web sources are obviously language dependent, and each language also requires a list of possible web sites with documents related to the topic.</p><p>We realized in the past edition that, since we organizers knew the test set, we used that information to select the keywords, and ensure the coverage of the questions. The effect is not only that background collections don't cover completely the topic, but also that the collections have some bias with respect to the real distribution of concepts. In this year's campaign, the assumption that the ideal background collection should include all relevant documents for the topic (and only them) is explicit, and we organizers bear it in mind. Thus, we face the same problem as traditional Information Retrieval: we want all relevant documents (and only them), and we use queries (keywords) to retrieve them The first strategy with the aim of ensuring the coverage of the topic as much as possible is to make the topic specific enough (e.g., AIDS medicaments rather than AIDS). The second strategy is to try to cover (at least partially) each of the possible "dimensions/aspects" of that topic. How? First, by detecting a good central overview text, such as a Wikipedia article that "defines" the topic, "suggests" its principal aspects, and provides links to additional good material. Then, organizers enumerate these dimensions and prepare a set of queries for each dimension. They document this process with three benefits: (i) to know what organizers and participants can expect or not from the collection; (ii) to give another dimension of re-usability; and (iii) to explore how Machine Reading will connect to Information Retrieval in the future. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">TEST SET PREPARATION</head><p>This year the datasets was created for the following seven languages: Arabic, Bulgarian, English, German, Italian, Romanian and Spanish. The dataset was created following the methodology developed last year consisting of the following steps:</p><p>1. Four English documents were selected for each of the four topics (Aids, Climate Change, Music and Society, Alzheimer's). These were selected from copyright-free sources (see Table <ref type="table" coords="5,470.14,150.42,4.18,8.96" target="#tab_1">2</ref>) and these represented the test documents against which questions were asked.</p><p>2. In order to have a set of identical questions for the seven languages above, we needed to have the selected test documents translated. For this purpose, expert translators were recruited form the Translation for Progress<ref type="foot" coords="5,205.61,206.64,3.00,5.40" target="#foot_0">1</ref> platform for all languages. On the whole, 57 translators were contacted and asked to perform the translations in a couple of weeks' time. Most of the translations were of a high quality and were delivered within the agreed timescale.</p><p>3. To ensure that translations were faithful to the original document in both meaning and style and of good quality, all the documents were manually checked and corrected when necessary. We wanted to avoid a situation where portions of the original English text were left out of the translation in a particular target language, or perhaps modified or interpreted in a particular manner which would have made the question impossible to answer in that language.</p><p>4. Ten multiple-choice questions were then devised for each test document. A question always had five candidate answers from which to choose, with one clearly correct answer and four clearly incorrect answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Once the questions had been composed in the language of the original author, each was then translated into English. The English versions of the questions and candidate answers were carefully checked by a referee to verify that they were clear, that the intended answer was clearly correct, that the intended answer was in the test document, and that the other candidate answers were clearly incorrect. Questions were modified accordingly.</p><p>6. The English versions were then used to translate each question into each of the seven languages of the task. The same process was used to translate each candidate answer (five per query) into the seven languages.</p><p>7. The result of this process was a set of 160 questions in seven languages, each with five multiple-choice answers, also in those seven languages. The final step was to check that the answer to each question was in fact present in the test document for all the languages of the task. For all questions, the direct answer was contained in the test document; however answering the questions typically required some background knowledge and some form of inference. The required knowledge could be linguistic or could involve basic world knowledge. Linguistic knowledge concerns, for example, the ability to perform co-reference resolution or detect paraphrases on the lexical or syntactic level. World knowledge has to be inferred from the background collection. For instance, the text might mention Barack Obama while the question might refer to the first African American President. The fact that Barack Obama is the first African American President needs to be learnt from the background collection in order to be able to answer the question.</p><p>Typical types of world knowledge involve, for instance, knowledge about the basic referents in a text, e.g., being aware that Yucca Mountain is in Nevada. Another type of world knowledge involves knowledge of "life scripts" such as "visiting a restaurant". Finally, the inference required can also be complex, involving several steps. For example, answering a question might require combining knowledge from the background collection with knowledge from the test document itself. For instance, the question "Who is the wife of the person who won the Nobel Peace Prize in 1992?" contains two facts P and Q, where P="wife of Y=?" and Q="winner of Nobel Peace Prize in 1992=Y". The latter information can be gleaned from the background collection whereas the former is contained within the test document itself.</p><p>For each test document, we aimed for a combination of simple, medium, and difficult questions. At most six questions per document did not require knowledge from the background collection. Two of these were simple questions, i.e., the answer and the fact questioned could be found in the same sentence in the test document. Four questions were of intermediate difficulty in that the answer and the fact questioned were not in the same sentence and could, in fact, be several sentences apart. Finally, the remaining four questions did require utilizing information from the background collection. While not all question types require inference based on the background collection, all of them required some form of textual and linguistic knowledge, such as the ability to detect paraphrases, as we made an effort to re-formulate questions in such a way that the answers could not be found by simple word overlap detection. For each question, we kept track of the inference required to answer it. This made it easier to ensure that that inference could in fact be drawn on the basis of the background collection, i.e., that the background collection did indeed contain the relevant fact. It also makes it possible to carry out further analyses regarding which questions or types of questions were difficult for the systems and why.</p><p>When creating the questions, we took care not to introduce any artificial patterns that would help finding the correct answer. Thus we ensured that all answer choices for a question were approximately the same length and consistent with respect to formulation and content, that all of the wrong answers were plausible, and that the placement of the correct answers was random and balanced.</p><p>Table <ref type="table" coords="8,97.70,418.17,4.98,8.96" target="#tab_3">4</ref> below shows a classification of the questions according to how much and what type of background knowledge they required. The table also provides the average c@1 obtained for each type of question. It can be seen that, unsurprisingly, the types of questions that require little knowledge and inference are generally answered more successfully. Questions requiring inference are by far the hardest, while it does not seem to make much difference whether the knowledge required is found within the test document or in the background collection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Tools and Infrastructure</head><p>This year, CELCT developed a series of infrastructure components to help manage the QA4MRE exercise. Many processes and requirements were to be dealt with:</p><formula xml:id="formula_0" coords="8,70.94,690.11,5.98,7.98">o</formula><p>The need to develop a proper and coherent tool for the management of the data produced during the campaign, to store it and to make it re-usable, as well as to facilitate the analysis and comparison of results; o</p><p>The necessity of assisting the different organizing groups in the various tasks of the data set creation and to facilitate the process of collection and translation of questions;</p><p>o</p><p>The possibility for participants to directly access the data, submit their own runs (this also implied some syntax checks of the format), and later, get the detailed viewing of the results and statistics.</p><p>A series of automatic web interfaces were specifically designed for each of these purposes, with the aim of facilitating the data processing and, at the same time, showing the users only what they needed for the task they had to accomplish. The main characteristic of these interfaces is the flexibility of the system specifically centred on the user's requirements.</p><p>While designing the interfaces for question collection and translation, one of the first issues to be dealt with was the fact of having many assessors, a big amount of data, and a long process. So tools must ensure an efficient and consistent management of the data, allowing:</p><p>1.</p><p>Alteration of the data already entered at any time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Revision of the data by the users themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Consistency propagation ensuring that modifications automatically re-model the output in which they are involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Real time calculation of statistics and evaluation measures.</p><p>In particular, ensuring the consistency of data is a key feature in data management. For example, if a typo is corrected in the Translation Interface, the modification is automatically updated also in the Gold Standard files, in the Test Set files, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EVALUATION</head><p>Since one of the objectives of the task is to assess the ability of systems to understand texts through their answers to questions about those texts, the evaluation focuses on measuring this understanding by computing the correctness of the responses given to the multiple-choice tests. Furthermore, we follow the line introduced in ResPubliQA 2009 [1] of promoting the development of systems able to reason about the correctness of their responses with the aim of reducing the amount of incorrect answers given as output. Thus, this year's evaluation remains quite similar to the one of the last edition.</p><p>Given a question with its corresponding candidate answers, a participant system can return two kinds of responses:</p><p> An answer selected from the set of candidate ones for that question,  A NoA answer. This response is given when the system considers it is not able to find enough evidence about the correctness of candidate answers and it prefers not to answer the question instead of giving an incorrect answer. Thus, it gains some partial credit proportional to the performance shown with the answered questions. Moreover, the system can return as a hypothetical answer the candidate one that it would have been selected, which allows us to give some feedback about its validation performance.</p><p>The assessments of system's responses are given automatically by comparing them against the gold standard collection with human-made annotations. Therefore, no manual assessment was required, which reduces the effort of the evaluation once the collections have been created and facilitates the future development of systems. Each system's response to a question receives one and only one of the following three possible assessments:</p><p> Right if the system has selected the correct answer among the set of candidate ones of the given question;  Wrong if the system has selected one of the wrong answers;  NoA if the system has decided not to answer the question. Where the system returned a hypothetical answer, this answer was assessed as NoA_R in the case of it being correct or NoA_W if it was wrong.</p><p>Given these assessments, we decided to evaluate systems from two different perspectives:</p><p>1. A question-answering approach, as in the traditional evaluation performed in past campaigns, where we just evaluate the ability of systems answering a set of questions. 2. A reading-test evaluation, obtaining figures for each particular reading test and topics. This perspective permits us to evaluate whether a system was able to understand a document and to what degree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Measure</head><p>We use c@1 as the main evaluation measure for this year's campaign. c@1 was first introduced in ResPubliQA 2009 [1] and is fully described in [2]. The formulation of c@1 is given in Formula (1).</p><formula xml:id="formula_1" coords="10,235.09,140.66,126.97,27.12">) ( 1 1 @ n n n n n c R U R   (1)</formula><p>where n R : number of questions correctly answered. n U : number of questions unanswered. n: total number of questions c@1 acknowledges returning NoA answers in the proportion that a system answers questions correctly, which is measured using the traditional accuracy (the proportion of questions correctly answered). Thus, a higher accuracy over answered questions would give more value to unanswered questions, and therefore, a higher final c@1 value. By selecting this measure we wanted to encourage the development of systems able to check the correctness of their responses because NoA answers add value to the final value, while incorrect answers do not.</p><p>As a secondary measure, we also provided scores according to accuracy (see Formula ( <ref type="formula" coords="10,448.11,296.85,3.51,8.96">2</ref>)), the traditional measure applied to past QA evaluations at CLEF. We define accuracy considering both answered and unanswered questions.</p><formula xml:id="formula_2" coords="10,234.45,345.35,128.56,27.31">n n n accuracy UR R   (2)</formula><p>where n R : number of questions correctly answered. n UR : number of unanswered questions whose candidate answer was correct. n: total number of questions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Question Answering Perspective Evaluation</head><p>In the Question Answering perspective we measure systems' performance over a set of questions without considering the ability of a system to understand a certain document. This is an approach similar to the one applied in QA@CLEF campaigns before 2010.</p><p>The information considered for each system at this level is:</p><p> Total number of questions ANSWERED. This number is divided into: o total number of questions ANSWERED with a RIGHT answer, o total number of questions ANSWERED with a WRONG answer.</p><p> Total number of questions UNANSWERED (a NoA response was given). This number is divided into: o total number of questions UNANSWERED with a RIGHT candidate answer, o total number of questions UNANSWERED with a WRONG candidate answer, o total number of questions UNANSWERED with an EMPTY candidate answer.</p><p>This information is used for calculating the following scores:</p><p> An overall c@1 over the whole collection (a set of 160 questions),  A c@1 score for each topic (40 questions for each topic),  An overall accuracy score (over the 160 questions of the test collection, considering also the candidate answers given to unanswered questions as it has been explained above),  The proportion of answers correctly discarded (see Formula (3)) in order to evaluate the validation performance.</p><formula xml:id="formula_3" coords="11,186.00,75.62,225.25,33.96">UE UW UR UE UW n n n n n discarded correctly     _ (3)</formula><p>where: n UR : number of unanswered questions whose candidate answer was correct n UW : number of unanswered questions whose candidate answer was incorrect n UE : number of unanswered questions whose candidate answer was empty</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Reading Perspective Evaluation</head><p>The objective of the reading perspective evaluation is to offer information about the performance of a system "understanding" the meaning of each single document. This understanding is evaluated by means of multiplechoice tests with ten questions per document.</p><p>This evaluation is performed taking as reference the c@1 scores achieved for each test (one document with its ten questions). Afterwards, these c@1 scores can be aggregated at topic and global levels in order to obtain the following values:</p><p> Median, average and standard deviation of c@1 scores at test level, grouped by topic,  Overall median, average and standard deviation of c@1 values at test level.</p><p>The median c@1 has been provided under the consideration that it can be more informative at reading level than average values. This is because median is less affected by outliers than average, and therefore, it offers more information about the ability of a system to understand a text.</p><p>This approach allows us to evaluate systems in a similar way to the manner new language learners are graded. Thus, we can consider that a system passes a test from this evaluation perspective if it achieves a score equal or higher than 0.5. In the case of obtaining an overall average c@1 higher than 0.5, we say that the system passes this evaluation perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Random Baselines</head><p>We propose here a simple baseline to which participants can be compared. Since participant systems can decide to answer or not to answer a given question, we must decide which behaviour must follow our baseline. For simplification purposes, the proposed baseline answers all the questions, randomly selecting each answer from the set of candidate ones.</p><p>This baseline has five possibilities when trying to answer a question: it can select the correct answer to the question, or it can select one of the four incorrect answers. Then, the overall result of this random baseline is 0.2 (both for accuracy and for c@1). Systems applying a certain kind of processing and reasoning should be able to outperform this baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">PARTICIPATION and RESULTS</head><p>From an initial amount of 25 groups that registered to the main task and signed the license agreement to download the background collections, 11 of them finally submitted at least one run, resulting in 43 runs in 7 languages (Arabian, Bulgarian, German, English, Spanish, Italian and Romanian). Table <ref type="table" coords="11,431.58,635.95,4.98,1.00" target="#tab_6">7</ref> shows the number of runs per language.</p><p>There were only 3 cross-lingual runs and all from the same group. The language with the highest amount of runs was, as usual, English with 20 submissions, while Spanish and Italian received only one run per language. Thus, no comparison in these two languages can be performed.</p><p>Tables 5 to 7 summarise the characteristics of the submissions.  Table <ref type="table" coords="12,97.56,580.91,4.98,8.96" target="#tab_9">9</ref> shows the average results for each one of the proposed 16 reading comprehension tests according to c@1. The Table shows that, except for Test 8, the mean value was higher than the proposed baseline, while only half of them were higher in the previous edition <ref type="bibr" coords="12,263.69,601.76,3.24,5.83" target="#b1">2</ref> .</p><p>The mean values for all the tests where under 0.5, the value needed to pass the evaluation from the reading perspective. This result suggests that systems are still far away from obtaining satisfactory results according to this perspective. We can see in Table <ref type="table" coords="13,156.20,107.82,10.01,8.96" target="#tab_10">10</ref> how results across tests in the same topic are more similar than in 2011, which suggests that this year's collections are more homogenous. On the other hand, Table <ref type="table" coords="13,378.48,119.22,9.90,8.96" target="#tab_10">10</ref> shows the mean scores per topic. The scores across topics seem remarkably similar except for Topic 1 (AIDS), which seems to be a bit easier. Table <ref type="table" coords="13,96.26,200.10,10.02,8.96" target="#tab_11">11</ref> shows the results of all the submitted runs grouped by language. It can be seen how most of the systems were able to beat the baseline (only 8 runs performed lower), with at least a system per language able to do so. This amount is higher than in the previous edition (where only half of the systems outperformed the baseline), which is further evidence of the improvement of systems' performance in this edition. Systems were able to find answers for more than 36% of questions in all languages (combination row in Table <ref type="table" coords="13,70.94,257.58,8.93,8.96" target="#tab_11">11</ref>) except Spanish where there was only one run (also cross-language runs have been considered). Considering all languages, 99% of questions received at least one correct answer by at least one system. The best results were obtained in English, where the highest score was obtained by jucs12013enen with 0.65. This value is 25 percentage points higher than the next system (vulc12014enen at 0.40). In fact, jucs12013enen was the only system able to pass the evaluation according to the reading perspective. This system obtained c@1 values over 0.60 for all the topics except for Topic 2 (Climate change). We can consider it a very good result if we compare that system with a person over such complex questions.</p><p>Regarding cross language runs, all of them were from the onto group over different target languages with Romanian as source, which does not allow to make any comparison. All these runs obtained the same result (0.29 of c@1).  <ref type="table" coords="14,97.08,553.19,10.01,8.96" target="#tab_13">13</ref> compares the performance of systems in these two editions of QA4MRE. There has been an overall improvement across all runs between last year and this year (0.21 increased to 0.26), as well as an improvement across best runs of each participant group (0.28 to 0.32). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Analysis of the Use of External Knowledge</head><p>This task tries also to promote the use and combination of external sources of knowledge in order to help answering questions as it has been said above. In order to study it, we asked participants to report the resources employed to assist in answering the questions and we summarise this information in Table <ref type="table" coords="14,435.29,705.74,8.47,8.96" target="#tab_14">14</ref>. 53% of the submitted runs did not employ any kind of external resources, while 23% used only the background collection. The remainder of runs used additional resources, either with or without using the background collection. These observations suggest that the inclusion of such external sources and their exploitation is not yet widely adopted. Moreover, as shown in Table <ref type="table" coords="15,259.43,230.94,8.37,8.96" target="#tab_14">14</ref>, more detailed information about the external sources used for each participant can be seen in Table <ref type="table" coords="15,221.08,242.34,10.01,8.96" target="#tab_5">16</ref> of Appendix 3.</p><p>A subsequent analysis of questions revels that questions requiring no extra knowledge were not much easier than the others. In fact, some of them seem to be considerably harder than some questions that require external resources. This observation suggests that in order to answer questions, the fact of having to compose two or more parts to form an answer is harder than just matching a single piece of text. However, whether the pieces of the answer are in the main text or in a background resource collection does not make much difference. It is more relevant for the performance how difficult the pieces are to match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Analysis of Systems</head><p>Table <ref type="table" coords="15,96.72,372.21,10.01,8.96" target="#tab_4">15</ref> in Appendix 3 summarizes the set of techniques that participants have reported are being used in their systems. There was only one system that did not perform any kind of question analysis, while most of the other systems employed questions patterns, with a high proportion of systems acquiring them automatically.</p><p>Regarding the linguistic processing, the most popular techniques were PoS tagging, the use of NER tools and dependency parsers, which were also some of the most applied techniques in previous editions. However, very few systems explore the use of deeper techniques relying on semantics, while only one relied on logic representation and a theorem prover.</p><p>Those two systems applying the most different techniques (jucs and idrq) were the ones that best performed in their languages (English and Arabic respectively). However, system vulc, which performs very well in English, reported only the use of phrase transformations. Therefore, it does not seem to be very clear which is the best combination of techniques in order to obtain a good performance. Evaluation frameworks such as the one presented in this paper must be used by researchers for exploring and answering such questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>While this year's results show some improvement compared to last year, specially respect to the respective baselines, the majority of systems are still far from being able to pass a Reading Comprehension test. Nevertheless, best systems are, in general, very close to achieve this goal.</p><p>When we defined the task we kept in mind three main ideas: that we are developing a validation technology able to determine if a particular answer is correct or not; that knowledge is crucial for understanding; and that a large set of documents related to a topic could be an additional source of background knowledge. We discuss each in turn:</p><p>The first question is whether the technology developed so far is just ranking the options or it is validating them. The difference is important: What happens if we don't provide the options? Most systems use a kind of similarity measure or they don't use validation at all. Thus, more than validating the answers, systems are ranking them. This leads to the need of a change for next campaign. Again, the option of gaining partial credit by leaving some questions unanswered and reduce the number of incorrect answers is not enough. We need to introduce an explicit assessment of the ability to reject candiate answers when they are incorrect. This could be done easily in our framework if we introduce a significant portion of questions where none of the options are correct and a last option in all questions "None of the answers above are correct". If a significant portion of questions (up to 40%) have no correct answer among its options, this will give as a new baseline to beat: a dummy system that always chose there is no correct answer as default.</p><p>About the second and third issues, it seems that the use of external resources help to improve results, but this is not so clear in the case of background collections. Although we have refined the methodology to build the background collections this year, most participants don't seem to know how to gather usable background knowledge from it. Moreover, it seems that the use of other external resources benefit more than the use of the background collections. We need to decide on this issue because the organization is spending a lot of resources in creating theses collections <ref type="bibr" coords="16,186.14,175.32,3.00,5.40" target="#b2">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX 1: Overall results at TOPIC level: Median, Average, and Standard</head><p>Deviation for all runs RUN_NAME Overall c@1 c@1 topic_1 c@1 topic_2 c@1 topic_3 c@1 topic_4 btbn12011bgbg 0,29 0,25 0,25 0,35 0,33 btbn12021bgbg 0,12 0,10 0,03 0,18 0,18 btbn12031bgbg 0,20 0,23 0,15 0,23 0,20 diue12012enen 0,29 0,33 0,28 0,14 0,40 diue12024enen 0,31 0,40 0,25 0,20 0,40 fdcs12011enen 0,12 0,16 0,07 0,16 0,08 fdcs12021enen 0,19 0,30 0,14 0,18 0,12 fdcs12032enen 0,14 0,23 0,07 0,16 0,08 fdcs12042enen 0,20 0,30 0,14 0,18 0,12 idrq12011arar 0,13 0,25  x System analyzes the questions and the answers and it is looking for overlapping n-grams from the analyses of the question and the answers.</p><p>The answer with greatest overlapping is selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>diue x</head><p>The first run is based on surface text analysis, with no grammatical nor semantic processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>fdcs</head><p>The system uses redundancies in the collection  The system uses lexical distances such as Word Proximity and similarity measures to select the candidate answer more related with the question. The system also uses Latent Semantic Analysis to extract latent topics from the test documents loga</p><p>x</p><p>The system parses questions, answers and documents. Coreference resolution is applied to the document representation. The system constructs a hypothesis from question and answer parse. It then tries to prove the hypothesis from the logical document The system relies on approximate string matching -an approach that makes it easily adaptable to any language without significant modifications. The analysis comprises two phases: 1. Extraction of sentences that match best the query and answer string uaic x x x</p><p>Romanian system uses syntactic similarity and the English system uses semantic similarity. vulc</p><p>x</p><p>The system uses a simple principle: An answer A is likely to be the right answer to the question Q if a sentence like A in the document is very close (within 1 or 2 sentences) to a sentence Sq like Q in the document.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="23,88.58,632.83,19.60,12.00;23,190.10,635.65,42.57,9.05;23,243.77,610.45,4.98,9.05;23,276.41,609.98,218.22,7.31;23,276.41,619.10,221.62,7.31;23,276.41,628.34,216.27,7.31;23,276.41,637.58,193.13,7.31;23,88.58,647.71,18.96,12.00;23,209.81,647.41,38.94,12.29;23,276.41,652.46,133.66,7.31"><head></head><label></label><figDesc>IDRAAQ" is developed for Arabic QA integrates three levels of processing: -Keyword-based level: uses a Arabic WordNet based Query Expansion module -Structure-based level: consists in measuring the Density Distance N-gram Model of candidate jucs x x x It uses Lexical, Syntactic, Semantic level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,76.34,659.00,446.80,77.46"><head>Table 1 : Size of the background collections in the various languages for all topics</head><label>1</label><figDesc></figDesc><table coords="3,76.34,671.10,446.80,65.35"><row><cell></cell><cell>AR</cell><cell>BG</cell><cell>DE</cell><cell>EN</cell><cell>ES</cell><cell>IT</cell><cell>RO</cell></row><row><cell>TOPICS</cell><cell># docs</cell><cell># docs</cell><cell># docs</cell><cell># docs</cell><cell># docs</cell><cell># docs</cell><cell># docs</cell></row><row><cell></cell><cell>KB</cell><cell>KB</cell><cell>KB</cell><cell>KB</cell><cell>KB</cell><cell>KB</cell><cell>KB</cell></row><row><cell>ALZHEIMER</cell><cell>19,278 docs 173,951 KB</cell><cell>19,412 docs 194,326 KB</cell><cell>18,506 docs 146,965KB</cell><cell>13,045 docs 254,924 KB</cell><cell>6,199 docs 42,899 KB</cell><cell>9,008 docs 60,819 KB</cell><cell>9,590 docs 121,413 KB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,79.94,530.09,439.06,108.53"><head>Table 2 : Test Documents Topic No.</head><label>2</label><figDesc></figDesc><table coords="5,81.74,542.09,437.26,96.53"><row><cell></cell><cell></cell><cell>Source</cell><cell>Author</cell><cell>Title</cell><cell>LICENSE</cell><cell>Words</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>"The Dis-</cell><cell>Creative commons</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Integration of</cell><cell>Attribution</cell></row><row><cell></cell><cell></cell><cell>http://www.fpif.org/article</cell><cell></cell><cell>U.S. Global</cell><cell></cell></row><row><cell>AIDS</cell><cell>1</cell><cell>s/the_dis-integration_of_us_global_</cell><cell>Jodi L. Jacobson</cell><cell>AIDS Funding" (Washington,</cell><cell></cell><cell>1350</cell></row><row><cell></cell><cell></cell><cell>aids_funding</cell><cell></cell><cell>DC: Foreign</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Policy In Focus,</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>March 3, 2003)</cell><cell></cell></row></table><note coords="6,147.26,323.18,91.30,8.10;6,147.26,333.50,93.95,8.10;6,147.26,343.94,85.11,8.10;6,147.26,354.26,92.67,8.10;6,147.26,364.58,47.71,8.10;6,147.26,375.38,92.91,8.10;6,147.26,385.82,71.91,8.10;6,147.26,396.14,95.43,8.10;6,147.26,406.46,42.66,8.10;7,70.94,362.13,452.90,8.96;7,70.94,373.65,453.46,8.96;7,70.94,385.17,453.16,8.96;7,70.94,396.57,452.83,8.96;7,70.94,408.09,453.13,8.96;7,70.94,419.61,301.04,8.96;7,70.94,442.55,250.88,9.06;7,70.94,465.59,230.31,8.96;7,70.94,488.63,257.60,8.96;7,70.94,511.67,350.88,8.96;7,70.94,534.59,298.28,8.96"><p><p><p><p>http://chevyvolt.cm.fmpu b.net/#http://boingboing.n et/2011/08/05/3-thingsyou-need-to-know-aboutbiofuels.html http://www.scidev.net/en/ policy-briefs/brazilclimate-change-a-countryprofile.html</p>For each text in the test set 10 multiple choice questions were created. Each question had five answer options. The questions covered five different question types: purpose, method, causal, factoid, and which-is-true. Factoid questions were divided into the following sub-types: Location, Number, Person, List, Time and Unknown. Examples of the basic question types are given below. We took care to spread the question types evenly for a given test document, aiming for two questions per type. The exact breakdown of the number of questions per type in the test collection is provided in Table</p>3</p>below. Example questions: PURPOSE: What is the aim of Obama's cap-and-trade policy? METHOD: How could vast quantities of petrol be saved? CAUSAL: What is the reason for the high price of solar energy? FACTOID (time): When are bioethanol and biodiesel expected to become widely used? WHICH-IS-TRUE: Which of the following goals is Europe committed to?</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,171.86,557.93,249.23,106.12"><head>Table 3 : Distribution of question types Question type</head><label>3</label><figDesc></figDesc><table coords="7,362.11,569.93,58.98,20.41"><row><cell>Total number</cell></row><row><cell>of questions</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,83.42,498.89,402.28,113.90"><head>Table 4 : Classification of questions according to the knowledge required to answer them Types of question #of questions</head><label>4</label><figDesc></figDesc><table coords="8,465.67,511.31,18.67,8.96"><row><cell>c@1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="11,174.98,732.92,245.57,8.89"><head>Table 5 : Overall participants and runs in QA4MRE tasks</head><label>5</label><figDesc></figDesc><table coords="12,70.94,73.60,444.35,44.02"><row><cell>REGISTERED</cell><cell>PARTICIPANTS</cell><cell>PARTICIPANTS</cell><cell>TOTAL NUMBER OF</cell></row><row><cell>PARTICIPANTS</cell><cell>DOWNLOADING</cell><cell>SUBMITTING RUNS</cell><cell>RUNS</cell></row><row><cell></cell><cell>THE TEST SETS</cell><cell></cell><cell></cell></row><row><cell>38</cell><cell>24</cell><cell>21</cell><cell>88</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,70.94,134.76,431.51,68.06"><head>Table 6 : Participants and runs per tasks</head><label>6</label><figDesc></figDesc><table coords="12,70.94,146.46,431.51,56.36"><row><cell>NUMBER of PARTICIPANTS</cell><cell></cell><cell>NUMBER of RUNS</cell><cell>88</cell></row><row><cell>MAIN</cell><cell>11</cell><cell>MAIN</cell><cell>43</cell></row><row><cell>BIOMEDICAL about</cell><cell>7</cell><cell>BIOMEDICAL about</cell><cell>42</cell></row><row><cell>ALZHEIMER</cell><cell></cell><cell>ALZHEIMER</cell><cell></cell></row><row><cell>MODALITY AND NEGATION</cell><cell>3</cell><cell>MODALITY AND NEGATION</cell><cell>3 zip</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="12,114.24,231.48,398.72,153.04"><head>Table 7 : Runs submitted per language in the QA4MRE Main Task Target languages (corpus and answer)</head><label>7</label><figDesc></figDesc><table coords="12,114.24,265.40,398.72,119.12"><row><cell></cell><cell></cell><cell>AR</cell><cell>BG</cell><cell>DE</cell><cell>EN</cell><cell>ES</cell><cell>IT</cell><cell>RO</cell><cell>Total</cell></row><row><cell>(questns)</cell><cell>AR BG DE</cell><cell>4</cell><cell>5</cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>4 5 3</cell></row><row><cell>Source langs</cell><cell>EN ES IT RO</cell><cell></cell><cell>1</cell><cell></cell><cell>20 1</cell><cell>1</cell><cell>1 1</cell><cell>6</cell><cell>20 1 1 9</cell></row><row><cell></cell><cell>Total</cell><cell>4</cell><cell>6</cell><cell>3</cell><cell>21</cell><cell>1</cell><cell>2</cell><cell>6</cell><cell>43</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="12,70.94,400.41,453.52,66.44"><head>Table 8</head><label>8</label><figDesc>below shows the percentage of correct and NoA answers for different question types. Percentages of correct answers overall for Purpose, Factoid and Which-is-true are very similar at around 25%. Method and Causal are not much lower at 22.24% and 20.86%. NoA scores are similar over different question types; the highest is Which-is-true at 17.32% and the lowest is Method at 15.56%. Hence, while Method and Causal might be a bit more difficult than the other question types, possibly due to the fact that they tend to require more inference, overall the question types were quite balanced with respect to difficulty.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="12,115.82,483.53,363.49,80.53"><head>Table 8 : Percentage of Correct and NoA answers according to different question type Question type % of correct answers % of NoA answers</head><label>8</label><figDesc></figDesc><table coords="12,120.50,507.13,323.90,56.93"><row><cell>PURPOSE</cell><cell>25.23%</cell><cell>17.14%</cell></row><row><cell>METHOD</cell><cell>22.24%</cell><cell>15.56%</cell></row><row><cell>CAUSAL</cell><cell>20.86%</cell><cell>17.70%</cell></row><row><cell>FACTOID*</cell><cell>25.25%</cell><cell>16.79%</cell></row><row><cell>WHICH-IS-TRUE</cell><cell>25.28%</cell><cell>17.32%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="12,111.38,673.28,413.72,43.61"><head>Table 9 : Mean Scores for each Reading Test</head><label>9</label><figDesc></figDesc><table coords="12,111.38,684.70,413.72,32.18"><row><cell></cell><cell cols="2">Topic 1</cell><cell></cell><cell></cell><cell cols="2">Topic 2</cell><cell></cell><cell></cell><cell cols="2">Topic 3</cell><cell></cell><cell></cell><cell cols="2">Topic 4</cell><cell></cell></row><row><cell>Test</cell><cell>Test</cell><cell>Test</cell><cell>Test</cell><cell>Test</cell><cell>Test</cell><cell>Test</cell><cell>Test</cell><cell>Test</cell><cell>Test</cell><cell>Test</cell><cell>Test</cell><cell>Test</cell><cell>Test</cell><cell>Test</cell><cell>Test</cell></row><row><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="13,189.86,154.20,211.41,31.49"><head>Table 10 : Mean Scores for each Topic</head><label>10</label><figDesc></figDesc><table coords="13,189.86,165.50,211.41,20.18"><row><cell></cell><cell>Topic 1</cell><cell>Topic 2</cell><cell>Topic 3</cell><cell>Topic 4</cell></row><row><cell>Average</cell><cell>0.32</cell><cell>0.24</cell><cell>0.25</cell><cell>0.24</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="13,157.94,303.99,279.56,445.26"><head>Table 11 : c@1 in participating systems according to the language</head><label>11</label><figDesc></figDesc><table coords="13,157.94,317.63,279.56,431.62"><row><cell>System name</cell><cell>AR</cell><cell>BG</cell><cell>DE</cell><cell>EN ES</cell><cell>IT</cell><cell>RO</cell></row><row><cell>Combination</cell><cell cols="4">38.75 66.87 36.87 93.75</cell><cell cols="2">46.50 55.62</cell></row><row><cell>jucs12013enen</cell><cell></cell><cell></cell><cell></cell><cell>0.65</cell><cell></cell><cell></cell></row><row><cell>vulc12014enen</cell><cell></cell><cell></cell><cell></cell><cell>0.40</cell><cell></cell><cell></cell></row><row><cell>vulc12034enen</cell><cell></cell><cell></cell><cell></cell><cell>0.38</cell><cell></cell><cell></cell></row><row><cell>onto12031itit</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.35</cell><cell></cell></row><row><cell>vulc12024enen</cell><cell></cell><cell></cell><cell></cell><cell>0.35</cell><cell></cell><cell></cell></row><row><cell>l2fi12041enen</cell><cell></cell><cell></cell><cell></cell><cell>0.34</cell><cell></cell><cell></cell></row><row><cell>onto12081roro</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.34</cell></row><row><cell>l2fi12031enen</cell><cell></cell><cell></cell><cell></cell><cell>0.33</cell><cell></cell><cell></cell></row><row><cell>diue12024enen</cell><cell></cell><cell></cell><cell></cell><cell>0.31</cell><cell></cell><cell></cell></row><row><cell>onto12021enen</cell><cell></cell><cell></cell><cell></cell><cell>0.31</cell><cell></cell><cell></cell></row><row><cell>onto12071bgbg</cell><cell></cell><cell>0.30</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>btbn12011bgbg</cell><cell></cell><cell>0.29</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>diue12012enen</cell><cell></cell><cell></cell><cell></cell><cell>0.29</cell><cell></cell><cell></cell></row><row><cell>onto12011bgbg</cell><cell></cell><cell>0.28</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>onto12091dede</cell><cell></cell><cell></cell><cell>0.28</cell><cell></cell><cell></cell><cell></cell></row><row><cell>onto12101eses</cell><cell></cell><cell></cell><cell></cell><cell>0.28</cell><cell></cell><cell></cell></row><row><cell>uaic12082enen</cell><cell></cell><cell></cell><cell></cell><cell>0.28</cell><cell></cell><cell></cell></row><row><cell>loga12023dede</cell><cell></cell><cell></cell><cell>0.26</cell><cell></cell><cell></cell><cell></cell></row><row><cell>uaic12092enen</cell><cell></cell><cell></cell><cell></cell><cell>0.26</cell><cell></cell><cell></cell></row><row><cell>loga12011dede</cell><cell></cell><cell></cell><cell>0.25</cell><cell></cell><cell></cell><cell></cell></row><row><cell>uaic12024roro</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.25</cell></row><row><cell>uaic12062enen</cell><cell></cell><cell></cell><cell></cell><cell>0.25</cell><cell></cell><cell></cell></row><row><cell>uaic12042roro</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.24</cell></row><row><cell>uaic12014roro</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.23</cell></row><row><cell>uaic12034roro</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.23</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" coords="14,70.94,462.75,397.53,99.40"><head>Table 12 : c@1 in participating systems (cross-lingual) according to the language</head><label>12</label><figDesc></figDesc><table coords="14,70.94,476.53,361.48,85.62"><row><cell>System name</cell><cell>AR BG</cell><cell>DE</cell><cell>EN</cell><cell>ES</cell><cell>IT</cell><cell>RO</cell></row><row><cell>onto12041robg</cell><cell>0.29</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>onto12051roen</cell><cell></cell><cell></cell><cell>0.29</cell><cell></cell><cell></cell><cell></cell></row><row><cell>onto12061roit</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.29</cell><cell></cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" coords="14,174.38,599.57,246.44,42.41"><head>Table 13 : Average Scores over all runs and over best runs</head><label>13</label><figDesc></figDesc><table coords="14,190.94,610.99,209.41,30.98"><row><cell></cell><cell>over all runs</cell><cell>over all best runs</cell></row><row><cell>QA4MRE 2012</cell><cell>0.26</cell><cell>0.32</cell></row><row><cell>QA4MRE 2011</cell><cell>0.21</cell><cell>0.28</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" coords="14,147.14,728.43,300.95,9.96"><head>Table 14 : Categorisation of runs, depending on the resources used</head><label>14</label><figDesc></figDesc><table coords="15,83.42,74.32,58.21,8.96"><row><cell>Types of runs</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="5,76.58,692.95,447.88,8.10;5,78.14,703.27,446.44,8.10;5,78.14,713.59,446.13,8.10;5,78.14,723.91,446.07,8.10;5,78.14,734.35,446.23,8.10;5,78.14,744.67,128.39,8.10"><p>http://www.translationsforprogress.org/main.php A Translation Exchange site linking volunteer translators (e.g., linguistics students or professionals in foreign languages interested in building experience as translators can link up with low-budget organizations who are in need of translation work, but without the budget to pay for it. There are currently over 1450 registered volunteer translator members (for 13 language combinations) and over 160 organization members. Translation for Progress database is open for viewing for the general public, but if you wish to post your profile or contact a volunteer translator, a registration is required.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="12,76.22,744.67,230.13,8.10"><p>It must be mentioned that there were 12 tests in QA4MRE 2011</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="16,76.94,727.24,285.90,9.05"><p>Note this year we Google's API wasn't available for research purposes.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work has been partially supported by the <rs type="funder">Spanish Ministry of Science and Innovation</rs>, through the project <rs type="projectName">Holopedia</rs> (<rs type="grantNumber">TIN2010-21128-C02</rs>), and the <rs type="funder">Regional Government of Madrid</rs>, through the project <rs type="projectName">MA2VICMR</rs> (<rs type="grantNumber">S2009/TIC1542</rs>). This work has been partially supported by the <rs type="funder">PROMISE Network of Excellence</rs> (<rs type="grantNumber">258191</rs>). Special thanks are due <rs type="person">Giovanni Moretti</rs> (<rs type="affiliation">CELCT, Trento, Italy</rs>) for the technical support in the management of all data and evaluation scripts of the campaign.</p><p>We would also like to acknowledge the volunteer translators that contributed to the creation of the dataset: <rs type="person">Mercedes Marta Moreno</rs>; <rs type="person">Juan Manuel Pérez Rojas</rs>; <rs type="person">Adriana Pedemonte</rs>; <rs type="person">Sophie</rs>; <rs type="person">Ana Casillas Tomasin</rs>; <rs type="person">Mayra Alvarez</rs>; <rs type="person">MARÍA SOL ACCOSSATO</rs>; <rs type="person">Natalia Steckel</rs>; <rs type="person">María Constanza Galli</rs>; <rs type="person">Fiorela</rs>; <rs type="person">Hanna</rs>; <rs type="person">Yessica V. Apolo Martínez</rs>; <rs type="person">Fatima Alvarez</rs>; <rs type="person">Alberto Mengibar Martin</rs>; <rs type="person">Taras Giovanna</rs>; <rs type="person">Pamela Aikpa Gnaba</rs>; <rs type="person">Danielle</rs>; <rs type="person">Marco Menegazzi</rs>; <rs type="person">Alfredo Lo Bello</rs>; <rs type="person">Chiara S.</rs>; <rs type="person">Martina Scarano</rs>; <rs type="person">Nunzio</rs>; <rs type="person">Francesca Rubino</rs>; <rs type="person">Katia G</rs>; <rs type="person">Lucia Zirattu</rs>; <rs type="person">Camilla Cosmelli</rs>; <rs type="person">Sara Colombo</rs>; <rs type="person">Chiara Gavasso</rs>; <rs type="person">Katie M</rs>; <rs type="person">Antti</rs>; <rs type="person">Saskia Scharnowski</rs>; <rs type="person">Jeffrey Bunce</rs>; <rs type="person">Gabriele Mark</rs>; <rs type="person">Melanie Liebchen</rs>; <rs type="person">Helena Knaup</rs>; <rs type="person">Judith Müller</rs>; <rs type="person">Kathrin Meier</rs>; <rs type="person">Anika Abel</rs>; <rs type="person">Eva Wagle-Fopp</rs>; <rs type="person">Franziska Bioh</rs>; <rs type="person">Irina Rata</rs>; <rs type="person">Nadia Bucurenci</rs>; <rs type="person">Daniela Arsinel</rs>; <rs type="person">Cristina Manoli</rs>; luminita <rs type="person">Isaic</rs>; <rs type="person">Nicoleta Mihaita</rs>; <rs type="person">Mohamed ElGohary</rs>; <rs type="person">Dina Awadallah</rs>; <rs type="person">Nawel</rs>; <rs type="person">Amal</rs>; <rs type="person">Sarah</rs>; <rs type="person">Shameem</rs>; <rs type="person">Rabie Mustapha</rs>; <rs type="person">Difaf Sharba</rs>; <rs type="person">Amani</rs>; <rs type="person">Khebouri Amina</rs>; <rs type="person">Manel Rada</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_9wqXrpj">
					<idno type="grant-number">TIN2010-21128-C02</idno>
					<orgName type="project" subtype="full">Holopedia</orgName>
				</org>
				<org type="funded-project" xml:id="_Uyk2VXK">
					<idno type="grant-number">S2009/TIC1542</idno>
					<orgName type="project" subtype="full">MA2VICMR</orgName>
				</org>
				<org type="funding" xml:id="_EmyCyut">
					<idno type="grant-number">258191</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>btbn12021bgbg 0,10 0,30 0,00 0,00 0,10 0,00 0,00 0,00 0,30 0,20 0,10 0,10 0,10 0,20 0,30 0,10 btbn12031bgbg 0,20 0,20 0,30 0,20 0,10 0,30 0,10 0,10 0,10 0,30 0,20 0,30 0,10 0,20 0,30 0,20 diue12012enen 0,50 0,20 0,20 0,40 0,30 0,20 0,40 0,20 0,00 0,30 0,00 0,20 0,60 0,30 0,40 0,30 diue12024enen 0,70 0,30 0,20 0,40 0,20 0,30 0,30 0,20 0,20 0,30 0,10 0,20 0,60 0,30 0,60 0,10 fdcs12011enen 0,00 0,16 0,32 0,16 0,00 0,00 0,00 0,20 0,15 0,00 0,00 0,42 0,34 0,00 0,00 0,00 fdcs12021enen 0,36 0,26 0,33 0,24 0,15 0,12 0,00 0,20 0,24 0,00 0,00 0,39 0,32 0,16 0,00 0,00 fdcs12032enen 0,00 0,26 0,42 0,24 0,00 0,00 0,00 0,20 0,15 0,00 0,00 0,42 0,34 0,00 0,00 0,00 fdcs12042enen 0,33 0,26 0,30 0,30 0,15 0,12 0,00 0,20 0,24 0,00 0,00 0,39 0,32 0,16 0,00 0,00 idrq12011arar 0,32 0,16 0,32 0,18 0,30 0,34 0,00 0,00 0,00 0,00 0,19 0,00 0,00 0,00 0,00 0,18 idrq12021arar 0,52 0,30 0,28 0,30 0,30 0,42 0,00 0,00 0,26 0,00 0,00 0,00 0,00 0,19 0,30 0,14 jucs12013enen 0,77 0,70 0,78 0,80 0,42 0,48 0,15 0,17 0,60 0,55 0,65 0,78 0,84 0,90 0,66 0,60 l2fi12011enen 0,00 0,40 0,20 0,20 0,50 0,20 0,10 0,10 0,20 0,10 0,10 0,00 0,40 0,30 0,30 0,10 l2fi12021enen 0,50 0,10 0,30 0,22 0,12 0,10 0,30 0,11 0,44 0,10 0,33 0,33 0,10 0,10 0,12 0,10 l2fi12031enen 0,50 0,60 0,22 0,30 0,70 0,10 0,30 0,40 0,30 0,40 0,20 0,24 0,10 0,55 0,30 0,10 l2fi12041enen 0,60 0,40 0,33 0,40 0,60 0,20 0,30 0,30 0,20 0,60 0,20 0,24 0,10 0,50 0,30 0,20 loga12011dede 0,24 0,12 0,30 0,36 0,15 0,00 0,15 0,00 0,77 0,32 0,30 0,14 0,15 0,16 0,36 0,17 loga12023dede 0,36 0,12 0,30 0,36 0,15 0,13 0,15 0,00 0,77 0,32 0,30 0,14 0,15 0,16 0,36 0,17 mira12011arar 0,16 0,00 0,16 0,32 0,32 0,48 0,00 0,00 0,16 0,00 0,00 0,16 0,48 0,00 0,00 0,16 mira12021arar 0,20 0,30 0,10 0,30 0,40 0,20 0,20 0,20 0,30 0,10 0,00 0,20 0,20 0,20 0,20 0,00 onto12011bgbg 0,20 0,50 0,20 0,50 0,30 0,40 0,30 0,10 0,20 0,30 0,60 0,00 0,20 0,30 0,10 0,20 onto12021enen 0,60 0,20 0,20 0,70 0,40 0,20 0,30 0,10 0,20 0,40 0,50 0,20 0,20 0,30 0,10 0,40 onto12031itit 0,40 0,30 0,60 0,50 0,40 0,40 0,30 0,30 0,20 0,40 0,40 0,10 0,50 0,30 0,20 0,30 onto12041robg 0,10 0,20 0,30 0,30 0,30 0,40 0,30 0,20 0,30 0,50 0,60 0,10 0,40 0,20 0,30 0,10 onto12051roen 0,10 0,20 0,30 0,30 0,30 0,40 0,30 0,20 0,30 0,50 0,60 0,10 0,40 0,20 0,30 0,10 onto12061roit 0,10 0,20 0,30 0,30 0,30 0,40 0,30 0,20 0,30 0,50 0,60 0,10 0,40 0,20 0,30 0,10 onto12071bgbg 0,60 0,30 0,30 0,50 0,30 0,10 0,40 0,20 0,20 0,30 0,70 0,00 0,40 0,20 0,20 0,10 onto12081roro 0,40 0,60 0,10 0,50 0,40 0,30 0,20 0,20 0,40 0,50 0,40 0,10 0,40 0,40 0,30 0,30 onto12091dede 0,40 0,10 0,40 0,40 0,30 0,30 0,20 0,20 0,30 0,20 0,30 0,10 0,10 0,40 0,40 0,30 onto12101eses 0,30 0,40 0,30 0,30 0,20 0,40 0,40 0,20 0,20 0,30 0,50 0,20 0,30 0,20 0,20 0,10 uaic12014roro 0,20 0,30 0,10 0,12 0,33 0,33 0,30 0,11 0,20 0,10 0,30 0,20 0,30 0,33 0,12 0,28 uaic12024roro 0,40 0,30 0,20 0,22 0,33 0,33 0,30 0,11 0,20 0,10 0,30 0,20 0,30 0,33 0,12 0,28 uaic12034roro 0,40 0,30 0,10 0,22 0,33 0,33 0,30 0,12 0,20 0,10 0,30 0,10 0,30 0,33 0,12 0,00 uaic12042roro 0,22 0,22 0,26 0,28 0,20 0,50 0,20 0,11 0,24 0,12 0,24 0,11 0,33 0,12 0,33 0,24 uaic12052roro 0,28 0,00 0,18 0,17 0,26 0,50 0,15 0,13 0,00 0,00 0,14 0,14 0,13 0,16 0,26 0,26 uaic12062enen 0,20 0,10 0,50 0,30 0,44 0,20 0,22 0,00 0,12 0,40 0,28 0,11 0,14 0,16 0,24 0,33 uaic12072enen 0,24 0,00 0,55 0,36 0,42 0,13 0,32 0,00 0,00 0,39 0,16 0,12 0,17 0,00 0,15 0,00 uaic12082enen 0,33 0,24 0,48 0,30 0,16 0,12 0,33 0,36 0,12 0,45 0,44 0,00 0,12 0,17 0,28 0,30 uaic12092enen 0,30 0,20 0,55 0,30 0,12 0,11 0,30 0,42 0,11 0,33 0,40 0,13 0,11 0,14 0,22 0,30 uaic12102enen 0,26 0,14 0,48 0,39 0,00 0,12 0,32 0,36 0,00 0,34 0,39 0,00 0,15 0,17 0,00 0,30 vulc12014enen 0,70 0,50 0,60 0,40 0,50 0,70 0,30 0,10 0,50 0,40 0,50 0,20 0,20 0,10 0,40 0,30 vulc12024enen 0,60 0,60 0,40 0,60 0,50 0,60 0,00 0,20 0,20 0,30 0,30 0,20 0,20 0,10 0,40 0,40 vulc12034enen 0,70 0,60 0,50 0,50 0,50 0,80 0,10 0,20 0,20 0,30 0,30 0,10 0,20 0,30 0,40 0,40 Average 0,34 0,28 0,31 0,33 0,29 0,27 0,21 0,16 0,24 0,26 0,29 0,17 0,27 0,23 0,24 0,20 Median 0,32 0,26 0,30 0,30 0,30 0,30 0,30 0,20 0,20 0,30 0,30 0,14 0,20 0,20 0,26 0,18</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX 2: Overall results at READING TEST level: Median, Average, and Standard Deviation for all runs</head><p>c@1 r id_ 10 c@1 r id_11 c@1 r id_ 12 c@1 r id_ 13 c@1 r id_ 14 c@1 r id_ 15 c@1 r id_ 16 btbn12011bgbg 0,30 0,30 0,20 0,20 0,20 0,01 0,40 0,30 0,20 0,50 0,50 0,20 0,40 0,30 0,20 0,40 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX 3: SYSTEM DESCRIPTIONS</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="16,80.17,482.03,444.28,8.96;16,70.94,493.55,453.44,8.96;16,70.94,505.07,453.50,8.96;16,70.94,516.59,453.63,8.96;16,70.94,527.99,453.67,8.96;16,70.94,539.51,262.85,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="16,263.09,493.55,261.29,8.96;16,70.94,505.07,105.82,8.96">Overview of ResPubliQA 2009: Question Answering Evaluation over European Legislation</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Corina</forename><surname>Forascu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iñaki</forename><surname>Alegria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Petya</forename><surname>Osenova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,100.82,516.59,423.76,8.96;16,70.94,527.99,113.57,8.96">Multilingual Information Access Evaluation Vol. I Text Retrieval Experiments. Workshop of the Cross-Language Evaluation Forum</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Di Nunzio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kurimo</surname></persName>
		</editor>
		<editor>
			<persName><surname>Th</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Mostefa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Peñas</surname></persName>
		</editor>
		<editor>
			<persName><surname>Roda</surname></persName>
		</editor>
		<meeting><address><addrLine>CLEF; Corfu. Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2009-09-30">2009. 30 September -2 October. 2010</date>
		</imprint>
	</monogr>
	<note>Revised Selected Papers. Lecture Notes in Computer Science 6241</note>
</biblStruct>

<biblStruct coords="16,74.71,557.03,449.80,8.96;16,70.94,568.55,453.51,8.96;16,70.94,580.07,199.00,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="16,240.53,557.03,179.20,8.96">A Simple Measure to Assess Non-response</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alvaro</forename><surname>Rodrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,440.99,557.03,83.52,8.96;16,70.94,568.55,453.51,8.96;16,70.94,580.07,21.54,8.96">Proceedings of 49th Annual Meeting of the Association for Computational Linguistics -Human Language Technologies (ACL-HLT 2011)</title>
		<meeting>49th Annual Meeting of the Association for Computational Linguistics -Human Language Technologies (ACL-HLT 2011)<address><addrLine>Portland. Oregon. USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">June 19-24. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,74.70,597.59,447.09,8.96;16,70.94,608.99,431.82,8.96;16,70.94,620.51,203.14,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="16,491.74,597.59,30.05,8.96;16,70.94,608.99,130.30,8.96">Testing Lexical Approaches in QA4MRE</title>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luísa</forename><surname>Coheur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ana</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cristina</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ricardo</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">De</forename><surname>Matos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,220.85,608.99,281.92,8.96;16,70.94,620.51,50.84,8.96">Proceedings of CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers</title>
		<meeting>CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09-20">17-20 September, 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,74.70,643.55,443.96,8.96;16,70.94,655.10,418.34,8.96;16,70.94,666.62,119.78,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="16,81.02,643.55,142.20,8.96">Cross-Language Answer Validation</title>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Zhikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laura</forename><surname>Tolosi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Petya</forename><surname>Osenova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kiril</forename><surname>Simov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Georgi</forename><surname>Georgiev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,123.86,655.10,335.35,8.96">Proceedings of CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers</title>
		<meeting>CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09-20">17-20 September, 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,74.70,73.12,420.37,8.96" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="17,81.02,73.12,410.12,8.96">IDRAAQ: New Arabic Question Answering System Based on Query Expansion and Passage Retrieval</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="17,70.94,84.64,435.57,8.96;17,70.94,96.06,291.15,8.96" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Lahsen</forename><surname>Abouenour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karim</forename><surname>Bouzoubaa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<title level="m" coord="17,312.89,84.64,193.62,8.96;17,70.94,96.06,138.75,8.96">Proceedings of CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers</title>
		<meeting>CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09-20">17-20 September, 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,74.70,119.10,428.05,8.96;17,70.94,130.62,429.74,8.96;17,70.94,142.14,177.23,8.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="17,81.02,119.10,421.73,8.96;17,70.94,130.62,103.91,8.96">Question Answering Approach to the Multiple Choice QA4MRE Challenge. José Saias and Paulo Quaresma</title>
	</analytic>
	<monogr>
		<title level="m" coord="17,193.10,130.62,307.58,8.96;17,70.94,142.14,25.05,8.96">Proceedings of CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers</title>
		<meeting>CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09">2012. 17-20 September, 2012</date>
		</imprint>
	</monogr>
	<note>DI@UE in CLEF</note>
</biblStruct>

<biblStruct coords="17,74.70,165.06,408.37,8.96" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="17,81.02,165.06,397.87,8.96">Enhancing a Question Answering System with Textual Entailment for Machine Reading Evaluation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="17,485.62,165.06,27.75,8.96;17,70.94,176.48,446.69,9.06;17,70.94,188.10,448.81,8.96;17,70.94,199.62,47.17,8.96" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandru-Lucian</forename><surname>Gînscă</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Mihai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diana</forename><surname>Moruz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maria</forename><surname>Trandabat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emanuela</forename><surname>Husarciuc</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Boroș</surname></persName>
		</author>
		<title level="m" coord="17,81.74,188.10,335.21,8.96">Proceedings of CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers</title>
		<meeting>CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09-20">17-20 September, 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,74.70,222.66,446.96,8.96;17,70.94,234.06,423.26,8.96;17,70.94,245.58,177.23,8.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="17,81.02,222.66,209.71,8.96">Bulgarian Question Answering for Machine Reading</title>
		<author>
			<persName coords=""><forename type="first">Kiril</forename><surname>Simov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Petya</forename><surname>Osenova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Georgi</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Zhikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laura</forename><surname>Tolosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,186.50,234.06,307.70,8.96;17,70.94,245.58,25.05,8.96">Proceedings of CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers</title>
		<meeting>CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09-20">17-20 September, 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,74.70,268.62,180.80,8.96" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="17,80.90,268.62,149.23,8.96">The LogAnswer Project at QA4MRE</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,258.05,268.62,249.98,8.96;17,70.94,280.17,375.85,8.96" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Ingo</forename><surname>Glöckner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Björn</forename><surname>Pelzer</surname></persName>
		</author>
		<title level="m" coord="17,399.07,268.62,108.96,8.96;17,70.94,280.17,223.49,8.96">Proceedings of CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers</title>
		<meeting>CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09-20">17-20 September, 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,79.31,303.09,441.05,8.96;17,70.94,314.61,437.90,8.96;17,70.94,326.13,47.17,8.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="17,86.06,303.09,237.85,8.96">An Entailment-Based Approach to the QA4MRE Challenge</title>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Phil</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,70.94,314.61,335.11,8.96">Proceedings of CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers</title>
		<meeting>CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09-20">17-20 September, 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,79.32,349.17,389.80,8.96" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="17,86.06,349.17,378.89,8.96">Arabic QA4MRE at CLEF 2012: Arabic Question Answering for Machine Reading Evaluation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="17,471.70,349.17,52.73,8.96;17,70.94,360.57,438.06,8.96;17,70.94,372.09,375.85,8.96" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Omar</forename><surname>Trigui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lamia</forename><surname>Hadrich Belguith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hichem</forename><surname>Ben Amor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bilel</forename><surname>Gafsaoui</surname></persName>
		</author>
		<title level="m" coord="17,399.79,360.57,109.21,8.96;17,70.94,372.09,223.49,8.96">Proceedings of CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers</title>
		<meeting>CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09-20">17-20 September, 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,79.31,395.13,237.51,8.96" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="17,86.06,395.13,205.52,8.96">Question Answering System for QA4MRE@CLEF</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,319.37,395.13,202.57,8.96;17,70.94,406.65,445.68,8.96;17,70.94,418.17,330.15,8.96" xml:id="b16">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Pinaki</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Partha</forename><surname>Pakray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Somnath</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samadrita</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<title level="m" coord="17,361.63,406.65,155.00,8.96;17,70.94,418.17,177.63,8.96">Proceedings of CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers</title>
		<meeting>CLEF 2012 Evaluation Labs and Workshop -Working Notes Papers<address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09-20">17-20 September, 2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
