<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,159.36,115.82,296.54,12.93;1,196.08,133.82,223.24,12.93;1,237.72,151.70,139.84,12.93">DI@UE in CLEF2012: question answering approach to the multiple choice QA4MRE challenge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,238.44,189.19,44.00,9.96"><forename type="first">José</forename><surname>Saias</surname></persName>
							<email>jsaias@uevora.pt</email>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Informática</orgName>
								<orgName type="institution">ECT Universidade de Évora</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,305.04,189.19,72.06,9.96"><forename type="first">Paulo</forename><surname>Quaresma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Departamento de Informática</orgName>
								<orgName type="institution">ECT Universidade de Évora</orgName>
								<address>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,159.36,115.82,296.54,12.93;1,196.08,133.82,223.24,12.93;1,237.72,151.70,139.84,12.93">DI@UE in CLEF2012: question answering approach to the multiple choice QA4MRE challenge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9D25EBB0CD5C2B7756723513A8430682</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the 2012 edition of CLEF, the DI@UE team has signed up for Question Answering for Machine Reading Evaluation (QA4MRE) main task. For each question, our system tries to guess which of the five hypotheses is the more plausible response, taking into account the reading test content and the documents from the background collection on the question topic. For each question, the system applies Named Entity Recognition, Question Classification, Document and Passage Retrieval. The criteria used in the first run is to choose the answer with the smallest distance between question and answer key elements. The system applies a specific treatment for certain factual questions, with the categories Quantity, When, Where, What, and Who, whose responses are usually short and likely to be detected in the text. For the second run, the system tries to solve each question according to its category. Textual patterns used for answer validation and Web answer projection are defined according to the question category. The system answered to all 160 questions, having found 50 right candidate answers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the 2012 edition of Cross Language Evaluation Forum (CLEF), the Informatics Department of the University of Évora (DI@UE) team has signed up for Question Answering for Machine Reading Evaluation (QA4MRE)<ref type="foot" coords="1,425.64,574.76,3.97,4.84" target="#foot_0">1</ref> main task. This was the second year that we participated in a CLEF Lab using English as working language for the system. In previous work for QA@CLEF <ref type="bibr" coords="1,442.92,599.47,10.56,9.96" target="#b0">[1,</ref><ref type="bibr" coords="1,455.04,599.47,7.04,9.96" target="#b1">2]</ref>, we focused in Portuguese, but this language was not available in QA4MRE. The objective is to solve questions that are given in the form of multiple choice, each having five options, and only one correct answer <ref type="bibr" coords="1,373.32,635.35,9.99,9.96" target="#b2">[3]</ref>. For the main task of this year there are 160 questions, 40 more than last year <ref type="bibr" coords="2,382.08,118.39,9.99,9.96" target="#b3">[4]</ref>. Background collections have received more texts beyond those that already existed for the previous three topics, and a new topic appeared, with Alzheimer related documents. We kept the approach used in 2011 <ref type="bibr" coords="2,298.56,154.27,9.99,9.96" target="#b4">[5]</ref>, making a few adjustments to certain types of questions. Instead of objectively seek an answer to each question, as we would do in a regular QA process, we focus on assessing answer candidates. The textual justification for each answer, requested in 2011 <ref type="bibr" coords="2,375.12,190.15,9.99,9.96" target="#b5">[6]</ref>, is no longer required in the system results, which allows selection techniques independent from the reference corpus. The system architecture and the employed resources are described in the next section. The third section presents the methodology used for question processing, including some examples. Section 4 lists the results of the system, and the last section is devoted to the analysis of results and considerations on the work done.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Resources and Architecture</head><p>The system maintains the architecture defined for the previous year participation. In Figure <ref type="figure" coords="2,206.28,349.75,4.98,9.96" target="#fig_0">1</ref> we can see the different system modules. The XML Layer receives the input, does the parsing and organization of the questions with their multiple choice answers, while maintaining the connection to the topic to which they relate and to their particular reading test document. When all questions are processed, this component generates the XML output and makes sure the syntax is correct and conforming to the DTD. The Question Classifier module makes a linguistic analysis to the question text to determine its category. The system uses the parser CandC with the Boxer<ref type="foot" coords="2,476.28,432.56,3.97,4.84" target="#foot_1">2</ref> semantic tool, that can produce Discourse Representation Structures (DRS) <ref type="bibr" coords="2,467.28,445.39,9.99,9.96" target="#b6">[7]</ref>. Question type is later considered for assessing each response in a more specific and targeted way. Currently, we focus on factual response categories, such as quantities, dates, names and short descriptions. The Libs Module contains the Background Collections (BC), which include the English version of the four 2012 topics (AIDS; Climate Change; Music and Society; Alzheimer ) documents. This corresponds to almost 3 gigabytes of text. For text retrieval we keep using the Lucene<ref type="foot" coords="2,327.12,528.20,3.97,4.84" target="#foot_2">3</ref> search engine. Wordnet <ref type="bibr" coords="2,441.96,529.15,10.56,9.96" target="#b7">[8]</ref> is the resource used for synonym and hyperonym check, definitions and morphological normalization, consulted through the Java API for WordNet Searching<ref type="foot" coords="2,445.68,552.20,3.97,4.84" target="#foot_3">4</ref> . The Local KB has a starting knowledge base containing common sense facts about places, entities and events. It can assist in Named Entity Recognition (NER) process or compatibility validation of terms for very specific cases not covered by Wordnet.</p><p>The Answer Analyzer is responsible for assess each answer choice for a question. This includes a linguistic analysis of the text of the response and a textual search process. The search can be simple or defined according to the category of question, through textual patterns for answer projection over the BC or over the Web. With the information collected for each candidate answer to a question, the Answer Selector module applies a criteria to choose the most plausible answer. These criteria may be more general, if the question was not classified in any specific category, or more directed and concrete, as we shall see in the next section. For each question, our system tries to guess which of the five hypotheses is more plausible response, taking into account the reading test content and the documents from the background collection on the question topic (Aids, Climate change, Music and Society, and Alzheimer). In this edition, we submitted two runs for QA4MRE evaluation. The first run applies a generic strategy of surface text analysis on the reading test and documents retrieved from background collections, with no other external resources. For each question, the process begins with Named Entity Recognition, for prior identification of any entity names, dates, quantities or other expressions that influence question interpretation. Then, processing continues with question classification, document retrieval and passage retrieval. The system performs a search for documents in the BC that can support one of the possible answers to the question. Each candidate answer has a set of retrieved passages, which are text segments. For each of the multiple choices we intend to verify if both question and answer key elements are present in the text segments, and what is the distance between them. The criteria used in the first run is to choose the answer with the smallest distance between question and answer key elements. The question key element to find in the text segments is the question focus, the entity or object that the question refers to. In both cases, the stop words are filtered from key elements. Because the textual justification was no longer needed, we decided to answer all questions using the broader heuristic whenever a specialized strategy was not applicable. Questions with different nature are processed differently. The system applies a specific treatment for certain factual questions, whose class and focus are identified. Such specialized strategy is applied to factoid questions with the categories Quantity, When, Where, What, and Who, whose responses are usually short and likely to be detected in the text. Question classification requires a question text analysis, in particular the identification of the interrogative term and the question focus. As an example, the question 'How many degrees did Burney receive from Oxford? ' has Quantity category. Its focus is Burney, and what we need about it is the number of degrees received from Oxford. In another example, the interrogative used in question 'Where was Burney working when he first conceived the idea of writing a music history? ' determines its classification in the type Where, guiding further processing to a location. The use of textual patterns is very common in question answering mechanisms, as in <ref type="bibr" coords="4,157.68,477.07,9.99,9.96" target="#b8">[9]</ref>, <ref type="bibr" coords="4,173.76,477.07,15.60,9.96" target="#b9">[10]</ref> or <ref type="bibr" coords="4,203.88,477.07,14.69,9.96" target="#b10">[11]</ref>. In previous QA work, we used patterns in Senso system <ref type="bibr" coords="4,467.28,477.07,9.99,9.96" target="#b1">[2]</ref>, but tuned to the Portuguese language. Answer scenarios explored in that system patterns are not directly applicable to English sentence structure. Therefore, since 2011 we have been adjusting the textual patterns to identify possible factual responses. For the second run, the system tries to solve each question according to its category, adopting a specialized strategy of resolution. The textual patterns used for answer validation are defined according to the category, representing common cases for that type of question. The system checks the presence of question and answer key elements on a text segment based on term exact match as done for the first run, firstly, but also through semantic compatibility (synonym, hyperonym, base form). This is a semantic query expansion of the search terms. Without the need to associate with each answer a supporting text, it becomes possible to use also background collections independent heuristics, such as Web answer projection to validate the options for the question. This technique is only used when the question is classified as one of the factoid categories supported by the system. An example where the Web answer projection helped to identify the correct choice is 'What is the population of Brazil? '. The correct answer appears on the reading test, but away from the question focus terms. Projecting the answer choices on the web, the answer 180 millions emerged as the right one. The decision to choose the answer to a question is based on the following:</p><p>1. The criteria used in the first run is to choose the answer with the smallest distance between question and answer key elements. 2. The second run aims for a more informed choice process. If the system finds some question category answer pattern in a retrieved document, or on the Web, for a single answer option, then that option is the chosen. Even if other options have results for the general text surface search, the answer patterns under the question category have priority. 3. If there is a tie, in any case, the choice falls on the option having more support text segments, which are the retrieved text passages where answer patterns or key search terms were found. 4. If the surface term search, for the general case, or the answer pattern search does not return any results, for any of the five hypotheses, then the question remains unanswered.</p><p>In the following section we discriminate the results obtained in each run submitted to QA4MRE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>In the first approach, the system answered to 156 questions using the surface based technique. Figure <ref type="figure" coords="5,241.92,453.79,4.98,9.96">2</ref> illustrates the proportion between the amount unanswered questions, and also the correct and the mistaken results. The system gave 45 correct answers and the remaining 111 were wrong. The questions referred Fig. <ref type="figure" coords="5,232.44,655.02,4.11,8.52">2</ref>. Evaluation at QA level for the first run Fig. <ref type="figure" coords="6,227.16,261.18,4.11,8.52">3</ref>. Evaluation at QA level for the second run to in the previous section have been answered, but the choice was not right<ref type="foot" coords="6,464.52,296.12,3.97,4.84" target="#foot_4">5</ref> in any of them.</p><p>In the first run 4 questions were unanswered due to an unexpected parse error. That problem was fixed for the second run. In this last run, which is the most complete, the system changed the response in 20 of the 156 questions previously answered with the surface text search approach.</p><p>The system kept the answer to both Burney questions mentioned in section 3, keeping the erroneous choice. But for 'Which pupil of Dr John Blow taught Charles Burney? ', a What class question, the system changed from a wrong answer to the right answer (Edmund Baker ), in the second run.</p><p>In other examples, such as 'How many countries have acted effectively against AIDS? ' and 'Where is the epicenter of the AIDS pandemic? ', were answered correctly in both runs.</p><p>The chart in Figure <ref type="figure" coords="6,230.64,452.35,4.98,9.96">3</ref> summarizes the evaluation results for our system. The number of hits increased to 50. At the same time, 4 more questions have been processed, leaving no question unanswered, and the number of wrong answers was even reduced by one.</p><p>Of the four unanswered questions in the first run, three were answered incorrectly. Only the first of these, 'Name two styles which have contributed to pop music:', got the right answer on the second run.</p><p>Table <ref type="table" coords="6,161.76,536.11,4.98,9.96">1</ref> shows a more detailed assessment with the breakdown of values for each run. The system first attempts resulted in a 0.28 accuracy and 0.29 C@1 values. C@1 is a balanced measure rewarding systems that, for the same number of correct answers, decrease the number of incorrect results by leaving some questions unanswered <ref type="bibr" coords="6,188.76,583.87,14.69,9.96" target="#b11">[12]</ref>. The accuracy rose to 0.31 in the second run, and the overall C@1 measure was also 0.31. Table <ref type="table" coords="6,163.20,619.75,4.98,9.96" target="#tab_0">2</ref> shows a comparison between the result of this year's better run and the best result achieved by our system in 2011, where there were 120 questions for processing. The last section has some thoughts on these results and on our participation in this Lab. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In 2011, our system answered to 73 of 120 questions, finding correct 18 answers. In this edition, our system answered correctly to 50 questions out of 160. This represents a substantial improvement in accuracy, from 0.15 to 0.31. Compared with the previous year, the question classifier has improved, being more effective in assigning the category of factoid questions. We believe that this update in the question classifier was the key to improving outcomes, especially for allowing the application of specific procedures for each category of question. This year we also improved the text analysis performed on the question and answer hypotheses, using the CandC and Boxer tools.</p><p>Looking at the charts in figures 2 and 3, the wrong answers slice is significantly higher. Answer all questions may not have been a good decision. In future we can introduce a confidence factor or more appropriate criteria to decide between responding and non-responding. With such a procedure, the system can improve the C@1 measure. Errors in question classification not always determine a wrong answer. Question 'What is the external debt of all African countries? ' asks for a monetary value and should have been classified as Quantity. The system classified it as a What question, whose resolution process is not optimized for numeric values. Yet, both the surface search and the answer projection succeed, and the option chosen by the system was the correct in both runs. Despite the improved accuracy results, we consider that the number of wrong answers is very high. This may reflect a need to use more semantic based techniques, and perhaps to apply an intensive linguistic analysis to BC documents. This second participation in QA4MRE helped us to adjust our system to English, focusing primarily on factual answer questions and specific categories, but with an alternative methodology for the general case.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,234.96,525.10,145.36,8.97;3,150.82,245.20,313.42,255.67"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. System Architecture in 2012</figDesc><graphic coords="3,150.82,245.20,313.42,255.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,162.84,173.86,289.70,186.09"><head>Table 2 .</head><label>2</label><figDesc>Comparison with the results of previous participation</figDesc><table coords="7,162.84,173.86,289.70,165.33"><row><cell cols="2">unanswered</cell><cell cols="2">answered</cell><cell></cell><cell></cell><cell>all</cell><cell></cell></row><row><cell>Run</cell><cell>#</cell><cell cols="6"># Right Wrong # Accuracy C@1</cell></row><row><cell>01</cell><cell>4</cell><cell>156</cell><cell>45</cell><cell>111</cell><cell>160</cell><cell>0.28</cell><cell>0.29</cell></row><row><cell>02</cell><cell>0</cell><cell>160</cell><cell>50</cell><cell>110</cell><cell>160</cell><cell>0.31</cell><cell>0.31</cell></row><row><cell></cell><cell cols="5">Table 1. Detailed evaluation</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">unanswered</cell><cell cols="2">answered</cell><cell></cell><cell>all</cell><cell></cell></row><row><cell>Year/Run</cell><cell>#</cell><cell cols="6"># Right Wrong # Accuracy C@1</cell></row><row><cell>2011 best</cell><cell>47</cell><cell>73</cell><cell>18</cell><cell>55</cell><cell>120</cell><cell>0.15</cell><cell>0.21</cell></row><row><cell>2012 best</cell><cell>0</cell><cell>160</cell><cell>50</cell><cell>110</cell><cell>160</cell><cell>0.31</cell><cell>0.31</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.72,656.26,123.76,8.97"><p>http://celct.fbk.eu/QA4MRE/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,144.72,623.38,213.36,8.97"><p>http://svn.ask.it.usyd.edu.au/trac/candc/wiki/boxer</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,144.72,634.42,335.80,8.97;2,144.72,645.33,140.92,8.97"><p>Apache Lucene is an open source project with advanced indexing and searching features. http://lucene.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,144.72,656.26,321.03,8.97"><p>Java API for WordNet Searching: http://lyle.smu.edu/˜tspell/jaws/index.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="6,144.72,656.26,316.96,8.97"><p>According to the solutions disclosed in July 2012: QA4MRE-2012-EN GS.xml.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,138.32,257.61,342.23,8.97;8,142.32,268.66,338.06,8.97;8,142.32,279.58,322.36,8.97" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,286.32,257.61,194.23,8.97;8,142.32,268.66,82.76,8.97">The senso question answering approach to portuguese qa@clef-2007</title>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Saias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><surname>Quaresma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,303.60,268.66,176.78,8.97;8,142.32,279.58,113.83,8.97">CLEF 2007 Working Notes, Cross-Language Evaluation Forum Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="8,138.32,290.49,342.22,8.97;8,142.32,299.26,338.23,11.25;8,142.32,312.46,288.04,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,284.04,290.49,196.50,8.97;8,142.32,301.53,16.64,8.97">The senso question answering system at qa@clef 2008</title>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Saias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><surname>Quaresma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,324.97,299.26,155.58,11.25;8,142.32,312.46,162.87,8.97">Multiple Language Question Answering @ Cross-Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>Universidade de Évora</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="8,138.32,323.37,304.28,8.97" xml:id="b2">
	<monogr>
		<ptr target="http://celct.fbk.eu/QA4MRE/" />
		<title level="m" coord="8,146.88,323.37,163.73,8.97">Track Guidelines</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>QA4MRE@CLEF</note>
</biblStruct>

<biblStruct coords="8,138.32,332.13,342.23,11.25;8,142.32,345.33,217.08,8.97;8,379.80,345.64,100.80,8.26;8,142.32,356.56,338.21,8.26;8,142.32,367.29,308.10,8.97" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="8,379.80,345.64,100.80,8.26;8,142.32,356.56,334.06,8.26">Overview of QA4MRE at CLEF 2011: Question Answering for Machine Reading Evaluation</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Álvaro</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">E</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Sutcliffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Forascu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sporleder</surname></persName>
		</author>
		<ptr target="http://clef2011.org/resources/proceedings/OverviewQA4MREClef2011.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.32,378.22,342.32,8.97;8,142.32,389.14,338.19,8.97;8,142.32,400.18,338.35,8.97;8,142.32,411.10,153.51,8.97" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,283.44,378.22,197.20,8.97;8,142.32,389.14,100.41,8.97">The di@ue&apos;s participation in qa4mre: from qa to multiple choice challenge</title>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Saias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paulo</forename><surname>Quaresma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,174.72,400.48,197.55,8.26">CLEF 2011 Labs and Workshop: Notebook Papers</title>
		<editor>
			<persName><forename type="first">Vivien</forename><surname>Petras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pamela</forename><surname>Forner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.32,422.02,304.28,8.97" xml:id="b5">
	<monogr>
		<ptr target="http://celct.fbk.eu/QA4MRE/" />
		<title level="m" coord="8,146.88,422.02,163.73,8.97">Track Guidelines</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>QA4MRE@CLEF</note>
</biblStruct>

<biblStruct coords="8,138.32,433.06,337.70,8.97" xml:id="b6">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Hans</forename><surname>Kamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Uwe</forename><surname>Reyle</surname></persName>
		</author>
		<title level="m" coord="8,265.32,433.36,96.83,8.26">From Discourse to Logic</title>
		<meeting><address><addrLine>Dordrecht</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.32,443.98,342.30,8.97;8,142.32,454.90,52.78,8.97" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,221.04,444.28,157.92,8.26">Wordnet: A lexical database for English</title>
		<author>
			<persName coords=""><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,387.00,443.98,93.62,8.97;8,142.32,454.90,18.21,8.97">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,138.32,465.81,342.31,8.97;8,142.32,476.86,306.58,8.97" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="8,279.36,465.81,201.27,8.97;8,142.32,477.16,159.40,8.26">Soubbotin Use of Patterns for Detection of Likely Answer Strings: A Systematic Approach</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><forename type="middle">M</forename><surname>Soubbotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergei</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,310.56,476.86,105.47,8.97">Text REtrieval Conference</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.54,487.78,338.07,8.97;8,142.32,498.69,275.14,8.97" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="8,201.36,488.08,206.66,8.26">Automated email answering by text pattern matching</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sneiders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="8,415.32,487.78,65.30,8.97;8,142.32,498.69,107.91,8.97">IceTAL, Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">6233</biblScope>
			<biblScope unit="page" from="381" to="392" />
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,142.54,509.73,337.99,8.97;8,142.32,520.66,338.23,8.97;8,142.32,531.57,108.22,8.97" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="8,349.08,510.04,131.45,8.26;8,142.32,520.96,149.81,8.26">An alignment-based surface pattern for a question answering system</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">L</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">C</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">L</forename><surname>Hsu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>IEEE Systems, Man, and Cybernetics Society</publisher>
			<biblScope unit="page" from="172" to="177" />
		</imprint>
	</monogr>
	<note type="report_type">IRI</note>
</biblStruct>

<biblStruct coords="8,142.54,542.61,337.63,8.97;8,142.32,553.53,338.23,8.97;8,142.32,564.45,338.32,8.97;8,142.32,575.49,74.32,8.97" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,305.16,542.92,170.71,8.26">A Simple Measure to Assess Non-response</title>
		<author>
			<persName coords=""><forename type="first">Anselmo</forename><surname>Peñas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alvaro</forename><surname>Rodrigo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,142.32,553.53,338.23,8.97;8,142.32,564.45,165.31,8.97">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1415" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
