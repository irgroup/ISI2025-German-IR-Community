<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,126.50,152.67,342.30,12.64;1,222.05,170.67,151.33,12.64">DAEDALUS at RepLab 2012: Polarity Classification and Filtering on Twitter Data</title>
				<funder ref="#_cNfyRxd #_2qEu9yv #_DqbF9r4 #_P4PFCuh">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,154.70,210.18,83.59,8.96"><forename type="first">Julio</forename><surname>Villena-Román</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Carlos III de Madrid</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,251.45,210.18,74.55,8.96"><forename type="first">Sara</forename><surname>Lana-Serrano</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universidad Politécnica de Madrid</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,339.29,210.18,95.62,8.96"><forename type="first">Cristina</forename><surname>Moreno-García</surname></persName>
						</author>
						<author>
							<persName coords="1,182.66,222.06,87.46,8.96"><forename type="first">Janine</forename><surname>García-Morera</surname></persName>
						</author>
						<author>
							<persName coords="1,278.45,222.06,125.98,8.96"><forename type="first">José</forename><forename type="middle">Carlos</forename><surname>González-Cristóbal</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universidad Politécnica de Madrid</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,207.17,246.11,112.92,8.10"><roleName>Decisions</roleName><surname>Daedalus -Data</surname></persName>
						</author>
						<author>
							<persName coords="1,337.68,246.11,51.73,8.10"><forename type="first">S</forename><forename type="middle">A</forename><surname>Language</surname></persName>
						</author>
						<title level="a" type="main" coord="1,126.50,152.67,342.30,12.64;1,222.05,170.67,151.33,12.64">DAEDALUS at RepLab 2012: Polarity Classification and Filtering on Twitter Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">60E3A3FAA8491032DC65687310B45B42</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>RepLab</term>
					<term>CLEF</term>
					<term>reputation analysis</term>
					<term>profiling scenario</term>
					<term>filtering</term>
					<term>polarity classification</term>
					<term>sentiment analysis</term>
					<term>STILUS</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation at the RepLab 2012 profiling scenario, in both polarity classification and filtering subtasks. Our approach is based on 1) the information provided by a semantic model that includes rules and resources annotated for sentiment analysis, 2) a detailed morphosyntactic analysis of the input text that allows to lemmatize and divide the text into segments to be able to control the scope of semantic units and perform a finegrained detection of negation in clauses, and 3) the use of an aggregation algorithm to calculate the global polarity value of the text based on the local polarity values of the different segments, which includes an outlier filter. The system, experiments and results are presented and discussed in the paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>According to Merriam-Webster dictionary <ref type="foot" coords="1,304.97,529.01,3.00,5.40" target="#foot_0">1</ref> , reputation is the overall quality or character of a given person or organization as seen or judged by people in general, or, in other words, the general recognition by other people of some characteristics or abilities for a given entity. In turn, reputation analysis is the process of tracking, investigating and reporting an entity's actions and other entities' opinions about those actions. It covers many factors to calculate the market value of reputation. Reputation analysis has come into wide use as a major factor of competitiveness in the increasingly complex marketplace of personal and business relationships among people and companies. From the technology perspective, the first step towards the automatic reputation analysis is a sentiment analysis, i.e., the application of natural language processing and text analytics to identify and extract subjective information from texts about the sentiments, emotions or opinions contained.</p><p>Reputation analysis is a major technological challenge. The task is so hard that even humans often disagree on the sentiment of a given text. The fact that issues that one individual finds acceptable or relevant may not be the same to others, along with multilingual aspects, cultural factors and different contexts make it very hard to classify a text written in a natural language into a positive or negative sentiment. And the shorter the text is, for example, when analyzing Twitter messages or short comments in Facebook, the harder the task becomes.</p><p>RepLab <ref type="bibr" coords="2,170.30,234.18,11.72,8.96" target="#b0">[1]</ref> is a competitive evaluation exercise for reputation analysis, launched in 2012 edition of CLEF campaign, which focuses on two scenarios: profiling and monitoring scenario. For both scenarios, systems are provided with a set of tweets in Spanish and English related to several companies. The profiling scenario must annotate two kinds of information in those tweets: 1) filtering information, i.e., whether the tweets are related or not to the company, and 2) polarity classification of the tweet, i.e., if the tweet content has positive or negative implications for the company's reputation. The monitoring scenario consists of clustering a given stream of tweets, assigning relative priorities.</p><p>This paper describes our participation at the RepLab 2012 profiling scenario, in both polarity classification and filtering subtasks. We are a research group led by DAEDALUS <ref type="foot" coords="2,178.70,365.85,3.00,5.45" target="#foot_1">2</ref> , a leading provider of language-based solutions in Spain, and research groups of Universidad Politécnica and Universidad Carlos III of Madrid. We are longtime participants in CLEF, in many different tracks and tasks since 2003.</p><p>RepLab is a new task within CLEF. There was a related task in NTCIR three years ago called Multilingual Opinion Analysis Task <ref type="bibr" coords="2,315.41,414.21,10.69,8.96" target="#b1">[2]</ref>, active for two editions, focused on sentiment analysis. Another somewhat related task in CLEF was Web People Search <ref type="bibr" coords="2,124.82,438.21,10.69,8.96" target="#b2">[3]</ref>, focusing on the problem of ambiguity for organization names and the relevance of web data for reputation management purposes. We took part in both initiatives as participant research groups <ref type="bibr" coords="2,235.49,462.23,11.72,8.96" target="#b3">[4]</ref>  <ref type="bibr" coords="2,249.75,462.23,10.71,8.96" target="#b4">[5]</ref>.</p><p>Our approach to the polarity classification is based on 1) the information provided by a semantic model that includes rules and resources (polarity units, modifiers, stopwords) annotated for sentiment analysis, 2) a detailed morphosyntactic analysis of the input text that allows to lemmatize and split the text into segments in order to be able to control the scope of semantic units and perform a fine-grained detection of negation in clauses, and 3) the use of an aggregation algorithm to calculate the global polarity value of the text based on the local polarity values of the different segments, which includes an outlier detection. Our system, experiments and results achieved are presented and discussed in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Profiling Scenario</head><p>Reputation analysis is becoming a promising topic in the field of marketing and customer relationship management, as the social media and its associated word-ofmouth effect is turning out to be the most important source of information for companies and their customers' sentiments towards their brands and products. And this creates new market opportunities for the linguistic technology industry. Thus the main goal behind our participation was to evaluate, in a multilingual scenario and using social media data, the software and resources for sentiment analysis and named entity detection that have been developed by our company in the last year.</p><p>This year we focused on the profiling scenario, which includes two subtasks: polarity classification and filtering. The following sections give more in-depth details about our work in both subtasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.1</head><p>Polarity Classification Subtask</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Overview</head><p>The polarity classification is based on our software for multilingual sentiment analysis [6], which is available through a web API offered through a REST-based web service. This component performs an in-depth analysis of the input text to determine if it expresses a positive/negative/neutral sentiment or else no sentiment at all. First the local polarity of the different clauses in the text ("segments") is identified and then the relation among them is evaluated in order to obtain a global polarity value for the whole given text. The output for both the local and global polarity is encoded with a real number ranging from -1 (strong negative) to +1 (strong positive) and also a set of labels representing 5 discrete levels to simplify the post-processing: strong positive (P+), positive (P), neutral (NEU), negative (N), strong negative (N+), and one additional no-sentiment tag (NONE).</p><p>Apart from the text itself, which can be encoded in plain text, HTML or XML, another required input parameter is the semantic model to use in the sentiment evaluation. This semantic model defines the domain of the text (the analysis scenario) and is mainly based on an extensive set of dictionaries and rules that incorporate both the well-known "domain-independent" polarity values (for instance, in general, in all contexts, "good" is positive and "awful" is negative) and also the specificities of each analysis scenario (for instance, an "increase" in the "interest rate" is probably positive for financial companies but negative for the rest of the people). The semantic model also encodes implicitly the language of text.</p><p>Furthermore, the component is able to identify named entities and concepts, referred to as attributes, and assign a specific polarity value to them, depending on the selected semantic model and the context in which the attributes appear. In this case, this information has been used for the second subtask (identifying whether tweets are related or not to the companies).</p><p>The component makes an internal call to another software component [7], also accessible through a REST-based web service, in order to split the text into segments, perform the POS tagging and the extraction of their morphosyntactic structure to be used in the sentiment analysis, and identify the named entities and concepts.</p><p>The sentiment analysis process is described in detail in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Sentiment Analysis Process</head><p>The sentiment analysis is carried out in the following sequence of steps:</p><p>1. Segment detection. The text is parsed and split into segments. Although most times these segments are full sentences in "usual" texts (well-written news articles, blog posts, etc.), this is not the case in tweet messages, as the analysis depends on the presence of punctuation marks and correct capitalization of words. Figure <ref type="figure" coords="4,200.81,244.26,4.98,8.96">1</ref> and Figure <ref type="figure" coords="4,253.85,244.26,4.98,8.96" target="#fig_0">2</ref> show two examples of tweets in the test set.  2. Linguistic processing (lemmatization, morphosyntactic analysis and entity and concept detection). First each segment is tokenized (considering multiword units) and then each token is analyzed to extract its lemma(s).</p><p>In addition, a morphosyntactic analysis is performed to divide the segment into proposition or clauses. This division is useful, as described later, for detecting the negation and analyzing the effect of modifiers on the polarity values. Focusing on a given clause, it is assigned a "clause level" equal to 0, and any step into/out a subordinated clause adds/subtracts 1 from that clause level.</p><p>Last but not least, a named entity and concept recognition step is carried out, based in multilingual linguistic resources and heuristics for detecting unknown PERSON, LOCATION and/or ORGANIZATION entities.</p><p>Next Figure <ref type="figure" coords="4,206.21,587.39,4.98,8.96">3</ref> and Figure <ref type="figure" coords="4,261.41,587.39,4.98,8.96">4</ref> show the output of this step corresponding to the previous examples. A visual representation of the syntactic structure is shown in Figure <ref type="figure" coords="5,446.95,225.54,4.98,8.96">9</ref> and Figure <ref type="figure" coords="5,171.38,237.54,10.02,8.96">10</ref> in the Appendix.</p><formula xml:id="formula_0" coords="4,124.82,619.98,336.02,42.71">{ { { RT { @elpais_inter } } : } } { { { republica_arabe_de_egipto|egipto } { cancelar|cancela } { el acuerdo { de { gas } } } { con { estado_de_israel|israel } } } . } { { { el suministro egipcio } { suponer|suponía } { uno|1|un 40% { del { consumo israelí { de { gas_natural } } } } } } }</formula><p>3. Detection of negation. The next step is to iterate over every token of each segment to tag whether the token is affected by negation or not. If a given token is affected by negation, the eventual polarity level is reversed (turns from positive to negative and the other round).</p><p>For this purpose, the semantic model includes a list of negation units (NEG), such as the obvious negation particles (adverbs) such as "no", "ni" (in Spanish) or "not" (and its contracted form without/with the auxiliary verbs), "neither" (in English) but also words or expressions such as "carecer", "dejar de", "bajo ningún concepto" (in Spanish) or "against", "far from", "no room for" (in English).</p><p>Each NEG unit is considered to affect clauses with a relative (to the NEG unit) clause level up to a given threshold (NEGATION_LEVEL) and tokens separated a relative distance up to another threshold (NEGATION_MAXDISTANCE), excluding certain punctuation marks (brackets, quotes, colon and semicolon). For Twitter messages, the level threshold is -1thus a NEG unit affects to its own clause (group level = 0), any subordinate clause (group level &gt; 0) and its parent clause (group level = -1) -, and the maximum distance threshold is 20.</p><p>The information of negation is stored (as true or false) in each token to be used in the next step.</p><p>The previous examples do not include any negation unit, so all tokens are marked as positive. <ref type="bibr" coords="5,124.82,525.59,3.77,8.96" target="#b3">4</ref>. Detection of modifiers. Some special units (MOD units) do not assign a specific polarity value but operate as modifiers of this value, incrementing or decrementing it. MOD units included in the semantic model can be assigned a + (positive), ++ (strong positive), -(negative) or --(strong negative) value. For instance, "if "good" is positive (P), "very good" is be strong positive (P+), thus "very" would be a positive modifier (+); the opposite is the case of "less", which would be a negative modifier (-) ("less good" would be P-). Some other examples of modifiers are "adicional", "ampliación", "principal" (all positive) or "apenas", "medio" (negative) (in Spanish) or "additional", "a lot", "completely" (positive) or "descend", "almost" (negative) (in English).</p><p>Similarly to the negation detection, modifiers are considered to affect clauses with a relative level (MODIFIER_LEVEL) and tokens separated a relative distance (MODIFIER_MAXDISTANCE) up to a defined threshold values. For this task, the level threshold is 0 (only the clause itself and subordinated clauses) and the maximum distance threshold is 5.</p><p>The second previous example includes two positive (+) modifiers, "much" and "more".</p><p>5. Polarity tagging. The next step is to detect polarity units (POL units) in the segments. The POL units in the semantic model can be assigned one of the following values, ranging from the most positive to the most negative: P++, P+, P, P-, P--, N--, N-, N, N+ and N++.</p><p>To help to avoid false positives, the semantic model also includes stopword units (SW units).</p><p>Moreover, POL units can include a context filter, i.e., one or several words or expressions that must appear or not in the segment so that the unit is considered in the sentiment analysis. Obviously, context filters highly depend on the analysis domain. For example, there are many concepts that are positive (P) when increased (such as reputation, employment...) and negative (N) when decreased; this could be represented by the following set of rules (including macros):</p><formula xml:id="formula_1" coords="6,153.14,365.32,306.41,42.68">#INCREASE# increase|increment|grow|growth|gain|rise|go_up|climb #DECREASE# decrease|decrement|reduce|loss|do_down|descent reputation/#INCREASE# P reputation/#DECREASE# N</formula><p>or else, to increase the recall in the case of missing expressions:</p><formula xml:id="formula_2" coords="6,153.14,443.68,118.13,15.58">reputation/#INCREASE# P reputation N</formula><p>The final value for each POL unit is calculated from the polarity value of the POL unit in the semantic model, adding or subtracting the polarity value of the modifier (if the thresholds are fulfilled) and considering the negation (again, if the thresholds are fulfilled).</p><p>The previous examples are tagged as shown in Figure <ref type="figure" coords="6,389.59,522.71,4.98,8.96">5</ref> and Figure <ref type="figure" coords="6,442.75,522.71,3.77,8.96">6</ref>. 6. Segment scoring. To calculate the overall polarity of each segment, an aggregation algorithm is applied to the set of polarity values given by the POL units detected in the segment. The aggregation algorithm performs an outlier filtering to try to reduce the effect of miss detections of NEG, MOD or POL units, based on a threshold over the standard deviation from the average of values. The aggregation algorithm finally calculates the average and the standard deviation of the set of accepted values, which is assigned as the score of the segment.</p><p>In addition to this numeric score, to simplify the post-processing, discrete nominal values are also assigned to each segment: N+ if score &lt; -0.6, N if score &lt; -0.2, NEU (neutral) if score &lt; +0.2, P if score &lt; 0.6 or else P+. If there is no POL unit, the segment is assigned with a polarity value of NONE.</p><p>The standard deviation is an indication of the level of agreement within the segment. With this value, we can differentiate for instance whether a segment has a NEU score (near 0) because all present POL units or modifiers have a neutral sentiment, so the standard deviation is low, or else there are positive and negative units that lead to a low average but a high standard deviation value. The first case would be detected as AGREEMENT (standard deviation &lt; 0.2) and the second as DISAGREEMENT.</p><p>In the previous first example, all segments have one POL unit at maximum, so the segment average has the same value and an AGREEMENT label. The second example contains a segment with two POL units, "neat" and "tidy", which have the same score, so the segment has the same average value and an AGREEMENT label. The other segment has a DISAGREEMENT label because it contains one positive and one negative POL unit. Again, if there is no segment with polarity information (i.e., different from NONE), the text is assigned with a global polarity value of NONE.</p><p>In the first example, the global score has the same value as the only segment that has a sentiment score. In the second example, the global polarity turns to be NEU (neutral) with a DISAGREEMENT between the two segments. 8. Attribute scoring. Additionally, a similar process is applied to the named entities and concepts (the attributes) that have been detected in the segments during the morphosyntactic analysis to calculate their polarity, in this case, considering which POL unit (along with its modifier(s) and possible negation) is affecting each attribute, and using the same aggregation algorithm.</p><p>Figure <ref type="figure" coords="8,164.78,294.21,4.98,8.96" target="#fig_4">7</ref> and Figure <ref type="figure" coords="8,217.85,294.21,4.98,8.96" target="#fig_5">8</ref> show the final output in XML of the sentiment analysis.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Semantic Models</head><p>Currently there are several semantic models available, some of them developed for general-purpose sentiment analysis and some other for specific cases such as the financial, telecommunications and tourism domains. For the RepLab tasks, the general-purpose models for Spanish and English have been used. Those models where initially inspired in the linguistic resources provided by General Inquirer [7] in English, specifically, terms extracted from the "Positive", "Negative", "Strong" and "Weak" categories of Harvard IV-4 dictionary (included in the General Enquirer).</p><p>The following Table <ref type="table" coords="10,220.97,256.26,4.98,8.96" target="#tab_1">1</ref> presents some information about these models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Submissions</head><p>To perform the experiments of the polarity classification subtask, a client was developed for that web service. This client reads each tweet in the test corpus along with the language, makes a call to the web service and parses the response to adapt the returned values to the ones required in the task: P and P+ are "positive", N and N+ are "negative" and the rest (whether NEU or NONE) are tagged as "neutral". Just one submission for the polarity classification subtask was made: "replab2012_polarity_Daedalus_1". Results are discussed in the corresponding section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2</head><p>Filtering Subtask</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Overview</head><p>Our approach to the filtering subtask is to reuse the result of the named entity recognition step in the linguistic previous analysis, which was performed by calling another software component through a REST-based web service <ref type="bibr" coords="11,384.79,225.18,10.69,8.96">[7]</ref>.</p><p>The difficulty of the detection arises from the fact that entities may appear in different forms: for instance, "Banco Santander Central Hispano" may appear as "BSCH", "Banco Santander", "Banco de Santander", "Santander", etc. In addition, once detected, there is the problem of ambiguity, both among different categories and even within the same category: for instance, "Seville" may be the well-known city in Spain, the soccer team, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Named Entity Detection Process</head><p>The software uses the widely-adopted approach based on knowledge, i.e., manually-developed dictionaries and rule sets are used to perform the detection and classification. The main drawback of this approach is the high costs to develop and maintain the resources, as they are highly dependent on language and domain.</p><p>The current multilingual entity dictionaries include over 41 000 persons, 17 000 organizations and 45 000 locations. Apart from these common dictionaries, our software allows to include user dictionaries that are specific for a given domain and complement the common dictionaries.</p><p>In addition, rules apply regular expression patterns to the entities in the dictionaries to generate a set of possible variants in which that entity might occur, for instance:</p><p>(N)ame (S)urname :-Name / Surname / N. Surname / Name S. / N. S.</p><p>Fernando Alonso  Fernando / Alonso / F. Alonso / Fernando A. / F. A.</p><p>(A)aaa (of|the)? (B)bbb(of|the)? (C)ccc (of|the)? (D)ddd :-ABCD Organization of the Petroleum Exporting Countries  OPEC Thus our system allows the advanced recognition of unknown entities that are proposed as suggested entities: for instance, "Mr. Aaaaa Bbbbb" could be a PERSON name, "Bank of Ddddd" an ORGANIZATION, "Eeeee Square" a LOCATION, etc.</p><p>The process is as follows:</p><p>1. Text is segmented into units (words or multiword expressions).</p><p>2. Those units that are contained in any of the entity dictionaries are marked as candidate entities, no matter if they occur in the exact form or in a variant (alias).</p><p>3. If any unit matches more than one candidate entity, an heuristic-based disambiguation is carried out, using for instance the frequency of that unit in the text ("Castro" will be selected as "Fidel Castro" if that name is present in the García-Morera, José Carlos González-Cristóbal text and not "Raul Castro"), the presence of discursive clues (for instance, towards+LOCATION and article+ORGANIZATION: "towards Madrid" is disambiguated as the city and "this Madrid" as the soccer team), disambiguation based on geographical context (depending on the georeferences in the text), etc.</p><p>As a result, entities that appear in the text are returned, along with their class and position in the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Submissions</head><p>For carrying out the filtering task, three different specific dictionaries ("user dictionaries") have been defined, as described in Table <ref type="table" coords="12,355.51,285.21,3.71,8.96">2</ref>. Although it is possible to make those dictionaries language-specific, we mixed entries in both Spanish and English to simplify the processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2. Description of dictionaries.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dictionary</head><p>Contents Dictionary 1  List of entities in the test corpus, along with their well-known variants and aliases extracted from Wikipedia pages.  Products and services from those companies. </p><p>A list of stopwords for some very ambiguous entities (for instance, "BME" also means "Boston Most Elite" and "ING" is the abbreviation for "ingeniero" -engineer-in Spanish). Dictionary 2  The previous dictionary plus variants and aliases extracted from the company web sites.  Email addresses, usernames, hashtags used for those companies in social networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head></head><p>Stopwords now include references to foundations, external activities of the companies as sponsoring sporting events or competitions (to avoid positives, for instance, for "Liga BBVA", "Regata Mapfre", "Ferrari team"). Dictionary 3  Stopwords now include an extensive list of car models (to avoid positives, for instance, for "Chevrolet Camaro" or "VW Golf").</p><p>Similarly to the polarity classification subtask, a client was developed for the web service to perform the experiments. This client again reads each tweet in the test corpus along with the language, makes a call to the web service indicating one of the three different dictionaries at one time, and parses the response. If the expected entity is detected in the text, "yes" is assigned to the tweet and "no" otherwise.</p><p>We submitted three experiments corresponding to each dictionary: "replab2012_related_Daedalus_1", "replab2012_related_Daedalus_2" and "replab2012_related_Daedalus_3". Results are described in the next section. A similar analysis per entity is included in Table <ref type="table" coords="14,333.79,455.51,4.98,8.96">9</ref> in the Appendix. This table may help to improve our semantic model with specific resources for the companies involved.</p><p>Next Table <ref type="table" coords="14,187.34,491.51,4.98,8.96" target="#tab_4">6</ref> shows the results achieved by the top ranked experiments in the filtering subtask. The columns are the same as in previous tables. Again, in general for both languages, our experiments achieve the best results in terms of F-measure of all participants. However, in this case, the performance for English is considerably better for English than for Spanish, which is quite surprising for us. This issue has to be further analyzed.</p><p>In any case, the best result is obtained by the "replab2012_related_Daedalus_2" experiment, the one that includes stopwords to avoid matches for external activities (sponsoring, foundations) but does not include the list of car models. So that means that tweets talking about "Chevrolet Camaro" are considered to refer to "Chevrolet" but "Ferrari Team" does not refer to "Ferrari". This turns to be a bit inconsistent and raises some doubts about the criteria that have been used for the gold standard.</p><p>Results for filtering achieved per sector by our experiments for all languages in general are shown in Table <ref type="table" coords="15,235.73,397.29,3.77,8.96" target="#tab_5">7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future work</head><p>The significant differences in the results for English and Spanish in both tasks show that there is still much to do in both the enlargement of the semantic resources and also the improvement of the linguistic processing (specially the morphosyntactic analysis), in a general domain or may be focusing on different activity sectors. Future work must be oriented to those aspects. However, figures show that, despite of the difficulty of the tasks, results are quite acceptable and somewhat validate the fact that this technology may be already included into an automated workflow process for social media mining.</p><p>Regarding the polarity classification task, we think that possible future editions should consider the inclusion of a no-polarity label, in addition to positive, negative and neutral, to allow to differentiate whether the text has a neutral polarity (neither positive nor negative) or has no polarity at all. Furthermore, the addition of more levels such as strong positive and strong negative could also be interesting for the analysis scenario, although this obviously would increase the difficulty of tasks to a great extend.</p><p>On the other hand, the filtering task has some points of ambiguity and disagreement regarding the consideration of whether a tweet is related or not to a given company for the case of brand names of products or services, or sponsoring activities. We would thank the elaboration of clear guidelines with the annotation criteria in function of the context. Volkswagen 0.3900 0.2900 0.4500 0.3527 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Banking and Insurance</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,200.93,432.14,193.67,8.10"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Example of segment detection (example 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,196.34,671.95,202.54,8.10"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Figure 3. Example of linguistic processing (example 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,204.89,671.11,185.63,8.10"><head>{Figure 5 .Figure 6 .</head><label>56</label><figDesc>Figure 5. Example of polarity tagging (example 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,124.82,634.82,345.67,8.96;7,142.82,646.82,327.34,8.96;7,142.82,658.82,327.33,8.96;7,142.82,670.82,327.65,8.96;7,142.82,682.82,218.03,8.96"><head>7.</head><label></label><figDesc>Global text scoring. The same aggregation algorithm is applied to the local polarity values of each segment to calculate the global polarity value of the text, represented by an average value (both numeric and nominal values) that indicate the actual value and a standard deviation that indicates the level of agreement or disagreement within the different segments of the text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,233.69,683.11,128.17,8.10;8,229.22,315.40,138.36,359.54"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Final output (example 1).</figDesc><graphic coords="8,229.22,315.40,138.36,359.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,233.69,674.23,128.27,8.10;9,203.07,147.40,190.66,518.13"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Final output (example 2).</figDesc><graphic coords="9,203.07,147.40,190.66,518.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,214.85,274.10,165.80,274.88"><head>Table 1 .</head><label>1</label><figDesc>Contents of the semantic models.</figDesc><table coords="10,214.85,298.34,165.80,250.64"><row><cell>Type of unit</cell><cell cols="2">Spanish English</cell></row><row><cell>Negation (NEG units)</cell><cell>59</cell><cell>28</cell></row><row><cell>Modifiers (MOD units)</cell><cell>372</cell><cell>107</cell></row><row><cell>--</cell><cell>5</cell><cell>3</cell></row><row><cell>-</cell><cell>106</cell><cell>12</cell></row><row><cell>+</cell><cell>255</cell><cell>72</cell></row><row><cell>++</cell><cell>6</cell><cell>20</cell></row><row><cell>Polarity (POL units)</cell><cell>3 139</cell><cell>4 226</cell></row><row><cell>N++</cell><cell>10</cell><cell>78</cell></row><row><cell>N+</cell><cell>340</cell><cell>285</cell></row><row><cell>N</cell><cell>1 309</cell><cell>2 106</cell></row><row><cell>N-</cell><cell>206</cell><cell>209</cell></row><row><cell>N--</cell><cell>11</cell><cell>10</cell></row><row><cell>P--</cell><cell>15</cell><cell>6</cell></row><row><cell>P-</cell><cell>15</cell><cell>72</cell></row><row><cell>P</cell><cell>978</cell><cell>1 113</cell></row><row><cell>P+</cell><cell>248</cell><cell>325</cell></row><row><cell>P++</cell><cell>7</cell><cell>22</cell></row><row><cell>Stopwords (SW units)</cell><cell>91</cell><cell>33</cell></row><row><cell>Macros</cell><cell>27</cell><cell>10</cell></row><row><cell>TOTAL</cell><cell>3 688</cell><cell>4 404</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="14,124.82,149.99,345.64,196.74"><head>Table 4 .</head><label>4</label><figDesc>Polarity classification results per activity sector (all languages).Entities that have been marked with "no samples" in the "Sensitivity over Polarity" column of the result spreadsheet, listed in Table5, are not included in the calculations.</figDesc><table coords="14,183.14,173.87,229.11,124.29"><row><cell>Activity Sector</cell><cell>A</cell><cell>R</cell><cell>S</cell><cell>F(R,S)</cell></row><row><cell>Audiovisual</cell><cell cols="4">0.5900 0.4300 0.4900 0.4580</cell></row><row><cell>Automotive</cell><cell cols="4">0.4020 0.3300 0.3920 0.3530</cell></row><row><cell>Banking and Insurance</cell><cell cols="4">0.4000 0.4500 0.5483 0.4909</cell></row><row><cell>Energy</cell><cell cols="4">0.4780 0.4380 0.4060 0.4133</cell></row><row><cell>Personal care</cell><cell cols="4">0.4833 0.3750 0.3900 0.3619</cell></row><row><cell>Technology and Software</cell><cell cols="4">0.5000 0.3160 0.5000 0.3572</cell></row><row><cell>Telecommunications</cell><cell cols="4">0.5300 0.4200 0.4300 0.4249</cell></row><row><cell>Textile</cell><cell cols="4">0.5000 0.4500 0.5300 0.4867</cell></row><row><cell cols="5">Transport and Infrastructure 0.7300 0.4200 0.1300 0.1985</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="14,142.46,355.49,310.44,84.51"><head>Table 5 .</head><label>5</label><figDesc>Entities marked with "no samples" in the "Sensitivity over Polarity" column.</figDesc><table coords="14,164.66,379.46,265.81,60.54"><row><cell>Entity</cell><cell>Entity Name</cell><cell>Activity Sector</cell></row><row><cell cols="2">RL2012E12 Indra Sistemas, S. A.</cell><cell>Technology and Software</cell></row><row><cell cols="2">RL2012E15 ING Group</cell><cell>Banking and Insurance</cell></row><row><cell cols="3">RL2012E16 Bolsas y Mercados Españoles Banking and Insurance</cell></row><row><cell cols="2">RL2012E32 Wilkinson Sword</cell><cell>Personal care</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="14,168.14,521.32,259.11,162.21"><head>Table 6 .</head><label>6</label><figDesc>Filtering results.</figDesc><table coords="14,315.77,545.20,111.48,8.10"><row><cell>A</cell><cell>R</cell><cell>S</cell><cell>F(R,S)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="15,190.46,415.10,229.35,31.86"><head>Table 7 .</head><label>7</label><figDesc>Filtering results per activity sector (all languages). Again, entities that have been marked with "no samples" in the "Sensitivity over Filtering" column of the result spreadsheet, listed in Table8, are not included in the calculations.</figDesc><table coords="15,205.37,438.86,214.44,8.10"><row><cell>Activity Sector</cell><cell>A</cell><cell>R</cell><cell>S</cell><cell>F(R,S)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="16,141.14,320.21,312.79,134.79"><head>Table 8 .</head><label>8</label><figDesc>Entities marked with "no samples" in the "Sensitivity over Filtering" column.</figDesc><table coords="16,152.18,344.66,290.89,110.34"><row><cell>Entity</cell><cell>Entity Name</cell><cell>Activity Sector</cell></row><row><cell cols="3">RL2012E08 Banco Bilbao Vizcaya Argentaria, S.A. Banking and Insurance</cell></row><row><cell cols="2">RL2012E16 Bolsas y Mercados Españoles</cell><cell>Banking and Insurance</cell></row><row><cell>RL2012E17 Bankia</cell><cell></cell><cell>Banking and Insurance</cell></row><row><cell>RL2012E18 Iberdrola</cell><cell></cell><cell>Energy</cell></row><row><cell cols="2">RL2012E20 Mediaset S.p.A.</cell><cell>Audiovisual</cell></row><row><cell cols="2">RL2012E22 Industria de Diseño Textil, S.A.</cell><cell>Textile</cell></row><row><cell cols="2">RL2012E24 Bank of America Corporation</cell><cell>Banking and Insurance</cell></row><row><cell>RL2012E36 CaixaBank</cell><cell></cell><cell>Banking and Insurance</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="16,124.82,471.23,345.81,32.96"><head>Table 7</head><label>7</label><figDesc>and the same analysis per entity included in Table10in the Appendix again give insights of the sectors that are best covered by our resources and indicate the areas where to invest further efforts.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="19,157.82,176.63,279.44,513.14"><head>Table 10 .</head><label>10</label><figDesc>Filtering results per activity sector and entity (all languages).</figDesc><table coords="19,157.82,176.63,279.44,513.14"><row><cell>Banco Bilbao Vizcaya Argentaria, S.A.</cell><cell cols="4">0.5200 0.3900 0.4400 0.4135</cell></row><row><cell>Banco Santander, S.A.</cell><cell cols="4">0.6600 0.6700 0.7000 0.6847</cell></row><row><cell>Bank of America Corporation</cell><cell cols="4">0.4000 0.3700 0.5700 0.4487</cell></row><row><cell>Bankia</cell><cell cols="4">0.7900 0.4900 0.5000 0.4949</cell></row><row><cell>CaixaBank</cell><cell cols="4">0.3900 0.4600 0.5800 0.5131</cell></row><row><cell>MAPFRE</cell><cell cols="4">0.4400 0.3200 0.5000 0.3902</cell></row><row><cell>Energy</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BP p.l.c.</cell><cell cols="4">0.7500 0.6700 0.4200 0.5163</cell></row><row><cell>Endesa, S.A.</cell><cell cols="4">0.4000 0.4900 0.5500 0.5183</cell></row><row><cell>Gas Natural SDG, S.A.</cell><cell cols="4">0.2500 0.1600 0.1500 0.1548</cell></row><row><cell>Iberdrola</cell><cell cols="4">0.4800 0.5700 0.4900 0.5270</cell></row><row><cell>Repsol S. A.</cell><cell cols="4">0.5100 0.3000 0.4200 0.3500</cell></row><row><cell>Personal care</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Gillette</cell><cell cols="4">0.3000 0.6100 0.4700 0.5309</cell></row><row><cell>Nivea</cell><cell cols="4">0.4000 0.1400 0.3100 0.1929</cell></row><row><cell>Technology and Software</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bing</cell><cell cols="4">0.4500 0.3200 0.3400 0.3297</cell></row><row><cell>BlackBerry</cell><cell cols="4">0.5700 0.3600 0.5200 0.4255</cell></row><row><cell>Google Inc.</cell><cell cols="4">0.4300 0.2000 0.3900 0.2644</cell></row><row><cell>Indra Sistemas, S. A.</cell><cell>0.5000</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Microsoft Corporation</cell><cell cols="4">0.6100 0.6200 0.6300 0.6250</cell></row><row><cell>Yahoo! Inc.</cell><cell cols="4">0.4400 0.0800 0.6200 0.1417</cell></row><row><cell>Telecommunications</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Telefónica, S.A.</cell><cell cols="4">0.5300 0.4200 0.4300 0.4249</cell></row><row><cell>Textile</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Industria de Diseño Textil, S.A.</cell><cell cols="4">0.5000 0.4500 0.5300 0.4867</cell></row><row><cell>Transport and Infrastructure</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">International Consolidated Airlines Group 0.7300 0.4200 0.1300 0.1985</cell></row><row><cell>Activity Sector</cell><cell>A</cell><cell>R</cell><cell>S</cell><cell>F(R,S)</cell></row><row><cell>Automotive</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Bayerische Motoren Werke AG (BMW)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">replab2012_related_Daedalus_1 0.8500 0.0900 0.2200 0.1277</cell></row><row><cell cols="5">replab2012_related_Daedalus_2 0.7200 0.0200 0.0900 0.0327</cell></row><row><cell cols="5">replab2012_related_Daedalus_3 0.5200 0.0200 0.1300 0.0347</cell></row><row><cell>Chevrolet</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">replab2012_related_Daedalus_1 0.7600 0.1100 0.3100 0.1624</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,130.10,686.23,124.80,8.10"><p>http://www.merriam-webster.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,130.10,686.23,86.82,8.10"><p>http://www.daedalus.es/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2" coords="8,159.62,121.07,310.94,8.10"><p>Julio Villena-Román, Sara Lana-Serrano, Cristina Moreno-García, Janine</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3" coords="10,163.58,121.07,307.00,8.10;10,124.82,131.63,184.31,8.10"><p>Julio Villena-Román, Sara Lana-Serrano, Cristina Moreno-García, Janine García-Morera, José Carlos González-Cristóbal</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work has been partially supported by several Spanish research projects: <rs type="projectName">MA2VICMR</rs>: Improving the access, analysis and visibility of the multilingual and multimedia information in web for the <rs type="projectName">Region of Madrid</rs> (<rs type="grantNumber">S2009/TIC-1542</rs>), <rs type="projectName">MULTIMEDICA: Multilingual Information Extraction in Health domain</rs> and application to scientific and informative documents (<rs type="grantNumber">TIN2010-20644-C03-01</rs>) and <rs type="projectName">BUSCAMEDIA: Towards a semantic adaptation of multi-network-multiterminal digital media</rs> (<rs type="grantNumber">CEN-20091026</rs>). Authors would like to thank all partners for their knowledge and support.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_cNfyRxd">
					<orgName type="project" subtype="full">MA2VICMR</orgName>
				</org>
				<org type="funded-project" xml:id="_2qEu9yv">
					<idno type="grant-number">S2009/TIC-1542</idno>
					<orgName type="project" subtype="full">Region of Madrid</orgName>
				</org>
				<org type="funded-project" xml:id="_DqbF9r4">
					<idno type="grant-number">TIN2010-20644-C03-01</idno>
					<orgName type="project" subtype="full">MULTIMEDICA: Multilingual Information Extraction in Health domain</orgName>
				</org>
				<org type="funded-project" xml:id="_P4PFCuh">
					<idno type="grant-number">CEN-20091026</idno>
					<orgName type="project" subtype="full">BUSCAMEDIA: Towards a semantic adaptation of multi-network-multiterminal digital media</orgName>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Automotive</head><p>replab2012_related_Daedalus_1 0.7460 0.1880 0.2620 0.2550 replab2012_related_Daedalus_2 0.6620 0.1460 0.2620 0.1668 replab2012_related_Daedalus_3 0.5360 0.1080 0.2120 0.1192 Banking and Insurance replab2012_related_Daedalus_1 0.7663 0.0300 0.3567 0.0824 replab2012_related_Daedalus_2 0.7788 0.1033 0.7333 0.1772 replab2012_related_Daedalus_3 0.7788 0.1033 0.7333 0.1772 Energy replab2012_related_Daedalus_1 0.7680 0.2275 0.4625 0.3423 replab2012_related_Daedalus_2 0.7640 0.2475 0.5000 0.3804 replab2012_related_Daedalus_3 0.7640 0.2475 0.5000 0.3804 Personal care replab2012_related_Daedalus_1 0.7400 0.2900 0.4200 0.2793 replab2012_related_Daedalus_2 0.5233 0.2000 0.2300 0.1960 replab2012_related_Daedalus_3 0.5233 0.2000 0.2300 0.1960 Technology and Software replab2012_related_Daedalus_1 0.6217 0.2483 0.4567 0.2679 replab2012_related_Daedalus_2 0.7067 0.2717 0.4283 0.2870</p><p>Julio Villena-Román, Sara Lana-Serrano, Cristina Moreno-García, Janine García-Morera, José Carlos González-Cristóbal replab2012_related_Daedalus_2 0.8600 0.2400 0.5300 0.3304 replab2012_related_Daedalus_3 0.6300 0.1000 0.3800 0.1583 Ferrari S.p.A. replab2012_related_Daedalus_1 0.7800 0.3000 0.4700 0.3662 replab2012_related_Daedalus_2 0.6900 0.1900 0.3300 0.2412 replab2012_related_Daedalus_3 0.6500 0.1600 0.3100 0.2111 Fiat S.p.A. replab2012_related_Daedalus_1 0.7700 0.0000 0.0000 replab2012_related_Daedalus_2 0.4600 0.0900 0.3000 0.1385 replab2012_related_Daedalus_3 0.3000 0.0700 0.1800 0.1008 Volkswagen replab2012_related_Daedalus_1 0.5700 0.4400 0.3100 0.3637 replab2012_related_Daedalus_2 0.5800 0.1900 0.0600 0.0912 replab2012_related_Daedalus_3 0.5800 0.1900 0.0600 0.0912 Banking and Insurance Banco Santander, S.A. replab2012_related_Daedalus_1 0.7200 0.0500 0.4300 0.0896 replab2012_related_Daedalus_2 0.7400 0.0700 0.5900 0.1252 replab2012_related_Daedalus_3 0.7400 0.0700 0.5900 0.1252 ING Group replab2012_related_Daedalus_1 0.9700 0.0000 0.0000 replab2012_related_Daedalus_2 0.9600 0.2000 0.9600 0.3310 replab2012_related_Daedalus_3 0.9600 0.2000 0.9600 0.3310 MAPFRE replab2012_related_Daedalus_1 0.6400 0.0400 0.6400 0.0753 replab2012_related_Daedalus_2 0.6600 0.0400 0.6500 0.0754 replab2012_related_Daedalus_3 0.6600 0.0400 0.6500 0.0754 Energy BP p.l.c. replab2012_related_Daedalus_1 0.5400 0.0400 0.4000 0.0727 replab2012_related_Daedalus_2 0.6900 0.0800 0.6900 0.1434 replab2012_related_Daedalus_3 0.6900 0.0800 0.6900 0.1434 Endesa, S.A. replab2012_related_Daedalus_1 0.7500 0.0000 0.0000 replab2012_related_Daedalus_2 0.4300 0.0000 0.0000 replab2012_related_Daedalus_3 0.4300 0.0000 0.0000 Gas Natural SDG, S.A. replab2012_related_Daedalus_1 0.9100 0.8000 0.8600 0.8289 replab2012_related_Daedalus_2 0.9200 0.8100 0.8600 0.8343 replab2012_related_Daedalus_3 0.9200 0.8100 0.8600 0.8343 Repsol S. A. replab2012_related_Daedalus_1 0.7900 0.0700 0.5900 0.1252</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Results achieved by the top ranked experiments in the polarity classification subtask are shown in Table <ref type="table" coords="13,209.69,188.22,3.77,8.96">3</ref>. The columns in the table are Accuracy (A), Reliability (R), Sensitivity (S) and the typical F-measure calculated over Reliability and Sensitivity. The only experiment submitted achieved the best performance of all participants for all languages in general and specifically for Spanish. The difference between Spanish and English, though not very high, is probably because the linguistic processing modules (the tokenizer, stemmer and specially the morphosyntactic analyzer) and the resources included in the semantic model are better for the case of Spanish, the main target language of our market.</p><p>The different entities have been organized into a set of sectors of economic activity. Results achieved per sector by our experiment for all languages in general are shown in Table <ref type="table" coords="13,203.81,593.99,3.77,8.96">4</ref>.</p><p>This table gives an idea of the domains that are best covered by our semantic models. In this case, the "Banking and Insurance", "Audiovisual" and "Telecommunications" sectors are the best covered, whereas the "Transport and Infrastructure" (corresponding to "International Consolidated Airlines Group" entity) is by large the worst covered.</p><p>Julio Villena-Román, Sara Lana-Serrano, Cristina Moreno-García, Janine García-Morera, José Carlos González-Cristóbal   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="17,139.98,461.27,330.14,8.96;17,154.22,473.27,316.33,8.96;17,154.22,485.27,265.46,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="17,181.46,473.27,289.09,8.96;17,154.22,485.27,29.79,8.96">Overview of RepLab 2012: Evaluating Online Reputation Management Systems</title>
		<author>
			<persName coords=""><forename type="first">Enrique</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adolfo</forename><surname>Corujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edgar</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,190.82,485.27,199.43,8.96">CLEF 2012 Labs and Workshop Notebook Papers</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,139.98,497.27,330.63,8.96;17,154.22,509.27,316.61,8.96;17,154.22,521.27,206.22,8.96" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="17,218.45,509.27,247.44,8.96">Overview of Multilingual Opinion Analysis Task at NTCIR-7</title>
		<author>
			<persName coords=""><forename type="first">Yohei</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">Kirk</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lun-Wei</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noriko</forename><surname>Kando</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-10-24">October 24, 2008</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Informatics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="17,139.98,533.27,330.70,8.96;17,154.22,545.27,316.19,8.96;17,154.22,557.27,316.06,8.96;17,154.22,569.27,235.03,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="17,438.55,533.27,32.13,8.96;17,154.22,545.27,316.19,8.96;17,154.22,557.27,101.79,8.96">WePS-3 Evaluation Campaign: Overview of the Web People Search Clustering and Attribute Extraction Task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Artiles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Borthwick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sekine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,263.81,557.27,206.47,8.96;17,154.22,569.27,205.07,8.96">Proceeding of the Conference on Multilingual and Multimodal Information Access Evaluation (CLEF)</title>
		<meeting>eeding of the Conference on Multilingual and Multimodal Information Access Evaluation (CLEF)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,139.98,581.27,330.66,8.96;17,154.22,593.27,316.35,8.96;17,154.22,605.27,316.54,8.96;17,154.22,617.27,95.40,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="17,154.22,593.27,316.35,8.96;17,154.22,605.27,51.51,8.96">DAEDALUS at WebPS-3 2010: k-Medoids Clustering using a Cost Function Minimization</title>
		<author>
			<persName coords=""><forename type="first">Julio</forename><surname>Villena-Román</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sara</forename><surname>Lana-Serrano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>González-Cristóbal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="17,215.31,605.27,142.61,8.96">CLEF 2010 Labs and Workshops</title>
		<title level="s" coord="17,368.00,605.27,69.92,8.96">Notebook Papers</title>
		<meeting><address><addrLine>Padua Italy</addrLine></address></meeting>
		<imprint>
			<date>September</date>
			<biblScope unit="page" from="22" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="17,139.98,629.27,330.54,8.96;17,154.22,641.30,316.20,8.96;17,154.22,653.30,316.03,8.96;17,154.22,665.30,316.44,8.96;17,154.22,677.30,208.70,8.96;21,173.06,150.59,248.96,9.30;21,173.06,163.43,248.96,9.30;21,173.06,177.59,52.07,8.10" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="17,154.22,641.30,316.20,8.96;17,154.22,653.30,32.29,8.96">MIRACLE at NTCIR-7 MOAT: First Experiments on Multilingual Opinion Analysis</title>
		<author>
			<persName coords=""><forename type="first">Julio</forename><surname>Villena-Román</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sara</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lana-Serrano</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><forename type="middle">C</forename><surname>González-Cristóbal</surname></persName>
		</author>
		<idno>.1000 0.4500 0.1636 replab2012_related_Daedalus_3 0.8900 0.1000 0.4500 0.1636 Personal care</idno>
	</analytic>
	<monogr>
		<title level="m" coord="17,194.52,653.30,275.72,8.96;17,154.22,665.30,316.44,8.96;17,154.22,677.30,76.50,8.96">7th NTCIR Workshop Meeting. Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-Lingual Information Access</title>
		<meeting><address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12">December 2008</date>
		</imprint>
	</monogr>
	<note>replab2012_related_Daedalus_2 0.8900 0</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
