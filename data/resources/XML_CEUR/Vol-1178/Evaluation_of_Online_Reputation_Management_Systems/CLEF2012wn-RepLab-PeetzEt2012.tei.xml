<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,213.97,115.90,187.42,12.90;1,252.38,136.38,110.60,10.75">From Sentiment to Reputation ILPS at RepLab 2012</title>
				<funder ref="#_CtxM2kB">
					<orgName type="full">European Community</orgName>
				</funder>
				<funder ref="#_jBS3GxA">
					<orgName type="full">Center for Creation, Content and Technology</orgName>
					<orgName type="abbreviated">CCCT</orgName>
				</funder>
				<funder ref="#_g2ku7kz #_UMJbBgt #_VUPDwJ4">
					<orgName type="full">Netherlands Organisation for Scientific Research (NWO)</orgName>
				</funder>
				<funder ref="#_WtSBn9p">
					<orgName type="full">LiMoSINe</orgName>
				</funder>
				<funder ref="#_6VDzadP">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_suhCkvJ #_ezncbya #_ZTpSMCm #_9ZeTHec #_8YA3SPy #_fjCK6c2 #_WeE2FZe #_Db626x7">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,191.02,173.41,86.63,8.64"><forename type="first">Maria-Hendrike</forename><surname>Peetz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ISLA</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,284.15,173.41,69.07,8.64"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ISLA</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,372.59,173.41,51.75,8.64"><forename type="first">Anne</forename><surname>Schuth</surname></persName>
							<email>schuth@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department">ISLA</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,192.11,185.05,58.87,8.74"><forename type="first">{m</forename><forename type="middle">H</forename><surname>Peetz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ISLA</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,213.97,115.90,187.42,12.90;1,252.38,136.38,110.60,10.75">From Sentiment to Reputation ILPS at RepLab 2012</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">561696EB21F87FEEC14E7C06873E7E37</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We report on our participation in the profiling task of the first edition of the CLEF RepLab evaluation initiative. We assume that a statement-such as a tweet-that caries negative sentiment can have a positive impact on the reputation of the entity it talks about (and vice versa). Our model directly captures this impact by observing the reactions-such as replies-the statement solicits. We present the assumptions behind our model and the model itself. We find that given the current setting, results on the test set are strongly entity-dependent and that the test data is very different from the trial data. We conclude with a proposal on how to create a task that avoids such dataset dependent problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Reputation of an entity is an opinion about that entity, typically in a social context. Twitter as a opinion-conveying medium provides this social context: here, reputation is what Twitterers think of the entity. The reputation task in RepLab was to identify if a tweet's content has positive or negative implications on the reputation of a brand name. A tweet has mainly implications on opinions of people it reaches, directly or indirectly. Thus, an oracle or manual annotation would select all tweets uttered by Twitterers directly after they were read. One could then analyse manually if the original tweet had any implication on the opinion about the brands in those uttered tweets. Unfortunately, this is infeasible on many different levels. However, if we assume that tweets uttered in response to a tweet convey the implication on reputation of the entity mentioned in this tweet, we come to the first research question:</p><p>Can we bootstrap the implication on reputation from sentiment-annotated tweets in the replies and retweets to a source tweet?</p><p>This research question implies that our understanding of reputation is something very different to sentiment. Additionally, tweets have a lot of metadata that may contain information as to how far a tweet can be considered polarized with respect to reputation.</p><p>Can we use machine learning to learn an appropriate combination of features to classify the polarity of a tweet? Our work is organized as follows: We first describe our filtering methods in Section 2, we then continue with our polarity methods in Section 3. In Section 4, we describe how we use a machine learning approach to perform feature selection and classification. We then describe our experiments in Section 5. Results and and analysis are presented in Section 6. We finish with a conclusion in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Filtering Methods</head><p>The filtering task is to classify a tweet as relevant to a source entity or not. We were provided with the Wikipedia pages of a source entity. For the disambiguation of the source entities we semanticised the tweets with Wikipedia pages and disambiguated on the grounds of these pages. For each entity, we automatically assemble sets of Wikipedia pages that, if they are linked to in a tweet, this indicates the relevance of the tweet for a source entity. In the baseline we assume all tweets to be relevant for a source entity. In the following, we lay out how the related Wikipedia pages for a tweet are found using semanticising (see Section 2.1) and how from this, sets of entities (as their Wikipedia page) that are related to the source entity are created (see Section 2.2). In Section 2.3, we shortly explain how relevant tweets are selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semanticising</head><p>Each tweet can have possible semantic links to Wikipedia pages. Finding those links means disambiguating and finding concepts in a tweet. Following <ref type="bibr" coords="2,395.24,416.32,67.29,8.64">Meij et al. [2012]</ref>, we use two features: the LINKPROBABILITY and the COMMONNESS feature. The earlier is the probability that an n-gram in a tweet is a link in Wikipedia: how many occurrences of this n-gram are actually within hyperlinks to a page? The second feature COMMONNESS, is the probability of an n-gram to link to a certain concept. The product of the two features is the number of links to a concept if this n-gram is a link to a Wikipedia page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">List Aggregation</head><p>Top Entities For each source entity, we aggregate the number of times Wikipedia pages are linked in tweets. The top N most linked pages are the set TOPPAGES.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entities in Wikipedia Page</head><p>Another group on entities is selected with the help of the provided Wikipedia pages of the source entities (SOURCEPAGES). Here, we select all outgoing links to internal Wikipedia pages. Those pages are called WIKIPAGES.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combination of List</head><p>For each source entity, TOPWIKIPAGES is the intersection of the sets TOPPAGES and WIKIPAGES. Additionally, every list contains the pages in SOURCEPAGES.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Disambiguation</head><p>Finally, for the disambiguation, we assume that a tweet is relevant to a source entity if 1. there are links to Wikipedia pages found by the semanticiser, and 2. those links are in a set of related Wikipedia pages for the source entity: either TOPWIKIPAGES, TOPPAGES, WIKIPAGES, or SOURCEPAGES.</p><p>Our runs for the filtering task differ in the use of the set of related Wikipedia pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Polarity Methods</head><p>The polarity task asks to classify a tweet for a given entity into having an impact on the reputation of that entity or not. There are three classes of polarity, positive, negative, and neutral. This section proposes two groups of models: sentiment models (see Section 3.1) and reputation models (see Section 3.2). The two sentiment models build upon another, where sentiment model 1 is the first iteration of the iterative sentiment model 2. All reputation models are iterative and based on sentiment terms. They differ in the way they split positive and negative polarity vectors and in their initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sentiment Baselines</head><p>In the following we introduce two sentiment models. Sentiment model 1 (see Section 3.1) estimates sentiment based on the sentiment value of terms in a tweet, whereas sentiment model 2 (see Section 3.1) uses this as a initialization for an iterative approach.</p><p>Sentiment Model A simple way of estimating sentiment is to define sentiment as the sum of the sentiment of terms in a tweet. Manually annotated sentiment lists can be found in <ref type="bibr" coords="3,337.43,462.28,71.64,8.64">Hu and Liu [2004]</ref>, <ref type="bibr" coords="3,415.64,462.28,62.12,8.64">Liu et al. [2005]</ref>, and Pérez-Rosas et al. <ref type="bibr" coords="3,223.12,474.24,24.40,8.64">[2012]</ref>. We say S(w) is the sentiment for a term w. The sentiment for a tweet t and its terms terms(t) is</p><formula xml:id="formula_0" coords="3,234.65,504.61,119.80,27.27">sent(t) = 1 |terms(t)| w∈terms(t)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S(w).</head><p>(1)</p><p>We refer to this model as sentiment model 1.</p><p>Iterative Sentiment Model Language use in Twitter is very different from traditional texts. We use a more elaborate sentiment model where the sentiment terms are learnt on a Twitter corpus. For that, we use the sentiment vectors S(w) from Section 3.1 and learn Twitter specific sentiments in an iterative approach.</p><p>We estimate the sentiment of a tweet t in iteration i as</p><formula xml:id="formula_1" coords="3,231.62,639.25,248.97,27.27">sent i (t) = 1 |terms(t)| w∈terms(t) S i (w),<label>(2)</label></formula><p>and update the sentiment vector S i (w) based on all tweets T S i (w) =</p><formula xml:id="formula_2" coords="4,296.86,139.39,183.73,20.06">t∈T sent i-1 (t).<label>(3)</label></formula><p>We refer to this model as sentiment model 2. The initial sentiment sent 0 (t) is equivalent to the sentiment in sentiment model 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unsupervised Reaction Models Bootstrapped with Sentiment</head><p>The goal of this method is to learn the polarity of words with respect to an entity. In the sentiment models, we simply use the general sentiment that words carry, irrespective of the entity that is talked about. From examples, we know that the baseline approach is too simplistic. Depending on the context-the entity in question-the polarity of a word can be completely opposite from the general sentiment it caries. The obvious example is: R.I.P. Michael Jackson, we miss you. In this example, words carry negative sentiment (sadness) while the statement itself has a positive impact on the reputation of Michael Jackson. In the context of another entity, however, these words can carry a negative impact on the reputation. In this model, we intend to learn this in an unsupervised manner.</p><p>In the following we lay out the assumptions underlying the models in Section 3.2 and introduce the different reaction models, Model 1 (see Section 3.2), Model 2 (see Section 3.2), and Model 3 (see 3.2).</p><p>Assumptions Based on interviews with experts we hypothesize the following:</p><p>1. The message in a tweet is not necessarily about the entity we are concerned with. But, as tweets are rather short, we assume it is about some entity as soon as we find a reference to it. 2. A tweet with positive (negative) sentiment from a user who tweets mainly negative (positive) tweets has more impact on the reputation. 3. Positive sentiment can cancel negative sentiment and vice versa; positive reputation can cancel negative reputation and vice versa. 4. The impact on the reputation of an entity as represented in a tweet is based on the sentiment the tweet causes in other users.</p><p>Assumption 4 is the intuition that underlies our model. We hypothesize that the impact of a statement on reputation can be deduced from the sentiment of reactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reaction Model 1</head><p>We propose an iterative approach to estimate an entity e specific term vector W (e). The term vector is initialized with sentiment terms, similar to sentiment model 2 (see Section 3.1). We assume that the impact of reputation can be measured by the kind of replies and retweets it solicits. Thus, every iteration we estimate the polarity of a tweet based on the polarity contribution of the retweets and replies to a tweet. At the end of the iteration we update the term vectors W i based on this estimated polarity of a tweet and the previous term vector W i-1 . We assume that after N iterations W N (e) ≈ W (e) for all entities and we can estimate the polarity of every tweet, even if unseen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reaction Model 2</head><p>The number of tweets with positive and negative sentiment is skewed in the dataset. This influences the estimation of the term vector W -positive and negative influences do not cancel another out.</p><p>We propose separate reputation vectors W + and W -. The difference to Model 1 is the estimation of the polarity contribution and the iterative updating W + , W -: here we have different vectors for positive and negative polarities. As the reputation vectors are normalized at the end of each iteration and the influence of positive tweets is not overwhelming the negative tweets.</p><p>Reaction Model 3 The third reaction model differs from Model 1 with respect to the initialization. In Model 1 (see Section 3.2), the initial vector W 0 (e) is the sentiment vector S, so W 0 (e, w) = S(w). In this model, Model 3, the vector of the original sentiment does not interpolate into W 1 . That way, we have a stronger focus on the the actual reputation and have only terms in the sentiment lexicon that feature a strong polarity within Twitter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Classification</head><p>We use a machine learning approach for classification the classification of polarity. We view all our models, described in Section 3.1 through Section 3.1, as features and train a classifier based on the trial data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Feature descriptions</head><p>For each tweet we collect 25 features. Those 25 features include the sentiment and reputation models, as well as metadata features.</p><p>reactionmodel1 as described in Section 3.2 reactionmodel2 as described in Section 3.2 reactionmodel3 as described in Section 3.2 sentimentmodel1 as described in Section 3.1 sentimentmodel2 as described in Section 3.1 scaledreactionmodel3 scaled-or, centered-version of the reactionmodel3 feature scaledsentimentmodel1 scaled-or, centered-version of the sentimentmodel1 feature entity a reference to the entity as provided by the track organizers. Note that this feature can not be used in classifier that generalizes to the test collection. lang detected language knownlang whether lang is either english or spanish, the languages for which we have sentiment lexicons nrrt the number of retweets nrrp the number of replies nrreact the total number of reactions (nrrt + nrrp) nrpos the number of reactions with positive sentiment nrneg the number of reactions with negative sentiment nrreactionfriends the sum of the number of friends of the authors of all reaction tweets fractionpos the fraction of positive reactions (nrpos / nrreact) fractionneg the fraction of negative reactions (nrneg / nrreact) reactpossum the sum of sentiment of negative reactions reactnegsum the sum of sentiment of positive reactions friends the number of friends favorite whether a tweet was favorited userreactionmodel3 the sum of the reactionmodel3 for this user usercount the number of thwarts from this user useravgreactionmodel3 the average value of reactionmodel3 for this user</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Classifier</head><p>We train a simple tree classifier<ref type="foot" coords="6,260.33,255.53,3.49,6.05" target="#foot_0">1</ref> using the above features and subsets of these features on the trial data. We select the subsets of features based on the information gain of individual features, as illustrated in Table <ref type="table" coords="6,301.68,281.11,3.74,8.64">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental setup</head><p>In this section we describe the experiments to answer the research questions mentioned in Section 1. We describe the official and external datasets as well as their preprocessing in Section 5.1. The runs and their evaluation are described in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data</head><p>Twitter Dataset We used the dataset provided by the organizers of RepLab@CLEF. The dataset was split in labeled (unlabeled of the test set) and background datasets. In particular, the background dataset contains 238,000 and 1.2 million tweets for trial and test set, respectively. This means 40,000 and 38,000 tweets per entity, respectively. The set of labeled tweets in the trial dataset contains 1649 tweets, of which we managed to download 1553 tweets (94.1%). The set of unlabeled tweets for the test data contains 12400 tweets, of which we managed to download 11432 tweets (92%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Replies and Retweets to Tweets</head><p>The reputation models are based on the reactions to the tweets. For us, a reaction is a tweet that is either a reply or a retweet. We extracted ∼ 434,000 (17,000 per entity) reactions from the test background dataset and ∼ 50,000 (8,000 per entity) from the trial background dataset. These are supplemented with all (∼ 228,000,000) reactions from an (external) Twitter spritzer stream after the earliest date of a tweet in either trial or test data (25 October 2011). Those reactions were not necessarily reaction to tweets in the background and (un)-labeled corpora. Consider Table <ref type="table" coords="6,158.83,586.47,4.98,8.64" target="#tab_0">1</ref> for the number of reactions to tweets in the background dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentiment Lexicons</head><p>We use publicly available sentiment word lexicons in English <ref type="bibr" coords="6,134.77,626.42,32.72,8.64">[Hu and</ref><ref type="bibr" coords="6,169.97,626.42,108.75,8.64">Liu, 2004, Liu et al., 2005]</ref> and Spanish <ref type="bibr" coords="6,332.66,626.42,103.77,8.64">[Pérez-Rosas et al., 2012]</ref> as the vast majority of tweets are in either of these languages. Preprocessing We preprocess all tweets using the following procedure:</p><p>1. separate punctuation characters from word characters but: 2. keep mentions, hashtags and smilies intact; 3. casefolding; 4. tokenize by splitting on whitespace.</p><p>Additionally, we perform language identification on tweets using the method described in Carter et al. <ref type="bibr" coords="7,194.25,341.97,24.90,8.64">[2013]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation</head><p>We participated with 5 runs, see Table <ref type="table" coords="7,290.54,392.95,5.05,8.64">2</ref> for a description of these runs. The sentiment models were trained on the entire background corpora, entity unrelated. The reputation models were trained on the reactions as explained in Section 5.1. We estimated the performance of each run on the trial data for polarity and filtering separately and paired the best polarity run with the best filtering run, the second best polarity run with the second best filtering run, etc.</p><p>The evaluation measures we use are accuracy for the polarity and F-score for the relevance filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Analysis</head><p>In this section we answer the research questions mentioned in Section 1. We first analyze the official results of the runs in Section 6.1. Section 6.2 analyses how a different approach to set up the experiments is likely to be more realistic and successful in estimating polarity and relevance for tweets given an entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Results of the Runs</head><p>Table <ref type="table" coords="7,158.96,620.57,5.01,8.64" target="#tab_2">3</ref> shows the results of our runs on the trial data and the test data. We can see that the performance with respect to the evaluation measures of the test runs are roughly inversely proportional to the performance with respect to the evaluation measures of the trial runs for the polarity task as well as the filtering task. J4.8 using all features base 9 all+ent J4.8 on all features plus entity information base 10 best+ent J4.8 on the best features plus entity information</p><p>Table <ref type="table" coords="8,158.34,439.77,3.36,8.06">2</ref>. Run descriptions, sorted in descending performance on the trial set (see Table <ref type="table" coords="8,459.69,440.12,3.81,7.77" target="#tab_2">3</ref>) for polarity and filtering independently. The first 5 runs were submitted, the others serve as baselines for comparison in our analysis.</p><p>In particular, for the polarity task our best runs on the trial data are using all reputation and sentiment models and the language feature, while on the test data, this performs worst with respect to accuracy: the run with the highest accuracy uses no reputation models at all.</p><p>Table <ref type="table" coords="8,159.73,568.00,5.08,8.64" target="#tab_0">1</ref> shows the number of reactions and replies for the trial and test data. We can see that for the test data we used significantly more replies than the trial data, while the number of retweets remains about the same. We suspect that with a higher number of replies comes more noise that misguides the bootstrapping approach. In this respect, the trial and test data are very different and it is only natural that this is reflected in the quantitative evaluation.</p><p>For filtering, the highest F-score on the trial set was using all tweets. All more informed attempts to disambiguate could never reach the F-score of 96%: We found that the bigger the entity lists (thus recall) the higher the F-score. The relevance assignment by the retrieval method in the dataset creation seemed to have been very powerful.</p><p>The picture is different for the test data. Here, the F-score for the filter that considers all tweets to be relevant is 0.28, the lowest F-score of all. The best performing approaches are using the TOPPAGES set, either intersecting with WIKIPAGES or on its own. Again, in the trial set the observation is reversed: using the WIKIPAGES set lead to a higher performance with respect to F-score than using the TOPPAGES set.</p><p>On an entity-level we can see that for 13 out of the 31 tweets the baseline assigning all of the relevance performs best. However, it does hurt the filtering performance with respect to F-score for other entities so much that F-score drops to be the worst. The run ilps 2, even though it performs best with respect to the overall F-score, only has higher F-scores than ilps 2 for 8 entities, but the difference in F-score is on average 0.55, with the F-score for ilps 1 being zero or near zero in 5 out of the eight cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Entity-specific annotation</head><p>Table <ref type="table" coords="9,159.65,524.93,5.08,8.64">5</ref> shows the ranks of the features used for polarity of trial data when sorted by information gain. The feature entity encodes for which entity the datapoint (tweet) is supposed to be classified. Of course, this feature can not be used in a classifier trained for the test set. We can see that knowing the entity in beforehand has the greatest information gain. The accuracy of base 9 and base 10 on the trial set feature is 0.82, 25% better than the runs without this prior information. The trial set is too small for elaborate analysis, but we conclude that for the entities used in the trial set, a manual entity-specific seed annotation is more useful than an entity-ignorant annotation. As the number of entities is limited, we propose to manually annotate tweets for every entity and train classifiers on those tweets for future incoming tweets. To ensure that changes in the use of language in the tweets over time are captured, an adaptive interactive interface for the reputation manager seems most convenient. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In general, we found that the trial and test set were very different. For the polarity task we are able to say that reputation models works well for all trial entities, but not for the test entities. Additionally, we also found that for the filtering task the best performing run strongly varied per entity. Therefore, for future reputation management tasks we propose a more natural setting, where training entities and evaluation entities are the same. Entities are very different, and given the manpower of reputation management companies, it seems feasible to annotate a batch of tweets for each new entity that needs to be monitored. Results are likely to be more reliable and useful.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,134.47,121.25,346.12,95.51"><head>Table 1 .</head><label>1</label><figDesc>Mean reactions per entity, statistics per dataset. The min, max and standard deviation are shown as well. Note that the number of replies is very different for the test data.</figDesc><table coords="7,204.03,121.25,207.30,61.61"><row><cell></cell><cell>trial data</cell><cell>test data</cell></row><row><cell></cell><cell cols="2">mean min max std mean min max std</cell></row><row><cell cols="3">#retweets 4767 2620 8982 2131 5282 2059 14831 2925</cell></row><row><cell>#replies</cell><cell cols="2">72 28 151 39 554 57 1806 464</cell></row><row><cell cols="3">#reactions 4839 2648 9066 2153 5836 2203 15119 2930</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,134.47,121.25,347.25,172.22"><head>Table 3 .</head><label>3</label><figDesc>Results on the trial and test data for both the polarity and filtering. For most baseline runs, no test data is available.</figDesc><table coords="9,231.13,121.25,153.09,138.33"><row><cell></cell><cell cols="4">polarity accuracy filtering F-score</cell></row><row><cell>run</cell><cell>trial</cell><cell cols="2">test trial</cell><cell>test</cell></row><row><cell cols="2">ilps 1 0.77</cell><cell cols="2">0.36 0.96</cell><cell>0.28</cell></row><row><cell cols="2">ilps 2 0.69</cell><cell cols="2">0.38 0.84</cell><cell>0.36</cell></row><row><cell cols="2">ilps 3 0.71</cell><cell cols="2">0.41 0.85</cell><cell>0.35</cell></row><row><cell cols="2">ilps 4 0.74</cell><cell cols="2">0.40 0.87</cell><cell>0.34</cell></row><row><cell cols="2">ilps 5 0.61</cell><cell cols="2">0.43 0.79</cell><cell>0.29</cell></row><row><cell cols="2">base 6 0.33</cell><cell>0.33</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">base 7 0.55</cell><cell>0.44</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">base 8 0.74</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">base 9 0.81</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">base 10 0.82</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,134.47,121.25,346.27,112.42"><head>Table 4 .</head><label>4</label><figDesc>Runs on the test data with distribution of the 11432 instances over classes. If a value for the relevance was missing it was assumed to be relevant.</figDesc><table coords="10,228.81,121.25,157.74,78.53"><row><cell>polarity</cell><cell>relevance</cell><cell></cell></row><row><cell cols="3">run neutral positive negative yes no</cell></row><row><cell>ilps 1 7433 3562</cell><cell>437 11432</cell><cell>0</cell></row><row><cell>ilps 2 7032 4191</cell><cell cols="2">209 5593 5839</cell></row><row><cell>ilps 3 5841 5036</cell><cell cols="2">555 7597 3835</cell></row><row><cell>ilps 4 6589 4638</cell><cell cols="2">205 6504 4928</cell></row><row><cell>ilps 5 5643 5700</cell><cell cols="2">89 5083 6349</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="6,144.73,657.08,110.83,7.77"><p>We use the WEKA[Hall et al.,  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1" coords="6,257.80,657.08,177.08,7.77"><p>2009] implementation of C4.5 byQuinlan [1993]   </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This research was supported by the <rs type="funder">European Union</rs>'s <rs type="programName">ICT Policy Support Programme</rs> as part of the <rs type="programName">Competitiveness and Innovation Framework Programme, CIP ICT-PSP</rs> under grant agreement nr <rs type="grantNumber">250430</rs>, the <rs type="funder">European Community</rs>'s <rs type="programName">Seventh Framework Programme</rs> (<rs type="grantNumber">FP7/2007-2013</rs>) under grant agreements nr <rs type="grantNumber">258191</rs> (PROMISE) and <rs type="grantNumber">288024</rs> (<rs type="funder">LiMoSINe</rs>), the <rs type="funder">Netherlands Organisation for Scientific Research (NWO)</rs> under project nrs <rs type="grantNumber">612.061.814</rs>, <rs type="grantNumber">612.061.815</rs>, <rs type="grantNumber">640.004.802</rs>, <rs type="grantNumber">380-70-011</rs>, <rs type="grantNumber">727.011.005</rs>, <rs type="grantNumber">612.001.116</rs>, the <rs type="funder">Center for Creation, Content and Technology (CCCT)</rs>, the <rs type="projectName">Hyperlocal Service Platform</rs> project funded by the <rs type="programName">Service Innovation &amp; ICT program</rs>, the <rs type="projectName">WAHSP</rs> and <rs type="projectName">BILAND</rs> projects funded by the <rs type="programName">CLARIN-nl program</rs>, the <rs type="programName">Dutch national program</rs> <rs type="projectName">COMMIT</rs>, and by the <rs type="programName">ESF Research Network Program ELIAS</rs>. 8 References <rs type="person">S. Carter</rs>, <rs type="person">W. Weerkamp</rs>, and <rs type="person">E. Tsagkias</rs>. Microblog language identification: Overcoming the limitations of short, unedited and idiomatic text. Language Resources and Evaluation Journal, 2013. <rs type="person">M. Hall</rs>, <rs type="person">E. Frank</rs>, <rs type="person">G. Holmes</rs>, <rs type="person">B. Pfahringer</rs>, <rs type="person">P. Reutemann</rs>, and <rs type="person">I. Witten</rs>. The WEKA data mining software: an update. SIGKDD, 11(1):10-18, 2009. <rs type="person">M. Hu</rs> and <rs type="person">B. Liu</rs>. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168-177. ACM, 2004.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_6VDzadP">
					<orgName type="program" subtype="full">ICT Policy Support Programme</orgName>
				</org>
				<org type="funding" xml:id="_CtxM2kB">
					<idno type="grant-number">250430</idno>
					<orgName type="program" subtype="full">Competitiveness and Innovation Framework Programme, CIP ICT-PSP</orgName>
				</org>
				<org type="funding" xml:id="_suhCkvJ">
					<idno type="grant-number">FP7/2007-2013</idno>
					<orgName type="program" subtype="full">Seventh Framework Programme</orgName>
				</org>
				<org type="funding" xml:id="_ezncbya">
					<idno type="grant-number">258191</idno>
				</org>
				<org type="funding" xml:id="_WtSBn9p">
					<idno type="grant-number">288024</idno>
				</org>
				<org type="funding" xml:id="_g2ku7kz">
					<idno type="grant-number">612.061.814</idno>
				</org>
				<org type="funding" xml:id="_UMJbBgt">
					<idno type="grant-number">612.061.815</idno>
				</org>
				<org type="funding" xml:id="_VUPDwJ4">
					<idno type="grant-number">640.004.802</idno>
				</org>
				<org type="funding" xml:id="_ZTpSMCm">
					<idno type="grant-number">380-70-011</idno>
				</org>
				<org type="funding" xml:id="_9ZeTHec">
					<idno type="grant-number">727.011.005</idno>
				</org>
				<org type="funded-project" xml:id="_jBS3GxA">
					<idno type="grant-number">612.001.116</idno>
					<orgName type="project" subtype="full">Hyperlocal Service Platform</orgName>
					<orgName type="program" subtype="full">Service Innovation &amp; ICT program</orgName>
				</org>
				<org type="funded-project" xml:id="_8YA3SPy">
					<orgName type="project" subtype="full">WAHSP</orgName>
				</org>
				<org type="funded-project" xml:id="_fjCK6c2">
					<orgName type="project" subtype="full">BILAND</orgName>
					<orgName type="program" subtype="full">CLARIN-nl program</orgName>
				</org>
				<org type="funded-project" xml:id="_WeE2FZe">
					<orgName type="project" subtype="full">COMMIT</orgName>
					<orgName type="program" subtype="full">Dutch national program</orgName>
				</org>
				<org type="funding" xml:id="_Db626x7">
					<orgName type="program" subtype="full">ESF Research Network Program ELIAS</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
