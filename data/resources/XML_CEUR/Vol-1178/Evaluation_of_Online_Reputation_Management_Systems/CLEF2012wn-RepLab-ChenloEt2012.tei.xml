<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,203.24,115.96,208.89,12.62">FBM-Yahoo! at RepLab 2012</title>
				<funder>
					<orgName type="full">Ministerio de Ciencia e Innovación</orgName>
				</funder>
				<funder ref="#_DtA77cF">
					<orgName type="full">Holopedia Project</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,155.57,153.63,67.44,8.74"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Chenlo</surname></persName>
						</author>
						<author>
							<persName coords="1,233.56,153.63,60.64,8.74"><forename type="first">Jordi</forename><surname>Atserias</surname></persName>
							<email>jordi.atserias@barcelonamedia.org</email>
							<affiliation key="aff1">
								<orgName type="institution">Fundació Barcelona Media</orgName>
								<address>
									<addrLine>Av. Diagonal</addrLine>
									<postCode>177</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,304.76,153.63,75.24,8.74"><forename type="first">Carlos</forename><surname>Rodriguez</surname></persName>
							<email>carlos.rodriguez@barcelonamedia.org</email>
							<affiliation key="aff1">
								<orgName type="institution">Fundació Barcelona Media</orgName>
								<address>
									<addrLine>Av. Diagonal</addrLine>
									<postCode>177</postCode>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,407.16,153.63,48.16,8.74"><forename type="first">Roi</forename><surname>Blanco</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Centro de Investigación en Tecnoloxías da Información (CITIUS</orgName>
								<orgName type="institution">Univ. de Santiago de Compostela</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Yahoo</orgName>
								<address>
									<addrLine>! Research Av</addrLine>
									<postCode>177</postCode>
									<settlement>Diagonal, Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,203.24,115.96,208.89,12.62">FBM-Yahoo! at RepLab 2012</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2D4A286046698D4EF763EF29A67FCF17</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes FBM-Yahoo!'s participation in the profiling task of RepLab 2012, which aims at determining whether a given tweet is related to a specific company and, in if this being the case, whether it contains a positive or negative statement related to the company's reputation or not. We addressed both problems (ambiguity and polarity reputation) using Support Vector Machines (SVM) classifiers and lexicon-based techniques, building automatically company profiles and bootstrapping background data. Concretely, for the ambiguity task we employed a linear SVM classifier with a token-based representation of relevant and irrelevant information extracted from the tweets and Freebase resources. With respect to polarity classification, we combined SVM lexicon-based approaches with bootstrapping in order to determine the final polarity label of a tweet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>RepLab <ref type="bibr" coords="1,171.59,505.44,10.52,8.74" target="#b0">[1]</ref> addresses the problem of Reputation analysis, i.e. mining and understanding opinions about companies and individuals, a harder and still not well understood problem. FBM-yahoo! participates in the RepLab Profiling task <ref type="bibr" coords="1,470.08,529.35,10.52,8.74" target="#b0">[1]</ref> where systems are asked to annotate two kinds of information on tweets:</p><p>-Ambiguity: To determine if a tweet is related to the company using.</p><p>-Polarity for Reputation: To determine if the tweet have positive or negative implications for the company's reputation;</p><p>2 Ambiguity task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Company Representation</head><p>Twitter messages are short (up to 140 characters), hence, measures that account for the textual overlap between tweets and company names are in general not enough to classify a given tweet as relevant or irrelevant <ref type="bibr" coords="2,381.70,118.99,9.96,8.74" target="#b1">[2]</ref>, mostly due to data sparsity and lack of context <ref type="bibr" coords="2,262.71,130.95,9.96,8.74" target="#b2">[3]</ref>. In order to alleviate this problem, we turned into using the Freebase<ref type="foot" coords="2,233.79,141.33,3.97,6.12" target="#foot_0">4</ref> graph and Wikipedia<ref type="foot" coords="2,332.05,141.33,3.97,6.12" target="#foot_1">5</ref> as reliable sources of information for building expanded term-based representations of the different companies.</p><p>From the Freebase/Wikipedia pages of the companies we extracted automatically two sets of entities, namely related concepts and non-related concepts:</p><p>-Related Concepts (RC): represents the set of entities that are connected with the company in Freebase through the incoming (outgoing) links connected to the company's Freebase page. For example, in the case of Apple Inc., the related concepts set includes iPhoto, ichat, ibook, iTunes Store. -Non-Related Concepts (NRC): represents the set of common entities with which the current company could cause spurious term matches. This set is comprised of all Freebase entities with a name similar to that of the company's. This set is built automatically by querying Freebase with the query that identifies the company in the training data. From this set we remove the target company (if it was found), and all the entities that are already included in RC, and all entities that shared at least one non-common category with the target company. As an example of this process, in the case of Apple Inc. some of the non-related entities selected were "big apple" or "pine apple".</p><p>Then for each entity obtained following the previous method we have crawled its Wikipedia page <ref type="foot" coords="2,218.03,397.39,3.97,6.12" target="#foot_2">6</ref> and then we have used Lucene<ref type="foot" coords="2,364.15,397.39,3.97,6.12" target="#foot_3">7</ref> software to compute the following lists of keywords for each set of entities (RC, NRC):</p><p>entity names: Name of the entities related (non-related) to the company.</p><p>named entities in text: All named entities extracted by the Stanford Named-Entity Recognizer <ref type="bibr" coords="2,233.09,458.84,9.96,8.74" target="#b3">[4]</ref>. ngrams: Unigrams and bigrams (applying stemming and removing stopwords).</p><p>A weight w is associated to all of the obtained keywords (list of entities, named entities in text, ngrams). In the case of the entities, the weight is always 1. For named entities in text and ngrams, the weight is the ratio of documents that contain the concrete keyword.</p><p>These lists of keywords represent our profile for a given company as a bag of words model. We note that tweets could be written in English and Spanish and accordingly we have computed two different profiles for each company: one with the English version of Wikipedia an the other one with the Spanish version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training Process</head><p>In recent years, Machine Learning techniques have been deeply applied over Twitter data with relative success in many classification problems <ref type="bibr" coords="3,422.17,150.87,10.52,8.74" target="#b4">[5]</ref>  <ref type="bibr" coords="3,435.60,150.87,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="3,446.12,150.87,7.01,8.74" target="#b6">7]</ref>. Concretely, the best system in WePS-3 Evaluation Campaign <ref type="bibr" coords="3,394.10,162.83,9.96,8.74" target="#b7">[8]</ref>, where the main task consisted in identifying if a tweet that contains a company name is related or not to the company, employed a linear SVM classifier. Following this approach, we have trained a SVM linear classifier using the LibLinear package <ref type="bibr" coords="3,134.77,210.65,9.96,8.74" target="#b8">[9]</ref>. Table <ref type="table" coords="3,179.84,210.65,4.98,8.74">1</ref> lists the features that are being used to represent the data, which are broken down into matches from terms in the tweet in the company's profile (profile), features related to the company name in the tweet (company) and company-independent (tweet-only) features. If the tweet could be spam (a simple word appears more than three times).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Company</head><p>Whether or not any hashtag contains the name of the company.</p><p>If exists an URL that contains the name of the company.</p><p>If the tweet mention the name of the company (first letter in uppercase).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Profile</head><p>Score in the related entities list (sum of weights over terms matched). Score in the related ngrams list (sum of weights over terms matched). Score in the related topics list (sum of weights over terms matched). Score in the non-related entities list (sum of weights over terms matched). Score in the non-related ngrams list (sum of weights over terms matched). Score in the non-related topics list (sum of weights over terms matched).</p><p>Table <ref type="table" coords="3,248.23,393.14,4.46,8.77">1</ref>. List of disambiguation features.</p><p>Note that the last six features compare a given tweet with the profile computed for the company. The first six features are tweet-dependent, and they only need the text of the tweet and the query that represents the company. Using this representation we were able to learn a classifier over the trial set (six companies) that can be directly applied to the test data.</p><p>The following sections explain three different approaches (lexicon-based and distant supervision using hashtags and lexicons) we explored in order to determine whether a tweet has positive or negative implications for the company's reputation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Lexicon-Based Approaches</head><p>The most straightforward approaches employ an ensemble of several lexicons created with different methodologies in order to broaden coverage, especially across domains since some sentiment cues are used differently depending on the subject being commented.</p><p>In order to aggregate the lexicon scores into a final polarity measure, several formulas can be used, for instance:</p><formula xml:id="formula_0" coords="4,216.09,303.81,260.26,20.14">polScore(t, lan, q t ) = li∈lan polLex(t, l i , q t ) (<label>1</label></formula><formula xml:id="formula_1" coords="4,476.35,303.81,4.24,8.74">)</formula><p>where t is a tweet, lan is the language of the tweet, q t is a query, l i is one of the lexicons associated to lan and polLex(t, l i ) is a matching function between the lexicon l i and the tweet t. We have developed two different matching functions, polLex raw and polLex smooth . polLex raw is a simple aggregation measure that takes into account just the matchings between tweets and lexicons to compute the final polarity:</p><formula xml:id="formula_2" coords="4,210.48,413.25,270.11,20.81">polLex raw (t, l, q t ) = w l ∈l tf w l ,t • priorP ol(w l )<label>(2)</label></formula><p>where t represents a simple tweet, l is one of the lexicons associated to the language of the tweet, w l is an opinionated word from lexicon l, tf w l ,t is the frequency of w l in tweet t and priorP ol(w l ) is the polarity score of word w l in lexicon l. 8  On the other hand, polLexSmooth is an aggregation measure that takes into account the matchings between tweets and lexicons and the distance of these matchings to the company name to smooth the score of polarity of each word:</p><formula xml:id="formula_3" coords="4,176.80,536.61,303.80,27.55">polLex smooth (t, l, q t ) = 1 |q t | qi∈qt w l ∈l∩t 1 d w l ,qi • priorP ol(w l )<label>(3)</label></formula><p>where d w l ,qt is the distance of the tweet term w l to query term q i . Finally, we decide the final classification of each tweet using the following simple thresholding:</p><formula xml:id="formula_4" coords="4,211.28,611.43,269.31,35.52">pol(t) =    positive if polScore(t, l, q t ) &gt; 0 neutral if polScore(t, l, q t ) = 0 negative if polScore(t, l, q t ) &lt; 0 (4)</formula><p>Note that it is possible to compute two different values for polScore(t, l, q t ) by applying either Equation 2 or Equation 3 to the formula in Equation <ref type="formula" coords="5,452.79,130.95,3.87,8.74" target="#formula_0">1</ref>. Full details about which methods have been used in the runs submitted can be found in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Distant Supervision</head><p>Traditional opinion mining methods proposed in the literature are often based on machine learning techniques, using as primary features a vocabulary of unigrams and bigrams collected from training data <ref type="bibr" coords="5,316.72,249.33,14.61,8.74" target="#b9">[10]</ref>.</p><p>Following this approach and we have used a linear SVM to classify tweets as positive, neutral or negative.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2. List of polarity features.</head><p>The lexicon-based approaches previously described do not require training and can be directly applied over test data. However, the proposed data representation requires some amount of training data to compute the vocabulary features for each tweet which was not available at training time. Moreover, due to the fact that the companies in the test set belong to different domains (e.g. banks vs technology), the terms (and even their senses) used for express opinions may change from one company to another.</p><p>For that reason, we learnt different a model for each company in which we automatically generated a set of labelled examples from their background model. Other recent work on this area has focused on distantly supervised methods which learn the polarity classifiers from data with noisy labels such as emoticons and hashtags <ref type="bibr" coords="5,194.93,656.12,10.52,8.74" target="#b5">[6]</ref>  <ref type="bibr" coords="5,208.76,656.12,14.61,8.74" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Distant Supervision using Hashtags</head><p>Similarly to <ref type="bibr" coords="6,189.89,139.06,14.61,8.74" target="#b10">[11]</ref>, for each polarity class (i.e. positive, negative and neutral) we have performed the following process to automatically generate positive, neutral and negative labelled examples:</p><p>1. Selecting all hashtags that were used in more than 5 tweets in the background model of the company. 2. Removing the noisy content (spam, repeated tweets, retweets, etc.) for each hashtag.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Using the equation 1 in conjunction with equation 2 as matching function to</head><p>select the top 5 positive/negative/neutral hashtags, according to the ratio of tweets of each hashtag that were classified as positive/negative/neutral. 4. Selecting the top 20 tweets of each polarity from top hashtags. This bootstrapping process enables to obtain up to 100 positive, negative and neutral labelled examples (i.e. up to 300 examples in total) to train different classifiers.</p><p>Once we have generated our labelled examples, we have trained a positive classifier (positive examples against negative plus neutral examples), and a negative classifier (negative examples against positive plus neutral examples) for each company in the test set. We have also trained the best thresholds that separated the positive and the negative examples for each classifier. Finally, we combined the two classifiers and the thresholds learned to decide if a given tweet had to be tagged as positive, neutral or negative.</p><p>Learning the Best Threshold In the previously described approach, we selected the class decision threshold for a classifier using data which could potentially contain noisy labels and consequently could harm the performance of our system. To alleviate this problem, we randomly assessed 50 examples from the background data of each company and we selected the positive/negative thresholds for each classifier according to the the class distribution found in the data. Full details about which runs submitted were built with this kind of training can be found in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Distant Supervision using lexicons</head><p>This distant supervision method is similar to the one explained in Section 3.3, with the difference that it makes use of the polarity lexicons instead of the tweet hashtags.</p><p>The following process is undertaken for each polarity class (i.e. positive, negative and neutral), in order to automatically generate positive, neutral and negative labelled examples for each company:</p><p>1. Select as positive examples tweets that only have positive matches sorted by the number of matches in the lexicon.</p><p>2. Select as neutral examples tweets that no matches ordered by the tweet length. 3. Select as negative examples tweets that only have negative matches sorted by the number of matches in the lexicon.</p><p>Similarly to the distant supervision method using hashtags doing this bootstrapping process we select up to 100 positive, negative and neutral labelled examples (i.e. up to 300 examples in total) in order to train different classifiers for each company. These examples are selected in order of their number of matches.</p><p>The final classifier is built using the thresholded ensemble described in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submitted Runs</head><p>FBM-Yahoo! participated in the profiling task of RepLab 2012 competition with 5 different runs <ref type="foot" coords="7,199.29,306.39,3.97,6.12" target="#foot_4">9</ref> . The particular details on how the FBM-Yahoo! 5 runs runs were made can be found in Table <ref type="table" coords="7,254.83,319.92,3.87,8.74">3</ref>. All runs use the method explained in section 2.1 to classify a tweet as relevant or irrelevant, but they differ on the polarity method used to compute the final label of a tweet (i.e. positive,negative or neutral ).</p><p>Regarding the polarity lexicon based method described in section 3.1 we employed a total of six different polarity lexicons for English (including OpinionFinder <ref type="bibr" coords="7,483.70,367.74,18.18,8.74" target="#b12">[13]</ref>, AFINN <ref type="bibr" coords="7,171.21,379.70,14.61,8.74" target="#b13">[14]</ref>, Qwordnet <ref type="bibr" coords="7,231.06,379.70,18.83,8.74" target="#b14">[15]</ref>, dictionaries from the Linguistic Inquiry and Word Count (LIWC) text analysis system <ref type="bibr" coords="7,291.89,391.65,15.50,8.74" target="#b15">[16]</ref> <ref type="foot" coords="7,307.39,390.08,7.94,6.12" target="#foot_5">10</ref> and five polarity lexicons for Spanish. Following <ref type="bibr" coords="7,178.93,403.61,15.50,8.74" target="#b16">[17]</ref> we also combine these lexicons with a lexicon based on emoticons.</p><p>Since the resources available for Spanish are scarce, we translated some of the resources available for English, for instance, some baseline lexicons like the one used by OpinionFinder (the MPQA Subjectivity Lexicon), or AFINN <ref type="bibr" coords="7,462.32,439.47,14.61,8.74" target="#b13">[14]</ref>. In order to resolve ambiguities in this bilingual dictionary and to adapt it to micro-blogging usage, we selected the translation alternative that occurred most frequently on an alternative large (100,000) Spanish Twitter corpus (different from the one provided by RepLab).</p><p>As an additional approach we used author-assessed datasets to create polar lexicons from customer reviews, in this case, from 100,000 good vs. bad comments sent to Hotels.com and other such sites, like movie comments from volunteer reviewers and professionals. A Naive Bayes classifier was trained, from which a list of class-discriminative unigrams and bigram was extracted. Only adjectives and adverbs from those list were filtered to create a data-driven polar lexicon, similar to the method of Banea and Mihalcea <ref type="bibr" coords="7,331.22,570.98,15.50,8.74" target="#b17">[18]</ref> that employs an automatically translated corpus. Finally, starting from a small, manually crafted dictionary, we expanded its polar entries via WordNet synsets. Table <ref type="table" coords="8,225.98,115.92,4.46,8.77">3</ref>. List of submittedd runs to Profiling Task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run Id</head><p>Description BMedia1 Distant Supervision using Hashtags (see section 3.2) BMedia2 Lexicon-Based using polLexraw(t, l) BMedia3 Lexicon-Based using polLex smooth (t, l, qt) BMedia4 Distant Supervision using Hashtags with threshold from dist. (see section 3.3) BMedia5 Distant Supervision using Lexicons (see section 3.4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Conclusions</head><p>Table <ref type="table" coords="8,161.78,252.34,4.98,8.74" target="#tab_1">4</ref> shows the official evaluation results for Ambiguity and Polarity for Reputation tasks for the 5 runs submitted by FBM-Yahoo!.</p><p>Ambiguity task. On one hand, results show that our ambiguity method has a poor reliability (R) and sensibility (S) performance. On the other hand, the accuracy of the classifier is very high.</p><p>R and S are macro-measures that are equivalent to the product of precisions (reliability) and the product of recalls (sensitivity) over positive and negative classes. To put things in perspective, Table <ref type="table" coords="8,332.27,336.03,4.98,8.74" target="#tab_2">5</ref> reports the precision and recall values for each class. These results show that our model is classifying most of examples as positive (i.e related to the company), due to the fact that there is a lack of negative examples in training companies (more than 95% of examples are positive). Polarity for reputation task. According to the official measures (R and S), the runs that take into account just the overlapping between tweets and lexicons (i.e. BMedia2 and BMedia3 ) performed the best for polarity classification. Nonetheles, bootstrapping approaches were very competitive in terms of accuracy. In fact, the performance they achieved is very close to that of the lexicon-based approaches, and therefore the first conclusion we can extract from this evaluation is that distant supervised approaches take a limited advantage of training data in this benchmark. This could be due the fact that lexicons contribute for most of the model signal and might make difficult to learn anything from other sources of features. Moreover, the noise introduced by misclassification data in the training process could harm the performance of the learning process more than improve it.</p><p>Profiling task. In this task, all methods behave similarly in terms of performance, being BMedia4 the best run. This method combines the hashtag bootstrapping approach with the selection of a threshold for each classifier learnt from hand-classified tweets from background models. It is worth to remark that we have selected the best threshold for a classifier using data which contains noisy labels and consequently could harm the overall performance of the system. In order to overcome this problem, we set a different threshold for each classifier using background data. Results indicate that setting this threshold alleviates the score noise coming from lexicon bootstrapped examples.</p><p>Finally, as future work, we would like to explore how sentiment in Twitter streams are affected by real-world events, which affect severely Twitter topic trends. For example, if a football team loses a match, probably the next day the overall opinion about this team will be to negative. We would also like to study how to detect the polarity changes across the time and how to adapt our classification models to this new scenarios. More concretely, we would like to apply propensity scoring techniques <ref type="bibr" coords="9,264.05,442.04,16.13,8.74" target="#b18">[19,</ref><ref type="bibr" coords="9,280.18,442.04,12.10,8.74" target="#b19">20]</ref> to deal with the fact that training instances are governed by a distribution that differs greatly from the test distribution.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,166.34,334.07,20.07,6.12;5,299.88,334.07,39.75,6.12;5,168.77,342.44,258.34,6.12;5,165.70,370.74,21.36,6.12;5,189.85,350.81,46.38,6.12;5,189.85,358.78,58.20,6.12;5,189.85,366.75,71.87,6.12;5,189.85,374.72,259.81,6.12;5,189.85,382.69,153.13,6.12;5,189.85,390.66,100.75,6.12;5,189.85,398.63,124.05,6.12;5,168.44,418.96,15.87,6.12;5,189.85,407.00,60.13,6.12;5,189.85,414.97,75.91,6.12;5,189.85,422.94,89.00,6.12;5,189.85,430.91,73.76,6.12;5,169.57,451.24,13.62,6.12;5,189.85,439.28,106.01,6.12;5,189.85,447.25,107.90,6.12;5,189.85,455.22,182.28,6.12;5,189.85,463.19,182.28,6.12"><head></head><label></label><figDesc>features: Unigrams and bigrams from training examples. Tweet Size of tweet. Number of links. Number of hashtags. If the tweet could be spam (a single word appears more than three times). Number of exclamations and interrogations. Number of uppercase letters. Number of lengthening phenomena. POS Number of verbs. Number of adjectives. Number of proper names. Number of pronouns. Pol. Number of positive emoticons. Number of negative emoticons. Lexicon polarity score using as matching function 2. Lexicon polarity score using as matching function 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,134.77,275.32,345.83,32.65"><head></head><label></label><figDesc>Table 2 lists the features employed to represent the data, which are broken down into tweet-based features, part of speech-based features and lexicon-based features.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,175.79,415.91,263.78,95.29"><head>Table 4 .</head><label>4</label><figDesc>Submitted runs for Profiling task at RepLab 2012.</figDesc><table coords="8,178.77,435.42,257.83,75.78"><row><cell>Ambiguity 11</cell><cell cols="2">Polarity</cell><cell>Profiling</cell></row><row><cell cols="2">Acc. R S F(R,S) Acc. R</cell><cell cols="2">S F(R,S) Acc.</cell></row><row><cell cols="3">BMedia1 .736 .166 .123 .103 .429 .283 .270 .269</cell><cell>.333</cell></row><row><cell cols="3">BMedia2 .736 .166 .123 .103 .409 .332 .365 .335</cell><cell>.335</cell></row><row><cell cols="3">BMedia3 .736 .166 .123 .103 .375 .288 .347 .308</cell><cell>.326</cell></row><row><cell cols="3">BMedia4 .736 .166 .123 .103 .390 .265 .258 .252</cell><cell>.358</cell></row><row><cell cols="3">BMedia5 .736 .166 .123 .103 .409 .287 .321 .290</cell><cell>.335</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,212.39,558.59,190.58,51.86"><head>Table 5 .</head><label>5</label><figDesc>Ambiguity performance per class</figDesc><table coords="8,248.03,580.26,119.30,30.18"><row><cell></cell><cell cols="2">Precision Recall F1</cell></row><row><cell>positive</cell><cell>0.38</cell><cell>0.66 0.47</cell></row><row><cell cols="2">negative 0.13</cell><cell>0.04 0.06</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="2,144.73,613.61,108.27,7.47"><p>http://www.freebase.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1" coords="2,144.73,624.57,112.98,7.47"><p>http://www.wikipedia.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2" coords="2,144.73,634.88,335.87,7.86;2,144.73,645.84,232.23,7.86"><p>In the data-set tweets are written in either English or Spanish. For this reason we have downloaded and stored both versions when possible.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3" coords="2,144.73,657.44,112.98,7.47"><p>http://lucene.apache.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_4" coords="7,144.73,612.96,335.86,7.86;7,144.73,623.92,335.86,7.86;7,144.73,634.88,78.19,7.86"><p>Another run (UNED 5) was submitted in collaboration with UNED which combines all FBM-Yahoo! and UNED runs. The details on the combination are described at see section 3 of<ref type="bibr" coords="7,208.58,634.88,14.34,7.86" target="#b11">[12]</ref> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_5" coords="7,144.73,645.84,335.87,7.86;7,144.73,656.80,159.16,7.86"><p>Mapping positive and negative sentiments to numeric polarities, expanding the lexicon to possible morphological variants.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work is partially funded by the <rs type="funder">Holopedia Project</rs> (<rs type="grantNumber">TIN2010-21128-C02-02</rs>), <rs type="funder">Ministerio de Ciencia e Innovación</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_DtA77cF">
					<idno type="grant-number">TIN2010-21128-C02-02</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.96,579.83,337.63,7.86;9,151.52,590.78,329.07,7.86;9,151.52,601.74,161.80,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,401.84,579.83,78.75,7.86;9,151.52,590.78,231.73,7.86">Overview of replab 2012: Evaluating online reputation management systems</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Corujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,410.00,590.78,70.59,7.86;9,151.52,601.74,128.86,7.86">CLEF 2012 Labs and Workshop Notebook Papers</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.96,612.83,337.64,7.86;9,151.52,623.79,249.07,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,336.02,612.83,144.57,7.86;9,151.52,623.79,92.02,7.86">It was easy, when apples and blackberries were only fruits</title>
		<author>
			<persName coords=""><forename type="first">Surender</forename><surname>Reddy Yerva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zoltán</forename><surname>Miklós</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,265.11,623.79,24.70,7.86">CLEF</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Notebook Papers</note>
</biblStruct>

<biblStruct coords="9,142.96,634.88,337.64,7.86;9,151.52,645.84,329.07,7.86;9,151.52,656.80,60.92,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,258.71,634.88,151.36,7.86">Finding support sentences for entities</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,427.86,645.84,23.62,7.86">SIGIR</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Crestani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Marchand-Maillet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Efthimiadis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Savoy</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="339" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,119.67,337.64,7.86;10,151.52,130.63,329.07,7.86;10,151.52,141.59,84.99,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,372.07,119.67,108.52,7.86;10,151.52,130.63,172.10,7.86">Feature-rich part-of-speech tagging with a cyclic dependency network</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,362.49,130.63,118.10,7.86;10,151.52,141.59,16.79,7.86">Proceedings of HLT-NAACL 2003</title>
		<meeting>HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="252" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,152.55,337.63,7.86;10,151.52,163.51,177.13,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,292.10,152.55,188.49,7.86;10,151.52,163.51,57.60,7.86">Classifying sentiment in microblogs: is brevity an advantage?</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bermingham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,227.28,163.51,28.15,7.86">CIKM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1833" to="1836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,174.47,337.63,7.86;10,151.52,185.43,139.88,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,294.05,174.47,186.54,7.86;10,151.52,185.43,43.76,7.86">Twitter sentiment classification using distant supervision</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Bhayani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,203.35,185.43,42.50,7.86">Processing</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,196.39,337.63,7.86;10,151.52,207.34,329.07,7.86;10,151.52,218.30,329.07,7.86;10,151.52,229.26,48.12,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,405.84,196.39,74.75,7.86;10,151.52,207.34,59.05,7.86">Sentiment analysis of twitter data</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Vovsha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Passonneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,236.17,207.34,244.42,7.86;10,151.52,218.30,46.96,7.86">Proceedings of the Workshop on Language in Social Media (LSM 2011)</title>
		<meeting>the Workshop on Language in Social Media (LSM 2011)<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06">June 2011</date>
			<biblScope unit="page" from="30" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,240.22,337.63,7.86;10,151.52,251.18,329.07,7.86;10,151.52,262.14,252.48,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,406.72,240.22,73.87,7.86;10,151.52,251.18,329.07,7.86;10,151.52,262.14,19.07,7.86">Weps-3 evaluation campaign: Overview of the web people search clustering and attribute extraction tasks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Artiles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Borthwick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sekine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,192.57,262.14,173.24,7.86">CLEF (Notebook Papers/LABs/Workshops</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.96,273.10,337.63,7.86;10,151.52,284.06,329.07,7.86;10,151.52,295.02,70.14,7.86" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,385.23,273.10,95.36,7.86;10,151.52,284.06,113.14,7.86">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J H X R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,287.83,284.06,156.57,7.86">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,305.98,337.98,7.86;10,151.52,316.93,286.08,7.86" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,308.35,305.98,172.24,7.86;10,151.52,316.93,111.99,7.86">Thumbs up? sentiment classification using machine learning techniques</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,285.73,316.93,91.18,7.86">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,327.89,337.98,7.86;10,151.52,338.85,153.58,7.86" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,313.31,327.89,167.28,7.86;10,151.52,338.85,82.59,7.86">Twitter sentiment analysis: The good the bad and the omg! In</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Kouloumpis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,240.85,338.85,29.65,7.86">ICWSM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,349.81,337.97,7.86;10,151.52,360.77,329.07,7.86;10,151.52,371.73,161.80,7.86" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,312.87,349.81,167.72,7.86;10,151.52,360.77,237.04,7.86">Using an emotion-based model and sentiment analysis techniques to classify polarity for reputation</title>
		<author>
			<persName coords=""><forename type="first">Jorge</forename><surname>Carrillo De Albornoz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">C Y E A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,411.72,360.77,68.88,7.86;10,151.52,371.73,128.86,7.86">CLEF 2012 Labs and Workshop Notebook Papers</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,382.69,337.98,7.86;10,151.52,393.65,202.63,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,307.70,382.69,172.90,7.86;10,151.52,393.65,93.39,7.86">Recognizing contextual polarity in phraselevel sentiment analysis</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,266.86,393.65,52.76,7.86">HLT/EMNLP</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,403.02,337.97,9.45;10,151.52,415.56,103.22,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,211.97,404.61,268.62,7.86;10,151.52,415.56,41.49,7.86">A new ANEW: Evaluation of a word list for sentiment analysis in microblogs</title>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">Å</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,201.25,415.56,24.82,7.86">CoRR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,426.52,337.98,7.86;10,151.52,437.48,329.07,7.86;10,151.52,448.44,329.07,7.86;10,151.52,459.40,329.07,7.86;10,151.52,470.36,286.44,7.86" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,290.30,426.52,190.29,7.86;10,151.52,437.48,22.96,7.86">Q-wordnet: Extracting polarity from wordnet senses</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Agerri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>García-Serrano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,332.04,448.44,148.56,7.86;10,151.52,459.40,285.79,7.86">Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC&apos;10)</title>
		<editor>
			<persName><forename type="first">)</forename><surname>Chair</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">C C</forename><surname>Choukri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Maegaard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Mariani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Odijk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Piperidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Rosner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Tapias</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename></persName>
		</editor>
		<meeting>the Seventh International Conference on Language Resources and Evaluation (LREC&apos;10)<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA)</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,481.32,337.98,7.86;10,151.52,492.28,235.31,7.86" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">E</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">J</forename><surname>Booth</surname></persName>
		</author>
		<title level="m" coord="10,337.12,481.32,143.48,7.86;10,151.52,492.28,38.68,7.86">Linguistic inquiry and word count: Liwc 2001</title>
		<meeting><address><addrLine>Mahway</addrLine></address></meeting>
		<imprint>
			<publisher>Lawrence Erlbaum Associates</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,503.24,337.98,7.86;10,151.52,514.19,329.07,7.86;10,151.52,525.15,149.78,7.86" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,285.29,503.24,195.31,7.86;10,151.52,514.19,73.79,7.86">Emoticon smoothed language models for twitter sentiment analysis</title>
		<author>
			<persName coords=""><surname>Kun-Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wu-Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,252.84,514.19,227.74,7.86;10,151.52,525.15,116.27,7.86">Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the Twenty-Sixth AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,536.11,337.98,7.86;10,151.52,547.07,329.07,7.86;10,151.52,558.03,148.44,7.86" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,263.63,536.11,216.97,7.86;10,151.52,547.07,72.77,7.86">Learning multilingual subjective language via crosslingual projections</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Banea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,245.22,547.07,235.37,7.86;10,151.52,558.03,115.96,7.86">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,568.99,337.97,7.86;10,151.52,579.92,313.46,7.89" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,312.17,568.99,168.42,7.86;10,151.52,579.95,18.13,7.86">Discriminative Learning Under Covariate Shift</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Brückner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,177.37,579.95,155.11,7.86">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="2137" to="2155" />
			<date type="published" when="2009-09">September 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.62,590.91,337.97,7.86;10,151.52,601.84,311.35,7.89" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,295.98,590.91,180.86,7.86">Linear-time estimators for propensity scores</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,151.52,601.87,238.29,7.86">Journal of Machine Learning Research -Proceedings Track</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="93" to="100" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
