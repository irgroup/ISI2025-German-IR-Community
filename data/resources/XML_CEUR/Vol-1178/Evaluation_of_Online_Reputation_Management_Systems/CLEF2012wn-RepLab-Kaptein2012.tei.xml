<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,197.49,115.96,220.39,12.62;1,227.12,133.89,161.11,12.62">Learning to Analyze Relevancy and Polarity of Tweets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,273.29,172.17,68.77,8.74;1,293.34,193.73,28.67,7.86"><forename type="first">Rianne</forename><forename type="middle">Kaptein</forename><surname>Oxyme</surname></persName>
							<email>rianne@oxyme.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,197.49,115.96,220.39,12.62;1,227.12,133.89,161.11,12.62">Learning to Analyze Relevancy and Polarity of Tweets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FB59911D97EFF443A8E4101C113437E1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of Oxyme in the profiling task of the RepLab workshop. We use a machine learning approach to predict the relevancy and polarity for reputation. The same classifier is used for both tasks. Features used include query dependent features, relevancy features, tweet features and sentiment features. An important component of the relevancy features are manually provided positive and negative feedback terms. Our best run uses a Naive Bayes classifier and reaches an accuracy of 41.2% on the profiling task. Relevancy of tweets is predicted with an accuracy of 80.9%. Predicting polarity for reputation turns out to be more difficult, the best polarity run achieves an accuracy of 38.1%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The constantly growing volume of online opinions cannot longer be ignored by any company. The unsolicited opinions of consumers in the public domain are both a threat and an opportunity. Negative messages can harm your company's reputation, while positive messages or adequate responses to negative messages can lift your reputation. The goal of the RepLab Profiling task is to assign two important annotations to tweets:</p><p>1. Relevancy: Is the tweet related to the company? 2. Polarity for Reputation: Does the tweet have positive or negative implications for the company's reputation?</p><p>Concerning relevancy there are some differences compared to standard information retrieval tasks such as web search :</p><p>-Standing queries In most search scenarios the users create queries on the fly according to their information need at that moment. Queries might be adjusted according to the search results retrieved by their initial queries. For this task however the information need is clear, i.e. all tweets about the company in question. The same query is used to retrieve results over a longer period of time.</p><p>-Binary relevancy decisions Although relevancy of a search result is usually a binary decision, it is either relevant or non-relevant, the output of search systems is often a ranking of documents with the most relevant documents on top. For this task no ranking is generated, only the binary annotation relevant or non-relevant is assigned to each tweet.</p><p>Determining the polarity for reputation has some differences in comparison with standard sentiment analysis, but in our approach which learns from the training data this should not be an issue. The biggest challenges for the polarity for reputation analysis are:</p><p>-Dealing with two different languages: English and Spanish.</p><p>-The companies in the training data and the test data are different.</p><p>The main goals of our participation in this task are to:</p><p>-Explore explicit relevance feedback</p><p>In web search it has always been difficult to extract more information from the user than the keyword query. In this type of search however queries are used for an extended period of time to retrieve many results, so the pay-off of explicit relevance feedback is higher. -Devise a transparent method to analyse polarity for reputation Most Twitter sentiment analysis tools are far from perfect, and reach an accuracy anywhere between 40% and 70%. We do not expect our approach to work perfect either, so what is important in the interaction with users is to be able to explain why a tweet is tagged with a certain sentiment.</p><p>In the next section we describe our approach to determining relevancy and polarity of tweets. Section 3 describes the experimental set-up and results. Finally, in Section 4 the conclusions are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>To determine the relevancy and the polarity for reputation we use a machine learning approach which is the same for both tasks. We use standard machine learning algorithms, including Naive Bayes, Support Vector Machines and Decision Trees. The features used in the machine learning algorithms are a combination of features found in related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Query Dependent Features</head><p>The first group of features we use to determine the relevancy of tweet are features that depend on the query only, and not on specific tweets as suggested by <ref type="bibr" coords="2,467.31,613.48,9.96,8.74" target="#b5">[5]</ref>.</p><p>In our approach we use the following features:</p><p>-Wikipedia Disambiguation This feature has 4 possible values, the higher the value, the more ambiguous the query:</p><p>• 0: There is no disambiguation page for the entity name • 1: There is a disambiguation page, but the page with the entity name leads directly to the entity • 2: There is a disambiguation page, and the page with the entity name leads to this disambiguation page. • 3: The page with the entity name leads to another entity -Is the query also a common first or last name? -Is the query also a dictionary entry? -Is the query identical to the entity name? Here we disregard corporation types such as 'S.A'. Abbreviations such as 'VW', and partial queries such as 'Wilkinson' for the entity 'Wilkinson Sword' are examples where the query is not identical to the query name. -The amount of negative feedback terms. This feature has 3 possible values:</p><p>• 0: No negative feedback terms • 1: 1 to 3 negative feedback terms • 2: 4 to 10 negative feedback terms • 3: More than 10 negative feedback terms -Query difficulty, this feature is a combination of all features above. The higher the value of this feature, the more difficult it will be to retrieve relevant results.</p><p>All of these features are language dependent, since for example a common name in English does not have to be a common name in Spanish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Relevancy Features</head><p>The second group of features is based on relevancy. We use the language modeling approach to determine the relevancy of the content of a tweet. Besides the search term we also use manual relevance feedback. For each query in both languages we generate a list of positive and negative relevance feedback terms. To generate the list of feedback terms we make use of the background corpus that is provided. From the background corpus that consists of 30,000 tweets crawled per company name we extract the most frequently used terms and visualize these in wordclouds using a wordcloud generator tool <ref type="bibr" coords="3,334.93,512.62,9.96,8.74" target="#b0">[1]</ref>. For each query we create two wordclouds, one wordcloud from the tweets that contain the search term and one wordcloud from the tweets that do not contain the search term.</p><p>The tweets that do contain the search term can still contain positive and negative feedback terms. In Figure <ref type="figure" coords="3,291.77,560.48,4.24,8.74">2</ref>.2 the wordcloud for 'Nivea' is shown. By clicking on a term in the cloud, the tweets containing that term are displayed. This allows us to quickly explore the large amount of tweets. In the example we clicked on the term 'song'. When we inspect the associated tweets, it turns out 'Nivea' is also the name of a band. We therefore add this term to the negative feedback terms, as well as other words related to the band Nivea, such as the song titles 'Complicated' and 'Don't mess with my men'. Positive feedback terms include 'body', 'cream', 'care', etc. There is also a number of words in the wordcloud which are general words which could appear both in relevant and non-relevant tweets, such as 'follow' and 'lol'. These words are not added to any of the feedback term sets.</p><p>In Figure <ref type="figure" coords="4,192.27,369.74,4.24,8.74">2</ref>.2 the wordcloud of the tweets that do not contain the search term as a separate word is shown. This can happen for example when the search term is part of a username. In this case we inspect the tweets to see if the username belongs to a Twitter account about Nivea only, or owned by Nivea. If not, we add the username to the negative feedback terms. Also, in case of doubt the public Twitter user profile can be checked. In Figure <ref type="figure" coords="4,342.56,429.51,4.24,8.74">2</ref>.2 we clicked on the username '@nivea mariee '. From the displayed tweets we conclude this account is not relevant for Nivea, so we add '@nivea mariee ' to the negative feedback terms. The other terms are treated the same way as the terms in the previous wordcloud, so for each term we decide whether to add it to the positive or negative feedback terms or neither. There is actually quite some overlap between the two clouds, since all of the tweets are search results returned by Twitter for the query 'Nivea'. If we would have some training data available for this company we could look at the words which are used more often in positively or negatively rated tweets.</p><p>To calculate the relevancy score we use the following formula: where R stands for the probability a document D is relevant given query Q, t stands for a term in a document or a query, and C for the background collection. The positive query terms Q pos consist of the search terms plus the positive feedback terms. The document probabilities P (t|D) are smoothed as Fig. <ref type="figure" coords="5,181.47,317.00,4.13,7.89">2</ref>. The wordcloud from tweets that do not contain the term 'Nivea' follows to avoid taking the log of 0:</p><formula xml:id="formula_0" coords="5,228.01,370.84,153.75,8.74">P (t|D) = αP (t|D) + (1 -α)P (t|C)</formula><p>where we use a value of 0.1 for the smoothing parameter α.</p><p>Q neg contains the negative feedback terms. All probabilities are calculated in the log space, so that the numbers do not become too small. P (R|D) is the prior probability of a document being relevant. Here we use a length prior based on the number of characters in a tweet to favour longer tweets.</p><p>The relevancy scores of each query are normalised using min-max normalization, where the minimum score is simulated by a document that contains only the negative relevance feedback terms, and the maximum score is simulated by a document that contains only the search words and the positive feedback terms.</p><p>The machine learning algorithm uses the following relevancy features:</p><p>-Relevancy score -Tweet content contains a query term -Tweet content contains a positive feedback term -Tweet content contains a negative feedback term -Username equals the query -Username contains a negative feedback term</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Tweet Features</head><p>The last group of features are mostly general tweet features. Similar features have been used in <ref type="bibr" coords="5,215.63,637.20,9.96,8.74" target="#b2">[3]</ref>.</p><p>-Length of tweet in characters -Tweet contains a link -Tweet contains a link to the domain of the entity -Tweet is a direct message, i.e. starts with a username -Tweet is a retweet -Tweet is question</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Sentiment Features</head><p>To determine the sentiment of a tweet we make use of the scores generated by the SentiStrength algorithm <ref type="bibr" coords="6,280.15,242.26,9.96,8.74" target="#b1">[2]</ref>. SentiStrength generates sentiment scores based on predefined list of words and punctuation with associated positive or negative term weights. The rated training tweets are used to add words to the dictionary and to optimize term weights. Each language has its own lists of words. The tweets are scanned and and all words bearing sentiment, negating words, words boosting sentiment, question words, slang and emoticons are tagged. Using positive and negative weights which can be optimized using training data, the classification of a tweet is determined.</p><p>All together we now have 21 features we can use to determine relevancy and polarity for reputation of tweets. We use the data mining tool Weka <ref type="bibr" coords="6,454.81,350.83,10.52,8.74" target="#b4">[4]</ref> for training and testing the classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>The profiling task consists of two separate sub tasks: filtering and determining the polarity for reputation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Set-Up</head><p>We have created and submitted 5 runs to the RepLab workshop. For each run we made three choices of options:</p><p>1. The Machine Learning classifier to use: Naive Bayes (NB), Support Vector Machine (SVM) or Decision Tree (J48). 2. Which attributes to use as features for the classifier. To determine the relevancy we always user all attributes as described in the previous section. To determine polarity for reputation we use either all attributes, or only the SentiStrength scores. 3. Whether to train two separate classifiers for English tweets and Spanish tweets (separate) or train only one classifier that handles both Spanish and English tweets (merged).</p><p>Table <ref type="table" coords="6,162.16,656.12,4.98,8.74" target="#tab_0">1</ref> shows which options are used in the 5 submitted runs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overall Results Profiling</head><p>The overall results of the profiling task combining filtering and polarity for reputation are presented in Table <ref type="table" coords="7,268.51,273.50,3.87,8.74" target="#tab_0">1</ref>. Our run OXY 2, using a Naive Bayes classifier and using all possible features in a single classifier for both English and Spanish tweets performs best. On average over the topics, 41.2% of the tweets is annotated correctly. From all submitted runs to the workshop this is the best score.</p><p>In the remainder of this section we take a closer look at the results of the two subtasks: filtering and polarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results Filtering</head><p>The results of the filtering task are shown in table 2. Our best run, OXY 2 is the Naive Bayes classifier using all of the attributes described in Section 2.</p><p>Comparing the scores to the overall workshop results, we see that all our runs outperform all other runs when we look at the measure of accuracy, but we score mediocre looking at the F(R,S)-filtering measure. The reason for this is that the F(R,S)-filtering measure does not reward.</p><p>There are some Twitter specific issues that we take into account:</p><p>-The Twitter search returns not only results where the search terms are found in the content of the tweet, but also where the search terms are found as a part of the username, or in an external link. We do not use the information contained in the external links. To calculate our relevancy score we only take into account the content of the tweet. In fact, if the search term occurs in the username, this can mean three things: 1. The Twitter account is owned by the company, e.g. 'Lufthansa USA. Tweets from these account can all be considered relevant. 2. The company name is used in a different context, it could be for example also a last name, e.g. 'dave gillette'. Tweets from these accounts mostly non-relevant. 3. The username is referring to the company name because he or she likes to be associated with the company, e.g. 'Mr Bmw'. The number of relevant tweets from these accounts varies, some only tweet about the company they are named after, others hardly do. It is relatively easy to manually retrieve the official Twitter accounts for a company, and regard their tweets as relevant. It is harder to distinguish between the other two cases, but luckily they can be treated in the same way. That is, do not regard the occurrence of the company name in the username on its own as a signal for relevancy, but check whether the content of the tweet also contains relevant terms.</p><p>For our submitted runs we tried to separate relevant and non-relevant Twitter accounts by adding the usernames to the positive and negative relevance feedback. Although these are high quality indicators of relevance, their coverage in the dataset is small. Therefore for example we do not see them in the generated decision trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results Polarity for Reputation</head><p>The results of the task to determine polarity for reputation can be found in Table <ref type="table" coords="8,163.21,536.37,3.87,8.74" target="#tab_2">3</ref>. There is not one run that performs best at all evaluation measures. The run OXY 3 performs best looking at accuracy with an accuracy of 0.381, but run OXY 4 performs best looking at the F(R,S)-filtering score with a score of 0.256. In general the scores are not very impressive. Classifying all tweets as positive results in an accuracy of 0.438, which is a better accuracy than any of our runs. An advantage of the SentiStrength approach is that we can see why a tweet is classified into a certain sentiment. Let's look at some examples. The following tweet is correctly tagged as positive: '@NIVEA Australia love your products thanks <ref type="bibr" coords="8,340.77,644.16,12.89,8.74" target="#b1">[2]</ref>for following :) [1 emoticon] [sentence: 2,-1]'</p><p>The word 'thanks' is recognized as a positive term, as well as the happy emoticon :). The word 'love' however is not tagged as positive.</p><p>Incorrectly classified as negative due to the term 'swear' is the following tweet: 'Best <ref type="bibr" coords="9,156.03,167.01,12.76,8.74" target="#b1">[2]</ref>smelling body wash known to man ,I swear[-2]!!!![-0.6 punctuation emphasis][sentence: 2,-3] #apricot #nivea http://instagr.am/p/Jv5o0UG8Gy [sentence: 1,-1]'</p><p>A problem is that words have different meanings in different context. Apparently in our training data the word 'love' occurs in both positive and negative tweets, since it was not tagged as positive. A bigger training set might solve part of this problem. When we train the classifier on the test data, the word 'love' gets a positive value of 3.</p><p>Another problem occurs when also other companies or products are mentioned in the tweet, e.g. in the tweet: 'This boots hand cream is just rubbish[-3][-1 booster word]!![-0.6 punctuation emphasis][sentence: 1,-3] Gonna buy my nivea back today mchew [sentence: 1,-1]' At the moment we only calculate polarity over the whole tweet. When other entities occur in the tweet, polarity should only be calculated over the part of the tweet that deals with the entity that is the topic of the search.</p><p>As we discussed earlier, a problem for our classifier is the lack of training data. First of all, the amount of training data is small. Secondly, the companies in the training and test set are different. When our classifier is trained on a sufficiently large dataset for a company or a industry such as the automotive industry, and tested with data from the same company or industry, better results will be obtained. The best submitted run in the workshop attains an accuracy of 0.487, so we can conclude that classifying tweets on polarity of reputation is indeed a difficult task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>The tasks in the RepLab workshop allow us to work on some relatively new and challenging problems, i.e. how to detect the relevancy and polarity of reputation of tweets. In our submission we make use of manually selected feedback terms. In contrast to web search tasks it is more likely we will be able to obtain manual feedback terms from users since the queries remain the same for an extended period of time. The effort of providing feedback terms is paid back by receiving a better set of search results.</p><p>We make use of a machine learning approach to predict the relevancy and polarity of tweets. We include query dependent features, relevancy features, tweet features, and sentiment features. The features are language independent, i.e. the same 22 features are calculated for English and Spanish tweets.</p><p>Our approach works very well to predict the relevancy of tweets. An average accuracy of 80.9% is achieved using a Naive Bayes classifier. Predicting the polarity of reputation turns out to be a much harder task. Our best run achieves an accuracy of 38.1% using the J48 decision tree classifier. One of the main problems with our machine learning approach is the limited amount of training data. When the training and test data would contain the same companies, the results would already improve.</p><p>The best run of the profiling task, that is the combination of the filtering and polarity task is achieved by our run that uses a Naive Bayes classifier which uses all possible features in a single classifier for both English and Spanish tweets. It reaches an accuracy of 41.2%, which is the highest accuracy of all official RepLab submissions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,180.27,313.00,254.82,7.89;4,134.77,115.83,448.80,182.40"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The wordcloud from tweets containing the term 'Nivea'</figDesc><graphic coords="4,134.77,115.83,448.80,182.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,137.35,559.33,133.53,8.74;4,273.10,573.28,25.18,6.12;4,300.44,559.33,171.99,8.74;4,262.79,588.64,7.75,8.74;4,272.20,602.59,25.90,6.12;4,300.44,588.64,171.99,8.74"><head></head><label></label><figDesc>log P (R|D, Q) = log P (R|D) + t∈Qpos (P (t|Q) log P (t|D) -P (t|Q) log P (t|C)) -t∈Qneg (P (t|Q) log P (t|D) -P (t|Q) log P (t|C))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="5,134.77,115.83,444.40,186.40"><head></head><label></label><figDesc></figDesc><graphic coords="5,134.77,115.83,444.40,186.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,199.79,115.91,212.72,83.46"><head>Table 1 .</head><label>1</label><figDesc>Runs and Overall Results Profiling</figDesc><table coords="7,199.79,136.31,212.72,63.06"><row><cell cols="2">Run ID Classifier Attributes</cell><cell cols="2">Languages Accuracy</cell></row><row><cell>OXY 1 SVM</cell><cell>All</cell><cell>Merged</cell><cell>0.387</cell></row><row><cell>OXY 2 NB</cell><cell>All</cell><cell>Merged</cell><cell>0.412</cell></row><row><cell>OXY 3 J48</cell><cell>All</cell><cell>Merged</cell><cell>0.346</cell></row><row><cell>OXY 4 J48</cell><cell cols="2">SentiStrength Merged</cell><cell>0.359</cell></row><row><cell>OXY 5 J48</cell><cell>All</cell><cell>Separate</cell><cell>0.344</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,190.19,115.91,231.90,83.46"><head>Table 2 .</head><label>2</label><figDesc>Filtering results</figDesc><table coords="8,190.19,136.31,231.90,63.06"><row><cell cols="5">Run ID Accuracy R-Filtering S-Filtering F(R,S)-Filtering</cell></row><row><cell>OXY 1</cell><cell>0.798</cell><cell>0.217</cell><cell>0.174</cell><cell>0.139</cell></row><row><cell>OXY 2</cell><cell>0.809</cell><cell>0.235</cell><cell>0.272</cell><cell>0.197</cell></row><row><cell>OXY 3</cell><cell>0.790</cell><cell>0.198</cell><cell>0.128</cell><cell>0.096</cell></row><row><cell>OXY 4</cell><cell>0.790</cell><cell>0.198</cell><cell>0.128</cell><cell>0.096</cell></row><row><cell>OXY 5</cell><cell>0.778</cell><cell>0.157</cell><cell>0.111</cell><cell>0.085</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,190.19,223.31,231.90,83.46"><head>Table 3 .</head><label>3</label><figDesc>Polarity results</figDesc><table coords="8,190.19,243.71,231.90,63.06"><row><cell cols="5">Run ID Accuracy R-Filtering S-Filtering F(R,S)-Filtering</cell></row><row><cell>OXY 1</cell><cell>0.368</cell><cell>0.236</cell><cell>0.221</cell><cell>0.219</cell></row><row><cell>OXY 2</cell><cell>0.358</cell><cell>0.279</cell><cell>0.268</cell><cell>0.245</cell></row><row><cell>OXY 3</cell><cell>0.381</cell><cell>0.269</cell><cell>0.178</cell><cell>0.194</cell></row><row><cell>OXY 4</cell><cell>0.347</cell><cell>0.290</cell><cell>0.252</cell><cell>0.256</cell></row><row><cell>OXY 5</cell><cell>0.375</cell><cell>0.288</cell><cell>0.198</cell><cell>0.211</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,138.35,270.11,342.25,7.86;10,146.91,281.07,333.68,7.86;10,146.91,292.03,116.20,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,196.51,270.11,279.10,7.86">Using Wordclouds to Navigate and Summarize Twitter Search Results</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kaptein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,158.41,281.07,322.18,7.86;10,146.91,292.03,87.31,7.86">The 2nd European Workshop on Human-Computer Interaction and Information Retrieval (EuroHCIR)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,302.99,342.25,7.86;10,146.91,313.95,167.87,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,331.52,302.99,149.07,7.86;10,146.91,313.95,39.03,7.86">Sentiment strength detection for the social web</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Thelwall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Paltoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,194.57,313.95,29.76,7.86">JASIST</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="163" to="173" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,324.90,135.33,7.86;10,291.36,324.90,189.23,7.86" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="10,291.36,324.90,184.00,7.86">The University of Amsterdam at WePS3</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Tsagkias</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,146.91,335.86,333.68,7.86;10,146.91,346.82,122.42,7.86" xml:id="b3">
	<monogr>
		<title level="m" coord="10,389.81,335.86,90.78,7.86;10,146.91,346.82,88.99,7.86">CLEF (Notebook Papers/LABs/Workshops</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Pianta</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,357.78,342.24,7.86;10,146.91,368.74,291.50,7.86" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,311.98,357.78,168.62,7.86;10,146.91,368.74,84.21,7.86">Data Mining: Practical Machine Learning Tools and Techniques</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>Amsterdam</pubPlace>
		</imprint>
	</monogr>
	<note>3. edition</note>
</biblStruct>

<biblStruct coords="10,138.35,379.70,342.24,7.86;10,146.91,390.66,333.68,7.86;10,146.91,401.62,333.68,7.86;10,146.91,412.58,122.42,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,444.76,379.70,35.84,7.86;10,146.91,390.66,333.68,7.86;10,146.91,401.62,18.23,7.86">ITC-UT: Tweet Categorization by Query Categorization for On-line Reputation Management</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Matsushima</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,394.63,401.62,85.96,7.86;10,146.91,412.58,88.99,7.86">CLEF (Notebook Papers/LABs/Workshops</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Pianta</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
