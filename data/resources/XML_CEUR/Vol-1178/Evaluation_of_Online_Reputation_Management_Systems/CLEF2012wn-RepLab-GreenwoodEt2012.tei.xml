<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,191.29,115.96,232.77,12.62">Reputation Profiling with GATE</title>
				<funder ref="#_PVbWNKH">
					<orgName type="full">Engineering and Physical Sciences Research Council</orgName>
				</funder>
				<funder ref="#_fK3G8fH">
					<orgName type="full">EU-</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,180.07,153.63,86.57,8.74"><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Greenwood</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Univeristy of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,275.11,153.63,55.50,8.74"><forename type="first">Niraj</forename><surname>Aswani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Univeristy of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,358.22,153.63,77.07,8.74"><forename type="first">Kalina</forename><surname>Bontcheva</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Univeristy of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,191.29,115.96,232.77,12.62">Reputation Profiling with GATE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">94FF5301201663BE7E3E262619C56374</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Brand management has become increasingly difficult in the age of social media as the volume of opinions and discussions surrounding a companies products have swelled beyond the scale at which they can be reviewed manually. In this paper we report details of a system for monitoring Twitter 1 to find tweets relevant to a specific entity and the classification of such tweets based upon their reputational effect. The system was evaluated as part of the recent RepLab 2012 profiling task and the results from this evaluation show that our system out performs a number of naïve baseline approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Brand management has always been an important part of any companies public relations stratergy, but with the recent explosion in social media it has become more and more difficult to exhaustively monitor the vast amount of information in a timely fashion. The solution is to employ automatic methods to monitor social media for opinions. Such algorithms usually involve two stages. Firstly relevant documents (tweets, forum posts, etc.) have to be identified before a second phase can determine the opinions contained within them. The RepLab 2012 profilling task accurately mirrors this situation using Twitter; the task consists of filtering tweets to determine those which are related to a given entity, and a polarity task in which the tweets are to be labelled according to their effect on brand reputation.</p><p>In this paper we describe the GATE <ref type="bibr" coords="1,317.95,500.44,10.52,8.74" target="#b1">[2]</ref> based approaches to filtering and polarity we submitted to RepLab 2012 for evaluation. We treated filtering and polarity classification as two independent tasks and as such the majority of this paper is divided into two sections which detail the approaches we developed before concluding with a discussion of the the evaluation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Relevance</head><p>The first stage in any approach to reputation profiling invovles determining which "documents" (be they tweets, forum posts, etc.) are actually relevant. For example, a tweet containing the word Apple might refer to the fruit 2 , the well known manufacturer of computers<ref type="foot" coords="2,286.38,117.42,3.97,6.12" target="#foot_2">3</ref> , or the Beatles record company<ref type="foot" coords="2,434.18,117.42,3.97,6.12" target="#foot_3">4</ref> among a whole host of possibilities <ref type="foot" coords="2,246.24,129.37,3.97,6.12" target="#foot_4">5</ref> .</p><p>Our approach to determining relevance is based upon our recent research into disambiguation <ref type="bibr" coords="2,204.68,154.86,9.96,8.74" target="#b2">[3]</ref>. In this work we have been interested in determining which of a given set of DBpedia entries which share a common lexalization is actually being referred to. Similar to state-of-the-art methods, our algorithm uses the textual context, in which the particular candidate entity appears, in order to calculate a number of similarity metrics. In the current case this textual content includes the tweet itself, the expanded form of any hashtags <ref type="foot" coords="2,394.50,213.06,3.97,6.12" target="#foot_5">6</ref> and any pages the tweet explicitly links to. Then an overall score is produced for each candidate URI, based on a weighted sum of the following similarity metrics:</p><p>-String similarity: edit distance between the text string (such as Paris), and the lexicalisations of the entity URIs (e.g. Paris and Paris, Texas). -Structural similarity: calculated based on the ontology and instance property values in the Linking Open Data<ref type="foot" coords="2,294.83,289.09,3.97,6.12" target="#foot_6">7</ref> (LOD) resource. -Contextual similarity: calculated based on the probability that two words have a similar meaning, based on random indexing <ref type="bibr" coords="2,376.58,313.97,9.96,8.74" target="#b7">[8]</ref>. -Commonness: number of mentions of a specific URI as anchor text in Wikipedia (based on the commonness metric for Wikipedia pages <ref type="bibr" coords="2,388.45,337.26,9.96,8.74" target="#b4">[5]</ref>, also referred to as popularity <ref type="bibr" coords="2,200.61,349.22,10.30,8.74" target="#b6">[7]</ref>). This is the equivalent to assigning the most frequent sense in word sense disambiguation. However, unlike <ref type="bibr" coords="2,360.74,361.17,9.96,8.74" target="#b6">[7]</ref>, for efficiency we do not use Google queries as additional evidence.</p><p>Tie-breaks, i.e. candidate URIs with the same overall score, are resolved based on which one has the highest commonness score. If nevertheless more than one candidate remains, the instance which is more specific according to the LOD ontology is preferred.</p><p>Unfortunately, because of the nature of tweets, it often isn't possible to distinguish a noun (the fruit) from a proper noun (either of the two companies) due to the lack of case information etc. Due to this we assume that any reference to the entity, using any case, might be relevant and then disambiguate. This has the unfortunate side effect of assigning a URI to every mention. Our approach needs to be improved to incorporate the null assignment to handle cases where it is clear that the mention is not related to any of the known options.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Polarity</head><p>We treated the polarity task as a standard text classification problem and classified every tweet regardless of whether or not we deemed the tweet to be relevant<ref type="foot" coords="2,473.36,571.44,3.97,6.12" target="#foot_7">8</ref> .</p><p>We experimented with two different classification approaches; k-Nearest Neighbours and Naïve Bayes. Both approaches utilised the same GATE pipeline for pre-processing of the tweets. The rest of this section is split into three parts. Firstly we outline the pre-processing pipeline and then we detail the two classifiers we built for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pre-Processing</head><p>Both approaches to polarity classification are based (primarily) upon simple tokenization of the individual tweets. We have implemented a simple GATE pipeline which performs tokenization, taking in to account a number of token types specific to the way in which tweets are often written. This pipeline consists of the following processing resources (PR):</p><p>-Document Reset: a standard GATE PR which simply deletes existing annotations, allowing an application to be run multiple times for development purposes. -Tokenizer: the standard GATE UNICODE tokenizer.</p><p>-Sentence Splitter: As each document contains a single tweet, which we assume to be a single sentence, simply creates a single annotation spanning the entire tweet. When processing the training data features are created on each annotation recording the polarity and language of the tweet. -Hashtag Processor: a simple JAPE grammar which recognises hashtags, ensuring that a single hashtag is represented as a single token and a HashTag annotation. -Emoticon Processor: a combination of a gazetteer and a JAPE grammar which recognises emoticons, normalises them (see below) and ensures that they are represented as a single token and an Emoticon annotation. -Part-of-Speech Tagger: a standard GATE PR which assigns part-ofspeech (POS) tags to token annotations.</p><p>The only non-standard GATE component in this pipeline is the emoticon processor. The motivation behind this component is that with limited training data it is vital that we reduce, or eliminate, variation in expression of emoticons which may well be pivitol in conveying polarity. This normalization was performed by building a gazetteer of known emoticons<ref type="foot" coords="3,306.80,549.11,3.97,6.12" target="#foot_8">9</ref> . Each entry in the gazetteer was paired with a normalized form of the emoticon. For example :-), :), and :] are all normalised to :). The JAPE grammar then ensures a single token annotation spans the emoticon using the normalised form rather than the underlying characters.</p><p>The result of running this pre-processing pipeline is a document annotated with a sequence of tokens which can be used by a machine learning algorithm to learn a polarity classifier. The same pipeline is also used to pre-process the unseen tweets before the classifiers are applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">k-Nearest Neighbours Classification</head><p>The standard GATE distribution provides a number of machine learning tools which can be used to perform text classification<ref type="foot" coords="4,347.64,148.93,7.94,6.12" target="#foot_9">10</ref> . For these experiments into polarity classification we used the Batch Learning PR configured to perform k-Nearest Neighbours (k-NN) classification using an implementation from Weka <ref type="bibr" coords="4,462.18,174.41,13.80,8.74" target="#b3">[4]</ref>. Whilst space constraints preclude full details of the implementation (which can be found in <ref type="bibr" coords="4,191.64,198.32,10.79,8.74" target="#b1">[2]</ref>) the algorithm is configured to use the following features for learning: POS tags both 1-gram and 2-gram, emoticons, hashtags and the language of the tweet. Whilst it may seem strange that the words themselves (or at least their root forms) were not used to train the classifier, experimentation showed that including them led to a drop in performance of up to 5%. The reason behind this rather odd result is as yet unclear but may be related to both the small amount of training data (and hence only a small number of words occuring frequently) and the mixture of both English and Spanish text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Naïve Bayes Classification</head><p>Whilst the machine leanring PRs provided with GATE are easy to use and highly configurable we decided to implement a second polarity classifier to allow for more fine grained control over the entire process. We chose to build a Naïve Bayes classifier following the example in <ref type="bibr" coords="4,282.50,366.94,9.96,8.74" target="#b5">[6]</ref>. This approach essentially reduces down to using uni-grams to classify text as the algorithm assumes that the probability of a word occuring is independent of it's position within the document.</p><p>Our specific implementation used lowercased versions of tokens as well as emoticons and hashtags as the input to the learning algorithm. In an attempt to improve classification due to the small amount of text present in the training tweets we also made use of the supporting documents (i.e. pages linked to from tweets). Rather than blindly including these pages in to the training set, the algorithm was tweaked to include the word counts but to ignore the document lengths. This simple change to the Naïve Bayes algorith was made in an attempt to not skew the distributions, especially given that we classified each tweet in the test set using the tweet alone, and these are of a common length (i.e. usually short and never more than 140 characters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>We submitted two runs for the profiling task; each run paired one of the polarity approaches with our disambiguation based approach to relevance filtering; GATE 1 used the k-NN classifier for polarity classification while GATE 2 used the Naïve Bayes classifier. Unfortunately due to an error in the script used to combine the approaches for GATE 1 all tweets were classified as positive with respect to polarity. Whilst we list the results for this run below, we also detail a third submission, GATE 3, which we have evaluated (using the supplied gold standard) Accuracy R S F(R,S) GATE 0.52 0.12 0.13 0.09 all relevant 0.71 0 0 0 Table <ref type="table" coords="5,261.43,150.58,4.13,7.89">1</ref> independently. For comparison we have also included the baseline results. For full details of other submissions and the evaluation metrics used see the RepLab 2012 overview paper <ref type="bibr" coords="5,226.97,327.02,9.96,8.74" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Relevance Filtering</head><p>As we submitted the same relevance judgements for each of our runs Table <ref type="table" coords="5,475.61,377.94,4.98,8.74">1</ref> shows just a single GATE run instead of duplicating the values. Note that the all relevant baseline has an accuracy of 0.71 which shows the large bias within the evaluation set towards relevant tweets. A similar bias can also be found within the training data which results in only a small amount of non-relevant examples. This bias may be a general occurance or may be due to the specific entities chosen for this evaluation, i.e. many of the entities do not actually require disambiguation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Polarity Classification</head><p>Table <ref type="table" coords="5,163.11,512.55,4.98,8.74" target="#tab_0">2</ref> shows the results of our two attempts at polarity classification. Note that as previously mentioned we also report the results of the GATE 3 run, which while not an officially evaluated run, shows how our GATE 1 run should have performed.</p><p>We believe that relatively low performance of our classifiers is due to two things: the small amount of training data used and the difference in language use when expressing opinions across entities of different types. This second problem is probably more relevant than the lack of training data. During development we tested the algorithms using k-fold cross validation in two ways. In both cases we used six folds. One approach used the training data from one entity as a fold, and in the other data from all six entities were randomly split equally between the six folds. The average accuracy of the two approaches showed a difference of around 25%, with better performance being achieved when the folds were Accuracy GATE 1 0.35 GATE 2 0.33 GATE 3 0.34 all relevant and positive 0.27 all relevant and neutral 0.26 all relevant and negative 0.18 Table <ref type="table" coords="6,259.83,194.42,4.13,7.89">3</ref>. Combined Profilling Results generated randomly. We belive that this is due to the fact that none of the six entities in the training data overlap in the products or services they provide and as such the language used to talk about them differs greatly. The six entities were:</p><p>-Alcatel: a provider of backend communications equipment, usually sold to governments or telecommunication companies rather than end-users -Apple: a seller of consumer electronic goods including computers, phones and MP3 players -Armani: a high-end fashion label -Barclays: a British multinational banking and financial services company -Lufthansa: the largest airline in Europe -Marriott: a large chain of hotels and leisure resorts As you can imagine complaining about a late flight (Lufthansa) would use very different language to complaints about short battery life in a consumer electronics product (Apple). This suggests that when building a classifier the training data should contain tweets about a variety of different entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Combined Profiling Performance</head><p>The results of combining our approaches to relevance filtering and polarity classification can be seen in Table <ref type="table" coords="6,272.42,560.48,3.87,8.74">3</ref>. The combined evaluation was carried out by assuming a four class classification: not relevant, relevant and positive, relevant and neutral, and relevant and negative. As you can see from the results tables our combined approach to profilling out performed any of the three baseline systems included for comparison. While a system with accuracy of 0.35 (our best result obtained by the GATE 1 run) can not be considered a strong performer we feel that it provides an ideal basis for our ongoing work in this area. As noted above we have already highlighted a number of areas where the approach could be improved and work is already going on to move the algorithms forward.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,227.04,150.58,161.28,116.51"><head>Table 2 .</head><label>2</label><figDesc>Polarity Classification Results</figDesc><table coords="5,229.00,150.58,157.35,105.60"><row><cell cols="5">. Relevance Filtering Results</cell></row><row><cell></cell><cell cols="4">Accuracy R S F(R,S)</cell></row><row><cell>GATE 1</cell><cell>0.44</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>GATE 2</cell><cell cols="4">0.33 0.27 0.28 0.26</cell></row><row><cell>GATE 3</cell><cell cols="4">0.41 0.25 0.21 0.22</cell></row><row><cell>All positive</cell><cell>0.44</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>All neutral</cell><cell>0.33</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">All negative 0.23</cell><cell>0</cell><cell>0</cell><cell>0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.73,646.49,103.56,7.47"><p>http://www.twitter.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,144.73,657.44,160.05,7.47"><p>http://en.wikipedia.org/wiki/Apple</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,144.73,591.69,183.59,7.47"><p>http://en.wikipedia.org/wiki/Apple_Inc.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,144.73,602.65,188.29,7.47"><p>http://en.wikipedia.org/wiki/Apple_Corps</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="2,144.73,613.61,240.08,7.47"><p>http://en.wikipedia.org/wiki/Apple_(disambiguation)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="2,144.73,623.92,201.85,8.12"><p>Hashtags were expanded via http://tagdef.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="2,144.73,635.53,103.56,7.47"><p>http://linkeddata.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="2,144.73,645.84,335.87,7.86;2,144.73,656.80,130.43,7.86"><p>This also makes sense given that in RepLab 2012 the two tasks are evaluated separately as well as in combination.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="3,144.73,656.80,332.72,8.12"><p>http://en.wikipedia.org/wiki/List_of_emoticons was used as a starting point</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9" coords="4,144.73,656.80,223.00,8.12"><p>see http://gate.ac.uk/userguide/chap:ml for details</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research is partially supported by the <rs type="funder">EU-</rs>funded <rs type="projectName">FP7 TrendMiner</rs> <rs type="grantNumber">11</rs> project. <rs type="person">Kalina Bontcheva</rs> is supported by the <rs type="funder">Engineering and Physical Sciences Research Council</rs> (grant <rs type="grantNumber">EP/I004327/1</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_fK3G8fH">
					<idno type="grant-number">11</idno>
					<orgName type="project" subtype="full">FP7 TrendMiner</orgName>
				</org>
				<org type="funding" xml:id="_PVbWNKH">
					<idno type="grant-number">EP/I004327/1</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,138.35,222.29,342.25,7.86;7,146.91,233.25,333.68,7.86;7,146.91,244.21,159.24,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,395.19,222.29,85.41,7.86;7,146.91,233.25,239.42,7.86">Overview of RepLab 2012: Evaluating Online Reputation Management Systems</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Amigó</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Corujo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">D</forename><surname>Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,410.04,233.25,70.55,7.86;7,146.91,244.21,130.57,7.86">CLEF 2012 Labs and Workshop Notebook Papers</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,255.17,342.24,7.86;7,146.91,266.12,333.68,7.86;7,146.91,277.08,333.68,7.86;7,146.91,288.04,25.60,7.86" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Maynard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bontcheva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Tablan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Aswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Gorrell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Funk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Damljanovic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Greenwood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Saggion</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Petrak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Peters</surname></persName>
		</author>
		<title level="m" coord="7,319.71,277.08,160.88,7.86">Text Processing with GATE (Version 6)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,299.00,342.24,7.86;7,146.91,309.96,282.99,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,279.26,299.00,196.85,7.86">Named Entity Disambiguation using Linked Data</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Damljanovic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Bontcheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,160.99,309.96,240.23,7.86">Proceedings of the 9th Extended Semantic Web Conference</title>
		<meeting>the 9th Extended Semantic Web Conference</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,320.92,342.24,7.86;7,146.91,331.88,333.67,7.86;7,146.91,342.84,333.68,7.86;7,146.91,353.80,25.60,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,445.50,320.92,35.09,7.86;7,146.91,331.88,178.02,7.86">Weka: A machine learning workbench for data mining</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kirkby</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,332.22,331.88,148.37,7.86;7,146.91,342.84,273.72,7.86">Data Mining and Knowledge Discovery Handbook: A Complete Guide for Practitioners and Researchers</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1305" to="1314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,364.75,342.24,7.86;7,146.91,375.71,298.25,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,243.98,364.75,124.22,7.86">Learning to link with wikipedia</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,388.63,364.75,91.96,7.86;7,146.91,375.71,213.33,7.86">Proc. of the 17th Conf. on Information and Knowledge Management (CIKM)</title>
		<meeting>of the 17th Conf. on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="509" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,386.67,266.09,7.86" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<title level="m" coord="7,200.16,386.67,70.68,7.86">Machine Learning</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,397.63,342.24,7.86;7,146.91,408.59,333.68,7.86;7,146.91,419.55,71.69,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,295.04,397.63,185.55,7.86;7,146.91,408.59,61.15,7.86">Entity linking: Finding extracted entities in a knowledge base</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,230.39,408.59,250.20,7.86;7,146.91,419.55,43.03,7.86">Multi-source, Multi-lingual Information Extraction and Summarization</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,138.35,430.51,342.24,7.86;7,146.91,441.47,308.23,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,205.07,430.51,147.56,7.86">An introduction to random indexing</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sahlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,374.88,430.51,105.71,7.86;7,146.91,441.47,179.68,7.86">Proc. of the Methods and Applications of Semantic Indexing Workshop</title>
		<meeting>of the Methods and Applications of Semantic Indexing Workshop<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
