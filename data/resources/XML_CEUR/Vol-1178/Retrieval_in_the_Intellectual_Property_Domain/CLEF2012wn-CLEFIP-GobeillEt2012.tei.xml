<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,144.98,152.75,305.20,12.54;1,253.85,170.75,87.53,12.54">BiTeM site report for the Claims to Passage task in CLEF-IP 2012</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,230.45,210.08,56.25,9.05"><forename type="first">Julien</forename><surname>Gobeill</surname></persName>
							<email>julien.gobeill@hesge.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Information Studies</orgName>
								<orgName type="laboratory">BiTeM group</orgName>
								<orgName type="institution">University of Applied Sciences</orgName>
								<address>
									<settlement>Geneva</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,309.39,210.08,51.95,9.05"><forename type="first">Patrick</forename><surname>Ruch</surname></persName>
							<email>patrick.ruch@hesge.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Information Studies</orgName>
								<orgName type="laboratory">BiTeM group</orgName>
								<orgName type="institution">University of Applied Sciences</orgName>
								<address>
									<settlement>Geneva</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,144.98,152.75,305.20,12.54;1,253.85,170.75,87.53,12.54">BiTeM site report for the Claims to Passage task in CLEF-IP 2012</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DA024B8EBF1013255CB26EF7DF80C013</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Information Retrieval; Intellectual Property 1</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In CLEF-IP 2012, we participated in the Claims to Passage task where the goal was to return relevant passages according to sets of claims, for patentability or novelty search purposes. The collection contained 2.3M of documents, corresponding to an estimated volume of 250M of passages. To cope with the problems induced by this large dataset, we designed a two-step retrieval system. In the first step, the 2.3M of patent application documents were indexed ; for each topic, we then retrieved the k most similar documents with a classical Prior Art Search. Document representations and tuning of the IR engine were set relying on training data and on the expertise we acquired in past similar tasks. In particular, we used not only claims for topics, but also the full description of the application document, and the applicants/inventors details ; moreover, we discarded retrieved documents that didn't share at least one IPC code with the topic. The k parameter ranged from 5 to 1000 according to the computed run. In the second step, for each topic (i.e. "on the fly"), we indexed the passages contained in these k most similar documents and queried with the topic claims in order to obtained the final runs. Thus, we dealt with approximately 11M of passages instead of 250M. The best k parameter with the training data was 10. Hence, we decided to submit four runs with k set to 10, 20, 50, and 100. Finally, we analyzed the training data and observed that the position of a passage in the document played a role, as passages at the end of the description were more likely to be relevant. Thus, we re-ranked each run according to passages' positions in the document in order to submit four supplementary runs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>BiTeM (Bibliomics and Text Mining) is a research group located in Geneva, having a strong expertise on text mining in large corpora, especially in biomedicine. We already took part in several evaluation campaigns on Information Retrieval (IR) in the Intellectual Property domain, such as previous CLEF <ref type="bibr" coords="1,343.63,636.16,10.69,9.05" target="#b0">[1]</ref>, TREC <ref type="bibr" coords="1,380.74,636.16,14.89,9.05" target="#b1">[2]</ref>, or NTCIR <ref type="bibr" coords="1,439.78,636.16,14.74,9.05" target="#b2">[3]</ref>. In CLEF-IP 2012, we participated in the Claims to Passage task where the goal was to return relevant passages from patents contained in the collection according to sets of claims, for patentability or novelty search purposes. This task is known in computer science as Passage Retrieval, a subtask of Information Retrieval. We early identified two different strategies in order to retrieve the relevant passages: either a one-step retrieval, or a two-steps retrieval. The one-step retrieval consists in building a unique search engine by indexing all passages. The two-steps retrieval consists in building a first unique search engine by indexing all documents, then, for each topic (i.e. "on the fly"), building a second search engine by only indexing passages belonging to the retrieved documents. The CLEF-IP 12 collection contained 2.3M of documents, corresponding to an estimated volume of 250M of passages. To cope with the problems induced by this large dataset, we chose the two-steps retrieval strategy, as described in Fig. <ref type="figure" coords="2,315.42,258.11,3.87,9.05" target="#fig_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Strategies</head><p>In the first step, the 2.3M of patent application documents were indexed ; for each topic, we then retrieved the most similar documents with a classical Prior Art Search. Document representation and tuning of the IR engine were set relying on training data and on the expertise we acquired in past similar tasks. For document representation, we used titles, abstracts, claims, applicants and inventors details, and IPC codes (complete format, e.g. G09G 3/28). For topic representation, we exploited the provided patents and also used titles, abstracts, claims, applicants and inventors details, and IPC codes, along with the full description sections. We thus obtained, for each query, a set of retrieved patents. Then, we applied a supplementary post-processing strategy investigated in previous CLEF-IP, by discarding retrieved patents that did not share at least one IPC code with the topic patent.</p><p>In the second step, for each topic, we extracted passages contained in the k first retrieved patents. Then, for each topic, we indexed these passages and queried only with the claims provided in the topic in order to obtain our runs. Relying on training data, we evaluated different values of k, ranging from 5 to 1000.</p><p>Indexing and Retrieval were computed with the Terrier platform <ref type="bibr" coords="3,402.54,174.08,10.95,9.05" target="#b3">[4]</ref>, which is designed for large collections such as TREC or CLEF collections. We chose settings which proved to be efficient in the past competitions: PL2 as weighting scheme, and Bo1 as Query Expansion model, both with default parameters and with Porter stemming <ref type="bibr" coords="3,148.10,222.08,10.69,9.05" target="#b4">[5]</ref>. For multilingual purposes, we simply chose to only index English sections, and to use Google in order to translate the topics from French or German into English.</p><p>Finally, we analyzed the qrel provided with the training data, focusing on the position (within the description section) of the relevant passages. We thus divided, for each patent, the description section into ten equal parts. Then, we analyzed from which part came the passages contained in the qrel, and the passages contained in our run (for k=10). P qrel (i) is the percentage of relevant passages in the qrel that belong to the i-th part of the description, i ranging from 1 (the beginning) to 10 (the end). P run (i) is the percentage of relevant passages in our run that belong to the i-th part of the description Fig. <ref type="figure" coords="3,188.17,330.11,4.14,9.05" target="#fig_1">2</ref> illustrates these distributions. Both distributions are opposite. In the qrel, the passages belonging to the end of the description are more likely to be relevant. On the contrary, our search system tends to favor passages belonging to the beginning of the description. Hence, we computed weights W(i) for each part according to the qrel distribution, by dividing P qrel (i) by P run (i), then we re-ranked our runs by boosting scores according to these weights. This re-ranking strategy is obviously applied only to passages belonging to the description section.</p><p>For evaluations, we computed Mean Reciprocal Rank (MRR), which is the multiplicative inverse of the rank of the first correct returned answer <ref type="bibr" coords="3,379.99,681.16,10.69,9.05" target="#b4">[5]</ref>. Finally, we evaluated the impact of our re-ranking strategy with training data, and observed a slight improvement in terms of MRR, ranging from +2% to +6% according to the value of k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on training data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Hence, we decided to submit four official runs with different values of k: 10, 20, 50 and 100. As participants were allowed to submit up to 8 runs, we applied our reranking strategy to all the mentioned runs in order to obtain four supplementary official runs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,213.53,466.15,168.23,8.18;2,125.05,279.25,345.10,178.20"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The two-steps strategy we investigated.</figDesc><graphic coords="2,125.05,279.25,345.10,178.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,124.70,550.99,346.01,8.18;3,124.70,561.90,175.07,8.19;3,124.70,351.35,345.80,190.95"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Distribution of passages, according to their position in the description section, in the training data qrel and one of our runs (for k=10).</figDesc><graphic coords="3,124.70,351.35,345.80,190.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,124.70,177.07,345.88,181.09"><head>Table 1 .</head><label>1</label><figDesc>First, we evaluated different values of k with the training data. Results are presented in Tab. 1. It appears that the best value for k is 10. k=10 means that, for each query, we retrieved passages only within the 10 first retrieved patents. Results obtained with the training data, in terms of MRR, according to the k value</figDesc><table coords="4,263.33,226.85,60.51,108.15"><row><cell>k</cell><cell>MRR</cell></row><row><cell>5</cell><cell>0.014</cell></row><row><cell>10</cell><cell>0.017</cell></row><row><cell>20</cell><cell>0.01</cell></row><row><cell>50</cell><cell>0.013</cell></row><row><cell>100</cell><cell>0.013</cell></row><row><cell>200</cell><cell>0.007</cell></row><row><cell>500</cell><cell>0.004</cell></row><row><cell>1000</cell><cell>0.003</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="4,132.67,553.03,337.78,8.18;4,141.74,563.95,323.54,8.18" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,303.77,553.03,166.67,8.18;4,141.74,563.95,194.35,8.18">Simple pre and post processing strategies for partent searching in CLEF Intellectual Property Track</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gobeill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Pasche</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Teodoro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ruch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,360.94,563.95,77.78,8.18">Proceedings of CLEF</title>
		<meeting>CLEF</meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,132.67,575.02,337.95,8.18;4,141.74,586.06,328.94,8.18;4,141.74,596.98,324.20,8.18" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,416.23,575.02,54.39,8.18;4,141.74,586.06,93.71,8.18;4,261.51,586.06,209.16,8.18;4,141.74,596.98,212.97,8.18">Impact of Citations Feedback for Patent Prior Art Search and Chemical Compounds Expansion for Ad Hoc Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gobeill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gaudinat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Pasche</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Teodoro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Vishnyakova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ruch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,360.42,596.98,79.01,8.18">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
	<note>BiTeM site report for TREC Chemistry</note>
</biblStruct>

<biblStruct coords="4,132.67,608.02,338.19,8.18;4,141.74,619.06,329.03,8.18;4,141.74,629.98,53.43,8.18" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,303.41,608.02,167.45,8.18;4,141.74,619.06,266.40,8.18">Report on the NTCIR 2010 Experiments: automatic IPC encoding and novelty detection for effective patent mining</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Teodoro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Gobeill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Pasche</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ruch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,415.63,619.06,55.14,8.18;4,141.74,629.98,26.88,8.18">Proceedings of NTCIR</title>
		<meeting>NTCIR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,132.67,641.02,337.78,8.18;4,141.74,652.06,328.83,8.18;4,141.74,662.98,176.43,8.18" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,325.27,641.02,145.18,8.18;4,141.74,652.06,159.09,8.18">Research Directions in Terrier: a Search Engine for Advanced Retrieval on the Web</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Plachouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,308.93,652.06,161.64,8.18;4,141.74,662.98,84.39,8.18">Novatica/UPGRADE Special Issue on Next Generation Web Search</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="49" to="56" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,132.67,674.02,306.36,8.18" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,181.10,674.02,117.28,8.18">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,304.01,674.02,29.04,8.18">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,132.67,685.06,318.97,8.18" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,191.09,685.06,156.30,8.18">Overview of the Question Answering Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,353.53,685.06,69.40,8.18">Proceedings TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
