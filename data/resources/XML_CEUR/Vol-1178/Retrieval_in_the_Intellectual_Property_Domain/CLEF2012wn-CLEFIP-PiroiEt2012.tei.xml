<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,152.68,117.92,309.96,10.51;1,205.62,135.85,204.07,10.51">CLEF-IP 2012: Retrieval Experiments in the Intellectual Property Domain</title>
				<funder ref="#_SJSYU5V">
					<orgName type="full">Austrian Research Promotion Agency (FFG)</orgName>
				</funder>
				<funder ref="#_CkFr8GG">
					<orgName type="full">EU Network of Excellence PROMISE</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,207.49,170.89,55.95,9.96"><forename type="first">Florina</forename><surname>Piroi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Software Technology and Interactive Systems</orgName>
								<orgName type="institution">Vienna University of Technology</orgName>
								<address>
									<addrLine>Favoritenstrasse 9-11</addrLine>
									<postCode>1040</postCode>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.99,170.89,51.33,9.96"><forename type="first">Mihai</forename><surname>Lupu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Software Technology and Interactive Systems</orgName>
								<orgName type="institution">Vienna University of Technology</orgName>
								<address>
									<addrLine>Favoritenstrasse 9-11</addrLine>
									<postCode>1040</postCode>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,335.87,170.89,64.79,9.96"><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Software Technology and Interactive Systems</orgName>
								<orgName type="institution">Vienna University of Technology</orgName>
								<address>
									<addrLine>Favoritenstrasse 9-11</addrLine>
									<postCode>1040</postCode>
									<settlement>Vienna</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,204.26,182.85,58.66,9.96"><forename type="first">Walid</forename><surname>Magdy</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Qatar Computing Research Institute</orgName>
								<orgName type="institution">Qatar Foundation</orgName>
								<address>
									<settlement>Doha</settlement>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.47,182.85,65.74,9.96"><forename type="first">Alan</forename><forename type="middle">P</forename><surname>Sexton</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Birmingham</orgName>
								<address>
									<addrLine>Edgbaston Birmingham</addrLine>
									<postCode>B15 2TT</postCode>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,349.74,182.85,56.91,9.96"><forename type="first">Igor</forename><surname>Filippov</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Chemical Biology Laboratory</orgName>
								<orgName type="institution">SAIC-Frederick, Inc</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Frederick National Lab</orgName>
								<address>
									<postCode>21702</postCode>
									<settlement>Frederick</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,152.68,117.92,309.96,10.51;1,205.62,135.85,204.07,10.51">CLEF-IP 2012: Retrieval Experiments in the Intellectual Property Domain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E6B1454651271617A18751187243C43C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Clef-Ip test collection was rst made available in 2009 to support research in IR methods in the intellectual property domain. Since then several kinds of tasks, reecting various specic parts of patent expert's work ows, have been organized. We give here an overview of the tasks, topics, assessments and evaluations of the Clef-Ip 2012 lab.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The patent system encourages innovation by giving an advantage to people and/or companies that disclose their inventions to the open society. The advantage consists of exclusive rights on the published invention for a limited period of time, usually 20 years. A requirement for obtaining a patent, i.e. exclusive implementation and utilization rights, for an invention is that no similar invention was previously disclosed. Because of the high economic impact of a granted patent it is important that the specic searches during the examination of patent applications are thorough.</p><p>Current rates of technological development has resulted in a large increase in the number of patent applications led with the patent oces around the world. To keep up with the increase in the amount of data, appropriate information retrieval (IR) methods have to be developed and tested. The Clef-Ip test collection gives the IR tool developers a test bed to evaluate the performance of their tools in the intellectual property area (specically patents).</p><p>Since 2009 the Clef-Ip evaluation campaign (2009) and benchmarking labs (2010 and 2011) have posed their participants tasks that reect specic parts of a patent examiner's work-ow: nding prior art of a patent application, classifying the patent application according to the International Patent Classication System, using images in patent searches, classifying images occurring in patents.</p><p>This year there three tasks were organized, each concerning a dierent aspect of the data that can be found in a patent collection: Passage retrieval starting from Claims. The topics in this task were claims in patent application documents. Given a claim or a set of claims, the participants were asked to retrieve relevant documents in the collection and mark out the relevant passages in these documents.</p><p>Flowchart Recognition Task. The topics in this task are patent images representing ow-charts. Participants in this task were asked to extract the information in these images and return it in a predened textual format.</p><p>Chemical Structure Recognition Task. The topics in this task were patent pages in TIFF format. Participants had to identify the location of the chemical structures depicted on these pages and, for a specied subset of those diagrams, return the corresponding structure in a MOL le (a chemical structure le format).</p><p>The ideas behind the Clef-Ip task denition and their organization are instances of the use cases dened in the frame of the PROMISE 1 project.</p><p>The rest of the paper is organized as follows: Section 2 describes the Clef-Ip collection and each of the organized tasks. We detail, for each of the tasks, the idea behind proposing such a task, the sets of topics and their relevance judgements, and the measures used in assessing the retrieval eectiveness. Section 3 presents the results of the evaluation activites for each of the three tasks and a list of participants. We nish with Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The 201Clef-Ip Collection</head><p>This year's collection corpus is the same as the one used in the Clef-Ip 2011 lab, i.e. it contains patent documents derived from European Patent Oce (Epo) and World Intellectual Property Organization (Wipo) sources, stored as Xml les, corresponding to over 1.5 million patents published until 2002. This year's collection does not include the image data used in the Image classication and Image Retrieval task in the 2011 lab <ref type="bibr" coords="2,272.81,478.50,10.51,9.96" target="#b5">[6]</ref> . For a detailed description of the Clef-Ip collection we refer the reader to the previous Clef-Ip overview notes <ref type="bibr" coords="2,451.81,490.45,10.80,9.96" target="#b5">[6,</ref><ref type="bibr" coords="2,462.61,490.45,7.20,9.96" target="#b6">7,</ref><ref type="bibr" coords="2,469.81,490.45,7.20,9.96" target="#b7">8]</ref>. For a description of key terms and steps in a patent's lifecycle see <ref type="bibr" coords="2,427.12,502.41,9.94,9.96" target="#b4">[5]</ref>. To make this paper self contained, we re-state some facts about the Clef-Ip collection.</p><p>In the process of patenting, several, and potentially many, documents are published by a patent oce. The most common ones are the patent application, the search report, and the granted patent. In most of the cases when a patent application is published, the search report is contained in it, otherwise it is published at a later date. Each patent oce has its own identication system to uniquely distinguish the dierent patents. At the Epo and Wipo all patent documents belonging to the same patent are assigned the same numerical identier. All Xml documents in the Clef-Ip collection, according to the common Dtd, contain the following main Xml elds: bibliographic data, abstract, description, and claims. Not all Xml documents actually have content in these elds. For example, an A4 kind document is a supplementary search report and will therefore not usually contain the abstract, claims and description elds. The documents have a document language assigned to them (English, German or French). The main Xml elds can also have one of the three languages assigned to them, which can be dierent from the document language. Some Xml elds can occur more than once with dierent language attributes attached to them. For example, EP patent specication documents (EP-nnnnnnn-B1.xml) must contain claims in three languages (English, German and French).</p><p>We continue this section with a more detailed description of the three tasks organized in Clef-Ip 2012.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Passage Retrieval Starting From Claims</head><p>The importance of the claims in a patent is given by their role in dening the extent of the rights protection an applicant has secured. The decisions taken by patent examiners at patent oceswhen examining a patent applicationrefer to claims in the application document and provides a list of previously published (patent) documents relevant for the application at hand. Furthermore, they often underline passages in these documents to sustain their decisions.</p><p>The idea behind organizing this task is to investigate the support an IR system could give an IP expert in retrieving documents and passages relevant to a set of claims. The topics in this task are sets of claims extracted from actual patent application documents. Participants were asked to return documents in the Clef-Ip 2012 corpus and mark the passages relevant to the topic claims.</p><p>The participants were provided with a set of 51 training topics. Splitting by the document language of the application document where the topic claims appear, 18 topics were in German, 21 in English and 12 in French. For the test set we have created 105 topics, with 35 in each language. Both training and test topics were created manually. We describe below the steps in obtaining the topics and their relevance judgments.</p><p>A list of kind Epo kind codes is listed at https://register.epo.org/espacenet/ help?topic=kindcodes. Kind codes used by the Wipo are listed at http://www.wipo.int/patentscope/en/ wo_publication_information/kind_codes.html Topic Selection. Before actually creating the topics we have rst created a pool of candidate documents out of which we extracted the sets of topic claims. The documents in the pool were patent applications not occurring in the Clef-Ip data corpus (i.e published after 2001), with content in all main Xml elds, and with two to twelve citations in the collection corpus listed in their search report. We have counted only the highly relevant citations (marked with `X' or `Y' on the search reports) leaving out the citations about technological background of the invention (marked with `A' on the search reports). (In the Clef-Ip terminology, we have used thus only highly relevant direct citations occurring in the Clef-Ip corpus <ref type="bibr" coords="4,225.10,225.92,10.31,9.96" target="#b7">[8]</ref>.) In the Clef-Ip collection, there are few patents with a higher number of highly relevant citations, and these are usually very large documents, with a large patent family, and, by manual inspection, looking for candidate topics, dicult to handle. The next step in creating the topics and Fig. <ref type="figure" coords="4,248.08,382.56,4.45,11.45">1</ref>. Extract from a search report. their qrels consisted of examining the search reports to extract sets of claims and their relevant documents together with the passages in the documents. For each highly relevant citation document in the search report and in our collection, we extracted the claim numbers 3 the citation referred to. These formed the sets of claims for a candidate topic. We then looked at the mentioning of relevant passages in the cited document and decided if the candidate topic could be kept in the set of topics or not. Rejecting a topic candidate was done when: relevant documents were referring to gures only; no relevant passage specications were given for the listed citations, or it was mentioned that the whole document is relevant; the search report had the mention `Incomplete search' which usually means that the search done by the patent expert was not done for all claims.</p><p>From one patent application document it was possible to extract several sets of claims as topics, often with completely dierent sets of relevance judgments.</p><p>We illustrate how this step was done by an example: Figure <ref type="figure" coords="4,407.97,598.63,4.98,9.96">1</ref> shows a part of the search report for the EP-13884446 patent application. The numbers on the right hand side represent claim numbers. Two sets of claims can be identied: {1,2,3,7} and {8}. Leaving the references to gures out, there is enough infor-! Claims in patent documents are numbered for ease of reference mation to identify relevant passages in the two relevant citations, WO-0126537 and EP-1101450, so we kept the two sets of claims as topics 4 .</p><p>Concretely, a topic in this task is formulated as: topic_id is the identier of a topic Q0 is a value maintained for historical reasons doc_id is the identier of the patent document in which the relevant passages occur rel_psg_xpath is the XPpath identifying the relevant passage in the doc_id document psg_rank is the rank of the passage in the overall list of relevant passages psg_score is the score of the passage in the (complete) list of relevant passages Only one XPath per line was allowed in the result les. If more passages are considered relevant for a topic, these have to be placed on separate lines. The maximum number of lines in the result les is limited to containing 100 doc_ids when ignoring the XPaths.</p><p>Creating the Relevance Judgments. Both in the topics and in the relevance judgements, reference to the claims and relevant passages is encoded by means of XPaths. Where the search report referred to specic lines rather than paragraphs, we took as relevant the set of paragraphs fully covering those lines. Once the topics chosen, the last step was to actually create the les with the relevance judgements. We did this manually, by matching the passage indications in the search reports with the content of the patent documents and with the content and XPaths stored in the Xml les. To ease this process we have used a system developed in-house, a screenshot of which can be seen in Figure <ref type="figure" coords="5,425.40,563.45,3.88,9.96">2</ref>. We see in Figure <ref type="figure" coords="5,166.19,575.41,4.98,9.96">2</ref> that the qrel generating system has three main areas: a topic description area where, after typing in the patent application document identier (here, EP-1384446-A1), we can assign the topic an identier (unique in the system), we dene the set of claims in the topic, save it, navigate among its relevant documents with the `Prev' and `Next' buttons. " In the end, from the EP-13884446 application document we have extracted ve topics: PSG30 to PSG34.</p><p>Fig. <ref type="figure" coords="6,271.57,652.00,4.45,11.45">2</ref>. Creating the qrels.</p><p>a claim structure area where we display the claims and the claim tree. Also in this area we give a direct link to the application document on the Epo Patent Register server.</p><p>a qrel denition area where individual passages (corresponding to XPaths in the Xml documents) are displayed. Clicking on them will select them to be part of the topic's qrels. For convenience, we provide three buttons by which we can select with one click all of the abstract's, description's or claims' passages. When clicking on the 'Save QREL' button the selected passages are saved in the database as relevant passages for the topic in work.</p><p>Evaluation Measures. The evaluation of the passage retrieval task was carried out on two levels: those of the document and of the passage. The objective of measuring systems retrieval quality at the document level is to evaluate the system performance in retrieving whole relevant patent document. The objective of the passage level evaluation is to measure the system ranking quality of the relevant passages in the relevant patent documents.</p><p>Regarding the document-level evaluation, we focused on recall-oriented retrieval measures. Patent retrieval evaluation score <ref type="bibr" coords="7,355.56,326.68,10.48,9.96" target="#b2">[3]</ref> (Pres), Recall, and Map were used for evaluating the system performance in retrieving the relevant documents to a given topic. The cut-o used for computations was 100 patent documents (not passages!) in the results list.</p><p>At the passage-level we measured system performance for ranking the passages in a given relevant document according to their relevance to the topic. The measures of choice are mean average precision Map and precision. In more detail, the scores are calculated as follows:</p><p>For each document relevant to a given topic, we compute the average precision (AP) and precision. To dierentiate them from the usual average precision and precision measures calculated at topic level, we call them AP at document level, AP(D), and precision at document level, Precision(D). Then, for a certain topic T and a relevant document D i , the average precision at the D i document level is computed by the following equation:</p><formula xml:id="formula_0" coords="7,202.79,497.82,277.80,26.62">AP (D i , T ) = 1 n p (D i , T ) ∑ P recision(r) • rel(r)<label>(1)</label></formula><p>where n p (D i , T ) is the number of relevant passages in the document D i for the topic T , P recision(r) is the precision at rank r in the ranked passage list, and rel(r) is the relevance of the passage, a value in the set {0,1}. The precision at the D i document level for the topic T , P recision(D i , T ), is the percentage of retrieved relevant passages in the list of all retrieved passages for the document D i . We, thus, compute AP(D) and Precision(D) for all the relevant documents of a given topic, and the average of these scores is calculated to get the score of the given topic:</p><formula xml:id="formula_1" coords="7,167.81,642.20,312.78,25.32">AP (D, T ) = ∑ AP (Di,T ) n(T ) , P recision(D, T ) = ∑ P recision(Di,T ) n(T )<label>(2)</label></formula><p>where AP (D, T ) is the average precision per documents for topic T , P recision(D, T ) is the precision per documents for topic T , n(T ) is the number of relevant documents for topic T . Finally, Map(D) and Precision(D) are computed as the mean of AP (D, T ) and P recision(D, T ) across all topics.</p><p>The Map(D) and Precision(D) measures carry similarities with the measures used in INEX evaluation track, the `Relevant in Context' tasks <ref type="bibr" coords="8,407.94,192.93,10.47,9.96" target="#b1">[2]</ref> where instead of sequences of characters we look at XPaths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Flowchart Recognition</head><p>Patent images play an important role in the every-day work of IP specialists by helping them make quick decisions on the relevancy of particular documents to a patent document. The design of the Flow Chart Recognition task in the Clef-Ip lab aims at making the content of the patent images searchable and comparable, a topic of high interest for the IP specialists.</p><p>The topics in this task are patent images representing ow-charts. Participants in this task were asked to extract the information in these images and return it in a predened textual format. The set of training topics contained 50 ow-charts together with their textual representation (the qrels). The set of test topics contains 100 ow-charts. All images are black and white tif les. We were not interested, yet, in the connection between the patent images and the patent documents they occur in.</p><p>Topic Selection Our job in selecting the topics for this task was much easier than in the `Claims to Passage' task. This is due to the fact that in 2011 the Clef-Ip lab had organized an image classication task, where one of the classication classes was ow-charts. Having already a set of images containing ow-charts, we had to browse through them and select images for the topic sets. Since we chose to model the ow-charts as graphs, we left out from the topic selection images with 'wrong' graph data, like, for example, edges ending in the white.</p><p>Creating Relevance Judgments. Once the textual representation of the owcharts was xed, we have manually created the textual representations for each of the topics in the training and test sets. In Figure <ref type="figure" coords="8,357.73,583.72,4.98,9.96" target="#fig_0">3</ref> we can see an example of a ow-chart image and its textual representation. In the textual representation of a ow-chart, MT stands for meta information about the ow-chart (like number of nodes, edges, title), lines starting with NO describe the nodes of the graph, lines starting with DE describe directed edges, while lines starting with UE describe uni-directed edges in the graph. Lines beginning with CO denote comments that are not to be automatically processed. Evaluation Measure Since ow-charts can be modeled as graphs, to assess how good the image recognition is done in this specic task, the main evaluation measure is the graph distance metric based on the mcs, most common subgraph (see <ref type="bibr" coords="9,181.84,417.45,10.68,9.96" target="#b0">[1,</ref><ref type="bibr" coords="9,192.52,417.45,7.12,9.96" target="#b8">9]</ref>). The distance between the topic owchart F t and the submitted owchart F s is computed as:</p><formula xml:id="formula_2" coords="9,216.28,450.40,264.31,23.23">d(F t , F s ) = 1 - |mcs(F t , F s )| |F t | + |F s | -|mcs(F t , F s )|<label>(3)</label></formula><p>where |.| denotes the size of the owchart/graph. In our case, the size of the owchart is the number of edges plus the number of nodes. The distance between the topic and the submitted owcharts is to be computed at three levels: basic: only the owchart structure is taken into consideration, i.e. nodes and edges without type information and without text labels. intermediate: use the owchart structure together with the node types. complete: owchart structure, node types, text labels To actually compute the distance we use an in-house implementation of the McGregor algorithm for computing most common sub-graphs <ref type="bibr" coords="9,406.30,607.63,9.94,9.96" target="#b3">[4]</ref>.</p><p>In the process of developing these evaluations, we have found that the complete evaluation is better served by a combination of node type matching and edit-distance measuring of the text labels. This is because we cannot make a hard-match between the OCRed text provided by the participants and the one in the gold standard. Therefore, we allow the McGregor algorithm to generate all possible largest common sub-graphs, and compute the best, average and distribution of edit-distances between the nodes of each of these sub-graphs and the gold standard. This is unfortunately extremely time consuming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Chemical Structure Recognition</head><p>Given the importance of chemical molecular diagrams in patents, the ability to extract such diagrams from documents and to recognize them to the extent necessary to automatically compare them for similarity or identity to other diagrams is a potentially very powerful approach to identifying relevant claims. This task was divided into two parts, segmentation and recognition; Segmentation For this sub-task, 30 patents were selected, rendered to 300dpi monochrome multipage TIFF images and all chemical molecular diagrams were manually clipped from the images using a bespoke tool. This clipping tool recorded the minimal bounding box size and coordinates of each diagram clipped and the results recorded in a ground-truth comma separated value (CSV) le. The participants were asked to produce their own results CSV le containing this bounding box clip information for each diagram that their systems could identify.</p><p>Another bespoke tool was written to automatically compare the participants' results le with the ground truth le. This identied matches at various tolerance levels, where a match is awarded if every side of a participant's bounding bounding box is within the tolerance number of pixels of the corresponding side of a ground truth bounding box. Evaluation results were calculated for each of a range of tolerances starting at 0 pixels and increasing to the maximum number of pixels that still disallowed any single participant bounding boxes from matching more than one ground truth bounding box. This maximum limit in practice was 55 pixels, or just under 0.5 cm.</p><p>The number of true positive, false positive and false negative matches were counted for each tolerance setting, and from that the precision, recall and F 1measure was calculated.</p><p>Recognition A diagram recognition task requires the participants to take a set of diagrams, analyse them to some recognised format and submit their recognised format les for evaluation. In order to evaluate the results, these submitted format les must be compared to a ground-truth set of format les. Therein lies a dicult problem with respect to the chemical diagram recognition task for patent documents. The currently most complete standard format for chemical diagrams is the MOL le format. This format captures quite well fully specied chemical diagram molecules. However, it has become standard in patent documents to describe whole families or classes of molcules using diagrams that extend standard molecule diagrams with graphical representations of varying structures, called Markush structures. Markush structures cannot be represented in MOL les.</p><p>Given the standard nature of MOL les, there have been a signicant number of research and commercial projects to recognise diagrams with all the features that can be represented by MOL les. However, without standard extensions to MOL les to cope with Markush stuctures, there has been relatively little eort expended in recognising such extended diagrams. With the intention of fostering such eorts, the chemical structure recognition task for Clef-Ip 2012 was designed to expose participants to a relatively small number of the simpler of these extended structures, while also providing a large number of cases fully covered by the current MOL le standard.</p><p>A total of 865 diagram images, called the automatic set, were selected. The diagrams in this set were fully representable in standard MOL les. Evaluation of this set was carried out by automatically comparing the participants submitted MOL les with the ground truth MOL les using the open source chemistry toolbox, OpenBabel. The key tool in this set is the InChi representation (International Chemical Identier). OpenBabel was chosen among other tools oering similar functionality because it is free and available to everyone. The number of correctly matched diagrams (and the percentage correctly matched) were reported for each participant.</p><p>A manual set of 95 images were chosen which contain some amount of variability in their structure and which can only be represented in MOL les by some abuse of the MOL le standard. These can not be automatically evaluated as the OpenBabel system cannot deal with the resulting structures. However, such MOL les can still be rendered to an image format using the MarvinView tool from ChemAxon. Thus it was possibile to carry out the evaluation of this set by manual visual comparison of the original image, the MarvinView generated image of the ground-truth MOL le for the image, and the MarvinView generated image of the participant's submitted MOL le. To this end a bespoke web application was written to enable the organisers and participants to verify the manual visual evaluation.</p><p>It was less than satisfactory to have to carry out the evaluation of this latter set manually, and even more so that we had to exclude from the set structures that appear in patent les but which cannot be rendered from (abused) MOL les using MarvinView. This points strongly to a need in the community to develop either an extension or an alternative to MOL les that can fully support common Markush structures together with the necessary ancillary tools for manipulating, comparing and rendering such structures to images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Submissions and Results</head><p>Table <ref type="table" coords="11,162.64,619.59,4.98,9.96" target="#tab_2">1</ref> gives a list of institutions that submitted experiments to the Clef-Ip lab in 2012. Research collaborations between institutions can be identied by the RunID. Note that two dierent groups from the Vienna University of Technology have each contributed to dierently identied submissions (tuw and lut). 31 13 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation Results</head><p>Claims to Passage As stated in section 2.1, we computed two sets of measures, one at the document level, very similar to the measurements done in the last years, and one at the passage level. To compute measures at the document level we have ignored the passage information in the participants' submissions and kept only the &lt;topic, relevant document, rank&gt; tuples. On these we have computed Pres, Recall and Map, both for the complete set of topics, as well as split on languages. At the passage level, we have computed Map-and Precisionlike measures, by computing passage AP, respectively passage Precision, for each relevant document, then averaging over the topic's relevant documents. The nal scores are obtained by averaging over all queries. The solutions chosen by the submitting participants range from two-step retrieval approaches, namely a document level retrieval in the rst step and a passage level retrieval in the second step (bit and chm) to using Natural Language Processing techniques (lut). The tuw team used a distributed IR system by splitting the Clef-Ip collection by Ipc codes, while the hild team experimented with search types trigram-based searches. All participants have used translation tools on the generated queries.  Flow Chart Recognition Unfortunately, at the time of writing these workshop notes, the execution of the evaluation programm was not closed. We will post the results on the project's website, http://www.ifs.tuwien.ac.at/~clef-ip Chemical Structure Recognition Only one participant, saic, submitted an attempt at the chemical molecular diagram segmentation subtask. They submitted two attempts, both using as input the multi-page ti les provided by the organisers. The dierence was that in one run they used tiffsplit to separate the pages into individual les, while in the other one they used OSRA native le reading/rendering capability. They achieved signicantly better performance on the latter with results presented in Table <ref type="table" coords="13,319.72,631.54,4.98,9.96" target="#tab_3">2</ref> (note that a tolerance of 55 is just under 0.5cm): Both saic and uob submitted result sets (1 and 4 respectively) for the diagram recognition sub-task (Table <ref type="table" coords="13,313.11,655.45,3.88,9.96" target="#tab_4">3</ref>).  Clearly, both groups, unsurprisingly, found the diagrams with varying elements signicantly more challenging than the more standard xed diagrams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Final Observations</head><p>We have described the benchmarking activities done in the frame of the Clef-Ip 2012. One of the main challenges faced by the organizers were obtaining relevance judgments and choosing topics for the `Passage Retrieval Starting from Claims' task. The eort spent on this challenge prompted us to waive an additional pilot task originally proposed for this year, in which we were interested in nding description passages relevant to a claim in a patent application document.</p><p>Another challenge was nding proper measures to assess the eciency of the passage retrieval task as formulated in the Clef-Ip lab and for the `Flow-chart Recognition' task. The proposed measures are to be a starting point for further discussions on what is the best way to assess the eectiveness of these types of information retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="9,181.99,354.38,251.42,11.45;9,199.74,116.11,215.58,224.87"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. A ow-chart image and its textual representation.</figDesc><graphic coords="9,199.74,116.11,215.58,224.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="13,149.71,118.33,330.88,9.96;13,134.76,130.28,345.87,9.96;13,134.76,142.24,17.15,9.96;13,155.65,136.32,15.28,17.86;13,175.72,142.24,304.91,9.96;13,134.76,154.19,328.32,9.96"><head>Figure 4</head><label>4</label><figDesc>Figure4presents the Pres, Map and Recall at the document level, Figure5shows the Precision and Map at the passage levels for the complete set of topics.The tuw participant was left out in the passage level evaluations because they have submitted experiments referring to documents only, and not passages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="13,210.23,416.55,194.94,11.45;13,134.86,193.88,345.20,209.25"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Measures at relevant document level.</figDesc><graphic coords="13,134.86,193.88,345.20,209.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="14,215.15,357.96,185.07,11.45;14,134.97,130.75,345.04,213.79"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Measures at relevant passage level.</figDesc><graphic coords="14,134.97,130.75,345.04,213.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,134.76,610.60,345.92,54.67"><head></head><label></label><figDesc>B1 identies the European patent specication (i.e. text of the granted patent)2 .The EP and WO documents in the Clef-Ip collection are stored in an Xml format and are extracted from the Marec data collection. Marec contains over 19 million patent documents published by the Epo, Wipo, US Patents and Trade Organization and the Japan Patent Oce, storing them in an unied Xml format under one data denition document.</figDesc><table coords="2,134.76,610.60,345.92,54.67"><row><cell>document (kind code A1) of the European patent number 0402531, while EP-</cell></row><row><cell>0402531-</cell></row><row><cell>The dierent types of documents (application, granted patent, additional search reports, etc.) are distinguished by kind codes appended to the numerical iden-</cell></row><row><cell>tier. For example, EP-0402531-A1 is the identier of the patent application</cell></row><row><cell>http://www.promise-noe.eu</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="12,144.25,114.12,326.86,294.84"><head>Table 1 .</head><label>1</label><figDesc>List of participants and runs submitted</figDesc><table coords="12,144.25,138.41,326.86,270.56"><row><cell>RunID</cell><cell>Institution</cell><cell cols="2">Clm Fc Cs</cell></row><row><cell>uob</cell><cell>University of Birmingham, School of Computer</cell><cell>UK</cell><cell>x</cell></row><row><cell></cell><cell>Science</cell><cell></cell><cell></cell></row><row><cell>bit</cell><cell>Univ. of Applied Sciences, Information Studies,</cell><cell>CH x</cell><cell></cell></row><row><cell></cell><cell>Geneva</cell><cell></cell><cell></cell></row><row><cell>chm</cell><cell>Chemnitz University of Technology, Department of</cell><cell>DE x</cell><cell></cell></row><row><cell></cell><cell>Computer Science</cell><cell></cell><cell></cell></row><row><cell>cvc</cell><cell>Computer Vision Center, Universitat Autónoma de</cell><cell>ES</cell><cell>x</cell></row><row><cell></cell><cell>Barcelona</cell><cell></cell><cell></cell></row><row><cell>hild</cell><cell>Univ. Hildesheim, Information Science</cell><cell>DE x</cell><cell></cell></row><row><cell cols="2">humb-inr Humbold Univ., Dept, of German Language and</cell><cell>DE</cell><cell>x</cell></row><row><cell></cell><cell>Linguistics</cell><cell></cell><cell></cell></row><row><cell cols="2">humb-inr INRIA</cell><cell>FR</cell><cell>x</cell></row><row><cell>joann</cell><cell>Joanneum Research, Institute for Information and</cell><cell>AT</cell><cell>x</cell></row><row><cell></cell><cell>Communication Technologies</cell><cell></cell><cell></cell></row><row><cell>lut</cell><cell>University of Lugano</cell><cell>CH x</cell><cell></cell></row><row><cell>tuw</cell><cell>Univ. of Macedonia, Department of Applied</cell><cell>GR x</cell><cell></cell></row><row><cell></cell><cell>Informatics, Thessaloniki</cell><cell></cell><cell></cell></row><row><cell>saic</cell><cell>Chemical Biology Laboratory, SAIC-Frederick Inc.</cell><cell>US</cell><cell>x</cell></row><row><cell>lut</cell><cell>Vienna University of Technology, Inst. for Software</cell><cell>AT x</cell><cell></cell></row><row><cell></cell><cell>Technology and Interactive Systems</cell><cell></cell><cell></cell></row><row><cell>tuw</cell><cell>Vienna University of Technology, Inst. for Software</cell><cell>AT x</cell><cell></cell></row><row><cell></cell><cell>Technology and Interactive Systems</cell><cell></cell><cell></cell></row><row><cell>tuw</cell><cell>Univ. of Wolverhampton, School of Technology</cell><cell>UK x</cell><cell></cell></row><row><cell></cell><cell>Total:</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="14,176.14,395.72,263.12,87.05"><head>Table 2 .</head><label>2</label><figDesc>Chemical molecular diagram segmentation results.</figDesc><table coords="14,235.09,418.37,145.18,64.41"><row><cell cols="2">Tolerance Precision Recall</cell><cell>F1</cell></row><row><cell>0</cell><cell cols="2">0.70803 0.68622 0.69696</cell></row><row><cell>10</cell><cell cols="2">0.79311 0.76868 0.78070</cell></row><row><cell>20</cell><cell cols="2">0.82071 0.79543 0.80787</cell></row><row><cell>40</cell><cell cols="2">0.86696 0.84025 0.85340</cell></row><row><cell>55</cell><cell cols="2">0.88694 0.85962 0.87307</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="14,136.56,529.82,345.38,107.03"><head>Table 3 .</head><label>3</label><figDesc>Chemical diagram recognition results.</figDesc><table coords="14,136.56,557.17,345.38,79.69"><row><cell></cell><cell cols="7">Automatic Set #Structures Recalled % #Structures Recalled % #Structures Recalled % Manual Set Total</cell></row><row><cell>saic</cell><cell>865</cell><cell>761 88%</cell><cell>95</cell><cell>38</cell><cell>40%</cell><cell>960</cell><cell>799 83%</cell></row><row><cell>uob-1</cell><cell>865</cell><cell>832 96%</cell><cell>95</cell><cell>44</cell><cell>46%</cell><cell>960</cell><cell>876 91%</cell></row><row><cell>uob-2</cell><cell>865</cell><cell>821 95%</cell><cell>95</cell><cell>56</cell><cell>59%</cell><cell>960</cell><cell>877 91%</cell></row><row><cell>uob-3</cell><cell>865</cell><cell>821 95%</cell><cell>95</cell><cell>44</cell><cell>46%</cell><cell>960</cell><cell>865 90%</cell></row><row><cell>uob-4</cell><cell>865</cell><cell>832 96%</cell><cell>95</cell><cell>54</cell><cell>57%</cell><cell>960</cell><cell>886 92%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments This work was partly supported by the <rs type="funder">EU Network of Excellence PROMISE</rs> (<rs type="grantNumber">FP7-258191</rs>) and the <rs type="funder">Austrian Research Promotion Agency (FFG)</rs> <rs type="projectName">FIT-IT project IMPEx</rs> # (No. <rs type="grantNumber">825846</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_CkFr8GG">
					<idno type="grant-number">FP7-258191</idno>
				</org>
				<org type="funded-project" xml:id="_SJSYU5V">
					<idno type="grant-number">825846</idno>
					<orgName type="project" subtype="full">FIT-IT project IMPEx</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="15,138.34,408.68,342.36,9.22;15,146.91,419.64,286.59,9.22" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="15,283.63,408.68,197.07,9.22;15,146.91,419.64,71.97,9.22">A graph distance metric based on the maximal common subgraph</title>
		<author>
			<persName coords=""><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kim</forename><surname>Shearer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,227.33,422.32,109.24,5.64">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page">255259</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,138.34,430.45,342.30,9.22;15,146.91,441.41,333.74,9.22;15,146.91,455.05,333.71,5.64;15,146.91,466.01,333.68,5.64;15,146.91,474.28,325.57,9.22" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="15,194.90,441.41,120.83,9.22">Inex 2007 evaluation measures</title>
		<author>
			<persName coords=""><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jovan</forename><surname>Pehcevski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriella</forename><surname>Kazai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mounia</forename><surname>Lalmas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,335.47,444.09,145.18,5.64;15,146.91,455.05,333.71,5.64;15,146.91,466.01,45.32,5.64">Focused Access to XML Documents, 6th International Workshop of the Initiative for the Evaluation of XML Retrieval, INEX 2007</title>
		<title level="s" coord="15,210.64,476.96,140.49,5.64">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Dagstuhl Castle, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">December 17-19, 2007. 2008</date>
			<biblScope unit="volume">4862</biblScope>
			<biblScope unit="page">2433</biblScope>
		</imprint>
	</monogr>
	<note>Selected Papers</note>
</biblStruct>

<biblStruct coords="15,138.34,485.10,342.28,9.22;15,146.91,496.05,227.20,9.22" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="15,273.73,485.10,206.90,9.22;15,146.91,496.05,132.91,9.22">PRES: A score metric for evaluating recall-oriented information retrieval applications</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Magdy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,299.29,498.73,46.49,5.64">SIGIR 2010</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,138.34,506.87,342.28,9.22;15,146.91,517.82,228.61,9.22" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="15,234.65,506.87,245.97,9.22;15,146.91,517.82,57.06,9.22">Backtrack search algorithms and the maximal common subgraph problem</title>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">J</forename><surname>Mcgregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="15,212.46,520.50,82.75,5.64">Softw., Pract. Exper</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2334</biblScope>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,138.34,528.64,342.33,9.22;15,146.91,539.59,333.71,9.22;15,146.91,553.23,333.69,5.64;15,146.91,561.51,333.68,9.22;15,146.91,572.47,264.15,9.22" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="15,297.53,528.64,183.14,9.22;15,146.91,539.59,96.63,9.22">Eects of Language and Topic Size in Patent IR: An Empirical Study</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Piroi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="15,265.81,542.27,214.81,5.64;15,146.91,553.23,333.69,5.64;15,146.91,564.19,75.83,5.64">Information Access Evaluation. Multilinguality, Multimodality, and Visual Analytics, Third International Conference of the CLEF Initiative, CLEF 2012</title>
		<title level="s" coord="15,157.40,575.15,140.49,5.64">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012-09-20">September 17-20, 2012. September 2012</date>
			<biblScope unit="volume">7488</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,138.34,583.28,342.27,9.22;15,146.91,594.24,186.98,9.22" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Piroi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zenz</surname></persName>
		</author>
		<title level="m" coord="15,347.20,583.28,133.41,9.22;15,146.91,594.24,112.69,9.22">CLEF-IP 2011: Retrieval in the intellectual property domain</title>
		<imprint>
			<date type="published" when="2011-09">September 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="15,138.34,605.05,342.28,9.22;15,146.91,616.01,333.71,9.22;15,146.91,626.97,333.77,9.22;15,146.91,637.93,88.02,9.22;15,137.51,653.64,3.65,7.30;15,144.73,655.86,145.82,9.41" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="15,231.41,605.05,249.21,9.22;15,146.91,616.01,45.86,9.22">CLEF-IP 2010: Retrieval experiments in the intellectual property domain</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Piroi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tait</surname></persName>
		</author>
		<idno>IRFTR201000005</idno>
		<ptr target="http://www.joanneum.at/?id=3922" />
		<imprint>
			<date type="published" when="2010-09">September 2010</date>
			<pubPlace>Vienna</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Information Retrieval Facility</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>Also available as a Notebook Paper of the CLEF 2010 Informal Proceedings</note>
</biblStruct>

<biblStruct coords="16,138.34,118.89,342.22,9.22;16,146.91,129.85,333.70,9.22;16,146.91,140.81,333.71,9.22;16,146.91,154.44,333.72,5.64;16,146.91,162.72,264.09,9.22" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="16,323.16,118.89,157.40,9.22;16,146.91,129.85,145.59,9.22">CLEF-IP 2009: Retrieval Experiments in the Intellectual Property Domain</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Roda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Tait</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Piroi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Zenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="16,321.25,143.49,159.37,5.64;16,146.91,154.44,333.72,5.64;16,146.91,165.40,77.46,5.64">Multilingual Information Access Evaluation I. Text Retrieval Experiments 10th Workshop of the Cross-Language Evaluation Forum, CLEF 2009</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Di Nunzio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kurimo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Mostefa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Penas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Roda</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6241</biblScope>
			<biblScope unit="page">385409</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="16,138.34,173.68,342.28,9.22;16,146.91,184.64,287.11,9.22" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="16,414.62,173.68,66.00,9.22;16,146.91,184.64,71.16,9.22">Graph distances using graph union</title>
		<author>
			<persName coords=""><forename type="first">Walter</forename><forename type="middle">D</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Shoubridge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miro</forename><surname>Kraetzl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="16,226.31,187.32,109.23,5.64">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6/7</biblScope>
			<biblScope unit="page">701704</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
