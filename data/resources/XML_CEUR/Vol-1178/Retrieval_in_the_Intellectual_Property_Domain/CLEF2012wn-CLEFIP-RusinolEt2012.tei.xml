<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,155.07,116.95,305.21,12.62;1,183.74,134.89,247.87,12.62">CVC-UAB&apos;s participation in the Flowchart Recognition Task of CLEF-IP 2012</title>
				<funder ref="#_rNYB2yM">
					<orgName type="full">Generalitat de Catalunya</orgName>
				</funder>
				<funder ref="#_HqZXW2F #_M2DFxPZ #_3a43wSE #_RfHKgkU">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_b3sQEWD">
					<orgName type="full">Spanish Ministry of Education and Science</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,145.02,172.56,64.68,8.74"><forename type="first">Marçal</forename><surname>Rusiñol</surname></persName>
						</author>
						<author>
							<persName coords="1,217.47,172.56,98.31,8.74"><forename type="first">Lluís-Pere</forename><surname>De Las Heras</surname></persName>
						</author>
						<author>
							<persName coords="1,323.69,172.56,70.23,8.74"><roleName>Oriol</roleName><forename type="first">Joan</forename><surname>Mas</surname></persName>
							<email>jmas@cvc.uab.cat</email>
						</author>
						<author>
							<persName coords="1,397.25,172.56,68.61,8.74"><forename type="first">Ramos</forename><surname>Terrades</surname></persName>
						</author>
						<author>
							<persName coords="1,149.33,184.51,93.95,8.74"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
						</author>
						<author>
							<persName coords="1,251.23,184.51,53.76,8.74"><forename type="first">Anjan</forename><surname>Dutta</surname></persName>
							<email>adutta@cvc.uab.cat</email>
						</author>
						<author>
							<persName coords="1,313.08,184.51,69.85,8.74"><forename type="first">Gemma</forename><surname>Sánchez</surname></persName>
						</author>
						<author>
							<persName coords="1,410.28,184.51,55.74,8.74"><forename type="first">Josep</forename><surname>Lladós</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Vision Center</orgName>
								<orgName type="department" key="dep2">Dept. Ciències de la Computació</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Edifici O</orgName>
								<orgName type="institution">Universitat Autònoma de Barcelona</orgName>
								<address>
									<postCode>08193</postCode>
									<settlement>Bellaterra (Barcelona)</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,155.07,116.95,305.21,12.62;1,183.74,134.89,247.87,12.62">CVC-UAB&apos;s participation in the Flowchart Recognition Task of CLEF-IP 2012</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C1487C9FD9384A2CA424572B6C4015D5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Flowchart Recognition</term>
					<term>Text/Graphics Separation</term>
					<term>Rasterto-Vector Conversion</term>
					<term>Symbol Recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The aim of this document is to describe the methods we used in the flowchart recognition task of the CLEF-IP 2012 track. The flowchart recognition task consisted in interpreting flowchart linedrawing images. The participants are asked to extract as much as structural information in these images as possible and return it in a predefined textual format for further processing for the purpose of patent search. The Document Analysis Group from the Computer Vision Center (CVC-UAB) has been actively working on Graphics Recognition for over a decade. Our main aim in participating in the CLEF-IP flowchart recognition task is to test our graphics recognition architectures on this type of graphics understanding problem. Our recognition system comprises a modular architecture where modules tackle different steps of the flowchart understanding problem. A text/graphic separation technique is applied to separate the textual elements from the graphical ones. An OCR engine is applied on the text layer while on the graphical layer identify with nodes and edges as well as their relationships. We have proposed two different families of node and edge segmentation modules. One dealing with the raw pixel data and another working in the vectorial domain. The locations of nodes identified are fed to the recognizer module which is in charge of categorizing the node's type. We have proposed two different node descriptors for the recognizer module. The module analyzing the edges is analysing the connections between nodes and categorizes the edge style. Finally, a post-processing module is applied in order to correct some syntactic errors. We have submitted four different runs by combining the two variants of the segmentation module together with the two variants of the recognition module.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graphics Recognition can be defined as the branch of document analysis that focuses on the recovery of graphical information from documents. Graphical documents such as engineering drawings, diagrams, architectural floor plans, maps, etc. strongly depend on diagrammatic notations defined in terms of a visual language. A Graphics recognition system can be structured in three levels. First, a lexical level aims to extract basic infomation components such as graphical primitives (straight lines, arcs, solid areas) and text zones. The second level, the syntactic level, is related to the structure, i.e. how graphical entities are constructed. It can be modeled by a grammar describing the valid instances of diagrams according to the notation. The recognition involved in the syntactic level locates the graphical entities and classifies them into various classes by their shape and context. Finally, a functional or semantic level defines what the syntactically recognized entities mean in the context where they appear.</p><p>In this paper, we describe the flowchart recognition system that we proposed in the context of the Intellectual Property (IP) track at CLEF 2012. The IP track was first organized in 2009 as a prior art retrieval task. In 2010 and 2011, the track continued as a benchmarking activity of the CLEF conference and in 2011 two image-based tasks were added. One devoted to find patent documents relevant to a given patent document containing images and another aimed at categorizing given patent images into predefined categories of images (such as graph, flowchart, drawing, etc.). In CLEF-IP 2012 a new image-based task was proposed. The flowchart recognition task deals with the interpretation of flowchart line-drawing images. The participants are asked to extract as much structural information as possible in these images and return it in a predefined textual format for further processing for the purpose of patent search.</p><p>The Document Analysis Group from the Computer Vision Center (CVC-UAB) has been actively working on the Graphics Recognition research topic and in this notebook paper, we describe our submitted flowchart recognition system. The rest of this paper has been organized as follows. We detail in Section 2 the architecture overview and summarize the submitted runs. Section 3 details each of the modules that comprises the system's pipeline. In section 4 we present and analyze the obtained results and finally in section 5 we draw our concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Architecture Overview</head><p>The architecture of our recognition system has been designed as follows. As we can see in Fig. <ref type="figure" coords="2,202.50,560.31,3.87,8.74" target="#fig_0">1</ref>, the system pipeline has been structured in separate modules dealing with the different steps of the flowchart identification problem.</p><p>First of all, a text/graphic separation module has been applied to separate the textual elements from the graphical ones. An OCR engine has been applied on the text layer while on the graphical layer we analyze the nodes and edges. For nodes/edge segmentation we have applied two different strategies resulting in two alternative segmentation modules: a pixel-based and a vector-based approach. The vector-based approach requires a conversion module that transforms the raw pixel image into a vectorial representation. The output of the node segmentation module is a list of bounding-boxes of the detected nodes. These locations were subsequently fed to the recognizer module which is in charge of establishing the node's type. Here we have used two different node descriptors, namely a descriptor based on the geometric moments and the Blurred Shape Model (BSM) descriptor <ref type="bibr" coords="3,310.20,300.25,9.96,8.74" target="#b0">[1]</ref>, resulting in two alternatives for the node recognition module. The module analyzing the edges is devoted to assess which nodes are connected and classify the edges in terms of their style..Finally, a post-processing module has been applied in order to correct certain syntactic errors.</p><p>The combination of the two alternative segmentation modules with the two alternative recognition modules results to four system variants producing the four submitted runs summarized in Table <ref type="table" coords="3,319.32,384.04,3.87,8.74" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Module Description</head><p>Let us detail in this Section each of the modules composing our architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Text/Graphics Separation</head><p>The first step in the proposed flowchart recognition architecture is to separate text elements from graphical elements. Within the Document Image Analysis research field, various text/graphics separation algorithms have been developed over many years. One of the most used methodologies that yields acceptable results in a variety of mixed-type documents is the algorithm proposed by Tombre et al. in <ref type="bibr" coords="3,171.85,657.11,10.52,8.74" target="#b5">[6]</ref> based on the well-known approach of Fletcher and Kasturi in <ref type="bibr" coords="3,456.39,657.11,9.96,8.74" target="#b1">[2]</ref>. This module proceeds as follows, given an input image, features are extracted from connected components (height and width ratios, orientation, etc.). Then, several adaptive thresholds determine whether a connected component corresponds to a graphical element or a textual element. The connected components that the system is not certain about are assigned to an undetermind layer via a rejection criterion. The output of the module is an image consisting of three separate layers, namely the graphical, the textual and the undetermined one. We can see an example of the results obtained by this text/graphics separation module in Fig. <ref type="figure" coords="5,201.88,215.63,3.87,8.74" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Raster-to-vector Conversion</head><p>Some of the submitted runs are based on the analysis of vectorial data instead of working on the raw image domain. Thus we need a module that converts the raster image into a vectorial representation. We have used the raster-to-vector conversion described in <ref type="bibr" coords="5,240.24,302.41,10.52,8.74" target="#b4">[5]</ref> by Tombre et al. This algorithm first computes the skeleton of the image by means of a distance transform. Chains of pixels are then polygonally approximated by using the algorithm proposed by Rosin and West in <ref type="bibr" coords="5,147.14,338.28,10.52,8.74" target="#b3">[4]</ref> which is based on a recursive split-and-merge technique. We can see an example of the obtained results after the raster-to-vector conversion in Fig. <ref type="figure" coords="5,466.55,350.23,3.87,8.74" target="#fig_2">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Node Segmentation</head><p>We have submited two different alternatives for the node segmentation module depending on the primitives used to segment nodes and edges from the flowchart images. On one hand we have two system variants based on a node segmentation module that works with the raw pixels from the graphical layer and on the other hand we propose another segmentation based on the analysis of vectors arisen from the raster-to-vector conversion of the graphical layer. In both cases, the segmentation strategy first attempted to segment symbol nodes (i.e. rectangles, ovals, diamonds, circles, etc.) and then tried to segment the no-box nodes, corresponding to text which is not enclosed by any graphical entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pixel-based Node Segmentation</head><p>The node segmentation module takes as input the graphical layer obtained through the text/graphics separation module and outputs a list of boundingboxes where the nodes might be found. The pixel-based approach analyses connected components (CCs) to determine whether they might be a node or not. A CC is a maximal set of spatially connected pixels that share a common property (here, the same colour). In the current system connected components of interest are the ones corresponding to the white area inside the nodes and the main problem is to discriminate those from connected components corresponding to the background. After a preliminary step where very small and very large CCs are filtered, the remaining CCs are labeled as node candidates. Here, not all the remaining components correspond to nodes since the white areas produced by loops formed by edges connecting nodes are also detected as candidates. In order to discriminate the real node components from the rest we set device heuristics over a couple of features:</p><p>-Solidity: computed as the ratio between the number of pixels in the CCs and the area of the convex hull of the CCs. Since nodes tend to be convex, objects below a solidity threshold are rejected. -Vertical Symmetry: computed as the ratio between the amount of pixels in the right and the left part of the CCs. Since nodes tend to be vertically symmetric, objects below a symmetry threshold are rejected.</p><p>The remaining CCs after this filtering are considered as nodes and the boundingbox of each element is provided. Going back to the textual layer, once the symbol nodes have been detected, we can easily extract the no-box nodes as the text clusters that do not fall within any detected symbol node. We can see an example of the node segmentation output in Fig. <ref type="figure" coords="6,334.99,520.71,13.16,8.74" target="#fig_3">4 b</ref>). This segmentation strategy has been used in the runs R1 and R2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vectorial-based Node Segmentation</head><p>Distinctively from pixel-based segmentation, the Vectorial-based Node Segmentation takes as input the vectorial representation of the images obtained after the Raster-to-vector Conversion, see Fig. <ref type="figure" coords="6,344.97,609.29,13.22,8.74" target="#fig_2">3 b</ref>). It is based on the exploration of loops in the planar graphs obtained from the vectorial images. In these graphs, nodes are the vectorial lines and edges are the connection points between these lines. The seek for loops is driven by the implementation of the optimal algorithm for finding regions in a planar graph from Jiang et al. in <ref type="bibr" coords="6,424.78,657.11,9.96,8.74" target="#b2">[3]</ref>. As in the pixel-based segmentation, the heuristic processes based on Solidity and Symmetry presented above are followed to rule out inconsistent instances. Finally, the system outputs the bounding-boxes for the remaining nodes. The no-box nodes are determined by looking at textual clusters connected to shape nodes via an edge. This approach is implemented in runs R3 and R4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Node Recognition</head><p>The module devoted to recognize the nodes' types takes as input any of the two proposed node segmentations and tries to classify the node to one of the possible classes based on its shape. We have proposed two different shape recognizers.</p><p>Pattern recognition systems usually require shape descriptors invariant to rigid (scale, translation, rotation) and even affine transforms. However, in the context of flowchart recognition, the shape descriptors needed to recognize the nodes' type just need to be invariant to translation and scale, while a lack of invariance to other transforms is actually beneficial as it results to increased discrimination.</p><p>Translation invariance is not an issue since both node segmentations localised node areas. Both shape descriptors proposed are invariant to scale. In order to assign a node label a nearest neighbor classifier is used over a training set of labeled nodes. Let us detail the two different descriptors we used.</p><p>The Blurred Shape Model descriptor (BSM) was originally created to perform handwritten musical score recognition but it has also been applied to other related document analysis with different degrees of success <ref type="bibr" coords="7,421.41,414.04,9.96,8.74" target="#b0">[1]</ref>. The BSM descriptor is a zoning-based descriptor. Shapes are divided into a 15 × 15 regular grid. Then, the area and the center of gravity are computed for each cell. The final BSM descriptor is constructed by weighting the areas computed by the inverse of distances between two gravity centers of adjacent cells. This weighted average is performed in order to achieve robustness to local variations of the shape under analysis. A normalization by the object area is used in order to achieve invariance to scale. This shape recognizer is used in the runs R2 and R4.</p><p>Geometric Moments have been widely used as shape descriptors since lower order moments represent certain well known fundamental geometric properties of the underlying image functions. The central (p + q)-th order moment for a digital image I(x, y) is expressed by</p><formula xml:id="formula_0" coords="7,237.09,585.03,243.51,21.69">µ pq = x,y (x -x) p (y -ȳ) q I(x, y)<label>(1)</label></formula><p>The use of the centroid (x, ȳ) allow the descriptor to be invariant to translation. A normalization by the object area is used to achieve invariance to scale.</p><formula xml:id="formula_1" coords="7,236.21,645.87,244.39,24.39">η pq = µ pq µ γ 00 where γ = p + q 2 + 1<label>(2)</label></formula><p>We have used the geometric moments up to third order as a feature vector. This shape recognizer is implemented in the runs R1 and R3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Edge Segmentation and Node Linking</head><p>Edge segmentation has been done in a similar fashion in both the pixel-based and the vectorial-based approaches. Taking as input both the graphical layer and the bounding-boxes from the node segmentation we obtain an edge layer image. We can see an example in Fig. <ref type="figure" coords="8,302.66,216.55,3.87,8.74" target="#fig_3">4</ref>. In order to assess which nodes are connected by an edge we pair-wisely select connected components in the node layer and check whether any element of the edge layer provokes that those two disjoint components merge into a single one. An analysis of the edge stroke's width is performed to differentiate between directed and undirected edges. Since our node linking modules strongly depend on the extraction of connected components, they fail at detecting the dotted edges. The pixel-based approach fails if the edge is broken by some text but the vectorial-based one performs a post-processing step that merges colinear broken edges making it robust to such situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Text Recognition</head><p>The module dealing with text recognition receives as input the textual layer obtained by the text/graphics separation module and the bounding-boxes arisen from the node segmentation module. The contents of each bounding-box are processed by the commercial OCR from ABBYY 1 , and no further post-processing steps are applied. Taking a look at the raw output from the OCR, some simple steps for dealing with out-of-vocabulary elements would drastically improve the performance of this module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Post-processing</head><p>Testing our architecture against the training data we realized that when two edges collide, in the ground-truth we expect to see a new node of type point. Obviously our node segmentation and recognition do not tackle such nodes since they can not be found via a connected component analysis. We decided to include the detection of those nodes in a post-processing step.</p><p>The final graph is syntactically analyzed and when we find three or more nodes that are connected through exactly the same edge element, we add a new intermediate node of type point. This procedure is iteratively done since there are no node junctions with cardinality higher than two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>Let us first evaluate the proposed system's performance qualitatively by looking at specific cases where the we face some problems. Let us then present the obtained results. First of all, the node and edge segmentations are based on connected components. When either a node (Fig. <ref type="figure" coords="10,295.42,131.95,8.86,8.74">5a</ref>)) or an edge (Fig. <ref type="figure" coords="10,395.09,131.95,9.13,8.74">5b</ref>)) is broken it is usually not well segmented by any of the proposed methods. In addition, lowquality documents are hard to "read" by the OCR engine (Fig. <ref type="figure" coords="10,423.94,155.86,7.97,8.74">5c</ref>)). Finally, the text/graphics separation module is also based on the analysis of connected components, so when text characters overlap with graphical elements, they are not properly segmented. This is the case shown in Fig. <ref type="figure" coords="10,374.62,191.72,8.58,8.74">5d</ref>), where the character F of FROM and the character D from FIELD touch the diamond shape and thus are classified as graphics and assigned to the graphical layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we presented the flowchart recognition system that the Document Analysis Group from the Computer Vision Center (CVC-UAB) has submitted in the context of the Intellectual Property (IP) track at CLEF 2012. The architecture of our recognition system is structured in different modules dealing with different steps of the flowchart identification problem.</p><p>First of all, a text/graphic separation technique has been applied to separate the textual elements from the graphical ones. An OCR engine has been applied on the text layer while on the graphical layer we analyze the nodes and edges. For nodes/edge segmentation we have applied two different strategies: a pixel-based and a vector-based approach. The node locations have been fed to the recognizer module which is in charge of categorizing the node's type. Here we have used two different node descriptors, namely a descriptor based on the geometric moments and the Blurred Shape Model (BSM) descriptor. The module analyzing the edges determines which nodes are connected and categorizes the edge style. The combination of the two segmentation modules together with the two recognition modules results to the four system variants and an equal number of submitted runs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,228.88,218.49,157.59,7.89"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. System's architecture overview.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,134.77,528.88,345.83,7.89;4,134.77,539.86,200.79,7.86;4,219.83,374.94,86.42,128.15"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example of the text/graphics separation module. a) Original image, b) graphical layer, c) textual layer, d) undetermined layer.</figDesc><graphic coords="4,219.83,374.94,86.42,128.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,134.77,537.43,345.82,7.89;5,134.77,548.42,205.98,7.86;5,219.83,383.50,86.42,128.15"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Example of the raster-to-vector conversion module. a) Original image, b) vectorial representation applied to the graphical layer.</figDesc><graphic coords="5,219.83,383.50,86.42,128.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,134.77,401.64,345.82,7.89;8,134.77,412.62,101.68,7.86;8,175.20,247.71,86.42,128.15"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Example of the node and edge segmentation modules. a) Original image, b) node layer, b) edge layer.</figDesc><graphic coords="8,175.20,247.71,86.42,128.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,134.77,644.57,345.83,7.89;9,134.77,655.55,108.38,7.86"><head>4. 1 Fig. 5 .</head><label>15</label><figDesc>Fig. 5. Problematic cases. a) Broken node, b) broken edge, c) Low-quality text, d) Text/graphics overlapping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,160.05,413.81,295.26,56.50"><head>Table 1 .</head><label>1</label><figDesc>Submitted runs produced by the four different system variants.</figDesc><table coords="3,163.06,437.05,289.23,33.27"><row><cell>Id Run</cell><cell>Segmentation</cell><cell>Descriptor</cell></row><row><cell>R1 IP-FR-CLEF2012.CVC-UAB.GMOMENTS</cell><cell>Pixel-based</cell><cell>Geometric moments</cell></row><row><cell>R2 IP-FR-CLEF2012.CVC-UAB.BSM</cell><cell>Pixel-based</cell><cell>BSM</cell></row><row><cell cols="3">R3 IP-FR-CLEF2012.CVC-UAB.VECTORIALGMOMENTS Vectorial-based Geometric moments</cell></row><row><cell>R4 IP-FR-CLEF2012.CVC-UAB.VECTORIALBSM</cell><cell cols="2">Vectorial-based BSM</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="8,144.73,657.79,271.92,8.12"><p>ABBYY Finereader Engine 10 "http://www.abbyy.com/ocr_sdk/"</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work has been partially supported by the <rs type="funder">Spanish Ministry of Education and Science</rs> under projects <rs type="grantNumber">RYC-2009-05031</rs>, <rs type="grantNumber">TIN2011-24631</rs>, <rs type="grantNumber">TIN2009-14633-C03-03</rs>, <rs type="projectName">Consolider Ingenio</rs> <rs type="grantNumber">2010</rs>: <rs type="projectName">MIPRCV</rs> (<rs type="grantNumber">CSD200700018</rs>) and the grant <rs type="grantNumber">2009-SGR-1434</rs> of the <rs type="funder">Generalitat de Catalunya</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_b3sQEWD">
					<idno type="grant-number">RYC-2009-05031</idno>
				</org>
				<org type="funding" xml:id="_HqZXW2F">
					<idno type="grant-number">TIN2011-24631</idno>
				</org>
				<org type="funded-project" xml:id="_M2DFxPZ">
					<idno type="grant-number">TIN2009-14633-C03-03</idno>
					<orgName type="project" subtype="full">Consolider Ingenio</orgName>
				</org>
				<org type="funded-project" xml:id="_3a43wSE">
					<idno type="grant-number">2010</idno>
					<orgName type="project" subtype="full">MIPRCV</orgName>
				</org>
				<org type="funding" xml:id="_RfHKgkU">
					<idno type="grant-number">CSD200700018</idno>
				</org>
				<org type="funding" xml:id="_rNYB2yM">
					<idno type="grant-number">2009-SGR-1434</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,138.35,603.26,342.25,7.86;10,146.91,614.22,333.68,7.86;10,146.91,625.18,172.95,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,449.96,603.26,30.63,7.86;10,146.91,614.22,240.38,7.86">Blurred shape model for binary and grey-level symbol recognition</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fornés</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Pujol</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Radeva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lladós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,399.62,614.22,80.98,7.86;10,146.91,625.18,26.33,7.86">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1424" to="1433" />
			<date type="published" when="2009-11">November 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,138.35,635.88,342.25,7.86;10,146.91,646.84,333.67,7.86;10,146.91,657.79,177.25,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,274.04,635.88,206.56,7.86;10,146.91,646.84,109.89,7.86">A robust algorithm for text string separation from mixed text/graphics images</title>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">A</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kasturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,265.00,646.84,215.58,7.86;10,146.91,657.79,44.51,7.86">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="910" to="918" />
			<date type="published" when="1988-11">November 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.35,120.67,342.24,7.86;11,146.91,131.63,258.95,7.86" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,262.41,120.67,218.18,7.86;11,146.91,131.63,45.92,7.86">An optimal algorithm for extracting the regions of a plane graph</title>
		<author>
			<persName coords=""><forename type="first">X</forename><forename type="middle">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,201.20,131.63,109.27,7.86">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="553" to="558" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.35,142.59,342.24,7.86;11,146.91,153.55,177.36,7.86" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,261.72,142.59,167.47,7.86">Segmentation of edges into lines and arcs</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,438.12,142.59,42.47,7.86;11,146.91,153.55,71.10,7.86">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="114" />
			<date type="published" when="1989-05">May 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.35,164.51,342.25,7.86;11,146.91,175.46,333.68,7.86;11,146.91,186.42,333.68,7.86;11,146.91,197.38,52.25,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="11,421.33,164.51,59.27,7.86;11,146.91,175.46,199.57,7.86">Stable and robust vectorization: How to make the right choices</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Tombre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Ah-Soon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dosch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Massini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tabbone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,366.18,175.46,114.42,7.86;11,146.91,186.42,35.12,7.86">Graphics Recognition Recent Advances</title>
		<title level="s" coord="11,250.85,186.42,138.33,7.86">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1941</biblScope>
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,138.35,208.34,342.24,7.86;11,146.91,219.30,333.68,7.86;11,146.91,230.26,241.67,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,424.20,208.34,56.39,7.86;11,146.91,219.30,77.63,7.86">Text/graphics separation revisited</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Tombre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tabbone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Pélissier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Lamiroy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,243.81,219.30,110.82,7.86">Document Analysis Systems</title>
		<title level="s" coord="11,425.76,219.30,54.84,7.86;11,146.91,230.26,82.40,7.86">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2423</biblScope>
			<biblScope unit="page" from="615" to="620" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
