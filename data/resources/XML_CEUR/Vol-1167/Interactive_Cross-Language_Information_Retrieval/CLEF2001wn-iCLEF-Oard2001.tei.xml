<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,188.16,116.05,239.04,12.67">The CLEF 2001 Interactive Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,228.00,153.84,76.86,8.85"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<email>oard@glue.umd.edu.edu</email>
						</author>
						<author>
							<persName coords="1,327.41,153.84,60.03,8.85"><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">College of Information Studies</orgName>
								<orgName type="laboratory">Human Computer Interaction Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute for Advanced Computer Studies</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Departamento de Lenguajes y Sistemas Informáticos de Educación a Distancia E.T.S.I Industriales</orgName>
								<orgName type="institution">Universidad Nacional</orgName>
								<address>
									<addrLine>Ciudad Universitaria s/n</addrLine>
									<postCode>28040</postCode>
									<settlement>Madrid</settlement>
									<country key="ES">SPAIN</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,188.16,116.05,239.04,12.67">The CLEF 2001 Interactive Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8836085218B63CD3BFE6D392AAC1728B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The problem of finding documents written in a language that the searcher cannot read is perhaps the most challenging application of cross-language information retrieval technology. In interactive applications, that task involves at least two steps: (1) the machine locates promising documents in a collection that is larger than the searcher could scan, and (2) the searcher recognizes documents relevant to their intended use from among those nominated by the machine. The goal of the 2001 Cross-Language Evaluation Forum's experimental interactive track was to explore the ability of present technology to support interactive relevance assessment. This paper describes the shared experiment design used at all three participating sites, summarizes preliminary results from the evaluation, and concludes with observations on lessons learned that can inform the design of subsequent evaluation campaigns.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The problem of finding documents written in a language that the searcher cannot read is perhaps the most challenging application of Cross-Language Information Retrieval (CLIR) technology. In some cases (e.g., alerting the user to urgent new information), this might need to be a fully automatic process. In many applications, however, the effectiveness of fully automatic systems is limited by one or more of the following factors:</p><p>-The information need might initially be incompletely understood by the searcher. -The information need might initially not be well articulated, either because the system's capabilities are underutilized or because the system's query language is insufficiently expressive.</p><p>-The ambiguity introduced by the use of natural (i.e., human) language within documents may cause the system to retrieve some documents that are not useful and/or to fail to retrieve some documents that are useful.</p><p>For this reason, automatic search technology is often embedded within interactive applications to achieve some degree of synergy between the machine's ability to rapidly cull through enormous collections using relative simple techniques and a human searcher's ability to learn about their own information needs, to reformulate queries in ways that better express their needs and/or better match the system's capabilities, and to accurately recognize useful documents within a set of a limited size (perhaps 10-100 documents). The goal of the experimental interactive track at the 2001 Cross-Language Evaluation Forum (which we call iCLEF) is to begin the process of exploring these issues in the context of cross-language information retrieval. The process by which searchers interact with information systems to find documents has been extensively studied (for an excellent overview, see <ref type="bibr" coords="2,446.12,292.56,10.32,8.85" target="#b0">[1]</ref>). Essentially, there are two key points at which the searcher and the system interact: query formulation and document selection. Query formulation is a complex cognitive process in which searchers apply three kinds of knowledge-what they think they want, what they think the information system can do, and what they think the document collection being searched contains-to develop a query. The query formulation process is typically iterative, with searchers learning about the collection and the system, and often about what it is that they really wanted to know, by posing queries and examining retrieval results. Ultimately we must study the query formulation process in a cross-language retrieval environment if we are to design systems that effectively support real information seeking behaviors. We were concerned, however, that the open-ended nature of the query formulation process might make it difficult to agree on a sharp focus for quantitative evaluation in the near term. We therefore chose to focus on cross-language document selection for the initial iCLEF evaluation.</p><p>Interactive document selection is essentially a manual detection problemgiven the documents that are nominated by the system as being of possible interest, the searcher must recognize which documents are truly of interest. The main Cross-Language Evaluation Forum (CLEF) track evaluates the effectiveness of systems that develop a ranked list of documents that are possibly (and hopefully!) relevant to a query, so we took that as our starting point. The searcher's task thus becomes recognizing relevant documents in a language that they cannot read. Viewed from the perspective of system designers, the task is to present information (metadata, summaries, translations, etc.) that is sufficient to allow the user to make accurate relevance judgments.</p><p>Focusing on interactive CLIR is not actually be as a radical departure for CLEF as it might first appear. The principal CLEF evaluation measure-mean average precision (MAP )-actually models the automatic component of an interactive search process <ref type="bibr" coords="2,240.74,627.36,10.00,8.85" target="#b1">[2]</ref>. MAP is defined as:</p><formula xml:id="formula_0" coords="2,259.92,644.43,95.41,23.52">MAP = E i [E j [ j r(i, j) ]</formula><p>where E i [ ] is the sample expectation over a set of queries, E j [ ] is the sample expectation over the documents that are relevant to query i, and r(i, j) is the rank of the j th relevant document for query i. One way to think of MAP is as a measure of effectiveness for the one-pass interactive retrieval process shown in Figure <ref type="figure" coords="3,166.21,166.92,4.98,8.85" target="#fig_0">1</ref> in which: 1. The searcher creates a query in a manner similar to those over which the outer expectation is computed. 2. The system computes a ranked list in a way that seeks to place the topically relevant documents as close to the top of the list as is possible, given the available evidence (query terms, document terms, embedded knowledge of language characteristics such as stemming, . . . ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Use</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>The searcher starts at the top of the list and examines each document (and/or summaries of those documents) until they are satisfied. 4. The searcher becomes satisfied after finding some number of relevant documents, but we have no a priori knowledge of how many relevant documents it will take to satisfy the searcher. 5. The searcher's degree of satisfaction is related to the number of documents that they need to examine before finding the desired number of relevant documents.</p><p>Implicit in this process is the assumption that the user can recognize relevant documents when they see them. It is that question that we sought to explore at iCLEF. The remainder of this paper is organized as follows. Section 2 presents the basic experiment design that all three sites adopted and describes the shared evaluation resources that were provided. Section 3 then summarizes the research questions that each site explored and briefly summarizes some of the preliminary insights gained through cross-site comparison. Finally, Section 4 provides a preliminary recapitulation of some of the lessons we have learned that could inform the design of subsequent evaluation campaigns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experiment Design</head><p>Our experiment design closely follows the framework established over several years at the interactive track of the Text Retrieval Conferences (TREC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data</head><p>Document collection. We decided to use data from the CLEF 2000 campaign for several reasons:</p><p>-Ranked lists from existing automatic CLIR systems provide a representative sample of the input that an interactive document selection stage must be designed to handle. -The use of a common set of frozen ranked lists enhanced the potential for cross-site comparisons. -Relevance judgments for most of the top-ranked documents found in this way were already available, which made it possible for us to set the deadline for the interactive track about one month after the main CLEF 2000 task deadline in order to facilitate participation by teams that had wished to participate in both tasks. -Rights to use the CLEF 2000 collection for research purposes have already been arranged for CLEF participants.</p><p>We used top-1000 results from John Hopkins University for English documents (found for CLEF 2000 using French queries) and from University of Maryland for French documents (found after CLEF 2000 using English queries) as the basis for forming the ranked lists that would be used in the experiments. We chose to support more than one document language because alternatives were needed in order to satisfy our requirement that teams recruit only searchers that were not familiar with the document language. These top-1000 results were then used to produce top-50 English and top-50 French results for each topic by first removing any document for which a relevance judgment was unavailable and then selecting the top 50 remaining documents. This process made it possible to use runs that had not been included in the original CLEF 2000 judging pools without the added complexity of scoring documents for which no CLEF relevance judgments were available.</p><p>As a baseline Machine Translation (MT) system, we chose Systran professional 3.0 because it is representative of state-of-the-art systems for language pairs for which considerable interest exists. Another factor favoring selection of Systran is that its use by popular freely-available Web page translation services makes it a de facto baseline for this task. We chose to translate the French documents into English and the English documents into Spanish for the baseline translations since those language pairs met the needs of teams that we knew were planning to participate. Use of the baseline translations was not required, so in principle it would have been possible for teams that preferred other language pairs to participate as well. In practice, all participating teams did choose to use at least one set of the baseline translations for at least one of their two conditions.</p><p>Topics. For our experiment design we needed two "broad" topics that asked about some general subject that we thought would have many aspects, and two "narrow" topics that asked about some specific event. We selected those topics from among the 40 CLEF 2000 topics in the following manner:</p><p>-Discard topics that do not fall clearly into either the "broad" or or the "narrow" category. -Discard topics for which the relevance of a document could likely be judged simply by looking for a proper name (e.g. Suicide of Pierre Beregovoy). -Favor topics that were relatively easy to judge for relevance based on:</p><p>• a clear topic description, and Our choice of topics according to these criteria actually turned out to be more limited than we had expected. Table <ref type="table" coords="5,318.08,463.80,4.98,8.85" target="#tab_0">1</ref> shows our choices and the density of relevant documents for each topic. One interesting outcome of our topic selection process is that it turned out that the narrow topics consistently had far fewer known relevant documents in the CLEF-2000 collection than the broad topics. Thus, for this collection, "narrow" roughly equates to "sparse" and "broad" roughly equates to "dense". In addition to the topics chosen for the experiment, we suggested the use of topic 33 (Cancer genetics, a broad topic) for training searchers at the outset of their session. The same standard resources (top-50 lists and baseline translations) were therefore provided for topic 33 as well.</p><formula xml:id="formula_1" coords="5,158.64,244.83,4.98,9.96">•</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Search Procedure</head><p>The task assigned to each participant in an experiment was to begin at the top of a ranked list that had been produced by a cross-language retrieval system (see above) and to determine for as many documents in the list as practical in the allowed time whether that document was relevant, somewhat relevant, or not relevant to a topic described by a written topic description. The written topic description included the text from the title, description, and narrative fields of the CLEF 2000 topic description. A maximum of 20 minutes was allowed for each topic, and participants were to be told that "more credit will be awarded for accurately assessing relevant documents than for the number of documents that are assessed, because in a real application you might need to pay for a high-quality translation [of] each selected document." The participants were also afforded the ability to indicate if they were unsure of their assessment for a document, and they could also choose to leave some documents unassessed.</p><p>The participants were asked to complete eight questionnaires at specific points during their session:</p><p>-Before the experiment, about computer/searching experience and attitudes, and their degree of knowledge of the document collection, and their foreign language skills. (1) -After assessing the documents with respect to each topic. (4) -After completing the use of each system. ( <ref type="formula" coords="6,336.83,297.84,4.26,8.85">2</ref>) -After the experiment, about system comparisons and to provide feedback on the experiment design. (1)</p><p>These questionnaires closely followed the design of the questionnaires used in recent TREC interactive track evaluations. The questionnaires that we used, among with additional forms for recording the experimenter's observations during each search, can be found on the CLEF interactive track home page (which can be reached through http://www.clef-campaign.org). Each four-search session was designed to be completed in about three hours. This time included initial training, four 20-minute searches, all questionnaires, and two breaks (one following training, one between systems).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Presentation Order</head><p>We adopted a within-subject design in which each participant searched each topic with some system. Participants, topics and systems were distributed using a Latin square design in a manner similar to that used in the TREC interactive tracks. The presentation order for topics was varied systematically, with participants that saw the same topic-system combination seeing those topics in a different order. That design made it possible to control for fatigue and learning effects to some extent. An eight-participant presentation order matrix is shown in Table <ref type="table" coords="6,175.58,565.68,3.90,8.85" target="#tab_1">2</ref>. The minimum number of participants was set at 4, in which case only the top half of the matrix would be used. Additional participants could be added in groups of 4, with the same matrix being reused as needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Evaluation</head><p>As our principal measure of effectiveness we selected an unbalanced version of van Rijsbergen's F measure that we called F α : </p><formula xml:id="formula_2" coords="7,256.20,279.60,101.64,22.84">F α = 1 α/P + (1 -α)/R</formula><p>where P is precision and R is recall. Values of α above 0.5 emphasize precision, values below 0.5 emphasize recall. For this evaluation, α = 0.8 was chosen, modeling the case in which missing some relevant documents would be less objectionable than finding too many documents that, after perhaps paying for professional translations, turn out not to be relevant. The CLEF relevance judgments are two-state (relevant or not relevant), so we treated all judgments other than "relevant" ("somewhat relevant", "not relevant", "not enough information") as not relevant when computing F α . For contrast, we computed F 0.2 (which modeled a recall-biased searcher) in addition to F 0.8 , and participating teams were encouraged to explore additional measures that might better model cross-language retrieval tasks in which they were interested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>We established an email reflector for teams that were interested in participating in the evaluation and other interested parties. Twenty people from 12 university, industry and government organizations joined that list. Three of those teams completed the experiment and submitted results: Universidad Nacional de Educación a Distancia (UNED) from Spain, the University of Maryland (UMD) from the USA, and the University of Sheffield (SHEF) from the United Kingdom. In this section we summarize the research questions explored by each team.</p><p>The UNED experiments used Spanish native speakers, Systran translations from English as a baseline, and "pseudo-translations" based on phrasal alignment between the English and Spanish CLEF-2001 collections as the contrastive condition. The hypotheses tested was that pseudo-translation would permit faster judgments without significant loss in precision. Eight monolingual Spanish-speaking searchers completed the task. In addition, a group of 8 searchers with a medium knowledge of English, and another 8-searcher group with a good knowledge of English, also completed the task.</p><p>The University of Maryland used four native English speakers to compare the utility of word-for-word gloss translations (that can be developed quickly using limited resources) with results obtained using the baseline Systran translations. The hypothesis tested was that a combination of word-for-word gloss translation and query-term highlighting in the retrieved documents could provide a useful basis for relevance assessment.</p><p>The University of Sheffield used 8 native English-speaking searchers to compare monolingual and cross-language and document selection. The specific tasks included selecting French documents using Systran translations, and selecting documents from the (untranslated) English collection. Because both collections were used, the SHEF experiments offer a useful basis for comparison with both the UMD and UNED results.</p><p>Table <ref type="table" coords="8,177.49,264.72,4.98,8.85" target="#tab_2">3</ref> summarizes the results obtained for both languages. Figure <ref type="figure" coords="8,450.89,264.72,4.98,8.85" target="#fig_1">2</ref> illustrates the French results using a recall-precision plot, and Figure <ref type="figure" coords="8,426.87,276.72,4.98,8.85" target="#fig_2">3</ref> provides a similar depiction for English. For comparison, a naive searcher that marked every document as relevant would achieve a precision of 0.30 for English or 0.22 for French, with a recall of 1.0 in either case. Our analysis for the submitted results is not yet complete, but we are already able to make the following observations:</p><p>-The fact that every system achieved better precision than could have been obtained through the naive selection of every document suggests that every technique that was tried has some merit. -The usefulness of Systran translations for this task appears to be consistent across sites (for French-to-English, at SHEF and UMD), but not across languages (where both precision and recall with English-to-Spanish translations were well below that achieved with French-to-English translations). -Monolingual assessment appears to be substantially better (in both precision and recall) than cross-language assessment using Systran, and cross-language assessment using Systran appears to be substantially better (in both precision and recall) than the word-for-word gloss translation technique that was tried at UMD. -The display of translated phrases (UNED's pseudo-translations) appears to increase recall with no adverse affect on precision.  -There was a very substantial difference between CLEF relevance judgments (which would receive a F measure of 1.0 for any α) and monolingual assessment at iCLEF.</p><p>There are several possible explanations for this last point:</p><p>-Any pair of assessors will naturally disagree about some judgments, and assessors that that lack expertise in a topic typically exhibit less agreement than experts would. -CLEF assessors must judge every document as relevant/not relevant, while our searchers could also choose somewhat relevant, not enough information, or leave the document unjudged. -iCLEF searchers must make their judgments in a more sharply limited period. -iCLEF searchers were given instructions that were intended to bias them in favor of precision. Pooled relevance assessment, by contract, places a premium on careful consideration of every document in the assessment pool. -Assessors in a formal evaluation could discuss difficult judgments with other assessors, thereby reflecting some degree community consensus in those cases.</p><p>The iCLEF searchers produce only personal opinions.. -CLEF assessors evaluate documents in an arbitrary order, while iCLEF searchers have additional information available (the order of the documents in the ranked list).</p><p>By characterizing the degree to which a time-constrained interactive searcher's judgment might differ from that exercised to establish ground truth for an information retrieval evaluation, we have gained an unexpected insight that might prove useful in the design of adaptive filtering and relevance feedback evaluations, even in a monolingual context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Looking to the Future</head><p>Although our conclusions are necessarily quite preliminary at this point, we have learned a number of interesting things in these experiments. Our thinking on next steps is organized in two parts: what we might do to improve the evaluation of cross-language document selection, and how we might approach evaluation of some of the other tasks that are also important to interactive CLIR. Some ideas that we are considering for future evaluations of document selection are:</p><p>-Consideration of measures other than F α -Establishing an agreed framework for statistical significance testing and then using that framework as a basis for establishing the minimum required number of participants in each experiment. -Exploring experiment designs that could yield insight into the difference between monolingual and cross-language performance on the same document collection.</p><p>-Capturing separate values for confidence and relevance assessment, rather than treating "unsure" as an assessment value. -Exploring tasks other than a simple yes/no decision (e.g., creating suitable ground truth for evaluating multi-valued relevance judgments, evaluating aspectual recall for topics that have a rich substructure, or designing a question answering task). -Providing shared tools that can reduce barriers to participation in the evaluation campaign (e.g., user interface toolkits that include provisions for logging interactive relevance judgments).</p><p>Among the other tasks that are related to interactive CLIR, recognition of suitable terms for query translation and enrichment seems like it may be a sufficiently well formed problem to permit a tractable experiment design. We plan to explore these ideas and others when we meet in Darmstadt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>One of the most valuable products of iCLEF has been the emergence of a community of interest around the subject of interactive cross-language retrieval. One important part of this community of interest is a set of researchers that think of themselves as working on task-situated machine translation (where cross-language relevance assessment is the task). Task-based evaluation frameworks have recently been receiving greater attention from machine translation researchers (for example, see <ref type="bibr" coords="11,260.69,397.44,10.33,8.85" target="#b2">[3]</ref>). Addressing the CLIR challenge is naturally an interdisciplinary endeavor, and the potential for close links between CLIR and machine translation researchers should therefore be very much in our mutual interest.</p><p>Although only three sites participated in this first cooperative evaluation of interactive CLIR, we feel that we achieved our initial goals. We gained a better understanding of the issues that need to be addressed to conduct such evaluations, discovered other researchers with similar interests, and obtained some interesting results. We hope that our email reflector will help to nurture and grow that community as we discuss what we have learned and add people that will bring new perspectives. Next year's iCLEF (assuming there is a next year-something we must discuss) should therefore benefit in many ways from what we have learned. But regardless of what happens next year, we believe that iCLEF has been an example of CLEF at its best-discovering interesting questions and providing the resources needed to begin to answer them. results, Gina Levow (Maryland) for providing top-1000 automatic French results, Clara Cabezas (Maryland) for producing the final document lists used in the evaluation, Jianqiang Wang (Maryland) for providing the Systran translations, and Fernando López-Ostenero (UNED) for managing the iCLEF Web page and email reflector, developing the evaluation scripts, and helping with many other aspects of the evaluation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,212.76,315.79,189.59,7.96"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A one-pass monolingual search process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,237.12,367.51,140.95,7.96"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of French results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,235.68,645.07,143.87,7.96"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Overview of English results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,141.00,245.52,339.63,170.76"><head>Table 1 .</head><label>1</label><figDesc>little need for specialized background knowledge.-Favor topics with a greater number of known relevant documents in the top-50 for both languages. Selected topics</figDesc><table coords="5,361.08,309.94,82.33,7.89"><row><cell>Relevant Fraction</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,149.88,117.94,315.46,117.26"><head>Table 2 .</head><label>2</label><figDesc>Presentation order for topics and association of topics with systems.</figDesc><table coords="7,215.16,117.94,184.97,96.38"><row><cell cols="2">Participant Block #1</cell><cell>Block #2</cell></row><row><cell>1</cell><cell cols="2">System 1: 11-17 System 2: 13-29</cell></row><row><cell>2</cell><cell cols="2">System 2: 11-17 System 1: 13-29</cell></row><row><cell>3</cell><cell cols="2">System 1: 17-11 System 2: 29-13</cell></row><row><cell>4</cell><cell cols="2">System 2: 17-11 System 1: 29-13</cell></row><row><cell>5</cell><cell cols="2">System 1: 11-17 System 2: 29-13</cell></row><row><cell>6</cell><cell cols="2">System 2: 11-17 System 1: 29-13</cell></row><row><cell>7</cell><cell cols="2">System 1: 17-11 System 2: 13-29</cell></row><row><cell>8</cell><cell cols="2">System 2: 17-11 System 1: 13-29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,158.64,347.71,297.65,76.24"><head>Table 3 .</head><label>3</label><figDesc>Overview of results.</figDesc><table coords="8,158.64,347.71,297.65,55.36"><row><cell cols="2">English documents</cell><cell cols="2">French documents</cell></row><row><cell>System</cell><cell>P R F0.8 F0.2</cell><cell>System</cell><cell>P R F0.8 F0.2</cell></row><row><cell cols="2">SHEF-Monolingual .59 .40 .45 .39</cell><cell cols="2">UMD-MT .76 .58 .61 .57</cell></row><row><cell>UNED-Phrases</cell><cell>.47 .34 .35 .32</cell><cell cols="2">SHEF-MT .67 .46 .59 .48</cell></row><row><cell>UNED-MT</cell><cell>.48 .22 .28 .21</cell><cell cols="2">UMD-Gloss .51 .27 .29 .26</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors are grateful to <rs type="person">Carol Peters</rs> (<rs type="affiliation">CNR-IEI Pisa</rs>) for her support and encouragement, <rs type="person">Paul Over</rs> (<rs type="affiliation">NIST</rs>) and <rs type="person">Bill Hersh</rs> (<rs type="affiliation">OHSU</rs>) for generously offering advice and resources based on their experience in the TREC interactive track, <rs type="person">Paul McNamee</rs> (<rs type="affiliation">Johns Hopkins APL</rs>) for providing top-1000 automatic English</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="12,138.33,234.31,342.23,7.96;12,146.88,245.23,333.81,7.96;12,146.88,256.27,333.31,7.96;12,146.88,267.19,89.78,7.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,222.59,234.31,131.88,7.96">User interfaces and visualization</title>
		<author>
			<persName coords=""><forename type="first">Marti</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<ptr target="http://www.sims.berkeley.edu/∼hearst/irbook/chapters/chap10.html" />
	</analytic>
	<monogr>
		<title level="m" coord="12,280.20,245.23,123.66,7.95">Modern Information Retrieval</title>
		<editor>
			<persName><forename type="first">Ricardo</forename><surname>Baeza</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">-</forename><surname>Yates</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Berthier</forename><surname>Ribeiro-Neto</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.32,278.11,342.16,7.96;12,146.88,289.16,333.56,7.96;12,146.88,300.08,151.85,7.96;12,315.73,300.08,164.65,7.96;12,146.88,310.38,85.93,8.97" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,234.24,278.11,246.25,7.96;12,146.88,289.16,79.99,7.96">Evaluating interactive cross-language information retrieval: Document selection</title>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<ptr target="http://www.glue.umd.edu/∼oard/research.html" />
	</analytic>
	<monogr>
		<title level="m" coord="12,348.13,289.16,132.31,7.95;12,146.88,300.08,118.95,7.95">Proceedings of the First Cross-Language Evaluation Forum</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<meeting>the First Cross-Language Evaluation Forum</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,138.32,322.04,342.28,7.96;12,146.88,332.96,333.59,7.96;12,146.88,343.88,333.64,7.96;12,146.88,354.92,316.28,7.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,284.25,322.04,196.36,7.96;12,146.88,332.96,85.11,7.96">Predicting what MT is good for: User judgments and task performance</title>
		<author>
			<persName coords=""><forename type="first">Kathryn</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,146.88,343.88,304.87,7.95">Third Conference of the Association for Machine Translation in the Americas</title>
		<title level="s" coord="12,285.10,354.92,154.11,7.96">Lecture Notes in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">David</forename><surname>Farwell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Laurie</forename><surname>Gerber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998-10">October 1998</date>
			<biblScope unit="volume">1529</biblScope>
			<biblScope unit="page" from="364" to="373" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
