<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,208.08,35.07,179.28,22.40;1,152.16,56.91,290.88,22.40">iCLEF 2001 at Maryland: Comparing Word-for-Word Gloss and MT</title>
				<funder ref="#_8P74MMN">
					<orgName type="full">D ARPA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,198.00,90.15,82.32,16.80"><forename type="first">Jianqiang</forename><surname>Wang</surname></persName>
							<email>wangjq@glue.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">College of Information Studies</orgName>
								<orgName type="department" key="dep2">Institute for Advanced Computer Studies</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,307.20,90.15,89.76,16.80"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
							<email>oard@glue.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">College of Information Studies</orgName>
								<orgName type="department" key="dep2">Institute for Advanced Computer Studies</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,208.08,35.07,179.28,22.40;1,152.16,56.91,290.88,22.40">iCLEF 2001 at Maryland: Comparing Word-for-Word Gloss and MT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1F391EC8734A0AD917DD0E77A4831E85</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For the rst interactive Cross-Language Evaluation Forum, the Maryland team focused on comparison of term-for-term gloss translation with full machine translation for the document selection task. The results show that (1) searchers are able to make r e l e v ance judgments with translations from either approach, and (2) the machine translation system achieved better e ectiveness than the gloss translation strategy that we tried, although the di erence is not statistically signi cant. It was noted that the \somewhat relevant" category was used di erently by searchers presented with gloss translations than with machine translations, and some reasons for that di erence are suggested. Finally, the results suggest that the F measure used in this evaluation is better suited for use with topics that have m a n y k n o wn relevant d o c u m e n ts than those with few.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the process of interactive cross-language information retrieval (CLIR), there are two p o i n ts where interaction with the searcher is possible: query formulation and document selection. The focus of this paper is on the interactive document selection task. Ranked retrieval systems nominate promising documents for examination by the user by placing them higher in a ranked list. The searcher's task is then to examine those documents and select the ones that help to meet their information need. The query formulation process and the actual use of the documents selected by the user is outside the scope of the work reported in this paper. Focusing on one aspect of the problem in this way m a k es it possible to gain insight through the use of metrics that are appropriate for document selection, a well-studied problem in other contexts.</p><p>One important use for CLIR systems is to help searchers nd information that is written in a language with which they are not familiar. In such an application, the query would be posed in a language for which the user has an adequate active (i.e., writing) vocabulary, and the document selection process would be performed in a language for which the searcher has at least an adequate passive (i.e., reading) vocabulary. Since we h a ve assumed that the document(s) being sought are not expressed in such a language, some form of translation is required.</p><p>We view translation as a user interface design challenge, in which the goal is to provide the user with the information needed to perform some task|in this case document selection. There has been an extensive e ort to develop so-called \Machine Translation" (MT) systems to produce (hopefully) uent and accurate translation of every language that is presently studied at the Cross-Language Evaluation Forum (CLEF) into English. No such systems yet exist for most of the world's languages, however, and the cost of building a sophisticated MT system for every written language would indeed be staggering. This is an important c hallenge, since a substantial portion of the world's knowledge is presently recorded in English, and the vast majority o f t h e w orld's people cannot even nd that information. Supporting search b y users that know only a lesser-developed language is only one of many capabilities that will be needed if we are to address what has been called the \digital divide" on a global scale. But it is one that we believe could be addressed with emerging broad-coverage language technologies. We therefore have chosen to use this rst interactive CLEF (iCLEF) evaluation to begin to explore that question.</p><p>We h a ve i d e n ti ed three factors that a ect the utility of translation technology for the document selection task: accuracy, uency, and focus. By \accuracy" we mean the degree to which a translation re ects the intent of the original author. Both lexical selection (word choice) and presentation order can a ect accuracy. <ref type="bibr" coords="2,137.28,0.15,4.08,8.16">1</ref> By \ uency" we mean the degree to which a translation can be used quickly to achieve the intended purpose (in this case, document selection). Again, both lexical selection and presentation order can a ect uency. <ref type="bibr" coords="2,175.44,24.15,4.08,8.16">2</ref> By focus, we mean the degree to which the reader's attention can be focused on the portions of a translated document that best support the intended task{in this case the recognition of relevant d o c u m e n ts from among those nominated by the system. Presentation of summaries and highlighting query terms in the retrieved documents are typical examples of focus. Our intuition suggests that accuracy is essential for the document selection task, but that there is a tradeo between uency and focus, with lower uency being acceptable if e ective focus mechanisms are provided. The iCLEF evaluation was well timed to allow us to begin to explore these questions.</p><p>For iCLEF, we c hose to compare MT with a one-best term-by-term gloss translation technique that we had originally developed to demonstrate the degree of translation quality that could be achieved for resource-poor languages. We had already adapted this system to support controlled user studies for some exploratory work on interactive document selection in the CLIR track of the 2000 Text Retrieval Conference (TREC) 4], so only minor modi cations were needed to conform to the iCLEF requirements. We obtained a numb e r o f i n teresting results, including:</p><p>Searchers are able to make some useful relevance judgments with either type of translation MT achieved better e ectiveness than gloss translation, although the di erence was not statistically signi cant The \somewhat relevant" category was used di erently by participants in our experiment depending on whether MT or gloss translations were being examined.</p><p>The F e ectiveness measure does not seem to be well suited for use with topics that have few relevant documents. The remainder of the paper is organized as follows. Section 2 provides an overview of the iCLEF experiment design. Section 3 then describes the design and implementation of our system, details of the experiment procedure, and a description of the characteristics of the participants in our experiment. Section 4 presents the results, drawing on both quantitative and qualitative methods, and raises some experiment design issues. Finally, Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Over the past decade, research on CLIR has focused on development a n d e v aluation of automatic approaches for ranking documents in a language di erent from that of the query. Present fully automatic techniques can do this fairly well, performing at perhaps 80% of what can be achieved by a monolingual information retrieval system under similar conditions when measured using mean average precision 3]. Ranking documents is only one step in a search process, however some means of selecting documents from that list is needed. One possible strategy would be to build an automatic classi er that could make a sharp decision about whether each d o c u m e n t is relevant or not. Such an approach w ould have problems, however, since users often don't express their information needs clearly. Indeed, they may not even know their information needs clearly at the outset of a search session. For this reason, ranked retrieval systems are often used interactively, with the user browsing the ranked list and selecting interesting documents. Research o n i n teractive retrieval strongly suggests that people are quite good at this task, performing quite well even when using ranked lists produced by systems that are well below the current state-of-the-art 1]. It is an open question, however, whether a similar strategy would be e ective if automatically produced translations of otherwise unreadable documents would be su cient to obtain a similar e ect in interactive CLIR applications. The goal of the iCLEF evaluation is to bring together a research community to explore that question 2].</p><p>The principal objective of the rst iCLEF evaluation was to develop an experiment design that could yield insight i n to the e ectiveness of alternative techniques for supporting cross-language document s election. Participating sites could choose from two tasks: Selection of French documents or selection of English documents. We c hose to work on selection of French documents since knowledge of French among the pool of possible participants in our experiment w as more limited than knowledge of English. French test collection contained four search topics for use in the experiment, plus a fth practice topic.</p><p>For each topic, the following resources were provided: An English topic description, consisting of title, description, and narrative t h a t s e r v ed as a basis for the CLIR system's query, A ranked list of the top 50 documents produced automatically by a CLIR system using an English query, The original French v ersion of each document, and An English translation of each document that was produced using Systran Professional 3.0. The four topics included two \broad" topics that asked about a general subject (e.g. Conference o n B i r t h Control) and two \narrow" topics that asked about some speci c event (e.g., Nobel Prize for Economics in 1994 ). Relevance judgments for the top-50 documents for each topic were also known, but those judgments were used only to evaluate the results after the experiment w as completed. As might b e expected, it turned out that in every case there were more relevant d o c u m e n ts in the top-50 for the broad topics than for the narrow ones.</p><p>The iCLEF experiment w as designed in a manner similar to that used in the TREC Interactive T rack, in which a Latin square design is used to block topic and searcher e ects so that the system e ect can be characterized. Table <ref type="table" coords="3,163.92,358.11,5.04,14.40" target="#tab_0">1</ref> shows the order in which topic-system combinations were presented to users. In this design, every searcher sees all four topics, two with one system and two with the other. The order in which topics and systems are presented is varied systematically in order to minimize the impact of fatigue and learning e ect on the observability of the system e ect. We realized at the outset that four participants was an undesirably small number given the large variability that has been observed in human performance of related tasks, but time and resource limitations precluded our use of a larger sample.</p><p>The task to be performed at each participating site included: Design and implement t wo i n teractive document selection systems. Use of the Systran translations was optional, but we c hoose to use them as our MT system.</p><p>Have participants make r e l e v ance judgments for each t o p i c . E a c h participant w as allowed 20 minutes for each topic (including reading the topic description, reading as many documents or document summaries as time allowed, and making relevance judgments). For each document, the participant was asked to select one of four possible judgments: \not relevant," \somewhat relevant," \relevant," or \unsure." A \not judged" response was also available.</p><p>Ask each searcher to complete questionnaires regarding their background, each search, each system, and their subjective assessment o f t h e t wo systems. Provide the participants judgments to the iCLEF coordinators in a standard format for scoring. Conduct data analysis using the scored results and other measurements that were recorded and retained locally. An unbalanced version of van Rijsbergen's F measure was selected for use as the o cial e ectiveness measure for the evaluation:</p><formula xml:id="formula_0" coords="3,246.00,652.59,102.00,28.08">F = 1 =P + ( 1 ; )=R</formula><p>where P is precision and R is recall. Values of could range between 0 and 1, with values above 0 . 5 emphasizing precision and values below 0.5 emphasizing recall 5]. For iCLEF, 0.8 was selected as the value for which the experiments were to be designed, modeling a situation in which nding documents accurately is more important than nding all the relevant documents. The participants were told that they should approach the task with that in mind. For the o cial results, judgments of \somewhat relevant," \unsure," and \not judged" were treated as \not relevant."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Maryland iCLEF Experiments</head><p>For the past few years, our team at Maryland has focused on low-cost techniques for extending CLIR capabilities to new languages. Our initial work was based on using existing bilingual term lists to perform dictionary-based CLIR, and it is that technique that we adapted to perform gloss translation for these experiments. The basic idea is to nd source language (in this case, French) terms in the bilingual term list and then replace them with the corresponding target language term(s) (in this case, English). For resource-poor languages we could conceivably obtain bilingual term lists by scanning (or even rekeying) a printed bilingual dictionary or by training a statistical translation model on translation-equivalent text pairs that might be automatically farmed from the Web|for these experiments we used a bilingual term that we had downloaded form the Web for CLEF 2000 5]. This resource contained approximately 35,000 term pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Gloss Translation</head><p>Bilingual term lists found on the Web often contain an eclectic combination of root and in ected forms. We therefore applied the same backo translation strategy that we h a ve previously used for automatic retrieval to extend the source-language (French) coverage of the term list. The rst step was to remove all punctuation and convert every character to unaccented lower case in both the documents and the term list. This had the e ect of minimizing problems due to character encoding. The translation process then proceeded in the normal reading order through the text, using greedy longest string matching to identify terms in the document that can be translated using the bilingual term list. If no multi-word or single word match is found, the French w ord in the document is stemmed and a match with the term list is attempted again. If that fails, the previous step is repeated using a second version of the bilingual term list in which all source language terms have been stemmed. <ref type="bibr" coords="4,355.92,376.23,4.08,8.16" target="#b1">3</ref> If the source-language term was still not found in the term list, it was copied unchanged into the translated document. We used the stemmer that we had developed for CLEF 2000 for this purpose. Bilingual term lists typically contain several possible translations for some terms. In past work, we h a ve explored display strategies for presenting multiple alternatives, but for our iCLEF experiments we c hose only a single translation for each term because we w anted to focus on a single factor (the translation strategy). As we h a ve before, we c hose the English translation that occurred most often in the Brown Corpus (a balanced corpus of English) when more than one possible translation was present in the term list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Machine Translation</head><p>Maryland also performed full machine translation, contributing the results for use as the translations that were provided to all participating teams. Producing of the English translations of the French documents was relatively straightforward. First, we used Systran Professional 3.0 to translate the French collection into English. We then corrected some SGML tags that were inadvertently translated or mangled in some way (e.g., white spaces was inserted within the tags) and corrected them using a simple Perl script. After the translated collection was released, we found some additional mangled SGML symbols in the document titles, so we deleted these symbols. Punctuation and untranslatable words are handled di erently by Systran|punctuation and upper/lower case are retained and untranslatable words are displayed in upper case with accents retained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">User Interface</head><p>Because we wished to compare translation strategies, we sought to minimize the e ect of presentation di erences by using the same user interface with both types of translation. The user interface for our experiment w as based on an existing system that we h a d d e v eloped for our TREC-9 CLIR track experiments 4]. The system uses a Web-based server-side architecture. Searchers interact with the system using a W eb browser, and their relevance judgments are recorded by t h e s e r v er when a search is completed. A search starts when a searcher selects a topic and a translation option (MT or Gloss) and ends when the relevance judgments for that topic are nished. A search-ID is assigned to each search so that multiple searches can be tracked simultaneously, but participants in the study completed the task individually so this capability w as not needed. The system included the following capabilities: Provide topic selection and translation option selection mechanisms. Display topic descriptions based on the searcher's selection. The topic is displayed separately prior to the ranked document list so that the searcher can read and understand it before making any relevance judgments, and it remains displayed at the top of the page once the ranked list is displayed as a ready reference. Display a r a n k ed list providing summary information for the top 50 documents for the selected topic (see Figure <ref type="figure" coords="5,171.36,220.83,3.92,14.40" target="#fig_0">1</ref>). The summary information that we displayed for this experiment is simply the translation of its title, as speci ed by the appropriate SGML tag. Query terms (i.e., any term in the topic description) that appeared in a translated summary were detected using string matching and highlighted in red and rendered in italics. A set of ve radio buttons under each title allowed relevance judgments to be selected, with \not judged" initially selected for all documents. Display the translation of the full text of a document in a separate window whenever that document is selected by a searcher. All translations are performed in advance and cached within the server, so no speed di erence between translation types is apparent to the searcher. Again, query terms that appeared in a translated document w ere highlighted in red and rendered in italics.</p><p>Record the amount of time spent on judging each d o c u m e n t. This was implemented with a Javascript timer built in a CGI script. The timer was started when the title link was selected, and stopped when one of the relevance judgment radio buttons was selected. One can easily see this method fails to record the time correctly if the judgment w as based solely on a displayed summary since in that case the title link would never be selected. It would be hard to do better without an eye t r a c ker, since multiple summaries are displayed on the same page. On the other hand, since the summaries are very short (often only one line on the screen), the time required to render a judgment i n s u c h cases is likely to be quite small. Simultaneously record the relevance judgments for all documents when a search is completed. This design allows users to make a quick pass through the documents and then go back f o r a m o r e detailed examination if they desire. The submit button is at the bottom of the ranked list page (not shown in the Figure <ref type="figure" coords="5,207.84,483.87,3.92,14.40" target="#fig_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Searcher Characteristics</head><p>We had originally intended to recruit graduate students in library science to participate in our experiment, since we expect that librarians could make extensive use of CLIR systems when conducting searches on behalf of people with di erent language skills. The fact that the experiments were performed during summer session limited the pool of potential participants, however, and the 3-hour search session made participation less appealing even though we o ered a cash payment ($20) to each participant. As the deadline approach, we therefore became somewhat less selective. Of the four participants in our experiments, two (umd01 and umd03) held a Masters degree in Library Science. Both of those participants were doctoral students in the College of Information Studies, and both have i n terests in information retrieval and human-computer interaction. A third subject (umd02) has a Masters degree in Computer Science and some familiarity with cross-language retrieval and is working as a user interface programmer. The fourth participants (umd04) has a Bachelors degree in religion, is currently working as a nancial controller, and professed no interest in the technical details of what we w ere doing. The ages of the four participants range were between 28 and 35 at the time of the experiment. None of the participants had been involved in previous interactive retrieval experiments of this sort, but all had at least ve y ears of online searching experience. All four participants reported a great deal of experience searching the World Wide Web and a great deal of experience of using a point-click i n terface. Our observations during the experiment agreed with their assessments on this point.</p><p>In addition to the backgrounds described above, the following self-reported characteristics of distinguished an individual participants from the group: umd01. Participant umd01 reported 14 years of searching experience, much more than any other participant.</p><p>umd03. Participant umd03 was the only one to report good reading skills in French (the others reporting poor skills or none). Knowledge of French w as disallowed by the track guidelines, and we had mentioned this when recruiting subjects. When we s a w this answer on the questionnaire at the beginning of the session, we w ere therefore somewhat surprised. Unfortunately, there was not su cient time remaining before the deadline to recruit an additional participant. Interestingly, after the experiment, participant umd03 mentioned in a casual conversation that they had studied French in high school. Clearly we need to give more thought t o h o w w e conduct language skills screening.</p><p>umd04. Participant umd04 was the only one of the four with no experience searching online commercial systems, the only one to report typically searching less than once a day (for umd04, the response was twice a week) and the only one to give a neutral response to the question of how they feel about searching (the others reporting that they either enjoy or strongly enjoy searching).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Experiment Procedure</head><p>The iCLEF experiment in Maryland started on June 27, 2001, and ended on July 9, 2001. We began with a small (two-user) pilot study, after which w e made some changes to our system. We then conducted a halfhour peer review session with several graduate students who were working on computational linguistics. After a few further changes, we froze the con guration of the interface for the experiments reported in this paper.</p><p>The four search sessions were conducted individually by the rst author of this paper. Upon arrival, a searcher was rst given a 10-minute brief introduction to the goal of the study, the procedure of the experiment, the tasks he or she was expected to complete, and the time allocation for each step. Then a 5minute pre-search questionnaire was completed. The major purpose of that questionnaire was to collect basic demographic information and information about the searcher's experience with searching, using point-click i n terface, and reading the document language. Following that was a 30-minute tutorial in which the two systems were introduced. The tutorial was conducted in a hands-on fashion|the searcher practiced using the systems while reading printed instructions line-by-line. The experimenter followed along with the searcher, pointing out speci c details that might h a ve been incompletely understood when necessary. W e found that all the searchers learned how to use the systems in less than 30 minutes. After this step, the searcher was asked to take a 10-minute break. Interestingly, no participant thought this break was necessary, and none took it. The rst search then started.</p><p>For each search, the experimenter would tell the participant which topic and system to select, and then the experimenter would quietly observe the search process and take observation notes. Participants did occasionally ask questions of the experimenter, but we tried to minimize this tendency. E a c h s e a r c h w as followed by a 5-minute questionnaire regarding the searcher's familiarity with the topic, the ease of getting started with making relevance judgments for that topic, and their degree of con dence in the judgments that they had made. When two searches with the same system were completed, a questionnaire regarding the searcher's experience with that system was conducted. That was followed by a 10-minute break and then the process was repeated with the second system. After all the four searches were completed, an exit questionnaire was completed. That questionnaire sought the participant's subjective comparison of the two systems and provided an unstructured space for additional comments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The hypotheses that we wished to test was that MT and gloss translation can both support e ective interactive cross-language document selection. Formally, w e seek to reject two n ull hypotheses:</p><p>The F measure achieved by gloss translation could be achieved by f o l l o wing a rule that does not involve looking at the translations at all. The F measure achieved using the MT system is the same as that which w ould be achieved using the gloss translation system. In this section we rst examine the results using the o cial measure (F 0:8 ), then look at two v ariants on the computation of F , and then conclude by suggesting some alternative metrics that could prove t o b e useful in future evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">O cial Results</head><p>Table <ref type="table" coords="7,98.88,548.91,5.04,14.40" target="#tab_1">2</ref> shows the o cial results on a per-search basis, and Table <ref type="table" coords="7,368.40,548.91,5.04,14.40" target="#tab_2">3</ref> shows the result of averaging the F 0:8 measures of the two participants that experienced each condition. Three of the four searchers did better with MT than gloss translation on broad topics, and all four searchers did better with MT on narrow topics. A two-tail paired t-test (p&lt;0.05), found no signi cant di erence in either case, however, at p &lt; 0:05). This is probably due to the an insu cient n umbers of degrees of freedom in our test (i.e., too few participants), since the trend seems quite clear. So although we cannot reject the second of our null hypothesis, the preponderance of the evidence suggests that MT is better for this task than our present implementation of gloss translation when scored using the o cial measure.</p><p>A couple of observations are easily made from Table <ref type="table" coords="7,325.92,644.67,3.96,14.40" target="#tab_2">3</ref>. The values of F 0:8 for narrow topics are consistently higher than the values for broad topics. This suggests that searchers are typically able to make relevance judgments more accurately for narrow topics than for broad ones. observation is that the values of F 0:8 for broad topics exhibit a strong central tendency by clustering fairly well around the mean, for narrow topics the values have a bimodal distribution with peaks near zero and one.</p><p>In order to test our rst null hypothesis, we m ust construct some simple strategy that does not require looking at the documents. One way to do this is to simply selects all 50 documents in the ranked list as relevant. That guarantees a recall of 1.0 (since we compute recall over the relevant documents in the top-50, not over all relevant documents known to CLEF). The precision is then the fraction of the entire list that happens to be relevant, which i s m uch larger for broad topics than narrow ones. The average over all topics for F 0:8 when computed in this way is 0.26. All participants beat that value by a t least a factor of two when using the MT system, and two of the four participants beat it by t h a t m uch when using gloss translation. From this we tentatively conclude that both MT and gloss translation can be useful, but that there is substantial variation across the population of searchers with regard to their ability to use gloss translations for this purpose. The rst part of this conclusion is tentative because we have not yet tried some other rules (e.g., always select the top 10 documents, or select di erent n umbers of documents for broad and narrow topics) that might produce higher values for F 0:8 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Descriptive Data Analysis</head><p>No single measure can re ect every interesting aspect of the data, so we performed some descriptive data analysis to further explore our results. Figure <ref type="figure" coords="8,276.48,521.31,21.36,14.40" target="#fig_1">2 (a)</ref> shows the average number of documents to receive each t ype of relevance judgment b y topic and system type. In that gure, we treat the o cial CLEF judgments as a third \system" for which only two t ypes of judgment w ere provided. Clearly, m a n y m o r e documents were left unjudged for broad topics than for narrow ones. The highly skewed distribution of judgments on nonrelevant documents is particularly striking, suggesting that there is something about narrow topics that helps users to make more total judgments and to get the balance between relevant a n d not relevant judgments about right, regardless of the system type. One other observation that we could make is that for broad topics, our participants seemed to exhibit a greater proclivity to assess documents as relevant than as not relevant (based on the fraction of the o cial judgments that they achieved in each category). That may, h o wever, be an artifact of the presence of a greater density of truly relevant documents near the top of any w ell constructed ranked list.</p><p>Examining the time required to make r e l e v ance judgments provides another perspective on our results. As Figure <ref type="figure" coords="8,117.36,664.83,5.04,14.40" target="#fig_1">2</ref> (b) shows, \unsure" and \somewhat relevant" judgments took longer on average than \rel- evant" judgments, and \not relevant" judgments could be performed the most quickly. This was true for both topic types, and it helps to explain why n a r r o w topics (which h a ve few relevant documents) had fewer \not judged" cases. The seemingly excessive time required to reach a judgment of \somewhat relevant" when using gloss translation results from a single data point, and therefore provides little basis for any sort of inference.</p><p>The total number of documents of each r e l e v ance judgment t ype (across both topic types) is: \not relevant:" 398, \somewhat relevant:" 57, \relevant:" 89, and \unsure:" 20. Comparing these numbers with the average amount of time per document o f e a c h relevance type in Figure <ref type="figure" coords="9,433.20,273.87,5.04,14.40" target="#fig_1">2</ref> (b), we see a clear inverse relationship between the numb e r o f d o c u m e n ts and time required to assign a document to that category. One possible explanation for this would be a within-topic learning e ect, in which searchers learn to recognize documents in a category based on their recollection of documents that have been previously assigned to that category. Our observation of search behavior o ers some evidence to support this speculation. We observed that some searchers often modi ed their relevance judgment, either right afterwards or later when they worked on a di erent d o c u m e n t. In that second case, presumably their judgment of the relevance of the later document seemed to be related to the relevance of a previously judged document. We observed that other searchers rarely changed their relevance judgments, however, so it is not clear how p e r v asive this e ect is.</p><p>It is interesting to note that the track guidelines did not provide any formal de nition for the types of relevance judgments, presumably assuming that both experimenters and searchers would understand them based on the common meanings of the terms. In our study, w e p r o vided no further explanation of the judgment t ypes to our participants, and no searcher expressed any confusion regarding this terminology. For this reason, we decided to explore whether the participants interpreted these terms consistently. That is the focus of the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparing Strict and Loose Relevance Judgments</head><p>For the o cial results \somewhat relevant" was treated as \not relevant." For the sake of brevity, w e will refer to that as \strict" relevance judgment. We could equally well choose to treat \somewhat relevant" as \relevant," a scenario that we call \loose" relevance judgments. Our key idea was simple: we recomputed the F 0:8 measure with all \somewhat relevant" judgments treated as \relevant," and if the measure increased, it would indicates that on average the participants were being stricter than necessary in making their relevance judgments. Table <ref type="table" coords="9,249.12,559.23,5.04,14.40" target="#tab_3">4</ref> shows the F 0:8 value by search with loose relevance judgments, and Table <ref type="table" coords="9,117.36,571.23,5.04,14.40" target="#tab_4">5</ref> compares the average F 0:8 value by systems and judgment t ype. Higher values are obtained from loose judgments in both cases, but the improvement is far larger for gloss translation than for MT.</p><p>Figure <ref type="figure" coords="9,114.72,594.99,5.04,14.40">3</ref> depicts this di erence for each of the 16 searches, with bars above t h e X axis indicating that loose judgments produce higher values and values below the axis indicating that strict judgments would have been better. Two trends are evident in this data. First, broad topics bene t more from loose judgments than narrow topics. Second, the improvement for gloss translation was more consistent than the improvement for MT. There were 40 judgments of \somewhat relevant" for MT, but only 17 for gloss translation, so more does not seem to be better in this case. It seems that the \somewhat relevant" judgments that people made with MT and and gloss translation were actually di erent in some  fundamental way. One possibility is that our participants treated \somewhat relevant" as a variant o f \unsure," perhaps assigning \somewhat relevant" when they had some inkling that a document might be relevant (i.e., there were not completely unsure).</p><p>Figure <ref type="figure" coords="10,102.24,502.35,3.96,14.40">3</ref>: E ect of loose (better above axis) and strict (better below axis) relevance on F 0:8 . Left: broad topics, right: narrow topics. Each bar is labeled with searcher-topic-system (e.g., u1-t11-m means searcher umd01, Topic11, MT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Recall-Oriented Measures</head><p>It is not possible to determine how a recall-oriented searcher would have b e h a ved from our data because we g a ve the searchers instructions that we expected would cause them to be biased in favor of precision. Nonetheless, we can gain some insight i n to the behavior of recall-oriented measures by computing F 0:2 rather than F 0:8 . T able 6 shows the average values for F 0:2 by topic and system type with strict judgments. Comparison with Table <ref type="table" coords="10,175.92,632.19,5.04,14.40" target="#tab_2">3</ref> shows that MT and gloss translation now a c hieve comparable results on broad topics, with one searcher doing better with gloss, a second doing better with MT, and the other two doing poorly with both. The results for narrow topics are more consistent, with MT beating gloss translation for every searcher for with both precision-oriented and recall-oriented measures. This should not be too surprising, however, since there are so few relevant documents to be found in the case of narrow topics that recall may not be a discriminating factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Individual Di erences</head><p>Figure <ref type="figure" coords="11,103.20,215.79,5.04,14.40" target="#fig_2">4</ref> shows the average F for each participant using all four variants of that measure that were de ned above ( t wo v alues for , with strict and loose relevance for each). Clearly, participant umd01 outperformed the other three, regardless of what measure we use. Recall that umd01, who reported far more experience with online searching than any other participant, is now w orking as an IR system designer. Participant umd01 judged 186 of the 200 available documents in the time allowed, on average the other 3 participants could judged an average of 141 documents. Since most unjudged documents were for broad topics, for which a n a verage of almost 40% of the documents were relevant, our measures penalized searchers more for failing to nish their judgments for broad than for narrow topics. Two other factors that had been of potential concern to us turned out not to make m uch of a di erence. The rst of these was that participant umd03 reported that they had good reading skills in French. As Figure <ref type="figure" coords="11,101.76,521.31,5.04,14.40" target="#fig_2">4</ref> shows, that participant actually achieved the lowest average values for three of the four measures (although two or three other participants were close in every case), and Table <ref type="table" coords="11,423.36,533.31,5.04,14.40" target="#tab_2">3</ref> shows that this poor performance was consistent for both MT and gloss translation. The other factor we had concern about was that some of the subjects might actually know quite a bit about one of the topics. This actually did happen in one case, again with searcher umd03, for topic 29. As it turned out, the value of F for that search w as zero for both values of . Go gure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Subjective E v aluation</head><p>After each experiment, we solicited comments from our participants on the two systems and their degree of con dence in the relevance judgments that they had made. All searchers found the gloss translations were di cult to comprehend, and three of the four participants indicated that it was di cult or very di cult to judge the relevance of documents using gloss translations. All three of those participants felt that their judgment w ould have been even more accurate if they had been able to look at higher quality translations. The exception was participant umd01 who thought i t w as easy to judge relevance with gloss translations and had con dence in the judgments they made with that system. All participants felt that it was easy to make relevance judgments with the MT system, and three of the four indicated that they liked the translation quality (umd02 didn't comment). Two felt that an even higher quality translation could still make relevance judgment m uch easier, while the other two thought i t w ould only help a little bit.</p><p>In comparing the two systems, two participants felt that the di culty of learning to use the two systems was comparable, while the other two felt that the MT system was easier to learn. Three of the four found the MT system easier to use while the remaining participant (umd01 again) found the gloss translation system easier to use. In amplifying on this, participant umd01 wrote that they believed that the gloss translation system seems easier to browse for \factual search questions."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Given that iCLEF is the rst multi-site evaluation or interactive cross-language document selection, we are quite satis ed with the degree of insight that our experiments have provided. Our results suggest that both full machine translation and simple term-for-term gloss translation strategies provide a useful basis for selecting documents in an unfamiliar language, but that there is substantial room for improvement o ver our present gloss translation technique for this task. Perhaps more importantly, w e h a ve found insight in our data into factors that we had not previously considered, such as the importance of providing clear facilities for distinguishing between uncertainty and partial relevance. We h a ve also learned something about the strengths and weakness of our present measures, with perhaps the most important p o i n t being that narrow topics pose a fundamentally di erent search task than broad topics. Perhaps we will ultimately nd that it would be best to model those di erent tasks using di erent e ectiveness measures. This rst iCLEF has indeed pointed the way t o wards an interesting and important set of questions, but much remains to be done.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="6,159.60,352.35,276.24,16.00;6,88.80,0.02,417.60,342.44"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Display of the ranked list of documents (MT).</figDesc><graphic coords="6,88.80,0.02,417.60,342.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,70.80,145.23,454.32,16.00;9,70.80,156.99,421.92,16.00;9,304.80,0.04,216.36,123.11"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) Average number of judgments (b) Average time per judgment, by judgment type. In each c hart, broad topics are on the left and narrow topics are on the right.</figDesc><graphic coords="9,304.80,0.04,216.36,123.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="11,158.16,472.35,279.12,16.00;11,153.60,324.27,288.00,138.24"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Average F by searcher, , and relevance type.</figDesc><graphic coords="11,153.60,324.27,288.00,138.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="10,153.60,315.63,288.00,175.68"><head></head><label></label><figDesc></figDesc><graphic coords="10,153.60,315.63,288.00,175.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,507.36,643.71,17.04,14.40"><head>Table 1 :</head><label>1</label><figDesc>iCLEF-2001 experiment design as run. Topics 11 and 13 are broad, Topic 17 and 29 are narrow.</figDesc><table coords="2,507.36,643.71,17.04,14.40"><row><cell>The</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,438.96,668.67,85.20,14.40"><head>Table 2 :</head><label>2</label><figDesc>F 0:8 by search, as run, strict relevance (o cial results).</figDesc><table coords="7,438.96,668.67,85.20,14.40"><row><cell>Another interesting</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,97.20,262.11,401.52,16.00"><head>Table 3 :</head><label>3</label><figDesc>Average F 0:8 by topic type and system, strict relevance (o cial results).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="10,93.12,10.35,409.20,204.48"><head>Table 4 :</head><label>4</label><figDesc>F 0:8 by search, as run, loose relevance.</figDesc><table coords="10,93.12,10.35,409.20,204.48"><row><cell></cell><cell></cell><cell>MT</cell><cell></cell><cell></cell><cell></cell><cell>GLOSS</cell><cell></cell><cell></cell></row><row><cell cols="9">Searcher Topic11 Topic13 Topic17 Topic29 Topic11 Topic13 Topic17 Topic29</cell></row><row><cell>umd01 umd02 umd03 umd04</cell><cell>0.80 0.74</cell><cell>0.29 0.20</cell><cell>1 1</cell><cell>0.65 0.67</cell><cell>0.38 0.68</cell><cell>0.35 0.17</cell><cell>0 1</cell><cell>0.80 0</cell></row><row><cell></cell><cell></cell><cell cols="2">Relevance</cell><cell></cell><cell>Average F 0:8 MT GLOSS</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Strict Loose</cell><cell></cell><cell></cell><cell>0.61 0.29 0.67 0.42</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Relative improvement 10% 45%</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="10,174.24,233.31,246.96,16.00"><head>Table 5 :</head><label>5</label><figDesc>Comparison of strict and loose relevance.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="11,138.24,10.35,318.96,130.24"><head>Table 6 :</head><label>6</label><figDesc>Average F 0:2 by topic type and system, strict relevance.</figDesc><table coords="11,164.40,10.35,266.40,95.76"><row><cell>Topic</cell><cell>Broad</cell><cell>Narrow</cell><cell>Average</cell></row><row><cell cols="4">Searcher MT GLOSS MT GLOSS MT GLOSS</cell></row><row><cell cols="4">umd01 0.33 0.43 umd02 0.52 0.03 0.93 1 umd03 0.03 0.09 1 umd04 0.09 0.08 0.70 0.55 0.40 0.31 0.93 0.67 0.68 0 0.73 0.01 0 0.52 0.05</cell></row><row><cell cols="4">Average 0.24 0.16 0.91 0.37 0.57 0.62</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,86.16,663.03,431.52,12.00"><p>Consider the case of \Harry hit Tom" and \Tom hit Harry" to see why presentation order can be an accuracy issue.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,86.16,672.63,262.56,12.00"><p>For example, \Tom hitting by Harry" is understandable, but dis uent.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,86.16,620.07,438.72,12.00;4,70.80,629.67,130.80,12.00"><p>Multi-word expressions in the source language are removed from the stemmed term list, so only single-word matches are possible in these last two steps.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors would like to thank <rs type="person">Clara Cabezas</rs> for assistance in setting up the systems used in the study, <rs type="person">Gina Levow</rs> for help with gloss translation, <rs type="person">Bob Allen</rs> for advice on statistical signi cance testing, our participants for their willingness to invest their time in this study, a n d e v eryone from the <rs type="institution">CLIP lab</rs> helping with the peer review of our system. This work has been supported in part by <rs type="funder">D ARPA</rs> cooperative agreement <rs type="grantNumber">N660010028910</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8P74MMN">
					<idno type="grant-number">N660010028910</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,86.40,453.87,438.00,14.40;12,86.40,465.87,437.76,14.40;12,86.40,477.87,438.00,14.40;12,86.40,489.63,88.56,14.40" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,148.08,465.87,221.01,14.40">Do batch and user evaluations give the same results</title>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Turpin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Susan</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dale</forename><surname>Kraemer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lynetta</forename><surname>Sacherek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Olson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,389.76,466.47,134.40,13.60;12,86.40,478.47,407.18,13.60">Proceedings of the 23nd Annual International ACM SIGIR Conference o n R esearch and Development in Information Retrieval</title>
		<meeting>the 23nd Annual International ACM SIGIR Conference o n R esearch and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1998-08">August 1998</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,86.40,509.79,438.00,14.40;12,86.40,521.55,438.00,14.40;12,86.40,533.55,212.64,14.40" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,171.84,509.79,348.58,14.40">Evaluating interactive cross-language information retrieval: Document selection</title>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<ptr target="http://www.glue.umd.edu/oard/research.html" />
	</analytic>
	<monogr>
		<title level="m" coord="12,189.84,522.15,252.76,13.60">Proceedings of the First Cross-Language Evaluation Forum</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<meeting>the First Cross-Language Evaluation Forum</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,86.40,553.47,438.00,14.40;12,86.40,565.47,424.80,14.40" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,269.76,553.47,157.42,14.40">Cross-language information retrieval</title>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anne</forename><forename type="middle">R</forename><surname>Diekema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,447.60,554.07,76.80,13.60;12,86.40,566.07,156.43,13.60">Annual Review of Information Science a n d T echnology</title>
		<imprint>
			<publisher>American Society for Information Science</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,86.40,585.39,438.24,14.40;12,86.40,597.39,438.00,14.40;12,86.40,609.39,87.60,14.40" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,367.68,585.39,156.96,14.40;12,86.40,597.39,72.05,14.40">TREC-9 experiments at Maryland: Interactive CLIR</title>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gina-Anne</forename><surname>Levow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clara</forename><forename type="middle">I</forename><surname>Cabezas</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov" />
	</analytic>
	<monogr>
		<title level="m" coord="12,182.40,597.99,209.47,13.60">The Ninth Text Retrieval Conference (TREC-9)</title>
		<imprint>
			<date type="published" when="2000">vember 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,86.40,629.31,438.24,14.40;12,86.40,641.07,438.00,14.40;12,86.40,653.07,374.88,14.40" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,345.12,629.31,179.52,14.40;12,86.40,641.07,157.10,14.40">CLEF experiments at Maryland: Statistical stemming and backo translation</title>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gina-Anne</forename><surname>Levow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clara</forename><forename type="middle">I</forename><surname>Cabezas</surname></persName>
		</author>
		<ptr target="http://www.glue.umd.edu/oard/research.html" />
	</analytic>
	<monogr>
		<title level="m" coord="12,352.56,641.67,171.84,13.60;12,86.40,653.67,76.12,13.60">Proceedings of the First Cross-Language Evaluation Forum</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<meeting>the First Cross-Language Evaluation Forum</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
