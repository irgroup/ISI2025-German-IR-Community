<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,71.76,83.42,451.36,15.41;1,150.48,99.26,294.10,15.41">Vector-based Semantic Analysis using Random Indexing and Morphological Analysis for Cross-Lingual Information Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,232.08,118.49,54.87,10.89"><forename type="first">Jussi</forename><surname>Karlgren</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SICS</orgName>
								<address>
									<settlement>Stockholm www</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,293.57,118.49,69.15,10.89"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">SICS</orgName>
								<address>
									<settlement>Stockholm www</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,71.76,83.42,451.36,15.41;1,150.48,99.26,294.10,15.41">Vector-based Semantic Analysis using Random Indexing and Morphological Analysis for Cross-Lingual Information Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6F4B0BB8132C8E43CE4B77F438FB3F7E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meaning in Information</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview</head><p>We have built a query expansion and translation tool. When used in one single language it will expand the terms of a query using a thesaurus built for that purpose; when used across languages it will provide numerous translations and near translations for the source language terms. The underlying technology we are testing is that of vector-based semantic analysis, an analysis method related to latent semantic indexing based on stochastic pattern computing, described in other publications. Our tool is built for people who want to formulate a query in another language, and is designed for interactive use. In this year's CLEF we have used it automatically with no human intervention to produce queries for crosslingual retrieval, but we have also designed a pleasing window-based interface for experimentation. The queries we produced were tested on a standard Inquery installation at another site. Our approach, as a data-intensive method, relies on the availability of reasonably large amounts of relevant training data and on the adequate preprocessing of the training material. This paper will briefly describe how we acquired training data, aligned it, analyzed it using morphological analysis tools, and finally built a thesaurus using the data, but will concentrate on an overview of vector-based semantic analysis and how stochastic pattern computing differs from latent semantic indexing in its current form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vector-Based Semantic Analysis using Random Indexing</head><p>Vector-based semantic analysis is a technology for extracting semantically similar terms from textual data by observing the distribution and collocation of terms in text. We have been experimenting with vector-based semantic analysis using stochastic patterns in a technique we call Random Indexing. The result of running a vector-based semantic analysis on a text collection is in effect a thesaurus: an associative model of term meaning. This can be used to build a synonym tool for application e.g. in query expansion. Similarly, the vector-based semantic analysis can be used to find correspondences across languages. If multi-lingual data are used, correspondences from them are as easy to establish as within a language. Random Indexing uses sparse, high-dimensional random index vectors to represent documents (or context regions or textual unit of any size). Given that each document has been assigned a random index vector, term similarities can be calculated by computing a terms-by-contexts co-occurrence matrix. Each row in the matrix represents a term, and the term vectors are of the same dimensionality as are the random vectors assigned to documents. Each time a term is found in a document, that document's random index vector is added to the row for the term in question. In this way, terms are represented in the matrix by high-dimensional semantic context vectors which contain traces of each context the term has been observed in. The underlying assumption is that semantically similar terms will occur in similar contexts, and that their context vectors therefore will be similar to some extent. Thus, it should be possible to calculate the semantic similarity between any given terms by calculating the similarity between their context vectors (mathematically, this is done by calculating the cosine of the angles between the context vectors). This similarity measure will thus reflect the distributional (or contextual) similarity between terms. This technique is akin to latent semantic analysis or indexing, except that no dimension reduction such as singular value decomposition is needed to reduce the dimensions of the co-occurrence matrix, since the dimensionality of the random index vectors is smaller than the number of documents in the text data. This makes the technique more efficient than the latent semantic indexing methods, since singular value decomposition is a computationally demanding operation: random index vectors of a dimension on the order of 1000 may be used to cover a wide range of vocabulary. The technique is also eminently more easily scalable and more flexible as regards unexpected data than are methods which rely on dimensional reduction: a new document does not require a larger matrix but will simply be assigned a new random index vector of the same dimensionality as the preceding ones and a new term requires no more than a new row in the matrix. The size of the context used to accumulate the terms-by-contexts matrix may range from just a few adjacent words on each side of the focus word to entire documents consisting of more than hundred words. Both document-based cooccurrence statistics and narrow context windows have been used in experiments with Random Indexing to calculate semantic term vectors with favorable results. <ref type="bibr" coords="2,238.80,366.29,3.24,7.17">1</ref> In the present experiment, we have used Random Indexing to index aligned bilingual corpora and extract semantically similar words across languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Data</head><p>For data we used a large number of documents of European legislation. The CLEF queries were used to retrieve documents from the WWW service provided by the commission in French, Swedish, and English. 77 documents of a few hundred sentences each were retrieved in each language. The corpus used in these experiments consists of documents in several different languages downloaded from the Eur-Lex website (http://europa.eu.int/eur-lex/) by using keywords in the CLEF queries as search terms. It should be pointed out that, due to the somewhat narrow topical spread of the Eur-Lex database (which consists of legislation texts from the European Union), not every topic returned satisfying search results. Indeed, a few CLEF queries did not return one single relevant document, such as for example keywords to the query: "C044: Indurain Wins Tour. Reactions to the fourth Tour de France won by Miguel Indurain. Relevant documents comment on the reactions to the fourth consecutive victory of Miguel Indurain in the Tour de</p><p>France. Also relevant are documents discussing the importance of Indurain in world cycling after this victory." Each document was downloaded in each of the languages used in the experiments (English, French and Swedish). The documents were concatenated language by language to produce training corpora consisting of roughly 2 million words in each language. Thus, the training data consisted of the same text translated into different languages. This amount of training data is probably near the absolute minimum to be able to provide any reasonable semblance of thesaurus functionality for our purposes; a topically uneven distribution will be immediately reflected in the consistency of results across topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Morphological Analysis</head><p>The texts were morphologically analyzed using tools from Conexor. Conexor's tools provided morphological base forms and, for Swedish, compound splitting. In future experiments, we intend to try make use of more text-oriented analyses as provided by syntactic components of the tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alignment Algorithm</head><p>The translated texts were preprocessed by a series of Perl scripts aligned using a dynamic weighting algorithm written in SICStus Prolog. The algorithm gave high weight to headings and subheadings, anchoring the alignment, and then aligned sentences between anchor points using a matching function based on word overlap and relative length of clauses. This step was much aided by the fact that European legislation is written to match clause by clause between the various official languages: the alignment algorithm could assume that a perfect match was attainable. In fact, this principle is not always adhered to, and the alignment continues to be a non-trivial problem. Unfortunately, processing constraints of various sorts proved problematic for the alignment algorithm. Only about half of the available data were actually processed through the aligner and this, after the data collection itself, proved the most crucial bottleneck for our query processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Thesaurus Tool for Cross Lingual Query Expansion</head><p>The first step in constructing a bilingual thesaurus is to assign a 1,000-dimensional sparse random index vector to each aligned document in the bilingual corpus. These 1,000-dimensional random index vectors consist of 6 randomly distributed -1s and +1s. An index vector thus has 3 randomly distributed -1s and 3 randomly distributed +1s, with the rest set to zero. The random index vectors are then used to accumulate one words-by-contexts cooccurrence matrix per language by adding a document's index vector to the row for a given word every time the word occurs in that document. Words are thus represented in the words-by-contexts matrices by 1,000-dimensional context vectors that represent the relative meaning of words.</p><p>The assumption is that, since the documents will (hopefully) be close translations of each other, they will (hopefully) consist of words that are close translations of each other. As the context vectors effectively consist of the sum of the index vectors of the documents that the words occur in, words that occur in similar documents -i.e. that "are about" similar things -will get similar context vectors. So by comparing the context vectors across languages -i.e. by calculating the cosine of the angles between the vectors -it is possible to extract for each word its nearest neighbors in the other language. Presumably, the nearest neighbor will be a translation, or a near translation, of the word in question.</p><p>So by extracting the five highest correlated terms for each word, we effectively produced a bilingual thesaurus. We also used a threshold for the correlations to avoid extracting terms with very low correlation to the focus word (since terms with low correlation are assumed to be semantically unrelated, or at least not comparatively similar). Such cases may appear for example when the focus word has a low frequency of occurrence in the training data. The threshold was set to exclude words with a correlation less than 0,2 (where 1 could be thought of as a complete match -i.e. two words that have occurred in exactly the same documents -and 0 a complete disaster -i.e. two words that have not co-occurred in a single document (although the randomness of the index vectors makes it possible for even distributionally unrelated words to get a correlation higher than 0)) to the target word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retrieval and CLEF results</head><p>The retrieval itself was done using an Inquery system set up by the Information Sciences department at Tampere University. <ref type="bibr" coords="4,109.92,343.49,3.24,7.17">2</ref> The finished queries were re-edited to Inquery query syntax and retrieved from the CLEF database. We submitted four runs to CLEF: three French-English bilingual runs: description (sicsfed), narrative (sicsfen), narrative+morphological analysis (sicsfenf), and one Swedish-English run: narrative+morphological analysis (sicssen). Using the narrative rather than the description gave about double the number of query terms (more than 20 terms per query rather than about ten); using the morphological analysis almost doubled the number of terms again. The Swedish run provided rather fewer query terms than did the French runs, owing to better aligned training data as far as we can determine.</p><p>The retrieval results by CLEF standards can fairly be characterized as underwhelming. For most queries in our submitted runs our results are well under the median. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run name Average precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interactive Interface and Web Service Availability</head><p>The thesaurus itself is quite demanding as regards memory and will naturally run on some server. It has been built with no regard to usability issues. For experimentation and demonstration purposes, we have designed and built Synkop, an interface for accessing the thesaurus tool over the WWW. Synkop, written entirely in Java using the Swing package, should be able to run on most platforms as a client to a thesaurus server. The thesaurus, as trained for CLEF purposes, will be made network accessible by us using some standard protocol in the near future. The example interface can be downloaded from our web page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lessons Learnt and Proposals for Future CLEFs</head><p>Our approach is based on constructing useful queries, not on more effective retrieval as retrieval. We used Inquery for testing our queries. A standard system for this purpose would be useful -and we suspect other groups might be interested in the same. Could we somehow organize a set-up where some site which experiments with retrieval systems sets up some such service over the net?</p><p>We have several query-oriented problems to work on for future years. This year we paid no attention to interaction between query terms, but translated them one by one. Next year we intend to address the likely collocations between synonym candidates to weed out unlikely combinations. In addition, we made little use of the language analysis tools at our disposal; the syntactic processing may well be crucial for improving understanding of context similarity and we plan to experiment with including syntactic information into the random indexing scheme during next year. Both query analysis and syntax should help us improve precision somewhat.</p><p>But recall is the major problem. Training data and its preprocessing are the major bottlenecks for our approach. Our results will vary with query topic and the availability of training data for that specific topic and domain. This year we only used one information source for the training data and used a an experimental and not very satisfactory alignment tool; for future years we intend to add sources and partially automate the training data acquisition process.</p><p>In a real-life retrieval situation the queries will have to be inspected by the person performing the search: we have built a demonstration interface to illustrate a likely retrieval process where someone searches data in a language they know but do not know well. We will most likely not attempt interactive experimentation, but the interface demonstration will serve as an example of what functionality we can provide. The main aim will continue to be to understand textuality and text understanding -information access is an excellent testing ground for the hypotheses we are working on.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,66.00,492.89,408.66,85.88"><head>Relevant returned Terms/query</head><label></label><figDesc></figDesc><table coords="4,66.00,513.15,353.79,65.62"><row><cell>SICSFED</cell><cell>0,1646</cell><cell>390</cell><cell>12</cell></row><row><cell>SICSFEN</cell><cell>0,2216</cell><cell>433</cell><cell>26</cell></row><row><cell>SICSFENF</cell><cell>0,0864</cell><cell>342</cell><cell>48</cell></row><row><cell>SICSSEN</cell><cell>0,0139</cell><cell>211</cell><cell>34</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,67.98,650.09,432.34,10.89;2,62.40,661.37,453.24,10.89;2,62.40,672.89,73.24,10.89"><p>For reports on experiments using these different context sizes, see Kanerva, P.,Kristofersson, J. and Holst, A.  (2000): Random Indexing of Text Samples for Latent Semantic Analysis. In Gleitman, L.R. and Josh, A.K. (Eds.). Proceedings of the</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1" coords="2,138.10,672.89,364.01,10.89;2,62.40,684.41,462.35,10.89;2,62.40,695.93,432.63,10.89"><p>22nd Annual Conference of the Cognitive Science Society (p. 1036). Mahwah, New Jersey: Erlbaum; and Karlgren, J. and Sahlgren, M. (2001): From Words to Understanding. In Kanerva et al. Eds Real World Intelligence, forthcoming as a CSLI publication, Stanford: Center for the Study of Language and Information.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="4,67.98,695.93,452.85,10.89"><p>We very gratefully thank Heikki Keskitalo for helping us gain access to the Inquery system used at his department.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
