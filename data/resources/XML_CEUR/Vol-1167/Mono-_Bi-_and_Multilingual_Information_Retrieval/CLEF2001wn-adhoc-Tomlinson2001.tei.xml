<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.04,72.62,321.32,15.41">Hummingbird&apos;s Fulcrum SearchServer at CLEF 2001</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2001-08-04">August 4, 2001</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,257.76,100.01,76.81,10.89"><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hummingbird Ottawa</orgName>
								<address>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.04,72.62,321.32,15.41">Hummingbird&apos;s Fulcrum SearchServer at CLEF 2001</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2001-08-04">August 4, 2001</date>
						</imprint>
					</monogr>
					<idno type="MD5">BBA16EAC74099124B9D3B268AD4D4528</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hummingbird submitted ranked result sets for all 5 Monolingual Information Retrieval tasks (German, French, Italian, Spanish and Dutch) of the Cross-Language Evaluation Forum (CLEF) 2001. For each language, at least one SearchServer run had more average precision scores above the median than below. The submitted German, Dutch and Spanish runs, compared to the medians in average precision by topic, had a significance level of less than 1% (by the sign test), which is statistically significant. SearchServer's linguistic expansion was found to boost all investigated precision scores, including average precision and Precision@5, for all 6 investigated languages (German, French, Italian, Spanish, Dutch and English). Enabling linguistic expansion was found to increase average precision by 43% in German, 30% in Dutch, 18% in French, 16% in Italian, 12% in Spanish and 12% in English. In terms of the number of topics of higher and lower average precision, the German, Dutch and Spanish runs with linguistics enabled, compared to corresponding runs with linguistics disabled, had a significance level of less than 1% (by the sign test).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hummingbird's Fulcrum SearchServer kernel is an indexing, search and retrieval engine for embedding in Windows and UNIX information applications. SearchServer, originally a product of Fulcrum Technologies, was acquired by Hummingbird in 1999. Fulcrum, founded in 1983 in Ottawa, Canada, produced the first commercial application program interface (API) for writing information retrieval applications, Fulcrum Ful/Text. The SearchServer kernel is embedded in <ref type="bibr" coords="1,296.94,438.17,4.92,10.89" target="#b4">5</ref> Hummingbird products, including SearchServer, an application toolkit used for knowledge-intensive applications that require fast access to unstructured information.</p><p>SearchServer supports a variation of the Structured Query Language (SQL), called SearchSQL, which has extensions for text retrieval. SearchServer conforms to subsets of the Open Database Connectivity (ODBC) interface for C programming language applications and the Java Database Connectivity (JDBC) interface for Java applications.</p><p>Almost 200 document formats are supported, such as Word, WordPerfect, Excel, PowerPoint, PDF and HTML. Many character sets and languages are supported, including the major European languages, Japanese, Korean, Chinese, Greek and Arabic. SearchServer's Intuitive Searching algorithms were updated for version 4.0 which shipped in Fall 1999, and in subsequent releases of other products. SearchServer 5.0, which shipped in Spring 2001, works in Unicode internally <ref type="bibr" coords="1,342.73,564.65,11.68,10.89" target="#b2">[3]</ref> and contains improved natural language processing technology, particularly for languages with many compound words, such as German, Dutch and Finnish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>All experiments were conducted on a single-cpu desktop system, OTWEBTREC, with a 600MHz Pentium III cpu, 512MB RAM, 186GB of external disk space on one e: partition, and running Windows NT 4.0 Service Pack 6. For the official CLEF runs, internal development builds of SearchServer 5.0 were used (5.0.501.115 1 Core Technology, Research and Development, stephen.tomlinson@hummingbird.com plus some experimental changes motivated by tests on the CLEF 2000 collections). For the diagnostic runs, internal build 5.0.504.157 was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Setup</head><p>We describe how SearchServer was used to handle the 5 Monolingual tasks of CLEF 2001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>The CLEF 2001 collections consisted of tagged (SGML-formatted) news articles (mostly from 1994) in 6 different languages: German, French, Italian, Spanish, Dutch and English. The articles (herein called "documents" or "logical documents") were combined into "library files" of typically a few hundred logical documents each. The collections were made available in compressed tar archives on the Internet. We downloaded the archives, uncompressed them, untarred them and removed any support files (e.g. dtd files) which weren't considered part of the collection. The library files were stored in 6 subdirectories of e:\data\CLEF (German, French, Italian, Spanish, Dutch and English). No further pre-processing was done on the data, i.e. SearchServer indexed the library files directly. Operations such as identifying logical documents and enforcing the restrictions on fields permitted for indexing were handled by the text reader (described below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language</head><p>Text For more information on the CLEF collections, see the CLEF web site <ref type="bibr" coords="2,351.60,441.77,10.45,10.89" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Text Reader</head><p>To index and retrieve data, SearchServer requires the data to be in Fulcrum Technologies Document Format (FTDF). SearchServer includes "text readers" for converting most popular formats (e.g. Word, WordPerfect, etc.) to FTDF. A special class of text readers, "expansion" text readers, can insert a row into a SearchServer table for each logical document inside a container, such as directory or library file. Users can also write their own text readers in C for expanding proprietary container formats and converting proprietary data formats to FTDF.</p><p>The library files of the CLEF 2001 collections consisted of several logical documents, each starting with a &lt;DOC&gt; tag and ending with a &lt;/DOC&gt; tag. After the &lt;DOC&gt; tag, the unique id of the document, e.g. SDA.940101.0001, was included inside &lt;DOCNO&gt;..&lt;/DOCNO&gt; tags. The custom text reader called cTREC, originally written for handling TREC collections <ref type="bibr" coords="2,284.71,604.97,10.82,10.89" target="#b5">[6]</ref>, handled expansion of the library files of the CLEF collections and was extended to support the CLEF guidelines of only indexing specific fields of specific documents.</p><p>In expansion mode (/E switch), cTREC scans the library file and for each logical document determines its start offset in the file (i.e. offset of &lt;DOC&gt; tag), its length in bytes (i.e., distance to &lt;/DOC&gt; tag), and extracts its document id (from inside &lt;DOCNO&gt;..&lt;/DOCNO&gt; tags). SearchServer is instructed to insert a row for each logical document. The filename column (FT_SFNAME) stores the library filename. The text reader column (FT_FLIST) includes the start offset and length for the logical document (e.g. nti/t=Win_1252_UCS2:cTREC/C/100000/30000). The document id column (controllable with the /d switch), contains the document id.</p><p>In CLEF format translation mode (/C switch), cTREC inserts a control sequence to turn off indexing at the beginning of the document. cTREC inserts a control sequence to enable indexing after tags for which indexing is permitted (e.g. after &lt;TEXT&gt; in a Frankfurter Rundschau document), and inserts control sequences to disable indexing just before its corresponding closing tag (e.g. &lt;/TEXT&gt;). The document type is determined from the prefix of the DOCNO (e.g. all Frankfurter Rundschau document ids had "FR" as a prefix). The entities described in the DTD files were also converted, e.g. "&amp;equals;" was converted to the equal sign "=".</p><p>The documents were assumed to be in the Latin-1 character set, the code page which, for example, assigns eacute (é) hexadecimal 0xe9 or decimal 233. cTREC passes through the Latin-1 characters, i.e. does not convert them to Unicode. SearchServer's Translation Text Reader (nti), was chained on top of cTREC and the Win_1252_UCS2 translation was specified via its /t option to translate from Latin-1 to the Unicode character set desired by SearchServer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Indexing</head><p>A separate SearchServer table was created for each language, created with a SearchSQL statement such as the following: The TABLE_LANGUAGE parameter specifies which language to use when performing linguistic operations at index time, such as breaking compound words into component words and stemming them to their base form.</p><formula xml:id="formula_0" coords="3,108.00,316.11,171.22,11.14">CREATE SCHEMA CLEF01DE CREATE</formula><p>The STOPFILE parameter specifies a stop file containing typically a couple hundred stop words to not index; the stop file also contains instructions on changes to the default indexing rules, for example, to enable accentindexing, or to change the apostrophe to a word separator. The PERIODIC parameter prevents immediate indexing of rows at insertion time. The BASEPATH parameter specifies the directory from which relative filenames of insert statements will be applied. The DOCNO column was assigned number 128 and a maximum length of 256 characters.</p><p>Here are the first few lines of the stop file used for the French task:</p><formula xml:id="formula_1" coords="3,108.00,522.03,123.98,79.06">IAC = "\u0300-\u0345" PST="'`" STOPLIST = a à afin # 112</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>stop words not shown</head><p>The IAC line enables indexing of the specified accents (Unicode combining diacritical marks 0x0300-0x0345). Accent indexing was enabled for all runs except the Italian and English runs. Accents were known to be specified in the Italian queries but were not consistently used in the Italian documents. The PST line adds the specified characters (apostrophes in this case) to the list of word separators. The apostrophes were changed to word separators for all submitted runs except the German and English runs. Probably it would have made no difference to have also included it in the German runs. Note that the IAC syntax is new to SearchServer 5.0, and the interpretation of the PST line may differ from previous versions.</p><p>Into each table, we just needed to insert one row, specifying the top directory of the library files for the language, using an Insert statement such as the following: Validating library file rows invoked the cTREC text reader in expansion mode to insert a row for each logical document in the library file, including its document id.</p><formula xml:id="formula_2" coords="4,108.00,106.83,194.83,11.14">INSERT INTO CLEF01DE ( FT_SFNAME,</formula><p>After validating the table, SearchServer indexed the table, in this case using up to 256MB of memory for sorting (as per the BUFFER parameter) and using temporary sort files of up to 2GB (as per the TEMP_FILE_SIZE parameter) (no CLEF collection actually required a sort file that big). The index includes a dictionary of the distinct words (after some Unicode-based normalizations, such as converting to upper-case and decomposed form) and a reference file with the locations of the word occurrences. Additionally, by default, each distinct word is stemmed and enough information saved so that SearchServer can efficiently find all occurrences of any word which has a particular stem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Search Techniques</head><p>The CLEF organizers created 50 "topics" and translated them into many languages. Each translated topic set was provided in a separate file (e.g. the German topics were in a file called "Top-de01.txt"). The topics were numbered from C041 to C090. Each topic contained a "Title" (subject of the topic), "Description" (a onesentence specification of the information need) and "Narrative" (more detailed guidelines for what a relevant document should or should not contain). The participants were asked to use the Title and Description fields for at least one automatic submission per task this year to facilitate comparison of results.</p><p>We created an ODBC application, called QueryToRankings.c, based on the example stsample.c program included with SearchServer, to parse the CLEF topics files, construct and execute corresponding SearchSQL queries, fetch the top 1000 rows, and write out the rows in the results format requested by CLEF. SELECT statements were issued with the SQLExecDirect api call. Fetches were done with SQLFetch (typically 1000 SQLFetch calls per query).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Intuitive Searching</head><p>For all runs, we used SearchServer's Intuitive Searching, i.e. the IS_ABOUT predicate of SearchSQL, which accepts unstructured text. For example, for the German version of topic C041, the Title was "Pestizide in Babykost" (Pesticides in Baby Food), and the Description was "Berichte über Pestizide in Babynahrung sind gesucht" (Find reports on pesticides in baby food). A corresponding SearchSQL query would be: </p><formula xml:id="formula_3" coords="4,108.00,639.63,35.42,11.14">SELECT</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Linguistic Expansion</head><p>SearchServer uses lexicon-based natural language processing technology to "stem" each distinct word to one or more base forms, called stems. For example, in English, "baby", "babied", "babies", "baby's" and "babying" all have "baby" as a stem. Compound words in languages such as German, Dutch and Finnish should produce multiple stems; e.g., in German, "babykost" has "baby" and "kost" as stems.</p><p>By default, Intuitive Searching stems each word in the query, counts the number of occurrences of each stem, and creates a vector. Optionally some stems are discarded (secondary term selection) if they have a high document frequency or to enforce a maximum number of stems, but we didn't discard any stems for our CLEF runs. The index is searched for documents containing terms which stem to any of the stems of the vector.</p><p>Whether or not to stem, whether or not to allow multiple stems per term, and which language to use are controlled by the VECTOR_GENERATOR set option: The above settings were the ones used for the submitted runs, and subsequent experiments (below) confirmed they were the best ones. The main issue was whether to specify "/base" or "/base/single" in the first part of the VECTOR_GENERATOR. Experiments on last year's collections found that "/base" worked better for German and "/base/single" worked better for French and Italian. For languages with compound words, such as German, "/base" is necessary for all components of the compound words to be included. For languages with few compound words, in the rare cases when multiple stems are returned, they often are spurious; e.g. in English, for "Acknowledgements", the 2 stems currently returned are "acknowledgement" and "acknowledgment", which appear to be just alternate spellings, so keeping both stems would improperly double-weight the term, hurting ranking. Based on this reasoning, we assumed "/base" would be better for Dutch (because Dutch contains many compound words), and "/base/single" would be better for Spanish (because it does not contain many compound words); again, experiments below confirmed these were the best choices.</p><p>Besides linguistic expansion, we did not do any other kinds of query expansion. For example, we did not use approximate text searching for spell-correction because the queries were believed to be spelled correctly. We did not use row expansion or any other kind of blind feedback technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Statistical Relevance Ranking</head><p>SearchServer calculates a relevance value for a row of a table with respect to a vector of stems based on several statistics. The inverse document frequency of the stem is estimated from information in the dictionary. The term frequency (number of occurrences of the stem in the row (including any term that stems to it)) is determined from the reference file. The length of the row (based on the number of indexed characters in all columns of the row, which is typically dominated by the external document), is optionally incorporated. The already-mentioned count of the stem in the vector is also used. SearchServer synthesizes this information into a relevance value in a manner similar to <ref type="bibr" coords="6,232.07,83.93,11.61,10.89" target="#b3">[4]</ref> (particularly the Okapi approach to term frequency dampening) and also shares some elements of <ref type="bibr" coords="6,188.69,95.45,10.45,10.89" target="#b4">[5]</ref>. SearchServer's relevance values are always an integer in the range 0 to 1000.</p><p>SearchServer's RELEVANCE_METHOD setting can be used to optionally square the importance of the inverse document frequency (by choosing a RELEVANCE_METHOD of 'V2:4' instead of 'V2:3'). SearchServer's RELEVANCE_DLEN_IMP parameter controls the importance of document length (scale of 0 to 1000) to the ranking.</p><p>Because the evaluation program (trec_eval) may re-order rows with the same relevance value, we post-process the results files by adding a descending fraction (0.999, 0.998, 0.997, etc.) to the relevance values of the rows for each topic to ensure that the order of evaluation is identical to the order SearchServer returned the documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Query Stop Words</head><p>Our QueryToRankings program removed words such as "find", "relevant" and "document" from the topics before presenting them to SearchServer, i.e. words which are not stop words in general but were commonly used in the topics as general instructions. The lists for the CLEF languages were developed by examining the CLEF 2000 topics (not this year's topics). After receiving the relevance assessments this year, we did an experiment, and this step had only a minor benefit; the average precision increased just 1% or 2% in all languages (in absolute terms, from 0.0031 in Italian to 0.0107 in French). It doesn't appear to be important to comb the old topics files for potential query stop words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Below we present an analysis of our results, including results of some unofficial "diagnostic" runs. We look at the following evaluation measures: Precision is the percentage of retrieved documents which are relevant. Precision@n is the precision after n documents have been retrieved. Average precision for a topic is the average of the precision after each relevant document is retrieved (using zero as the precision for relevant documents which are not retrieved). Recall is the percentage of relevant documents which have been retrieved. Interpolated precision at a particular recall level for a topic is the maximum precision achieved for the topic at that or any higher recall level. For a set of topics, the measure is the average of the measure for each topic (i.e. all topics are weighted equally).</p><p>The Monolingual Information Retrieval tasks were to run 50 queries against document collections in the same language and submit a list of the top-1000 ranked documents to CLEF for judging (in June 2001). The 5 languages were German, French, Italian, Spanish and Dutch. CLEF produced a "qrels" file for each of the 5 tasks: a list of documents judged to be relevant or not relevant for each topic. From these, the evaluation measures were calculated with Chris Buckley's trec_eval program. Additionally, the CLEF organizers translated the topics into many more languages, including English, and also provided a comparable English document collection, for use in the multilingual task. By grepping the English results out of the multilingual qrels, we were able to produce a comparable monolingual English test collection for diagnostic runs.</p><p>For some topics and languages, no documents were judged relevant. The precision scores are just averaged over the number of topics for which at least one document was judged relevant.</p><p>When comparing two runs and finding that one run had a higher score on more topics than the other, we like to know how likely it is that this result could have happened by chance. For each result, we use the sign test to compute the significance level (also known as the p-value) of the result in relation to the hypothesis that when the two runs differ in their average precision score on a topic (by the 4 th decimal place), they are equally likely to have the higher score. If the significance level is 1% or less, then we can say the hypothesis is contradicted by the result at the 1% level and consider the result to be statistically significant, i.e. this result is unlikely to have happened if the runs were really using equally viable approaches regarding the average precision measure.</p><p>The computation of the significance level is straightforward. Let f(x,n) = (n!/((n-x)!x!))*(0.5^n), which is the probability of x successes in n trials when the probability of success is 0.5. Let X 1 be the number of topics on which run 1 scored higher, and let X 2 be the number of topics on which run 2 scored higher. Let n = X 1 + X 2 . Let X min = min(X 1 , X 2 ). If X 1 &lt;&gt; X 2 , then the significance level is 2*(f(0,n) + f(1,n) + … + f(X min ,n)). If X 1 ==X 2 , then the significance level is 100%. Example: if in 50 topics, one run scores higher in 34, lower in 15, and ties in 1, then the significance level is 2*(f(0,49) + f(1,49) + … + f(15,49)), which is 0.9%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Submitted runs</head><p>Tables 3.1 to 3.5 show the precision scores of our submitted runs for each language of the Monolingual Information Retrieval task. The CLEF organizers have also given the participants the median average precision scores on each topic for each language. We show the number of topics on which our runs scored higher, lower and tied (to 4 decimal places) with the median in average precision, and compute the significance level.</p><p>For each language, at least one SearchServer run had more average precision scores above the median than below. The submitted German, Dutch and Spanish runs had a significance level of less than 1%.</p><p>All submitted runs used both the Title and Description fields. Runs humDE01, humFR01, humIT01, humES01 and humNL01 used relevance method 'V2:3' and RELEVANCE_DLEN_IMP 500; these settings typically worked best on last year's collections. Runs humDE01x, humFR01x, humIT01x, humES01x, humNL01x used relevance method 'V2:4' and RELEVANCE_DLEN_IMP 750, which worked well on the TREC-9 Main Web Task last year <ref type="bibr" coords="7,139.10,327.77,10.95,10.89" target="#b5">[6]</ref>. After receiving the relevance assessments, preliminary experiments suggest that a combination of 'V2:3' and 750 would have been best for most languages this year, but it makes little difference. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,108.00,319.85,390.73,109.78"><head>Table 1 : Sizes of CLEF 2001 Collections</head><label>1</label><figDesc></figDesc><table coords="2,108.00,319.85,390.73,85.30"><row><cell></cell><cell>Size (uncompressed)</cell><cell>Number of Documents</cell><cell>Number of Library Files</cell></row><row><cell>German</cell><cell>555,285,140 bytes (530 MB)</cell><cell>225,371</cell><cell>520</cell></row><row><cell>French</cell><cell>253,528,734 bytes (242MB)</cell><cell>87,191</cell><cell>682</cell></row><row><cell>Italian</cell><cell>290,771,116 bytes (277MB)</cell><cell>108,578</cell><cell>721</cell></row><row><cell>Spanish</cell><cell>544,347,121 bytes (519MB)</cell><cell>215,738</cell><cell>364</cell></row><row><cell>Dutch</cell><cell>558,560,087 bytes (533MB)</cell><cell>190,604</cell><cell>1228</cell></row><row><cell>English</cell><cell>441,048,231 bytes (421MB)</cell><cell>113,005</cell><cell>365</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,108.00,316.11,259.78,67.78"><head>TABLE CLEF01DE</head><label>CLEF01DE</label><figDesc></figDesc><table /><note coords="3,108.00,327.39,141.70,11.14;3,108.00,338.67,135.79,11.14;3,108.00,349.95,123.98,11.14;3,108.00,361.47,47.23,11.14;3,108.00,372.75,141.70,11.14"><p>(DOCNO VARCHAR(256) 128) TABLE_LANGUAGE 'GERMAN' STOPFILE 'GER_AW.STP' PERIODIC BASEPATH 'E:\DATA\CLEF';</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,72.00,106.83,451.64,136.79"><head></head><label></label><figDesc>The VALIDATE TABLE option of the VALIDATE INDEX statement causes SearchServer to review whether the contents of container rows, such as directory rows and library files, are correctly reflected in the table. In this particular case, SearchServer initially validated the directory row by inserting each of its sub-directories and files into the table. Then SearchServer validated each of those directory and library file rows in turn, etc.</figDesc><table coords="4,72.00,106.83,402.13,79.54"><row><cell>FT_FLIST ) VALUES</cell></row><row><cell>('GERMAN', 'cTREC/E/d=128:s!nti/t=Win_1252_UCS2:cTREC/C/@:s');</cell></row><row><cell>To index each table, we just executed a Validate Index statement such as the following:</cell></row><row><cell>VALIDATE INDEX CLEF01DE VALIDATE TABLE</cell></row><row><cell>TEMP_FILE_SIZE 2000000000 BUFFER 256000000;</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,108.00,639.63,407.29,56.50"><head></head><label></label><figDesc>This query would create a working table with the 2 columns named in the SELECT clause, a REL column containing the relevance value of the row for the query, and a DOCNO column containing the document's identifier. The ORDER BY clause specifies that the most relevant rows should be listed first. The statement "SET MAX_SEARCH_ROWS 1000" was previously executed so that the working table would contain at most 1000 rows.</figDesc><table coords="4,108.00,639.63,407.29,56.50"><row><cell>RELEVANCE('V2:3') AS REL, DOCNO</cell></row><row><cell>FROM CLEF01DE</cell></row><row><cell>WHERE FT_TEXT IS_ABOUT 'Pestizide in Babykost Berichte über Pestizide</cell></row><row><cell>in Babynahrung sind gesucht'</cell></row><row><cell>ORDER BY REL DESC;</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,108.00,317.69,353.99,109.53"><head>Table 2 : Recommended VECTOR_GENERATOR Settings (SearchServer 5.0)</head><label>2</label><figDesc></figDesc><table coords="5,108.00,317.69,338.68,36.09"><row><cell>Language</cell><cell>Recommended VECTOR_GENERATOR (SearchServer 5.0)</cell></row><row><cell>German</cell><cell>'word!ftelp/lang=german/base | * | word!ftelp/lang=german/expand'</cell></row><row><cell>French</cell><cell>'word!ftelp/lang=french/base/single | * | word!ftelp/lang=french/expand'</cell></row></table><note coords="5,108.00,355.13,25.14,10.89;5,163.21,355.13,282.47,10.89;5,108.00,367.37,31.16,10.89;5,163.20,367.37,292.42,10.89;5,108.00,379.61,24.05,10.89;5,163.20,379.61,250.68,10.89;5,108.00,391.85,30.07,10.89;5,163.20,391.85,290.13,10.89"><p>Italian 'word!ftelp/lang=italian/base/single | * | word!ftelp/lang=italian/expand' Spanish 'word!ftelp/lang=spanish/base/single | * | word!ftelp/lang=spanish/expand' Dutch 'word!ftelp/lang=dutch/base | * | word!ftelp/lang=dutch/expand' English 'word!ftelp/lang=english/base/single | * | word!ftelp/lang=english/expand'</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,90.00,375.05,395.14,146.50"><head>Table 3 .1: Precision of Submitted German runs</head><label>3</label><figDesc></figDesc><table coords="7,90.00,375.05,395.14,146.50"><row><cell></cell><cell>AvgP</cell><cell>P@5</cell><cell>P@10</cell><cell>P@20</cell><cell>Rec0</cell><cell>Rec30</cell><cell>vs Median</cell><cell>SigL</cell></row><row><cell>humDE01</cell><cell>0.4403</cell><cell>54.7%</cell><cell>49.8%</cell><cell>44.6%</cell><cell>0.7836</cell><cell>0.5377</cell><cell>39-8-2</cell><cell>0.0%</cell></row><row><cell>humDE01x</cell><cell>0.4474</cell><cell>56.3%</cell><cell>51.4%</cell><cell>44.8%</cell><cell>0.8206</cell><cell>0.5521</cell><cell>36-9-4</cell><cell>0.0%</cell></row><row><cell>Median</cell><cell>0.3660</cell><cell>n/a</cell><cell>n/a</cell><cell>n/a</cell><cell>n/a</cell><cell>n/a</cell><cell>0-0-49</cell><cell>-</cell></row><row><cell>Run</cell><cell>AvgP</cell><cell>P@5</cell><cell>P@10</cell><cell>P@20</cell><cell>Rec0</cell><cell>Rec30</cell><cell>vs Median</cell><cell>SigL</cell></row><row><cell>humFR01</cell><cell>0.4825</cell><cell>55.5%</cell><cell>44.3%</cell><cell>36.3%</cell><cell>0.8149</cell><cell>0.5936</cell><cell>28-14-7</cell><cell>4.4%</cell></row><row><cell>humFR01x</cell><cell>0.4789</cell><cell>49.8%</cell><cell>42.2%</cell><cell>35.4%</cell><cell>0.7788</cell><cell>0.5811</cell><cell>27-18-4</cell><cell>23%</cell></row><row><cell>Median</cell><cell>0.4635</cell><cell>n/a</cell><cell>n/a</cell><cell>n/a</cell><cell>n/a</cell><cell>n/a</cell><cell>0-0-49</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="7,90.00,535.13,395.14,84.33"><head>Table 3 .2: Precision of Submitted French runs</head><label>3</label><figDesc></figDesc><table coords="7,90.00,571.13,395.14,48.33"><row><cell>Run</cell><cell>AvgP</cell><cell>P@5</cell><cell>P@10</cell><cell>P@20</cell><cell>Rec0</cell><cell>Rec30</cell><cell>vs Median</cell><cell>SigL</cell></row><row><cell>humIT01</cell><cell>0.4555</cell><cell>54.0%</cell><cell>49.4%</cell><cell>41.2%</cell><cell>0.7830</cell><cell>0.5727</cell><cell>23-20-4</cell><cell>76%</cell></row><row><cell>humIT01x</cell><cell>0.4332</cell><cell>50.6%</cell><cell>46.2%</cell><cell>39.8%</cell><cell>0.7069</cell><cell>0.5421</cell><cell>15-30-2</cell><cell>2.6%</cell></row><row><cell>Median</cell><cell>0.4578</cell><cell>n/a</cell><cell>n/a</cell><cell>n/a</cell><cell>n/a</cell><cell>n/a</cell><cell>0-0-47</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="7,201.60,633.05,192.16,10.89"><head>Table 3 .3: Precision of Submitted Italian runs</head><label>3</label><figDesc></figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Glossary:</head><p>AvgP: Average Precision (defined above) P@5, P@10, P@20: Precision after 5, 10 and 20 documents retrieved, respectively Rec0, Rec30: Interpolated Precision at 0% and 30% Recall, respectively vs Median: Number of topics on which the run scored higher, lower and equal (respectively) to the median average precision (to 4 decimal places) SigL: Significance Level: the probability of a result at least as extreme (vs Median) assuming it is equally likely that a differing score will be higher or lower</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Impact of Linguistic Expansion</head><p>To measure the benefits of SearchServer's linguistic technology, Tables 4.1 to 4.6 show runs which were done with a more recent SearchServer build in August 2001. For each language, the runs vary in their VECTOR_GENERATOR setting. The first run set VECTOR_GENERATOR to the empty string, which disables linguistic processing. The second run set /base but not /single in the first part of the VECTOR_GENERATOR, which allowed all stems for a term to be added to the vector. The third run set /base and /single in the first part of the VECTOR_GENERATOR, allowing only a single stem to be added to the vector for each term. For all these runs, both the Title and Description were used, the relevance method was 'V2:3' and the RELEVANCE_DLEN_IMP setting was 750: Impact of Linguistic Expansion: For all 6 investigated languages (German, French, Italian, Spanish, Dutch and English), SearchServer's linguistic expansion was found to boost all investigated precision scores, including average precision and Precision@5. For 3 languages, German, Dutch and Spanish, the results (of comparing the number of topics of higher and lower average precision with linguistics enabled and disabled) had a significance level of less than 1%.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="10,85.93,109.13,302.54,10.89" xml:id="b0">
	<monogr>
		<ptr target="http://www.clef-campaign.org/" />
		<title level="m" coord="10,85.93,109.13,172.51,10.89">Cross-Language Evaluation Forum web site</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,89.07,132.17,434.44,10.89;10,72.00,143.69,218.95,10.89" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,173.75,132.17,124.22,10.89">CLEF 2000 Result Overview</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Braschler</surname></persName>
		</author>
		<ptr target="http://www.iei.pi.cnr.it/DELOS/CLEF/CLEF_OVE.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="10,423.48,132.17,95.02,10.89">CLEF 2000 Workshop</title>
		<imprint/>
	</monogr>
	<note>Slides of presentation at</note>
</biblStruct>

<biblStruct coords="10,86.73,166.73,437.01,10.89;10,72.00,178.25,221.62,10.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,166.48,166.73,204.10,10.89">Converting the Fulcrum Search Engine to Unicode</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Hodgson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,392.72,166.73,131.03,10.89;10,72.00,178.25,43.47,10.89">Sixteenth International Unicode Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-03">March 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,87.53,201.29,436.12,10.89;10,72.00,212.81,451.44,10.89;10,72.00,224.09,284.27,10.89" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,488.42,201.29,35.23,10.89;10,72.00,212.81,31.07,10.89">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec3/t3_proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="10,213.02,212.81,243.45,10.89">Overview of the Third Text REtrieval Conference (TREC-3)</title>
		<title level="s" coord="10,468.18,212.81,55.26,10.89">NIST Special</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="500" to="226" />
		</imprint>
		<respStmt>
			<orgName>City University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="10,87.07,247.13,436.22,10.89;10,72.00,258.65,451.66,10.89;10,72.00,270.17,315.70,10.89" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,411.44,247.13,71.88,10.89">AT&amp;T at TREC-7</title>
		<author>
			<persName coords=""><forename type="first">Amit</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donald</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec7/t7_proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="10,225.61,258.65,265.24,10.89">Proceedings of the Seventh Text REtrieval Conference (TREC-7)</title>
		<title level="s" coord="10,501.64,258.65,22.02,10.89;10,72.00,270.17,76.26,10.89">NIST Special Publication</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Seventh Text REtrieval Conference (TREC-7)</meeting>
		<imprint>
			<biblScope unit="page" from="500" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,86.95,293.21,436.52,10.89;10,72.00,304.73,451.50,10.89;10,72.00,316.25,340.02,10.89;10,72.00,350.57,137.76,10.89" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,257.94,293.21,202.01,10.89">Hummingbird&apos;s Fulcrum SearchServer at TREC-9</title>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><surname>Blackwell</surname></persName>
		</author>
		<ptr target="http://trec.nist.gov/pubs/trec9/t9_proceedings.html" />
	</analytic>
	<monogr>
		<title level="m" coord="10,254.51,304.73,264.30,10.89">Proceedings of the Ninth Text REtrieval Conference (TREC-9)</title>
		<title level="s" coord="10,72.00,316.25,100.58,10.89">NIST Special Publication</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Ninth Text REtrieval Conference (TREC-9)</meeting>
		<imprint>
			<biblScope unit="volume">500</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.00,373.61,417.58,10.89;10,72.00,385.13,397.35,10.89" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="10,228.33,373.61,261.25,10.89;10,72.00,385.13,392.94,10.89">SearchSQL and Intuitive Searching are the intellectual property of Hummingbird Ltd. All other company and product names are trademarks of their respective owners</title>
		<author>
			<persName coords=""><forename type="first">Fulcrum</forename><surname>Hummingbird</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Searchserver</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
