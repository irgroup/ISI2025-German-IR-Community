<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,90.00,71.42,415.21,15.41">IR-n, a passage retrieval system from University of Alicante, at Clef 2001</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,225.12,98.81,64.78,10.89"><forename type="first">Fernando</forename><surname>Llopis</surname></persName>
							<email>llopis@dlsi.ua.esdepto.lenguajes</email>
							<affiliation key="aff0">
								<orgName type="department">Sistemas Informáticos</orgName>
								<orgName type="institution">Universidad de Alicante Campus de</orgName>
								<address>
									<addrLine>San Vicente del Raspeig Apartado 99</addrLine>
									<postCode>03080</postCode>
									<settlement>Alicante</settlement>
									<country>Spainj</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,302.20,98.81,36.90,10.89"><forename type="first">Jose</forename><surname>Luis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sistemas Informáticos</orgName>
								<orgName type="institution">Universidad de Alicante Campus de</orgName>
								<address>
									<addrLine>San Vicente del Raspeig Apartado 99</addrLine>
									<postCode>03080</postCode>
									<settlement>Alicante</settlement>
									<country>Spainj</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,90.00,71.42,415.21,15.41">IR-n, a passage retrieval system from University of Alicante, at Clef 2001</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F1DC4091FDFC7688245A3FD26A0C1DA4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous works showed that the use of document passages like basic unit of information, to calculate the relevance of a document to a question, improve the results of the information retrieval systems sensibly. However, IR community has not arrived to a consent about how to define those text passages so that the system can improve the efficiently. This paper reports on experiments with IR-n system, a information retrieval system based on the selection of passages of variable size as basic unit of information, in the monolingual (Spanish) and bilingual (Spanish-English) tasks at CLEF-2001. The IR-n system has been developed this year in the Language Processing and Information Systems research group at the University of Alicante.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Information retrieval systems (RI) has as the main objective select, from a collection of documents, most relevant ones for a certain question. These systems measures the level of similarity between a document and the question. The frequency of appearance of the terms of the question in the document is very important to calculate this similarity. This can cause, in texts of considerable size mainly, that appearance high frequencies of some terms of the question, propitiates that a document can be considered relevant without being really.</p><p>An alternative to this model is the proposal of systems of RI that values the relevance of the documents in function of the relevance of the fragments or passages that forms them, where each passage is a group of contiguous text inside the document. This approach denominated passage retrieval, (PR), allows that the calculation of similarity is not affected excessively by the size of the document and can determine with more precision the part of the document that is more relevant to the question (It's very important when the document has a great size).</p><p>Systems that uses technical of PR are more complex than systems of traditional RI. First, because they require store a more quantity of information for each one of the terms in the document (usually the position that occupies in the document) and in second place, that the number necessary calculations to evaluate the relevance of each one of the passages of the document is higher. Nevertheless, evaluations carried out in other works <ref type="bibr" coords="1,198.71,518.57,11.81,10.89" target="#b0">[1]</ref>[2] <ref type="bibr" coords="1,222.32,518.57,11.81,10.89" target="#b7">[8]</ref> reflects that the increment of complexity is rewarded with better results.</p><p>In <ref type="bibr" coords="1,96.45,553.13,11.60,10.89" target="#b1">[2]</ref> the types of passages are divided in three classes: discourse (based upon textual discourse units); there is a type classification, Semantic (based upon the subject or content of the text) or window (based on the number of words).</p><p>The IR-n system that develops this work is included in the models of PR based on the discourse The system uses passages of variable size that are defined based on a certain number of sentences. The passages are generated in overlapping way in the document, that is to say, if the size of the paragraph is N, the first paragraph will be formed from sentence 1 to N, the second from 2 to N+1 and so on. This way, the similarity of each one of the passages of a document with the question will be evaluated and finally this document will be punctuated with the best valuation that any of the passages that forms it has obtained.</p><p>This paper is structured in the next way, in the following section IR-n system architecture is described. Afterwards we analyse the results obtained by a the different test we do at clef 2001. Finally we extracted initial conclusions and open directions for future work . IR-n system has been implemented in C++ in a cheap Linux box. We have good times in process of indexing and retrieval. IR-n system is structured in three modules: Indexing module, Questions module and Retrieval module. First module processes and indexes all the collection of documents; Question module processes the question and expands or translates, if it is necessary. Retrieval module ranks the documents according to a similarity of a documents and query. Figure <ref type="figure" coords="2,262.34,459.05,4.92,10.89">1</ref> shows the architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Indexing module</head><p>This module has as main objective the generation of the dictionaries with the necessary information to use in the retrieval process. The indexing terms consists of character strings made up of letters, numbers and symbols whose length is minor than 21 characters. Previous to their indexation documents are preprocessed to detect sentence boundaries, and part of speech tagging terms. Most frequent terms are eliminated using of a list of words (stop-words) also .</p><p>The system requires storing for each term, the number of documents where each term appears and for each one of the texts the number of appearances of each term in the text and the position of each word in the text (sentence number and order inside the sentence). This supposes an increment of the information to store regarding those systems of information retrieval based on complete documents.</p><p>For each term we store the stem when we work with English documents and lemma in Spanish documents. It is due to we think that in Spanish the stem may not be relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Question module</head><p>In bilingual task, the first step of question pre-processing consists on translating the topic from Spanish to English using a commercial translator. We want to test if it is possible to use commercial translators and to have good results.</p><p>The following step eliminates stop-words ant detects stem (English topics) or lemma(Spanish topics) in both tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dictionary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relevant Documents</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RETRIEVAL-MODULE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Documents</head><p>Sentence Detector POS-tagging</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INDEXING -MODULE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Questions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence Detector POS-tagging Translator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QUESTIONS -MODULE</head><note type="other">WordNet Commercia Translator Figure 1</note><p>. IR-r system architecture Query expansion using semantic information was used in one of the tests. Basically consists in obtaining the synonyms for each term of topic, using Wordnet, a lexical thesaurus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Retrieval module</head><p>This module is the one in charge of recovering the documents in function of its similarity with the question. The process of measure the similarity of each document and presentation of results is the following one:</p><p>1. Order the question terms from smaller to larger in function of the number of documents of the collection in those that they appears. 2. Obtain the documents that contains at least one term.</p><p>3. Calculate the value of similarity of each document 4. Order the documents in function of their similarity to the question. 5. Visualization of the results in form of orderly list.</p><p>To calculate the similarity of topic with a document, system calculate the similarity of topic with each passage of document (where a passage is a number of sentences contiguous in the document) first and after system assigns the document the highest value in similarity of the passages that forms it</p><p>The similarity of topic with a passage is calculated in the next way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarity of the passage</head><formula xml:id="formula_0" coords="3,84.96,343.17,198.32,37.01">= ∑ ∧ ∈ d p t t q, t p, W * W Where:</formula><p>Wp,t = log(fp,t + 1). being fp,t the number of appearances of term t in passage p.</p><p>Wq,t = loge(fq,t + 1) * idf. Being fq,t the number of appearances of term t in question q. idf = loge(N / ft + 1). being N the number of documents of the collection, and ft is the number of different documents where the term appears t.</p><p>As it can be observed, the formulation used to value the similarity between each passage and the question is similar to the measure of the cosine <ref type="bibr" coords="3,245.02,484.25,15.31,10.89" target="#b9">[10]</ref>. The only difference is that the normalization that uses this measure is omitted, we think that this normalization is not necessary due to the size of passages is not so much different between them.</p><p>One of the main things for us was to determine the number of sentences to improve the results. For it, we test passages of several sizes (in number of sentences) in the collection and topics of past year. The results can be seen at table 1.  <ref type="table" coords="4,266.39,266.68,5.47,10.98">1</ref> It can be observe that better results are obtained when the passages are formed by 20 sentences, and then this is the size we choose for Clef 2001 experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Precision in Passage retrieval</head><p>We take another measure for reducing the memory requirements and execution time. In the step of the obtain the documents that contains at leas one term of the topic, we work with a limitation, only adding new documents until arrive to 5% of the number of total documents. This measure is mentioned as efficient in <ref type="bibr" coords="4,131.62,359.45,10.54,10.89" target="#b0">[1]</ref>, we have also corroborated that increasing these percentage sensitive improvements is not obtained in the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments and results</head><p>This year we have participated in two tasks in clef 2001, bilingual (Spanish-English) and monolingual (Spanish). The bilingual task consists in querying in Spanish a document collection of English texts. Monolingual task consists query a Spanish collection in Spanish. The test collection for CLEF 2001 consists of SGML-formatted documents from national newspapers (Los Angeles Times -1994) for the first task and news agency (Agencia EFE S.A. Spanish news agency-1994) for the second task, main topic set consists of 50 topics and is prepared in Spanish.</p><p>We have carried out 3 runs in monolingual task (called EI, PR, PRM) and 4 runs in bilingual task(called EI, PR, PRM ,and EXP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Runs descriptions</head><p>EI run. This run calculates the similarity of the topic with each document using a standard method of information retrieval (cosine measure <ref type="bibr" coords="4,235.26,580.25,15.74,10.89" target="#b9">[10]</ref>) based on complete documents, PR run. This run uses the method proposed in IR-n system.</p><p>EXP run. This run was used only for bilingual task. It consists in adding a topic, the main synonyms for each term and after execute the retrieval process proposed in IR-n system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PRM run.</head><p>In this run we determine the sentence boundaries in topic (including title, description and narrative)first , we calculate for each sentence of topic the measures of similarity (like PR run) and we order the relevant documents in function of the mean of measures of similarity of each sentence. This measure gives more importance to title, due to terms of title usually appears in narrative too.</p><p>The first run uses title and description of the topic, second and third only use only title and the fourth use all terms from the topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results in Monolingual Task</head><p>In monolingual we have carried out three tests (EI; PR and PRM).</p><p>In this task, the results using passage retrieval (PR and PRM runs) give no significant improvement than using Information retrieval with full documents. We think that this results may be due to the size of most of documents of the collection.</p><p>The results using short query (PR run) are comparable using long query (PRM run). It may be due to then most of the relevant documents that includes narrative terms, including title terms too..  Recall Precision EI PR PRM</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results in Bilingual task</head><p>In monolingual we carried out four tests (EXP, EI, PR and PRM).</p><p>We have obtained the better results with the PRM test. In bilingual task we have a significant improvement using passage retrieval. The sensible difference, in this case, between using long or short queries (PRM and PR runs) may be due to many relevant documents containing the terms of topic narrative and no the terms of title. Another important aspect is the bad result we have obtained using query expansion with semantic information. The fact of expand the topic terms has produced that the result has even been worse than the standard measure. The comparative of the results of several runs in both tasks has demonstrate us that the line we choose (Passage retrieval) is better than work with full documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Results of IR-n system al clef 2001</head><p>Tables <ref type="table" coords="6,114.17,399.77,4.92,10.89">2</ref> and<ref type="table" coords="6,139.01,399.77,4.92,10.89">3</ref> depicts the results of our runs and the median results obtained by the other systems at clef 2001. In these tables we report the median average precision for all the individual queries. In this bilingual task IR-n system has obtained a sensibly better result than the median at clef 2001. We have realized that the behaviour of system in monolingual and bilingual are really similar. Also, it is possible that if we don't have so many errors in the process of tag the Spanish documents , the result in monolingual task had been better than bilingual task.</p><p>Nevertheless it is strange the median at clef 2001 is very different between both tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and future work</head><p>As the results demonstrate , use of groups of sentences like basic text unit for the measure of the similarity between questions and documents in the environment of a RI systems, has been revealed as a very effective technique. Nevertheless it is possible than the selection of the appropriate size of passage depends on the type of documents collection. In bilingual task (La Times ) we have obtained a considerable improvement when we use Passage Retrieval techniques, but this improvement is small when we work with the document collection in monolingual task ( Efe).</p><p>Also, the type of collection is significant in respect of the type of question. The results reach using title+narrative are better than using only title + description, however the improve reached is no significant in Spanish collection and is significant in English collection.</p><p>The use of techniques of query expansion has reached sensible worst results than the rest of test, It is possible that using this techniques with passage retrieval directly, giving the same weight at all the terms ( base and expanded terms) it is no effective.</p><p>The use of a commercial translator in the bilingual task, without manual supervision, does not seems being a bad election. In fact our results in Bilingual task are over the median results.</p><p>After this first experience we open several lines of future work. We want to study the number of sentences that should conform a paragraph to improve the results, and possibly determine this number in function of the type of question or document collection. We think that optimise the searching process with the intention of reducing the temporary complexity of the process is important .In spite of the bad results reached using query expansion we want to continue studying the advantages and inconveniences of carrying out a process of expansion of the questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,84.96,197.69,209.06,10.89"><head>Figure 2</head><label>2</label><figDesc>Figure 2 depicts the results reach with these tree runs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,246.99,232.36,151.22,13.35"><head>Figure 2</head><label>2</label><figDesc>Figure 2. Monolingual task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,84.96,629.21,216.72,10.89"><head>Figure 3 Figure 3</head><label>33</label><figDesc>Figure 3 depict the results reached with these four tests</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,84.96,435.88,425.34,302.34"><head></head><label></label><figDesc>In table2we have realized that our results are worse than the median in clef 2001. We think that our result may be due to the incorrect election of a tagger we use for Spanish we use. In experimental test we detect many errors in the process of lemma election.</figDesc><table coords="6,214.32,435.88,166.67,302.34"><row><cell>Monolingual</cell><cell>Average</cell><cell>Increment</cell></row><row><cell>task</cell><cell>precision</cell><cell></cell></row><row><cell>Median</cell><cell>0.4976</cell><cell>0.0</cell></row><row><cell>Runs</cell><cell></cell><cell></cell></row><row><cell>PRM</cell><cell>0,3528</cell><cell>-28.09</cell></row><row><cell>PR</cell><cell>0,3287</cell><cell>-33.94</cell></row><row><cell>EI</cell><cell>0,3297</cell><cell>-33.74</cell></row><row><cell cols="3">Table2 . IR-n at clef 2001</cell></row><row><cell cols="3">Monolingual task</cell></row><row><cell>Monolingual</cell><cell>Average</cell><cell>Increment</cell></row><row><cell>task</cell><cell>precision</cell><cell></cell></row><row><cell>Median</cell><cell>0.2422</cell><cell>0.0</cell></row><row><cell>Runs</cell><cell></cell><cell></cell></row><row><cell>PRM</cell><cell>0,3759</cell><cell>+55.20</cell></row><row><cell>PR</cell><cell>0,2725</cell><cell>+12.514</cell></row><row><cell>EXP</cell><cell>0.1672</cell><cell>-30.96</cell></row><row><cell>EI</cell><cell>0,2197</cell><cell>-9.28</cell></row><row><cell cols="3">Table3 . IR-n at clef 2001</cell></row><row><cell cols="2">Bilingual task</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="7,98.89,501.29,395.51,10.89;7,84.96,512.81,320.45,10.89" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,263.48,501.29,201.81,10.89">Efficient Passage Ranking for Document Databases</title>
		<author>
			<persName coords=""><surname>Kaszkiel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zobel</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Sacks-Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,471.99,501.29,22.42,10.89;7,84.96,512.81,142.09,10.89">ACM transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="406" to="439" />
			<date type="published" when="1999-10">October 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,98.89,524.33,392.55,10.89;7,84.96,535.85,401.45,10.89;7,84.96,547.37,35.26,10.89" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,146.29,524.33,176.56,10.89">Passage-level evidence in document retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,339.49,524.33,151.96,10.89;7,84.96,535.85,293.20,10.89">Proceedings of the 17 th Annual ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 17 th Annual ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="302" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,96.60,558.89,356.07,10.89;7,84.96,570.41,392.99,10.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,200.41,558.89,252.26,10.89;7,84.96,570.41,129.44,10.89">Web Document Retrieval using Passage Retrieval, Connectivity Information, and Automatic Link</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Crivellari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Melucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,231.72,570.41,241.61,10.89">proceedings of the ninth Text REtrieval Conference(TREC-9)</title>
		<meeting>the ninth Text REtrieval Conference(TREC-9)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,95.29,581.93,353.16,10.89;7,84.96,593.45,87.71,10.89" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,135.51,581.93,139.59,10.89">Fujitsu Laboratories TREC9 Report</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Namba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,292.40,581.93,156.05,10.89;7,84.96,593.45,83.10,10.89">proceedings of the ninth Text REtrieval Conference(TREC-9)</title>
		<meeting>the ninth Text REtrieval Conference(TREC-9)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="7,103.81,604.97,395.67,10.89;7,84.96,616.25,418.42,10.89;7,84.96,627.77,67.52,10.89" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,237.46,604.97,205.40,10.89">Subtopic structuring for full-length document access</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Plaunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,252.24,616.25,190.97,10.89">Proceedings of the 16th ACM-SIGIR conference</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Khorfage</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Rasmussen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Willet</surname></persName>
		</editor>
		<meeting>the 16th ACM-SIGIR conference<address><addrLine>Pittsburgh, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,98.89,639.29,396.02,10.89;7,84.96,651.05,244.42,10.89" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,167.76,639.29,196.78,10.89">Multi-Paragraph Segmentation of Expository Text</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,371.11,639.29,123.81,10.89;7,84.96,651.05,201.71,10.89">Procs. 32nd Annual Meeting of the Assoc. for Computational Linguistics (ACL-94)</title>
		<meeting>s. 32nd Annual Meeting of the Assoc. for Computational Linguistics (ACL-94)</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,96.44,662.33,317.65,10.89" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="7,309.33,662.33,82.61,10.89">Ed morgan Kaufman</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Managing</forename><surname>Gygabytes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,98.89,673.85,368.55,10.89;7,84.96,685.37,242.71,10.89" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,186.08,673.85,150.70,10.89">Passage Retrieval Revisited SIGIR &apos;97</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kaszkiel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,342.82,673.85,124.62,10.89;7,84.96,685.37,75.16,10.89">Proceedings of the 20th Annual International ACM</title>
		<meeting>the 20th Annual International ACM<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">July 27-31, 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,99.21,696.89,398.88,10.89;7,84.96,708.41,197.87,10.89" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,241.99,696.89,195.37,10.89">TREC and TIPSTER Experiments with INQUERY</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Broglio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,451.09,696.89,47.01,10.89;7,84.96,708.41,104.15,10.89">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="327" to="343" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,101.58,719.93,367.09,10.89;7,84.96,731.45,264.57,10.89" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="7,169.68,719.93,298.99,10.89;7,84.96,731.45,98.94,10.89">Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison Wesley Publishing</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
