<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,233.84,74.10,127.14,12.53;1,211.76,90.18,171.67,12.53;1,252.80,106.02,89.52,12.53">Experiments with the Eurospider Retrieval System for CLEF 2001</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,190.64,144.64,67.50,9.07"><forename type="first">Martin</forename><surname>Braschler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Eurospider Information Technology AG</orgName>
								<address>
									<addrLine>Schaffhauserstr. 18</addrLine>
									<postCode>8006</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,265.04,144.64,71.54,9.07"><forename type="first">Bärbel</forename><surname>Ripplinger</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Eurospider Information Technology AG</orgName>
								<address>
									<addrLine>Schaffhauserstr. 18</addrLine>
									<postCode>8006</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,343.52,144.64,60.63,9.07"><forename type="first">Peter</forename><surname>Schäuble</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Eurospider Information Technology AG</orgName>
								<address>
									<addrLine>Schaffhauserstr. 18</addrLine>
									<postCode>8006</postCode>
									<settlement>Zürich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,233.84,74.10,127.14,12.53;1,211.76,90.18,171.67,12.53;1,252.80,106.02,89.52,12.53">Experiments with the Eurospider Retrieval System for CLEF 2001</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E982874A00B897FE0E4F9E4BC109EDB4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Eurospider participated in both the multilingual and monolingual retrieval tasks for CLEF 2001. Our multilingual experiments, the main focus of this year's work, combine multiple approaches to cross-language retrieval: machine translation, similarity thesauri, and machine-readable dictionaries. We experimented with both query translation and document translation. The monolingual experiments focused on the use of two fundamentally different stemming components: one commercially and one linguistically motivated stemmer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes our experiments conducted for CLEF 2001. Much of the work for this year builds directly on ideas we already applied to last year's experiments <ref type="bibr" coords="1,294.07,355.60,10.71,9.07" target="#b0">[1]</ref>. First, we present our system setup, and outline some details of the collection and indexing. This is followed by a description of the particular characteristics of the individual experiments, including a comparison to last year, and a preliminary analysis of our results. The paper closes with a discussion of our findings. Eurospider participated in the multilingual and German and French monolingual retrieval tasks. Our experiments in multilingual retrieval try to combine as many approaches to translation as possible in order to obtain a robust system that delivers good results in the widest range of situations possible: we used similarity thesauri, machinereadable dictionaries and a machine translation system. We tried both document and query translation. The focus of the monolingual experiments was an investigation into various aspects of stemming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Setup</head><p>For our runs, we used the standard Eurospider retrieval system, a core part of all Eurospider commercial products, enhanced by some experimental multilingual information access (MLIA) components.</p><p>Indexing: Indexing of German documents and queries for the multilingual task used the German Spider stemmer, which is based on a dictionary coupled with a rule set for decompounding of German nouns. Indexing of French documents and queries for the multilingual task used the French Spider stemmer. French accents were retained. Some more in-depth experiments regarding stemming for German and French were carried out for the monolingual task. Indexing of Italian documents and queries used the Spider Italian rule-based stemmer. For the La Stampa documents, there was a simple preprocessing that replaced the combination "vowel + quote" with an accented vowel, to normalize the alternative way of representation for accented characters in this subcollection. This simple rule produces some errors if a word was intentionally quoted, but the error rate was considered too small to justify the development of a more sophisticated replacement process. This heuristic was not necessary for the AGZ/SDA Italian texts. Indexing of English documents used an adapted version of the Porter rule-based stemmer. Indexing of the Spanish documents used a new experimental stemmer specifically developed for this task.</p><p>The Spider system was configured to use a straight Lnu.ltn weighting scheme for retrieval, as described in <ref type="bibr" coords="1,506.47,724.24,10.71,9.07" target="#b4">[5]</ref>.</p><p>The CLEF multilingual test collection consists of newspaper and newswire articles for German (Frankfurter Rundschau, Der Spiegel, SDA), French (Le Monde, ATS), Italian (La Stampa, AGZ), English (LA Times) and, new in 2001, Spanish (EFE). There are additional documents in Dutch and German, which are used for special subtasks that we did not participate in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multilingual Retrieval</head><p>We spent our main effort on our experiments for the multilingual task. The goal of this task in CLEF is to pick a topic language, and use the queries to retrieve documents independent of their language. I.e., a mixed result list has to be returned, potentially containing documents in all languages (English, French, German, Italian and Spanish). We submitted four runs for this task, labeled EIT01M1N, EIT01M2N, and EIT01M3D/EIT01M3N. They represent increasingly complex experiments. All runs use the German topics; the "N" runs use all topic fields, whereas the "D" run uses title+description only. We investigated both query translation (also abbreviated "QT" in the following) and document translation ("DT"). Technologies used for query translation were similarity thesauri ("ST"), machine-readable dictionaries ("MRD") and a commercially available machine translation ("MT") system. For document translation, only the MT system was used. Following is a description of these key technologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarity Thesaurus:</head><p>The similarity thesaurus is an automatically calculated data structure, which is built on suitable training data. It links terms to lists of their statistically most similar counterparts <ref type="bibr" coords="2,446.47,338.56,10.71,9.07" target="#b1">[2]</ref>. If multilingual training data is used, the resulting thesaurus is also multilingual. Terms in the source language are then linked to the most similar terms in the target language <ref type="bibr" coords="2,253.51,361.60,10.71,9.07" target="#b3">[4]</ref>. Such a thesaurus can be used to produce a "pseudo-translation" of the query by substituting the source language terms with those terms from the thesaurus that are most similar to the query as a whole. Because some of the data that was newly added to the CLEF collections overlaps with the training data of the thesauri we used for our 2000 CLEF experiments, we had to rebuild all thesauri to make sure that the training data is completely disjoint from the CLEF collection. For German/French and German/Italian, we used training data provided by the Schweizerische Depeschenagentur (SDA), which is from a different time period than the SDA data in CLEF. For German/Spanish, we aligned German SDA with Spanish texts from Reuters and Agence France Presse (AFP). Since this thesaurus is only used to search the Spanish subcollection (EFE), the use of all SDA data was acceptable. For German/English, we used German SDA data aligned to English Associated Press (AP) data. There was a considerable difference in the amount of training data available to build the thesauri. While the training data for German/French and German/Italian was substantial (roughly 10 and 9 years of newswire articles, respectively), we started from scratch for German/English (we used no German/English ST in 2000) and German/Spanish. This means that the resulting thesauri for these latter language combinations were not as well refined as for the earlier language pairs. We expected this to have a significant impact on retrieval quality. In all cases, training used comparable corpora, not parallel corpora that contain real translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Machine-Readable Dictionaries:</head><p>This year, we added general-purpose MRDs to our experiments. These dictionaries were used for pre-translation of queries, without a proper integration into the weighting mechanism of the system. Therefore, we used a heuristic to decide on the number of potential translations generated for ambiguous terms. This lack of integration limited the control over terms left untranslated due to gaps in the dictionary. Machine translation system: For a limited number of language pairs, commercial end-user machine translation products are available. Since some of these systems are inexpensive and run on standard PC hardware, we decided to loosely combine such a product with both our translation component and our retrieval software. We used MT to translate both the document collection and the queries.</p><p>The ranked lists for the four multilingual runs were obtained as follows:</p><p>EIT01M1N: This run is based on one large, unified index containing all German documents plus the MTtranslations of all English, French, Italian and Spanish documents. Because we had no direct German/Spanish machine translation available, we used a two-step German/English/Spanish translation in this case. We then performed straight German monolingual retrieval on this index. An added benefit is the avoidance of the merging problem that typically arises when results are calculated one language at a time. Since only one search has to be performed on one index, a single ranked list is obtained.</p><p>EIT01M2N: This is an experiment based on query translation. We obtained individual bilingual runs for each language pair (German/French, German/Italian, German/Spanish, and German/English). For each pair, we used three different translation strategies: similarity thesaurus, machine translation, and machine-readable dictionaries. The ranked lists obtained were then merged to produce the bilingual results. In a last step, these bilingual results, plus a monolingual German run, were merged into the final multilingual result.</p><p>EIT01M3D/EIT01M3N: The two runs are related: EIT01M3D used only the title and description fields, whereas EIT01M3N used all topic fields. The two experiments combine all elements described for the EIT01M1N (DTbased) and EIT01M2N (QT-based) runs. EIT01M3N is the result of merging these two runs, whereas EIT01M3D is the result of merging the two corresponding title+description runs (which were not submitted as official experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Monolingual Retrieval</head><p>Our interest in the monolingual track was to investigate the effects of stemming for the German and the French language. We had the opportunity this year to use the MPRO morpho-syntactic analysis for some research experiments. This analysis component contains elaborate linguistic information, among them base forms and compound analysis for German <ref type="bibr" coords="3,201.19,448.00,10.71,9.07" target="#b2">[3]</ref>. By standard, the Eurospider system uses stemming procedures that have been adapted over the years specifically to the needs communicated by customers of Eurospider's commercial retrieval systems. We were interested to see how such a "commercial" approach compares to a more linguistically motivated alternative. We submitted three runs for the German monolingual task: EIT01GGSSN, an all-topic-fields run using the original Spider stemmer; EIT01GGLUN, a run using the MPRO morpho-syntactic analysis, and EIT01GGLUD, a variant of the second run, using topic+description fields only.</p><p>For French monolingual, we also submitted three runs, EIT01FFFN, a run using a new experimental variant of the French Spider stemmer, and EIT01FFLUN, a run using the MPRO analysis. Our third run, EIT01FFFD, was mistakenly lost during the submission process, and therefore had to be disqualified from the official evaluation. While analyzing our results, we found a bug in EIT01FFFN. After fixing this bug (missing accents in queries), performance improved significantly.</p><p>The concentration on stemming means that we did not use some "enhancements", such as blind feedback, that probably would have increased overall performance, but that we felt make it harder to investigate the impact of stemming. The runs therefore are simplistic: removal of stopwords, stemming, and then straight retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Multilingual: One of the major obstacles when performing our experiments for this year was a lack of suitable training data to build the German/English and German/Spanish similarity thesauri. We built the thesauri for these two languages even though we expected their quality to be inadequate. The reason was two-fold: one, to investigate the effects of using (too) little training data, and two, to build a system that treats all languages equally.</p><p>Analysis of the QT-based run (EIT01M2N) shows that both the German/English and German/Spanish components performed poorly, and therefore hurt overall performance. Unfortunately, the German/French and German/Italian thesauri also did not perform as well as last year. We assume this to be due to the exclusion of the SDA data from 1994 (which was used in the CLEF document collection). However, the German/French and German/Italian thesauri performed much better than their English and Spanish counterparts due to their much larger training sets.</p><p>The dictionary-based components we introduced into the QT-based run suffered from a similar problem, with the Italian dictionary being very small. Again, we expected this to hurt overall performance, but the Italian dictionary was used to allow consistent handling of the languages.</p><p>Comparing the three translation methods used for each language pair (MT, ST, MRD), machine translation generally performed best. There are, however, big performance differences between more "popular" language combinations (German/English) and less "popular" ones (German/Italian).</p><p>The similarity thesaurus did significantly worse. Like last year, we observe that a sizable part of the difference is caused by a subset of queries that completely fail to retrieve anything relevant. The remaining queries performs well, but the average performance suffers from the outliers. We observed last year that the combination of machine translation with similarity thesauri substantially outperformed the use of a single strategy. This year, the combination generally gives only performance comparable to machine translation alone, probably due to the less appropriate quality of the thesauri. However, we observed an increase in recall, and also better performance in the high precision range. The dictionary-based translations overall performed similarly to the similarity thesaurus-based translations.</p><p>When combined with machine translation, the dictionary gave no advantage, instead negatively affecting retrieval performance. The combined run produces the best results, and does so on a consistent basis. As shown in table <ref type="table" coords="4,464.71,449.44,3.78,9.07" target="#tab_2">3</ref>, the majority of queries improves, often substantially, in terms of average precision when compared to the DT-only or QTonly run. The picture is less conclusive for the comparison between DT-only and QT-only. This seems to indicate that the mixture works well and boosts performance. For French, we observed good performance of the MT translations. The similarity thesaurus performed appropriately, but not as well as last year. This may be due to the mismatch between the time period covered in the training data and the CLEF test set. The performance of dictionary-based translations was adequate, thanks to the large dictionary for French.</p><p>Combining MT with ST benefits mainly long queries, because the number of queries failing completely was higher for the short queries. In both cases, long and short, combination helped in high precision situations. Further combination with MRD brought no additional improvement.</p><p>For English, MT performed well, as expected. The similarity thesaurus performed poorly, because the English/German thesaurus had the least appropriate training data available (time shift and too little volume). This means that combination with ST and dictionary negatively affected the English component.</p><p>The Italian ST outperformed the thesauri for other languages. While not as good as the Italian MT translations, a full 13 queries performed at least 10% better based on the ST translations than when using the MT system.</p><p>Combining MT with ST outperformed MT alone, especially in high precision situations. The Italian dictionary was the smallest, and consequently of no additional benefit.</p><p>Our work in Spanish was started specifically for CLEF. The performance of the German/Spanish thesaurus was adequate, given the little time available. Having training data from the same time period as the CLEF test data proved to be an advantage, even though the pool of training data was not as big as necessary to achieve the same quality as the French or Italian thesauri. The Spanish dictionary did not contribute positively to the overall performance of the German/Spanish component run.</p><p>We are pleased to see that our runs compare favorably when compared to other entries in CLEF. Table <ref type="table" coords="5,491.83,245.92,5.04,9.07" target="#tab_3">4</ref> shows an analysis of per-query performance compared to the median performance of all participants. All runs are above a "theoretical median": the average of the median average precision values. Especially the combination runs performed strongly and were among the best entries for CLEF 2001. Monolingual: For German, the elaborate morpho-syntactic analysis of MPRO seems to bring a slight improvement over the more conventional Spider stemmer. However, the number of queries affected positively and negatively by over 10% in average precision is equal. We intend to conduct an in-depth analysis on the difference of the two approaches to stemming in the future. All German runs performed well when compared to the median performance.</p><p>For French, the broken run EIT01FFFN performs poorly. When fixed, performance improves substantially, and outperforms the MPRO analysis component. Taking into account the simplicity of the monolingual experiments, we consider the performance to be satisfactory. The German performed well, beating the median of all CLEF submissions consistently, while the fixed French run also topped median performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary</head><p>This year, we tried a combination of three different translation strategies: machine translation, similarity thesauri and machine-readable dictionaries. The results when using the thesauri were not as remarkable as last year, because of the lack of appropriate training data for some language combinations. Adding the similarity thesaurus to machine translation showed potential in the same areas we identified already last year, i.e. substantial benefit in recall and the high precision range for a number of queries.</p><p>The general-purpose dictionaries we introduced this year did not improve the performance of our experiments.</p><p>The monolingual experiments concentrated on an investigation into stemming behavior. We tested both the standard Spider stemmer, which is commercially motivated, and stemming based on the MPRO morphosyntactic component. Based on our CLEF 2001 results, we plan to conduct an in-depth analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,130.40,641.86,342.48,73.94"><head>Table 1 .</head><label>1</label><figDesc>Size of the machine-readable dictionaries used for the multilingual experiments</figDesc><table coords="2,130.40,659.68,342.48,56.11"><row><cell>Language Pair</cell><cell># of Entries</cell></row><row><cell>German -English</cell><cell>486,851</cell></row><row><cell>German -French</cell><cell>70,161</cell></row><row><cell>German -Italian</cell><cell>7,953</cell></row><row><cell>German -Spanish</cell><cell>36,636</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,130.39,360.58,342.48,73.94"><head>Table 2 .</head><label>2</label><figDesc>Average precision numbers for the multilingual experiments</figDesc><table coords="4,130.39,378.64,342.48,55.87"><row><cell>Runs against Multilingual Collection</cell><cell>Average Precision</cell></row><row><cell>EIT01M1N (DT; TDN)</cell><cell>0.3099</cell></row><row><cell>EIT01M2N (QT; TDN)</cell><cell>0.2773</cell></row><row><cell>EIT01M3D (Combination; TD)</cell><cell>0.3128</cell></row><row><cell>EIT01M3N (Combination; TDN)</cell><cell>0.3416</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,130.39,506.98,337.20,108.26"><head>Table 3 .</head><label>3</label><figDesc>Comparison of average precision numbers for individual queries</figDesc><table coords="4,130.39,524.80,337.20,90.43"><row><cell>Comparison</cell><cell>better;</cell><cell>better;</cell><cell>worse;</cell><cell>worse;</cell></row><row><cell>Avg. Prec. per Query</cell><cell>diff.&gt;10%</cell><cell>diff.&lt;10%</cell><cell>diff.&lt;10%</cell><cell>diff.&gt;10%</cell></row><row><cell>EIT01M3N (comb.) vs.</cell><cell>20</cell><cell>21</cell><cell>9</cell><cell>0</cell></row><row><cell>EIT01M1N (DT)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EIT01M3N (comb.) vs.</cell><cell>20</cell><cell>16</cell><cell>9</cell><cell>5</cell></row><row><cell>EIT01M2N (QT)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EIT01M1N (DT) vs.</cell><cell>17</cell><cell>8</cell><cell>9</cell><cell>16</cell></row><row><cell>EIT01M2N (QT)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,130.40,303.22,339.48,107.30"><head>Table 4 .</head><label>4</label><figDesc>Officially submitted runs (multilingual task) compared to median of all submitted runs (on individual query basis)</figDesc><table coords="5,130.40,331.36,339.48,79.15"><row><cell>Run</cell><cell>Best</cell><cell cols="2">Above Median</cell><cell>Below</cell><cell cols="2">Worst Avg. Prec</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>vs. Theor.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Median</cell></row><row><cell>EIT01M1N</cell><cell>1</cell><cell>23</cell><cell>0</cell><cell>25</cell><cell>1</cell><cell>+0.0351</cell></row><row><cell>EIT01M2N</cell><cell>1</cell><cell>21</cell><cell>6</cell><cell>22</cell><cell>0</cell><cell>+0.0024</cell></row><row><cell>EIT01M3D</cell><cell>1</cell><cell>28</cell><cell>1</cell><cell>20</cell><cell>0</cell><cell>+0.0379</cell></row><row><cell>EIT01M3N</cell><cell>1</cell><cell>36</cell><cell>1</cell><cell>12</cell><cell>0</cell><cell>+0.0667</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,130.39,533.38,342.48,96.74"><head>Table 5 .</head><label>5</label><figDesc>Average precision numbers for the monolingual experiments</figDesc><table coords="5,130.39,551.20,342.48,78.91"><row><cell>Runs against Multilingual Collection</cell><cell>Average Precision</cell></row><row><cell>EIT01GGSN (German; TDN)</cell><cell>0.4285</cell></row><row><cell>EIT01GGLUN (German; TDN)</cell><cell>0.4408</cell></row><row><cell>EIT01GGLUD (German; TD)</cell><cell>0.4132</cell></row><row><cell>EIT01FFFN (French; TDN) (official/broken)</cell><cell>0.3848</cell></row><row><cell>EIT01FFFN (French; TDN) (unofficial/fixed)</cell><cell>0.4712</cell></row><row><cell>EIT01FFLUN (French; TDN)</cell><cell>0.4471</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,130.39,72.82,337.20,108.50"><head>Table 6 .</head><label>6</label><figDesc>Comparison of average precision numbers for individual queries</figDesc><table coords="6,130.39,90.88,337.20,90.43"><row><cell>Comparison</cell><cell>better;</cell><cell>better;</cell><cell>worse;</cell><cell>worse;</cell></row><row><cell>Avg. Prec. per Query</cell><cell>diff.&gt;10%</cell><cell>diff.&lt;10%</cell><cell>diff.&lt;10%</cell><cell>diff.&gt;10%</cell></row><row><cell>EIT01GGLUN vs.</cell><cell>8</cell><cell>19</cell><cell>14</cell><cell>8</cell></row><row><cell>EIT01GGSN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EIT0FFLUN vs.</cell><cell>19</cell><cell>10</cell><cell>14</cell><cell>4</cell></row><row><cell>EIT01FFFN (official/broken)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EIT01FFLUN vs.</cell><cell>4</cell><cell>14</cell><cell>20</cell><cell>8</cell></row><row><cell>EIT01FFFN (unofficial/fixed)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,130.39,207.70,339.48,130.10"><head>Table 7 .</head><label>7</label><figDesc>Officially submitted runs compared to median of all submitted runs (on individual query basis)</figDesc><table coords="6,130.39,235.84,339.48,101.95"><row><cell>Run</cell><cell>Best</cell><cell cols="2">Above Median</cell><cell>Below</cell><cell cols="2">Worst Avg. Prec</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>vs. Theor.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Median</cell></row><row><cell>EIT01GGSN</cell><cell>3</cell><cell>27</cell><cell>4</cell><cell>15</cell><cell>0</cell><cell>+0.0625</cell></row><row><cell>EIT01GGLUN</cell><cell>1</cell><cell>30</cell><cell>6</cell><cell>12</cell><cell>0</cell><cell>+0.0748</cell></row><row><cell>EIT01GGLUD</cell><cell>1</cell><cell>28</cell><cell>4</cell><cell>16</cell><cell>0</cell><cell>+0.0472</cell></row><row><cell>EIT01FFFN (brk)</cell><cell>1</cell><cell>13</cell><cell>5</cell><cell>28</cell><cell>2</cell><cell>-0.0787</cell></row><row><cell>EIT01FFFN (cor)</cell><cell>(2)</cell><cell>(23)</cell><cell>(4)</cell><cell>(20)</cell><cell>(0)</cell><cell>+0.0076</cell></row><row><cell>EIT01FFLUN</cell><cell>4</cell><cell>17</cell><cell>7</cell><cell>21</cell><cell>0</cell><cell>-0.0164</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgements</head><p>We thank <rs type="institution">IAI Saarbrücken</rs> for the opportunity to use MPRO. <rs type="person">Anne Göhring</rs> worked on the French and Spanish Spider stemmers, and <rs type="person">Ron Caneel</rs> updated the similarity thesaurus components. Some of the tools for converting result lists were originally written by <rs type="person">Min-Yen Kan</rs> for the Eurospider TREC-8 experiments.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="6,74.29,640.18,449.97,8.21;6,82.15,650.50,241.56,8.21" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,186.79,640.18,224.43,8.21">Experiments with the Eurospider Retrieval System for CLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schäuble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,446.23,640.18,78.03,8.21;6,82.15,650.50,16.42,8.21">Proceedings of CLEF 2000</title>
		<title level="s" coord="6,105.19,650.50,128.33,8.21">Lecture Notes in Computer Science</title>
		<meeting>CLEF 2000</meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2000">2000. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.29,661.06,449.94,8.21;6,82.15,671.38,286.20,8.21" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,146.71,661.06,119.17,8.21">Concept Based Query Expansion</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Frei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,282.55,661.06,241.68,8.21;6,82.15,671.38,138.13,8.21">Proceedings of the 16th ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 16th ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="160" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.29,681.70,450.06,8.21;6,82.15,692.02,83.64,8.21" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,138.55,681.70,133.78,8.21">The Use of NLP Techniques in CLIR</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ripplinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,288.79,681.70,97.54,8.21">Proceedings of CLEF 2000</title>
		<title level="s" coord="6,392.47,681.70,128.02,8.21">Lecture Notes in Computer Science</title>
		<meeting>CLEF 2000</meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.29,702.34,449.94,8.21;6,82.15,712.66,442.24,8.21;6,82.15,722.98,39.00,8.21" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,249.43,702.34,258.96,8.21">Cross-language information retrieval in a multilingual legal domain</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sheridan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schäuble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,82.15,712.66,393.30,8.21">Proceedings of the First European Conference on Research and Advanced Technology for Digital Libraries</title>
		<meeting>the First European Conference on Research and Advanced Technology for Digital Libraries</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="253" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.29,733.30,450.02,8.21;6,82.15,743.62,325.08,8.21" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,219.67,733.30,150.54,8.21">Pivoted Document Length Normalization</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,387.67,733.30,136.64,8.21;6,82.15,743.62,244.69,8.21">Proceedings of the 19th ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 19th ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
