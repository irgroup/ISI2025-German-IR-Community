<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,90.00,85.27,397.72,12.83;1,246.00,101.02,87.76,12.83">JHU/APL Experiments at CLEF: Translation Resources and Score Normalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,217.50,131.84,61.45,8.78"><forename type="first">Paul</forename><surname>Mcnamee</surname></persName>
							<email>mcnamee@jhuapl.edu</email>
						</author>
						<author>
							<persName coords="1,298.04,131.84,63.34,8.78"><forename type="first">James</forename><surname>Mayfield</surname></persName>
							<email>mayfield@jhuapl.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University Applied Physics Lab</orgName>
								<address>
									<addrLine>11100 Johns Hopkins Road</addrLine>
									<postCode>20723-6099</postCode>
									<settlement>Laurel</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The Johns Hopkins University Applied Physics Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,90.00,85.27,397.72,12.83;1,246.00,101.02,87.76,12.83">JHU/APL Experiments at CLEF: Translation Resources and Score Normalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FB3BEDF3AC1BE9DAC3530627DBCDEF6F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>participated in three of the five tasks of the CLEF-2001 evaluation, monolingual retrieval, bilingual retrieval, and multilingual retrieval. In this paper we describe the fundamental methods we used and we present initial results from three experiments. The first investigation examines whether residual inverse document frequency can improve the term weighting methods used with a linguisticallymotivated probabilistic model. The second experiment attempts to assess the benefit of various translation resources for cross-language retrieval. Our last effort is to improve cross-collection score normalization, a task essential for the multilingual problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="596.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The Hopkins Automated Information Retriever for Combing Unstructured Text (HAIRCUT) is a research retrieval system developed at the Johns Hopkins University Applied Physics Laboratory (APL). The design of HAIRCUT was influenced by a desire to compare various methods for lexical analysis and tokenization; thus the system has no commitment to any particular method. With western European languages we typically use both unstemmed words and overlapping character n-grams as indexing terms, and previous experimentation has led us to believe that a combination of both approaches enhances performance <ref type="bibr" coords="1,468.75,386.84,10.83,8.77" target="#b6">[7]</ref>.</p><p>We participated in three tasks at this year's workshop, monolingual, cross-language, and multilingual retrieval. All of our official submissions were automated runs and our official cross-language runs relied on query translation using one of two machine translation systems. In the sections that follow, we first describe our standard methodology and we then present initial results from three experiments. The first investigation examines whether residual inverse document frequency can improve the term weighting methods used with a linguistically-motivated probabilistic model. The second experiment attempts to assess the benefit of various translation resources for cross-language retrieval. Our last effort is to improve cross-collection score normalization, a task essential for the multilingual problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>For the monolingual tasks we used twelve indices, a word and an n-gram (n=6) index for each of the six languages. For the bilingual and multilingual tasks we used the same indices with translated topic statements. Information about each index is provided in Table <ref type="table" coords="1,272.98,562.34,3.71,8.78" target="#tab_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Construction</head><p>Documents were processed using only the permitted tags specified in the workshop guidelines. First SGML macros were expanded to their appropriate Unicode character. Then punctuation was eliminated, letters were downcased, and only the first four of a sequence of digits were preserved (e.g., 010394 became 0103##). Diacritical marks were preserved. The result is a stream of words separated by spaces. Exceedingly long words were truncated; the limit was 35 characters in the Dutch and German languages and 20 otherwise. When using n-grams we extract indexing terms from the same stream of words; thus, the n-grams may span word boundaries, but sentence boundaries are noted so that n-grams spanning sentence boundaries are not recorded. N-grams with leading, central, or trailing spaces are formed at word boundaries. For example, given the phrase, "the prime minister," the following 6-grams are produced. The use of overlapping character n-grams provides a surrogate form of morphological normalization. For example, in Table <ref type="table" coords="2,145.88,426.59,4.88,8.78" target="#tab_1">2</ref> above, the n-gram "minist" could have been generated from several different forms like administer, administrative, minister, ministers, ministerial, or ministry. It could also come from an unrelated word like feminist. Another advantage of n-gram indexing comes from the fact that n-grams containing spaces can convey phrasal information. In the table above, 6-grams such as "rime-m", "ime-mi", and "memin" may act much like the phrase "prime minister" in a word-based index using multiple word phrases.</p><p>At last year's workshop we explored language-neutral retrieval and avoided the use of stopword lists, lexicons, decompounders, stemmers, lists of phrases, or manually-built thesauri <ref type="bibr" coords="2,409.50,506.84,10.83,8.78" target="#b5">[6]</ref>. Such resources are seldom in a standard format, may be of varying quality, and worst of all, necessitate additional software development to utilize. Although we are open to the possibility that such linguistic resources may improve retrieval performance, we are interested in how far we can push performance without them. We followed the same approach this year.</p><p>We conducted our work on four Sun Microsystems workstations that are shared with about 30 other researchers. Each machine has at least 1GB of physical memory and we have access to dedicated disk space of about 200GB. The use of character n-grams increases the size of both dictionaries and inverted files, typically by a factor of five or six, over those of comparable word-based indices. Furthermore, when we use pseudo-relevance feedback we use a large number of expansion n-grams. As a consequence, runtime performance became an issue that we needed to address. Over the last year we made a number of improvements to HAIRCUT to reduce the impact of large data structures, and to allow the system to run in less memory-rich environments.</p><p>To minimize the memory consumption needed for a dictionary in a large term-space, we developed a multitiered cache backed by a B-tree. If sufficient memory is available, term/term-id pairs are stored in a hash table; if the hash table grows too large, entries are removed from the table, but still stored in memory as compressed B-tree nodes; if the system then runs out of memory data are written to disk.</p><p>To reduce the size of our inverted files we applied gamma compression <ref type="bibr" coords="2,360.00,737.09,10.79,8.77" target="#b8">[9]</ref> and saw our disk usage shrink to about 1/3 of its former size. HAIRCUT also generates dual files, an analogous structure to inverted files that are document-referenced vectors of terms; the dual files also compressed rather nicely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Processing</head><p>HAIRCUT performs rudimentary preprocessing on topic statements to remove stop structure, e.g., phrases such as "… would be relevant" or "relevant documents should…." . We have constructed a list of about 1000 such English phrases from previous topic sets (mainly TREC topics) and these have been translated into other languages using commercial machine translation. Other than this preprocessing, queries are parsed in the same fashion as documents in the collection.</p><p>In all of our experiments we used a linguistically motivated probabilistic model for retrieval. Our official runs all used blind relevance feedback, though it did not improve retrieval performance in every instance. To perform relevance feedback we first retrieved the top 1000 documents. We then used the top 20 documents for positive feedback and the bottom 75 documents for negative feedback; however, we removed any duplicate or near duplicate documents from these sets. We then select terms for the expanded query based on three factors, a term's initial query term frequency (if any); the cube root of the (α=3, β=2, γ=2) Rocchio score; and a term similarity metric that incorporates IDF weighting. The 60 top ranked terms are then used as the revised query with words as indexing terms; 400 terms are used with 6-grams. In previous work we penalized documents containing only a fraction of the query terms; we are no longer convinced that this technique adds much benefit and have discontinued its use. As a general trend we observe a decrease in precision at very low recall levels when blind relevance feedback is used, but both overall recall and mean average precision are improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monolingual Experiments</head><p>Once again our approach to monolingual retrieval focused on language-independent methods. We submitted two official runs for each target language, one using the mandated &lt;title&gt; and &lt;desc&gt; fields (TD runs) and one that added the &lt;narr&gt; field as well (TDN runs), for a total of 10 submissions. These official runs were automated runs formed by combining results from two base runs, one using words and one using n-grams.</p><p>In all our experiments we used a linguistically motivated probabilistic model. This model has been described in a report by Hiemstra and de Vries <ref type="bibr" coords="3,236.25,404.84,10.83,8.77" target="#b4">[5]</ref>, which compares the method to traditional models. This is essentially the same approach that was used by BBN in TREC-7 <ref type="bibr" coords="3,338.25,416.09,11.79,8.77" target="#b7">[8]</ref> which was billed as a Hidden Markov Model. The similarity calculation that is performed is:</p><formula xml:id="formula_0" coords="3,106.50,436.37,219.55,37.99">( ) ) , ( ) ( ) 1 ( ) , ( ) , ( q t f terms t t mrdf d t f d q Sim ∏ = ⋅ - + ⋅ = α α Equation 1. Similarity calculation.</formula><p>where f(t,d) is the relative frequency of term t in document d (or query q) and mrdf(t) denotes the mean relative document frequency of t. The parameter α is a tunable parameter that can be used to ascribe a degree of importance to a term. For our baseline system we simply fix the value of α at 0.3 when words are used as indexing terms. Since individual n-grams tend to have a lower semantic value than words a lower α is indicated; we use a value of 0.15 for 6-grams.</p><p>In training experiments using the TREC-8 test collection we found performance remained acceptable across a wide range of values. When blind relevance feedback is applied we do not adjust this importance value, and instead just expand the initial query.  For each language results using both TD and TDN queries are shown when words, 6-grams, or a combination of the two is used. Unsurprisingly longer queries were more effective. 6-gram runs most often had better performance than words, but this was not the case in French or Italian. Combination of the two methods yielded a slight improvement.</p><p>We were interested in performing an experiment to see if baseline performance could be improved by adjusting the importance parameter α for each query term, Residual inverse document frequency (RIDF) <ref type="bibr" coords="4,498.00,444.59,11.79,8.77" target="#b1">[2]</ref> is a statistic that represents the burstiness of a term in the documents in which it occurs (see Equation <ref type="formula" coords="4,505.66,455.84,4.88,8.78">2</ref>below). Terms with high RIDF tend to be distinctive, so when they are present, they occur more frequently within a document than might otherwise be expected; terms with low RIDF tend to occur indiscriminately. Numerals and adverbs, and to some extent adjectives all tend to have low RIDF. For example, the English words briefly and computer both occur in just over 5000 LA Times articles, yet computer appears 2.18 times per occurrence, on average, while briefly almost always appears just once (1.01 times on average). By taking this into account, we hope to minimize the influence that a word like briefly has on document scores (aside: Yamamoto and Church have recently published an efficient method for computing RIDF for all substrings in a collection <ref type="bibr" coords="4,119.25,548.09,14.64,8.77" target="#b9">[10]</ref>). Our approach was as follows. For each query term, we adjust the importance value, α, for each term depending on RIDF. We linearly interpolate the RIDF value based on the minimum and maximum values in the collection and multiply by a constant k to determine the adjusted α . For these initial experiments we only considered k=0.2. We are still analyzing these results, however the preliminary indications are promising. Figure <ref type="figure" coords="4,460.76,737.84,4.88,8.78" target="#fig_1">2</ref> shows the change in average precision when applying this rudimentary method.  We observe a small positive effect, particularly with intermediate-length queries. One possible explanation for why the improvement does not occur with very short queries (e.g., title-only) is because these queries are unlikely to contain low-RIDF terms (being short and to the point), and the adjustment in importance value is unwarranted. As yet, we have no explanation for why long queries (TDN or those with expanded queries) do not seem to gain much with this method. As time permits an analysis of individual topics may reveal what is happening.</p><formula xml:id="formula_1" coords="4,133.50,558.31,123.32,28.80">      - - = - ) ( 1 1 log ) ( ) (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Term-Specific Adjustment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bilingual Experiments</head><p>Our goal for the bilingual task was to assess retrieval performance when four approaches to query translation are used, commercial machine translation software; publicly available bilingual wordlists; parallel corpora mined from the Web; and untranslated queries. The last is only likely to succeed when languages share word roots. We wanted to attempt as many of the topic languages as possible, and managed to use all but Thai.</p><p>In the past we observed good performance when commercial machine translation is used, and so all of our official runs used MT. Since only four official runs were permitted, we had a hard time choosing which topic languages to use. We attempted the Dutch bilingual task as well as the English task and ended up submitting runs using French, German, and Japanese topics against English documents, and using English topics for the Dutch documents.  <ref type="table" coords="5,95.70,745.34,3.72,8.78">3</ref>. Official results for the bilingual task At the time of this writing we are still working on our dictionary and corpus-based methods, and will present results from these experiments in a revised version of this manuscript. We now discuss some experiments on the English bilingual collection using MT-translated and untranslated queries. Systran supports translation from Chinese, French, German, Italian, Japanese, Russian, and Spanish to (American) English; to translate Dutch, Finnish, and Swedish topics we used the on-line translator at http://www.tranexp.com/. High quality machine translation can result in excellent cross-language retrieval; our official bilingual runs achieve 81% of the performance (on average) of a comparable monolingual baseline.</p><p>Although we generally use relevance feedback and are accustomed to seeing a roughly 25% boost in performance from its use, we observed that it was not always beneficial. This was especially the case with longer queries (TDN vs. Title-only) and when the translation quality was very high for the language pair in question. In Figure <ref type="figure" coords="6,158.91,199.34,4.88,8.78" target="#fig_2">3</ref> (below), we compare retrieval performance using words as indexing terms when relevance feedback is applied. When 6-grams were used the results were similar. Figure <ref type="figure" coords="6,99.32,536.84,3.72,8.78" target="#fig_2">3</ref>. Bilingual performance using words as indexing terms, examining the effect of relevance feedback. Untranslated English topics are shown at the left.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Translations of Topic 41 into English</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>German</head><p>&lt;DE-title&gt; pestizide in baby food &lt;DE-desc&gt; reports on pestizide in baby food are looked for.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;EN-title&gt; Pesticides in Baby Food</head><p>&lt;EN-desc&gt; Find reports on pesticides in baby food.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spanish</head><p>&lt;ES-title&gt; Pesticidas in foods for you drink &lt;ES-desc&gt; Encontrar the news on pesticidas in foods stops you drink.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finnish</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt;FI-title&gt; Suppression-compositions lasten valmisruuassa &lt;FI-desc&gt; Etsi raportteja suppression-aineista lasten valmisruuassa.</head><p>French &lt;FR-title&gt; Of the pesticides in food for babies &lt;FR-desc&gt; To seek documents on the pesticides in food for babies.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relevance Feedback Not Always Helpful</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source Language Mean Average Precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Japanese</head><p>&lt;JP-title&gt;Damage by disease and pest pest control medicine in baby hood &lt;JP-desc&gt;The article regarding the damage by disease and pest pest control medicine in the baby hood was searched to be. The Finnish translations are poor in quality, which explains the rather low relative performance when those topics were used. However, looking over the translated topics we observe that many untranslated terms are near cognates to the proper English word. For example, pestizide (German), pesticidas (Spanish), and pesticidi (Italian) are easily recognizable. Similarly, 'baby hood' is phonetically similar to 'baby food', an easy to understand mistake when Japanese phonetic characters are used to transliterate a term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dutch</head><p>In TREC-6, Buckley et al. explored cross-language English to French retrieval using cognate matches <ref type="bibr" coords="7,495.75,371.84,10.27,8.77" target="#b0">[1]</ref>.</p><p>They took an 'English is misspelled French' approach and attempted to 'correct' English terms into their proper French equivalents, projecting that 30% or so of non stopwords could be transformed automatically.</p><p>Their results were unpredictably good, and they reported bilingual performance of 60% of their monolingual baseline. Although this approach is non-intuitive, it can be used as a worst-case approach when few or no translation resources are available, so long as the source and target languages are compatible. Furthermore, it can certainly be used as a lower bound on CLIR performance that can serve as a minimal standard by which to assess the added benefit of additional translation resources.</p><p>While Buckley et al. manually developed rules to spell-correct English into French, this work may be entirely unnecessary when n-gram indexing is used, since n-grams provide a form of morphological normalization. Thus we consider a more radical hypothesis than 'English is misspelled French', namely, 'other languages are English.' We now examine more closely the relative performance observed when words and 6-grams are used without spelling correction.</p><p>Figure <ref type="figure" coords="7,100.37,544.34,4.88,8.78" target="#fig_8">4</ref> is a plot that compares the efficacy of machine-translated queries to untranslated queries for the English bilingual task. Since we have argued that relevance feedback does not have a large effect, we will only compare runs that do not use it. The data in the leftmost column is a monolingual English baseline, the unstarred columns in the central region are runs using machine translation for various source languages, and the rightmost area contains runs that used untranslated source language queries against the English collection. For each combination of translation method and source language six runs are shown using titleonly, TD, or TDN topic statements and either words or 6-grams.</p><p>Several observations can be made from this plot. First, we observe that longer topic statements tend to do better than shorter ones; roughly speaking, TDN runs are about 0.05 higher than corresponding TD runs, and TD runs are about the same amount better than title-only runs. Secondly we note that 6-grams tend to outperform words; the mean relative difference among comparable MT runs is 5.95%. Looking at the various source languages we note that as a group, the Systran translated runs (DE, ES, FR, IT, JP, RU, and ZH) outperform the InterTran translated queries (FI, NL, and SV); this may reveal an underlying difference in product quality, however a better comparison would be to use languages they translate in common. Translation quality is rather poor for the Finnish and Swedish topics (InterTran) and also with the Chinese topics (Systran). Averaging across all source languages, the translated runs have performance between 41-63% of the top monolingual English run when words are used, and 41-70% when 6-grams are used.</p><p>The untranslated queries plotted on the right clearly do worse than their translated equivalents. Averaging across the seven languages encoded in ISO-8859-1, word runs achieve performance between 9-15% of the top monolingual English run, but 6-gram runs do much better and get performance between 22-34% depending on the topic fields used. The mean relative advantage when n-grams are used on these topics is 183%, almost a doubling in efficacy over words. The 6-grams achieve 54% of the performance of the machine-translated runs. Though not shown in the plot, relevance feedback actually does enhance these untranslated 6-gram runs even though we have shown that relevance feedback did not significantly affect translated topics. One final observation is that shorter queries are actually better when words are used; we suspect that this is because longer topics may contain more matching words, but not necessarily the key words for the topic.</p><p>One concern we have with this analysis is that we are comparing an aggregate measure, mean average precision. For untranslated topics, we imagine that the variance in performance is greater over many topics since some topics will have almost no cognate matches. We hope to examine individual topic behavior in the future.</p><p>We looked for this effect in other measures besides average precision. Recall at 1000 documents was effectively doubled when 6-grams were used instead of words; roughly 70% of the monolingual recall was observed. Averaged across language, Precision at 5 documents was 0.1921 when 6-grams were used with TDN topics with blind relevance feedback. Thus even this rudimentary approach can expected to find one relevant document on average in the top five documents.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean Average Precision by</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multilingual Experiments</head><p>When combining several runs, one must either use document rank as a measure of the importance of a document, or try to make some sense out of system-generated scores. Using rank is problematic when the two runs cover different documents. For example, if a different index is built for each language in a multilingual retrieval task, there is no way to distinguish a language that has many relevant documents from one that has few or no relevant documents using rank alone. On the other hand, raw scores are not typically comparable. For example, the scores produced by our statistical language model are products, with one factor per query term. Even if the individual factors were somehow comparable (which they are not), there is no guarantee that a query will have the same number of terms when translated into two or more different languages. Other similarity metrics suffer from similar difficulties. Thus, score normalization is crucial if scores are to be used for run combination.</p><p>We tried a new score normalization technique this year. We viewed scores as masses, and normalized by dividing each individual score by the sum of the masses of the top 1000 documents. (Because our probabilistic calculations are typically performed in log space, and scores are therefore negative, we achieved the desired effect by using the reciprocal of a document's score as its mass.) Our previous method of score normalization was to interpolate scores for a topic within a run onto [0,1]. We were concerned that this would cause documents in languages with few or no relevant documents for a topic to appear comparable to top-ranked documents in a language with many relevant documents. While there was no appreciable difference between the two methods in this year's multilingual task (at least in average precision) we did see an eight percent improvement in precision at five documents using the new normalization (compare aplmuena with aplmuend).</p><p>We are still investigating rank-based combination as well, though we submitted no official runs using this technique. Our preliminary findings show little difference compared to score-based combination.</p><p>We were intrigued by a method that the U.C. Berkeley team used for multilingual merging in TREC-7 <ref type="bibr" coords="9,498.00,279.59,11.29,8.77" target="#b3">[4]</ref> and in last year's CLEF workshop <ref type="bibr" coords="9,219.00,291.59,10.27,8.77" target="#b2">[3]</ref>, where documents from all languages were indexed as a common collection. Queries were translated into all target languages and the resulting collective query was run against the collection. Berkeley's results using this approach in last year's multilingual task (run BKMUEAA1) were comparable to runs that used a merging strategy. We were inspired to try this method ourselves and built two unified indices, one using words and one using 5-grams. Using unstemmed words as indexing terms, our performance with this method was poor (run aplmuenc); however, we did see a significant improvement using 5-grams instead (see Table <ref type="table" coords="9,268.06,360.59,3.62,8.78" target="#tab_5">4</ref>). Still, our attempts using a unified term space have not resulted in better scores than approaches combining separate retrievals in each target language. We will continue to examine this method because of its desirable property of not requiring cross-collection score normalization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>The second Cross-Language Evaluation Forum workshop has offered a unique opportunity to investigate multilingual retrieval issues for European languages. We participated in three of the five tasks and were able to conduct several interesting experiments. Our first investigation into the use of term-specific adjustments using a statistical language model showed that a small improvement can be obtained when residual inverse document frequency is utilized. However, this conclusion is preliminary and we do not feel that we completely understand the mechanism involved.</p><p>Our second experiment is only partially completed; we compared bilingual retrieval performance when two query translation methods are used. The first method using extant commercial machine translation gives very good results that approach a monolingual baseline. We also showed that reasonable performance can be obtained when no attempt whatsoever is made at query translation, and we have demonstrated that overlapping character n-grams have a strong advantage over word-based retrieval in this scenario. The method is of course only practicable when related languages are involved. We think this result is significant for several reasons. First, it quantifies a lower bound for bilingual performance that other approaches may be measured against. Secondly, it implies that translation to a related language, when translation to the target language of interest is infeasible, may form the basis of a rudimentary retrieval system. We hope to augment this work by also comparing the use of parallel corpora and publicly available bilingual dictionaries in the near future.</p><p>Multilingual retrieval, where a single source language query is used to search for documents in multiple target languages, remains a critical challenge. Our attempt to improve cross-collection score normalization was not successful. We will continue to investigate this problem, which will only grow more difficult as a greater number of target languages is considered.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,70.50,364.34,438.60,8.77;4,70.50,375.59,437.84,8.77;4,70.50,386.84,437.86,8.77;4,70.50,398.84,385.35,8.77"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. Comparison of retrieval performance across target languages. For each language results using both TD and TDN queries are shown when words, 6-grams, or a combination of the two is used. Unsurprisingly longer queries were more effective. 6-gram runs most often had better performance than words, but this was not the case in French or Italian. Combination of the two methods yielded a slight improvement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,245.25,573.78,1.72,5.56;4,236.25,573.78,4.85,5.56;4,228.00,575.27,4.70,9.53;4,177.75,567.77,2.94,9.53;4,156.00,567.77,17.38,9.53;4,137.25,567.77,2.94,9.53;4,108.75,567.77,24.03,9.53;4,106.50,590.09,372.61,8.77;4,160.50,601.34,302.84,8.77"><head>Equation 2 .</head><label>2</label><figDesc>Computing residual inverse document frequency for a term. The log term in the equation represents the expected IDF if the term had a Poisson distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,258.00,692.95,9.77,5.67;4,211.50,692.95,11.12,5.67;4,258.00,677.20,9.77,5.67;4,220.50,671.12,3.60,9.72;4,213.75,671.12,3.60,9.72;4,122.25,678.62,3.60,9.72;4,114.75,678.62,3.60,9.72;4,234.00,686.87,24.15,9.72;4,187.50,686.87,24.15,9.72;4,234.00,671.12,24.15,9.72;4,216.75,671.12,3.00,9.72;4,188.25,671.12,24.15,9.72;4,175.50,678.62,4.80,9.72;4,118.50,678.62,3.00,9.72;4,143.25,684.70,20.34,5.67;4,226.50,683.34,5.93,14.07;4,225.75,667.59,5.93,14.07;4,182.25,675.09,2.70,14.07;4,168.00,675.09,5.93,14.07;4,128.25,675.09,14.31,14.07;4,107.25,675.09,6.81,14.07;4,106.50,700.15,212.44,12.70"><head>Equation 3 .</head><label>3</label><figDesc>Computing a term-specific value for α .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,70.50,358.34,438.65,8.77;5,70.50,369.59,423.60,8.77"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Impact on mean average precision when term-specific adjustments are made. 6-gram indexing shown for six query types (different topic fields and use of pseudo-relevance feedback ) in each language.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,487.50,285.74,4.90,7.42;6,487.50,300.74,20.82,7.42;6,487.50,315.74,10.97,7.42;6,487.50,330.74,25.61,7.42;6,487.50,345.74,15.99,7.42;6,487.50,360.74,32.18,7.42;7,70.50,83.84,28.94,8.78;7,142.50,83.84,185.89,8.78;7,142.50,95.84,364.40,8.78"><head></head><label></label><figDesc>Pesticidi in the alimony for children &lt;IT-desc&gt; Trova documents that they speak about the pesticidi in the alimony for children.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="7,142.50,164.09,164.74,8.78;7,142.50,176.09,310.21,8.78;7,70.50,198.59,33.57,8.78;7,142.50,198.59,215.11,8.78;7,142.50,210.59,317.83,8.78;7,70.50,233.09,33.65,8.78;7,142.50,233.09,154.36,8.78;7,142.50,245.09,209.05,8.78;7,70.50,267.59,32.93,8.78;7,142.50,267.59,233.87,8.77;7,142.50,279.59,239.85,8.77"><head></head><label></label><figDesc>&lt;NL-title&gt; Pesticide within babyvoeding &lt;NL-desc&gt; Missing unpleasant documents via pesticide within babyvoeding.Russian&lt;RU-title&gt; pesticides in the children's nourishment of &lt;RU-desc&gt; to find articles about the pesticides in the children's nourishment of Swedish &lt;SV-title&gt; Bekdmpningsmedel a baby &lt;SV-desc&gt; Svk report a bekdmpningsmedel a baby.Chinese&lt;ZH-title&gt; In baby food includes report which in pesticide &lt;ZH-desc&gt; inquiry concerned baby food includes pesticide.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="8,257.28,337.48,155.27,7.44;8,102.75,557.23,15.72,7.44;8,102.75,524.23,15.72,7.44;8,102.75,490.48,15.72,7.44;8,102.75,457.48,15.72,7.44;8,102.75,423.73,15.72,7.44;8,102.75,389.98,15.72,7.44;8,125.25,567.75,274.00,6.44;8,231.75,581.23,60.82,7.44;8,90.98,493.47,7.44,20.78;8,90.98,458.96,7.44,32.18;8,90.98,419.83,7.44,36.81;8,435.00,425.23,30.69,7.44;8,435.00,437.98,36.67,7.44;8,435.00,449.98,41.34,7.44;8,435.00,462.73,18.64,7.44;8,435.00,475.48,23.43,7.44;8,435.00,488.23,29.99,7.44"><head></head><label></label><figDesc>ES FI FR IT JP NL RU SV ZH DE* ES* FI* FR* IT* NL* SV*</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="8,70.50,618.59,438.62,8.77;8,70.50,629.84,123.63,8.77"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Comparing word and n-gram indexing on machine-translated, and untranslated topics. Untranslated topics are indicated with a star.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="1,70.50,562.34,362.26,188.77"><head>Table 1 .</head><label>1</label><figDesc>Index statistics for the CLEF-2001 test collection</figDesc><table coords="1,144.75,562.34,288.01,176.77"><row><cell></cell><cell></cell><cell>.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell># docs</cell><cell>collection size</cell><cell>name</cell><cell># terms</cell><cell>index size</cell></row><row><cell></cell><cell></cell><cell>(MB gzipped)</cell><cell></cell><cell></cell><cell>(MB)</cell></row><row><cell>Dutch</cell><cell>190,604</cell><cell>203</cell><cell>words</cell><cell>692,745</cell><cell>162</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">6-grams 4,154,405</cell><cell>1144</cell></row><row><cell cols="2">English 110,282</cell><cell>163</cell><cell>words</cell><cell>235,710</cell><cell>99</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">6-grams 3,118,973</cell><cell>901</cell></row><row><cell>French</cell><cell>87,191</cell><cell>93</cell><cell>words</cell><cell>479,682</cell><cell>84</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">6-grams 2,966,390</cell><cell>554</cell></row><row><cell cols="2">German 225,371</cell><cell>207</cell><cell>words</cell><cell>1,670,316</cell><cell>254</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">6-grams 5,028,002</cell><cell>1387</cell></row><row><cell>Italian</cell><cell>108,578</cell><cell>108</cell><cell>words</cell><cell>1,323,283</cell><cell>146</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">6-grams 3,333,537</cell><cell>694</cell></row><row><cell cols="2">Spanish 215,737</cell><cell>185</cell><cell>words</cell><cell>382,664</cell><cell>150</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">6-grams 3,339,343</cell><cell>1101</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,70.50,194.84,437.13,206.02"><head>Table 2</head><label>2</label><figDesc></figDesc><table coords="2,177.00,194.84,225.52,182.61"><row><cell>Term</cell><cell>Document</cell><cell>Collection</cell><cell>IDF RIDF</cell></row><row><cell></cell><cell>Frequency</cell><cell>Frequency</cell><cell></cell></row><row><cell>-the-p</cell><cell>72,489</cell><cell cols="2">241,648 0.605 0.434</cell></row><row><cell>the-pr</cell><cell>41,729</cell><cell cols="2">86,923 1.402 0.527</cell></row><row><cell>he-pri</cell><cell>8,701</cell><cell cols="2">11,812 3.663 0.364</cell></row><row><cell>e-prim</cell><cell>2,827</cell><cell cols="2">3,441 5.286 0.261</cell></row><row><cell>-prime</cell><cell>3,685</cell><cell cols="2">5,635 4.903 0.576</cell></row><row><cell>prime-</cell><cell>3,515</cell><cell cols="2">5,452 4.971 0.597</cell></row><row><cell>rime-m</cell><cell>1,835</cell><cell cols="2">2,992 5.910 0.689</cell></row><row><cell>ime-mi</cell><cell>1,731</cell><cell cols="2">2,871 5.993 0.711</cell></row><row><cell>me-min</cell><cell>1,764</cell><cell cols="2">2,919 5.966 0.707</cell></row><row><cell>e-mini</cell><cell>3,797</cell><cell cols="2">5,975 4.860 0.615</cell></row><row><cell>-minis</cell><cell>4,243</cell><cell cols="2">8,863 4.699 1.005</cell></row><row><cell>minist</cell><cell>15,428</cell><cell cols="2">33,731 2.838 0.914</cell></row><row><cell>iniste</cell><cell>4,525</cell><cell cols="2">8,299 4.607 0.821</cell></row><row><cell>nister</cell><cell>4,686</cell><cell cols="2">8,577 4,557 0.816</cell></row><row><cell>ister-</cell><cell>7,727</cell><cell cols="2">12,860 3.835 0.651</cell></row></table><note coords="2,99.95,380.84,407.68,8.77;2,70.50,392.09,332.06,8.77"><p>. Example 6-grams produced for the input "the prime minister." Term statistics are based on the LA Times subset of the CLEF-2001 collection. Dashes indicate whitespace characters.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,70.50,578.65,440.11,189.71"><head>Table 2 .</head><label>2</label><figDesc>Official results for monolingual task. The shaded rows contain results for comparable, unofficial English runs.</figDesc><table coords="3,94.50,578.65,391.15,166.46"><row><cell></cell><cell cols="2">topic fields average</cell><cell>recall</cell><cell cols="4"># topics # ≥ median # ≥ best # = worst</cell></row><row><cell></cell><cell></cell><cell>precision</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>aplmodea</cell><cell>TDN</cell><cell>0.4596</cell><cell>2086 / 2130</cell><cell>49</cell><cell>36</cell><cell>11</cell><cell>0</cell></row><row><cell>aplmodeb</cell><cell>TD</cell><cell>0.4116</cell><cell>2060 / 2130</cell><cell>49</cell><cell>40</cell><cell>6</cell><cell>0</cell></row><row><cell>aplmoena</cell><cell>TDN</cell><cell>0.4896</cell><cell>838 / 856</cell><cell>47</cell><cell cols="2">unofficial English run</cell><cell></cell></row><row><cell>aplmoenb</cell><cell>TD</cell><cell>0.4471</cell><cell>840 / 856</cell><cell>47</cell><cell cols="2">unofficial English run</cell><cell></cell></row><row><cell>aplmoesa</cell><cell>TDN</cell><cell>0.5518</cell><cell>2618 / 2694</cell><cell>49</cell><cell>36</cell><cell>16</cell><cell>1</cell></row><row><cell>aplmoesb</cell><cell>TD</cell><cell>0.5176</cell><cell>2597 / 2694</cell><cell>49</cell><cell>31</cell><cell>6</cell><cell>0</cell></row><row><cell>aplmofra</cell><cell>TDN</cell><cell>0.4210</cell><cell>1202 / 1212</cell><cell>49</cell><cell>24</cell><cell>11</cell><cell>0</cell></row><row><cell>aplmofrb</cell><cell>TD</cell><cell>0.3919</cell><cell>1195 / 1212</cell><cell>49</cell><cell>19</cell><cell>4</cell><cell>2</cell></row><row><cell>aplmoita</cell><cell>TDN</cell><cell>0.4346</cell><cell>1213 / 1246</cell><cell>47</cell><cell>32</cell><cell>8</cell><cell>1</cell></row><row><cell>aplmoitb</cell><cell>TD</cell><cell>0.4049</cell><cell>1210 / 1246</cell><cell>47</cell><cell>26</cell><cell>6</cell><cell>1</cell></row><row><cell>aplmonla</cell><cell>TDN</cell><cell>0.4002</cell><cell>1167 / 1224</cell><cell>50</cell><cell>40</cell><cell>12</cell><cell>0</cell></row><row><cell>aplmonlb</cell><cell>TD</cell><cell>0.3497</cell><cell>1149 / 1224</cell><cell>50</cell><cell>37</cell><cell>3</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="9,70.50,415.90,437.68,152.96"><head>Table 4 .</head><label>4</label><figDesc>Multilingual results</figDesc><table coords="9,70.50,415.90,437.68,140.96"><row><cell></cell><cell>topic</cell><cell>index</cell><cell>normalization</cell><cell>average</cell><cell>recall</cell><cell>Prec. @ 5</cell><cell># ≥</cell><cell># ≥</cell><cell># =</cell></row><row><cell></cell><cell>fields</cell><cell>type(s)</cell><cell>method</cell><cell>precision</cell><cell>(8138)</cell><cell></cell><cell>median</cell><cell>best</cell><cell>worst</cell></row><row><cell>aplmuena</cell><cell>TD</cell><cell>words +</cell><cell>mass</cell><cell>0.2979</cell><cell>5739</cell><cell>0.5600</cell><cell>25</cell><cell>2</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell>6-grams</cell><cell>contribution</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">aplmuenb TDN</cell><cell>words</cell><cell>mass</cell><cell>0.3033</cell><cell>5707</cell><cell>0.5800</cell><cell>31</cell><cell>3</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>contribution</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>aplmuenc</cell><cell>TD</cell><cell>unified</cell><cell>NA</cell><cell>0.1688</cell><cell>2395</cell><cell>0.5600</cell><cell>9</cell><cell>1</cell><cell>9</cell></row><row><cell></cell><cell></cell><cell>words</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>aplmuend</cell><cell>TD</cell><cell>words +</cell><cell>linear</cell><cell>0.3025</cell><cell>5897</cell><cell>0.5240</cell><cell>32</cell><cell>1</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell>6-grams</cell><cell>interpolation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>aplmuene</cell><cell>TD</cell><cell>unified</cell><cell>NA</cell><cell>0.2593</cell><cell>4079</cell><cell>0.5960</cell><cell cols="3">unofficial run</cell></row><row><cell></cell><cell></cell><cell>5-grams</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,88.50,224.60,420.74,8.10;10,70.50,234.35,438.74,8.10;10,70.50,244.85,97.52,8.10" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,261.19,224.60,228.77,8.10">Using Clustering and Super Concepts within SMART: TREC-6</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Walz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,212.25,234.35,237.34,8.10">Proceedings of the Sixth Text REtrieval Conference (TREC-6)</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Sixth Text REtrieval Conference (TREC-6)</meeting>
		<imprint>
			<publisher>NIST Special Publication</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="500" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,88.50,265.85,252.08,8.10;10,341.25,264.15,4.58,5.40;10,349.65,265.85,159.63,8.10;10,70.50,276.35,255.65,8.10" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,151.69,265.85,68.11,8.10">One Term or Two?</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,242.79,265.85,97.80,8.10;10,341.25,264.15,4.58,5.40;10,349.65,265.85,159.63,8.10;10,70.50,276.35,177.65,8.10">the Proceedings of the 18 th International Conference on Research and Development in Information Retrieval (SIGIR-95)</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="310" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,88.50,296.60,420.74,8.10;10,70.50,307.10,348.70,8.10" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,258.25,296.60,250.99,8.10;10,70.50,307.10,108.46,8.10">Cross-Language Retrieval for the CLEF Collections -Comparing Multiple Methods of Retrieval</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Petras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,194.25,307.10,158.44,8.10">Working Notes of the CLEF-2000 Workshop</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,88.50,328.10,420.77,8.10;10,70.50,337.85,438.80,8.10;10,70.50,348.35,239.91,8.10" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,253.25,328.10,256.01,8.10;10,70.50,337.85,192.37,8.10">Manual Queries and Machine Translation in Cross-language Retrieval and Interactive Retrieval with Cheshire II at TREC-7</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,440.25,337.85,69.05,8.10;10,70.50,348.35,161.55,8.10">Proceedings of the Seventh Text REtrieval Conference (TREC-7)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Seventh Text REtrieval Conference (TREC-7)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="527" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,88.50,369.35,420.76,8.10;10,70.50,379.85,219.67,8.10" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="10,199.95,369.35,309.31,8.10;10,70.50,379.85,23.11,8.10">Relating the new language models of information retrieval to the traditional retrieval models</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>De Vries</surname></persName>
		</author>
		<idno>TR-CTIT-00-09</idno>
		<imprint>
			<date type="published" when="2000-05">May 2000</date>
		</imprint>
	</monogr>
	<note type="report_type">CTIT Technical Report</note>
</biblStruct>

<biblStruct coords="10,88.50,400.10,420.75,8.10;10,70.50,410.60,439.49,8.10;10,70.50,421.10,271.53,8.10" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,243.48,400.10,229.27,8.10">A Language-Independent Approach to European Text Retrieval</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Piatko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,116.25,410.60,358.99,8.10">Cross-Language Information Retrieval and Evaluation: Proceedings of the CLEF 2000 Workshop</title>
		<title level="s" coord="10,482.48,410.60,27.51,8.10;10,70.50,421.10,96.94,8.10">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2069</biblScope>
			<biblScope unit="page" from="129" to="139" />
		</imprint>
	</monogr>
	<note>forthcoming</note>
</biblStruct>

<biblStruct coords="10,88.50,441.35,420.72,8.10;10,70.50,451.85,368.91,8.10" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,243.89,441.35,160.62,8.10">The JHU/APL HAIRCUT System at TREC-8</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Piatko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,133.50,451.85,227.57,8.10">Proceedings of the Eighth Text REtrieval Conference (TREC-8)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Eighth Text REtrieval Conference (TREC-8)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="445" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,88.50,472.85,420.73,8.10;10,70.50,483.35,80.33,8.10;10,150.75,481.65,6.00,5.40;10,160.20,483.35,349.06,8.10;10,70.50,493.10,99.70,8.10" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,272.43,472.85,203.68,8.10">A Hidden Markov Model Information Retrieval System</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,498.18,472.85,11.05,8.10;10,70.50,483.35,80.33,8.10;10,150.75,481.65,6.00,5.40;10,160.20,483.35,345.17,8.10">the Proceedings of the 22 nd International Conference on Research and Development in Information Retrieval (SIGIR-99)</title>
		<imprint>
			<date type="published" when="1999-08">August 1999</date>
			<biblScope unit="page" from="214" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,88.50,514.10,340.53,8.10" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="10,214.44,514.10,71.96,8.10">Managing Gigabytes</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Bell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
	<note>Chapter 3</note>
</biblStruct>

<biblStruct coords="10,88.50,535.10,420.76,8.10;10,70.50,544.85,296.94,8.10" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,206.80,535.10,302.46,8.10;10,70.50,544.85,79.19,8.10">Using Suffix Arrays to Compute Term Frequency and Document Frequency for all Substrings in a Corpus</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,169.50,544.85,94.06,8.10">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
