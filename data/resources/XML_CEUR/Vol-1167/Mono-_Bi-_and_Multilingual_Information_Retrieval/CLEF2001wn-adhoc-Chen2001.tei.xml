<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,81.00,116.02,433.21,12.91">Multilingual Information Retrieval Using English and Chinese Queries</title>
				<funder ref="#_MvD92qn">
					<orgName type="full">DARPA (Department of Defense Advanced Research Projects Agency)</orgName>
				</funder>
				<funder ref="#_uhVSh6f #_2gYnudC">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,270.24,155.51,54.86,10.76"><forename type="first">Aitao</forename><surname>Chen</surname></persName>
							<email>aitao@sims.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Management and Systems</orgName>
								<orgName type="institution">University of California at Berkeley</orgName>
								<address>
									<postCode>94720</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,81.00,116.02,433.21,12.91">Multilingual Information Retrieval Using English and Chinese Queries</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BC7E5A394B531F01D08F1ED82364E183</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We participated in the CLEF 2001 monolingual, bilingual, and multilingual tasks. Our interests in these tasks are to test the utility of applying Chinese word segmentation algorithms to German decompounding, to experiment with techniques for combining translations from diverse resources, and to experiment with different approaches to multilingual retrieval. This paper describes our retrieval experiments. ½ ½• ÐÓ Ç´Ê É µ The documents are ranked in decreasing order by their relevance probability È ´Ê É µ with respect to a query. The coefficients were determined by fitting training data to the logistic regression model using a statistical software package. We refer readers to reference [3] for more details.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>At CLEF 2001, we participated in the monolingual, bilingual, and multilingual tasks. Our interest in monolingual task is to test the idea of treating the German decompounding problem as that of Chinese word segmentation and applying Chinese word segmentation algorithms to split German compounds into their constituent words. Our interest in cross-language is to experiment with techniques for combining translations from diverse resources. We are also interested in different approaches to the multilingual retrieval task and various strategies for merging intermediate results to produce a final ranked list of documents for a multilingual retrieval run. In our experiments, we used English and Chinese topics. In translating the topics into the document languages which are English, French, German, Italian, and Spanish, we used two machine translators, one bilingual dictionary, two parallel text corpora, and one Internet search engine.</p><p>We submitted several official runs to the multilingual, bilingual, and monolingual tasks and performed more unofficial runs. To differentiate the unofficial runs from the official ones, the IDs of the official runs are all in uppercase, and IDs of the unofficial runs are all in lowercase. The unofficial runs are those evaluated locally with the official release of the relevance judgments for CLEF 2001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Document Ranking</head><p>The document ranking formula we used in all of our retrieval runs was Berkeley's TREC-2 formula <ref type="bibr" coords="1,490.07,558.22,10.58,8.97" target="#b2">[3]</ref>. The logodds of relevance of document to query É is given by</p><formula xml:id="formula_0" coords="1,120.00,583.17,331.20,18.96">ÐÓ Ç´Ê É µ Ð Ó È ´Ê É µ È ´Ê É µ ¿ ½ • ¿ £ Ü ½ • ¼ ¿¿¼ £ Ü ¾ ¼ ½ ¿ £ Ü ¿ • ¼ ¾ £ Ü</formula><p>where È ´Ê É µ is the probability of relevance of document with respect to query É, È ´Ê É µ is the probability of irrelevance of document with respect to query É. The four composite variables Ü ½ Ü ¾ Ü ¿ , and Ü are defined as follows:</p><formula xml:id="formula_1" coords="1,162.91,617.49,361.52,30.18">Ü ½ ½ Ô Ò•½ È Ò ½ ÕØ ÕÐ •¿ , Ü ¾ ½ Ô Ò•½ È Ò ½ ÐÓ Ø Ð• ¼ , Ü ¿ ½ Ô Ò•½ È Ò ½ ÐÓ Ø Ð , Ü Ò,</formula><p>where Ò is the number of matching terms between a document and a query, Õ Ø is the within-query frequency of the th matching term, Ø is the within-document frequency of the th matching term, Ø is the occurrence frequency in a collection of the th matching term, Õ Ð is query length (number of terms in a query), Ð is document length (number of terms in a document), and Ð is collection length, i.e. the number of occurrences of all terms in a test collection. The relevance probability of document with respect to query É can be written as follows given the logodds of relevance. È ´Ê É µ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Monolingual retrieval experiments</head><p>We present an algorithm to break up German compounds into their constituent words. We treat the German decompounding problem in the same way as the Chinese word segmentation problem which is to segment a string of characters into words. We applied the Chinese segmentation algorithm as described in section 4.1 to decompose German compound words. First, we created a base German word lexicon consisting of all the words, including compounds, found in the German collection for the multilingual task. The uppercase letters were changed to lower case. Second, we identify all possible ways to break up a compound into its constituent words found in the base German lexicon. Third, we compute the probabilities for all possible ways to break up a compound into its constituent words, and choose the segmentation of the highest probability. For example, a compound</p><formula xml:id="formula_2" coords="2,70.86,194.76,453.57,24.18">½ ¾ ¿ may be split into either ½ ½ ¾ ¿ Û ½ Û ¾ Û ¿ , or ¾ ½ ¾ ¿ Û Û , where Û ½ ½ ¾ , Û ¾ ¿ , Û ¿ , Û</formula><p>½ ¾ ¿ , and Û ¿ are German words. The probability</p><formula xml:id="formula_3" coords="2,70.86,218.64,373.41,12.24">of splitting into Û ½ Û ¾ Û ¿ is computed as Ô´ ½ µ Ô´Û ½ Û ¾ Û ¿ µ Ô´Û ½ µ £ Ô´Û ¾ µ £ Ô´Û ¿ µ,</formula><p>and the probability of splitting into Û Û is estimated by Ô´ ¾ µ Ô´Û Û µ Ô´Û µ £ Ô´Û µ. If Ô´ ½ µ is larger than Ô´ ¾ µ, then the compound is split into the three words Û ½ , Û ¾ , and Û ¿ ; otherwise it is split into the two words Û and Û .</p><p>As in Chinese word segmentation, the probability of a word is estimated by its relative frequency in the German document collection. That is, Ô´Û µ Ø ´Û µ È Ò ½ Ø ´Û µ, where Ø ´Û µ is the number of times word Û occurs in the collection, including the cases where Û is a consituent word in compounds; and Ò is the number of unique words, including compounds, in the collection. We submitted two official German monolingual runs labeled BK2GGA1 and BK2GGA2, and two official Spanish monolingual runs labeled BK2SSA1 and BK2SSA2. The first run used title, description, and narrative fields in the topics, while the second run used title and description only. The stopwords were removed from both documents and topics, compounds were split into their constituent words, then words were stemmed using the Muscat German stemmer. Both the compounds and their constituent words were kept in indexing. Both runs were carried out without query expansion. The results are in table <ref type="table" coords="2,285.36,362.98,3.74,8.97" target="#tab_1">2</ref> To provide a base for comparison, three additional runs whose labels are in lower case were carried out. The two official runs with three unofficial runs were summarized in table 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Chinese topics preprocessing</head><p>We first break up a Chinese sentence into text fragments consisting of only Chinese characters. Generally there are many ways to segment a fragment of Chinese text into words. We segment Chinese texts in two steps. First, we examine all the possible ways to segment a Chinese text into words found in a Chinese dictionary. Second, we compute the probabilities of all the segmentations and choose the segmentation with the highest probability. The probability of a segmentation is the product of the probabilities of the words making up the segmentation. For example, let Ë ½ ¾ Ò be a fragment of Chinese text consisting of Ò Chinese characters. Suppose one of the segmentation for the Chinese text is Ë Ï ½ Ï ¾ Ï Ñ , then the probability of this segmentation is computed as follows:</p><formula xml:id="formula_4" coords="3,205.86,197.13,318.58,36.75">Ô´Ë µ Ô´Ï ½ Ï ¾ Ï Ñ µ Ñ ½ Ô´Ï µ<label>(1)</label></formula><p>and</p><formula xml:id="formula_5" coords="3,237.24,261.63,287.20,29.31">Ô´Ï µ Ø ´Ï µ È AE ½ Ø ´Ï µ<label>(2)</label></formula><p>where Ø ´Ï µ is the number of times the word Ï occurs in a Chinese corpus, and AE is the number of unique words in the corpus. Ô´Ï µ is just the maximum likelihood estimate of the probability that the word Ï occurs in the corpus. For a Chinese text, we first enumerate all the possible segmentations with respect to a Chinese dictionary, then we compute the probability for each segmentation. The segmentation of the highest probability is chosen as the final segmentation for the Chinese text. We used the Chinese corpus of the English-Chinese CLIR track at TREC-9 for estimating word probabilities. The Chinese corpus is about 213 MB in size and consist of about 130,000 newspaper articles.</p><p>A commonly used Chinese segmentation algorithm is the longest-matching method which repeatly chops off the longest initial string of characters that appears in the segmentation dictionary until the end of the sentence. A major problem with the longest-matching method is that a mistake often leads to multiple mistakes immediately after the point where the mistake is made. All dictionary-based segmentation methods suffer from the out-of-vocabulary problem. When a new word is missing in the segmentation dictionary, it is often segmented into a sequence of single or two-character words. Based on this observation, we combine the consecutive single-character terms into one word after removing the stopwords from the segmented Chinese topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Chinese topics translation</head><p>The segmentation and de-segmentation of the Chinese topics result in a list of Chinese words for each topic. We translate the Chinese topic words into English using three resources: 1) a Chinese/English bilingual dictionary, 2) two Chinese/English parallel corpora, 3) a Chinese Internet search engine. First, we look up each Chinese word in a Chinese-English bilingual wordlist prepared by the Linguistic Data Consortium and publicly available from http://morph.ldc.upenn.edu/Projects/Chinese/. The wordlist has about 128,000 Chinese words, each paired with a set of English words. If a Chinese word has only one, two or three English translations, we retain them all, otherwise we choose the three translations that occur most frequently in the Los Angeles Times collection which is part of the document collections for the CLEF 2001 multilingual task.</p><p>We created a Chinese-English bilingual lexicon from two Chinese/English parallel corpora, the Hong Kong News corpus and the FBIS corpus. The Hong Kong News corpus consists of the daily Press Release of the Hong Kong Government in both Chinese and English during the period of from April, 1998 through March, 2001. The source Chinese documents and English documents are not paired. So for each Chinese document, we have to identify the corresponding English document. We first aligned the Hong Kong News corpus at the document level using the LDC bilingual wordlist. Then we aligned the documents at the sentence level. Unlike the Hong Kong News corpus, the Chinese documents and their English translations are paired in the FBIS corpus. The documents in the FBIS corpus are usually long, so we first aligned the parallel documents at the paragraph level, then at the sentence level. We adapted the length-based alignment algorithm proposed by Gale and Church <ref type="bibr" coords="3,448.80,696.04,11.62,8.97" target="#b4">[5]</ref> to align parallel English/Chinese text. We refer interested readers to the paper in <ref type="bibr" coords="3,327.95,707.98,11.68,8.97" target="#b0">[1]</ref> for more details.</p><p>From the aligned pairs of Chinese/English sentences, we created a Chinese/English bilingual lexicon based on co-occurrence of word pairs across the aligned sentences. We used the maximum likelihood ratio measure proposed by Dunning <ref type="bibr" coords="3,162.11,743.86,11.62,8.97" target="#b3">[4]</ref> to compute the association score between a Chinese word and an English word. The bilingual lexicon takes as input a Chinese word and returns a ranked list of English words. We looked up each Chinese topic word in this bilingual Chinese/English lexicon, and retained the top two English words.</p><p>For the Chinese words that are missing in the two bilingual lexicons, we submitted them one by one to Yahoo!China, a Chinese Internet search engine at http://chinese.yahoo.com. Each entry in the search result pages has one or two sentences that contain the Chinese word searched. For each Chinese word, we downloaded all the search result pages if there are fewer than 20 result pages, or the first 20 pages if there are more than 20 result pages. Each result page contains 20 entries. From the downloaded result pages for a Chinese word, we extracted the English words in parentheses that follow immediately after the Chinese word. If there are English words found in the first step, we keep all the English words as the translations of the Chinese word. And if the first step failed to extract any English words, we extracted the English words appearing after the Chinese words. If there are more than 5 different English translations extracted from the result pages, we keep the top three most frequent words in the translations. Otherwise we keep all English translations. We refer interested readers to the paper in <ref type="bibr" coords="4,498.13,205.60,11.68,8.97" target="#b1">[2]</ref> for more details. This technique is based on the observation that the original English proper nouns sometimes appear in parentheses immediately after the Chinese translation. This technique should work well for proper nouns which are often missing in dictionaries. For many of the proper nouns in the CLEF 2001 Chinese topics missing in both the LDC bilingual dictionary and the bilingual dictionary created from parallel Chinese/English corpora, we extracted their English translations from the Yahoo!China search results. The last step in translating Chinese words into English is to merge the English translations obtained from the three resources mentioned above and weight the English translation terms. We give an example to illustrate the merging and weighting of the English translation terms. If a Chinese word has three English translation terms ½ ¾ , and ¿ from the LDC bilingual dictionary; and two English translation terms ¾ and from the bilingual dictionary created from the parallel texts. Then the set of words ½ ¾ ¿ ¾ constitute the translation of the Chinese word. There is no translation terms from the third resource because we submit a Chinese word to the search engine only when the Chinese word is not found in both bilingual dictionaries. Next we normalize the weight of the translation terms so that the sum of their weights is one unit. For the example, the weights are distributed among the four unique translation terms as follows: ½ ¾, ¾ , ¿ ¾, and ¾. Note that the weight for the term ¾ is twice of that for the other three terms because it came from both dictionaries. We believe a translation term appearing in both dictionaries are more likely to be the appropriate translation than the ones appearing in only one of the dictionaries. Finally we multiply the weight by the frequency of the Chinese word in the original topic. So if the Chinese word occurs three times in the topic, the final weights assigned to the English translation terms of the Chinese word are ½ , ¾ ½ ¾, ¿</p><p>, and . The English translations of the Chinese topics were indexed and searched against the LA Times collection. We submitted two Chinese-to-English bilingual runs, one using all three topics fields, and the other using title and description only. Both runs were carried out without pre-translation or post-translation query expansion. The documents and English translations were stemmed using the Muscat English stemmer. The performance of these two runs are summarized in table <ref type="table" coords="4,203.36,492.52,3.74,8.97" target="#tab_2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Multilingual retrieval</head><p>We participated in the multilingual task using both English and Chinese topics. Our main approach was to translate the source topics into the document languages which are English, French, German, Italian, and Spanish, perform retrieval runs separately for each language, then merge the individual results for all five document languages into one ranked list of documents. We created a separate index for each of the five document collections by language. The stopwords were removed, words were stemmed using Muscat stemmers, and all uppercase letters were changed to lower case. The topics were processed in the same way.</p><p>For the multilingual retrieval experiments using English topics, we translated the English topics directly into French, German, Italian, and Spanish using both Systran translator and L&amp;H Power translator. The topic translations of the same language from both translators were combined by topic, and then searched against the document collection of the same language. So for each multilingual retrieval run, we had five ranked list of documents, one for each document language. The five ranked lists of documents were merged to produce the final ranked list of documents for each multilingual run.</p><p>Our merging strategy is to combine all five intermediate runs and rank the documents by adjusted weights. Before we merge the intermediate runs, we made two adjustments to the estimated probability of document relevance in the intermediate runs. First, we reduced the estimated probability of document relevance by 20% (i.e, multiplying the original probability by .8) for the English documents retrieved using the un-translated English source topics. Then we added a value of 1.0 to the estimated probability of relevance for the top-ranked 50 documents in all monolingual runs. After these two adjustments to the estimated probability, we combined all five intermediate runs, sorted the combined results by adjusted probability of relevance, then took the top-ranked 1000 documents for each topic to create the final ranked list of documents. The aim of making the first adjustment is to make the estimated probability of relevance for all document languages comparable. Since translating topics from the source language to a target language probably introduces information loss to some degree, the estimated probability of relevance for the same topic may be slightly underestimated for the target language. In order to make the estimated probabilities for the documents retrieved using the original topics and using the translated topics comparable, the estimated probabilities for the documents retrieved using the original topics should be slightly lowered. The intention of making the second adjustment is to make sure that the top-ranked 50 documents in each of the intermediate results will be among the top-ranked 250 documents in the final ranked list.</p><p>For the multilingual retrieval experiments using Chinese topics, we translated the Chinese topics word by word into English, French, German, Italian, and Spanish in two stages. First, we translated the Chinese topics into English using three resources: 1) a bilingual dictionary, 2) two parallel corpora, and 3) one Chinese search engine. The procedure of translating Chinese topics into English was described in section 4. The English translations from the source Chinese topics consist of not sentences but words. Second, we translated the English words into French, German, Italian, and Spanish using both Systran translator and L&amp;H translator for lack of resources to directly translate the Chinese topics into these languages. The rest is the same as for multilingual experiments using English topics.</p><p>We submitted four official multilingual runs, two using English topics and two using Chinese topics. The official runs are summarized in table 6. The multilingual run labeled BK2MUEAA1 was produced by combining the monolingual run bk2eea1 (.5553), and four cross-language runs bk2efa1 (.4776), bk2ega1 (.3789), bk2eia1 (.3934), bk2esa1 (.4703). The multilingual run labeled BK2MUCAA1 was produced by combining five crosslanguage runs, BK2CEA1, bk2cfa1, bk2cga1, bk2cia1, and bk2csa1. The performance of these five cross-language runs using Chinese topics is presented in table <ref type="table" coords="5,257.24,713.32,3.74,8.97" target="#tab_4">5</ref>.</p><p>The problem of merging multiple runs into one is closely related to the problem of calibrating the estimated probability of document relevance and the problem of estimating the number of relevant documents with respect to a given query in a collection. If the estimated probability of document relevance is well calibrated, that is, the estimated probability is close to the true probability of relevance, then it would be trivial to combine multiple runs into one, since all one needs to do will be to combine the multiple runs and re-rank the documents in the estimated probability of relevance. If the number of relevant documents with respect to a given query could be well estimated, then one could take the number of documents from each individual run that is proportional to the number of estimated relevant documents in each collection. Unfortunately neither one of the problems is easy to solve. Since merging multiple runs is not an easy task, an alternative approach to this problem is to work on it indirectly, that is, transform it into another problem that may be easier to solve. There are two alternative approaches to the problem of multilingual information retrieval. The first method works by translating the source topics into all document languages, combining the source topics and their translations in document languages, and then searching the combined, multilingual topics against a single index of documents in all languages. The second method works by translating all documents into the query language, then performing monolingual retrieval against the translated documents which are all in the same language as that of the query.</p><p>We applied the first alternative method to the multilingual IR task. We translated the source English topics directly into French, German, Italian, and Spanish using both Systran translator and L&amp;H Power translator. Then we combined the English topics with the other four translations of both translators into one set of topics. The within-query term frequency is reduced by half. We used the multilingual topics for retrieval against a single index of all documents. The performance of this run labeled bk2eaa4 is shown in table <ref type="table" coords="6,407.23,400.90,3.74,8.97" target="#tab_6">7</ref> were not able to apply the second alternative method. Instead, we experimented with the method of translating the French, Italian, German, and Spanish documents retrieved in the intermediate runs back into English, and then carring out a monolingual retrieval run. We did not use Systran translator or L&amp;H Power translator to translate the retrieved documents into English. We compiled a wordlist from the documents retrieved, then submitted the wordlist into Systran. The translation results of the wordlist were used to translate word by word the retrieved documents into English. The overall precision is .3648 for this run labeled bk2eaa5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have tested the idea of treating the German decompounding problem in the same way as the Chinese word segmentation problem. The decompounding of German compound words did not improve precision. We believe the problem is that the decompounding algorithm failed to consistently decompose German compounds into their consitituent words. We observed that multi-word compounds are sometimes split into single words and shorter compounds. We also presented a method for combining translations from three different translation resources which seems to work well. We experimented with three approaches to multilingual retrieval. The method of translating the documents retrieved in the intermediate runs back into the language of the source topics, and then carring out monolingual retrieval achieved better precision than the other two methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,70.86,362.98,453.51,220.41"><head>Table 1 . Monolingual IR performance. evaluated locally and the results are in table 1.</head><label>1</label><figDesc>. The monolingual runs for the other three languages were</figDesc><table coords="2,101.46,385.90,392.25,197.49"><row><cell>Run ID</cell><cell></cell><cell>bk2eea1</cell><cell>bk2ffa1</cell><cell>BK2GGA1</cell><cell>bk2iia1</cell><cell>BK2SSA1</cell></row><row><cell>Language</cell><cell></cell><cell>English</cell><cell>French</cell><cell>German</cell><cell>Italian</cell><cell>Spanish</cell></row><row><cell cols="3">Average Precision 0.5553</cell><cell>0.4743</cell><cell>0.4050</cell><cell>0.4370</cell><cell>0.5302</cell></row><row><cell cols="2">Overall Recall</cell><cell>95.33%</cell><cell>98.84%</cell><cell>92.63%</cell><cell>95.83%</cell><cell>95.06%</cell></row><row><cell></cell><cell></cell><cell cols="5">(816/856) (1198/1212) (1973/2130) (1194/1246) (2561/2694)</cell></row><row><cell>Run ID</cell><cell cols="3">Topic Fields Features</cell><cell cols="3">Overall Recall Average Precision</cell></row><row><cell cols="2">BK2GGA1 T,D,N</cell><cell cols="4">+stemming, +decompounding 92.63%</cell><cell>0.4050</cell></row><row><cell cols="2">BK2GGA2 T,D</cell><cell cols="4">+stemming, +decompounding 88.31%</cell><cell>0.3683</cell></row><row><cell>bk2gga3</cell><cell>T,D,N</cell><cell cols="4">+stemming, -decompounding 90.94%</cell><cell>0.4074</cell></row><row><cell>bk2gga4</cell><cell>T,D,N</cell><cell cols="4">-stemming, +decompounding 89.81%</cell><cell>0.3594</cell></row><row><cell>bk2gga5</cell><cell>T,D,N</cell><cell cols="4">-stemming, -decompounding 88.12%</cell><cell>0.3673</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,82.80,596.27,430.52,21.22"><head>Table 2 . German monolingual retrieval performance. The total number of German relevant documents for 49 topics is 2130.</head><label>2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="4,70.86,492.52,454.95,206.07"><head>Table 3 . Chinese to English bilingual retrieval performance. are</head><label>3</label><figDesc>. The results of the cross-language runs from English to the other four languages in table 4, and the results of the cross-language runs from Chinese to all five document languages are in table 5.</figDesc><table coords="4,76.86,515.38,448.94,183.21"><row><cell>Run ID</cell><cell cols="3">Topic Fields Translation Resources</cell><cell cols="3">Overall Recall Average Precision</cell></row><row><cell cols="2">BK2CEA1 T,D,N</cell><cell cols="3">dictionary, parallel texts, search engine 755/856</cell><cell cols="2">0.4122</cell></row><row><cell cols="2">BK2CEA2 T,D</cell><cell cols="3">dictionary, parallel texts, search engine 738/856</cell><cell cols="2">0.3683</cell></row><row><cell>Run ID</cell><cell>Topic Topic</cell><cell cols="2">Document Translation</cell><cell>Overall</cell><cell>Average</cell><cell>% Monolingual</cell></row><row><cell></cell><cell cols="3">Fields Language Language Resources</cell><cell>Recall</cell><cell cols="2">Precision Performance</cell></row><row><cell cols="2">bk2efa1 T,D,N English</cell><cell>French</cell><cell cols="3">Systran+L&amp;H Power 1186/1212 0.4776</cell><cell>100.7%</cell></row><row><cell cols="2">bk2ega1 T,D,N English</cell><cell>German</cell><cell cols="3">Systran+L&amp;H Power 1892/2130 0.3789</cell><cell>93.56%</cell></row><row><cell cols="2">bk2eia1 T,D,N English</cell><cell>Italian</cell><cell cols="3">Systran+L&amp;H Power 1162/1246 0.3934</cell><cell>90.02%</cell></row><row><cell cols="2">bk2esa1 T,D,N English</cell><cell>Spanish</cell><cell cols="3">Systran+L&amp;H Power 2468/2694 0.4703</cell><cell>88.70%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,217.80,711.47,159.92,9.22"><head>Table 4 . Bilingual IR performance.</head><label>4</label><figDesc></figDesc><table coords="5,117.42,72.88,360.42,81.09"><row><cell>Run ID</cell><cell>Topic Topic</cell><cell cols="2">Document Overall</cell><cell>Average</cell><cell>%Monolingual</cell></row><row><cell></cell><cell cols="3">Fields Language Language Recall</cell><cell cols="2">Precision Performance</cell></row><row><cell cols="2">BK2CEA1 T,D,N Chinese</cell><cell>English</cell><cell>755/856</cell><cell>0.4122</cell><cell>74.23%</cell></row><row><cell>bk2cfa1</cell><cell>T,D,N Chinese</cell><cell>French</cell><cell cols="2">1040/1212 0.2874</cell><cell>60.59%</cell></row><row><cell>bk2cga1</cell><cell>T,D,N Chinese</cell><cell>German</cell><cell cols="2">1605/2130 0.2619</cell><cell>64.67%</cell></row><row><cell>bk2cia1</cell><cell>T,D,N Chinese</cell><cell>Italian</cell><cell cols="2">1004/1246 0.2509</cell><cell>57.41%</cell></row><row><cell>bk2csa1</cell><cell>T,D,N Chinese</cell><cell>Spanish</cell><cell cols="2">2211/2694 0.2942</cell><cell>55.49%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="5,217.80,166.91,159.92,9.22"><head>Table 5 . Bilingual IR performance.</head><label>5</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,121.50,72.88,352.26,79.73"><head>Table 6 . Multilingual retrieval performance.</head><label>6</label><figDesc></figDesc><table coords="6,121.50,72.88,352.26,57.63"><row><cell>Run ID</cell><cell cols="4">Topic Language Topic Fields Overall Recall Average Precision</cell></row><row><cell cols="2">BK2MUEAA1 English</cell><cell>T,D,N</cell><cell>5953/8138</cell><cell>0.3424</cell></row><row><cell cols="2">BK2MUEAA2 English</cell><cell>T,D</cell><cell>5686/8138</cell><cell>0.3029</cell></row><row><cell cols="2">BK2MUCAA1 Chinese</cell><cell>T,D,N</cell><cell>4738/8138</cell><cell>0.2217</cell></row><row><cell cols="2">BK2MUCAA2 Chinese</cell><cell>T,D</cell><cell>4609/8138</cell><cell>0.1980</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="6,135.30,400.90,388.95,78.28"><head>Table 7 . Multilingual IR performance.</head><label>7</label><figDesc>. For lack of resources, we</figDesc><table coords="6,135.30,423.82,324.60,33.27"><row><cell>Run ID</cell><cell cols="4">Topic Language Topic Fields Overall Recall Average Precision</cell></row><row><cell cols="2">bk2eaa3 English</cell><cell>T,D,N</cell><cell>5551/8138</cell><cell>0.3126</cell></row><row><cell cols="2">bk2eaa4 English</cell><cell>T,D,N</cell><cell>5697/8138</cell><cell>0.3648</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0" coords="2,88.78,680.87,158.39,10.76;2,82.20,708.16,441.95,8.97;2,70.86,720.10,50.96,8.97"><p>Bilingual retrieval experimentsIn this section we will describe the pre-processing of the Chinese topics and translation of the Chinese topics into English.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p>This research was supported by <rs type="funder">DARPA (Department of Defense Advanced Research Projects Agency)</rs> under research contract <rs type="grantNumber">N66001-97-8541</rs>; AO# <rs type="grantNumber">F477</rs>: <rs type="projectName">Search Support for Unfamiliar Metadata Vocabularies</rs> within the <rs type="institution">DARPA</rs> <rs type="projectName">Information Technology Office</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_MvD92qn">
					<idno type="grant-number">N66001-97-8541</idno>
				</org>
				<org type="funded-project" xml:id="_uhVSh6f">
					<idno type="grant-number">F477</idno>
					<orgName type="project" subtype="full">Search Support for Unfamiliar Metadata Vocabularies</orgName>
				</org>
				<org type="funded-project" xml:id="_2gYnudC">
					<orgName type="project" subtype="full">Information Technology Office</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,87.43,177.36,436.57,8.07;7,87.48,188.34,436.22,8.07;7,87.48,199.32,61.91,8.07" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,204.66,177.36,319.35,8.07;7,87.48,188.34,28.47,8.07">Alignment of english-chinese parallel corpora and its use in cross-language information retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,131.45,188.34,279.33,8.07">19th International Conference on Computer Processing of Oriental Languages</title>
		<meeting><address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">May 14-16 2001</date>
			<biblScope unit="page" from="251" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,87.43,209.76,436.34,8.07;7,87.48,220.74,436.09,8.07;7,87.48,231.66,170.02,8.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,201.96,209.76,321.82,8.07;7,87.48,220.74,72.85,8.07">Combining multiple sources for short query translation in chinese-english cross-language information retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,176.19,220.74,343.24,8.07">Proceedings of the Fifth International Workshop on Information Retrieval with Asian Languages</title>
		<meeting>the Fifth International Workshop on Information Retrieval with Asian Languages<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-10-01">Sept. 30-Oct 1 2000</date>
			<biblScope unit="page" from="17" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,87.43,242.16,436.37,8.07;7,87.48,253.08,407.54,8.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,225.73,242.16,298.08,8.07;7,87.48,253.08,35.43,8.07">Full text retrieval based on probabilistic equations with coefficients fitted by logistic regression</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">S</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,219.62,253.08,173.63,8.07">The Second Text REtrieval Conference (TREC-2)</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1994-03">March 1994</date>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,87.43,263.58,436.12,8.07;7,87.48,274.50,20.17,8.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,133.88,263.58,223.46,8.07">Accurate methods for the statistics of surprise and coincidence</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Dunning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,364.64,263.58,91.70,8.07">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="61" to="74" />
			<date type="published" when="1993-03">March 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,87.43,285.00,436.19,8.07;7,87.48,295.92,63.71,8.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,201.47,285.00,192.34,8.07">A program for aligning sentences in bilingual corpora</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">A</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,401.30,285.00,91.82,8.07">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="75" to="102" />
			<date type="published" when="1993-03">March 1993</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
