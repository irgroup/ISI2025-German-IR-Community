<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,91.68,71.18,411.81,15.41;1,246.96,95.42,100.92,15.41">Across the Bridge: CLEF 2001 -Non-English Monolingual Retrieval. The French task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,223.44,143.69,65.42,10.89"><forename type="first">Eugenia</forename><surname>Matoyo</surname></persName>
						</author>
						<author role="corresp">
							<persName coords="1,301.43,143.69,65.79,10.89"><forename type="first">Tony</forename><surname>Valsamidis</surname></persName>
							<email>a.valsamidis@gre.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Greenwich</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Old Royal Naval College</orgName>
								<address>
									<addrLine>Park Row</addrLine>
									<postCode>SE10 9LS</postCode>
									<settlement>Greenwich, London</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,91.68,71.18,411.81,15.41;1,246.96,95.42,100.92,15.41">Across the Bridge: CLEF 2001 -Non-English Monolingual Retrieval. The French task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">08B8B5F5264C6B69A0062C8D5B1D49E7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This paper presents work on document retrieval based on participation in the Cross-Language</head><p>Evaluation Forum (CLEF) 2001 task of non-English monolingual retrieval task using French only. In summary, the experiment findings indicate that Okapi, the text retrieval system in use, can successfully be used for non-English text retrieval although a lot of internal pre-processing is required in the basic search system to convert the documents and topics into Okapi access formats. Various shell scripts were written to achieve the conversion in a Unix environment, failure of which would significantly have impeded the overall performance. Based on the experiment findings using Okapi, which was originally designed for the English Language, it was clear that there was significant difference between French and English retrieval depending on the retrieval system in use.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Participation in the CLEF 2001 monolingual task <ref type="bibr" coords="1,270.48,441.41,7.69,7.17" target="#b0">[2]</ref> for French serves as a stepping stone toward the multilingual cross-language information retrieval. The background to the CLEF Campaigns as presented in the European Research Letter (2000) <ref type="bibr" coords="1,165.90,475.73,7.80,7.17" target="#b2">[4]</ref> triggered interest toward participation in this year's information retrieval task, which was our first attempt to participate in any of the CLEF tasks. The main task attempted was that of monolingual (non-English) Information Retrieval on French topics and documents as assigned by CLEF and retrieval was based on automatic query construction using Okapi. The main reason for choice of automatic query retrieval as opposed to manual query expansion was largely the time constraint. Automation implies that documents were automatically retrieved by the system without the intervention of the experimenter in interactive manual query expansion <ref type="bibr" coords="1,110.70,579.41,7.56,7.17" target="#b7">[9]</ref> , which would have been time consuming and time was a limited resource given that the experiments had to be completed in time for the CLEF submission deadline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Objectives</head><p>The primary aim of this experiment was three-fold:</p><p>Crossing the Bridge: An attempt at participation in the CLEF tasks on Information Retrieval for better understanding of issues in Cross-language information retrieval.</p><p>To investigate whether Okapi, an experimental text retrieval system from City University of London <ref type="bibr" coords="2,130.32,86.93,7.82,7.17">[1]</ref> , could successfully be used to provide a useful interface in the retrieval of French documents using indexing methods and stopwords approach.</p><p>To investigate whether techniques applied for English text retrieval differ significantly from those used for French retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GENERAL SYSTEM DESCRIPTION</head><p>The monolingual experiments for French documents were carried out using Okapi, a text retrieval system project based at City University, London, which is used solely for research purposes. The Okapi system requires either a Solaris on Sun environment or Linux on Intel environment to run the software. It uses a probabilistic model <ref type="bibr" coords="2,513.70,243.41,10.70,7.17" target="#b10">[10]</ref> of information retrieval, which was first developed by Robertson <ref type="bibr" coords="2,332.88,260.69,7.71,7.17" target="#b5">[7]</ref> . This model performs in an iterative process, which uses the ranking of document listings based on indexing, term weighting function and word stemming rules for optimizing search queries. The three major components of Okapi are: Indexing Software, which enables users to create and index Okapi type databases</p><p>The Basic Search System (BSS), which is a set of low level commands to enable users to build their own interface around it.</p><p>Okapi Interactive Interface, which is the graphical user interface, which calls BSS command in a manner such as to hide the complexity from the user.</p><p>It is important to point out here that Okapi system was originally designed for use with English. Although it had been used in similar text retrieval experiments for languages other than English <ref type="bibr" coords="2,421.51,433.25,7.66,7.17" target="#b7">[9]</ref> , using it for the CLEF experiment for monolingual tasks was the very first time that it has been used for French. The section below describes the work that had to be done in the Okapi basic search system to allow for French monolingual retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PRE-PROCESSING TECHNIQUES</head><p>Prior to carrying out the formal runs, it was necessary to carry out various pre-processing tasks on the topics and the French collection using various shell scripts based on previous experiments in the Text Retrieval Conference (TREC) <ref type="bibr" coords="2,102.50,574.61,7.56,7.17" target="#b4">[6]</ref> series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Database Integration</head><p>The two separate French collections, le_monde.tar.gz (Initial zipped file of 154 MB) and sda_french -Initial zipped file of 80 MB), were integrated into a single database, le_m+dsa_fr upon which to carry out the query processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conversion</head><p>A shell script (convert_topic) was written to reformat the 50 CLEF French topics by altering the title from Fr-Title to Title (because retrieval was required for the Title field) and changing the document numbers by removing the preceding CO. This reconstruction and manipulation of the topics and documents was necessary to convert the topics into Okapi access format as well as to enable the reuse of previous shell scripts, which had been used in similar TREC experiments. Various changes to the scripts were, however, required to customize them for the CLEF collection for French. Below is the script convert_topic: # Script to convert CLEF French topics to TREC format sed -e 's/FR-//g' \ -e 's/C0//' #Substitute nothing for FR globally from the topic titles to be plain &lt;title&gt; # Substitute nothing for CO globally for the Document numbers to be in plain numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stoplist</head><p>The stoplist, also referred to as stopwords or stopterms, is made up of common, generally used words in a language collection, which are considered irrelevant for the purpose of information retrieval because of their high frequency of occurrence. The stoplist comprised 248 terms and was derived both from several sources to include common French terms <ref type="bibr" coords="3,201.86,314.69,8.58,7.17" target="#b1">[3 ]</ref> , information compiled by Professor Jaques Savoy of the University of Neuchatel for the CLEF web site <ref type="bibr" coords="3,215.86,331.97,7.70,7.17" target="#b3">[5]</ref> and based on Prof. Savoy's publication in the Journal of the American Society for Information Science <ref type="bibr" coords="3,200.64,349.25,7.89,7.17" target="#b6">[8]</ref> . All accented words were removed from the stoplist because Okapi cannot handle accented words.</p><p>A shell conversion script was required to convert the stoplist to Okapi-accessible formats because a H was required before each word, a comma [,] after each word and finally a colon [:] at the end of each line. An example of the conversion is from avec to Havec,: to comply with Okapi formats. This was achieved using an emacs editing utility in the Unix environment to write a simple conversion script:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#For every line #French stoplist</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Replace string ^J with : H # H to indicate stop term</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Replace string ^J with : ,: # to conform with Okapi acceptable for stopterms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stemming</head><p>There was no stemming applied at all for this experiment. Porter's stemming algorithm, which is configured to work for Muscat -an open source search engine -had been intended for use with Okapi but failed to run successfully. The Muscat stemmer depends on its own character coding, which Okapi could not recognize because it does not work with unicode. A frontend shell script was written to adapt Okapi Basic Search System (BSS) to the stemmer but this also failed to work. Given more time, the script may have been successfully debugged and configured for Okapi but this was not accomplished due to time limitations. Since Porter's stemmer for English has successfully been used previously with Okapi in similar TREC experiments <ref type="bibr" coords="3,474.05,686.45,7.56,7.17" target="#b7">[9]</ref> , it implies that Okapi could not properly handle the accented words in the French language for the stemmer to work in this experiment. Thus stemming was omitted altogether and it may be worthwhile to note that this lack of stemming has resulted in less accurate results in the final formal runs for the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Indexing</head><p>Indexing of terms was accomplished in Okapi using an inbuilt Okapi indexing utility to index the integrated collection. The utility was called using two of Okapi's BSS (basic search system) programmes ix1 and ixf as shown below:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ix1 -delfinal le_m+sda_fr 1 | ixf le_m+sda_fr 1</head><p>The same programmes were used to index the 50 CLEF topics for the combined collection: ix1 -mem 50 -delfinal -doclens le_m+sda_fr 0 | ixf le_m+sda_fr 0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Testing -Comparison with CLEF input checker</head><p>Throughout the experiments, it was important to ensure that the results would be in the required format for CLEF. The test runs were, therefore, validated against the CLEF input checker, CheckInput.pl. This was a Perl script, which compared the run results against the input checker for error identification in an effort to expose possible defects in the runs before submission of the formal runs. The input checker revealed that there were no serious errors in the run results, although a couple of topic sets yielded error on retrieval, possibly as a result of a segmentation bug in the Okapi BSS release.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>THE FORMAL RUNS</head><p>The formal runs were automatic and query processing for the French topics and documents was done in three separate runs, each using the the same French stoplist:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 1 -retrieval of documents by topic Title, Description and Narrative Run 2 -retrieval by topic Title and Description only Run 3 -retrieval by topic Title only</head><p>The graph below indicates the order of importance as assigned by the system, and retrieval by Title, Description and Narrative received higher ranking: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PRESENTATION AND RESULTS</head><p>The formal runs were zipped using gZip in Unix and were submitted by FTP to CLEF in ASCII as binary format and the results were sent out together with a README file describing the different runs. The formal results from CLEF for the submitted runs were as expected; Run gre3, which had the least priority was not judged.</p><p>Similarly, in a graph of recall versus precision, run gre3 scored the least precision values. Our results for the experiment can be summarised as:</p><p>CLEF evaluation results for run gre1: ====================================================================== This run was JUDGED, i.e. the results file contributed to the relevance assessment pool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLEF evaluation results for run gre2: ======================================================================</head><p>This run was JUDGED, i.e. the results file contributed to the relevance assessment pool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLEF evaluation results for run gre3: ======================================================================</head><p>This run was NOT JUDGED, i.e. because of limited evaluation resources, the result file did not directly contribute to the relevance assessment pool. However, the run was subject to all other standard processing, and is still scored as an official submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig 2.0 Summary of Run Results</head><p>Although all of our experiment runs had a relatively poor performance against the comparison to the 'median' graph by topic from CLEF, which gave an indication of how well our results were according to other groups, run gre1 had a considerably better performance than both runs gre2 and gre3. Run gre3 had the worst median performance of the three runs. The average precision (non -interpolated) for all relevant documents was 1.0000 for both runs gre1 and gre2 but only 0.3381 for run gre3. Our deduction from the runs thus shows that document retrieval using the topic Title, Description and Narrative fields yields far better results than attempted document retrieval by omission of any of these fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSIONS AND FUTURE WORK</head><p>Work done on similar TREC experiments proved to be useful for the CLEF experiments by enabling reuse of previous scripts. Failure of Okapi to properly handle accents in the French language leads to our conclusion that procedures for monolingual information retrieval are not completely language independent. Different languages present different problems depending on the information retrieval system in use. Working with a French collection does not guarantee the use of established methods (such as indexing and stoplists), which would work well with English text retrieval. Methods that may be highly efficient for certain language typologies may not be so effective for others. Future work would involve adapting Porter's stemmer to Okapi and attempting the cross-language information retrieval in the multi-lingual tasks. We did not really expect great results, this being the first time round, but it is our hope that future participation will considerably yield better results, especially after sharing experience with other participants at the CLEF Workshop.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,70.80,729.29,389.21,10.89"><head>Fig 1. 0</head><label>0</label><figDesc>Fig 1.0 Chart showing Results per experiment run for the same topic and ranked documents</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>Without <rs type="person">Steve Walker</rs>, the inventor of Okapi, we would not have been able to meet the CLEF deadline for participation in the 2001. He carried out a large proportion of the pre-processing activities for data conversion into access formats for both Okapi and CLEF. He also handled the testing of our formal runs against the CLEF input checker based upon his previous experience from participation in the Text Retrieval Conference (TREC) [6] series.</p><p>Special acknowledgement of help and collaboration also goes to <rs type="person">Surinder Singh Dio</rs> of the Unix help at the <rs type="affiliation">University of Greenwich</rs>. He set up the okapi pack in the computing laboratory at the university and took great pains to set up the environment variable settings as required for the successful operation of the Okapi software.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="6,76.89,440.33,447.83,10.89;6,88.80,457.61,285.31,10.89" xml:id="b0">
	<monogr>
		<ptr target="http://www.iei.pi.cnr.it/DELOS/CLEF/clef01.html" />
		<title level="m" coord="6,88.80,440.33,267.06,10.89">Cross Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2000-05">2001. 14 March 2000. May 2001</date>
		</imprint>
	</monogr>
	<note>CLEF: CLEF Agenda for</note>
</biblStruct>

<biblStruct coords="6,74.61,474.89,449.61,10.89;6,88.80,492.17,233.94,10.89" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="6,182.40,474.89,201.18,10.89">Fluent French: Experiences of an English speaker</title>
		<author>
			<persName coords=""><forename type="first">Erik</forename><forename type="middle">T</forename><surname>Mueller</surname></persName>
		</author>
		<ptr target="http://www.signiform.com/french/" />
		<imprint>
			<date type="published" when="1998-06-04">1998. June 4, 2001</date>
			<publisher>Signiform</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,76.84,509.45,447.82,10.89;6,88.80,526.73,435.79,10.89;6,88.80,544.01,49.47,10.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,273.30,509.45,251.35,10.89;6,88.80,526.73,110.71,10.89">Cross-Language System Evaluation: the CLEF Campaigns. European Research Letter</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,270.52,526.73,254.07,10.89;6,88.80,544.01,44.97,10.89">Journal of American Society for Information Science and Technology</title>
		<imprint/>
	</monogr>
	<note>forthcoming</note>
</biblStruct>

<biblStruct coords="6,74.49,561.05,418.53,10.89" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,151.67,561.05,54.94,10.89">Stopword List</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
		<ptr target="http://www.unine.ch/info/clef/" />
		<imprint>
			<date type="published" when="2001-06-05">2001. June 5, 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.49,578.33,417.94,10.89" xml:id="b4">
	<monogr>
		<ptr target="http://trec.nist.gov/" />
		<title level="m" coord="6,88.80,578.33,171.49,10.89">Text Retrieval Conference, Test Collections</title>
		<imprint>
			<date type="published" when="2001-05">May 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,78.37,595.61,446.05,10.89;6,88.80,612.89,301.68,10.89" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="6,284.95,595.61,159.24,10.89">The Probabilistic Retrieval Model</title>
		<ptr target="http://www.soi.city.ac.uk/research/cisr/okapi/prm.html" />
		<imprint>
			<date type="published" when="2001-05">May 2001</date>
		</imprint>
		<respStmt>
			<orgName>Centre for Interactive Systems Research</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.92,630.17,449.66,10.89;6,88.80,647.45,238.57,10.89" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,156.48,630.17,295.94,10.89">A stemming procedure and stopword list for general French corpora</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,467.39,630.17,57.19,10.89;6,88.80,647.45,165.57,10.89">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="944" to="952" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,74.49,664.73,318.02,10.89" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
		<title level="m" coord="6,139.92,664.73,218.09,10.89">Experiments on Interfaces to Support Query Expansion</title>
		<imprint>
			<biblScope unit="page" from="8" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,394.97,664.73,120.26,10.89;6,88.80,682.01,390.28,10.89" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
		<title level="m" coord="6,142.36,682.01,297.38,10.89">Laboratory experiments with Okapi: participation in the TREC programme</title>
		<imprint>
			<biblScope unit="page" from="20" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,481.54,682.01,16.40,10.89;6,88.80,699.05,421.96,10.89;6,88.80,716.33,421.80,10.89;6,88.80,733.61,128.72,10.89" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,201.94,699.05,193.24,10.89;6,134.17,716.33,241.36,10.89">Application of probabilistic methods to Chinese text retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,417.41,716.33,93.19,10.89;6,88.80,733.61,71.33,10.89">Special issue of Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="74" to="79" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note>Research and evaluation in information retrieval</note>
</biblStruct>

<biblStruct coords="7,79.00,71.21,437.23,10.89;7,88.80,88.49,312.41,10.89" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<ptr target="http://citeseer.nj.nec.com/jones98probabilistic.html" />
		<title level="m" coord="7,280.08,71.21,236.15,10.89;7,88.80,88.49,39.01,10.89">A probabilistic model of information retrieval: development and status</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
