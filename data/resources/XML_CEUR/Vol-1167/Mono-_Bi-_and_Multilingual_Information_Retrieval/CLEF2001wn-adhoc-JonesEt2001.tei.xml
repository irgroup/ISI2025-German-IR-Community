<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,84.76,112.86,424.49,15.15;1,151.30,134.78,291.40,15.15">Exeter at CLEF 2001: Experiments with Machine Translation for Bilingual Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,159.89,167.31,95.90,10.48"><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Exeter</orgName>
								<address>
									<postCode>EX4 4PT</postCode>
									<settlement>Exeter</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,299.60,167.31,130.61,10.48"><forename type="first">Adenike</forename><forename type="middle">M</forename><surname>Lam-Adesina</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Exeter</orgName>
								<address>
									<postCode>EX4 4PT</postCode>
									<settlement>Exeter</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,84.76,112.86,424.49,15.15;1,151.30,134.78,291.40,15.15">Exeter at CLEF 2001: Experiments with Machine Translation for Bilingual Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AC389D7EC92441DB02CFCD6B8EB747DA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The University of Exeter participated in the CLEF 2001 bilingual task. The main objectives of our experiments were to compare retrieval performance for different topic languages with similar easily available machine translation resources and to explore the application of new pseudo relevance feedback techniques recently developed at Exeter to Cross-Language Information Retrieval (CLIR). We also report recent experimental results from our investigations of the combination of results from alternative machine translation outputs; specifically we look at the use of data fusion of the output from individual retrieval runs and merging of alternative topic translations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.273" lry="841.887"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The CLEF 2001 bilingual task is the first standardised evaluation to enable comparison of Cross-Language Information Retrieval (CLIR) behaviour for European and Asian language pairings. The primary objective of our participation in the bilingual task was to explore retrieval behaviour with the English language document collection with an many of the translated topic sets as possible. Our official submissions covered French, German, Chinese and Japanese topics. In this paper we also report results for English, Italian and Spanish topics for comparison. In order to compare results for different language pairings fairly we wished to use similar translation resources for each pair. To this end we chose to use commercially developed machine translation (MT) resources. For our official submissions we used the online babelfish translation system (available at http://babelfish.altavista.com based on SYSTRAN . In this paper we report comparative results for French, German, Italian and Spanish using Globalink Power Translator Pro Version: 6.4 . There is an underlying assumption with this approach that the translation resources for each language pair have been subject to an amount of development which makes such a comparison fair. In addition, we report more recent results combining the outputs of these MT resources using data fusion and query combination. Our general approach was to use a topic translation strategy for CLIR. Topic statements were submitted to the selected MT system, the output collected and then applied to the information retrieval system.</p><p>Pseudo-relevance feedback has been shown to be effective in many retrieval applications including CLIR <ref type="bibr" coords="1,98.94,606.13,10.52,8.74" target="#b0">[1]</ref>  <ref type="bibr" coords="1,112.05,606.13,9.97,8.74" target="#b1">[2]</ref>. We have recently conducted experimental work with the Okapi BM25 probabilistic retrieval model and a new pseudo relevance feedback query-expansion method using document summaries <ref type="bibr" coords="1,508.72,618.08,9.97,8.74" target="#b2">[3]</ref>. This work also investigated a novel approach to term-selection that separates the choice of relevant documents from the selection of a pool of potential expansion terms. These techniques were shown to be considerably more effective than using full-document expansion on the TREC-8 ad hoc retrieval task. The result was an improvement of around 15% in average precision on short topic statements compared to a baseline without feedback. Our CLEF 2001 submission investigated the application of this technique to CLIR.</p><p>The remainder of this paper is organised as follows: Section 2 reviews the information retrieval methods used, Section 3 outlines the features of our summarisation system, Section 4 describes our combination methods, Section 5 gives experimental results, and Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Information Retrieval Approach</head><p>The experiments were carried out using the City University research distribution version of the Okapi system. The documents and search topics were processed to remove stop words from a list of around 260 words, suffix stripped using Porter stemming <ref type="bibr" coords="2,291.95,121.75,10.52,8.74" target="#b3">[4]</ref> and terms were further indexed using a small set of synonyms. Document terms are weighted using the Okapi combined weight (cw ), often known as BM25, originally developed in g <ref type="bibr" coords="2,133.71,157.62,11.63,8.74" target="#b4">[5]</ref> and further elaborated in <ref type="bibr" coords="2,262.20,157.62,9.97,8.74" target="#b5">[6]</ref>. The BM25 cw for a term is calculated as follows,</p><formula xml:id="formula_0" coords="2,179.00,178.54,214.87,22.87">cw(i, j) = cf w(i) × tf (i, j) × (K1 + 1) K1 × ((1 -b) + (b × ndl(j))) + tf (i, j)</formula><p>where cw(i, j) represents the weight of term i in document j, cf w(i) is the standard collection frequency (inverse document frequency) weight, tf (i, j) is the document term frequency, and ndl(j) is the normalized document length. ndl(j) is calculated as, ndl(j) = dl(j) Average dl for all documents ,</p><p>where dl(j) is the length of j. K1 and b are empirically selected tuning constants for a particular collection. K1 is designed to modify the degree of effect of tf (i, j), while constant b modifies the effect of document length. High values of b imply that documents are long because they are verbose, while low values imply that they are long because they are multitopic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Relevance Feedback</head><p>Assuming that relevant documents are available within a collection, the main reason that they may not be retrieved is the query-document match problem. Short and imprecise often results in relevant documents being retrieved at low rank or not being retrieved at all, and retrieval of non-relevant documents at high rank. Relevance feedback (RF) using query expansion is one method which seeks to overcome the query-document match problem. Pseudo-relevance feedback (PRF) methods in which a number of topic ranked documents are assumed to be relevant are on average found to give improvement in retrieval performance; although this is usually smaller than that observed for true RF. Post-translation PRF has been shown to be effective in CLIR in various studies including <ref type="bibr" coords="2,351.48,457.64,10.52,8.74" target="#b1">[2]</ref> [1].</p><p>The main implementational issue for PRF is the selection of appropriate expansion terms. In PRF problems can arise when terms taken from assumed relevant documents that are actually non-relevant, are added to the query causing a drift in the focus of the query. If the initial retrieval results are good and a large proportion of the documents retrieved at high rank are relevant, feedback is likely to improve retrieval performance. A further problem can arise since many documents are multi-topic, i.e. they deal with several different topics. This means that only a portion of a document retrieved in response to a given query may actually be relevant. Nevertheless standard RF treats the whole document as relevant, the implication of this being that using terms from non-relevant sections of these documents for expansion may also cause query drift. The exclusion of terms from non-relevant sections of documents, or those present in non-relevant documents which are not closely related to the concepts expressed in the initial query, could thus be beneficial to PRF and potentially in true RF as well.</p><p>These issues have led to several attempts to develop automatic systems that can concentrate user's attention on the parts of the text that possess a high density of relevant information. This method known as passage retrieval <ref type="bibr" coords="2,159.47,625.01,10.52,8.74" target="#b6">[7]</ref> [8] has the advantage of being able to provide an overview of the distribution of the relevant pieces of information within the retrieved documents. However, this method has not been found to provide significant improvement in retrieval performance. We have developed a novel approach to the exclusion of terms from consideration based on document summarization. In this method only terms present in the summarized documents are considered for query expansion. Earlier experiments <ref type="bibr" coords="2,72.00,684.78,10.52,8.74" target="#b8">[9]</ref> [10] demonstrated that selecting best passages from documents for query expansion is very effective in reducing the number of inappropriate possible feedback terms taken from multi-topic or non-relevant document. In <ref type="bibr" coords="2,136.20,708.69,15.50,8.74" target="#b10">[11]</ref> Tombros showed that query-biased summaries are more effective than using simple leading sentence summaries for user relevance decisions. Thus in our summaries we also make use of query-biased summaries. A related approach to the one reported here is described by Strzalkowski in <ref type="bibr" coords="2,72.00,744.56,15.50,8.74">[12]</ref> where a RF procedure using summaries of retrieved relevant documents is used. A weakness of this approach is that all terms from the summaries were added to the query. We prefer to adopt the approach taken in the Okapi TREC submissions <ref type="bibr" coords="3,284.00,87.97,15.50,8.74" target="#b11">[13]</ref> [14] <ref type="bibr" coords="3,321.03,87.97,15.50,8.74" target="#b13">[15]</ref> which expand queries conservatively using only a small number of terms chosen using a statistical selection criteria <ref type="bibr" coords="3,390.20,99.93,14.62,8.74" target="#b14">[16]</ref>.</p><p>The expansion terms were ranked using the Robertson selection value (rsv) <ref type="bibr" coords="3,420.35,111.89,14.62,8.74" target="#b14">[16]</ref>, defined as,</p><formula xml:id="formula_1" coords="3,241.75,133.11,90.58,9.30">rsv(i) = r(i) × rw(i)</formula><p>where r(i) is again the number of relevant documents containing term i, and rw(i) is the standard Robertson/Sparck Jones relevance weight <ref type="bibr" coords="3,256.23,166.29,14.62,8.74" target="#b15">[17]</ref>. rw(i) is defined as,</p><formula xml:id="formula_2" coords="3,176.44,186.51,220.00,22.87">rw(i) = log (r(i) + 0.5)(N -n(i) -R + r(i) + 0.5) (n(i) -r(i) + 0.5)(R -r(i) + 0.5)</formula><p>where n(i) is the total number of documents containing term i, R is the total number of relevant documents for this query, and N is the total number of documents. The rsv has generally been based on taking an equal number of relevant documents for both the available expansion terms and term ranking. In our experiments we have explored the use of a more sophisticated approach which takes a smaller number of relevant documents to determine the pool of potential expansion terms than the number of documents used to determine the rsv ranking. It should be noted that the r(i) value for term each i is calculated based on its occurrence in the entire document rather than on the summary alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Summary Generation</head><p>Summary generation methods seek to identify document contents that convey the most "important" information within the document, where importance may depend on the use to which the summary is to be put. Since we require a very robust summarizer for the different text types likely to be encountered within a retrieval system we adopt a summarisation method based on sentence extraction. Sentence extracted summaries are formed by scoring the sentences in the document using various criteria, ranking the sentences, and then taking a number of the top ranking sentences as the summary.</p><p>Each sentence score is computed as the sum of its constituent words and other scores. The following section describes the summary generation methods used in this investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Luhn's Keyword Cluster Method</head><p>The first component of our summaries uses Luhn's classic cluster measure <ref type="bibr" coords="3,402.79,488.49,14.62,8.74" target="#b16">[18]</ref>. In order to determine the sentences of a document that should be used as the summary, a measure is required by which the information content of all the sentences can be analysed and graded. Luhn concluded that the frequency of a word occurrence in an article, as well as its relative position determines its significance in that article. Following the work of Tombros <ref type="bibr" coords="3,252.59,536.31,14.62,8.74" target="#b10">[11]</ref>, which studied summarization of TREC documents, the required minimum occurrence count for significant terms in a medium-sized TREC document was taken to be 7; where a medium sized document is defined as one containing no more than 40 sentences and not less than 25 sentences. For documents outside this range, the limit for significance is computed as,</p><formula xml:id="formula_3" coords="3,233.63,593.40,106.82,9.30">ms = 7 + (0.1(L -N S))</formula><p>for documents with NS &lt; 25, and</p><formula xml:id="formula_4" coords="3,233.63,635.85,106.82,9.30">ms = 7 + (0.1(N S -L))</formula><p>for documents with NS &gt; 40 where ms = the measure of significance L = Limit (25 for NS &lt; 25 and 40 for NS &gt; 40) N S = number of sentences in the document In order to score sentences based on the number of significant words contained in them, Luhn reasoned that whatever the topic under consideration the closer certain words are, the more specifically an aspect of the subject is being treated. Hence, wherever clusters of significant words are found, the probability is very high that the information being conveyed is most representative of the article. Luhn specified that two significant words are considered significantly related if they are separated by not more than five insignificant words. Thus, a cluster of significant words is created whereby significant words are separated by not more than five non-significant words as illustrated below.</p><p>"The sentence [scoring process utilises information both from the structural] organization."</p><p>The cluster of significant words is given by the words in the brackets ([-]), where significant words are shown in bold. The cluster significance score factor for a sentence is given by the following formula</p><formula xml:id="formula_5" coords="4,259.05,143.27,54.27,23.89">SS1 = SW 2 T W</formula><p>where SS1 = the sentence score SW = the number of bracketed significant words (in this case 3) T W = the total number of bracketed words (in this case 8) Thus SS1 for the above sentence is 1.125. If two or more clusters of significant words appear in a given sentence, the one with the highest score is chosen as the sentence score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Title Terms Frequency Method</head><p>The title of an article often reveals the major subject of that document. In a sample study the title of TREC documents was found to convey the general idea of its contents. Thus, a factor in the sentence score is the presence of title words within the sentence. Each constituent term in the title section is looked up in the body of the text. For each sentence a title score is computed as follows,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SS2 = T T S T T T</head><p>where SS2 = the title score for a sentence T T S = the total number of title terms found in a sentence T T T = the total number of terms in a title T T T is used as a normalization factor to ensure that this method does not have an excessive sentence score factor contribution relative to the overall sentence score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Location/Header Method</head><p>Edmundson <ref type="bibr" coords="4,126.72,453.51,15.50,8.74" target="#b17">[19]</ref> noted that the position of a sentence within a document is often useful in determining its importance to the document. Based on this, Edmundson defined a location method for scoring each sentence based on whether it occurs at the beginning or end of a paragraph or document.</p><p>To determine the effect of this sentence scoring method on the test collection a further sample study was conducted. This confirmed that the first sentences of a TREC document often provide important information about the content of the document. Thus the first two sentences of an article are assigned a location score computed as follows,</p><formula xml:id="formula_6" coords="4,262.14,544.55,48.02,22.31">SS3 = 1 N S</formula><p>where SS3 = the location score for a sentence N S = the number of sentences in the document Furthermore, section headings within the documents were found to provide information about the different sections discussed in the documents. Thus, marked section headings were given a similar location score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Query-Bias Method</head><p>The addition of a sentence score factor bias to score sentences containing query terms more highly can reduce the query drift caused by the use of bad feedback terms. Thus, whether a relevant or nonrelevant document is used, the feedback terms are taken from the most relevant section identified in the document, in relation to the submitted query. In order to generate a query biased summary in this work, each constituent sentence of a document being processed is scored based on the number of query terms it contains. The following situation gives an example of this method. For a query "falkland petroleum exploration" and a sentence "The british minister has decided to continue the ongoing petroleum exploration talks in the falkland area", the query score SS4 is computed as follows, SS4 = tq 2 nq where tq = the number of query terms present in a sentence nq = the number of terms in a query Therefore the query score SS4 for the above sentence is 3. This score is assigned based on the belief that the number of query terms contained in a sentence, the more likely it is that this sentence conveys a large amount of information related to the query. This is the same method used in <ref type="bibr" coords="5,445.10,182.11,14.62,8.74" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Combining the Scores</head><p>The previous sections outlined the components used in scoring sentences to generate the summaries used in this work. The final score for each sentence is calculated by summing the individual score factors obtained for each method used. Thus the final score for each sentence is</p><formula xml:id="formula_7" coords="5,215.41,268.89,143.25,8.74">SSS = SS1 + SS2 + SS3 + SS4</formula><p>where SSS = Sentence Significance Score.</p><p>The summarisation system was implemented so that the relative weight of each component of SSS could be varied. In order to generate an appropriate summary it is essential to place a limit on the number of sentences to be used as the summary content. To do this however it is important to take into consideration the length of the original document and the amount of information that is needed. The objective of the summary generation system is to provide terms to be used for query expansion, and not to act as a stand alone summary that can be used to replace the entire documents. Hence the optimal summary length is a compromise between maintaining terms that can be beneficial to the retrieval process, while ensuring that the length is such that non relevant terms are kept to the barest minimum if they cannot be removed totally.</p><p>Experiments were performed with various maximum summary lengths to find the best one for termselection. The lower limit of the summary length was set at 15document collection also consisted of very short documents. Thus high ranked sentences up to the maximum summary length and not less than the set minimum summary length are presented as the summary content for each document summarized. Inspection of our example summaries showed them to be reasonable representations of the original documents. However, in our case an objective measure of summary quality is their overall effect on retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Combination Methods</head><p>The combination of evidence from multiple information sources has been-shown to be useful for text retrieval in TREC <ref type="bibr" coords="5,154.47,543.50,14.62,8.74" target="#b18">[20]</ref>. In our experiments we examine two forms of index combination defined in <ref type="bibr" coords="5,503.74,543.50,14.62,8.74" target="#b18">[20]</ref>: data fusion and query combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.0.1">Data Fusion</head><p>For data fusion the ranked document lists produced independently by topics translated using Babelfish and Power Translator Pro were combined by adding the corresponding query-document matching scores from the two lists and forming a new re-ranked list using the composite scores. We investigated both simple summing of the matching scores and summation of after the scores had separately been normalised with respect to the highest score in each list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.0.2">Query Combination</head><p>One of the important issues in query translation for CLIR is the choice of the best translation(s) of the search terms. The output of an individual MT system gives the best overall translation available for the input given its rules and dictionary. Thus different MT systems often given different translated outputs. In query combination the translated queries produced by the two MT systems were combined into a single representation to score against document archive. A set of combined queries was formed by taking the unique items from the existing query sets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>This section describes the establishment of the parameters of our experimental system and gives results from our CLEF 2001 investigation. We report procedures for the selection of system parameters, baseline retrieval results for different language pairs and translation systems without application of feedback, corresponding results with use of feedback, and results for our data combination experiments. In all cases the results use mandatory Title and Description fields from the search topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Selection of System Parameters</head><p>Various parameters had to be selected for our experimental system. In order to do this with carried out a series of development runs using the CLEF 2000 bilingual test collection. This data consisted of the English document set and topic sets in French, German, Italian and Spanish. The Okapi parameters were set as follows: K1 = 1.0 and b = 0.5. In the pseudo relevance feedback runs 5 documents were assumed to be relevant in each case for term selection, document summaries comprised the best scoring 4 sentences in each case. Following experimentation with the sentence scoring components it was found that the best retrieval results were achieved when the Luhn and Title where given twice the relative weight compared to the Location and Query-Bias methods. The top 20 ranked expansion terms taken from these summaries were added to the original topic in each case. The rsv values to rank the potential expansion terms were selected by assuming the top 20 ranked documents were relevant. The original topic terms are upweighted by a factor of 3.5 relative to terms introduced by pseudo relevance feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baseline Results</head><p>Tables <ref type="table" coords="6,103.00,660.41,4.98,8.74" target="#tab_0">1</ref> and<ref type="table" coords="6,130.03,660.41,4.98,8.74" target="#tab_1">2</ref> show baseline retrieval results for topic translation using Babelfish and Power Translator Pro respectively. From these tables can be seen that the two MT systems give comparable performance for the European languages, although the result for French with Babelfish is particularly good. The worst overall result is achieved for the Chinese topics, although performance for Japanese is similar to that for the European languages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Feedback Results</head><p>Tables <ref type="table" coords="7,104.79,429.84,4.98,8.74" target="#tab_2">3</ref> and<ref type="table" coords="7,135.37,429.84,4.98,8.74" target="#tab_3">4</ref> show retrieval results after the application of our summary based pseudo relevance feedback method. The results for French, German, Chinese and Japanese in Table <ref type="table" coords="7,448.23,441.79,4.98,8.74" target="#tab_2">3</ref> are our official submissions for the CLEF 2001 bilingual task. It can be seen that in all but case feedback improves the average precision. The one exception is for Chinese where performance is marginally worse after feedback. The reasons for this decrease in average precision have not yet been investigated. The average improvement in performance is around 5%. This is somewhat less than the 15% improvement that we observed in our previous experiments with the TREC-8 ad hoc task. Further investigation is needed to establish the reasons for this. One reason may be that in our previous work we worked only with the topic Title fields, meaning that there is often significant room for improvement in retrieval performance for individual topics by adding additional terms to the request. In the case of the CLEF runs here we are using both the Title and Description fields meaning that there may be less room for improvement from adding additional terms. Further experimental will be carried out to explore this possibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Combination Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Data Fusion</head><p>Baseline Results Tables <ref type="table" coords="7,196.14,626.01,4.98,8.74" target="#tab_4">5</ref> and<ref type="table" coords="7,224.46,626.01,4.98,8.74" target="#tab_5">6</ref> show baseline results for Data Fusion with simple and normalised score addition respectively. Results using normalised scores are worse than the simple addition method, which overall generally gives a small improvement in performance compared to either translation on in isolation. The result for German is particularly good, giving the same value as the English baseline. This result is unusually good for CLIR, but appears from investigation to be correct.</p><p>Feedback Results Tables <ref type="table" coords="7,198.21,699.73,4.98,8.74" target="#tab_6">7</ref> and<ref type="table" coords="7,224.33,699.73,4.98,8.74" target="#tab_7">8</ref> show results for Data Fusion with summary based feedback applied. Feedback again generally results in an improvement in average precision, except for the case of German, where as noted previous the baseline Data Fusion was unusually good. Simple score addition is still superior to addition of normalised scores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Query Combination</head><p>Tables 9 and 10 show baseline and feedback retrieval results respectively for Query Combination. It can be seen that the performance of Query Combination compared to the individual translation and the Data Fusion methods is rather unpredictable. Based on the results here Data Fusion appears to be a better strategy for combining information from multiple topic translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Concluding Remarks and Further Work</head><p>This paper has presented our results for the CLEF 2001 bilingual English language retrieval task. The results indicate that similar retrieval results are achieved using different commercial machine translation systems, but that some improvement can often be gained from applied Data Fusion methods to the output from the retrieval systems for different topic translations. Results for six different query-document pairs indicate that similar performance can be achieved for CLIR for Asian and European language topics for retrieval of English document despite the greater difference between the languages in the former case. However, the result for Chinese topics is the worst, and further investigation is required to better understand the reason for this. In addition, we intend to do query by query analysis of retrieval performance across the different languages pairs to investigate the effect of individual translation effects on retrieval behaviour. The application of our summary based pseudo relevance feedback method was generally shown to be effective, although the improvement was generally less than hoped for. This result will also be the subject of further investigation. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,104.51,74.82,384.98,236.97"><head>Table 1 :</head><label>1</label><figDesc>Baseline retrieval results for topic translation using Babelfish.</figDesc><table coords="6,104.51,74.82,384.98,236.97"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Topic Language</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="7">English French German Italian Spanish Chinese Japanese</cell></row><row><cell>Prec.</cell><cell>5 docs</cell><cell>0.494</cell><cell>0.477</cell><cell>0.392</cell><cell>0.383</cell><cell>0.417</cell><cell>0.336</cell><cell>0.434</cell></row><row><cell></cell><cell>10 docs</cell><cell>0.406</cell><cell>0.366</cell><cell>0.330</cell><cell>0.298</cell><cell>0.353</cell><cell>0.287</cell><cell>0.332</cell></row><row><cell></cell><cell>15 docs</cell><cell>0.353</cell><cell>0.326</cell><cell>0.288</cell><cell>0.257</cell><cell>0.312</cell><cell>0.253</cell><cell>0.268</cell></row><row><cell></cell><cell>20 docs</cell><cell>0.317</cell><cell>0.289</cell><cell>0.263</cell><cell>0.231</cell><cell>0.284</cell><cell>0.231</cell><cell>0.245</cell></row><row><cell cols="2">Av Precision</cell><cell>0.484</cell><cell>0.473</cell><cell>0.398</cell><cell>0.375</cell><cell>0.389</cell><cell>0.341</cell><cell>0.411</cell></row><row><cell cols="2">% change CLIR</cell><cell>-</cell><cell>-2.3%</cell><cell cols="3">-17.8% -22.5% -19.6%</cell><cell>-29.5%</cell><cell>-15.1%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Topic Language</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">English French German Italian Spanish</cell><cell></cell></row><row><cell></cell><cell>Prec.</cell><cell>5 docs</cell><cell>0.494</cell><cell>0.438</cell><cell>0.438</cell><cell>0.472</cell><cell>0.464</cell><cell></cell></row><row><cell></cell><cell></cell><cell>10 docs</cell><cell>0.406</cell><cell>0.368</cell><cell>0.349</cell><cell>0.364</cell><cell>0.383</cell><cell></cell></row><row><cell></cell><cell></cell><cell>15 docs</cell><cell>0.353</cell><cell>0.332</cell><cell>0.302</cell><cell>0.321</cell><cell>0.340</cell><cell></cell></row><row><cell></cell><cell></cell><cell>20 docs</cell><cell>0.317</cell><cell>0.296</cell><cell>0.271</cell><cell>0.288</cell><cell>0.303</cell><cell></cell></row><row><cell></cell><cell cols="2">Av Precision</cell><cell>0.484</cell><cell>0.438</cell><cell>0.439</cell><cell>0.427</cell><cell>0.417</cell><cell></cell></row><row><cell></cell><cell cols="2">% change CLIR</cell><cell>-</cell><cell>-9.5%</cell><cell>-9.3%</cell><cell cols="2">-11.8% -13.8%</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,115.51,334.88,362.98,8.74"><head>Table 2 :</head><label>2</label><figDesc>Baseline retrieval results for topic translation using Power Translator Pro.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,72.00,74.82,450.00,271.69"><head>Table 3 :</head><label>3</label><figDesc>Topic Language English French * German * Italian Spanish Chinese * Japanese * Retrieval results for topic translation using Babelfish with summary-based expansion term selection. * indicates official CLEF 2001 submitted run.</figDesc><table coords="7,81.81,99.13,420.35,247.39"><row><cell>Prec.</cell><cell>5 docs</cell><cell cols="2">0.498</cell><cell cols="2">0.477</cell><cell>0.421</cell><cell>0.400</cell><cell>0.477</cell><cell>0.357</cell><cell>0.426</cell></row><row><cell></cell><cell>10 docs</cell><cell cols="2">0.400</cell><cell cols="2">0.366</cell><cell>0.336</cell><cell>0.320</cell><cell>0.394</cell><cell>0.296</cell><cell>0.362</cell></row><row><cell></cell><cell>15 docs</cell><cell cols="2">0.362</cell><cell cols="2">0.326</cell><cell>0.301</cell><cell>0.286</cell><cell>0.342</cell><cell>0.258</cell><cell>0.305</cell></row><row><cell></cell><cell>20 docs</cell><cell cols="2">0.329</cell><cell cols="2">0.289</cell><cell>0.275</cell><cell>0.252</cell><cell>0.299</cell><cell>0.224</cell><cell>0.266</cell></row><row><cell cols="2">Av Precision</cell><cell cols="2">0.517</cell><cell cols="2">0.489</cell><cell>0.415</cell><cell>0.395</cell><cell>0.423</cell><cell>0.336</cell><cell>0.431</cell></row><row><cell cols="4">% change no FB. +6.8%</cell><cell cols="2">+3.4%</cell><cell>+4.3%</cell><cell cols="2">+5.3% +8.7%</cell><cell>-1.5%</cell><cell>+4.9%</cell></row><row><cell cols="2">% change CLIR</cell><cell>-</cell><cell></cell><cell cols="2">-5.4%</cell><cell>-19.7%</cell><cell cols="2">-23.6% -18.1%</cell><cell>-35.0%</cell><cell>-16.6%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Topic Language</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">English French German Italian Spanish</cell></row><row><cell></cell><cell cols="2">Prec.</cell><cell cols="2">5 docs</cell><cell>0.498</cell><cell>0.464</cell><cell>0.472</cell><cell>0.481</cell><cell>0.481</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">10 docs</cell><cell>0.400</cell><cell>0.402</cell><cell>0.381</cell><cell>0.396</cell><cell>0.411</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">15 docs</cell><cell>0.362</cell><cell>0.346</cell><cell>0.318</cell><cell>0.233</cell><cell>0.355</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">20 docs</cell><cell>0.329</cell><cell>0.316</cell><cell>0.284</cell><cell>0.295</cell><cell>0.313</cell></row><row><cell></cell><cell></cell><cell cols="2">Av Precision</cell><cell></cell><cell>0.517</cell><cell>0.466</cell><cell>0.456</cell><cell>0.432</cell><cell>0.419</cell></row><row><cell></cell><cell cols="6">% change no FB +6.8% +6.4%</cell><cell>+3.9%</cell><cell cols="2">+1.2% +0.5%</cell></row><row><cell></cell><cell cols="4">% change CLIR</cell><cell>-</cell><cell>-9.9%</cell><cell cols="3">-11.8% -16.4% -18.9%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,72.00,369.61,450.00,20.69"><head>Table 4 :</head><label>4</label><figDesc>Retrieval results for topic translation using Power Translator Pro with summary-based expansion term selection.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,153.95,74.82,286.12,235.03"><head>Table 5 :</head><label>5</label><figDesc>Baseline retrieval results for Data Fusion.</figDesc><table coords="8,153.95,74.82,286.12,235.03"><row><cell></cell><cell></cell><cell></cell><cell cols="3">Topic Language</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">English French German Italian Spanish</cell></row><row><cell>Prec.</cell><cell>5 docs</cell><cell>0.494</cell><cell>0.477</cell><cell>0.494</cell><cell>0.485</cell><cell>0.464</cell></row><row><cell></cell><cell>10 docs</cell><cell>0.406</cell><cell>0.385</cell><cell>0.394</cell><cell>0.387</cell><cell>0.385</cell></row><row><cell></cell><cell>15 docs</cell><cell>0.353</cell><cell>0.342</cell><cell>0.352</cell><cell>0.333</cell><cell>0.342</cell></row><row><cell></cell><cell>20 docs</cell><cell>0.317</cell><cell>0.303</cell><cell>0.305</cell><cell>0.296</cell><cell>0.301</cell></row><row><cell cols="2">Av Precision</cell><cell>0.484</cell><cell>0.479</cell><cell>0.484</cell><cell>0.426</cell><cell>0.423</cell></row><row><cell cols="2">% change</cell><cell>-</cell><cell>-1.0%</cell><cell>-0.0%</cell><cell cols="2">-12.0% -12.6%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Topic Language</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">English French German Italian Spanish</cell></row><row><cell>Prec.</cell><cell>5 docs</cell><cell>0.494</cell><cell>0.477</cell><cell>0.481</cell><cell>0.481</cell><cell>0.451</cell></row><row><cell></cell><cell>10 docs</cell><cell>0.406</cell><cell>0.379</cell><cell>0.379</cell><cell>0.377</cell><cell>0.368</cell></row><row><cell></cell><cell>15 docs</cell><cell>0.353</cell><cell>0.333</cell><cell>0.326</cell><cell>0.322</cell><cell>0.326</cell></row><row><cell></cell><cell>20 docs</cell><cell>0.317</cell><cell>0.296</cell><cell>0.287</cell><cell>0.283</cell><cell>0.289</cell></row><row><cell cols="2">Av Precision</cell><cell>0.484</cell><cell>0.463</cell><cell>0.467</cell><cell>0.417</cell><cell>0.420</cell></row><row><cell cols="2">% change</cell><cell>-</cell><cell>-4.3%</cell><cell>-3.5%</cell><cell cols="2">-13.8% -13.2%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="8,131.47,332.94,331.08,8.74"><head>Table 6 :</head><label>6</label><figDesc>Baseline retrieval results for Data Fusion with score normalisation.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,102.26,74.82,389.48,236.97"><head>Table 7 :</head><label>7</label><figDesc>Retrieval results for Data Fusion with summary-based expansion term selection.</figDesc><table coords="9,153.95,74.82,286.12,236.97"><row><cell></cell><cell></cell><cell></cell><cell cols="3">Topic Language</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">English French German Italian Spanish</cell></row><row><cell>Prec.</cell><cell>5 docs</cell><cell>0.498</cell><cell>0.489</cell><cell>0.502</cell><cell>0.481</cell><cell>0.506</cell></row><row><cell></cell><cell>10 docs</cell><cell>0.400</cell><cell>0.415</cell><cell>0.396</cell><cell>0.400</cell><cell>0.413</cell></row><row><cell></cell><cell>15 docs</cell><cell>0.362</cell><cell>0.352</cell><cell>0.338</cell><cell>0.335</cell><cell>0.345</cell></row><row><cell></cell><cell>20 docs</cell><cell>0.329</cell><cell>0.322</cell><cell>0.313</cell><cell>0.305</cell><cell>0.303</cell></row><row><cell cols="2">Av Precision</cell><cell>0.517</cell><cell>0.489</cell><cell>0.476</cell><cell>0.451</cell><cell>0.426</cell></row><row><cell cols="2">% change</cell><cell>-</cell><cell>-5.4%</cell><cell>-7.9%</cell><cell cols="2">-12.8% -17.6%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Topic Language</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">English French German Italian Spanish</cell></row><row><cell>Prec.</cell><cell>5 docs</cell><cell>0.498</cell><cell>0.489</cell><cell>0.494</cell><cell>0.472</cell><cell>0.485</cell></row><row><cell></cell><cell>10 docs</cell><cell>0.400</cell><cell>0.411</cell><cell>0.377</cell><cell>0.389</cell><cell>0.400</cell></row><row><cell></cell><cell>15 docs</cell><cell>0.362</cell><cell>0.343</cell><cell>0.316</cell><cell>0.329</cell><cell>0.338</cell></row><row><cell></cell><cell>20 docs</cell><cell>0.329</cell><cell>0.317</cell><cell>0.292</cell><cell>0.292</cell><cell>0.297</cell></row><row><cell cols="2">Av Precision</cell><cell>0.517</cell><cell>0.489</cell><cell>0.463</cell><cell>0.436</cell><cell>0.426</cell></row><row><cell cols="2">% change</cell><cell>-</cell><cell>-5.4%</cell><cell cols="3">-10.4% -15.7% -17.6%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,72.00,334.88,450.00,20.69"><head>Table 8 :</head><label>8</label><figDesc>Retrieval results for Data Fusion with score normalisation with summary-based expansion term selection.</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,92.47,658.13,429.52,8.74;8,92.48,670.08,429.52,8.74;8,92.48,682.04,429.52,8.74;8,92.48,693.99,230.83,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,387.67,658.13,134.33,8.74;8,92.48,670.08,328.89,8.74">A Comparison of Query Translation Methods for English-Japanese Cross-Language Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">H</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kumano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,443.56,670.08,78.44,8.74;8,92.48,682.04,429.52,8.74;8,92.48,693.99,37.33,8.74">Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>San Fransisco</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="269" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,92.47,713.92,429.51,8.74;8,92.48,725.87,429.53,8.74;8,92.48,737.83,429.55,8.74;8,92.48,749.78,52.31,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,236.89,713.92,285.10,8.74;8,92.48,725.87,140.27,8.74">Phrasal Translation and Query Expansion Techniques for Cross-Language Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,259.21,725.87,262.80,8.74;8,92.48,737.83,301.22,8.74">Proceedings of the 20th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 20th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="84" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.47,376.72,429.52,8.74;9,92.48,388.68,429.52,8.74;9,92.48,400.63,361.10,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,274.37,376.72,247.63,8.74;9,92.48,388.68,97.55,8.74">Applying Summarization Techniques for Term Selection in Relevance Feedback</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Lam-Adesina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,213.37,388.68,308.63,8.74;9,92.48,400.63,239.53,8.74">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.47,420.56,330.96,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,155.06,420.56,140.79,8.74">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,304.41,420.56,34.40,8.74">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.47,440.49,429.52,8.74;9,92.48,452.44,429.52,8.74;9,92.48,464.40,429.52,8.74;9,92.48,476.35,26.29,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,245.26,440.49,276.74,8.74;9,92.48,452.44,149.92,8.74">Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,264.96,452.44,257.04,8.74;9,92.48,464.40,293.56,8.74">Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Dublin</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.47,496.28,429.52,8.74;9,92.48,508.23,429.53,8.74;9,92.48,520.19,52.58,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,423.93,496.28,76.64,8.74">Okapi at TREC-4</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Payne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,192.43,508.23,266.29,8.74">Overview of the Fourth Text REtrieval Conference (TREC-4)</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="73" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.47,540.11,429.53,8.74;9,92.48,552.07,429.52,8.74;9,92.48,564.02,156.28,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,154.65,540.11,205.90,8.74">Passage-Level Evidence in Document Retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,384.43,540.11,137.57,8.74;9,92.48,552.07,425.37,8.74">Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Dublin</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="302" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.47,583.95,429.52,8.74;9,92.48,595.90,429.52,8.74;9,92.48,607.86,171.89,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,336.14,583.95,185.86,8.74;9,92.48,595.90,177.80,8.74">Highlighting Relevant Passages for users of the Interactive SPIDER Retrieval System</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Knaus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mittendorf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schauble</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sheridan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,290.79,595.90,231.21,8.74;9,92.48,607.86,42.54,8.74">Proceedings of the Fourth Text REetrieval Conference (TREC-4)</title>
		<meeting>the Fourth Text REetrieval Conference (TREC-4)</meeting>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="233" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.47,627.78,429.53,8.74;9,92.48,639.74,429.52,8.74;9,92.48,651.69,252.23,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="9,205.03,627.78,273.64,8.74">Query Expansion Using Local and Global Document Analysis</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,502.89,627.78,19.11,8.74;9,92.48,639.74,429.52,8.74;9,92.48,651.69,105.20,8.74">Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Zurich</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="4" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.47,671.62,429.53,8.74;9,92.48,683.57,429.52,8.74;9,92.48,695.53,88.28,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="9,133.31,671.62,174.50,8.74">Relevance Feedback with too much Data</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,328.44,671.62,193.57,8.74;9,92.48,683.57,355.63,8.74">Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="337" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,92.47,715.45,429.51,8.74;9,92.48,727.41,429.52,8.74;9,92.48,739.36,329.98,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="9,241.63,715.45,280.36,8.74;9,92.48,727.41,26.42,8.74">The advantages of query-biased summaries in Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Tombros</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,140.32,727.41,381.68,8.74;9,92.48,739.36,164.41,8.74">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="2" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.47,398.59,429.52,8.74;10,92.48,410.54,429.52,8.74;10,92.48,422.50,93.54,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,448.35,398.59,73.65,8.74;10,92.48,410.54,3.88,8.74">Okapi at TREC-3</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Hancok-Beaulieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,225.10,410.54,263.12,8.74">Overview of the Third Text REtrieval Conference (TREC-3</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.47,442.42,429.54,8.74;10,92.48,454.38,429.52,8.74;10,92.48,466.33,268.41,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,315.11,442.42,206.91,8.74;10,92.48,454.38,103.85,8.74">Okapi at TREC-7: automatic ad hoc, filtering, vls and interactive track</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Beaulieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,394.50,454.38,127.51,8.74;10,92.48,466.33,139.05,8.74">Overview of the Seventh Text REtrieval Conference (TREC-7)</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="253" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.47,486.26,429.53,8.74;10,92.48,498.21,394.74,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,243.66,486.26,67.84,8.74">Okapi/Keenbow</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,92.48,498.21,265.38,8.74">Overview of the Eighth Text REtrieval Conference (TREC-8)</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<publisher>NIST</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="151" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.47,518.14,429.53,8.74;10,92.48,530.09,22.70,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,171.55,518.14,167.62,8.74">On term selection for query expansion</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,349.71,518.14,113.05,8.74">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="359" to="364" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.47,550.02,429.52,8.74;10,92.48,561.97,232.97,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,256.71,550.02,152.61,8.74">Relevance weighting of search terms</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,417.12,550.02,104.87,8.74;10,92.48,561.97,136.17,8.74">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="129" to="146" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.47,581.90,429.52,8.74;10,92.48,593.85,145.87,8.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,155.93,581.90,217.68,8.74">The Automatic Creation of Literature Abstracts</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">P</forename><surname>Luhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,387.18,581.90,134.82,8.74;10,92.48,593.85,53.54,8.74">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="165" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.47,613.78,429.53,8.74;10,92.48,625.73,22.70,8.74" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,179.98,613.78,171.37,8.74">New Methods in Automatic Astracting</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">P</forename><surname>Edmundson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,362.16,613.78,86.26,8.74">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="264" to="285" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,92.47,645.66,429.53,8.74;10,92.48,657.61,429.53,8.74;10,92.48,669.57,22.70,8.74" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,335.05,645.66,186.96,8.74;10,92.48,657.61,176.79,8.74">Combining the evidence of multiple query representations for information retrieval</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">J</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,281.30,657.61,180.13,8.74">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="431" to="448" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
