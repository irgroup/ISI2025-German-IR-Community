<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,137.00,82.53,319.05,15.50">Using Statistical Translation Models for Bilingual IR</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,220.00,118.20,66.39,12.18"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
							<email>nie@iro.umontreal.ca</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire RALI Département d&apos;Informatique et Recherche opérationnelle</orgName>
								<orgName type="institution">Université de Montréal C.P</orgName>
								<address>
									<addrLine>succursale Centre-ville Montréal</addrLine>
									<postBox>6128</postBox>
									<postCode>H3C 3J7</postCode>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,297.19,118.20,75.82,12.18"><forename type="first">Michel</forename><surname>Simard</surname></persName>
							<email>simardm@iro.umontreal.ca</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire RALI Département d&apos;Informatique et Recherche opérationnelle</orgName>
								<orgName type="institution">Université de Montréal C.P</orgName>
								<address>
									<addrLine>succursale Centre-ville Montréal</addrLine>
									<postBox>6128</postBox>
									<postCode>H3C 3J7</postCode>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,137.00,82.53,319.05,15.50">Using Statistical Translation Models for Bilingual IR</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">14E92C4B4B8DCF0D68B7F6DDF03AFD3C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This report describes our test on using statistical translation models for bilingual IR tasks in CLEF-2001. These translation models have been trained on a set of parallel web pages automatically mined from the Web. Our goal is to compare the following approaches: -using the original parallel corpora or a cleaned corpora to train translation models; -using the raw translation probabilities to weigh query words or combine the probabilities with IDF; -using different cut-off probability values in the translation models (i.e. delete the translations lower than a threshold). Our results show that: -the models trained on the original parallel corpus work better than those on the cleaned corpora; -the combination of the probabilities with IDF is beneficial; -and it is better to cut-off the translation models at a certain value (0.01 in our case) than not cut them.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>There have been several experiments on using statistical translation models for CLIR <ref type="bibr" coords="1,423.85,438.09,38.71,11.07" target="#b3">[Franz98,</ref><ref type="bibr" coords="1,466.08,438.09,26.02,11.07" target="#b4">Nie99]</ref>. It has been shown that with a proper use of the translation models, we can obtain effectiveness comparable to that using a good machine translation system. In our previous studies we have successfully mined several large sets of parallel web pages, and used them to train translation models for the following language pairs: French-English, German-English, Italian-English and Chinese-English. The model training on these corpora has been the same as that using a manually prepared parallel corpus. However, there are several differences between these corpora and a manually prepared corpus. In particular, the corpora with parallel web pages contain much more noise (non-parallel pairs).</p><p>Therefore, some special means seem to be necessary to explore the best uses of the parallel web pages in the CLIR tasks. In this report, we describe our attempts to make better translation models from the parallel corpora, and to make better use of the trained translation models in CLEF bilingual IR tasks. The following problems will be investigated in our tests:</p><p>-the clean up of the parallel corpora in order to create neater corpora; -the cut-off of translation models in order to eliminate unreliable translations; -the use of two-directional query translation; -and the combination of translation models with bilingual dictionaries to increase the coverage.</p><p>In the following sections, let us first recall briefly the corpora and the training of translation models. Then we will report each of the above experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Parallel text mining and translation model building</head><p>We used the same sets of parallel web pages as the last year. The three corpora of French, Italian and German with English have been constituted automatically <ref type="bibr" coords="1,271.21,697.09,35.85,11.07" target="#b1">[Chen00]</ref>. Their sizes are as follows: The training of translation models is as follows: Given a set of parallel texts in two languages, it is first aligned into parallel sentences. The criteria used in sentence alignment are the position of the sentence in the text (parallel sentences have similar positions in two parallel texts), the length of the sentence (they are also similar in length), and so on <ref type="bibr" coords="2,411.63,184.09,33.86,11.07" target="#b2">[Gale93]</ref>. In <ref type="bibr" coords="2,465.22,184.09,43.75,11.07" target="#b6">[Simard92]</ref>, it is proposed that cognates may be used as an additional criterion. Cognates refers to the words (e.g. proper names) or symbols (e.g. numbers) that are identical (or very similar in form) in two languages. If two sentences contain such cognates, it provides additional evidence that they are parallel. It has been shown that the approach using cognates performs better than the one without cognates. Before the training of models, each corpus is aligned into parallel sentences using cognate-based alignment algorithm.</p><formula xml:id="formula_0" coords="2,271.00,75.09,133.00,11.07">E-F E-G E-I</formula><p>Once a set of parallel sentences is obtained, word translation relations are estimated. First, it is assumed that every word in a sentence may be the translation of every word in its parallel sentence. Therefore, the more two words appear often in parallel sentences, the more they are thought of to be translation of one another. In this way, we obtain the initial probabilities of word translation.</p><p>At the second step, the probabilities are submitted to a process of Expectation Maximization (EM) in order to maximize the probabilities with respect to the given parallel sentences. The algorithm of EM is described in <ref type="bibr" coords="2,71.00,322.09,41.78,11.07" target="#b0">[Brown93]</ref>. The final result is a probability function P(f|e) which gives the probability that f is the translation of e. Using this function, we can determine a set of probable word translations in the target language for each source word, or for a complete query in the source language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Cut-off of translation models</head><p>A translation model contains translations for every lexical item encountered in a training corpus, even if the item appeared only once. In the case where an item does not appear often, its translation is often sparse, i.e. it is translated by many different words with low probabilities. In fact, if a source word occurs only once in the training corpus, it may be translated by every word in the target sentence with quite comparable probabilities. Such translations are not reliable. In addition, storing the translations for every word requires a large space. It is not practical to use such a translation model in a real application. For example, the original French-English model contains more than 13 millions entries. Therefore, we exploited the possibility of reducing the translation models by cutting off the translations that we judge unreliable.</p><p>Two ways to cut off a model are investigated:</p><p>-We only keep a fixed number of lexical items in the model; -We remove all the translations below a certain threshold of translation probability. We tested several numbers in the first cut-off method: 1 million (1M), 100 thousands (100K), 10 thousands (10K), 5 thousands (5K) and 1 thousand (1K), and the thresholds tested are 0.05, 0.1 and 0.25. Below is a summary of the results with the CLEF-2000 data 1 . We can see that the cut-off by probability is a better solution. In particular, when the threshold is set at 0.1, we obtain generally good results. In fact, the unreliable translations usually have low probabilities. So by eliminating low-probability translations, a large part of unreliable translations is removed.</p><p>Given that the words used in the CLEF queries are not rare words, we did not encounter the problem of increasing unknown words by cutting off the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Cleaning up of parallel corpora</head><p>The parallel corpora gathered from the Web contain a certain proportion of non-parallel texts. In fact, it is impossible to eliminate all the non-parallel texts in such a corpus automatically. The question we raised is whether it is helpful to use additional criteria to filter the corpora so that certain non-parallel texts can be eliminated. We did this filtering first on the Chinese-English corpus, also gathered with the same mining tool. The filtering criteria are the following <ref type="bibr" coords="3,224.34,311.09,29.49,11.07" target="#b5">[Nie01]</ref>:</p><p>-Tighten the control on text length: If the lengths of two presumably parallel texts are too different from the typical length ratio of the two language, then the texts are considered to be non-parallel. -Control on empty alignment: Once an automatic alignment algorithm is applied on a pair of texts, if the proportion of empty alignments (a sentence is aligned with nothing, or n:0 and 0:n alignments) is higher than a threshold, then the pair is eliminated. -Using a bilingual dictionary in alignment: In order to increase the accuracy of sentence alignment, we also examine the proportion of words in the sentence that have known translations (stored in the dictionary) in the corresponding sentence. This proportion is taken into account and used in combination with the length criterion in a length-based alignment algorithm <ref type="bibr" coords="3,410.90,419.09,33.88,11.07" target="#b2">[Gale93]</ref>. In our tests on Chinese-English corpus, we found that after the filtering, both the translation accuracy judged with 200 randomly selected words and the effectiveness of CLIR between English and Chinese have been increased significantly. Below are the best improvements we have been able to obtain: The results show a large drop of effectiveness in the de-en case. In the case of it-en, it is quite comparable to that without filtering. This counter-performance may be due to several factors:</p><p>-The parameters determined for the Chinese-English corpus (that are used during the filtering) are not necessarily suitable to the corpora of European languages; -There may be a flaw in the training of the translation models, as this was done in a hurry in the last minutes. We are now investigating both factors, and hope to find an explanation soon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Two-direction translation of queries</head><p>It is observed that certain common English words often appear as one of the top-raked translations of queries (e.g. make, provide, due, etc.). This is because these words frequently appear in the training corpora, and they are not considered as stopwords. So they are considered to be highly probable translations for many words in French, German and Italian. However, if we also consider translations of these English words back to the source languages, it would likely be translated to many different words, i.e. their translations would not be concentrated on the same source words for which they have been suggested as translations. Therefore, a combination of both directions in translation could eliminate such common words in query translations. This is the intuition of using the two-direction translation of queries.</p><p>In our implementation, the translation probabilities of both directions are multiplied, i.e. for a pair (e, f) of English and French, the probability of P(e|f) is multiplied by the probabilities of Σf' P(f'|e) where f' is all the words in the original French query.</p><p>However, our test did not show that this idea or this implementation works well. We observe a large decrease in performance compared with Tables <ref type="table" coords="4,353.81,530.09,5.00,11.07" target="#tab_1">2</ref> and<ref type="table" coords="4,378.03,530.09,3.73,11.07" target="#tab_2">3</ref>. A possible reason is the filtering of the translations: after the translation of each query, we only keep the translations whose probability is higher than a certain threshold (0.001). So in many cases, there are much less than 50 translation words (as in the case of one-directional translations). As a matter of fact, the average numbers of English translation words per query are about 14 for French queries, less than 17 for German queries and less than 16 for Italian queries. This shows that the way we used to keep translation words is too selective. <ref type="bibr" coords="4,344.00,585.09,11.13,11.07">As</ref>  While we observe that many false translation words have been eliminated, some new ones have been introduced (e.g. cad, part, and sedan), and the word "architettura" is translated by itself, probably due to the appearances of this word in English texts. So globally, we did not obtain any benefit from this two-directional translation. We will experiments some variances of this method in order to know if it is the idea or our particular implementation method that did not work well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Combination with dictionaries</head><p>In the past, we have found that we can greatly improve the CLIR effectiveness by combining a translation model with a bilingual dictionary. The method we tested is to assign a fixed value to those translations that are found in the dictionary. In other words, if a translation is stored in the dictionary, then the weight of this translation will be incremented by a fixed value.</p><p>We observed that common words often have more translations than specialised words. As a result, common words will be represented by more translations, and indirectly, their importance in the query is increased unduly. In order to take into account the commonness of a translation word, we combine the translation probabilities with the idf value of the translation word. The intuition is to increase the probabilities of the uncommon translation words and lower the probabilities of common translation words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Submitted runs</head><p>We submitted 9 bilingual runs, three for each of the language pairs: fr-en, de-en and it-en.</p><p>The first group of runs uses only the 50 first translation words and the translation probabilities. The translation models used are cut-off by a threshold of 0.1 (P≥0.1). These runs are prefixed by RaliP01.</p><p>The second group combines the models used in the previous group with dictionaries -FreeDict that we found on the Web ( http://www.freedict.com/dictionary/index.html) . In particular, we used the dictionaries of French to English, German to English and Italian to English. All the translations in the dictionaries are assigned a fixed value (0.001). These runs are prefixed by RaliM001.</p><p>The third group also combines with the dictionaries, but the weights attributed to the translations are their idf. These runs are prefixed by RaliMidf.</p><p>The effectiveness (non-interpolated average precision) of these runs are summarised in the following Globally, we see that the combination with a dictionary (in one of the two ways) leads to some improvement. In particular, for fr-en case, the RaliMidf case is 5.3% better than the RaliP01 case. For de-en, the improvement is even greater (20.7%). This is probably because of the relatively poor coverage of German-English corpus (this corpus is the smallest among all we have mined). On the other hand, there is no observable advantage for the it-en case to combine with a dictionary. Our focus in this CLEF was on fr-en case, because for both French and English, we have a good lemmatizer (in fact, we transform each word in its form of citation). For the two other language, a simple stemmer found from the Web ( http://www.muscat.com , however, the site does not seem to contain the stemmers) was used. For German, this is certainly not enough because of agglutination.</p><p>The three best submitted runs are compared with the medium effectiveness in the following table:</p><formula xml:id="formula_1" coords="5,165.00,623.09,295.02,41.07">RaliMidfF2E RaliMidfD2E RaliM001D2E ≥ medium 41 27 27 &lt; medium 6 20<label>20</label></formula><p>Table <ref type="table" coords="5,231.37,668.09,3.75,11.07">9</ref>. Comparison with medium effectiveness From this first comparison, we see that the RaliMidfF2E run (the run using a translation model mixed with dictionary translations whose weights are determined by idf) is quite good and well above the medium performance. On the other hand, for Italian and German queries, the performances are quite comparable to the medium performances. We think that the main reason is that we do not have a good stemmer for both languages.</p><p>The good effectiveness of the fr-en run shows that it is a good idea to merge a translation model with a bilingual dictionary and to weight the dictionary translations by idf. In so doing, the statistical translation model can be completed by the dictionary, and the idf factor assigns the dictionary translations with a reasonable weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Final remarks</head><p>In this series of tests on bilingual IR, we wanted to test several ideas:</p><p>-the use of cut-offs of statistical translation models, -the clean-up of the training corpora, -the use of two-directional query translation, -and the combination of translation models with bilingual dictionaries. Model cut-off was shown to be not only a way to reduce the space required to store the translation models, but also a way to increase the quality of query translation. This is due to the elimination of many unreliable translations during the cut-offs. In addition, we found that it is better to cut off the models according to the translation probabilities than by size of the model.</p><p>However, the clean up of initial training corpora did not show any improvement in the CLIR tasks. This seems to be contradictory with our recent experiments on Chinese-English CLIR. We would like to further examine the processes of our clean up and model training in order to find the reasons for this.</p><p>The use of two-directional query translation did not improve the CLIR performance either. Several factors in our implementation may have affected this test, in particular, the setting of translation probability threshold of 0.001 that leads to few translation words per query. Additional tests are undertaken in order to find a more satisfactory explanation.</p><p>Finally, the combination of translation models with bilingual dictionaries is, in general, beneficial. In particular, for the case of French-English CLIR that we focused on, we observed a certain degree of improvement. In this case, we found it a good idea to assign a weight to the dictionary translations according to their idf values. This would assign a common translation word with a low weight, and a fairly rare word with high weight. This approach seems to be very intuitive.</p><p>We are doing additional test. We hope to be able to report a more thorough account of this series of tests at the workshop.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,178.00,90.09,241.05,41.07"><head>Table 1 .</head><label>1</label><figDesc>Size of the training corpora</figDesc><table coords="2,178.00,90.09,241.05,26.07"><row><cell>Text Pairs</cell><cell>18 807</cell><cell cols="2">10 200</cell><cell>8 504</cell></row><row><cell>Volume (Mb)</cell><cell cols="2">174 198 77</cell><cell>100</cell><cell>50</cell><cell>68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,85.00,572.09,438.05,171.07"><head>Table 2 .</head><label>2</label><figDesc>Cut-off of models by size</figDesc><table coords="2,169.00,572.09,248.00,56.07"><row><cell></cell><cell>1M</cell><cell>100K</cell><cell>10K</cell><cell>5K</cell><cell>1K</cell></row><row><cell>de-en</cell><cell cols="5">0.1684 0.1559 0.1403 0.1212 0.0714</cell></row><row><cell>it-en</cell><cell cols="5">0.2442 0.2237 0.2426 0.2059 0.0989</cell></row><row><cell>fr-en</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note coords="2,85.00,719.87,438.05,12.29;2,85.00,732.09,224.00,11.07"><p>1 Note: The experimental results on fr-en are missing. We are redoing the experiments to complete the missing numbers and will report them at the workshop.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="3,85.00,135.09,306.00,39.07"><head>Table 3 .</head><label>3</label><figDesc>Cut-off of models by probabilitiesNote: All the queries are translated by the 50 strongest translation words.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="3,143.00,484.09,308.04,195.07"><head>Table 4 .</head><label>4</label><figDesc>Numbers and proportions of correct first translations for 200 words</figDesc><table coords="3,197.00,484.09,185.00,195.07"><row><cell></cell><cell></cell><cell>Combination</cell></row><row><cell>Direction</cell><cell>No filter</cell><cell>Best filtering</cell></row><row><cell>E-C</cell><cell>161 (80.50%)</cell><cell>183 (91.50%)</cell></row><row><cell>C-E</cell><cell>154 (77.00%)</cell><cell>173 (86.50%)</cell></row><row><cell></cell><cell></cell><cell>Combination</cell></row><row><cell>Direction</cell><cell>No filter</cell><cell>Best filtering</cell></row><row><cell>E-C</cell><cell>0.1843</cell><cell>0.2013</cell></row><row><cell></cell><cell>(47.11%)</cell><cell>(50.63%)</cell></row><row><cell>C-E</cell><cell>0.1898</cell><cell>0.2063</cell></row><row><cell></cell><cell>(49.16%)</cell><cell>(53.43%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="3,71.00,680.09,452.03,58.07"><head>Table 5 .</head><label>5</label><figDesc>CLIR effectiveness on two sets of TREC documentsWe expected to have similar results with the same filtering on the corpora of European languages. However, our tests showed a decrease in performances. That we can see by comparing the following table with Tables 2 and 3.</figDesc><table coords="4,180.00,75.09,242.00,56.07"><row><cell></cell><cell>1M</cell><cell>100K</cell><cell>P≥0.05 P≥0.1 P≥0.25</cell></row><row><cell>de-en</cell><cell cols="3">0.0764 0.0745 0.0777 0.0751 0.0669</cell></row><row><cell>it-en</cell><cell cols="3">0.2209 0.2418 0.2453 0.2448 0.2363</cell></row><row><cell>fr-en</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="4,217.00,132.09,159.02,11.07"><head>Table 6 .</head><label>6</label><figDesc>With filtered training corpora.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="4,71.00,585.09,452.00,133.48"><head></head><label></label><figDesc>an example, the first Italian query (on "reconstruction of Berlin") is translated by the following words:</figDesc><table coords="4,107.00,626.71,399.01,91.86"><row><cell>berlin</cell><cell>0.242987</cell><cell>architectural</cell><cell>0.048981</cell></row><row><cell>cad</cell><cell>0.147230</cell><cell>reconstruction</cell><cell>0.038330</cell></row><row><cell>architecture</cell><cell>0.110667</cell><cell>architettura</cell><cell>0.027761</cell></row><row><cell>part</cell><cell>0.100185</cell><cell>town</cell><cell>0.026326</cell></row><row><cell>rebuild</cell><cell>0.080169</cell><cell>sedan</cell><cell>0.022886</cell></row><row><cell>city</cell><cell>0.079734</cell><cell>reconstruct</cell><cell>0.010781</cell></row><row><cell>wall</cell><cell>0.063963</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="5,187.00,371.09,323.01,85.07"><head>table :</head><label>:</label><figDesc></figDesc><table coords="5,187.00,400.09,206.02,56.07"><row><cell></cell><cell>RaliP01</cell><cell>RaliM001</cell><cell>RaliMidf</cell></row><row><cell>fr-en</cell><cell>0.3499</cell><cell>0.3564</cell><cell>0.3685</cell></row><row><cell>de-en</cell><cell>0.2124</cell><cell>0.2188</cell><cell>0.2565</cell></row><row><cell>it-en</cell><cell>0.2731</cell><cell>0.2742</cell><cell>0.2562</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="5,182.00,460.09,243.00,11.07"><head>Table 8 .</head><label>8</label><figDesc>Summary of the effectiveness of the submitted runs</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,118.52,459.09,404.49,11.07;6,89.00,470.09,369.02,11.07" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,402.00,459.09,121.01,11.07;6,89.00,470.09,133.03,11.07">The mathematics of machine translation: Parameter estimation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">D J</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,229.10,470.09,107.22,11.07">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="263" to="312" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,113.30,481.09,409.71,11.07;6,80.00,492.09,245.03,11.07" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,197.65,481.09,325.35,11.07;6,80.00,492.09,83.01,11.07">Automatic construction of parallel English-Chinese corpus for cross-language information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Y</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,169.54,492.09,46.89,11.07">Proc. ANLP</title>
		<meeting>ANLP<address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="21" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,109.00,506.09,414.00,11.07;6,89.00,517.09,138.01,11.07" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,226.01,506.09,227.49,11.07">A program for aligning sentences in bilingual corpora</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">A</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">W</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,462.00,506.09,61.01,11.07;6,89.00,517.09,43.68,11.07">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="102" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,113.00,531.09,410.00,11.07;6,89.00,542.09,341.97,11.07" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,274.99,531.09,222.23,11.07">Ad hoc and multilingual information retrieval at IBM</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Mccarley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,508.00,531.09,15.00,11.07;6,89.00,542.09,179.58,11.07">The Seventh Text Retrieval Conference (TREC-7)</title>
		<imprint>
			<publisher>NIST SP</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="157" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.00,556.09,418.02,11.07;6,89.00,567.09,434.06,11.07;6,89.00,578.09,39.00,11.07" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,299.00,556.09,224.02,11.07;6,89.00,567.09,231.00,11.07">Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the Web</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Isabelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,329.00,567.09,93.46,11.07">ACM-SIGIR conference</title>
		<meeting><address><addrLine>Berkeley, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,106.00,592.09,416.97,11.07;6,89.00,603.09,167.01,11.07" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,183.88,592.09,194.15,11.07">Filtering noisy parallel corpora of web pages</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,387.87,592.09,135.10,11.07;6,89.00,603.09,94.87,11.07">IEEE symposium on NLP and Knowledge Engineering</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct coords="6,121.30,617.09,401.72,11.07;6,89.00,628.09,434.01,11.07;6,89.00,639.09,122.00,11.07" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,277.15,617.09,241.50,11.07">Using Cognates to Align Sentences in Parallel Corpora</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Isabelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,89.00,628.09,434.01,11.07;6,89.00,639.09,45.80,11.07">Proceedings of the 4th International Conference on Theoretical and Methodological Issues in Machine Translation</title>
		<meeting>the 4th International Conference on Theoretical and Methodological Issues in Machine Translation<address><addrLine>Montreal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
