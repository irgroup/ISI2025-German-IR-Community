<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,105.84,74.06,383.62,15.41;1,233.28,90.14,128.77,15.41">Thomson Legal and Regulatory at CLEF 2001: monolingual and bilingual experiments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,122.88,126.65,85.80,10.89"><forename type="first">Hugo</forename><surname>Molina-Salgado</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>610 Opperman Drive</addrLine>
									<postCode>55123</postCode>
									<settlement>Eagan</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,215.54,126.65,70.70,10.89"><forename type="first">Isabelle</forename><surname>Moulinier</surname></persName>
							<email>isabelle.moulinier@westgroup.com</email>
							<affiliation key="aff0">
								<address>
									<addrLine>610 Opperman Drive</addrLine>
									<postCode>55123</postCode>
									<settlement>Eagan</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,292.89,126.65,55.11,10.89"><forename type="first">Mark</forename><surname>Knutson</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>610 Opperman Drive</addrLine>
									<postCode>55123</postCode>
									<settlement>Eagan</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,354.94,126.65,85.55,10.89"><roleName>Kirat</roleName><forename type="first">Elizabeth</forename><surname>Lund</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>610 Opperman Drive</addrLine>
									<postCode>55123</postCode>
									<settlement>Eagan</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,442.94,126.65,29.52,10.89;1,288.48,138.17,18.59,10.89"><forename type="first">Sekhon</forename><surname>Tlr</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>610 Opperman Drive</addrLine>
									<postCode>55123</postCode>
									<settlement>Eagan</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,105.84,74.06,383.62,15.41;1,233.28,90.14,128.77,15.41">Thomson Legal and Regulatory at CLEF 2001: monolingual and bilingual experiments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F2ED0A673826F15068E846A93CFC73B1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Thomson Legal and Regulatory participated in the monolingual track for all five languages and in the bilingual track with Spanish-English runs. Our monolingual runs for Dutch, Spanish and Italian use settings and rules derived from our runs in French and German last year.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Thomson Legal and Regulatory (TLR) participated in CLEF-2001 with two goals: reuse of rules and settings inside a family of languages for monolingual retrieval, and start our effort on bilingual retrieval.</p><p>In our monolingual runs, we considered Dutch and German as being one family of languages, while French, Spanish and Italian formed another. We used the parameters we derived from our runs at CLEF-2000 for German and French for each language in their respective family. In addition, we investigated the use of phrases for French and Spanish document retrieval.</p><p>Our first attempt at the bilingual track was from Spanish queries to English documents. In that task, we experimented with combining various resources for query translation. Our submitted runs used similarity thesauri and a machine-readable dictionary to translate a Spanish query into a single English query. We also compared our official runs with the merging of individual runs, one per translation resource.</p><p>In this paper, we briefly present our search engine and the settings common to all experiments. Then, we discuss our bilingual effort. Finally, we describe our participation in the monolingual track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General System Description</head><p>The WIN system is a full-text natural language search engine, and corresponds to TLR/West Group's implementation of the inference network retrieval model. While based on the same retrieval model as the INQUERY system [BCC93], WIN has evolved separately and focused on the retrieval of legal material in large collections in a commercial environment that supports both Boolean and natural language searches <ref type="bibr" coords="1,84.96,529.85,28.93,10.89" target="#b4">[Tur94]</ref>.</p><p>WIN has also been modified to support non-English document retrieval. This included localization of tokenization rules (for instance, handling elision for French and Italian) and stemming. Stemming of non-English terms is performed using a third-party toolkit, the LinguistX platform ® commercialized by Inxight<ref type="foot" coords="1,113.28,580.13,3.24,7.17" target="#foot_0">1</ref> . A variant of the Porter stemmer is used for English.</p><p>The WIN engine supports various strategies for computing term beliefs and document scores. We used a standard tf-idf for computing term beliefs in all our runs. Among all the variants, we retained document scoring and portion scoring. Document scoring assigns a score to the document as a whole. This was used in our bilingual runs. Portion scoring finds the best dynamic portion in a document and combines the score of the best portion to the score of the whole document. Portion scoring can be considered as an approximation of paragraph scoring (we used this setting last year for French), when documents have no paragraph. We used portion scoring in our monolingual runs.</p><p>A WIN query consists of concepts extracted from natural language text. Normal WIN query processing eliminates stopwords and noise phrases (or introductory phrases), recognizes phrases or other important concepts for special handling, and detects misspellings. Many of the concepts ordinarily recognized by WIN are specific to English documents, in particular within the legal domain. Query processing in WIN usually relies on various resources: a stopword list, a list of noise phrases ("Find cases about…", "A relevant document describes…") , a dictionary of (legal) phrases, and a list of common misspelled terms.</p><p>We used stopword and noise phrases lists for all languages, while for French and monolingual Spanish, we also used a phrase dictionary. We used our French and German stopword lists from last year, the Dutch list given on the CLEF homepage, and compiled Spanish and Italian stopword lists from various sources on the Web. For all languages, we extracted introductory phrases from the query sets of previous CLEF and TREC conferences. As we had no Italian speaker in our team, our introductory list in Italian is very limited and simple.</p><p>Finally, we submitted two sets of runs: runs including only the title and description fields from the CLEF topic, and runs including the whole topic. The former runs are labeled with 'td' and doubled weighted the title fields. The latter are labeled with 'tdn' and used a weight of 4 for the title field, 2 for the description field, and 1 for the narrative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spanish-English bilingual retrieval experiments and results</head><p>In our bilingual runs, we concentrated on query translation and more specifically the combination of various translation resources. We used three main resources, a machine-readable dictionary (MRD) that we downloaded from http://www.freedict.com and two different similarity thesauri. Coverage of these resources is reported in Table <ref type="table" coords="2,204.01,281.21,3.75,10.89" target="#tab_0">1</ref>.</p><p>We implemented a variant of the similarity thesaurus approach described in [PBS97] for multilingual retrieval. We used a parallel corpus, the UN parallel text corpus produced by the Linguistic Data Consortium. We generated two different thesauri: a unigram thesaurus and a bigram thesaurus. Our intent with the bigram thesaurus was to capture some phrase translation. We limited the number of bigrams by constraining bigrams to not contain stopwords, and by frequency thresholding. We used at most 15 translations from each thesaurus, and also used a threshold on the similarity to filter out translations that we thought would not be helpful. This threshold was determined on training data from CLEF 2000. We used all translations from the MRD. In all cases, multiple translations of the same Spanish term were grouped as the same concept given a translation source. Our official runs relied on the a priori approach, combining translations during construction. Runs tlres2entdw and tlres2entdnw<ref type="foot" coords="2,207.84,605.09,3.24,7.17" target="#foot_1">2</ref> combined only the unigram thesaurus to the dictionary, while runs tlres2entdb and tlres2entdb combined both thesauri with the dictionary. In Table <ref type="table" coords="2,112.35,635.93,3.77,10.89" target="#tab_1">2</ref>, we also report a posteriori merging using the following conversion: b refers to the bigram thesaurus, u to the unigram thesaurus and d to the dictionary. The different scoring methods are indicated by a s for score and r for rank. Thus, the run labeled b+u+d_r_tdn refers to combining both thesauri and the MRD using rank-based merging. In all cases, terms not found in any resource were left intact in the query. Figure <ref type="figure" coords="3,114.51,564.65,4.92,10.89">1</ref> summarizes the impact of combining resources, as it shows runs using individual resources as well as our official runs using all fields in the CLEF topics. Run b_tdn used only the bigram thesaurus, run u_tdn the unigram thesaurus, while run d_tdn used the MRD. We did not report runs using only the title and description fields from the CLEF topics, as they showed the same behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1 Summary of our Spanish to English bilingual runs</head><p>Table <ref type="table" coords="4,111.18,425.45,4.92,10.89" target="#tab_1">2</ref> shows that our official runs performed well in this year's evaluation. The results show a slight advantage in using the bigram thesaurus in combination with the other resources. However, the bigram thesaurus on its own shows very poor performance. There are two main reasons for that behavior. First, the coverage of the bigram thesaurus was poor: only a limited number of bigrams were found in the queries, some queries having no bigram. Second, English bigrams sometimes were not present in the retrieval collection, as thesauri were constructed on a non-related corpus. This resulted in some queries returning very few or no documents.</p><p>The poor performance of the bigram thesaurus also impacted our a posteriori merging. Indeed, runs using the bigram thesaurus show lower average precision than runs only using the unigram thesaurus and the MRD. The score-based technique performed better than the rank-based technique. The rank-based technique reported here is based on the product of the ranks. As a result, documents with very different ranks in the individual runs are penalized.</p><p>Finally, a priori merging performed better than both techniques for a posteriori merging. One reason is that the number of non-translated terms diminished when resources are combined a priori. Further analysis is needed to better understand the difference in behaviors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monolingual retrieval experiments and results</head><p>In our monolingual runs, we considered two aspects: families of languages, and the use of a phrase dictionary. We used the same rules for Dutch and German on the one hand, and French, Spanish and Italian on the other. In addition, we introduce a phrase dictionary in some of our Spanish and French runs.</p><p>German and Dutch were considered as compounding languages. Using the LinguistX morphological analyzer allowed us to detect compound words and break them at indexing and search time. We used a structured query and loose noun phrases to represent compounds.</p><p>For French, Spanish and Italian, we allowed the LinguistX morphological analyzer to generate several stems (we did not disambiguate using part-of-speech tags). Multiple stems were grouped as a single concept (using a OR/SYN or a SUM node for instance) in the structured query. For French and Spanish, we generated a dictionary of roughly 1000 noun phrases. We extracted noun phrases from the French and Spanish document collections, we then derived some rules to filter out proper nouns like "Bill Clinton" and phrases we thought non-content bearing such as "année dernière" or "premier trimestre". Finally, we manually filtered the 1500 most frequent noun phrases to remove noisy phrases not captured by our simple rules. Examples of phrases are "unión europea" and "casque bleu".</p><p>Table <ref type="table" coords="5,112.10,123.05,4.92,10.89" target="#tab_3">3</ref> summarizes our results. Runs marked with the sign † are unofficial runs; for these runs the comparison to the median is indicative. We have included a corrected value for all our official runs. During the analysis of our results, we realized that while our documents were ranked correctly according to the scores in the engine, some of the scores reported were incorrect due to a conversion error in Java. This influenced our performance, since the evaluation program trec_eval resorts documents based on their score.</p><p>Using a phrase dictionary was neither harmful, nor helpful. We observed that phrases from the dictionary were found in only one fifth of the queries. For those queries, there is no clear emerging behavior: some perform better using phrases, while others do not. The difference in precision per query between the two runs is usually very small. Our results for compounding languages are in the better half of the participants for these runs, so are our Spanish results. We believe that reusing of settings in a family of languages is indeed helpful. We need to perform some further analysis to confirm that belief.</p><p>Our Italian run was hindered by the lack of a good noise phrase list, as some of our structured queries still contained terms like information or document.</p><p>While we used last year's settings for French, we did not achieve the performance we were aiming for. So far, we have identified two reasons. First, our noise phrase list for French missed capturing some of the patterns used in this year's topics. When we manually cleaned the topics, we observed an improvement in the average precision. Some topics, however, benefited from non-content bearing terms that were not very frequent in the collection (for instance énumérant in queries 59 and 71). Next, while we originally intended to consider a term with multiple stems as a single concept, we realized that our scoring was overweighing such a term. Changing the behaviour would also have helped our French runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Final remarks</head><p>One of the problems in our bilingual runs was the coverage of the translation resources. Many translated queries still included original Spanish terms. In order to solve that problem, we can either use a MRD with a wider coverage (20,000 entries is a rather limited dictionary), or try to get a better coverage from the similarity thesauri. Better coverage may be achieved by using a parallel/comparable corpus in the same domain as the retrieval collections, if not the retrieval collections themselves (see <ref type="bibr" coords="6,452.63,223.13,31.45,10.89" target="#b1">[SBS97]</ref>). We will be investigating alignments of documents in related collections in the future.</p><p>Our monolingual runs contain no query expansion or pseudo-relevance feedback. Once we have refined the list used in query processing, for instance adding a list for misspelled terms, we will focus on automatic query expansion to try and enhance our searches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,84.96,411.29,425.81,177.21"><head>Table 1 : Coverage of the translation resources used</head><label>1</label><figDesc></figDesc><table coords="2,84.96,432.17,399.55,28.89"><row><cell></cell><cell>Dictionary</cell><cell>Unigram Thesaurus</cell><cell>Bigram Thesaurus</cell></row><row><cell>Spanish</cell><cell>19,466 terms</cell><cell>33,674 terms</cell><cell>42,081 bigrams</cell></row></table><note coords="2,84.96,485.93,425.76,10.89;2,84.96,497.21,425.62,10.89;2,84.96,508.73,425.81,10.89;2,84.96,520.25,425.58,10.89;2,84.96,531.77,425.58,10.89;2,84.96,543.29,425.49,10.89;2,84.96,554.81,425.69,10.89;2,84.96,566.33,425.50,10.89;2,84.96,577.61,15.58,10.89"><p>We investigated two main approaches to combine our translation resources: a priori merging, i.e. combining translations during query construction, and a posteriori merging, i.e. merging runs produced by queries translated from a single resource. For a posteriori merging, we used a score-based and a rankbased technique to generate the merged score. The score based technique relies on a feature of the WIN engine. WIN computes the best score a document can achieve for a given query. We used that maximum score to normalize individual runs. Normalized runs are merged in a straightforward manner. The rank based technique is also fairly simple. The score in the merged result list is a function of the ranks in the original lists. Here, we report experiments using the sum of the logarithms of the document rank in each run.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,84.96,94.49,405.66,435.69"><head>Table 2 Results from our bilingual Spanish to English experiments</head><label>2</label><figDesc>Comparison to the median is indicative. If these runs had been included, the median will be different.</figDesc><table coords="3,316.32,115.37,134.18,10.89"><row><cell>Performance of individual queries</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="5,84.96,252.89,405.66,428.49"><head>Table 3 : Summary of all monolingual runs</head><label>3</label><figDesc></figDesc><table coords="5,312.00,273.77,134.18,10.89"><row><cell>Performance of individual queries</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,90.54,756.89,341.94,10.89"><p>Information can be found at http://www.inxight.com/products_sp/linguistx/index.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,91.03,745.61,419.80,10.89;2,84.96,757.13,170.80,10.89"><p>The only difference between runs tlrdetdw and tlrdetdnw and between runs tlres2entdb and tlres2entdnb is whether the narrative field is used or not.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,124.32,312.65,380.24,10.89;6,504.72,311.09,5.76,7.17;6,99.36,323.93,335.88,10.89" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,275.35,312.65,127.87,10.89">The INQUERY retrieval system</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Broglio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,421.65,312.65,82.92,10.89;6,504.72,311.09,5.76,7.17;6,99.36,323.93,282.39,10.89">Proceedings of the 3 rd International Conference on Database and Expert Systems Applications</title>
		<meeting>the 3 rd International Conference on Database and Expert Systems Applications<address><addrLine>Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,122.14,341.69,388.51,10.89;6,99.36,352.97,411.29,10.89;6,99.36,364.49,156.42,10.89" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,301.74,341.69,208.91,10.89;6,99.36,352.97,54.75,10.89">Cross-lingual information retrieval in a multilingual legal domain</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sheridan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schäuble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,178.80,352.97,331.85,10.89;6,99.36,364.49,127.90,10.89">Proceedings of the First European Conference on Research and Advanced Technology for Digital Libraries</title>
		<meeting>the First European Conference on Research and Advanced Technology for Digital Libraries</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,130.87,382.25,347.04,10.89;6,99.36,393.53,411.05,10.89;6,99.36,405.05,263.20,10.89" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,323.17,382.25,154.74,10.89;6,99.36,393.53,145.97,10.89">TREC-3 Ad Hoc Retrieval and Routing Experiments using the WIN System</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Flood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,269.29,393.53,236.44,10.89">Overview of the 3rd Text Retrieval Conference (TREC-3)</title>
		<meeting><address><addrLine>Gaithersburg, MD</addrLine></address></meeting>
		<imprint>
			<publisher>NIST Special Publication</publisher>
			<date type="published" when="1995-04">April 1995</date>
			<biblScope unit="page" from="500" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,123.28,422.57,387.21,10.89;6,99.36,434.09,229.90,10.89" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,173.04,422.57,191.97,10.89">Inference Networks for Document Retrieval</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, University of Massassuchets</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct coords="6,123.19,451.61,387.44,10.89;6,99.36,463.13,175.64,10.89;6,275.04,461.57,5.39,7.17;6,286.14,463.13,224.53,10.89;6,99.36,474.65,206.88,10.89" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,173.08,451.61,337.55,10.89;6,99.36,463.13,50.61,10.89">Natural language vs. Boolean query evaluation : a comparison of retrieval performance</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,174.48,463.13,100.52,10.89;6,275.04,461.57,5.39,7.17;6,286.14,463.13,224.53,10.89;6,99.36,474.65,148.63,10.89">Proceedings of the 17 th Annual International Conference on Research and Development in Information Retrieval</title>
		<meeting>the 17 th Annual International Conference on Research and Development in Information Retrieval<address><addrLine>Dublin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
