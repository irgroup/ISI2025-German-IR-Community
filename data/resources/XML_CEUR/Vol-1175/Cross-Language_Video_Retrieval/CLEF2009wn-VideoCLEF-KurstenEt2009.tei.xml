<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,98.52,110.86,414.95,15.15;1,91.39,132.78,429.23,15.15">Chemnitz at VideoCLEF 2009: Experiments and Observations on Treating Classification as IR Task</title>
				<funder ref="#_MKrR6gk #_2TAxUwY">
					<orgName type="full">German Federal Ministry of Education and Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,230.80,166.68,57.28,8.74"><forename type="first">Jens</forename><surname>Kürsten</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Faculty of Computer Science</orgName>
								<orgName type="department" key="dep2">Dept. Computer Science and Media</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.77,166.68,70.43,8.74"><forename type="first">Maximilian</forename><surname>Eibl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Faculty of Computer Science</orgName>
								<orgName type="department" key="dep2">Dept. Computer Science and Media</orgName>
								<orgName type="institution">Chemnitz University of Technology</orgName>
								<address>
									<postCode>09107</postCode>
									<settlement>Chemnitz</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,98.52,110.86,414.95,15.15;1,91.39,132.78,429.23,15.15">Chemnitz at VideoCLEF 2009: Experiments and Observations on Treating Classification as IR Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BEC995FAC955B78D68DA7ECFFB79175B</idno>
					<note type="submission">April 2007 to March 2012</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing Measurement</term>
					<term>Performance</term>
					<term>Experimentation Automatic Speech Transcripts</term>
					<term>Video Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of the Chemnitz University of Technology in the Video-CLEF 2009 classification task. Our motivation lies in its close relation to our research project sachsMedia 1 . In our second participation in the task we experimented with treating the task as IR problem and used the Xtrieval framework [3] to run our experiments. We proposed a automatic threshold estimation to limit the number of documents per label since too many returned documents hurt the overall correct classification rate. Although the experimental setup was enhanced this year and the data sets were changed we found that the IR approach still works quite well. Our query expansion approach performed better than the baseline experiments in terms of mean average precision. We also showed that combining the ASR transcriptions and the archival metadata improves the classification performance, unless query expansion is used in the retrieval phase.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Motivation</head><p>This article describes a system and its configuration that was used for our participation in the VideoCLEF classification task. The task was to categorize dual-language video into 46 given classes based on provided ASR transcripts <ref type="bibr" coords="1,144.29,612.46,10.52,8.74" target="#b4">[5]</ref> and additional archival metadata. In a mandatory experiment only the ASR transcripts of the videos had to be used as source for classification. Furthermore each of the given video documents can have none, one or even multiple labels. Hence the task can be characterized as a real world scenario in the field of automatic classification.</p><p>Our participation in the task is motivated by the its close relation to our research project sachsMedia 1 . The main goals of the project are twofold. The first main objective is automatic extraction of low level features from audio and video for automated annotation of poorly described material in archives. On the other hand sachsMedia aims to support local TV stations in Saxony to replace analog distribution technology with innovative digital distribution services. A special problem of the broadcasters is the accessibility of their archives for end users. Though we are currently developing algorithms for automatic extraction of low-level metadata the VideoCLEF classification task is a direct use case within our project. The remainder of the article is organized as follows. In section 2 we briefly review existing approaches and describe the system architecture and its main configuration. In sections 3 and 4 we present the results of preliminary and officially submitted experiments and interpret the results. A summary of our observations and experiences is given in section 5. The final section concludes the experiments with respect to our expectations and gives and outlook to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Architecture and Configuration</head><p>Since the classification task was an enhanced modification of last years VideoCLEF classification task <ref type="bibr" coords="2,527.85,236.39,9.96,8.74" target="#b3">[4]</ref>, we give a brief review on previously used approaches. There were mainly two distinct ways to approach the classification task: (a) collecting training data from external sources like general Web content or Wikipedia to train a text classifier or (b) treat the problem as information retrieval task. Villena and Lana <ref type="bibr" coords="2,486.56,272.25,10.52,8.74" target="#b7">[8]</ref> combined both ideas by obtaining training data from Wikipedia and assigning the class labels to the indexed training data. The metadata from the video documents were used as query on the training corpus and the dominant label of the retrieved documents was assigned as class label. Newman and Jones <ref type="bibr" coords="2,425.10,308.12,10.52,8.74" target="#b5">[6]</ref> as well as Perea-Ortega et. al. <ref type="bibr" coords="2,102.36,320.07,10.52,8.74" target="#b6">[7]</ref> approached the problem merely as IR task and achieved similar strong performance. Kürsten et. al. <ref type="bibr" coords="2,85.76,332.03,10.52,8.74" target="#b1">[2]</ref> and He et. al. <ref type="bibr" coords="2,163.98,332.03,10.52,8.74" target="#b0">[1]</ref> tried to solve the problem with state of the art classifiers like k-NN and SVM. Both used Wikipedia articles to train their classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Resources</head><p>Given the impressions from last year's evaluation and the huge success of the IR approaches as well as the enhancement of the task to a larger number of class labels and more documents, we decided to treat the problem as an IR task. Hence we used the Xtrieval framework <ref type="bibr" coords="2,372.38,414.17,10.52,8.74" target="#b2">[3]</ref> to create an index on the provided metadata. This index was composed of three fields, one with the ASR output, one with the archival metadata and a third one containing both. To process the tokens a language specific stopword list<ref type="foot" coords="2,467.83,436.51,3.97,6.12" target="#foot_0">2</ref> and the Dutch stemmer from the Snowball project 2 was applied. We used the class labels to query our video document index. The Lucene<ref type="foot" coords="2,152.08,460.42,3.97,6.12" target="#foot_2">4</ref> retrieval core with the default vector-based IR model was utilized within our framework. In the retrieval phase we used an English thesaurus<ref type="foot" coords="2,296.35,472.37,3.97,6.12" target="#foot_3">5</ref> in combination with the Google AJAX language API<ref type="foot" coords="2,536.66,472.37,3.97,6.12" target="#foot_4">6</ref> for query expansion purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">System Configuration and Parameters</head><p>The following list briefly explains some of our system parameters and their values for the experimental evaluation.</p><p>• Query Expansion (QE): The most frequent term from the top-5 documents was used to reformulate the original query.</p><p>• Thesaurus Term Query Expansion (TT): Thesaurus term query expansion was used for those queries, which returned less than two documents (even after QE).</p><p>• Multi-label Limit (DpL): DpL denotes the maximum number of assigned documents per class label and it was used to manually set a threshold for the document cut-off in the result sets.</p><p>• Source Field (SF): The metadata source was variated to indicate which source is most reliable and whether their combination yields to improvement of the classification or not.</p><p>Due to the problem of determining the document cut-off level a priori we calculated the following threshold for each query. The threshold is based on the scores of the retrieved documents per class label. Thereby denotes the average score and is the maximum score of the documents retrieved. stands for the total number of document retrieved for a specific class label. In this section we report results that were obtained by running various system configurations on the provided training data. In table 1 columns 2-5 refer to specific system parameters that were introduced in section 2.2. Please note that the utilization of the threshold formula is denoted with x in column DpL, which means that the number of assigned documents can be different for each class label.</p><p>Regarding the evaluation of the task we had a problem with calculating the measures. We report two values for MAP due to a peculiarity in our Xtrieval framework, which allows the system to return two documents with identical RSV. The trec eval 7 tool seems to penalize this behavior by randomly reordering the result set. Thus the MAP values reported by trec eval and our framework (labeled MAP* in the following tables) have marginal variations. Unfortunately we were neither able to correct the behavior of our system nor could we find out when or why the trec eval tool reorders our result sets. Thus, we decided to report both MAP values for our experiments in agreement with the task organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experiments on the Training Data</head><p>For evaluation of the classification performance the total number of assigned labels (SumL), the ratio of correct assigned labels (CR), averaged recall (AR) over all class labels and mean average precision (MAP) are reported. Table <ref type="table" coords="3,161.17,406.20,4.98,8.74" target="#tab_0">1</ref> is divided into three sections with respect to the used metadata sources. In the five rightmost columns the best values for each section of the table are emphasized bold and the best value over all sections is marked bold and italic. The following observations can be made by analyzing the experimental results. No matter which metadata source was used, the experiment without limitation of the class labels per document had the best performance in terms of AR and MAP (see ID's cut2, cut5 and cut11). The drawback of those runs is that they have very low correct classification rates (CR) of about 3% for the ASR data and about 8% when using archival metadata alone or in combination with ASR data. In contrast to that the experiments without any form of query expansion (see ID's cut1, cut4 and cut10) had the highest correct classification rates (CR) from 33% up to 47%. However, this is more a result from limiting to one document per label, which also yields to lower performance in terms of AR and MAP. Numerous experiments with either manual or automatic thresholds to limit the assigned documents per label were conducted. The results show that it is possible to improve CR substantially and almost sustain the best MAP values (compare cut5 to cut9 and cut11 to cut15). Nevertheless for those runs the AR was significantly lower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiments on the Test Data</head><p>In this section we report the experimental results on the evaluation data set. Please note that we run all configurations from section 3.1 again, because we wanted to figure out if our observations on the training data are also valid on the test data set. Experiments that were submitted for official evaluation by the organizers of the task are denoted with *. Again in table 2 columns 2-5 contain parameters of our system, which are briefly explained in section 2.2. The performance of the experiments is reported with respect to overall sum of assigned label (SumL), the average ratio of correct classifications (CR) as well as average recall (AR) and mean average precision (MAP). Corresponding to section 3.1 table 2 is also divided into three sections with respect to the used metadata sources. In the five rightmost columns the best values for each section of the table are emphasized bold and the best value over all sections is marked bold and italic. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Observations and Interpretation</head><p>In general we see similar behavior on both the training and the test data set. For all data sources used the best correct classification rate (CR) is achieved without using any form of query expansion (see ID's cut1, cut4 and cut10). The best overall (CR) was achieved by only using archival metadata in the retrieval phase. Since the archival metadata consists of intellectual annotations this is a very straightforward finding. Another obvious observation is, that the best overall results in terms of MAP and AR were also achieved on the archival metadata. Nevertheless the gap to the best results when combining ASR output with archival metadata is very small (compare cut5 to cut11). Regarding our proposed automatic threshold calculation for limitation of the number of assigned documents per label the results are twofold. On the one hand there is a slight improvement in terms of MAP and AR compared to low manually fixed thresholds between 1 and 3 assigned documents per label. On the other hand the overall correct classification rate (CR) decreases in the same magnitude MAP and AR are increasing, which is another very straightforward finding. The interpretation of our experimental results led us to the conclusion that using MAP for evaluating a multi-label classification task is somehow questionable. The main reason in our point of view is that MAP does not take into account the overall correct classification rate CR. Let us take a look on the two best performing experiments using archival metadata and ASR transcriptions either in table 1 or 2 (see ID's cut10 and cut15). The difference in terms of MAP is about 6% or 12%, but the gain in terms of CR is about 293% or 337% respectively. In our opinion in a real world scenario were assignment of class labels to video documents should be completely automatic it would be essential to take into account the overall ratio of correct assigned labels. Our prosposal for future evaluations is to combine measures that take into account the position of the correct assigned labels in a result set (like MAP or averaged R-Precision) with the micro or macro correct classification rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Result Analysis -Summary</head><p>The following list provides a short summary of our observations and findings from the participation in the VideoCLEF classification task in 2009.</p><p>• Classification as an IR task: According to the experiences from last year, we conclude that treating the given task as a traditional IR task with some modifications is a quite successful approach.</p><p>• Query Expansion: Both types of query expansion improved the results in terms of MAP and AR but had very low correct classification rates CR.</p><p>• Metadata Sources: Combining both ASR output and archival metadata improves MAP and AR when no query expansion is used. For those experiments where query expansion was used there is no gain in terms of MAP and AR comparing archival metadata runs to experiments which used both data sources.</p><p>• Label Limits: We compared an automatically calculated threshold to low manual set thresholds and found that the automatic threshold works better in terms of MAP and AR.</p><p>• Evaluation Measure: In our opinion using MAP as evaluation measure for a multi-label classification task is questionable. We would prefer a measure that takes into account both correct classification rate and averaged recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>This year we used the Xtrieval framework for the VideoCLEF classification task. In our experimental evaluation we can confirm the observations from last year, where approaches treating the task as IR problem were most successful. We proposed an automatic threshold to limit the number of assigned documents per class label to keep high correct classification rates. This seems to be the main issue that could be worked on in the future. A manual limitation of assigned documents per label is not an appropriate solution to a comparable real world problem, where possibly tens or hundred of thousand video documents should be labeled with maybe hundreds of different topic labels. Furthermore one could try to evaluate different retrieval models or try to combine the results from those models to gain a better overall performance. Finally it should be evaluated if assigning field boosts to the metadata sources could improve performance in the combined retrieval setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,81.96,450.42,436.54,237.00"><head>Table 1 :</head><label>1</label><figDesc>Evaluation Results on the Training Data</figDesc><table coords="3,93.50,471.76,425.00,196.43"><row><cell>ID</cell><cell>SF</cell><cell cols="3">QE TT DpL</cell><cell>SumL</cell><cell>CR</cell><cell cols="2">AR MAP*</cell><cell>MAP</cell></row><row><cell>cut1 l1 base</cell><cell>asr</cell><cell>no</cell><cell>0</cell><cell>1</cell><cell cols="2">33 0.3333</cell><cell cols="2">0.0558 0.0485</cell><cell>0.0485</cell></row><row><cell>cut2 l0 qe</cell><cell>asr</cell><cell>yes</cell><cell>5</cell><cell cols="2">∞ 1,566</cell><cell cols="4">0.0390 0.3096 0.1072 0.1099</cell></row><row><cell>cut3 l1 qe</cell><cell>asr</cell><cell>yes</cell><cell>5</cell><cell>1</cell><cell>181</cell><cell>0.1602</cell><cell cols="2">0.1472 0.0993</cell><cell>0.1006</cell></row><row><cell>cut4 l1 base</cell><cell>meta</cell><cell>no</cell><cell>0</cell><cell>1</cell><cell cols="2">70 0.4714</cell><cell cols="2">0.1675 0.1554</cell><cell>0.1546</cell></row><row><cell>cut5 l0 qe</cell><cell>meta</cell><cell>yes</cell><cell>5</cell><cell cols="2">∞ 1,932</cell><cell cols="4">0.0813 0.7970 0.4933 0.4999</cell></row><row><cell>cut6 l1 qe</cell><cell>meta</cell><cell>yes</cell><cell>5</cell><cell>1</cell><cell>188</cell><cell>0.3617</cell><cell cols="2">0.3452 0.2969</cell><cell>0.2985</cell></row><row><cell>cut7 l2 qe</cell><cell>meta</cell><cell>yes</cell><cell>5</cell><cell>2</cell><cell>312</cell><cell>0.3013</cell><cell cols="2">0.4772 0.3890</cell><cell>0.3928</cell></row><row><cell>cut8 l3 qe</cell><cell>meta</cell><cell>yes</cell><cell>5</cell><cell>3</cell><cell>368</cell><cell>0.3043</cell><cell cols="2">0.5685 0.4349</cell><cell>0.4395</cell></row><row><cell>cut9 lx qe</cell><cell>meta</cell><cell>yes</cell><cell>5</cell><cell>x</cell><cell>395</cell><cell>0.2886</cell><cell cols="2">0.5787 0.4361</cell><cell>0.4407</cell></row><row><cell cols="2">cut10 l1 base asr + meta</cell><cell>no</cell><cell>0</cell><cell>1</cell><cell cols="2">108 0.4537</cell><cell cols="2">0.2487 0.2177</cell><cell>0.2163</cell></row><row><cell>cut11 l0 qe</cell><cell cols="2">asr + meta yes</cell><cell>5</cell><cell cols="2">∞ 1,999</cell><cell cols="2">0.0795 0.8071</cell><cell cols="2">0.5036 0.4975</cell></row><row><cell>cut12 l1 qe</cell><cell cols="2">asr + meta yes</cell><cell>5</cell><cell>1</cell><cell>205</cell><cell>0.3659</cell><cell cols="2">0.3807 0.3056</cell><cell>0.3059</cell></row><row><cell>cut13 l2 qe</cell><cell cols="2">asr + meta yes</cell><cell>5</cell><cell>2</cell><cell>336</cell><cell>0.3036</cell><cell cols="2">0.5178 0.4035</cell><cell>0.3993</cell></row><row><cell>cut14 l3 qe</cell><cell cols="2">asr + meta yes</cell><cell>5</cell><cell>3</cell><cell>414</cell><cell>0.2874</cell><cell cols="2">0.6041 0.4574</cell><cell>0.4523</cell></row><row><cell>cut15 lx qe</cell><cell cols="2">asr + meta yes</cell><cell>5</cell><cell>x</cell><cell>470</cell><cell>0.2681</cell><cell cols="2">0.6396 0.4741</cell><cell>0.4689</cell></row></table><note coords="3,81.96,678.98,111.70,8.44"><p><p>7 </p>http://trec.nist.gov/trec eval</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,91.01,357.74,429.98,217.77"><head>Table 2 :</head><label>2</label><figDesc>Evaluation Results on the Test Data</figDesc><table coords="4,91.01,379.08,429.98,196.43"><row><cell>ID</cell><cell>SF</cell><cell cols="3">QE TT DpL</cell><cell>SumL</cell><cell>CR</cell><cell cols="2">AR MAP*</cell><cell>MAP</cell></row><row><cell>cut1 l1 base*</cell><cell>asr</cell><cell>no</cell><cell>0</cell><cell>1</cell><cell>27</cell><cell>0.0741</cell><cell cols="2">0.0101 0.0104</cell><cell>0.0067</cell></row><row><cell>cut2 l0 qe</cell><cell>asr</cell><cell>yes</cell><cell>5</cell><cell cols="2">∞ 1,966</cell><cell cols="4">0.0310 0.3065 0.1015 0.1010</cell></row><row><cell>cut3 l1 qe*</cell><cell>asr</cell><cell>yes</cell><cell>5</cell><cell>1</cell><cell cols="2">171 0.1111</cell><cell cols="2">0.0958 0.0848</cell><cell>0.0842</cell></row><row><cell>cut4 l1 base</cell><cell>meta</cell><cell>no</cell><cell>0</cell><cell>1</cell><cell cols="2">63 0.6349</cell><cell cols="2">0.2010 0.2004</cell><cell>0.2003</cell></row><row><cell>cut5 l0 base</cell><cell>meta</cell><cell>yes</cell><cell>5</cell><cell cols="2">∞ 1,778</cell><cell cols="2">0.0889 0.7940</cell><cell cols="2">0.4478 0.4505</cell></row><row><cell>cut6 l1 base</cell><cell>meta</cell><cell>yes</cell><cell>5</cell><cell>1</cell><cell>194</cell><cell>0.3763</cell><cell cols="2">0.3668 0.2863</cell><cell>0.2867</cell></row><row><cell>cut7 l2 base</cell><cell>meta</cell><cell>yes</cell><cell>5</cell><cell>2</cell><cell>300</cell><cell>0.3300</cell><cell cols="2">0.4975 0.3693</cell><cell>0.3706</cell></row><row><cell>cut8 l3 base</cell><cell>meta</cell><cell>yes</cell><cell>5</cell><cell>3</cell><cell>354</cell><cell>0.3051</cell><cell cols="2">0.5427 0.3974</cell><cell>0.4006</cell></row><row><cell>cut9 lx base</cell><cell>meta</cell><cell>yes</cell><cell>5</cell><cell>x</cell><cell>389</cell><cell>0.2853</cell><cell cols="2">0.5578 0.4039</cell><cell>0.4073</cell></row><row><cell cols="2">cut10 l1 base* meta + asr</cell><cell>no</cell><cell>0</cell><cell>1</cell><cell cols="2">112 0.5000</cell><cell cols="2">0.2814 0.2541</cell><cell>0.2586</cell></row><row><cell>cut11 l0 qe</cell><cell cols="2">meta + asr yes</cell><cell>5</cell><cell cols="2">∞ 1,885</cell><cell cols="2">0.0838 0.7940</cell><cell cols="2">0.4404 0.4389</cell></row><row><cell>cut12 l1 qe*</cell><cell cols="2">meta + asr yes</cell><cell>5</cell><cell>1</cell><cell>196</cell><cell>0.3622</cell><cell cols="2">0.3568 0.2552</cell><cell>0.2531</cell></row><row><cell>cut13 l2 qe</cell><cell cols="2">meta + asr yes</cell><cell>5</cell><cell>2</cell><cell>328</cell><cell>0.3018</cell><cell cols="2">0.4975 0.3712</cell><cell>0.3704</cell></row><row><cell>cut14 l3 qe*</cell><cell cols="2">meta + asr yes</cell><cell>5</cell><cell>3</cell><cell>393</cell><cell>0.2723</cell><cell cols="2">0.5379 0.3837</cell><cell>0.3813</cell></row><row><cell>cut15 lx qe</cell><cell cols="2">meta + asr yes</cell><cell>5</cell><cell>x</cell><cell>444</cell><cell>0.2455</cell><cell cols="2">0.5478 0.3869</cell><cell>0.3844</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0" coords="2,86.11,658.71,206.28,6.99"><p>http://snowball.tartarus.org/algorithms/dutch/stop.txt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1" coords="2,86.11,668.21,228.40,6.99"><p>http://snowball.tartarus.org/algorithms/dutch/stemmer.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2" coords="2,86.11,677.72,91.28,6.99"><p>http://lucene.apache.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3" coords="2,86.11,687.22,279.66,6.99"><p>http://de.openoffice.org/spellcheck/about-spellcheck-detail.html#thesaurus</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="2,86.11,696.72,217.17,6.99"><p>http://code.google.com/apis/ajaxlanguage/documentation</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank the <rs type="institution">VideoCLEF</rs> organizers and the <rs type="institution">Netherlands Institute of Sound and Vision (Beeld &amp; Geluid)</rs> for providing the data sources for the task. This work was accomplished in conjunction with the project sachsMedia, which is funded by the <rs type="programName">Entrepreneurial Regions 8 program</rs> of the <rs type="funder">German Federal Ministry of Education and Research</rs>.</p></div>
			</div>
			<div type="funding">
<div> <ref type="bibr" coords="1,81.96,689.71,3.65,5.24" target="#b0">1</ref> <p>Funded by the <rs type="programName">Entrepreneurial Regions program</rs> of the <rs type="funder">German Federal Ministry of Education and Research</rs> from</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_MKrR6gk">
					<orgName type="program" subtype="full">Entrepreneurial Regions 8 program</orgName>
				</org>
				<org type="funding" xml:id="_2TAxUwY">
					<orgName type="program" subtype="full">Entrepreneurial Regions program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="6,86.36,140.75,454.77,8.74;6,86.36,152.70,416.28,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,347.41,140.75,193.72,8.74">The University of Amsterdam at VideoCLEF</title>
		<author>
			<persName coords=""><forename type="first">Jyin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wouter</forename><surname>Weerkamp</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martha</forename><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,113.47,152.70,196.35,8.74">Working Notes for the CLEF 2008 Workshop</title>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09">2008. 17-19 September. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,86.36,172.63,454.77,8.74;6,86.36,184.58,454.77,8.74;6,86.36,196.54,22.69,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,327.09,172.63,214.04,8.74;6,86.36,184.58,90.94,8.74">VideoCLEF 2008: ASR Classification based on Wikipedia Categories</title>
		<author>
			<persName coords=""><forename type="first">Jens</forename><surname>Kürsten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maximlian</forename><surname>Eibl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,184.81,184.58,191.99,8.74">Working Notes for the CLEF 2008 Workshop</title>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09-19">17-19 September. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,86.36,216.46,454.77,8.74;6,86.36,228.42,454.77,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,328.87,216.46,212.27,8.74;6,86.36,228.42,33.48,8.74">Extensible Retrieval and Evaluation Framework: Xtrieval</title>
		<author>
			<persName coords=""><forename type="first">Jens</forename><surname>Kürsten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maximilian</forename><surname>Eibl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,127.43,228.42,170.86,8.74">LWA 2008: Lernen -Wissen -Adaption</title>
		<title level="s" coord="6,417.03,228.42,94.22,8.74">Workshop Proceedings</title>
		<meeting><address><addrLine>Würzburg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10">October 2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,86.36,248.34,454.77,8.74;6,86.36,260.30,454.77,8.74;6,86.36,272.25,233.91,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,324.89,248.34,216.25,8.74;6,86.36,260.30,295.29,8.74">Overview of VideoCLEF 2008: Automatic Generation of Topic-based Feeds for Dual Language Audio-Visual Content</title>
		<author>
			<persName coords=""><forename type="first">Martha</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eamonn</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gareth</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,390.43,260.30,150.71,8.74;6,86.36,272.25,41.10,8.74">Working Notes for the CLEF 2008 Workshop</title>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09-19">17-19 September. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,86.36,292.18,454.77,8.74;6,86.36,304.13,454.77,8.74;6,86.36,316.09,244.52,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,318.87,292.18,222.27,8.74;6,86.36,304.13,200.96,8.74">Overview of VideoCLEF 2009: New Perspectives on Speech-based Multimedia Content Enrichment</title>
		<author>
			<persName coords=""><forename type="first">Martha</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eamonn</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gareth</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,121.87,316.09,107.01,8.74">Working Notes of CLEF</title>
		<editor>
			<persName><forename type="first">Francesca</forename><surname>Borri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alessandro</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009-09">2009. September 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,86.36,336.01,454.77,8.74;6,86.36,347.97,233.91,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,276.79,336.01,80.74,8.74">DCU at VideoClef</title>
		<author>
			<persName coords=""><forename type="first">Eamonn</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,388.65,336.01,152.49,8.74;6,86.36,347.97,41.10,8.74">Working Notes for the CLEF 2008 Workshop</title>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09">2008. 17-19 September. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,86.36,367.89,454.77,8.74;6,86.36,379.85,389.17,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,422.14,367.89,93.67,8.74">SINAI at VideoCLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arturo</forename><surname>Perea-Ortega</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montejo-Raéz</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Teresa Martín-Valdivia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,86.36,379.85,196.35,8.74">Working Notes for the CLEF 2008 Workshop</title>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09">2008. 17-19 September. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,86.36,399.78,454.77,8.74;6,86.36,411.73,454.77,8.74;6,86.36,423.69,68.51,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,294.83,399.78,246.30,8.74;6,86.36,411.73,121.38,8.74">MIRACLE at VideoCLEF 2008: Classification of Multilingual Speech Transcripts</title>
		<author>
			<persName coords=""><forename type="first">Julio</forename><surname>Villena-Román</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sara</forename><surname>Lana-Serrano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,218.41,411.73,199.90,8.74">Working Notes for the CLEF 2008 Workshop</title>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09-19">17-19 September. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
