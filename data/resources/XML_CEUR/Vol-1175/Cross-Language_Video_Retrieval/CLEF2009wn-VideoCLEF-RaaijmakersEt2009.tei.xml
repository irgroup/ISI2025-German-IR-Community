<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,112.68,147.79,377.38,17.20">A cocktail approach to the VideoCLEF&apos;09 linking task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,201.96,182.13,92.43,9.96"><forename type="first">Stephan</forename><surname>Raaijmakers</surname></persName>
							<email>stephan.raaijmakers@tno.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">TNO Information and Communication Technology</orgName>
								<address>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,335.91,182.13,65.08,9.96"><forename type="first">Corn√©</forename><surname>Versloot</surname></persName>
							<email>corne.versloot@tno.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">TNO Information and Communication Technology</orgName>
								<address>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,273.12,196.05,56.58,9.96"><forename type="first">Joost</forename><surname>De Wit</surname></persName>
							<email>joost.dewit@tno.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">TNO Information and Communication Technology</orgName>
								<address>
									<settlement>Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,112.68,147.79,377.38,17.20">A cocktail approach to the VideoCLEF&apos;09 linking task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4990F65534E791D0298BF366E2415396</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.2.7 [Artificial Intelligence]: Natural Language Processing -text analysis; I.3.1 [Information Storage and retrieval]: Content Analysis and indexing -linguistic processing Algorithms</term>
					<term>Experimentation Wikipedia</term>
					<term>Data Mining</term>
					<term>Semantic Annotation</term>
					<term>Word Sense Disambiguation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe the TNO approach to the Finding Related Resources or linking task of VideoCLEF09. Our system consists of a weighted combination of off-theshelf and proprietary modules, including the Wikipedia Miner toolkit of the University of Waikato. Using this cocktail of largely off-the-shelf technology allows for setting a baseline for future approaches to this task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Finding Related Resources or linking task of VideoCLEF'09 consists of relating Dutch automatically transcribed TV speech to English Wikipedia content. For a total of 45 video episodes, a total of 165 anchors (speech transcripts) have to be linked to related Wikipedia articles. Technology emerging from this task will contribute to a better understanding of Dutch video for non-native speakers.</p><p>The TNO approach to this problem consists of a cocktail of off-the-shelf techniques. Central to our approach is the use of the Wikipedia Miner toolkit developed by researchers at the University of Waikato<ref type="foot" coords="1,137.16,625.97,3.95,6.97" target="#foot_1">2</ref> (see <ref type="bibr" coords="1,164.98,627.33,10.29,9.96" target="#b3">[4]</ref>). The so-called Wikifier functionality of the toolkit detects Wikipedia topics from raw text, and generates cross-links from input text to a relevance-ranked list of Wikipedia pages.</p><p>We investigated two possible options for bridging the gap between Dutch input text and English Wikipedia pages: translating queries to English prior to the detection of Wikipedia topics, and translating Wikipedia topics detected in Dutch texts to English Wikipedia topics. In the latter case, the use of Wikipedia allows for an abstraction of raw queries to Wikipedia topics, for which the translation process in theory is less complicated and error prone. Specific to our approach is a weighted combination of various modules, and the use of a specially developed part-of-speech tagger for uncapitalized speech transcripts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System setup</head><p>In this section, we describe the setup of our system. In subsections 2.1, 2.2 and 2.3, we describe the essential ingredients of our system. In subsection 2.4, we define a number of linking strategies based on these basic ingredients, which are combined into scenarios for our runs (section 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">From Dutch to English</head><p>For the translation of Dutch text to English (and following <ref type="bibr" coords="2,355.58,260.25,10.29,9.96" target="#b0">[1]</ref>), we used the Yahoo! BabelFish translation service <ref type="foot" coords="2,169.68,270.77,3.95,6.97" target="#foot_2">3</ref> . An example of the output of this service is given in Figure <ref type="figure" coords="2,439.84,272.13,3.90,9.96" target="#fig_0">1</ref>. Since people, organizations and locations often have entries in Wikipedia, accurate proper name detection is important for this task. Erroneous translation to English of Dutch names (e.g. 'Frans Hals' becoming 'French Neck') should be avoided. Proper name detection prior to translation allows for exempting the detected names from translation. A complicating factor is formed by the fact that the transcribed speech in the various broadcastings is in lowercase, which makes the recognition of proper names challenging, since important capitalization features can no longer be used. In order to address this problem, we trained a maximum entropy part-of-speech tagger: an instance of the Stanford tagger<ref type="foot" coords="2,223.32,641.21,3.95,6.97" target="#foot_3">4</ref> (see <ref type="bibr" coords="2,249.82,642.57,10.22,9.96" target="#b4">[5]</ref>). The tagger was trained on a 700K part-of-speech tagged corpus of Dutch, after having decapitalized the training data. The feature space consists of a 5-cell bidirectional window addressing part-of-speech ambiguities and prefix and suffix features up to a size of 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">A Dutch Wikifier</head><p>The imperfect English translation by Babel Fish was observed to be the main reason for erroneous Wikify results. In order to omit the translation step, we ported the English Wikifier of the Wikipedia Miner toolkit to Dutch, for which we used the Dutch Wikipedia dump and Perl scripts provided by developers of the Wikipedia Miner toolkit. The resulting Dutch Wikifier ('NL Wikifier' in Figure <ref type="figure" coords="3,132.57,177.57,4.43,9.96" target="#fig_2">3</ref>) has exactly the same functionality as the English version, but unfortunately contains a lot less pages than the English version (a factor 6 less). Even so, the translation process now is narrowed down to translating detected Wikipedia topics (the output of the Dutch Wikify step) to English Wikipedia topics. For the latter, we implemented a simple database facility (to which we shall refer with 'The English Topic Finder') that uses the cross-lingual links between topics in the Wikipedia database for carrying out the translation of Dutch topics to English topics.</p><p>An example of the output of the English and Dutch Wikifiers for the query in Figure <ref type="figure" coords="3,472.64,249.33,5.03,9.96" target="#fig_0">1</ref> is given in Figure <ref type="figure" coords="3,133.66,261.21,3.90,9.96" target="#fig_1">2</ref>. The different rankings of the various detected topics are represented as a tag cloud with different font sizes, and can be extracted as numerical scores from the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Text retrieval</head><p>In order to be able to entirely by-pass the Wikipedia Miner toolkit, we deployed the Lucene search engine ( <ref type="bibr" coords="3,157.07,551.49,10.93,9.96" target="#b1">[2]</ref>) for performing the matching of raw, translated text with Wikipedia pages. Lucene was used to index the Dutch Wikipedia with the standard Lucene indexing options. Dutch speech transcripts were simply provided to Lucene as a a disjunctive (OR) query, with Lucene returning the best matching Dutch Wikipedia pages for the query. The HTML of these pages was subsequently parsed in order to extract the English Wikipedia page references (which are indicated in Wikipedia, whenever present).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Linking strategies</head><p>The set of techniques just described leads to a total of four basic linking strategies.</p><p>Of the various combinatorial possibilities of these strategies, we selected 5 combinations for our submitted runs. The basic linking strategies are: Strategy 1: proper names only (the top row in Figure <ref type="figure" coords="3,360.89,707.37,4.43,9.96" target="#fig_2">3</ref>) Following proper name recognition, a quasi-document is created that only consists of all recognized proper names. The Dutch Wikify tool is used to produce a ranked list of Dutch Wikipedia pages for this quasi-document. Subsequently, the topics of these pages are linked to English Wikipedia pages with the English Topic Finder.</p><p>Strategy 2: proper names preservation (second row in Figure <ref type="figure" coords="4,394.91,322.77,4.43,9.96" target="#fig_2">3</ref>) Dutch text is translated to English with Babelfish. Any proper names found in the part-of-speech tagged Dutch text are added to the translated text as untranslated text, after which the English Wikifier is applied, producing a ranked list of matching Wikipedia pages.</p><p>Strategy 3: topic to topic linking (3rd row from the top in Figure <ref type="figure" coords="4,415.70,384.45,4.43,9.96" target="#fig_2">3</ref>) The original Dutch text is wikified using the Dutch Wikify tool, producing a ranked list of Wikipedia pages. The topics of these pages are subsequently linked to English Wikipedia pages with the English Topic finder.</p><p>Strategy 4: text to page linking (bottom row in Figure <ref type="figure" coords="4,350.57,446.25,4.43,9.96" target="#fig_2">3</ref>) Lucene matches queries with Dutch Wikipedia pages. The English topic finder tries to find the corresponding English Wikipedia pages for the Dutch topics in the pages returned by Lucene. This strategy omits the use of the Wikifier and was used as a fall-back option, if none of the other modules delivered a result.</p><p>A thresholded merging algorithm removes any results below an estimated threshold and blends the remaining results into a single ordered list of Wikipedia topics, using estimated weights for the various sources of these results. Several different merging techniques were used for different runs; these will be discussed in subsection 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Run scenarios</head><p>In this section, we describe the configurations of the 5 runs we submitted. We were specifically interested in the effect of proper name recognition, the relative contributions of the Dutch and English Wikifiers, and the effect of full-text Babelfish translation as compared to a topic-to-topic translation approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 1</head><p>All four linking strategies were used to produce the first run. A weighted merger ('Merger' in Figure <ref type="figure" coords="4,120.94,698.61,4.43,9.96" target="#fig_2">3</ref>) was used to merge the results from the different strategies. The merger works as follows:</p><p>1. English Wikipedia pages referring to proper names are uniformly ranked before all other results.</p><p>2. The rankings produced by the second linking strategy (rank EN ) and third linking strategy (rank DU ) for any returned Wikipedia page p are combined according to the following scheme:</p><formula xml:id="formula_0" coords="5,191.64,144.92,321.26,17.29">rank(p) = ((rank EN (p) * 0.2) + (rank DU (p) * 0.8)) * 1.4<label>(1)</label></formula><p>The Dutch score was found to be more relevant than the English one (hence the 0.8 vs. 0.2 weights). The sum of the Dutch and English score is boosted with an additional factor of 1.4, awarding the fact that both linking strategies come up with the same result.</p><p>3. Pages found by linking strategy 2 but not by linking strategy 3 are added to the result and their ranking score is boosted with a factor of 1.1.</p><p>4. Pages found by linking strategy 3 but not by linking strategy 2 are added to the result (but their ranking score is not boosted).</p><p>5. If linking strategies 1 to 3 did not produce results, the results of linking strategy 4 are added to the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 2</head><p>Run 2 is the same as run 1 with the exception that linking strategy 1 is left out (no proper name recognition).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 3</head><p>Run 3 is similar to run 1, but does not boost results at the merging stage, and averages the rankings of the second and third linking strategy. This means that the weights used by the merger in run 1 (0.8, 0.2 and 1.4) are resp. 0.5, 0.5 and 1.0 for this run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 4</head><p>Run 4 only uses linking strategy 1 and 3. This means that no translation from Dutch to English is performed. In the result set, the Wikipedia pages returned by linking strategy 1 are ordered before the results from linking strategy 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 5</head><p>Run 5 uses all linking strategies except linking strategy 1 (it omits proper name detection). In this run a different merging strategy is used:</p><p>1. If linking strategy 2 produces any results, add those to the final result set and then stop.</p><p>2. If linking strategy 2 produces no results, but linking strategy 3 does, add those to the final result and stop.</p><p>3. If none of the preceding linking strategies produces any results, add the results from linking strategy 3 to the final result set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and discussion</head><p>For VideoCLEF'09, two groups submitted runs for the linking task: Dublin City University and TNO. Two evaluation methods were applied by the task organizers to the submitted results. A team of assessors first achieved consensus on a primary link (the most important or descriptive Wikipedia article), with a minimum consensus among 3 people. All queries in each submitted run were scored for Mean Reciprocal Rank 5 for this primary link, as well as for recall. Subsequently, the annotators agreed on a set or related resources that necessarily included the primary link, in addition to secondary relevant links (minimum consensus of one person). Since this list of secondary links is non-exhaustive, for this measure only MRR is reported, and not recall. As it turns out, the unweighted combination of results (run 3) outperforms all other runs, followed by the thresholded combination (run 1). This indicates that the weights in the merging step are suboptimal. Omitting proper name recognition results in a noticeable drop of performance under both evaluation methods, underlining the importance of proper names for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run</head><p>In addition to the recall and MRR scores, the assessment team distributed the graded relevance scores (see <ref type="bibr" coords="6,140.70,539.01,10.70,9.96" target="#b2">[3]</ref>) assigned to all queries. In Figure <ref type="figure" coords="6,313.12,539.01,3.90,9.96">4</ref>, we plotted the difference per query of the obtained averaged relevance score with the total average obtained relevance scores for both the Dublin City University (DCU) and TNO runs. For every video, we averaged the relevance scores of the hits reported by DCU and TNO. Subsequently, for every TNO run, we averaged relevance scores for every query, and measured the difference with the averaged DCU and TNO runs. It can be clearly seen that Run 1 and 3 obtain the best results, producing only a small amount of queries below the mean. Most of the relevance results obtained from these runs are around the mean, showing that from the perspective of relevance quality, our best runs produce average results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this contribution, we have taken a technological and off-the-shelf-oriented approach to the problem of linking Dutch transcripts to English Wikipedia pages. Using a blend of commonly available 5 For a response r = r 1 , . . . , r Q to a ranking task, the MRR would be M RR = 1</p><formula xml:id="formula_1" coords="6,399.36,708.32,50.64,30.22">|Q| Q i=1 1 rank i</formula><p>, with rank i the rank of answer r i with respect to the correct answer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,186.60,534.93,229.61,9.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The result of Babelfish for a sample query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,136.20,469.29,330.46,9.96;3,131.45,357.02,339.76,87.89"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The result of the English and Dutch Wikifiers for a sample query.</figDesc><graphic coords="3,131.45,357.02,339.76,87.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,237.24,243.09,128.41,9.96;4,159.72,108.84,283.40,119.76"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: TNO system setup.</figDesc><graphic coords="4,159.72,108.84,283.40,119.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,90.00,169.89,422.73,262.44"><head>Table 1 :</head><label>1</label><figDesc>Recall and MRR for the primary link evaluation by TNO. (Average DCU scores were 0.21 and 0.14, resp.)</figDesc><table coords="6,233.88,169.89,135.18,262.44"><row><cell></cell><cell cols="2">Recall MRR</cell></row><row><cell>1</cell><cell>0.345</cell><cell>0.23</cell></row><row><cell>2</cell><cell cols="2">0.333 0.215</cell></row><row><cell>3</cell><cell cols="2">0.352 0.251</cell></row><row><cell>4</cell><cell cols="2">0.267 0.182</cell></row><row><cell>5</cell><cell cols="2">0.285 0.197</cell></row><row><cell>Average TNO</cell><cell>0.32</cell><cell>0.215</cell></row><row><cell>Run</cell><cell cols="2">MRR</cell></row><row><cell>1</cell><cell cols="2">0.46</cell></row><row><cell>2</cell><cell cols="2">0.428</cell></row><row><cell>3</cell><cell cols="2">0.484</cell></row><row><cell>4</cell><cell cols="2">0.392</cell></row><row><cell>5</cell><cell cols="2">0.368</cell></row><row><cell cols="3">Average TNO 0.43</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,105.60,444.69,391.53,9.96"><head>Table 2 :</head><label>2</label><figDesc>MRR for the secondary link evaluation by TNO. (Average DCU score was 0.21.)</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,105.24,717.98,407.25,7.97;1,90.00,727.46,422.40,7.97;1,90.00,736.94,24.94,7.97"><p>This work is supported by the European IST Programme Project FP6-0033812. This paper only reflects the authors' views and funding agencies are not liable for any use that may be made of the information contained herein.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,105.24,746.42,174.89,7.97"><p>See http://wikipedia-miner.sourceforge.net</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,105.24,695.95,113.31,7.42"><p>http://babelfish.yahoo.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,105.24,705.43,189.49,7.42"><p>http://nlp.stanford.edu/software/tagger.shtml</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>software resources (Babelfish, the Waikato Wikipedia Miner Toolkit, Lucene, and the Stanford maximum entropy part-of-speech tagger), we demonstrated that an unweighted combination produces competitive results. We hope to have demonstrated that this low-entry approach can be used as a baseline level that can inspire future approaches to this problem. A more accurate estimation of weights for the contribution of several sources of information can be carried out in future benchmarks, now that the VideoClef annotators have produced ground truth ranking data.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,105.47,640.65,407.22,9.96;7,105.48,652.65,406.97,9.96;7,105.48,664.65,217.33,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,267.71,640.65,244.98,9.96;7,105.48,652.65,42.52,9.96">Finding Similar Sentences across Multiple Languages in Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">F</forename><surname>Adafre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,169.64,652.65,342.81,9.96;7,105.48,664.65,128.82,9.96">Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 11th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="62" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.47,684.57,407.30,9.96;7,105.48,696.45,142.43,9.96" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,261.57,684.57,72.78,9.96">Lucene in Action</title>
		<author>
			<persName coords=""><forename type="first">Erik</forename><surname>Hatcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Otis</forename><surname>Gospodnetic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Manning Publications Co</publisher>
			<pubPlace>Greenwich, CT, USA</pubPlace>
		</imprint>
	</monogr>
	<note>In Action series</note>
</biblStruct>

<biblStruct coords="7,105.47,716.37,407.21,9.96;7,105.48,728.37,240.69,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,282.97,716.37,225.40,9.96">Using graded relevance assessments in IR evaluation</title>
		<author>
			<persName coords=""><forename type="first">Jaana</forename><surname>Kek√§l√§inen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kalervo</forename><surname>J√§rvelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,105.48,728.37,128.76,9.96">J. Am. Soc. Inf. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1120" to="1129" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.47,111.33,407.07,9.96;8,105.48,123.33,407.22,9.96;8,105.48,135.21,99.87,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,251.53,111.33,138.93,9.96">Learning to link with Wikipedia</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,412.46,111.33,100.08,9.96;8,105.48,123.33,286.79,9.96">Proceedings of the 17th ACM conference on Information and knowledge mining (CIKM&apos;08)</title>
		<meeting>the 17th ACM conference on Information and knowledge mining (CIKM&apos;08)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="509" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.47,155.25,407.19,9.96;8,105.48,167.13,407.12,9.96;8,105.48,179.13,406.97,9.96;8,105.48,191.01,111.45,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,327.65,155.25,185.01,9.96;8,105.48,167.13,176.01,9.96">Enriching the knowledge sources used in a maximum entropy part-of-speech tagger</title>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,302.10,167.13,210.50,9.96;8,105.48,179.13,406.97,9.96;8,105.48,191.01,22.47,9.96">Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000)</title>
		<meeting>the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
