<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,188.43,148.79,226.14,15.48;1,97.91,170.71,407.17,15.48;1,257.51,192.62,87.99,15.48">Overview of VideoCLEF 2009: New Perspectives on Speech-based Multimedia Content Enrichment</title>
				<funder ref="#_yajHhHc">
					<orgName type="full">European Commission&apos;s</orgName>
				</funder>
				<funder ref="#_tpFnSPr">
					<orgName type="full">PetaMedia Network of Excellence</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,182.51,226.71,58.93,8.64"><forename type="first">Martha</forename><surname>Larson</surname></persName>
							<email>m.a.larson@tudelft.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Mediamatics</orgName>
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,257.87,226.71,71.41,8.64"><forename type="first">Eamonn</forename><surname>Newman</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,345.70,226.71,70.32,8.64"><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Centre for Digital Video Processing</orgName>
								<orgName type="institution">Dublin City University</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,188.43,148.79,226.14,15.48;1,97.91,170.71,407.17,15.48;1,257.51,192.62,87.99,15.48">Overview of VideoCLEF 2009: New Perspectives on Speech-based Multimedia Content Enrichment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">32C434A4F333AA7A923DBB05CC25E2A4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries Measurement, Performance, Experimentation Video retrieval, Classification, Affect, Multimedia linking, Semantic theme classification, Speech recognition, Narrative peaks, Documentaries</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>VideoCLEF 2009 offered three tasks related to enriching video content for improved multimedia access in a multilingual environment. For each task, video data (Dutch-language television, predominantly documentaries) accompanied by speech recognition transcripts were provided. The Subject Classification Task involved automatic tagging of videos with subject theme labels. The best performance was achieved by approaching subject tagging as an information retrieval task and using both speech recognition transcripts and archival metadata. Alternatively, classifiers were trained using either the training data provided or data collected from Wikipedia or via general Web search. The Affect Task involved detecting narrative peaks, defined as points where viewers perceive heightened dramatic tension. The task was carried out on the "Beeldenstorm" collection containing 45 short-form documentaries on the visual arts. The best runs exploited affective vocabulary and audience directed speech. Other approaches included using topic changes, elevated speaking pitch, increased speaking intensity and radical visual changes. The Linking Task, also called "Finding Related Resources Across Languages," involved linking video to material on the same subject in a different language. Participants were provided with a list of multimedia anchors (short video segments) in the Dutch-language "Beeldenstorm" collection and were expected to return target pages drawn from English-language Wikipedia. The best performing methods used the transcript of the speech spoken during the multimedia anchor to build a query to search an index of the Dutchlanguage Wikipedia. The Dutch Wikipedia pages returned were used to identify related English pages. Participants also experimented with pseudo-relevance feedback, query translation and methods that targeted proper names.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>VideoCLEF 2009 <ref type="foot" coords="2,160.68,132.94,3.49,6.05" target="#foot_0">1</ref> is a track of the CLEF<ref type="foot" coords="2,256.32,132.94,3.49,6.05" target="#foot_1">2</ref> benchmark campaign and is devoted to tasks aimed at improving access to video content in multilingual environments. VideoCLEF develops new video retrieval related tasks and data sets with which to evaluate these tasks. During VideoCLEF 2009, three tasks were carried out. The Subject Classification Task required participants to automatically tag videos with subject theme labels (e.g., 'factories,' 'physics,' 'poverty', 'cultural identity' and 'zoos'). The Affect Task, also called "Narrative peak detection," involved automatically detecting dramatic tension in short-form documentaries. Finally, "Finding Related Resources Across Languages," referred to as the Linking Task, required participants to automatically link video to Web content that is in a different language, but on the same subject. The data sets for these tasks contained Dutch-language television content supplied by the Netherlands Institute of Sound and Vision <ref type="foot" coords="2,171.16,240.53,3.49,6.05" target="#foot_2">3</ref> (called in Dutch Beeld &amp; Geluid), which is one of the largest audio/video archives in Europe. Each participating site had access to video data, speech recognition transcripts, shot boundaries, shot-level keyframes and archival metadata supplied by VideoCLEF. Sites developed their own approaches to the tasks and were allowed to chose the method and features that they found most appropriate. Seven groups made submissions of task results for evaluation.</p><p>In 2009, the VideoCLEF track ran for the first time as a full track within the Cross-Language Evaluation Forum (CLEF) evaluation campaign. The track was piloted last year as VideoCLEF 2008 <ref type="bibr" coords="2,478.82,313.93,10.58,8.64" target="#b5">[6]</ref>. The VideoCLEF track is successor to the Cross-Language Speech Retrieval (CL-SR) track, which ran at CLEF from 2005 to 2007 <ref type="bibr" coords="2,166.70,337.84,10.58,8.64" target="#b6">[7]</ref>. VideoCLEF seeks to extend the results of CL-SR to the broader challenge of video retrieval. VideoCLEF is intended to complement the TRECVid benchmark <ref type="bibr" coords="2,392.36,349.80,11.62,8.64" target="#b7">[8]</ref> by running tasks related to the subject matter treated by video and emphasizing the importance of spoken content (via speech recognition transcripts). TRECVid has traditionally focused on what is depicted in the visual channel. In contrast, VideoCLEF concentrates on what is described in a video, in other words, what a video is about.</p><p>This paper describes the data sets and the tasks of VideoCLEF 2009 and summarizes the results achieved by the participating sites. We finish with a conclusion and an outlook for VideoCLEF 2010. For additional information concerning individual approaches used in 2009, please refer to the working notes papers of the individual sites.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Data</head><p>VideoCLEF 2009 used two data sets both containing Dutch-language television programs. Note that these programs are predominantly documentaries with the addition of some talk shows. This means that the data contains a great deal of conversational speech, including opinionated and subjective speech and speech that has been only loosely planned. In this way, the VideoCLEF data is different and more challenging than broadcast news data which largely involves scripted speech.</p><p>The VideoCLEF 2009 Subject Classification Task ran on TRECVid 2007/2008 data from Beeld &amp; Geluid. The Affect Task and Linking Task both ran on a data set containing material from the short-form documentary Beeldenstorm, also supplied by Beeld &amp; Geluid. For both data sets, Dutch-language speech recognition transcripts were supplied by the University of Twente <ref type="bibr" coords="2,365.19,576.32,10.58,8.64" target="#b2">[3]</ref>. The shot segmentation and the shot-level keyframe data were provided by Dublin City University <ref type="bibr" coords="2,365.80,588.28,10.58,8.64" target="#b0">[1]</ref>. Further details are given in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.1">TRECVid 2007/2008 data set</head><p>In 2009, VideoCLEF attempted to encourage cross-over from the TRECVid community by recycling the TRECVid data set for the Subject Classification Task. Notice that the Subject Classification Task is a fundamentally different task than what ran at TRECVid in 2007 and 2008. Subject Classification involves automatically assigning subject labels to videos at the episode level. The subject matter of the entire video is important, not just the concepts visible in the visual channel and not just the shot-level topic.</p><p>Classifying video, i.e., taking a video and assigning it a topic class subject label, is exactly what the archive staff does at Sound and Vision when they annotate video material that is to be stored in the archive. The class labels used for the VideoCLEF 2009 Subject Classification Task are a subset of labels that are used by archive staff. As a result, we (1) have gold standard topic class labels with which to evaluate classification (2) can be relatively certain that if these labels are already used for retrieval of material from the archive then they are relevant for video search in an archive setting, and we assume, beyond. Original Dutch-language examples of subject labels can be examined in the search engine. <ref type="foot" coords="3,414.51,182.40,3.49,6.05" target="#foot_3">4</ref>In the VideoCLEF 2009 Subject Classification Task, archivist-assigned subject labels were used as ground truth. <ref type="foot" coords="3,142.51,206.31,3.49,6.05" target="#foot_4">5</ref> The training set is a large subset of TRECVid 2007 and contains 212 videos. The test set is a large subset of TRECVid 2008 and contains 206 videos. Each video is an individual episode of a television show. Their length varies widely with the average length being around 30 minutes. Participants were also free to collection their own training data, if they wished. Note that the VideoCLEF 2009 Subject Classification set excludes several videos in the TRECVid collection for which archival metadata was not available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.2">Beeldenstorm data set</head><p>For both the Affect Task and Linking Task a data set consisting of 45 episodes of the documentary series Beeldenstorm (Eng. Iconoclasm) was used. The Beeldenstorm series consists of short-form Dutchlanguage video documentaries about the visual arts. Each episode lasts approximately eight minutes. Beeldenstorm is hosted by Prof. Henk van Os, known and widely appreciated, not only for his art expertise, but also for his narrative ability. This data set is also supplied by Beeld &amp; Geluid, but it is mutually exclusive with the TRECVid 2007/2008 data set. The narrative ability of Prof. van Os makes the Beeldenstorm set an interesting corpus to use for affect detection and the domain of visual arts offers a wide number of possibilities for interesting multimedia links for the linking task. Finally, the fact that each episode is short makes it possible for assessors to watch the entire episode when creating the ground truth. Knowledge of the complete context is important for relevance judgments for cross-language related resources and also for defining narrative peaks.</p><p>The ground truth for the Affect Task and Linking Task was created by a team of three Dutch-speaking assessors during a nine-day assessment and annotation event at Dublin City University referred to as Dublin Days. The videos were annotated with the ground truth with the support of the Anvil<ref type="foot" coords="3,434.57,466.71,3.49,6.05" target="#foot_5">6</ref> Video Annotation Research Tool <ref type="bibr" coords="3,149.21,480.33,10.58,8.64" target="#b4">[5]</ref>. Anvil makes it possible to generate frame-accurate video annotations in a graphic interface. Particularly important for our purposes was the support offered by Anvil for user-defined annotation schemes. Details of the ground truth creation are included in the discussions of the individual tasks in the following section.</p><p>2 Subject Classification Task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task</head><p>The goal of the Subject Classification Task is automatic subject tagging. Theme-based subject tags are assigned automatically to videos. The purpose of these tags is to make the videos findable to users that are searching and browsing the collection. The information needs (i.e., queries) of the users are not specified at the time of tagging. In VideoCLEF 2009, the Subject Classification Task had the specific goal of reproducing the subject labels that were hand assigned to the test set videos by archivists at Beeld &amp; Geluid. Since these subject labels are currently in use to archive and retrieve video in the setting of a large archive, we are confident in their usefulness for search and browsing in real-world information retrieval scenarios. The Subject Classification Task was introduced during the VideoCLEF 2008 pilot <ref type="bibr" coords="4,415.69,160.16,10.58,8.64" target="#b5">[6]</ref>. In 2009, the number of videos in the collection was increased from 50 to 418 and the number of subject labels increased from 10 to 46.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation</head><p>The Subject Classification Task is evaluated using Mean Average Precision (MAP). This choice of score is motivated by the popularity of techniques that approach the subject tagging task as an information retrieval problem. These techniques return, for each subject label, a ranked list of videos that should receive that label. MAP is calculated by taking the mean of the Average Precision over all subject labels. For each subject label, precision scores are calculated by moving down the results list and calculating precision at each position where a relevant document is retrieved. Average Precision is calculated by taking the average of the precision at each position. Calculations were performed using version 8.1 of the trec eval<ref type="foot" coords="4,509.02,301.33,3.49,6.05" target="#foot_6">7</ref> scoring package.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Techniques</head><p>Computer Science, Chemnitz University of Technology, Germany The task was treated as an information retrieval task. The test set was indexed using an information retrieval system and was queried using the subject labels as queries. Documents returned as relevant to a given subject label were tagged with that label. The number of documents receiving a given label was controlled by a threshold. The submitted runs varied with respect to whether or not the archival metadata was indexed in addition to the speech recognition transcripts. They also varied with respect to whether expansion was applied to the class label (i.e., the query). Expansion was performed by augmenting the original query with the most frequent term occurring in the top five documents returned by an initial retrieval round. If fewer than two documents were returned, queries were expanded using a thesaurus. SINAI Research Group, University of Jaén, Spain The SINAI 8 group approached the task as a categorization problem, training SVMs using the training data provided. One run, SINAI svm nometadata, extracted feature vectors from the speech transcripts alone and one run, SINAI svm withmetadata, made use of both speech recognition transcripts and metadata.</p><p>Computer Science, Alexandru Ioan Cuza University, Romania Classifiers were trained using data collected from Wikipedia or via general Web search. Results are not reported here, however, since the submitted runs were not carried out on the current test data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Results</head><p>The MAP of the results of the task are reported in Table <ref type="table" coords="4,317.11,617.82,3.74,8.64" target="#tab_0">1</ref>. The results confirm the viability of techniques that approach the Subject Classification Task as an information retrieval task. Such techniques proved useful in VideoCLEF 2008 <ref type="bibr" coords="4,199.07,641.73,11.62,8.64" target="#b5">[6]</ref> and also provide the best results in 2009 where the size of the collection and the label set increased. Also, consistent with VideoCLEF 2008 observations, performance is better when archival metadata is used in addition to speech recognition transcripts. Finally, after VideoCLEF 2008, we decided that we wanted to provide a training data set of speech transcripts generated video in 2009 to see whether training classifiers on data from the same domain as the test data would improve performance. The results of the runs submitted this year suggest that training classifiers on speech transcripts of samedomain video does not provide significantly better performance than exploiting Web data to support an information-retrieval-based approach.</p><p>There is general awareness shared by VideoCLEF participants that although MAP is a useful tool, it may not be the ideal evaluation metric for this task. The reader can refer to the working notes papers of the individual participants for discussion. The ultimate goal of subject tagging is to generate a set of tags for each video that will allow users to find that video while searching or browsing. The utility of a tag assigned to a given video is therefore not entirely independent of the other tags assigned. Under the current formulation of the task, the presence or absence of the tag is the only information that is of use to the searcher. The ranking of a video in a list of videos that are assigned the same tag is for this reason not directly relevant to the utility of that tag for the user.</p><p>3 Affect Task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task</head><p>The goal of the Affect Task at VideoCLEF 2009 was to automatically detect narrative peaks in documentaries. Narrative peaks were defined to be those places in a video where viewers report feeling a heightened emotional effect due to dramatic tension. This task was new in 2009. The ultimate aim of the Affect Task is to move beyond the information content of the video and to analyze the video with respect to characteristics that are important for viewers, but not related to the video topic.</p><p>Narrative peak detection builds on and extends work in affective analysis of video content carried out in the areas of sports and movies, cf. e.g., <ref type="bibr" coords="5,261.77,534.30,10.58,8.64" target="#b1">[2]</ref>. Viewers perceive an affective peak in sports videos due to tension arising from the spontaneous interaction of players within the constraints of the physical world and the rules and conventions of the game. Viewers perceive an affective peak in a movie due to the action or the plot line, which is carefully planned by the script writer and the filmmaker.</p><p>Narrative peaks in documentaries are a new domain in that they do not fall into either category. Documentaries convey information and often have storylines, but don't have the all-dominating plot trajectory of a movie. Documentaries often include extemporaneous narrative or interviews, and therefore also have a spontaneous component. The affective curve experienced by a viewer watching a documentary can be expected to be relatively subtly modulated.</p><p>It is important to differentiate narrative peak detection from other cases of affect detection, such as hotspot detection in meetings. Hotspots are moments during meetings where people are highly involved in the discussion <ref type="bibr" coords="5,148.69,665.81,10.58,8.64" target="#b8">[9]</ref>. Hotspots can be self-reported by meeting participants or annotated in meeting video by viewers. In either case, it is the participant and not the viewer whose affective reaction is being detected.</p><p>We chose the the Beeldenstorm series for the narrative peak detection task in order to make the task as simple and straightforward as possible in its initial year. Beeldenstorm features a single speaker, the host Prof. van Os, and covers a topical domain, the visual arts, that is rich enough to be interesting, yet is relatively constrained. These characteristics help us to control for the effects of personal style of the host and of viewer familiarity with topic in the affect and appeal task. Further, as mentioned above, the fact that the documentaries are short makes it possible for annotators to watch them in their entirety when annotating narrative peaks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation</head><p>For the purposes of evaluation, as mentioned above, three Dutch speakers annotated the Beeldenstorm collection by each identifying the three top narrative peaks in each video. Annotators were asked to mark the peaks where they felt the dramatic tension reached its highest level. They were not supplied with an explicit definition of a narrative peak. Instead, all annotators needed to form independent opinions of where they perceived narrative peaks. In order to make the task less abstract, they were supplied with the information that the Beeldenstorm series is associated with humorous and moving moments. They were told that they could use that information to formulate their notion of what constitutes a narrative peak. Peaks were required to be a maximum of ten seconds in length.</p><p>Although the annotators did not consult with each other about specific peaks, the team did engage in discussion during the definition process. The discussion ensured that there was underlying consensus about the approach to the task. In particular, it was necessary to check that annotators understood that a peak must be a high point in the storyline as measured by their perceptions of their own emotional reaction. Dramatic objects or facts in the spoken or visual content that were not part of the storyline as it was created by the narrator/producer were not considered narrative peaks. Regions in the video where the annotator guessed that the speaker or producer had intended there to be a peak, but where the annotator did not feel any dramatic tension were not considered to be peaks. An example of this would be a joke that the annotator did not understand completely.</p><p>The first two episodes for which the annotators defined peaks were discarded in order to assure that the annotators perception of a narrative peak had stabilized. This warm-up exercise was particularly important in light of the fact that at the end of the annotation effort, assessors reported that it was necessary to become familiar with the style and allow an affinity for the series to develop before they started to feel an emotional reaction to narrative peaks in the video.</p><p>The peaks identified by the assessors were considered to be a reflection of underlying "true" peaks in the narrative of the video. We assumed that the variation between assessors is the result of noise due to effects such as personal idiosyncracies. In order to generate a ground truth most highly reflective of "true" peaks, the peaks identified by the assessors were merged. The assessment team consisted of three members who each identified three peaks in 45 videos for a total of 405 peaks. The assessors were able to give a rough estimate of the minimum distance between peaks and on the basis of their observations, it was decided to consider two peaks that overlapped by at least two seconds to be realization of the same peak. After merging the peaks, 292 of the 405 peaks turned out to be distinct. The merging process was carried out by fitting a 10 second window to overlapping assessor peaks in order to ensure that merged peaks could never exceed the specified peak length of 10 seconds.</p><p>Evaluation involved the application of two scoring methods, the point-based approach and the peakbased approach. Under point-based scoring, the peaks chosen by each assessor are assessed without merging. A hypothesized peak receives a point in every case in which it falls within eight seconds of an assessor peak. The run score is the total number of peaks returned by all peak hypothesis in the run. A single episode can earn a run between three points (assessors chose completely different peaks) and nine points (assessors all chose the same peaks). There are no episodes in the set that fall at either of these extremes. The distribution of the peaks in the files is such that a perfect run would earn 246 points. Under peak-based scoring, a hypothesis is counted as correct if it falls within an 8 second window of a peak representing a merger of assessor annotations. Three different types of merged reference peaks are defined for peak-based scoring. Three different peak-based scores are reported that differ in the number of assessors required to agree in order for a region in the video to be considered a peak. Of the 293 total peaks identified, 203 peaks are "personal peaks" (peaks identified by only one assessor), 90 are "pair peaks" (peaks that are identified by at least two assessors) and 22 are "general peaks" (peaks upon which all three assessors agreed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Techniques</head><p>Narrative peak detection techniques were developed that used the visual channel, the audio channel and the speech recognition transcript. Each group took a different approach.</p><p>Computer Science, Alexandru Ioan Cuza University, Romania Based on the hypothesis that speakers raise their voices at narrative peaks, three runs were developed that made use of the intensity of the audio signal. A score was computed for each group of words that involved a comparison of intensity means and other statistics for sequential groups of words. The top three scoring points were hypothesized as peaks.</p><p>Computer Vision and Multimedia Laboratory, University of Geneva, Switzerland The assumption was made that dramatic peaks correspond to the introduction of a new topic and thus correspond to change in word use as reflected in the speech recognition transcripts. Additionally, the video and audio channel effects assumed to be indicative of peaks were explored. Finally, a weighting was deployed that gave more emphasis to positions at which peaks were expected to occur based on the distribution of peaks in the development data. The weighting is used in unige-cvml1, unige-cvml2 and unige-cvml3. Run unige-cvml1 uses text features alone. Run unige-cvml3 uses text plus elevated speaker pitch. Run unige-cvml2 uses text, elevated pitch and quick changes in the video. Run unige-cvml4 uses text only and no weighting. Run unige-cvml5 sets peaks randomly to provide a random baseline for comparsion.</p><p>Delft University of Technology and University of Twente, Netherlands Only features extracted from the speech transcripts were exploited. Run duotu09fix predicted peaks at fixed points chosen by analyzing the development data. Run duotu09ind used indicator words as cues of narrative peaks. Indicator words were chosen by analyzing the development data. Run duotu09rep applied the assumption that word repetition, reflecting the use of an important rhetorical device, would indicate a peak. Run duotu09pro used pronouns as indicators of audience directed speech and assumed that high pronoun densities would correspond to points where viewers feel maximum involvement. Run duotu09rat exploited the affective scores of words, building on the hypothesis that use of affective speech characterizes narrative peaks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>The results of the task are reported in Table <ref type="table" coords="7,267.69,508.85,3.74,8.64" target="#tab_1">2</ref>. The results make clear that it is quite challenging to effectively support the detection of narrative peaks using audio and video features. Recall that unige-cvml5 is a randomly generated run. Most runs failed to yield results appreciably better than this random baseline. The best scoring approaches exploited the speech recognition transcripts, in particular, the occurrence of pronouns reflecting user directed speech and the use of words with high effective ratings.</p><p>Because of the newness of the Narrative Peak Detection Task, the method of scoring is still a subject of discussion. The scoring method was designed such that algorithms were given as much credit as possible for agreement between the peaks they hypothesized and the peaks chosen by the annotators. See the working notes papers of individual participants for some additional discussion.</p><p>4 Linking Task</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task</head><p>The Linking Task, also called "Finding Related Resources Across Languages," involves linking episodes of the Beeldenstorm documentary (Dutch language) to Wikipedia articles about related subject matter (English language). This task was new in 2009. Participants were supplied with 165 multimedia anchors, short (ca. 10 seconds) segments, pre-defined in the 45 episodes that make up the Beeldenstorm collection. For each anchor, participants were asked to automatically generate a list of English language Wikipedia pages relevant to the anchor, ordered from the most to the least relevant. Notice that this task is designed such that it goes beyond a named-entity linking task. Although a multimedia anchor may contain a named entity (e.g., a person, place or organization) that is mentioned in the speech channel, it is not always the case. The topic being discussed in the video at the point of the anchor may not be explicitly named. Also, the representation of a topic in the video may be split between the visual and the speech channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>The ground truth for the linking task was created by the assessors. We adapted the four graded relevance levels used in <ref type="bibr" coords="8,146.30,469.47,11.62,8.64" target="#b3">[4]</ref> for application in the Linking Task. Level 3 links are referred to as primary links and are defined as "highly relevant -the page is the single page most relevant for supporting understanding of the video in the region of the anchor." There is only a single primary link per multimedia anchor representing the one best page to which that anchor can be linked. Level 2 links are referred to as secondary links and are defined as "fairly relevant -the page treats a subtopic (aspects) of the video in the region of the anchor." The final two levels: Level 1 (defined as: "marginally relevant, the page is not appropriate for the anchor") and Level 0 (defined as "irrelevant, the page is unrelated to the anchor"), were conflated and regarded as irrelevant. Links classified as Level 1 are generic links, e.g., "painting," or links involving a specific word that is mentioned, but is not really central to the topic of the video at that point.</p><p>Primary link evaluation For each video, the primary link was defined by consensus among three assessors. The assessors were required to watch the entire episode so as to have the context to decide the primary link. Primary links were evaluated using recall (correct links/total links) and Mean Reciprocal Rank (MRR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related resource evaluation</head><p>For each video, a set of related resources was defined. This set necessarily includes the primary link. It also includes other secondary links that the assessors found relevant. Only one assessor needed to find a secondary link relevant for it to be included. However, the assessors agreed on the general criteria to be applied when chosing a secondary link. Related resources were evaluated with MRR. The list of secondary links is not exhaustive, for this reason, no recall score is reported. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Techniques</head><p>Centre for Digital Video Processing, Dublin City University, Ireland The words spoken between the start point and the end point of the multimedia anchor (as transcribed in the speech recognition transcript) were used as a query and fired off against an index of Wikipedia. For dcu run1 and dcu run2 the Dutch Wikipedia was queried and the corresponding English page was returned. Stemming was applied in dcu run2. Dutch pages did not always have corresponding English pages. For dcu run3, the query was translated first and fired off against an English language Wikipedia index. For dcu run4 a Dutch query expanded using psuedo-relevance feedback was used.</p><p>TNO Information and Communication Technology, Netherlands A set of existing approaches were combined in order to implement a sophisticated baseline to provide a starting point for future research. A wikify tool was used to find links in the Dutch speech recognition transcripts and in English translations of the transcripts. Particular attention was given to proper names, with one strategy giving preference to links to articles with proper-name titles and another strategy ensuring that proper name information was preserved under translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>The results of the task are reported in Table <ref type="table" coords="9,265.76,696.29,4.98,8.64" target="#tab_2">3</ref> (primary link evaluation) and Table <ref type="table" coords="9,416.86,696.29,4.98,8.64" target="#tab_3">4</ref> (related resource evaluation). The best run used a combination of different strategies, referred to by TNO as a "cocktail." The techniques applied by DCU achieved a lower overall score, but proved to provide more robust improvements across queries. Details can be found in the working notes papers.</p><p>In 2009, VideoCLEF participants carried out three tasks, Subject Classification, Narrative Peak Detection and Finding Related Resources Across Languages. These tasks generate enrichment for spoken content that can be used to provide improvement in multimedia access and retrieval.</p><p>With the exception of the Narrative Peak Detection Task, participants concentrated largely on features derived from the speech recognition transcripts and did not exploit other audio information or information derived from the visual channel. Looking towards next year, we will continue to suggest that participants use a wider range of features. We plan to keep up our efforts to encourage cross-over from the TRECVid community, for example, by recycling the TRECVid data set for the Subject Classification Task.</p><p>We see the Subject Classification Task as developing increasingly towards a tag recommendation task, where systems are required to assign tags to videos. The tag set might not necessarily be known in advance. We expect that the formulation of this task as an information retrieval task will continue to prove useful and helpful, although we wish to move to metrics for evaluation that will better reflect the utility of the assigned tags in a real-world search or browsing situation.</p><p>In 2010, we intend to continue working with the collections from Beeld &amp; Geluid, but also add an additional data set of social video. Using this data set we hope to offer a task that will allow participants to make use of social information, i.e., friendship relationships between users in an online community, in order to improve video retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,210.64,119.13,179.23,112.13"><head>Table 1 :</head><label>1</label><figDesc>Subject Classification Results</figDesc><table coords="5,210.64,137.75,179.23,93.51"><row><cell>run ID</cell><cell>MAP</cell></row><row><cell>cut1 sc asr baseline</cell><cell>0.0067</cell></row><row><cell>cut2 sc asr expanded</cell><cell>0.0842</cell></row><row><cell cols="2">cut3 sc asr meta baseline 0.2586</cell></row><row><cell cols="2">cut4 sc asr meta expanded 0.2531</cell></row><row><cell cols="2">cut5 sc asr meta expanded 0.3813</cell></row><row><cell>SINAI svm nometadata</cell><cell>0.0023</cell></row><row><cell>SINAI svm withmetadata</cell><cell>0.0028</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,118.29,119.13,363.93,208.17"><head>Table 2 :</head><label>2</label><figDesc>Narrative peak detection results</figDesc><table coords="8,118.29,137.75,363.93,189.55"><row><cell>run ID</cell><cell>point-based</cell><cell>peak-based</cell><cell>peak-based</cell><cell>peak-based</cell></row><row><cell></cell><cell></cell><cell>&gt; 1 assessor</cell><cell>&gt; 2 assessors</cell><cell>&gt; 3 assessors</cell></row><row><cell></cell><cell></cell><cell cols="3">("personal peaks") ("pair peaks") ("general peaks")</cell></row><row><cell>duotu09fix</cell><cell>47</cell><cell>28</cell><cell>8</cell><cell>4</cell></row><row><cell>duotu09ind</cell><cell>55</cell><cell>38</cell><cell>12</cell><cell>2</cell></row><row><cell>duotu09rep</cell><cell>30</cell><cell>21</cell><cell>7</cell><cell>0</cell></row><row><cell>duotu09pro</cell><cell>63</cell><cell>44</cell><cell>17</cell><cell>4</cell></row><row><cell>duotu09rat</cell><cell>63</cell><cell>37</cell><cell>20</cell><cell>5</cell></row><row><cell>unige-cvml1</cell><cell>39</cell><cell>32</cell><cell>6</cell><cell>0</cell></row><row><cell>unige-cvml2</cell><cell>41</cell><cell>30</cell><cell>11</cell><cell>2</cell></row><row><cell>unige-cvml3</cell><cell>42</cell><cell>31</cell><cell>8</cell><cell>0</cell></row><row><cell>unige-cvml4</cell><cell>43</cell><cell>31</cell><cell>9</cell><cell>0</cell></row><row><cell>unige-cvml5</cell><cell>43</cell><cell>32</cell><cell>8</cell><cell>3</cell></row><row><cell>uaic-run1</cell><cell>33</cell><cell>26</cell><cell>7</cell><cell>2</cell></row><row><cell>uaic-run2</cell><cell>41</cell><cell>29</cell><cell>10</cell><cell>3</cell></row><row><cell>uaic-run3</cell><cell>33</cell><cell>24</cell><cell>7</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,90.00,119.13,423.00,147.99"><head>Table 3 :</head><label>3</label><figDesc>Linking results: Primary link evaluation. Raw count of primary links retrieved (out of 165) and Mean Reciprocal Rank</figDesc><table coords="9,246.98,149.71,106.54,117.42"><row><cell>run ID</cell><cell>raw MRR</cell></row><row><cell cols="2">dcu run1 44 0.182</cell></row><row><cell cols="2">dcu run2 44 0.182</cell></row><row><cell cols="2">dcu run3 13 0.056</cell></row><row><cell cols="2">dcu run4 38 0.144</cell></row><row><cell cols="2">tno run1 57 0.230</cell></row><row><cell cols="2">tno run2 55 0.215</cell></row><row><cell cols="2">tno run3 58 0.251</cell></row><row><cell cols="2">tno run4 44 0.182</cell></row><row><cell cols="2">tno run5 47 0.197</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,145.89,301.15,311.22,136.04"><head>Table 4 :</head><label>4</label><figDesc>Linking results: Related resource evaluation. Mean Reciprocal Rank.</figDesc><table coords="9,260.35,319.77,79.80,117.42"><row><cell>run ID</cell><cell>MRR</cell></row><row><cell cols="2">dcu run1 0.268</cell></row><row><cell cols="2">dcu run2 0.275</cell></row><row><cell cols="2">dcu run3 0.089</cell></row><row><cell cols="2">dcu run4 0.190</cell></row><row><cell cols="2">tno run1 0.460</cell></row><row><cell cols="2">tno run2 0.428</cell></row><row><cell cols="2">tno run3 0.484</cell></row><row><cell cols="2">tno run4 0.392</cell></row><row><cell cols="2">tno run5 0.368</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,104.35,712.57,115.23,6.91"><p>http://www.cdvp.dcu.ie/VideoCLEF</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,104.35,722.30,98.06,6.91"><p>http://www.clef-campaign.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,104.35,732.03,90.69,6.91"><p>http://www.beeldengeluid.nl</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="3,104.35,635.98,408.65,6.91;3,90.00,645.31,298.33,7.04"><p>Visit the search engine at http://zoeken.beeldengeluid.nl. A keyword search will return a results list with a column labeled Trefwoorden or keywords. These are the topic class subject labels that are used in the archive.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="3,104.35,655.18,408.65,6.91;3,90.00,664.64,423.00,6.91;3,90.00,674.10,423.00,6.91;3,90.00,683.57,423.00,6.91;3,90.00,693.03,423.00,6.91;3,90.00,702.50,423.00,6.91;3,90.00,711.96,423.00,6.91;3,90.00,721.43,423.00,6.91;3,90.00,730.89,308.09,6.91"><p>In total 46 labels were used: aanslagen (attacks), armoede (poverty), burgeroorlogen (civil wars), criminaliteit (crime), culturele identiteit (cultural identity), dagelijks leven (daily life), dieren (animals), dierentuinen (zoos), economie (economy), etnische minderheden (ethnic minorities), fabrieken (factories), families (families), gehandicapten (disabled), geneeskunde (medicine), geneesmiddelen (pharmaceutical drug), genocide (genocide), geschiedenis (history), gezinnen (families), havens (harbors), hersenen (brain), illegalen (undocumented immigrants), journalisten (journalist), kinderen (children), landschappen (landscapes), media (media), militairen (military personnel), musea (museums), muziek (music), natuur (nature), natuurkunde (physics), ouderen (seniors), pers (press), politiek (politics), processen (lawsuits), rechtszittingen (court hearings), reizen (travel), taal (language), verkiezingen (elections), verkiezingscampagnes (electoral campaigns), voedsel (food), voetbal (soccer), vogels (birds), vrouwen (women), wederopbouw (reconstruction), wetenschappelijk onderzoek (scientific research), ziekenhuizen (</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="3,398.09,730.89,32.67,6.91;3,100.86,739.10,98.64,8.43"><p>hospitals).<ref type="bibr" coords="3,100.86,739.10,2.99,5.18" target="#b5">6</ref> http://www.anvil-software.de/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="4,104.35,708.77,88.24,6.91"><p>http://trec.nist.gov/trec eval</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7" coords="10,104.35,716.01,79.61,6.91"><p>http://www.trebleclef.eu/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgements</head><p>We are grateful to TrebleCLEF, 9 a Coordination Action of <rs type="institution">European Commission</rs>'s <rs type="programName">Seventh Framework Programme</rs> for a grant that made possible the creation of a data set for the Narrative Peak Detection Task and the Linking Task. Thank you to the <rs type="institution">University of Twente</rs> for supplying the speech recognition transcripts and to the <rs type="institution">Netherlands Institute of Sound and Vision</rs> for supplying the video. Thank you to <rs type="institution">Dublin City University</rs> for providing the shot segmentation and keyframes and also for hosting the team of Dutch-speaking video assessors during the Dublin Days event. We'd also like to express our appreciation to <rs type="person">Michael Kipp</rs> for use of the Anvil Video Annotation Research Tool. The work that went into VideoCLEF 2009 has been supported, in part, by <rs type="funder">PetaMedia Network of Excellence</rs> and has received funding from the <rs type="funder">European Commission's</rs> <rs type="programName">Seventh Framework Programme</rs> under grant agreement no. <rs type="grantNumber">216444</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_tpFnSPr">
					<orgName type="program" subtype="full">Seventh Framework Programme</orgName>
				</org>
				<org type="funding" xml:id="_yajHhHc">
					<idno type="grant-number">216444</idno>
					<orgName type="program" subtype="full">Seventh Framework Programme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,106.60,533.31,406.40,8.64;10,106.60,545.10,406.40,8.81;10,106.60,557.05,188.39,8.81" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,405.74,533.31,107.26,8.64;10,106.60,545.27,152.54,8.64">Temporal video segmentation for real-time key frame extraction</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Calic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sav</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Izquierdo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Marlow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>O'connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,278.44,545.10,234.56,8.58;10,106.60,557.05,158.90,8.58">Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>the International Conference on Acoustics, Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,106.60,576.98,406.40,8.81;10,106.60,588.93,169.94,8.81" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,216.08,577.15,212.15,8.64">Affective video content representation and modeling</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L-Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,439.31,576.98,73.69,8.58;10,106.60,588.93,62.13,8.58">Multimedia, IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="154" />
			<date type="published" when="2005-02">Feb. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,106.60,609.03,406.40,8.64;10,106.60,620.81,406.41,8.81;10,106.60,632.77,141.34,8.81" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,291.53,609.03,221.47,8.64;10,106.60,620.98,114.62,8.64">Annotation of heterogeneous multimedia content using automatic speech recognition</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Huijbregts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Ordelman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jong</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,238.56,620.81,274.44,8.58;10,106.60,632.77,111.57,8.58">Proceedings of the International Conference on Semantic and Digital Media Technologies (SAMT)</title>
		<meeting>the International Conference on Semantic and Digital Media Technologies (SAMT)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,106.60,652.69,406.40,8.81;10,106.60,664.65,338.91,8.81" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,234.01,652.86,212.61,8.64">Using graded relevance assessments in IR evaluation</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kekäläinen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,456.12,652.69,56.88,8.58;10,106.60,664.65,230.51,8.58">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1120" to="1129" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,106.60,684.57,406.40,8.81;10,106.60,696.70,97.41,8.64" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,148.33,684.74,232.39,8.64">Anvil -a generic annotation tool for multimodal dialogue</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kipp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,401.09,684.57,107.47,8.58">Proceedings of Eurospeech</title>
		<meeting>Eurospeech</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1367" to="1370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,106.60,112.34,406.40,8.64;11,106.60,124.12,406.40,8.81;11,106.60,136.25,22.42,8.64" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,284.68,112.34,228.32,8.64;11,106.60,124.29,222.17,8.64">Overview of VideoCLEF 2008: Automatic generation of topic-based feeds for dual language audio-visual content</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,346.31,124.12,162.03,8.58">Proceedings of the CLEF 2008 Workshop</title>
		<meeting>the CLEF 2008 Workshop</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,106.60,156.17,406.40,8.64;11,106.60,167.96,334.30,8.81" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,396.56,156.17,116.44,8.64;11,106.60,168.13,123.51,8.64">Overview of the CLEF 2007 cross-language speech retrieval</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Hoffmannova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,248.01,167.96,163.32,8.58">Proceedings of the CLEF 2007 Workshop</title>
		<meeting>the CLEF 2007 Workshop</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,106.60,187.88,406.40,8.81;11,106.60,199.84,406.40,8.81;11,106.60,211.97,92.68,8.64" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,269.47,188.05,146.10,8.64">Evaluation campaigns and TRECVid</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,437.61,187.88,75.39,8.58;11,106.60,199.84,291.84,8.58">Proceedings of the ACM International Workshop on Multimedia Information Retrieval (MIR)</title>
		<meeting>the ACM International Workshop on Multimedia Information Retrieval (MIR)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="321" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,106.60,231.89,406.40,8.64;11,106.60,243.68,210.91,8.81" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,215.63,231.89,281.35,8.64">Spotting &quot;hot spots&quot; in meetings: Human judgments and prosodic cues</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Wrede</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Shriberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,106.60,243.68,106.56,8.58">Proceedings of Eurospeech</title>
		<meeting>Eurospeech</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="2805" to="2808" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
