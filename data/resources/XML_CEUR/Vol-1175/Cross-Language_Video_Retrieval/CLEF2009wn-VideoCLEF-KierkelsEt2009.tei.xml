<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,177.90,75.23,250.80,12.58;1,212.94,92.69,169.49,12.58">Identification of Narrative Peaks in Clips: Text Features Perform Best</title>
				<funder ref="#_mQtpJYp">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_9PxbjEr">
					<orgName type="full">European Community</orgName>
				</funder>
				<funder ref="#_7qVnKMJ">
					<orgName type="full">NoE PetaMedia</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,185.16,131.04,75.01,9.02"><forename type="first">Joep</forename><forename type="middle">J M</forename><surname>Kierkels</surname></persName>
							<email>joep.kierkels@unige.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department Battelle Building A</orgName>
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<addrLine>7 Route de Drize CH -1227</addrLine>
									<settlement>Carouge, Geneva</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.43,131.04,93.51,9.02"><forename type="first">Mohammad</forename><surname>Soleymani</surname></persName>
							<email>mohammad.soleymani@unige.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department Battelle Building A</orgName>
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<addrLine>7 Route de Drize CH -1227</addrLine>
									<settlement>Carouge, Geneva</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,370.18,131.04,48.10,9.02"><forename type="first">Thierry</forename><surname>Pun</surname></persName>
							<email>thierry.pun@unige.ch</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department Battelle Building A</orgName>
								<orgName type="institution">University of Geneva</orgName>
								<address>
									<addrLine>7 Route de Drize CH -1227</addrLine>
									<settlement>Carouge, Geneva</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,177.90,75.23,250.80,12.58;1,212.94,92.69,169.49,12.58">Identification of Narrative Peaks in Clips: Text Features Perform Best</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4BA535C50FD7DB1ABD5DCE16BA28D993</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Feature detection</term>
					<term>Video analysis</term>
					<term>Attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A methodology is proposed to identify narrative peaks in video clips. Three basic clip properties are evaluated which reflect on video, audio and text related features in the clip. Furthermore, the expected distribution of narrative peaks throughout the clip is determined and exploited for future predictions. Results show that only the text related feature, related to the usage of distinct words throughout the clip, and the expected peak-distribution are of use when finding the peaks. On the training set, our best detector had an accuracy of 47% in finding narrative peaks. On the test set, this accuracy dropped to 24%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A challenging issue in content-based video analysis techniques is the detection of sections that evoke increased levels of interest or attention in viewers of video-clips or documentaries. Once such sections are detected, other less relevant sections may be removed from a recording. By doing so, a summary of a clip can be created which allows for faster browsing through relevant sections. This will save valuable time of any viewer who merely wants to see an overview of the clip. Past studies on highlight detection often focus on analyzing sports-videos <ref type="bibr" coords="1,70.92,426.00,10.64,9.02" target="#b0">[1]</ref>, in which highlights usually show specific patterns related to ball or player movement. Although clips usually contain audio-, video, and spoken text-content, many existing approaches focus on merely one of these <ref type="bibr" coords="1,491.35,437.52,18.33,9.02">[2;3]</ref>. In the current paper, we will attempt to show results for all three modalities and try to identify which modality is actually the most valuable in the detection of segments which supposedly include narrative peaks. For our participation in the VideoCLEF 2009 subtask on "Affect and Appeal" <ref type="bibr" coords="1,397.23,472.02,10.61,9.02">[4]</ref>, we propose in this paper a methodology to identify narrative peaks in video clips. The clips that were used in this subtask were all taken from a Dutch program called "Beeldenstorm". They were in Dutch language, had duration between seven and nine minutes, consisted of video and audio, and had speech transcripts available. Detection accuracy was determined by comparison against manual annotations on narrative peaks provided by three (Dutch speaking) annotators.</p><p>While viewing the clips, we failed to see any clear indicators as to which specific audiovisual features could be used to identify narrative peaks, even when looking at the annotations that were provided. Furthermore we noticed that there was little consistency among the annotators because more than three narrative peaks were indicated for all clips. This led to our belief that tailoring any detection method to a single person's view on narrative peaks would not be fruitful and hence we decided to work only with basic features. We expect these features to be indicators of narrative peaks that are common to most observers, including the annotators.</p><p>Our approach for detecting peaks consists of a top-down search for relevant features, e.g., first we computed possibly relevant features and secondly we establish which of these features really enhance detection accuracy. We separately treated three different modalities.</p><p>• Video, taken from the available .mpg files, was used to determine at what place in the clip frames showed the largest change compared to a preceding frame. • Audio, taken from an mp3 conversion of the .mpg frames, was used to determine at what place in the clip the speaker has an elevated pitch or has an increased speech volume. • Text, taken from the available .mp7 files, was used to determine at what place in the clip the speaker introduced a new topic. Next to this, we considered the expected distribution of narrative peaks over clips. Details on how all these steps were implemented are given in Section 2, followed by results of our approach on the given training data in Section 3. In Section 4 several conclusions are drawn from these results.</p><p>In the VideoCLEF subtask, the focus of detecting segments of increased interest is on the data part, e.g., we analyze parts of the shown video-clip to predict their impact on a viewer. Even though it is outside the scope of the subtask, it is worth to mention that there exists a second approach to identifying segments of increased interest. This second approach focuses not on the data but directly on the reactions of a viewer, e.g., by monitoring his physiological activity such as heart-rate <ref type="bibr" coords="2,302.16,119.22,11.68,9.02" target="#b3">[5]</ref> or by filming his facial expressions <ref type="bibr" coords="2,468.41,119.22,10.63,9.02" target="#b4">[6]</ref>. Based on such reactions, the affective state of a viewer can be estimated and one can estimate levels of excitation, attention and interest in a viewer <ref type="bibr" coords="2,217.44,142.26,10.64,9.02" target="#b5">[7]</ref>. By themselves, physiological activity measures can thus be used to estimate interest, but they could also be used to validate the outcomes of data-based techniques. Because the evaluation in the VideoCLEF task will be against the explicit annotations provided by the three annotators we did not include recordings of physiological activity in our VideoCLEF contribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Feature extraction</head><p>For the different modalities, feature extraction will be described separately in the following subsections. As the topic of detecting affective peaks is quite unexplored, we decided to implement only basic features. This provides an initial idea of which features are useful, and future studies could focus on enhancing the relevant basic features. Feature extraction was implemented using Matlab.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Video features</head><p>Our key assumption for video features was that dramatic tension is related to big changes in video. It is a film editors choice to include change between different frames <ref type="bibr" coords="2,307.43,344.04,10.63,9.02" target="#b6">[8]</ref>, and we believe that this may be used to stress the importance of certain parts in the clip. Video is recorded at 25 frames/second. Because our narrative peak detector will output a 10 s window of enhanced dramatic tension, this precision level which is too large and merely slows down computations, furthermore frame changes are often not too obvious on subsequent frames. Our treatment of video-frames starts at frame 1 and subsequently jumps about 0.5 s to frame 13, frame 25, and so on. All frames are converted to grayscale levels. The difference between subsequent frames is computed as:</p><formula xml:id="formula_0" coords="2,165.66,425.27,251.26,29.85">13 1 1 1 ( , ) ( , ) , N M vid n m F n m F n m = = Δ = - ∑∑ (1.1)</formula><p>in which N and M are the width and the height of the frames and F 13 indicates the matrix containing pixels values of the 13 th frame. As a next step, the change in Δ vid , indicated as dΔ vid , over subsequent observations is determined and compared against a threshold value that reflects how much change in this value is observed when a scene changes. This threshold was determined on the training-set and set to 5*10 5 . If dΔ vid is below this threshold, it is set to zero. As a final step, the resulting dΔ vid is smoothed by averaging over a 10 s window, and the smoothed resulting signal is scaled to have a maximum absolute value of one and subsequently to have a mean of zero. Next, it is down-sampled again by a factor 2, resulting in vector video which contains 1 value per second as is illustrated in Fig. <ref type="figure" coords="2,192.44,541.26,9.82,9.02" target="#fig_0">1A</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Audio features</head><p>The key assumption for audio was that a speaker has an elevated pitch or has an increased speech volume when applying dramatic tension, as suggested in <ref type="bibr" coords="3,245.67,309.06,23.07,9.02">[9;10]</ref>. Audio is recorded at 44.1 kHz. The audio signal is divided in 0.5 s segments for which the average pitch of the speaker's voice is computed by imposing a Kaiser window and applying a Fast Fourier Transform. In the transformed signal, the frequency with maximum power is determined and is assumed to be the average pitch of the speaker's voice over this window. Next the difference in average pitch between subsequent segments is computed. If a segment's average pitch is less than 2.5 times as high as the pitch of the preceding segment, its pitch value is set to zero. This way, only those segments with strong increase in pitch (supposed indicator of dramatic tension) are kept.</p><p>Speech volume is determined by computing the averaged absolute value of the audio signal within the 0.5 s segment. As a final step again, the resulting signals for pitch and volume are both smoothed by averaging over a 10 s window, and the smoothed resulting signal is scaled to have a maximum absolute value of one and subsequently to have a mean of zero. Next, they are down-sampled by a factor 2, resulting in vectors audio1 and audio2 which both contain 1 value per second as is illustrated in Fig. <ref type="figure" coords="3,348.77,435.60,9.43,9.02" target="#fig_0">1B</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Text features</head><p>The main assumption for text is that dramatic tension starts by the introduction of a new topic, and hence involves the introduction of new vocabulary related to this topic. Text transcripts are obtained from the available .mp7 files. A histogram was computed for all unique words that occurred in the clip, counting their number of occurrences. Words that occurred only once were considered to be non-specific and were ignored. Words that occurred more than five times were considered too general and were also ignored. The remaining set of words is considered to be topic specific. Based on this set of words, we estimated where the changes in used vocabulary are the largest. A vector v filled with zeros was initialized, having a length equal to the number of seconds in the clip. For each remaining word, its first and last appearance in the .mp7 file was determined and was rounded off to whole seconds, subsequently all elements in v in between the elements corresponding to these obtained timestamps are increased by one. Again, the resulting vector v is averaged over a 10 s window, scaled and set to zero mean. The resulting vector text is illustrated in Fig. <ref type="figure" coords="3,297.63,606.54,9.43,9.02" target="#fig_0">1C</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Distribution of narrative peaks</head><p>A clip is directed by a program director and is intended to hold the attention of the viewer. To this end, it is expected that points of dramatic tension are distributed over the duration of the whole clip, and that not all moments during a clip are equally likely to have dramatic tension. For each dramatic tension-point as indicated by the annotators, its time of occurrence was determined (mean of start and stop timestamp) and a histogram, illustrated in Fig. <ref type="figure" coords="3,314.45,708.54,3.76,9.02" target="#fig_1">2</ref>, was created based on these occurrences. Based on this histogram, a weighting vector w was created for each recording. Vector w contains one element for each second of the clip. Each element's value is determined according to the histogram. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Fusion and selection</head><p>For fusion of the features, our approach merely consisted in giving equal importance to all used features. After fusion, the weights vector w can be applied and the final indicator of dramatic tension drama is derived as (shown for all three features):</p><p>( )</p><formula xml:id="formula_1" coords="4,128.70,335.63,288.22,36.84">1 2 . 2 T audio audio drama w video text + ⎛ ⎞ = ⋅ + + ⎜ ⎟ ⎝ ⎠ (1.1)</formula><p>The estimated three points of increased dramatic tension are then obtained by selecting the three maxima from drama. Our estimates for the three dramatic points are constructed by selecting the intervals starting 5s before these peaks and ending 5s afterwards. If either the second or third highest point in drama is within 10s of the highest point, the point is ignored in order to avoid having an overlap between the detected segments of increased dramatic tension. In those cases, the next highest point is used (provided that the new point is not within 10s)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation schemes and Results</head><p>Different combinations of the derived features were made and subsequently evaluated against the training data. The schemes we tested are listed in table <ref type="table" coords="4,236.53,509.76,3.76,9.02" target="#tab_0">1</ref>. If no weights are used (Scheme 8) vector w contains only ones. Scoring of evaluation results is performed based on agreement with the reviewers' annotations. Each time a peak that we detected coincides with (at least) one reviewer's annotation, a point is added. A maximum of three points can thus be scored per clip and since there are five clips in the training set, the maximum score for any scheme is 15. The obtained scores are shown in table <ref type="table" coords="4,285.59,660.90,3.76,9.02">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2. Results in training sets.</head><p>Scheme number BG_36941 BG_37007 BG_37016 BG_37036 BG_37111 Total</p><formula xml:id="formula_2" coords="5,165.30,113.01,283.68,80.70">1 0 0 1 1 1 3 2 2 1 1 1 1 6 3 2 1 1 2 1 7 4 0 1 2 1 1 5 5 1 2 2 1 0 6 6 2 1 1 2 1 7 7 1 1 2 1 0 5 8 0 1 1 1 0 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Discussion</head><p>As can be seen in table <ref type="table" coords="5,182.96,250.68,3.77,9.02">2</ref>, the best performing schemes are scheme 3 and scheme 6 which both result in 7 accurately predicted narrative peaks and hence an accuracy of 47%. These two schemes both include the text based feature and the weights vector. Scheme 6 also contains the audio based feature but fails to achieve an increased accuracy because of this inclusion. Considering that there is also strong disagreement between annotators, an accuracy of 47% (compared against the joint annotations of three annotators) shows the potential of using the automated narrative peak detector. The fact that this best performing scheme is only based on a text based feature corresponds well to our initial observation that there is no clear audiovisual characteristic of a narrative peak when observing the clips. All non-Dutch speaking observers failed to see indicators of narrative peaks. The observation that narrative peaks seem to correspond to the introduction of new topics in the clip can only be made when an observer understands the spoken content. It is expected that this observation is also true for clips in other languages. In our contributions to VideoCLEF, we included five runs, mainly corresponding to some of the different schemes that were previously used in table <ref type="table" coords="5,247.35,388.68,3.77,9.02" target="#tab_0">1</ref>. The results of our runs on the test-data, and their explanations are given in table 3. For number 5, all narrative peaks were randomly selected (for comparison). Evaluation of these runs was performed in two ways: Peak-based (similar to our scoring system on the training data) and Point-based which can be explained as follows; If a peak that we detected coincides with annotations of more than one reviewer annotation, multiple points are added. Hence the maximum-maximum score for a clip can be nine when annotators fully agree on segments, the minimum-maximum score remains three when annotators fully disagree. The difference between the two scoring system lies in the fact that the Point-based scoring system awards more than one point to segments which were selected by more then one annotator. If annotators agree on segments with increased dramatic tension, there will be (in total over three annotators) less annotated segments and hence the probability that by chance our automated approach selects an annotated segment will decrease. Therefore, awarding more points to the detection of these less probable segments seems logical. Moreover, a segment on which all annotators agree must be a really relevant segment of increased tension. On the other hand, this Pointbased approach to scoring gives equal points to having just one correctly detected segment in a clip (annotated by all three annotators) and to detecting all three segments correctly (each of them by one annotator). When considering that annotators may have different tastes and that one annotator could fully disagree with the other two annotators, it is unsatisfying that a system that would fully resemble this annotator's taste gets rewarded only the same number of points as a system that predicts merely one correct segment. In this view, a 100% correspondence with a human annotator should lead to an optimal score for this clip, since the program cannot be expected to outperform the people that are hired to evaluate it. Because our runs were selected based on the results that were obtained using the Peak-based scoring system, results on the test data are mainly compared to this scoring. First of all, it should be noted that results are never far better than random level, as can be seen by comparing to run number 5. Surprisingly, the Peak-based and Point-based scores show a distinctly different ranking of the runs. Run 1 performed the worst under the Point-based scoring, yet it performed best under the Peak-based scoring system. Based on the results obtained on the clips in the test set, it was expected that runs 1 and 3 would perform best. This is clearly reflected in the results we obtain when using the same evaluation method on the test clips, the Peak-based evaluation. However, with the Point-based scoring system this effect disappears. This may indicate that the main feature that we used, the text based feature based on the introduction of a new topic, does not reflect properly the notion of dramatic tension for all annotators, but is biased towards a single annotator. In Fig. <ref type="figure" coords="6,145.06,253.26,3.75,9.02" target="#fig_2">3</ref>, the score is shown when calculated based on only the annotations of a single annotator. It should be noted that in this setting, the Point-based and Peak-based scoring system are identical. In this figure, it can be seen that scoring based only on annotator 3 performed significantly worse than scoring based only on annotator 1 or annotator 2. This indicates that our approach is biased towards the opinions of some reviewers. Because our runs were selected based on those schemes that performed best under Peak-based metric, one should mainly compare to the Peak-based results on the test-clips. Knowing now the exact scoring system that was eventually employed in the VideoCLEF scoring, it would be recommendable to also compute the results on the training set using the Point-based metric. Possible, this would lead to a different ranking of the evaluated schemes that is more similar to the ranked results VideoCLEF reported on the Point-based evaluation. However, such a reflection on the training data was not performed because of the strict deadlines for submitting working notes (seven working-days after releasing the results).</p><p>a The Peak-based score reported here deviates slightly from the official Peak-based score reported by the VideoCLEF organizers.</p><p>In the VideoCLEF score, nearby peaks of different annotators were merged in order to create a new 10s segment. This implies that when an estimated peak is nearby a peak detected ONLY by annotator 2, this estimated peak will score points. However, if annotator 1 or 3 also selected a nearby peak, the interval in which points can be scored shifts and the same estimated peak may not score a point anymore. Since the second situation actually involves an estimated peak which is not far from two annotated peaks, it seems contradictory to the authors to not award points here. For evaluating the training data, points were awarded under similar circumstances and hence we did award points to such peaks also for the test data.</p><p>The subtask described in the VideoCLEF 2009 Benchmark Evaluation has proven to be a challenging and difficult one. Failing to see obvious features when viewing the clips and only seeing a mild connection between new topics and dramatic tension peaks, we resorted to the detection of the start of new topics in the text annotations of the provided video clips and the use of some basic video-and audio-based features. In our initial evaluation based on the training clips, the text based feature proved to be the most relevant one and hence our submitted evaluation-runs were centred around this feature. When using a consistent evaluation of training and test clips, the text based feature also led to our best results on the test data. The overall detection accuracy based on the text-based feature dropped from 47% correct detection on the training data to 24% on the test data. It should be stated that results on the test data were just mildly above random level.</p><p>The reported results based on the Point-based scoring differed strongly from the results obtained using the scoring system that was employed on the training data. It was shown that this is probably caused by a bias of our method towards the annotations given by annotators 1 and 2.</p><p>Given the challenging task that was given, it is our strong belief that the indication that text based features (related to the introduction of new topics) perform well, is a valuable contribution in the search for an improved dramatic tension detector.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,70.92,220.05,453.57,8.10;3,70.92,230.43,161.77,8.10"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Illustration of single modality feature values computed over time. A: Video feature, B: Audio features, C: Text feature. All figures are based on BG_37016.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,70.92,223.47,453.54,8.10;4,70.92,233.79,223.69,8.10"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Histogram that illustrates when dramatic tension-points occur in the clips according to the annotators. Note that during the first several seconds there is no tension-point at all.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,70.92,449.49,453.54,8.10;6,70.92,459.81,453.64,8.10;6,70.92,470.19,109.64,8.10"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Histograms that illustrates scoring based on single annotators (x axis). The upper row of histograms indicate scoring (y axis) for the different runs. The lower histogram shows the joint histogram of all runs except run 5, which involved random selection of segments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,70.92,532.95,396.22,74.10"><head>Table 1 .</head><label>1</label><figDesc>Schemes for feature combinations.</figDesc><table coords="4,128.34,556.83,338.80,50.22"><row><cell cols="3">Scheme number Used features Weights</cell><cell cols="2">Scheme number Used features</cell><cell>Weights</cell></row><row><cell>1</cell><cell>Video</cell><cell>Yes</cell><cell>5</cell><cell>Video, Text</cell><cell>Yes</cell></row><row><cell>2</cell><cell>Audio</cell><cell>Yes</cell><cell>6</cell><cell>Audio, Text</cell><cell>Yes</cell></row><row><cell>3</cell><cell>Text</cell><cell>Yes</cell><cell>7</cell><cell>Video, Audio, Text</cell><cell>Yes</cell></row><row><cell>4</cell><cell>Video, Audio</cell><cell>Yes</cell><cell>8</cell><cell>Text</cell><cell>No</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,70.92,73.26,351.37,73.48"><head>Table 3 .</head><label>3</label><figDesc>Results on the test set.</figDesc><table coords="6,173.10,85.12,249.19,61.61"><row><cell cols="4">run number (scheme nr) Score (Peak-based) a Score (Point-based)</cell></row><row><cell>1</cell><cell>3</cell><cell>33</cell><cell>39</cell></row><row><cell>2</cell><cell>7</cell><cell>30</cell><cell>41</cell></row><row><cell>3</cell><cell>6</cell><cell>33</cell><cell>42</cell></row><row><cell>4</cell><cell>8</cell><cell>32</cell><cell>43</cell></row><row><cell>5</cell><cell>--</cell><cell>32</cell><cell>43</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>The research leading to these results has received funding from the <rs type="funder">European Community</rs>'s <rs type="programName">Seventh Framework Programme</rs> [<rs type="grantNumber">FP7/2007-2011</rs>] under grant agreement n° <rs type="grantNumber">216444</rs> (see Article <rs type="grantNumber">II.30</rs>. of the Grant Agreement), <rs type="funder">NoE PetaMedia</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_9PxbjEr">
					<idno type="grant-number">FP7/2007-2011</idno>
					<orgName type="program" subtype="full">Seventh Framework Programme</orgName>
				</org>
				<org type="funding" xml:id="_mQtpJYp">
					<idno type="grant-number">216444</idno>
				</org>
				<org type="funding" xml:id="_7qVnKMJ">
					<idno type="grant-number">II.30</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,85.12,415.62,403.90,9.02;7,97.92,427.14,376.12,9.02;7,97.92,438.60,161.45,9.02" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,388.18,415.62,100.84,9.02;7,97.92,427.14,255.98,9.02">A framework for flexible summarization of racquet sports video using multiple modalities</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><forename type="middle">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,360.42,427.14,113.62,9.02;7,97.92,438.60,57.01,9.02">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="424" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,85.13,450.12,435.51,9.02;7,97.92,461.64,281.56,9.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,283.76,450.12,236.87,9.02;7,97.92,461.64,35.24,9.02">Dynamic video summarization using two-level redundancy detection</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">H</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,139.57,461.64,140.80,9.02">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="250" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,85.12,473.10,427.44,9.02;7,97.92,484.62,385.80,9.02;7,97.92,496.14,186.14,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,361.17,473.10,151.39,9.02;7,97.92,484.62,291.16,9.02">A highlight scene detection and video summarization system using audio feature for a Personal Video Recorder</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Otsuka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Nakane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Hatanaka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ogawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,395.90,484.62,87.82,9.02;7,97.92,496.14,87.24,9.02">IEEE Transactions on Consumer Electronics</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="112" to="116" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,85.12,519.12,435.67,9.02;7,97.92,530.58,414.67,9.02;7,97.92,542.10,141.18,9.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,317.67,519.12,203.12,9.02;7,97.92,530.58,318.71,9.02">Affective Characterization of Movie Scenes Based on Multimedia Content Analysis and User&apos;s Physiological Emotional Responses</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Chanel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J M</forename><surname>Kierkels</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,437.21,530.58,75.38,9.02;7,97.92,542.10,107.50,9.02">IEEE International Symposium on Multimedia</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,85.12,553.62,399.84,9.02;7,97.92,565.08,373.94,9.02" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,250.79,553.62,234.17,9.02;7,97.92,565.08,77.23,9.02">How to Distinguish Posed from Spontaneous Smiles using Geometric Features</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,195.25,565.08,196.50,9.02">ACM Int&apos;l Conf.Multimodal Interfaces (ICMI&apos;07)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,85.12,576.60,427.50,9.02;7,97.92,588.12,285.73,9.02" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,205.43,576.60,200.10,9.02">Towards detection of interest during movie scenes</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">J M</forename><surname>Kierkels</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,425.73,576.60,86.90,9.02;7,97.92,588.12,188.28,9.02">PetaMedia Workshop on Implicit, Human-Centered Tagging (HCT&apos;08</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Abstract only</note>
</biblStruct>

<biblStruct coords="7,85.12,599.58,432.77,9.02;7,97.92,611.10,140.84,9.02" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,245.25,599.58,193.29,9.02">Using film cutting techniques in interface design</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">P</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">J</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,445.08,599.58,72.80,9.02;7,97.92,611.10,41.97,9.02">Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="325" to="372" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,85.13,622.62,437.15,9.02;7,97.92,634.08,420.52,9.02;7,97.92,645.60,26.68,9.02" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,237.77,622.62,284.52,9.02;7,97.92,634.08,241.38,9.02">Measuring the effect of fundamental frequency raising as a strategy for increasing vocal intensity in soft, normal and loud phonation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Alku</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vintturi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Vilkman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,346.03,634.08,93.58,9.02">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3--4</biblScope>
			<biblScope unit="page" from="321" to="334" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,84.73,657.12,427.28,9.02;7,97.92,668.57,26.68,9.02" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="7,172.34,657.12,171.12,9.02">Intonation and evaluation in oral narratives</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wennerstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,349.77,657.12,85.40,9.02">Journal of Pragmatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1183" to="1206" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
