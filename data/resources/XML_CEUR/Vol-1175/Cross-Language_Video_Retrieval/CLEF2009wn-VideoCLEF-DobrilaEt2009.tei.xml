<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,189.84,142.31,231.72,11.85">UAIC: Participation in VideoCLEF Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,167.16,178.48,95.18,8.48"><forename type="first">Tudor-Alexandru</forename><surname>Dobrilă</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.68,178.48,96.54,8.48"><forename type="first">Mihail-Ciprian</forename><surname>Diaconaşu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,371.88,178.48,67.97,8.48"><forename type="first">Irina-Diana</forename><surname>Lungu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,280.68,189.28,50.23,8.48"><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,189.84,142.31,231.72,11.85">UAIC: Participation in VideoCLEF Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">88C090286FF3E4E4FF192C91B5509EAA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This year marked UAIC 1 's first participation at the VideoCLEF competition. Our group built two separated systems for tasks "Subject Classification" and "Affect Detection". For first task we created two resources starting from Wikipedia pages and pages identified with Google and used two tools for classification: Lucene and Weka. For the second task we extract the audio component from a given video file, with FFmpeg codec. After that, we computed the average intensity for each word from the transcript, by using Fast Fourier Transformations to analyze the sound. A brief description of our system components is given in this paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>VideoCLEF<ref type="foot" coords="1,189.12,401.72,2.81,5.11" target="#foot_1">2</ref> offers cross-language classification, retrieval and analysis tasks on a video collection containing documentaries and talk shows.</p><p>In 2009, the collection extended the corpus used for the 2008 VideoCLEF pilot track. Task participants were provided with video data along with speech recognition transcripts, archival metadata, shot segmentation and shot-level keyframes. Two classification tasks were evaluated: "Subject Classification", which involves automatically tagging videos with subject labels, and "Affect and Appeal", which involves classifying videos according to characteristics beyond their semantic content. The track was coordinated by Dublin City University (IE) and Delft University of Technology (NL).</p><p>Our team participated in the following tasks: in Subject Classification (in which participants must tagging automatically videos with subject labels such as 'Archeology', 'Dance', 'History', 'Music', etc.) and in Affect Detection (in which participants must select keyframes using a combination of video and speech/audio features and these selected keyframes should represent the semantic content of the video, e.g., an episode of a documentary).</p><p>The way in which we classified a video accordingly to it transcript is described in Section 2, while Section 3 is concerned with presentation of details related to the extraction of keyframes. Last Section presents conclusions regarding our participation in VideoCLEF 2009.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Subject Classification</head><p>In order to classify a video according to its transcripts we perform the following steps:</p><p>• </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Extract Relevant Words from Wikipedia</head><p>First of all, we found an URL pattern for each relevant article of each category. Using this pattern we identified the most important pages from Wikipedia. The source pages for these pages are retrieved directly from Wikipedia Server creating direct connections for each page and then these source pages are save into a single file according to each category.</p><p>Second of all, for each such a file, the XHTML/HTML tags are eliminated from the files and we save only the paragraphs (the information contained in the &lt;p&gt;&lt;/p&gt; tags).</p><p>Third of all, the stop words and punctuation signs are eliminated; and all words are transformed to lower case.</p><p>Next, we lemmatize all remaining words and for each lemma we count the number of appearances.</p><p>In the end we normalize to 1000 for each category the sum of number of appearances. This step was necessary because initial for some categories like Music the number of relevant pages was very high, and the sum of number of appearances was also very high in comparison with other categories. Without normalizing a transcript with a word from Music category is automatically classified in Music category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Extract Relevant Words from Google</head><p>This part is similar with part performed on Wikipedia with few differences. One of differences is related to fact that from relevant pages we extract only words from &lt;keywords&gt; tag. The second main difference is the fact that we split the content of the &lt;keyword&gt; tag after comma separator and in this way we considers important for one category a succession of few words. In this way we search the context in which relevant words for one category appear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Lucene</head><p>Lucene is a high performance, scalable Information Retrieval (IR) library. It allows adding indexing and searching capabilities to applications. Lucene is a mature, free, open-source project implemented in Java <ref type="bibr" coords="3,300.00,225.52,136.41,8.48" target="#b0">(Hatcher, E. and Gospodnetic, 2005)</ref>.</p><p>Instead to index files corresponding to categories created at previous steps from Google and Wikipedia, we created another files from these files in which every word appear by a number proportional with associated number from corresponding file. In this way the Lucene score will be higher if the word from associated file to categories has a higher number of appearances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Weka</head><p>Weka<ref type="foot" coords="3,165.12,331.88,2.81,5.11" target="#foot_2">3</ref> (Waikato Environment for Knowledge Analysis) is a popular suite of machine learning software written in Java, developed at the University of Waikato. The Weka workbench <ref type="bibr" coords="3,188.04,353.80,93.45,8.48" target="#b1">(Witten and Frank, 2005</ref>) contains a collection of visualization tools and algorithms for data analysis and predictive modeling, together with graphical user interfaces for easy access to this functionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Submitted Runs</head><p>We submitted 4 runs described below: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Affect Detection</head><p>At this part, our work is based on the assumption that a narrative peak is a point in the movie where the narrator raises his voice within a given phrase, in order to emphasize a certain idea. This means that a group of words is said more intensely than the way previous words are said and, since this applies in any language, we were able to develop a language independent application. This is why our approach is based on two aspects of the video: the sound and the ASR transcript. The first step is the extraction of the audio from a given video file, which we accomplished by using FFmpeg<ref type="foot" coords="4,265.68,463.16,2.81,5.11" target="#foot_3">4</ref> codec. We then computed the average intensity for each word from the transcript, by using Fast Fourier Transformations (FFT<ref type="foot" coords="4,450.24,473.96,2.81,5.11" target="#foot_4">5</ref> ) to analyze the sound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1:</head><p>Graph where the X-axis represents the words indexes and the Y-axis the average intensity of the words for BG_36926.cinepak.avi</p><p>We then computed a score for any group of words (which spanned between 5 and 10 seconds) based on the previous group of words. The score is a weighted mean of several metrics.</p><p>Example of results for BG_36926.cinepak.avi : Table <ref type="table" coords="6,225.48,173.31,3.27,7.60">3</ref>: Output example for "BG_36926.cinepak.avi" video &lt;?xml version="1.0" encoding="iso-8859-1" standalone="no"?&gt; &lt;annotation&gt; &lt;head&gt; &lt;specification src="Segment.xml"/&gt; &lt;video src="BG_36926.cinepak.avi"/&gt; &lt;info key="coder" type="String"&gt;Example&lt;/info&gt; &lt;/head&gt; &lt;body&gt; &lt;track name="Segment" type="primary"&gt; &lt;el end="396.6" index="0" start="389.42"&gt; &lt;attribute name="Label"&gt;peak&lt;/attribute&gt; &lt;/el&gt; &lt;el end="117.85" index="1" start="112.7"&gt; &lt;attribute name="Label"&gt;peak&lt;/attribute&gt; &lt;/el&gt; &lt;el end="241.23" index="2" start="234.02"&gt; &lt;attribute name="Label"&gt;peak&lt;/attribute&gt; &lt;/el&gt; &lt;/track&gt; &lt;/body&gt; &lt;/annotation&gt; We submitted 3 runs with following characteristics: In total, 60 hours of assessor time were devoted to creating the reference files of the narrative peaks for the 45 Beeldenstorm episodes used in the VideoCLEF 2009 Affect Task. Three assessors watched each of the 45 test files and marked their top three narrative peaks using the Anvil tool.</p><p>The results are presented in next table: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This paper presents the UAIC system which took part in the VideoCLEF 2009 competition. Our group built two separated systems for tasks "Subject Classification" and "Affect Detection".</p><p>For Subject Classification task we created two resources starting from Wikipedia pages and pages identified with Google search engine. These resources are then used by Lucene and Weka tools for classification.</p><p>For Affect Detection task we extract the audio component from a given video file, with FFmpeg codec. After that, we computed the average intensity for each word from the transcript, by using Fast Fourier Transformations to analyze the sound. In the end, like final result in this task we considered only the top 3 values obtained in previous step.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,214.92,432.03,181.82,7.70;4,143.28,251.04,324.24,169.32"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: UAIC system used in Affect Detection task</figDesc><graphic coords="4,143.28,251.04,324.24,169.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,182.40,450.15,246.59,155.98"><head>Table 1 :</head><label>1</label><figDesc>Subject Classification: Characteristics of Runs</figDesc><table coords="3,182.40,466.32,246.59,139.80"><row><cell>Run ID</cell><cell>Tools and Resources used</cell></row><row><cell>Run 1</cell><cell>• It uses only Lucene for classification</cell></row><row><cell></cell><cell>• Like resources are used both resources obtained</cell></row><row><cell></cell><cell>from Wikipedia and Google</cell></row><row><cell>Run 2</cell><cell>• It uses only Weka for classification</cell></row><row><cell></cell><cell>• Like resources are used only resources obtained</cell></row><row><cell></cell><cell>from Google</cell></row><row><cell>Run 3</cell><cell>• It uses only Weka for classification</cell></row><row><cell></cell><cell>• Like resources are used only resources obtained</cell></row><row><cell></cell><cell>from Wikipedia</cell></row><row><cell>Run 4</cell><cell>• It uses Weka and Lucene for classification</cell></row><row><cell></cell><cell>• Like resources are used both resources obtained</cell></row><row><cell></cell><cell>from Wikipedia and Google</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,177.96,449.55,253.39,177.10"><head>Table 4 :</head><label>4</label><figDesc>Affect Detection: characteristics of runs</figDesc><table coords="6,177.96,465.72,253.39,160.92"><row><cell>Run ID</cell><cell>Metrics Used When Computing The Score</cell></row><row><cell>Run 1</cell><cell>• Ratio of Current Group and Previous Group Means</cell></row><row><cell></cell><cell>of Intensities</cell></row><row><cell></cell><cell>• Ratio of Current Group and Previous Group Quartile</cell></row><row><cell></cell><cell>Coefficients of Dispersion</cell></row><row><cell>Run 2</cell><cell>• Ratio of Current Group and Previous Group Means</cell></row><row><cell></cell><cell>of Intensities</cell></row><row><cell></cell><cell>• Ratio of Current Group and Previous Group Quartile</cell></row><row><cell></cell><cell>Coefficients of Dispersion</cell></row><row><cell></cell><cell>• Ratio of Current Group and Previous Group</cell></row><row><cell></cell><cell>Coefficients of Variation</cell></row><row><cell>Run 3</cell><cell>• Ratio of Current Group and Previous Group Means</cell></row><row><cell></cell><cell>of Intensities</cell></row><row><cell></cell><cell>• Ratio of Current Group and Previous Group</cell></row><row><cell></cell><cell>Coefficients of Variation</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,164.16,205.71,282.52,69.10"><head>Table 5 :</head><label>5</label><figDesc>Runs Evaluation</figDesc><table coords="7,164.16,222.00,282.52,52.80"><row><cell>Run ID</cell><cell>Point based</cell><cell>Peaks based</cell><cell>Peaks based</cell><cell>Peaks based</cell></row><row><cell></cell><cell>scoring</cell><cell>scoring 1</cell><cell>scoring 2</cell><cell>scoring 3</cell></row><row><cell>Run 1</cell><cell>33</cell><cell>26</cell><cell>7</cell><cell>2</cell></row><row><cell>Run 2</cell><cell>41</cell><cell>29</cell><cell>10</cell><cell>3</cell></row><row><cell>Run 3</cell><cell>33</cell><cell>24</cell><cell>7</cell><cell>2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,148.20,635.10,82.58,7.66"><p>"Al. I. Cuza" University</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,148.32,644.94,171.43,7.66"><p>VideoCLEF: http://www.cdvp.dcu.ie/VideoCLEF/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,148.32,644.94,154.27,7.66"><p>Weka: http://www.cs.waikato.ac.nz/ml/weka/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,148.32,635.10,93.07,7.66"><p>FFmpeg: http://ffmpeg.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,148.32,644.94,196.29,7.66"><p>FFT: http://en.wikipedia.org/wiki/Fast_Fourier_transform</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We also like to give a special "thank you" to those who helped from the very beginning of the project: our colleagues from second year group 1 B.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="7,146.43,569.70,296.78,7.66" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Hatcher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Gospodnetic</surname></persName>
		</author>
		<title level="m" coord="7,268.32,569.70,55.24,7.66">Lucene in action</title>
		<imprint>
			<publisher>Manning Publications Co</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,146.43,579.54,321.95,7.66;7,153.96,589.26,314.33,7.66;7,153.96,598.86,171.31,7.66" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,237.84,579.54,230.54,7.66;7,153.96,589.26,23.58,7.66">Data Mining: Practical machine learning tools and techniques, 2nd Edition</title>
		<author>
			<persName coords=""><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<idno>Retrieved 2007-06-25</idno>
		<ptr target="http://www.cs.waikato.ac.nz/~ml/weka/book.html" />
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
