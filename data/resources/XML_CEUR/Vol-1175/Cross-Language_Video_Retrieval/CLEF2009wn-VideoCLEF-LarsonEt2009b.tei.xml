<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,93.80,148.79,415.41,15.48;1,133.59,170.71,335.82,15.48">Exploiting Speech Recognition Transcripts for Narrative Peak Detection in Short-Form Documentaries</title>
				<funder ref="#_8qMhVkw">
					<orgName type="full">European Commission</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,155.09,204.79,58.93,8.64"><forename type="first">Martha</forename><surname>Larson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mediamatics</orgName>
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,230.45,204.79,53.96,8.64"><forename type="first">Bart</forename><surname>Jochems</surname></persName>
							<email>b.e.h.jochems@student</email>
							<affiliation key="aff1">
								<orgName type="department">Human Media Interaction</orgName>
								<orgName type="institution">University of Twente</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,300.83,204.79,50.65,8.64"><forename type="first">Ewine</forename><surname>Smits</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mediamatics</orgName>
								<orgName type="institution">Delft University of Technology</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,367.90,204.79,75.54,8.64"><forename type="first">Roeland</forename><surname>Ordelman</surname></persName>
							<email>ordelman@ewi.utwente.nl</email>
							<affiliation key="aff1">
								<orgName type="department">Human Media Interaction</orgName>
								<orgName type="institution">University of Twente</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,93.80,148.79,415.41,15.48;1,133.59,170.71,335.82,15.48">Exploiting Speech Recognition Transcripts for Narrative Peak Detection in Short-Form Documentaries</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CDD9F4AA8F6A5182FE7FA99AFBA72A4A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing Measurement</term>
					<term>Performance</term>
					<term>Experimentation Spoken content</term>
					<term>Narrative</term>
					<term>Dramatic tension</term>
					<term>Speech recognition transcripts</term>
					<term>Short-form documentaries</term>
					<term>Dutch language</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Narrative peaks are points at which the viewer perceives a spike in the level of dramatic tension within the narrative flow of a video. This paper reports on four approaches to narrative peak detection in television documentaries that were developed by a joint team consisting of members from Delft University of Technology and the University of Twente within the framework of the VideoCLEF 2009 Affect Detection task. The approaches make use of speech recognition transcripts and seek to exploit various sources of evidence in order to automatically identify narrative peaks. These sources include speaker style (word choice), stylistic devices (use of repetitions), strategies strengthening viewers' feelings of involvement (direct audience address) and emotional speech. These approaches are compared to a challenging baseline that predicts the presence of narrative peaks at fixed points in the video, presumed to be dictated by natural narrative rhythm or production convention. Two approaches are tied in delivering top narrative peak detection results. One uses counts of first and second person pronouns to identify points in the video where viewers feel most directly involved. The other uses affective word ratings to calculate scores reflecting emotional language.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While watching video content, viewers feel fluctuations in their emotional response that can be attributed to their perception of changes in the level of dramatic tension. In the literature on affective analysis of video, two types of content have received particular attention: sports games and movies <ref type="bibr" coords="1,426.36,695.76,10.58,8.64" target="#b1">[2]</ref>. These two cases differ with respect to the source of viewer-perceived dramatic tension. In the case of sports, tension spikes arise as a result of the unpredictable interactions of the players within the rules and physical constraints of the game. In the case of movies, dramatic tension is carefully crafted into the content by a team including scriptwriters, performers, special effects experts, directors and producers. The difference between the two cases is the amount and nature of human intention -i.e., premeditation, planning, intervention -involved in the creation of the sequence of events that plays out over time (and space). We refer to that sequence as a narrative and to high points in the dramatic tension within that narrative as narrative peaks. We are interested in investigating a third case of video content, namely television documentaries. We consider documentaries to be a form of "edu-tainment," whose purpose is both to inform and entertain the audience. The approaches described and tested here have been developed in order to detect narrative peaks within documentary videos.</p><p>Our work differs in an important respect from previous work in the domains of sports and movies. Dramatic tension in documentaries is never completely spontaneous -the narrative curve follows a previously laid out plan, for example a script or an outline, that is carried out during the process of production. However, dramatic tension is characteristically less tightly controlled in a documentary than it would be in a movie. In a movie, the entire content is subordinated to the plot, whereas a documentary may follow one or more story lines, but it simultaneously pursues the goal of providing the viewer with factual subject matter. Because of these differences, we chose to dedicate separate and specific attention to the affective analysis of documentaries and in particular to the automatic detection of narrative peaks.</p><p>This area of investigation is quite challenging since fluctuations in dramatic tension in television documentaries are not associated with conventionalized events. If an event is a conventional trigger, a broad spectrum of viewers will agree about its contribution to the drama of the video content -think of goals in the game of soccer or a kiss in a romantic comedy. The subtleness with which narrative peaks manifest themselves in video documentaries makes the task challenging with respect to the difficulty of both automatically detecting such peaks and also evaluating the detection algorithm. Our interest is contextualized within the broader goal of automatic prediction of topic-independent viewer preference. Given two videos with comparable informational content, viewers will often decide to choose to watch one over the other. Our ultimate research aim is to explore the contribution that analysis of affective aspects of video content can make to the automatic prediction of viewer preference.</p><p>This paper reports on joint work carried out by research groups at two universities in the Netherlands, Delft University of Technology<ref type="foot" coords="2,215.56,421.50,3.49,6.05" target="#foot_0">1</ref> and the University of Twente, on the Affect Detection task of the Video-CLEF<ref type="foot" coords="2,114.36,433.46,3.49,6.05" target="#foot_1">2</ref> track of the 2009 Cross-Language Evaluation Forum (CLEF) <ref type="foot" coords="2,365.31,433.46,3.49,6.05" target="#foot_2">3</ref> benchmark evaluations. The Affect Detection task involves automatically identifying narrative peaks in short-form documentaries. In the rest of this paper, we first give a brief description of the data and the task. Then, we present the approach that we took to the task and give the details of the algorithms used in each of the five runs that we submitted. We report the results achieved by these runs and then conclude with a summary and outlook.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Set and Task Definition</head><p>The data set for the VideoCLEF 2009 Affect Detection task consisted of 45 episodes from the Dutchlanguage short-form documentary series called Beeldenstorm (in English, 'Iconoclasm'). The series treats topics in the visual arts, integrating elements from history, culture and current events. Beeldenstorm is hosted by Prof. Henk van Os, known not only for his art expertise, but also for his narrative ability. Henk van Os is highly acclaimed and appreciated in the Netherlands, where he has established his ability to appeal to a broad audience. <ref type="foot" coords="2,198.72,617.62,3.49,6.05" target="#foot_3">4</ref>Constraining the corpus to contain episodes from Beeldenstorm limits the spoken content to a single speaker speaking within the style of a single documentary series. This limitation is imposed in order to help control effects that could be introduced by variability in style or skill. Experimentation of the ability of algorithms to transfer performance to other domains is planned for future years. An additional advantage of using the Beeldenstorm series is that the episodes are relatively short, approximately eight minutes in length. Because they are short, the assessors who create the ground truth for the test collection are able to watch each video in its entirety. In short, the Beeldenstorm program provides a highly suitable corpus for developing and evaluating algorithms for narrative peak detection.</p><p>Ground truth was created for the Beeldenstorm by a team of assessors who speak Dutch natively or at an advanced level. The assessors were told that the Beeldenstorm series is known to contain humorous and moving moments and told that they could use that information to formulate an opinion of what constitutes a narrative peak. They were asked to mark the three points in the video where their perception of the level of dramatic tension reached the highest peaks. Peaks were required to be a maximum of ten seconds in length.</p><p>For the Affect Detection task of VideoCLEF 2009, task participants were supplied with an example set containing five Beeldenstorm episodes in which example narrative peaks had been identified by a human assessor. On the basis of their observations and generalizations concerning the peaks marked in the example set, the task participants designed algorithms capable of automatically detecting similar peaks in the test set. The test set contained 45 videos and was mutually exclusive with the example set. Participants were required to identify the three highest peaks in each episode. Up to five different runs (i.e., system outputs created according to different experimental conditions) could be submitted. Further details about the data set and the Affect Detection task for VideoCLEF 2009 can be found in the track overview paper <ref type="bibr" coords="3,469.50,291.67,10.58,8.64" target="#b3">[4]</ref>. Participants were provided with additional resources accompanying the test data, including transcripts generated by an automatic speech recognition system <ref type="bibr" coords="3,267.86,315.58,10.58,8.64" target="#b2">[3]</ref>. Our approaches, described in the next section, focus on exploiting the contents of the speech transcripts for the purpose of automatically detecting narrative peaks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Narrative Peak Detection Approaches</head><p>Our approaches consist of a sophisticated baseline and four other techniques for using speech recognition transcripts to automatically detect narrative peaks. We describe each algorithm in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Fixing Time Points (duotu09fix)</head><p>Our baseline approach duotu09fix 5 hypothesizes fixed time points for three narrative peaks in each episode. These points are completely independent of episode content and are the same for every episode. This approach attempts to exploit regularities that exist in the narrative structure of every episode of a documentary series as a result of production conventions or of general documentary structure (i.e., a documentary consists or an opening, a body and a conclusion). We chose this approach in order to establish a challenging baseline against which our speech-transcript-based peak detection algorithms can be compared. In order to choose the three fixed time points we analyzed the peak positions in the example set. In the examples, the midpoint of the first peak occurred between 28 secs and 1 min 6 secs after the start of the video. The midpoint of the final peak occurred between 6 mins 42 secs and 7 mins 40 secs into the video. We fixed a peak at the average position of the initial peak (44 secs) and the final peak (7 mins 9 secs). We added a third located at the average midpoint of the episode: 3 mins 40 secs. The fact that four of the five example episodes have a peak within 10 seconds of this point confirmed that we had made a good choice for the third fixed point peak.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Counting Indicator Words (duotu09ind)</head><p>We viewed the example videos and examined the words that were spoken during the narrative peaks that the assessor had marked in these videos. We formulated the hypothesis that the speaker applies a narrow range of strategies for creating narrative peaks in the documentary. These strategies might be reflected in a relatively limited vocabulary of words that could be used as indicators in order to predict the position of narrative peaks.</p><p>We compiled a list of narrative peak indicators by analyzing the words spoken during each of the example peaks and compiled a list of words and word-stems that seemed relatively independent of the topic at the point in the video and which could be plausibly characteristic of the general word use of the speaker during peaks. The indicator words selected are listed in Table <ref type="table" coords="3,368.01,716.08,3.74,8.64" target="#tab_0">1</ref>. It is noteworthy that most of these words are adjectives or adverbs and that they have a basic positive or negative meaning, or they serve as an intensifier. The word maar, 'but', appears to be an exception to this generalization. We included this word because it occurred in 20% of the peaks in the example set. 'But' is a lexical item frequently used to indicate a contrast with previously established state of knowledge or expectations. We hypothesize that its importance is related to the suspense introduced by statements that contrast with established knowledge or viewer expectations. The duotu09ind algorithm detects narrative peaks using the following sequence of steps. First, a set of all possible peak candidates was established by moving a 10-second sliding window over the speech recognition transcripts, advancing the window by one word at each step. Each peak candidate is maximally 10 seconds in length, but can be shorter if the speech in the window lasts for less than the 10-second duration of the window. Peak candidates of less than three seconds in length are discarded. Then, the peak candidates are ranked with respect to the raw count of the indicator words (cf. Table <ref type="table" coords="4,432.57,529.07,4.15,8.64" target="#tab_0">1</ref>) that they contain. The size limitation of the sliding window already introduces a normalizing effect and for this reason we do not undertake further normalization of the raw counts. Finally, peak candidates are chosen from the ranked list, starting at the top, until a total of three peaks has been selected. If a candidate has a midpoint that falls within eight seconds of the midpoint of a previously selected candidate occurring in the list, that candidate is discarded and the next candidate from the list is considered instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Counting Word Repetitions (duotu09rep)</head><p>Analysis of the word distributions in the example set suggested that repetition may be a stylistic device that is deployed to create peaks. Particular examples of the use of repetition during narrative peaks in the example episodes include, ...kunsttulpen mooie kunsttulpen... ('...artificial-tulips, beautiful artificial-tulips...'), <ref type="foot" coords="4,509.02,656.30,3.49,6.05" target="#foot_4">6</ref>...het is werkelijk een ervaring, een ervaring van stilte... ('...it is really an experience, an experience of tranquility...'),<ref type="foot" coords="4,146.77,680.21,3.49,6.05" target="#foot_5">7</ref> and ...wordt belangrijk, is altijd belangrijk geweest... ( '...will be important, has always been important...'). <ref type="foot" coords="4,167.05,692.16,3.49,6.05" target="#foot_6">8</ref> We do not attempt to measure repetition of phrases or of morphologically related words, but rather assume that counting repeated word forms will yield an adequate indicator or places in the documentary where repetition is being applied as a stylistic device.</p><p>The duotu09rep algorithm uses the same list of peak candidates described in the previous section in the explanation of duotu09ind. The peak candidates are ranked by the number of occurrences they contain of words that occur multiple times. In order to eliminate the impact of function words, stop word removal is performed before the peak candidates are scored. Three peaks are selected starting from the top of the ranked list of peak candidates, using the same procedure as was described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Counting First and Second Person Pronouns (duotu09pro)</head><p>We conjecture that dramatic tension rises along with the level to which the viewers feel that they are directly involved in the video content they are watching. The duotu09pro approach identifies two possible conditions of heightened viewer involvement: when viewers feel that the speaker in the videos is addressing them directly or as individuals, or, second, when viewers feel that the speaker is sharing something personal. Although we do not examine this aspect more closely here, it is possible that the importance of personal connection or personal revelation in documentary video is related to the fact that viewers perceive it to be a relatively rare event, which triggers them to sit up and take notice.</p><p>In the duotu09pro approach we use second person pronominal forms (e.g., u, 'you'; uw 'your') to identify audience directed speech and first person pronominal forms (e.g., ik, 'I') to identify personal revelation of the speaker. Notice that first person plural forms (e.g., wij 'we') might actually be correlated with either case, serving generally to draw the audience into the narrative. Cases of narrative peaks that support the viability of this approach occur in the example set, e.g., ...ziet u hoe diep de tulp in ons nationale volksziel is ingedrongen... ('...you see how deeply the tulip has penetrated our national consciousness...').<ref type="foot" coords="5,463.33,371.07,3.49,6.05" target="#foot_7">9</ref> In the case of Beeldenstorm, second person informal pronominal forms (e.g., je, 'you, your') should also be attributed this general role as well since they are used as impersonal pronouns to describe the thoughts and actions of a hypothetical person, rather than the viewer directly. This point is illustrated by the following narrative peak from the example set ...en als je nou naar Amsterdam gaat, naar het Museum Willet-Holthuysen, kijk, daar heb je wat ik 'total design' zou willen noemen... ('...and if you (informal) go to Amsterdam to the Willet-Holthuysen Museum, that's where you'll (informal) find what I call total design.')<ref type="foot" coords="5,450.68,442.80,6.97,6.05" target="#foot_8">10</ref> Dutch usage conventions prevent Prof. van Os from addressing his audience using the informal, although it must also be kept in mind that his ability to stretch conventions is part of his narrative talent.</p><p>The duotu09pro algorithm uses the same list of peak candidates and the same method of choosing from the ranked candidate lists that was used in duotu09ind and duotu09rep. For duotu09pro, the candidates are ranked according to the raw count of first and second person pronominal forms that they contain. Again, no normalization was applied to the raw count. It should also be noted that in this case no stop word removal was applied since first and second person pronouns are themselves function words and are included in standard formulations of stop word lists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.5">Calculating Affective Ratings (duotu09rat)</head><p>Our final approach to narrative peak detection is based on the hypothesis that dramatic tension rises when the speaker in the video uses speech made vivid by emotion. We conjecture that narrative peaks contain more emotion than other parts of the narrative. Human speech is an important conduit for the communication of emotions. Although emotion can be conveyed by prosodic variation, including changes in loudness, pitch and speed, emotion is also conveyed by the choice of lexical items. People tend to use specific words to express their emotions because there is a conventionalized relationship between certain words and certain emotions. In the field of psychology, one way of establishing the connection between word forms and emotions is to ask subjects to list the English words that describe specific emotions <ref type="bibr" coords="5,423.09,669.00,10.58,8.64" target="#b5">[6]</ref>.</p><p>The duotu09rat approach uses an affective rating score that is calculated in a straightfoward manner using known affective levels of words in order to identify narrative peaks. The approach makes use of Whissell's Dictionary of Affect in Language as deployed in the implementation of <ref type="bibr" coords="5,423.87,704.87,10.58,8.64" target="#b4">[5]</ref>, which is available Figure <ref type="figure" coords="6,217.64,261.95,3.88,8.64">1</ref>: Illustration of the 2D Emotion Space, from <ref type="bibr" coords="6,402.25,261.95,11.62,8.64" target="#b0">[1]</ref> online.<ref type="foot" coords="6,117.40,302.12,6.97,6.05" target="#foot_9">11</ref> This dictionary of words and scores focuses on the scales of pleasantness and arousal levels. The scales are alternately called evaluation and activation. Dietz and Lang <ref type="bibr" coords="6,372.90,315.75,11.62,8.64" target="#b0">[1]</ref> transformed these two scales to the two-dimensional emotion space depicted in Figure <ref type="figure" coords="6,305.60,327.70,3.74,8.64">1</ref>. Under our approach, narrative peaks are identified with a high arousal emotion combined with either a very pleasant or unpleasant emotion. In order to score words, we combine the evaluation and the activation scores into an overall affective word score. In order to apply the dictionary, we first translate the Dutch-language speech recognition transcripts into English using the Google Language API. <ref type="foot" coords="6,220.88,373.86,6.97,6.05" target="#foot_10">12</ref>The duotu09rat algorithm uses the same list of peak candidates used in duotu09ind, duotu09rep and duotu09pro. Candidates are ranked according to the average affective word score of the words that they contain. Words that are not contained in the dictionary are excluded from the calculation. Selection of peaks proceeds as in the other approaches with the exception of the fact the peak proximity condition was set to be more stringent. Edges of peaks are required to be 4 secs apart from each other. The imposition of the more stringent condition reflects an incidental difference in the experimental set up and does not represent an optimized value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>We tested our five experimental approaches on the 45 videos in the test set. Evaluation of results was carried out by comparing the peak positions hypothesized by each experimental system with peak positions that were set by human assessors. In total, three assessors viewed each of the test videos and set peaks at the three points where he or she felt most highly affected by narrative tension created by the video content. In total the assessors identified 293 distinct narrative peaks in the 45 test episodes. Peaks identified by different assessors were considered to be the same peak if they overlapped by at least two seconds. This value was set on the basis of observations by the assessor on characteristic distances between peaks. Overlapping peaks were merged by fitting the overlapped region with a ten second window. This process was applied so that merged peaks could never exceed the specified peak length of ten seconds.</p><p>Two methods of scoring the experiments were applied, the point-based approach and the peak-based approach. Under point-based scoring, a peak hypothesis scores a point for each assessor who selected a reference peak that is within eight seconds of that hypothesis peak. The total number of points returned by the run is the reported run score. A single episode can earn a run between three points (assessors chose completely different peaks) and nine points (assessors all chose the same peaks). In reality, no episode however, falls at either of these extremes. The distribution of the peaks in the files is such that a perfect run would earn 246 points. Under peak-based scoring, the total number of correct peaks is reported as the run score. Three different types of reference peaks are defined for peak-based scoring. The difference is related to the number of assessors required to agree for a point in the video to be counted as a peak. Of these 293 total peaks identified, 203 peaks are "personal peaks" (peaks identified by only one assessor), 90 are "pair peaks" (peaks that are identified by at least two assessors) and 22 are "general peaks" (peaks upon which all three assessors agreed). Peak-based scores are reported separately for each of these types of peaks. A summary of the results of the evaluation of our five approaches is given in Table2.</p><p>From these results it can be seen that duotu09pro, the approach that counted first and second person pronouns, and duotu09rat, the approach that made use of affective word scores are the best performing approaches. The approach relying on a list of peak indicator words, i.e., duotu09ind, performed surprisingly well considering that the list was formulated on the basis of a very limited number of examples.</p><p>It should be kept in mind, that the performance of a random classifier on the narrative peak detection task reaches a relatively high level since the videos are relatively short. Via simulation we calculated that an approach that randomly picks points at which to hypothesize three peaks in a file will automatically score, on average, approximately 40 points under the point-based scoring method. Under the peak-based method it would score on average 28 correct "personal peaks", nine correct "pair peaks" and two correct "general peaks." In light of these statistics, the approach duotu09rep, which counted use of repeated words, deserves further comment. This approach failed to achieve the performance level of the random baseline detector, which indicates that repetitions, as they are counted by our implementation of the algorithm, actually are a negative indicator for the existence of a peak. We believe that this result may be due to the fact that assessors tend not to set peaks at places where there might be disfluencies or unintentional repetitions. The speech recognition transcripts contain a high level of noise and it is conceivable that this noise contributes to creating word repetitions where none existed in the original speech. Such an effect could further prevent stylistic repetition from being effectively exploited for the purpose of peak detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Outlook</head><p>We have proposed five approaches to the automatic detection of narrative peaks in short-form documentaries and have evaluated these approaches within the framework of the VideoCLEF 2009 Affect Detection task, which uses a test set consisting of episodes from the Dutch language documentary on the visual arts called Beeldenstorm. Our proposed approaches exploit speech recognition transcripts. The two most successful algorithms are based on the idea that narrative peaks are perceived where particularly emotional speech is being used (duotu09rat) or when the viewer feels specifically addressed by or involved in the video (duotu09pro). These two approaches easily beat both the random baseline and also a challenging baseline approach hypothesizing narrative peaks at set positions in the video. Approaches based on capturing speaking style, either by using a set of indicator words typical for the speaker, or by trying to determine where repetition is being used as a stylistic device, proved less helpful. However, the experiments reported here are not extensive enough to exclude the possibility that they would perform well given a different implementation.</p><p>Future work will involve returning to many of the questions opened here, for example, while selecting peak-indicator words, we noticed that contrasts introduced by the word 'but' appear to often be associated with narrative peaks. Stylistic devices in addition to repetition, for example, use of questions, could also prove to be helpful. Under our approach, peak candidates are represented by their spoken content. We would also like to investigate the enrichment of the representations of peak candidates using words derived from surrounding regions in the speech transcripts or from an appropriate external text collection. Finally, we intend to develop peak detection methods based on the combination of information sources, in particular, exploring whether using pronoun occurrence based information can provide enhancement to affect based rating.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,90.00,119.13,423.00,255.19"><head>Table 1 :</head><label>1</label><figDesc>List of narrative peak indicators consisting of words and word stems (marked with *) selected from example episodes for use in duotu09ind</figDesc><table coords="4,240.59,149.71,121.81,224.62"><row><cell>Indicator</cell><cell>English</cell></row><row><cell cols="2">word or stem form</cell></row><row><cell>helemaal</cell><cell>entirely</cell></row><row><cell>eigenlijk*</cell><cell>actually</cell></row><row><cell>verkeerd*</cell><cell>wrong</cell></row><row><cell>werkelijk*</cell><cell>actually</cell></row><row><cell>gelukkig*</cell><cell>happy</cell></row><row><cell>mooi*</cell><cell>beautiful</cell></row><row><cell>grappig*</cell><cell>funny</cell></row><row><cell>ontzettend*</cell><cell>horribly</cell></row><row><cell>absolu(u)t*</cell><cell>absolute</cell></row><row><cell>fout*</cell><cell>wrong</cell></row><row><cell>genial*</cell><cell>ingenious</cell></row><row><cell>echt</cell><cell>true</cell></row><row><cell>wonder*</cell><cell>extraordinary</cell></row><row><cell>belangrijk*</cell><cell>important</cell></row><row><cell>maar</cell><cell>but</cell></row><row><cell>goed</cell><cell>good</cell></row><row><cell>(na)tuurlijk</cell><cell>naturally</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,99.37,119.13,401.77,117.71"><head>Table 2 :</head><label>2</label><figDesc>Narrative peak detection results</figDesc><table coords="7,99.37,137.75,401.77,99.09"><row><cell>measure</cell><cell cols="5">duotu09fix duotu09ind duotu09rep duotu09pro duotu09rat</cell></row><row><cell>point-based</cell><cell>47</cell><cell>55</cell><cell>30</cell><cell>63</cell><cell>63</cell></row><row><cell>peak-based &gt; 1 assessor</cell><cell>28</cell><cell>38</cell><cell>21</cell><cell>44</cell><cell>37</cell></row><row><cell>("personal peaks")</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>peak-based &gt; 2 assessors</cell><cell>8</cell><cell>12</cell><cell>7</cell><cell>17</cell><cell>20</cell></row><row><cell>("pair peaks")</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>peak-based &gt; 3 assessors</cell><cell>4</cell><cell>2</cell><cell>0</cell><cell>4</cell><cell>5</cell></row><row><cell>("general peaks")</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,104.35,710.33,302.93,6.91"><p>Delft University of Technology and Dublin City University are the coordinators of VideoCLEF</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,104.35,720.06,117.45,6.91"><p>http://www.cdvp.dcu.ie/VideoCLEF/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,104.35,729.79,98.06,6.91"><p>http://www.clef-campaign.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="2,104.35,739.51,167.78,6.91"><p>http://www.avro.nl/tv/programmas az/beeldenstorm/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4" coords="4,104.35,713.00,174.97,7.05"><p>from Beeldenstorm episode Tulpomanie, 'Tulip mania'</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5" coords="4,104.35,722.64,208.15,7.05"><p>from Beeldenstorm episode Rust bij Rothko, 'Peace with Rothko'</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6" coords="4,104.35,732.37,211.75,7.05"><p>from Beeldenstorm episode Maria Magdalena, 'Mary Magdalene'</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7" coords="5,104.35,724.03,174.97,7.05"><p>from Beeldenstorm episode Tulpomanie, 'Tulip mania'</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8" coords="5,104.35,733.76,197.79,7.05"><p>from Beeldenstorm episode Leven met kunst, 'Living with art'</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9" coords="6,104.35,737.53,281.48,6.91"><p>http://technology.calumet.purdue.edu/met/gneff/Publications/ica02/affectdictionary.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_10" coords="6,104.35,747.26,156.30,6.91"><p>http://code.google.com/intl/nl/apis/ajaxlanguage/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Acknowledgements</head><p>The research leading to these results was carried out to a substantial degree within the <rs type="institution">PetaMedia Network of Excellence</rs> and has received funding from the <rs type="funder">European Commission</rs>'s <rs type="programName">7th Framework Program</rs> under grant agreement no. <rs type="grantNumber">216444</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8qMhVkw">
					<idno type="grant-number">216444</idno>
					<orgName type="program" subtype="full">7th Framework Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,106.60,295.85,406.40,8.64;8,106.60,307.63,360.58,8.82" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,237.56,295.85,275.45,8.64;8,106.60,307.81,48.11,8.64">Aefective agents: Effects of agent affect on arousal, attention, liking and learning</title>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Annie</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,172.99,307.63,264.92,8.59">Proceedings of the Third Annual Cognitive Technology Conference</title>
		<meeting>the Third Annual Cognitive Technology Conference</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,106.60,327.55,406.40,8.82;8,106.60,339.51,194.00,8.82" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,239.83,327.73,213.06,8.64">Affective video content representation and modeling</title>
		<author>
			<persName coords=""><forename type="first">Alan</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li-Qun</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,464.57,327.55,48.43,8.59;8,106.60,339.51,86.20,8.59">Multimedia, IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="154" />
			<date type="published" when="2005-02">Feb. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,106.60,359.61,406.40,8.64;8,106.60,371.39,318.58,8.82" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,358.37,359.61,154.63,8.64;8,106.60,371.57,185.83,8.64">Annotation of heterogeneous multimedia content using automatic speech recognition</title>
		<author>
			<persName coords=""><forename type="first">Marijn</forename><surname>Huijbregts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roeland</forename><surname>Ordelman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Franciska</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jong</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,310.79,371.39,84.00,8.59">Proceedings of SAMT</title>
		<meeting>SAMT</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,106.60,391.49,406.40,8.64;8,106.60,403.45,406.40,8.64;8,106.60,415.22,252.01,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,317.14,391.49,195.86,8.64;8,106.60,403.45,195.04,8.64">Overview of VideoCLEF 2009: New perspectives on speech-based multimedia content enrichment</title>
		<author>
			<persName coords=""><forename type="first">Martha</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eamonn</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gareth</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,168.03,415.22,95.67,8.59">Working Notes of CLEF</title>
		<editor>
			<persName><forename type="first">Francesca</forename><surname>Borri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alessandro</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2009-09">2009. September 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,106.60,435.33,406.41,8.64;8,106.60,447.10,406.40,8.82;8,106.60,459.06,128.14,8.82" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,294.75,435.33,218.25,8.64;8,106.60,447.28,154.60,8.64">Assessing the affective aspect of languaging:the development of software for public relations</title>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Neff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bonita</forename><surname>Neff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Crandon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,279.37,447.10,233.63,8.59;8,106.60,459.06,80.05,8.59">The 52nd Annual Conference of the International Communication Association</title>
		<imprint>
			<date type="published" when="2002-07">July 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,106.60,478.98,365.54,8.82" xml:id="b5">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Plutchik</surname></persName>
		</author>
		<title level="m" coord="8,175.50,478.98,158.39,8.59">The Psychology and Biology of Emotion</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>HarperCollins</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
