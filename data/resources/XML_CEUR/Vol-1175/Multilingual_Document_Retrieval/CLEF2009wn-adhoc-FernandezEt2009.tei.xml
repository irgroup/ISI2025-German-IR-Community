<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,157.47,113.78,300.37,15.06">Alicante at CLEF 2009 Robust-WSD Task</title>
				<funder>
					<orgName type="full">El Taller Digital&quot;</orgName>
				</funder>
				<funder ref="#_8HyY3AU">
					<orgName type="full">Virtual Observatory for Technology Transfer</orgName>
				</funder>
				<funder ref="#_hqrK5Eh">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,189.56,152.15,63.66,10.46"><forename type="first">Javi</forename><surname>Fernández</surname></persName>
							<email>javier.fernandez@eltallerdigital.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Software and Computing Systems</orgName>
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<addrLine>San Vicente del Raspeig Road</addrLine>
									<postCode>03690</postCode>
									<settlement>Alicante</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,261.26,152.15,70.56,10.46"><forename type="first">Rubén</forename><surname>Izquierdo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Software and Computing Systems</orgName>
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<addrLine>San Vicente del Raspeig Road</addrLine>
									<postCode>03690</postCode>
									<settlement>Alicante</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,358.85,152.15,66.96,10.46"><forename type="first">José</forename><forename type="middle">M</forename><surname>Gómez</surname></persName>
							<email>jmgomez@ua.es</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Software and Computing Systems</orgName>
								<orgName type="institution">University of Alicante</orgName>
								<address>
									<addrLine>San Vicente del Raspeig Road</addrLine>
									<postCode>03690</postCode>
									<settlement>Alicante</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,157.47,113.78,300.37,15.06">Alicante at CLEF 2009 Robust-WSD Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CABCE719347DE8C2D984B3494FD112A7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we explore the use of semantic classes in an Information Retrieval system in order to improve the results in the Robust-WSD task at CLEF 2009. We use two ontologies of semantic classes (WordNet domain and Basic Level Concepts) to re-rank the retrieved documents and obtain better recall and precision. Finally, we implement an innovative method to weight the expanded terms taking into account the ones of the original query terms and their relations in WordNet with respect to the new ones which have demonstrated to improve the results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The two main goals of the Robust-WSD last year's edition task were to measure the robustness of the retrieval systems (good stable performance over all queries), and test the benefits of the use of Word Sense Disambiguation (WSD) on this kind of systems.</p><p>For our participation in the second edition, we decided to employ a system already implemented and evaluated in the last year edition as starting point, the one of Universidad Complutense de Madrid <ref type="bibr" coords="1,354.55,451.40,15.50,10.46" target="#b9">[10]</ref> due to its good results, availability and the possibility of easily adjusting the code to our objectives.</p><p>Our main strategy consists of experimenting the benefits semantic classes in Information Retrieval (IR) systems. Moreover, we propose an innovative and flexible way of weighting terms for the query expansion based on WordNet relations.</p><p>WSD, can be defined as the task of assigning the correct sense to words depending on the context in which they appear. It is a challenging task, difficult to be addressed and, despite the long time it is been studied, the results of state-ofart WSD systems are still a long way to be useful in other Natural Language Procesing applications, as shown in last international evaluations <ref type="bibr" coords="1,401.38,570.95,15.50,10.46" target="#b14">[15,</ref><ref type="bibr" coords="1,418.54,570.95,11.62,10.46" target="#b11">12]</ref>. Generally, supervised systems obtain better results than the unsupervised ones on previous cited international evaluations. The annotated corpora used commonly in supervised approaches are tagged manually by lexicographers with word senses taken from a particular lexical semantic resource (most commonly WordNet <ref type="bibr" coords="1,440.87,618.77,10.30,10.46" target="#b3">[4]</ref>.) This tool has been widely criticized for being a sense repository that often provides too fine-grained sense distinctions. On the one hand, too fine-grained senses are not useful for higher level applications like Machine Translation or Question Answering. On the other hand, it seems that many word-sense distinctions are too subtle to be captured by automatic systems with the current small volumes of word-sense annotated examples. This could be a reason of the poor results of current WSD systems.</p><p>A possible solution that has been explored is the use of semantic classes instead of word senses. The task of WSD consists of assigning the proper semantic class to each ambiguous word, instead of its word sense. The use of semantic classes has several advantages. Firstly, they provide richer and more useful information than word senses. For example, for IR could be more informative that the word church belongs to the semantic class building, instead of knowing that the correct sense to that word is the 1. Secondly, the average polysemy of texts is decreased with the use of semantic classes; in fact they can group in the same class several senses of a concrete word. Therefore, the classification task is simplified, and finally, the amount of training data for each classifier is increased because semantic classes can group together senses of different words, senses of the same word, and also senses of word of different morphological categories. As a consequence, the number of examples to train each classifier is increased in semantic class approaches, and the problem of the lack of data in alleviated.</p><p>In <ref type="bibr" coords="2,161.43,334.72,10.52,10.46" target="#b4">[5]</ref> they empirically explored on the supervised WSD task the performnace of different levels of abstraction provided by WordNet Domains <ref type="bibr" coords="2,418.80,346.67,9.96,10.46" target="#b7">[8]</ref>, SUMO labels <ref type="bibr" coords="2,154.86,358.62,9.96,10.46" target="#b8">[9]</ref>, Lexicographer Files of WordNet <ref type="bibr" coords="2,313.39,358.62,10.52,10.46" target="#b3">[4]</ref> and Basic Level Concepts <ref type="bibr" coords="2,441.72,358.62,9.96,10.46" target="#b5">[6]</ref>. They referred to this approach as class-based WSD since the classifiers were created at a class level instead of at a sense level. As we abovementioned, class-based WSD cluster senses of different words into the same explicit and comprehensive grouping. Only thoses cases belonging to the same semantic class are grouped to train the classifier. For example, the coarser word grouping obtained in <ref type="bibr" coords="2,465.10,418.40,15.50,10.46" target="#b13">[14]</ref> only has one remaining sense for "church". Using a set of Base Level Concepts <ref type="bibr" coords="2,134.77,442.31,9.96,10.46" target="#b5">[6]</ref>, the three senses of "church" are still represented by faith.n#3, building.n#1 and religious ceremony.n#1.</p><p>We are convinced that IR could take advantage of the use of word sense disambiguation, from a semantic class point of view instead from the traditional word sense point of view. Due to the data of the robust adhoc IR task has been processed automatically by two WSD systems, and the information of word senses is available, we do not run any class-based WSD system over the data.</p><p>This paper is organized as follows: the next section describes the architecture of our system. In section 3 we discuss the results of this system at CLEF 2009 Robust-WSD Task. Finally, in section 4 we draw the conclusions and future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Description of the System</head><p>The system architecture is shown in Figure <ref type="figure" coords="2,326.71,629.72,3.87,10.46">1</ref>.</p><p>As we can see in Figure <ref type="figure" coords="2,251.36,642.68,3.87,10.46">1</ref>, the user query is pre-parsed to obtain a set of terms without stopwords and any special symbol. Next, a ranked list of relevant docu-Fig. <ref type="figure" coords="3,258.24,124.59,4.12,9.41">1</ref>. Architecture of the system ments are retrieved using Lucene search engine <ref type="foot" coords="3,337.74,389.25,3.97,7.32" target="#foot_0">1</ref> . With the retrieved documents, the initial query, the relations of the external resource WordNet and state-of-art query expansions methods an expanded query is obtained. The terms of this new query are weighted taking into account the weights of the original query words, their relations in WordNet with respect to the new ones, the weight assigned by the WSD system to each sense and the weight returned by the expansion method. Once we obtain a new list of weighted terms, we do, again, a search but this time using the expanded query instead of the original one in order to retrieved a new ranked list of documents. Finally, we employ the semantic class information from two semantic resources (WordNet Domains and Base Level Concepts) in order to obtain a re-ranked document list as result.</p><p>The following sections present each of this processes in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Search engine and query expansion</head><p>As we mentioned previously, the search engine we employ is the one provided by the Universidad Complutense de Madrid . Its implementation is a modified version of Lucene which uses the BM25 probabilistic model <ref type="bibr" coords="3,389.27,598.55,15.50,10.46" target="#b12">[13]</ref> for the document retrieval. They have also implemented two state-of-art query expansion methods: Kullback-Liebler Divergence <ref type="bibr" coords="3,261.89,622.47,10.52,10.46" target="#b1">[2,</ref><ref type="bibr" coords="3,274.07,622.47,7.75,10.46" target="#b2">3]</ref> (an information-theoretic approach) and the Bo1 model <ref type="bibr" coords="3,184.50,634.42,10.52,10.46" target="#b6">[7,</ref><ref type="bibr" coords="3,196.67,634.42,12.73,10.46" target="#b10">11]</ref> (based on Divergence From Randomness <ref type="bibr" coords="3,392.30,634.42,10.30,10.46" target="#b0">[1]</ref>). We selected the Bo1 model since this approach obtains the best results. We also decided to use the same constant values than they used in the last CLEF Robust-WSD edition in order to compare the effectiveness of our methods of semantic classes.</p><p>As we can see in the Figure <ref type="figure" coords="4,277.39,154.20,3.87,10.46">1</ref>, we make two search processes. For the first retrieval process, query terms are lemmatized and stemmed in order to increase the system recall. The first Search module gets these terms as input and returns a list of relevant documents using the BM25 probabilistic model. Furthermore, in the Query expansion module, we expand the original query obtaining new terms by means of the Bo1 model.</p><p>Even if <ref type="bibr" coords="4,182.09,226.75,15.50,10.46" target="#b9">[10]</ref> proposed a method for weighting the expanded query terms based on WordNet, we preferred to use our own in fact they do not use all senses of each term but the weightest one. We decided to use all senses retrieved by the WSD system in order to improve the recall. In this way, the system searches all expanded terms in the relations of WordNet with respect to the synonyms, hyperonyms and hyponyms until a certain level or distance. For example, if the distance is 2, we search any expanded term among the hyperonym and hyponym synsets of the original terms but, also, the hyperonyms of the hyperonyms and the hyponyms of the hyponyms. The distance constant marks the allowed jumps to reach in the WordNet relations from the synsets of the query terms. We use all senses supplied for the WSD system for each query term taking into account the score given by these systems to each sense in order to calculate the weight of the expanded terms. Thefore, this distance factor is calculated by the following equation:</p><formula xml:id="formula_0" coords="4,209.41,407.80,271.18,12.93">weight(synset i,d ) = weight(synset i,d-1 ) * α d (1)</formula><p>We defined synset i,1 as a WordNet synset given and synset i,d as another WordNet synset which is related to the synset i,1 of a distance of d jumps (taken into acount only hyperonym and hyponym relations). Thus, weight(synset i,d ) is the weight of the synset i, d and weight(synset i,1 ) is the score given by the WSD system to the synset i, d. α is a constant whose value is between 0 and 1 and d the distance of synset i,d to the synset i,1 .</p><p>Once we calculated the previous synset weight, we combine this weight with the weight assigned by the expanded method bo1 in order to calculate the final term weight using the following equation:</p><formula xml:id="formula_1" coords="4,190.44,550.77,290.15,24.03">weight(term t ) = weight(synset i,d ) + weight 0 (term t ) 2<label>(2)</label></formula><p>Where weight(term t ) is the weight of the expanded term t which is grouped in the WordNet synset i, d, and weight 0 (term t ) is the weight assigned by bo1 to the term t.</p><p>Using these equations, we give importance to those expanded terms closely related with the original query terms and, in addition, we include the score given by the WSD system for each query term in the final term weight. Thus, we include all senses of a term in the search giving more importance those terms which are relationed with more likely senses and closer to the original query terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semantic classes</head><p>Our approach consists on mapping the assigned word senses to semantic classes, specifically to WordNet Domains labels and Basic Level Concepts.</p><p>WordNet Domains <ref type="bibr" coords="5,245.27,203.59,10.52,10.46" target="#b7">[8]</ref> is a hierarchy 165 Domain Labels used to label all the WordNet synsets. Information brought by Domain Labels is complementary to what is already in WordNet. First of all, a Domain Label can include synsets of different syntactic categories: for instance medicine groups together with senses from nouns, such as doctor or hospital, and from verbs, such as to operate. Second, a Domain Label may also contains senses from different WordNet sub hierarchies. For example, sport contains senses such as athlete, deriving from life form, game equipment physical object, sport from act and playing field from location.</p><p>Basic Level Concepts <ref type="bibr" coords="5,260.49,311.22,10.52,10.46" target="#b5">[6]</ref> are a set of concepts that result from the compromise between two conflicting principles of characterization:</p><p>-Represent as many concepts as possible; -Represent as many features as possible;</p><p>As a result, Basic Level Concepts typically occurs in the middle of hierarchies and less than the maximum number of relations.</p><p>The authors developed a method for the automatic selection of BLC from WordNet. They use a very simple method for deriving a small set of appropriate meanings using basic structural properties of WordNet. The approach considers:</p><p>-The total number of relations of every synset or just the hyponymy relations -Discard those BLCs that do not represent at least a number of synsets.</p><p>-Optionally, the frequency of the synsets (summing up the frequency of the senses provided by WordNet).</p><p>The process of automatic selection of BLC follows a bottom-up approach using the chain of hypernym relations. For each synset in WN, the process selects as its Base Level Concept the first local maximum according to the relative number of relations. For synsets having multiple hypernyms, the path having the local maximum with higher number of relations is selected. Usually, this process ends with a number of fake Base Level Concepts. That is, synsets having no descendants (or with a small number) but being the first local maximum according to the number of relations considered. Thus, the process concludes checking if the number of concepts subsumed by the preliminary list of BLC is higher than a certain threshold. For those BLC not representing enough concepts according to a certain threshold, the process selects the next local maximum following the hypernym hierarchy.</p><p>Thus, depending on the type of relations considered to be counted and the threshold established, different sets of BLC can be easily obtained for each WN version. For our work, we selected the set of BLC built using all kind of relations and a threshold of 20 as the minimum number of synsets that each BLC must subsume.</p><p>We explain now the representation of documents or queries with semantic classes of the contained on them. In the task data, each ambiguous word is annotated with their possible senses, each one with a certain probability. Starting from this information, we create a domain vector, for a query or for a document, containing all the semantic classes information of the query or document. The domain vector consists in a vector which, each element, represents a WordNet Domain or a Basic Level Concept and its associated weight. Note that there are 165 Domain Labels in WordNet Domains and 558 Basic Level Concepts for nouns. The way to build this vector is: each word has annotated several senses, with the associated probability; each word sense is mapped to its proper semantic class, and the element of the vector corresponding to this domain is increased with the probability associated to the word sense. After processing all terms, we obtain a domain vector representing the semantic information of the document or query. Finally to compare two documents, or a document and a query, and obtain their similarity in terms of their semantic content, we use the value of the cosine defined by the two domain vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Integration of Semantic classes in Robust Ad hoc</head><p>Once the final list of documents from the expanded query is retrieved, the Semantic re-ranking module re-arrange this list taking into account both the similarity returned by the BM25 probabilistic model and the similarity calculated by semantic class system. In order to do this, we studied the following 5 different equations:</p><formula xml:id="formula_2" coords="6,171.87,473.06,308.72,145.21">semsim 1 (i, j) = sim ij * sem ij (3) semsim 2 (i, j) =    simmax i + sim ij if sem ij &gt; h sim ij otherwise (4) semsim 3 (i, j) =    (simmax i + sim ij ) * sem ij if sem ij &gt; h sim ij otherwise (5) semsim 4 (i, j) =    simmax i + sim ij * sem ij if sem ij &gt; h sim ij otherwise (6) semsim 5 (i, j) = simmax i * sem ij + sim ij (<label>7</label></formula><formula xml:id="formula_3" coords="6,476.35,606.91,4.24,10.46">)</formula><p>Where semsim x (i, j) is the final similarity between the query i and the document j using the method x, sim ij is the similarity of the query i with respect to the document j returned by the search engine, sem ij is the same similarity but returned by the semantic class system, simmax i is the greatest value of similarity returned by the search engine for the query i and h is a constant which determines a semantic similarity threshold defined empirically.</p><p>As both similarity values (sim i and sem i j) are normalized ones, our first approximation was the equation 3. In this equation we simply multiply both values (the similarity returned by the search engine and the similarity obtained by the semantic class system) in order to obtain a new similarity value between the query i and the document j. With this equation both values have the same importance.</p><p>The equation 4 was thought in order to put those documents with a certain level of semantic relation with the query above other ones. Therefore, if the similarity obtained by the search engine will be summed to the greatest similarity if, and only if, the semantic class similarity between the document and the query is greater than a threshold h, otherwise only the BM25 similarity will be taken into account.</p><p>The next equation ( <ref type="formula" coords="7,239.64,299.50,4.24,10.46">5</ref>) is based on the previous one, however the semantic similarity multiplies the sum of the BM25 similarity for the document j and the greatest similarity. The main idea of this equation is to overlap some of the less semantic related documents which exceed the threshold with those which do not exceed.</p><p>In order to give more relevance those documents with high semantic similarity but taking into account the semantic class score in the final similarity value, the equation 6 was used.</p><p>Finally, the equation 7 multiplies the greatest similarity value with the document semantic similarity and, next, sums this result to the search engine document similarity. This equation tries to improve the search engine similarty value using the semantic similarity as reduction factor of maximum similarity value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>In this section we report the results of each one of our proposals.</p><p>For the evaluation of the Expanded query terms weighter, we set the value for two variables: α and d (distance). In order to get the best values for these variables, we experimented with several values for them. Table <ref type="table" coords="7,416.92,534.19,4.98,10.46">1</ref> presents two of the best results of those experiments. With α = 0.8 and d = 1 we improve the baseline GMAP in a 9.97%. With α = 0.92 and d = 6 we improve both the baseline MAP in a 0.02% and the baseline GMAP in a 8.19%. The results of BM25+Bo1+WD (α = 0.92, d = 6) correspond to our CLEF experiment named ali02wsd.</p><p>The results of our second proposal, the Semantic re-ranking module, depend on the function used for the integration of the documents' weights given by the semantic classes and the search engine. In addition, for some of the integration functions, the variable h (threshold) has to be set too. We experimented with those functions and different values for the threshold. For each integration func-Table <ref type="table" coords="8,200.71,114.63,4.12,9.41">1</ref>. Evaluation of the Expanded query terms weighter module MAP GMAP R-Prec P@5 P@10 BM25 + Bo1 (Baseline)</p><p>.3737 .1294 .0.3585 .4475 .3825 BM25 + Bo1 + WD (α = 0.8, d = 1) .3706 .1423 .3624 .4500 .3750 BM25 + Bo1 + WD (α = 0.92, d = 6) .3738 .1400 .3655 .4513 .3775 tion we obtained its best threshold value (when needed), as can be seen in Tables <ref type="table" coords="8,134.77,227.01,4.98,10.46" target="#tab_0">2</ref> and<ref type="table" coords="8,162.44,227.01,3.87,10.46" target="#tab_1">3</ref>. As we can see in Table <ref type="table" coords="8,256.20,393.00,3.87,10.46" target="#tab_0">2</ref>, some of the results of different integration functions are the same. In the case of RR2, RR3 and RR5, this occurs because the best results they can reach are the same as the baseline results. This integration functions do not improve the system. In the case of RR1 and RR4, they mathematical function are the same, except the second one, that can be affected by the threshold. Both of them obtain the best results for the WND model for all measures. Table <ref type="table" coords="8,177.55,630.72,4.98,10.46" target="#tab_1">3</ref> presents the results of the integration with the BLC20 model. The best results are obtained by the RR2, RR3 and RR4 integration functions. These are the functions that use a threshold. Note the threshold that gives the best is the same for all them. Thus, we can deduce that the threshold h = 0.8 is the ideal for determining if a semantic class is relevant or not.</p><p>For our final comparison, we chose the best integration function for each model (WND, BLC20), as shown on Table <ref type="table" coords="9,338.00,153.38,4.98,10.46">4</ref> and Figure <ref type="figure" coords="9,403.22,153.38,3.87,10.46" target="#fig_0">2</ref>. The results of BM25+Bo1+WND correspond to our CLEF experiment named ali01wnd. The integration of the semantic classes to the search engine improves the baseline results. With WND we improve both the baseline MAP in a 0.4% and the baseline GMAP in a 0.31%. With BLC20 we improve both the baseline MAP in a 0.64% and the baseline GMAP in a 1.77%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>The results of the experiments with our two proposals demonstrate improvements to the initial IR system. Regarding the Expanded query terms weighter module, we experimented with the weights of the terms in a probabilistic IR system. We have applied a smoothing function based on the WordNet distance to the weights given by the IR system. The experiments show GMAP improvements of nearly 10% but not significant MAP improvements.</p><p>As future work we propose to continue with the experiments on this module. For the propagation function 2, the search of the best values for α and d can be more exhaustive, finding better values for this variables. Moreover, new relations can be explored in WordNet (not only hyponyms and hyperonyms), in order to improve recall. Even new weight propagation functions can be proposed to better exploit the concept of distance in WordNet.</p><p>Regarding our second proposal, the Semantic re-ranking module, we have integrated the semantic classes to a IR system. We carried out this integration recalculating the weight of the documents retrieved depending on the similarity between the semantic class of each document and the semantic class of the query. The results of the experiments made reveal that the semantic classes resources can be effectively be integrated to the IR systems.</p><p>This module can also be led to new levels. We only used five simple integration functions for the search engine and the semantic classes weights. More functions can be studied with the purpose of finding the best way to integrate the available resources of semantic classes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="9,134.77,206.49,345.70,9.41;9,134.77,217.45,33.01,9.41"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Results of MAP and GMAP for models WND and BLC20 and each integration function</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,134.77,256.91,345.77,97.93"><head>Table 2 .</head><label>2</label><figDesc>Evaluation of the Semantic re-ranking module with WND and different</figDesc><table coords="8,134.77,267.87,323.51,86.96"><row><cell>integration functions</cell><cell></cell></row><row><cell></cell><cell>MAP GMAP R-Prec P@5 P@10</cell></row><row><cell>BM25 + Bo1 + WND + RR1</cell><cell>.3752 .1298 .3638 .4462 .3862</cell></row><row><cell cols="2">BM25 + Bo1 + WND + RR2 (h = 0.2) .3737 .1294 .3585 .4475 .3825</cell></row><row><cell cols="2">BM25 + Bo1 + WND + RR3 (h = 1.0) .3737 .1294 .3585 .4475 .3825</cell></row><row><cell cols="2">BM25 + Bo1 + WND + RR4 (h = 0.5) .3752 .1298 .3638 .4462 .3862</cell></row><row><cell>BM25 + Bo1 + WND + RR5</cell><cell>.3746 .1296 .3592 .4463 .3856</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,134.77,494.63,345.77,97.93"><head>Table 3 .</head><label>3</label><figDesc>Evaluation of the Semantic re-ranking module with BLC20 and different</figDesc><table coords="8,134.77,505.59,325.88,86.97"><row><cell>integration functions</cell><cell></cell></row><row><cell></cell><cell>MAP GMAP R-Prec P@5 P@10</cell></row><row><cell>BM25 + Bo1 + BLC20 + RR1</cell><cell>.3533 .1231 .3375 .4337 .3619</cell></row><row><cell cols="2">BM25 + Bo1 + BLC20 + RR2 (h = 0.8) .3776 .1317 .3609 .4437 .3806</cell></row><row><cell cols="2">BM25 + Bo1 + BLC20 + RR3 (h = 0.8) .3776 .1317 .3609 .4437 .3806</cell></row><row><cell cols="2">BM25 + Bo1 + BLC20 + RR4 (h = 0.8) .3776 .1317 .3609 .4437 .3806</cell></row><row><cell>BM25 + Bo1 + BLC20 + RR5</cell><cell>.3375 .1170 .3229 .4213 .3625</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,655.49,99.31,9.41"><p>http://lucene.apache.org</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This paper has been partially supported by the next projects: "<rs type="projectName">Question Answering Learning technologies in a multiLingual and Multimodal Environment (QALL-ME)</rs>" (<rs type="grantNumber">FP6 IST-033860</rs>), "<rs type="projectName">Intelligent, Interactive and Multilingual Text Mining based on Human Language Technologies (TEXT-MESS)</rs>" (<rs type="grantNumber">TIN2006-15265-C06-01</rs>) and "<rs type="funder">Virtual Observatory for Technology Transfer</rs>". We also want to thank the support given by "<rs type="funder">El Taller Digital"</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_hqrK5Eh">
					<idno type="grant-number">FP6 IST-033860</idno>
					<orgName type="project" subtype="full">Question Answering Learning technologies in a multiLingual and Multimodal Environment (QALL-ME)</orgName>
				</org>
				<org type="funded-project" xml:id="_8HyY3AU">
					<idno type="grant-number">TIN2006-15265-C06-01</idno>
					<orgName type="project" subtype="full">Intelligent, Interactive and Multilingual Text Mining based on Human Language Technologies (TEXT-MESS)</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,142.95,460.79,337.50,9.41;10,151.52,471.75,329.06,9.41;10,151.52,482.71,124.97,9.41" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="10,352.58,460.79,127.87,9.41;10,151.52,471.75,268.43,9.41">Probabilistic models of information retrieval based on measuring the divergence from randomness</title>
		<author>
			<persName coords=""><forename type="first">Gianni</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cornelis Joost</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,429.76,471.75,50.82,9.41;10,151.52,482.71,35.87,9.41">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="357" to="389" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.95,493.24,337.50,9.41;10,151.52,504.19,329.04,9.41;10,151.52,515.16,93.49,9.41" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,468.42,493.24,12.03,9.41;10,151.52,504.19,249.51,9.41">An information-theoretic approach to automatic query expansion</title>
		<author>
			<persName coords=""><forename type="first">Claudio</forename><surname>Carpineto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renato</forename><surname>De Mori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giovanni</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brigitte</forename><surname>Bigi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,411.30,504.19,69.27,9.41;10,151.52,515.16,18.20,9.41">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.95,525.69,337.63,9.41;10,151.52,536.64,163.34,9.41" xml:id="b2">
	<monogr>
		<title level="m" type="main" coord="10,318.41,525.69,126.06,9.41">Elements of information theory</title>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joy</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Wiley-Interscience</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.95,547.18,337.61,9.41;10,151.52,558.14,20.98,9.41" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="10,238.78,547.18,167.08,9.41">WordNet. An Electronic Lexical Database</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.95,568.67,337.50,9.41;10,151.52,579.63,328.99,9.41;10,151.52,590.59,69.82,9.41" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,316.21,568.67,164.24,9.41;10,151.52,579.63,84.02,9.41">An empirical study on class-based word sense disambiguation</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Izquierdo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Suárez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rigau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,258.83,579.63,22.21,9.41">EACL</title>
		<imprint>
			<publisher>The Association for Computer Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="389" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.95,601.12,337.50,9.41;10,151.52,612.08,329.02,9.41;10,151.52,623.04,329.00,9.41" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,311.60,601.12,168.85,9.41;10,151.52,612.08,54.31,9.41">Exploring the automatic selection of basic level concepts</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Izquierdo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rigau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,349.88,612.08,130.66,9.41;10,151.52,623.04,162.30,9.41">International Conference Recent Advances in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Galia</forename><surname>Angelova</surname></persName>
		</editor>
		<meeting><address><addrLine>Borovets, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="298" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.95,633.57,337.50,9.41;10,151.52,644.52,328.93,9.41;10,151.52,655.49,64.26,9.41" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,428.68,633.57,51.76,9.41;10,151.52,644.52,325.47,9.41">University of glasgow at trec 2005: Experiments in terabyte and enterprise tracks with terrier</title>
		<author>
			<persName coords=""><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vassilis</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,163.04,655.49,22.96,9.41">TREC</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,118.36,337.49,9.41;11,151.52,129.32,178.59,9.41" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="11,279.95,118.36,179.97,9.41">Integrating subject field codes into wordnet</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Cavaglià</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,151.52,129.32,82.29,9.41">Proceedings of LREC</title>
		<meeting>LREC<address><addrLine>Athens. Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.95,140.28,96.95,9.41;11,281.78,140.28,198.78,9.41;11,151.52,151.24,328.94,9.41;11,151.52,162.20,78.92,9.41" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="11,281.78,140.28,105.45,9.41">a standard upper ontology</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Niles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pease</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,408.07,140.28,72.49,9.41;11,151.52,151.24,328.94,9.41;11,151.52,162.20,20.95,9.41">Proceedings of the 2nd International Conference on Formal Ontology in Information Systems (FOIS-2001)</title>
		<meeting>the 2nd International Conference on Formal Ontology in Information Systems (FOIS-2001)</meeting>
		<imprint>
			<biblScope unit="page" from="17" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,173.15,337.83,9.41;11,151.52,184.12,89.70,9.41" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="11,330.40,173.15,150.04,9.41;11,151.52,184.12,19.06,9.41">Ucm-y!r at clef 2008 robust and wsd tasks</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">Peréz</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Agüera</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,190.01,184.12,21.73,9.41">CLEF</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,195.07,337.83,9.41;11,151.52,206.03,328.93,9.41;11,151.52,216.99,329.07,9.41;11,151.52,227.95,328.94,9.41;11,151.52,238.91,55.77,9.41" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="11,339.94,195.07,140.50,9.41;11,151.52,206.03,237.16,9.41">University of glasgow at trec 2004: Experiments in web, robust, and terabyte tracks with terrier</title>
		<author>
			<persName coords=""><forename type="first">Vassilis</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,433.92,216.99,22.95,9.41">TREC</title>
		<editor>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lori</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lori</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="500" to="261" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,249.86,337.84,9.41;11,151.52,260.83,329.03,9.41;11,151.52,271.79,329.04,9.41;11,151.52,282.74,320.61,9.41" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="11,444.87,249.86,35.58,9.41;11,151.52,260.83,210.70,9.41">Semeval-2007 task-17: English lexical sample, srl and all words</title>
		<author>
			<persName coords=""><forename type="first">Edward</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dmitriy</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martha</forename><surname>Dligach</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,380.52,260.83,100.04,9.41;11,151.52,271.79,268.95,9.41">Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007)</title>
		<meeting>the Fourth International Workshop on Semantic Evaluations (SemEval-2007)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007-06">June 2007</date>
			<biblScope unit="page" from="87" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,293.71,337.83,9.41;11,151.52,304.66,329.02,9.41;11,151.52,315.62,328.93,9.41;11,151.52,326.58,328.98,9.41;11,151.52,337.54,60.66,9.41" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,287.59,293.71,192.85,9.41;11,151.52,304.66,194.11,9.41">Some simple effective approximations to the 2poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,363.20,304.66,117.34,9.41;11,151.52,315.62,328.93,9.41;11,151.52,326.58,82.69,9.41">SIGIR &apos;94: Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,348.50,337.83,9.41;11,151.52,359.45,328.93,9.41;11,151.52,370.42,329.05,9.41;11,151.52,381.37,68.06,9.41" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,340.05,348.50,123.46,9.41">Learning to merge word senses</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,151.52,359.45,328.93,9.41;11,151.52,370.42,297.89,9.41">Proceedings of Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1005" to="1014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,142.61,392.33,337.83,9.41;11,151.52,403.30,328.96,9.41;11,151.52,414.25,329.00,9.41;11,151.52,425.21,196.60,9.41" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,260.54,392.33,103.76,9.41">The english all-words task</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,227.13,403.30,253.35,9.41;11,151.52,414.25,173.46,9.41">Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text</title>
		<editor>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Phil</forename><surname>Edmonds</surname></persName>
		</editor>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004-07">July 2004</date>
			<biblScope unit="page" from="41" to="43" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
