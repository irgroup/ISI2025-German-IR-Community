<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,110.99,148.86,381.02,15.15;1,229.94,170.78,143.12,15.15">Cross-lingual Information Retrieval based on Multiple Indexes</title>
				<funder ref="#_9hcHu66">
					<orgName type="full">German Research Foundation (DFG)</orgName>
				</funder>
				<funder>
					<orgName type="full">Multipla</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,109.91,204.67,52.74,8.74"><forename type="first">Philipp</forename><surname>Sorg</surname></persName>
							<email>sorg@aifb.uni-karlsruhe.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institut AIFB</orgName>
								<orgName type="institution">Universität Karlsruhe</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,170.40,204.67,59.43,8.74"><forename type="first">Marlon</forename><surname>Braun</surname></persName>
							<email>marlon.braun@t-online.dedavidnicolay85@yahoo.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institut AIFB</orgName>
								<orgName type="institution">Universität Karlsruhe</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,238.12,204.67,61.58,8.74"><forename type="first">David</forename><surname>Nicolay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut AIFB</orgName>
								<orgName type="institution">Universität Karlsruhe</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,373.64,204.67,71.54,8.74"><forename type="first">Philipp</forename><surname>Cimiano</surname></persName>
							<email>cimiano@techfak.uni-bielefeld.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Universität Bielefeld</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,110.99,148.86,381.02,15.15;1,229.94,170.78,143.12,15.15">Cross-lingual Information Retrieval based on Multiple Indexes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">633268315A3F5C58D3C6A63C803ADA57</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval Measurement</term>
					<term>Performance</term>
					<term>Experimentation Cross-language Information Retrieval</term>
					<term>Explicit Semantic Analysis</term>
					<term>Rank Aggregation</term>
					<term>Machine Translation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present the technical details of the retrieval system with which we participated at the CLEF09 Ad-hoc TEL task. We present a retrieval approach based on multiple indexes for different languages which is combined with a conceptbased retrieval approach based on Explicit Semantic Analysis. In order to create the language-specific indices for each language, a language detection approach is applied as preprocessing step. We combine the different indices through rank aggregation and present our experimental results with different rank aggregation strategies. Our results show that the use of multiple indices (one for each language) does not improve upon a baseline index containing documents in all languages. The combination with concept based retrieval, however, results in better retrieval performance in some of the cases considered. For the bi-lingual tasks the final retrieval results of our system were the 5th best results on the BL dataset and the second best on the BNF dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There are two important paradigms that can be applied to the problem of cross-language retrieval: concept-based retrieval approaches as well as approaches exploiting machine translation (MT). Concept-based methods map documents and queries into a language-independent concept space <ref type="bibr" coords="1,90.00,709.78,9.96,8.74" target="#b4">[5]</ref>. MT-based methods translate the queries or documents into the target language or into all target languages <ref type="bibr" coords="1,164.81,721.73,9.96,8.74">[3]</ref>.</p><p>Most machine translation based approaches work for specific language pairs. The topic is given in a specific source language and all documents in the corpus are given in a defined target language.</p><p>In this paper we extend this model to be able to handle corpora containing documents in multiple languages and moreover documents containing fields in different languages. Our approach is directly motivated by the CLEF Ad-hoc TEL task. Here the target collection contains documents in different languages and the task is to find relevant documents in all languages for given topics. Our hypothesis is that retrieval can be improved by translating topics to all languages of the corpus, performing a language specific search for each translation and aggregating all the results for the single indices into one final ranking.</p><p>Another important question we address in this paper is whether concept-and MT-based techniques can be successfully combined to increase the performance of CLIR compared to conceptbased and MT-based techniques alone.</p><p>For both problems, i.e. retrieval using multiple indices and combination of MT-based and concept based retrieval, relevance measures computed by different models have to be combined to an aggregated relevance score. A common approach to this problem that we also use in this paper is rank aggregation. This means that the final scores of each model are used as input values for the aggregation function. In the following we will describe the main techniques used in related work to combine different retrieval approaches.</p><p>In order to combine concept-based retrieval and term-based retrieval, Müller and Gurevych <ref type="bibr" coords="2,502.49,303.30,10.52,8.74" target="#b3">[4]</ref> use Wikipedia and Wiktionary as background knowledge to improve the retrieval performance on a mono-lingual search task. They were able to improve the performance measured by mean average precision by 34% compared to the bag-of-words baseline. Similar to our approach they use Explicit Semantic Analysis <ref type="bibr" coords="2,172.57,351.12,10.52,8.74" target="#b1">[2]</ref> for concept-based retrieval. In this paper we extend this approach to CLIR and investigate different strategies to combine evidence from different retrieval approaches.</p><p>Croft <ref type="bibr" coords="2,131.99,375.03,10.52,8.74" target="#b0">[1]</ref> describes different strategies to combine IR techniques. He shows that the task of combining the output of different retrieval systems can be modeled as the task of combining the output of multiple classifiers. He also presents different frameworks to combine multiple retrieval systems at different levels, e.g. at the representation level or at the output level. In our approach we use some of the score normalization algorithms presented by Croft. Our combination approaches are also inspired by this work, but we extend it by using machine learning to find optimal parameters of the combination.</p><p>The results of these different combination approaches show that evidence coming from different sources can be aggregated to achieve better performance of the overall retrieval system. In the context of our participation on CLEF this year, we investigate whether these techniques can also be used for the Ad-hoc task on the TEL datasets. Overall, we build on the system we presented at CLEF2008, which achieved a reasonable performance using concept based retrieval based on Explicit Semantic Analysis.</p><p>Our main contributions in this paper are the following ones:</p><p>• We extend both MT-based and concept-based retrieval into truly multi-lingual settings where not only the document collection can contain multiple languages but a document itself can contain fields in different languages. The main innovation is here that we maintain separate indices for each language and apply our combination strategies on the retrieval engines for each of these language-specific indices. Our results show that for the CLEF Ad-hoc TEL task we get a similar performance compared to a baseline system based on a single index, but no significant improvement over it.</p><p>• We also present an approach by which MT-based and concept-based retrieval (by ESA) can be combined through rank aggregation. This combination effectively increases the performance of the retrieval system for the bi-lingual task on the BL dataset using French topics and the ONB dataset using English and German topics.</p><p>The paper is structured as follows: In the Section 2 we describe our retrieval system and define MT-based retrieval, concept based retrieval as well as different aggregation approaches. In Section 3 we describe the used datasets and the preprocessing of the data. In Section 4 we present the experiments on the Ad-hoc TEL task using topics from CLEF2008, in Section 5 using topics from CLEF2009. We conclude in Section 6.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>The main idea behind our approach is to use multiple indices (one for each language under consideration, which are all the common European languages). These are indices of fields of documents in different languages as well as concept indices of documents. The basic idea is to combine retrieval results based on the different indices. Figure <ref type="figure" coords="3,298.32,425.18,4.98,8.74" target="#fig_0">1</ref> illustrates the different indices and processing steps which will be described in more detail in the following sections. But first we introduce some notation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notations</head><p>In the remaining article we use the following notations:</p><formula xml:id="formula_0" coords="3,104.94,514.28,408.06,29.16">• L = {α, β, γ, . . .}: A set of languages. • D = {d 1 , . . . , d n }: A text corpus consisting of multi-lingual documents. The function f α (d)</formula><p>selects all the document fragments of d in language α. D α = {f α (d 1 ), . . . , f α (d n )} defines a restriction of corpus D where all document consist of their fragments in language α.</p><p>• C = {c 1 , . . . , c m }: A set of concepts that define a concept space. Each concept has a textual description. We use c i both to refer to concept c i as well as to the description of c i . The intended meaning will be clear from the context.</p><p>• T α = {t α,1 , t α,2 , . . .}: A set of topics in language α that will be used to construct queries to the retrieval system. Each topic represents a certain information need. for the translation of a topic t α to language β we will use the notation t α→β .</p><p>• Statistics of a term w in document d of corpus D:</p><p>-TF d (w): Term frequency of w in document d.</p><p>-|d|: Document length of d.</p><p>-DF(w): Document frequency of w in corpus D.</p><p>-TF(w): Term frequency of w in corpus D.</p><p>n = |D|: Number of documents.</p><p>-| d|: Average document length in corpus D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Language Detection</head><p>In our settings, the document corpus consists of multi-lingual documents which contain content in multiple languages. In our approach we assume that the parts of a document which are in different languages are identified and labeled appropriately. This is essentially the way how the f α function described above is realized. This makes the application of language detection approaches necessary before indexing the documents (we will rely on different indices per language). In our settings the parts correspond to the fields of the documents in the TEL dataset which can be in different languages. In order to identify the language for each field, we exploit a language detection approach based on character n-grams models. The probability distributions for character sequences of the size n are used to classify text into a set of languages. We used a classifier provided by the Ling Pipe Identification Tool<ref type="foot" coords="4,215.31,264.46,3.97,6.12" target="#foot_0">1</ref> which was trained on corpora in different languages as described in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Machine Translation based CLIR</head><p>In the most simple case, the CLIR problem can be formulated as bilingual retrieval: given a topic t β in language β and a set of multi-lingual documents D, find relevant documents in D α . If all document fragments in D are of language α then D = D α , which is the most common scenario. In this case a MT system translating text from language β to α can be used to reduce the problem to mono-lingual retrieval by translating topic t β to a topic t β→α in language α. Mono-lingual retrieval models can then be used to define the relevance of documents in D α to the translated topic t β→α .</p><p>In our approach we extend the bi-lingual setting to multiple languages. As shown in Figure <ref type="figure" coords="4,508.02,407.95,4.98,8.74" target="#fig_0">1</ref> the first step is building indices for each language α that contain all terms of documents in D α . This means that index I α only contains information about text in language α. In the retrieval step, each topic is simultaneously translated into all languages and each translation of the topic is matched to the corresponding index. This results in a different ranking for each language. An overall ranking is computed through different aggregation approaches of these rankings which will be described in more detail in Section 2.5.</p><p>The matching of the translated topic to the language specific index is based on a monolingual retrieval model. In this paper we use models that have been implemented in the Terrier<ref type="foot" coords="4,508.53,502.02,3.97,6.12" target="#foot_1">2</ref> framework.</p><p>For mono-lingual IR, we use the following retrieval models:</p><formula xml:id="formula_1" coords="4,104.94,547.43,394.47,169.64">• DLH13 Score(t, d) := w∈t TF t (w) TF d (w) log TF d (w) | d||D| |d|TF(w) + .5 log 2πTF d (w)(1 -TF d (w) |d| ) TF d (w) + .5 • BB2 Score(t, d) := w∈t TF t (w) TF(w) + 1 DF(w)(NTF d (w) + 1) TF t (w) (-log(|D| -1) + Φ(|D| + TF(w) -1, |D| + TF(w) -NTF d (w) -2) -Φ(TF(w), TF(w) -NTF d (w))) with NTF d (w) = TF d (w) log 1 + | d| |d| and Φ(n, m) := m + .5 log n m + (n -m) log n. • LemurTF IDF Score(t, d) := w∈t TF t (w) 1.2TF d (w) TF d (w) + 1.2(.25 + .75 |d| | d| ) log |D| DF(w) 2 2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Concept-based CLIR</head><p>As an instance of concept-based CLIR we build on the CL-ESA approach previously presented in <ref type="bibr" coords="5,102.86,206.22,9.96,8.74" target="#b5">[6]</ref>. For the sake of completeness we first discuss Explicit Semantic Analysis and then the cross-language extension CL-ESA.</p><p>In our retrieval system each document is mapped by ESA into a conceptual representation (the Wikipedia article space) which can be understood as an interlingua-based representation abstracting from languages which is inherently able to represent documents with fields in different languages. As shown in Figure <ref type="figure" coords="5,235.16,265.99,4.98,8.74" target="#fig_0">1</ref> we follow two different approaches to build the index. One approach maps whole documents to the Wikipedia article space using ESA without considering that documents can contain different languages. The second approach classifies each field of a document into a corresponding language and then maps each field field into a concept vector using the a language-specific ESA instantiation.</p><p>We compare the performance of these approaches in our experiments. In both cases we rely on a single index for concept based retrieval, as the multiple languages are already considered in the concept mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Explicit Semantic Analysis (ESA)</head><p>ESA classifies given document d with respect to a set of explicitly given external categories C. Gabrilovich and Markovitch <ref type="bibr" coords="5,217.67,405.71,10.52,8.74" target="#b1">[2]</ref> have outlined the general theory behind ESA and in particular described its instantiation to the case of using Wikipedia articles as external categories. We will basically build on this instantiation which we briefly summarize in the following.</p><p>In essence, Explicit Semantic Analysis takes as input a document d and maps it to a highdimensional real-valued vector space. This vector space is spanned by a concept space C α = {c 1 , . . . , c m } in language α such that each dimension corresponds to concept c i . This mapping is given by the following function:</p><formula xml:id="formula_2" coords="5,224.93,475.86,149.27,31.91">Φ α : D → R |Cα| with Φ α (d) := AS(d, c 1 ), . . . , AS(d, c m )</formula><p>The function AS expresses the association strength between d and the concept c i . In the original ESA model AS is defined by sum of TF.IDF ci values of all words of w j ∈ d based on the textual description of concept c i . In previous work we examined the performance of different association strength functions for CLIR tasks <ref type="bibr" coords="5,236.41,554.67,9.96,8.74" target="#b6">[7]</ref>. Based on these result we use the following modified function:</p><formula xml:id="formula_3" coords="5,215.43,574.59,163.61,26.88">AS(d, c i ) := w∈d TF ci (w) |c i | log |C| DF(w)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Cross-lingual ESA (CL-ESA)</head><p>In this section we present the extension to ESA called CL-ESA (Cross-language Explicit Semantic Analysis). This is a relatively straightforward extension of ESA to a cross-lingual setting which we presented before in <ref type="bibr" coords="5,176.44,659.33,9.96,8.74" target="#b5">[6]</ref>. We will also describe how CL-ESA can be used for the semantic analysis for multi-lingual documents.</p><p>CL-ESA relies on the principle that concept vectors computed with respect to the Wikipedia database in one language can be translated into concept vectors with respect to another Wikipedia database relying on Wikipedia's language links 3 . This is done by mapping each dimension corresponding to article a in Wikipedia W α to the dimension corresponding to article b in Wikipedia W β so that there exists a language link from a to b. This means that article a and b are textual descriptions of the same concept. Given this mapping it is for example possible to compare documents in language α and β based on the mapped concept vector.</p><p>In general the concept space that is used for CL-ESA needs textual descriptions of all concepts in all supported languages. We will refer to the description of concept c i in language α by c i,α . For a multi-lingual document d CL-ESA is defined as follows:</p><formula xml:id="formula_4" coords="6,240.03,191.15,272.97,20.06">AS(d, c i ) := α∈L AS(d α , c i,α )<label>(1)</label></formula><p>When CL-ESA is instantiated using the Wikipedia database, the articles have to be restricted to the articles having cross-language links to articles in all languages in L. Then all concepts represented by an article in any language have descriptions in all other languages given by the linked articles, which is needed for our model. In the following m α→β : W α → W β defines the function mapping articles from W α according to language links to W β .</p><p>Given a target language α for the concept representation of a multi-lingual document d with respect to Wikipedia W α = {a 1 , a 2 , . . .}, the association strength defined in Equation 1 can be instantiated to Wikipedia by:</p><formula xml:id="formula_5" coords="6,212.82,321.49,177.36,20.14">AS Wα (d, a i ) := β∈L AS(f β (d), m α→β (a i ))</formula><p>Intuitively this is the association strength of a multi-lingual document d to a concept c represented by the Wikipedia article a i in language α. This value is defined by the sum of the association strength of all fragments f β (d) in languages β to the concept description of c in language β. This description is given by the article in W β to which a i links to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Retrieval using CL-ESA</head><p>Using the above defined association strength function, a mapping Φ of documents or topics to concept vectors can be defined as follows:</p><formula xml:id="formula_6" coords="6,218.53,459.03,162.06,9.65">Φ(d) := d = AS(d, c 1 ), . . . , AS(d, c m )</formula><p>Given the vector representations of topics and documents, similarity measures in vector space can be used to determine the relevance of documents to topics. In our previous work we defined the following relevance function <ref type="bibr" coords="6,214.00,500.85,9.96,8.74" target="#b6">[7]</ref>:</p><formula xml:id="formula_7" coords="6,230.96,518.76,141.08,8.74">rel(t, d) := Γ(Π(Φ(q)), Π(Φ(d))),</formula><p>where Π is a projection function which reduces the dimensionality of the vector. This is done for performance issues as efficient indexing is not possible without the reduction. In our framework we use Π m abs ( d) which selects the m dimensions with highest values in d, as this reduction function was shown to achieve good performance in CLIR tasks <ref type="bibr" coords="6,332.14,572.54,9.96,8.74" target="#b6">[7]</ref>.</p><p>Γ defines the vector space similarity. We used the cosine similarity that is defined as</p><formula xml:id="formula_8" coords="6,260.14,603.14,81.52,24.13">Γ cosine = &lt; t, d &gt; t d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Rank Aggregation</head><p>In our framework we aggregate two different kinds of rankings for a topic t. First, as we deal with multi-lingual documents and due to our separate language-specific indexing approach, for each language α ∈ L there is a ranking that expresses the relevance based on the text parts in language α. Second we compute a ranking based on the concept representation of topics and documents. In our framework we chose a two step rank aggregation approach. We first combine all text-based rankings and finally combine the resulting ranking with the concept-based ranking.</p><p>In the following we describe different rank aggregation methods which we used for either the first or the second step of rank aggregation. More details will be presented in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Linear Aggregation</head><p>As the first approach to aggregate different ranking scores we chose linear aggregation. This means that the final relevance score of a document is computed by the sum of all scores in the different rankings:</p><formula xml:id="formula_9" coords="7,228.28,167.71,146.45,20.06">score(t, d) := r∈R δ(r) score r (t, d)</formula><p>where R is a set of rankings and δ(r) a weighting function. In our experiments we use the following variations of this weighting function:</p><p>• Normalization using max score: δ(r) := 1/maxscore(r) Before the aggregation, each ranking is normalized to values in [0, 1]. This is done by dividing each ranking score by the maximum score.</p><p>• Normalization using the number of retrieved documents: δ(r) := |r|/ r ∈R |r | where |r| is the number of retrieved documents of ranking r. This weight corresponds to the share of the number of retrieved documents for one ranking to the total number of retrieved documents for all rankings.</p><p>• A priori weights based on language: δ(r α ) := P (α) This weighting function can applied to our first step of rank aggregation. In this case each ranking r α is weighted by the apriori probability for a document to be in a certain language α. We use the share of text parts in language α in relation to all text parts in the corpus a apriori probability P (α).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">Support Vector Machine Aggregation</head><p>As alternative approach to linear aggregation we considered rank aggregation based on Support Vector Machines (SVMs). For a given topic or document, a feature vector can be built by using the relevance score returned by each index. This is then used as input for a SVM classifier that predicts the relevance of the document on the basis of a combination of the ranking scores. This means that the results of each retrieval step on the different indices are used as feature values. The classification model is trained by using the relevance assessment available for the corpus. Each relevant document for a topic defines a positive training example, each non-relevant a negative one.</p><p>Using a linear kernel the model of the classifier corresponds to linear aggregation. By using non-linear kernels this can be extended to non-linear rank aggregation. In Section 4 we describe experiments with linear kernels and radial basis function kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>In this section we first introduce all datasets we used for our experiments. Then we describe the evaluation methodology and the evaluation measures. Finally we briefly present some details about our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">TEL Dataset</head><p>The TEL dataset was provided by the European Library in the context of the CLEF 2008/2009 ad-hoc track. This dataset consists of library catalog records of three libraries: the British Library (BL) with 1,000,100 records, the Austrian National Library (ONB) with 869,353 records and the Bibliotheque Nationale de France (BNF) with 1,000,100 records. While the BL contains a majority of English records, the ONB dataset of German records and the BNF dataset of French records, all collections also contain records in multiple languages. Each record consists of fields which again  may be of different languages. Not all of these fields describe the content of the record but contain also meta data such as the publisher name or year of publication.</p><p>As the CLEF topics are only targeted at the content fields, we first identified all content fields. Table <ref type="table" coords="8,116.65,370.78,4.98,8.74" target="#tab_1">1</ref> contains a list of the selected fields and the average count of each field for a record. Further we reduced additional noise by removing non-content terms like constant prefix or suffix terms from fields, e.g. the prefix term Summary in abstract fields.</p><p>In order to be able to use the library catalog records as multi-lingual documents as defined in Section 2 we also had to determine the language of each field. Our language detection approach is based on the language tags provided for 100.0% (BL), 89.916% (ONB), 81.64% (BNF) of all records as well as on the text-based language detection approach described in Section 2. Our analysis of the datasets showed that relying merely on the language tags introduces many errors in language assignment. First there are records tagged with the wrong language. Second, as there is only one tag per record, language detection based on tags is not adequate for records containing fields in different languages. Our language detection model determines the language for each field based on evidence from tags and from text based classification. Table <ref type="table" coords="8,407.83,502.29,4.98,8.74" target="#tab_2">2</ref> contains the language distribution in the TEL datasets based on the tags (Tag) as well as on our detection model (Det). A manual evaluation using a random selection of records showed that performance of the language detection approach on fields is reasonable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Wikipedia Database</head><p>For concept-based retrieval we used the Wikipedia database in English, German and French as concept space. As we rely on bijective mappings between articles across languages for CL-ESA, we selected only those articles that are connected via cross-language links between all three Wikipedia databases. In this case every article is a concept having textual descriptions in English, German and French, namely the article text. Using the snapshot by 03/12/2008 for English, 06/25/2008 for French, and 06/29/2008 for German, we obtained the aligned collection of 166,484 articles in all three languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Training Corpora for Language Detection</head><p>The language detection framework requires sufficiently large corpora in all languages the classifier is trained for. We rely on the Leipzig Corpora Collection </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Preprocessing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Language Detection</head><p>For language detection we used the n-gram language classifier included in the Ling Pipe software collection <ref type="foot" coords="9,130.96,298.94,3.97,6.12" target="#foot_5">6</ref> . The classifier was trained using the Leipzig and JRC-Acquis corpora. When a certain language was available in both corpora we preferred the data of the Leipzig Corpus, as this showed better results in a cross validation on the training data. We conducted multiple tests for verifying the effectiveness of the language detection model. The results showed that using a 5-gram model and a 100,000 character training is optimal in our case. Table <ref type="table" coords="9,141.34,360.29,4.98,8.74" target="#tab_3">3</ref> contains the classification results using different data sizes measured by the character size. The results show that the classifier achieves high performance of more than 97% accuracy for text containing more than 32 characters. As this is given for most fields in the TEL dataset this classifier is applicable for the language detection task in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Document Preprocessing</head><p>We used the following methods for the preprocessing of documents:</p><p>Tokenizer As tokenizer we used a standard white space tokenizer. All non-character tokens were deleted. For Wikipedia articles we also deleted all wiki markup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stop-Word Filtering</head><p>We used standard stop word lists in the languages English, German, Finnish, French, Italian, Portugese, Swedish, which were taken from the University of Neuchatel<ref type="foot" coords="9,158.08,514.58,3.97,6.12" target="#foot_6">7</ref> , and Danish, Spanish, Dutch and Norwegian, which were taken form Ranks.nl<ref type="foot" coords="9,505.76,514.58,3.97,6.12" target="#foot_7">8</ref> .</p><p>Stemmer We used the Snowball Stemmers<ref type="foot" coords="9,282.12,534.51,3.97,6.12" target="#foot_8">9</ref> to stem terms in English, German, French, Danish, Dutch, Finnish, Italian, Norwegian, Portugese and Swedish.</p><p>Fields in other languages than those mentioned above were not preprocessed using stemmers or stop word lists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation Measures</head><p>The relevance assessments for the search task are provided by CLEF, resulting from a pooled manual evaluation. As evaluation measure we report mean average precision (MAP), precision at a cutoff level of 10 (P@10) and recall at a cutoff level of 100 (R@100).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation</head><p>In our implementation we used different third party software tools as well as own implementations.</p><p>For text based retrieval including inverted indexes and scoring models we used the Terrier IR framework. For translating the topics to various languages we used the machine translation service provided by Google<ref type="foot" coords="10,174.20,164.70,7.94,6.12" target="#foot_9">10</ref> . We used our own implementation of CL-ESA for concept-based retrieval<ref type="foot" coords="10,501.79,164.70,7.94,6.12" target="#foot_10">11</ref> . We also implemented an inverted concept index that allows efficient retrieval based on the concept representations of topics and documents. For example, for the ONB dataset the inverted concept index has the size of approx. 26 GB and the average processing time of a topic is approx. 135 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments on CLEF08 Ad-hoc Topics</head><p>In this section we present the results of experiments using the CLEF08 Ad-hoc topics. As relevance assessments are available for these topics we used this task to optimize our system in respect to the retrieval model and the aggregation functions.</p><p>In all experiments we relied on the mono-lingual task, i.e. English topics for BL dataset, German topics for ONB and French topics for BNF. As all of these datasets contain documents in different languages, cross-lingual retrieval can be applied to find relevant documents in other languagesl. The mono-lingual task can therefore also be used to optimize the multi-lingual setting we propose in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mono-lingual Retrieval Model</head><p>First we conducted experiments to optimize the retrieval models for MT based IR. As this is based on mono-lingual retrieval we compared the performance of different State-of-the-Art retrieval models. The hypothesis here was that good performance in mono-lingual retrieval should also result in good performance in cross-lingual retrieval.</p><p>We rely on the retrieval models provided by the Terrier framework in our work. We selected the best retrieval model for each dataset according to MAP and got the following best retrieval results on the different TEL datasets: MAP of .34 on the BL dataset using model DLH13, MAP of .22 on the ONB dataset using model LemurTF IDF and MAP op .30 on the BNF dataset using model BB2. In the remainder of this paper we will report results relying on the best retrieval model for each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Rank Aggregation</head><p>As described above we defined two aggregation steps in our model. First the results of multiple text-based indexes are aggregated and afterwards the aggregated score is combined with conceptbased retrieval score. In the following experiments we used again the CLEF2008 topics for the Ad-hoc mono-lingual task. The first aggregation step was evaluated on all three TEL datasets. For the evaluation of the second step we only performed experiments on the BL dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Linear Aggregation for Multiple Indexes</head><p>The baseline for the proposed retrieval using multiple indexes is given by retrieval on a single index of all text in the documents without language classification. The performance of this baseline is shown in the first row of Table <ref type="table" coords="10,227.43,668.72,3.87,8.74">4</ref>.</p><p>As described in Section 2 we used different normalization and weighting models for linear aggregation of the multiple indexes. Table <ref type="table" coords="10,275.23,692.63,4.98,8.74">4</ref> contains all results of aggregation without normalization, using max score and using the number of retrieved documents of each index and aggregation using a priori weights. Table <ref type="table" coords="11,117.98,200.63,3.87,8.74">4</ref>: Results for MT-based retrieval on the CLEF08 mono-lingual task using a single index and using different rank aggregation methods for multiple indexes.</p><p>The results clearly show that our approaches to aggregate the results of the multiple indexes are not able to beat the baseline using a single index. Normalization based on the number of retrieved documents as well as a priori weights can both be used to achieve comparable performance in respect to MAP, P@10 and R@100. The results indicate that linear aggregation based on the multiple indexes seems not be able to improve the overall performance in this task.</p><p>As alternative approach to linear combination we experimented with Support Vector Machine based aggregation. To balance the ratio between the training data, we used all relevant documents for all topics as positive samples and randomly selected non-relevant documents as negative samples to achieve a ratio of positive/negative samples of 1/2.</p><p>As SVM implementation we used LIBSVM<ref type="foot" coords="11,290.84,350.43,7.94,6.12" target="#foot_11">12</ref> . Using the SVM type C-SVC (c=1) with a radial basis function kernel, the training data could be classified using a 5-fold cross validation with precision of .61 and recall of .42. However when using the trained model for the actual retrieval the MAP was very low with .01. When using a linear kernel, which would lead to a classifier that is comparable to linear aggregation, we were not able to learn the model as the learning algorithm did not terminate. Our assumption is that using these kernel functions it is not possible to separate the positive and negative samples in the feature space. This would also explain the bad performance of the resulting retrieval system. It might be possible to use SVMs for rank aggregation by using other kernels, but in the scope of this paper we did not investigate that idea any further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Linear Aggregation with Concept-Based Retrieval</head><p>In the technical report of last year, we presented results only based on concept-based retrieval using ESA <ref type="bibr" coords="11,113.16,515.85,9.96,8.74" target="#b5">[6]</ref>. In the current system we also investigate a modified version of the ESA-based mapping to the Wikipedia article space. The language classification step represents the TEL records as multi-lingual documents. This is used to map the documents fragments for each language to the concept space based on the Wikipedia databases in the corresponding languages. The concept vector representations of the different fragments are then combined to a single concept vector for each document as described in Section 2. Experiments on the CLEF08 mono-lingual task on the BL dataset showed an improvement of the new concept mapping model with respect to the model used in the last year experiments of 1% MAP, 7% P@10 and 5% R@100. For our experiments on the CLEF09 tasks we therefore used the new model.</p><p>In our final experiments using the CLEF08 topics we investigated the combination of MT based retrieval and concept-based retrieval. As for example suggested in <ref type="bibr" coords="11,406.69,635.40,10.52,8.74" target="#b3">[4]</ref> we also chose a linear aggregation function. The problem thereby is to find an optimal weight for each retrieval model. We approximated the optimal weight by a brute-force and systematic exploration of the parameter space. The results of this exploration for the BL dataset are presented in Figure <ref type="figure" coords="11,442.15,671.26,3.87,8.74" target="#fig_1">2</ref>. The left most bar represents MAP value giving full weight to the concept-based retrieval, while the right most bar represents the MAP giving full weight to the concept-based retrieval. The bars in between result from experiments using the combined approach with different weights. For the experiments using the CLEF2009 topics we used the best weightings derived from these experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments on CLEF09 Ad-hoc Topics</head><p>The CLEF09 Ad-hoc topics are similar to the topics from CLEF08. The 50 topics have the same format consisting of two fields, a short title containing 2-4 keywords and a description of the information item of interest in terms of 1-2 sentences. The objective is to query the selected target collection using topics in the same language (mono-lingual run) or topics in a different language (bi-lingual run) and to submit the results in a ranked list ordered with respect to decreasing relevance. In line with these objectives we submitted results of six different runs to CLEF08. These are the results of querying English, German and French topics to the BL, ONB and BNF datasets.</p><p>The results of our experiments are presented in Table <ref type="table" coords="12,347.53,607.12,3.87,8.74" target="#tab_5">5</ref>. The results using multiple indexes show that this approach was not able to beat the baseline. Using a single index for the TEL records without language classification and topics only translated into the main language of each dataset achieved better performance compared to our approach based on indexes for each language and multiple translations of the topic to the matching languages. Another result is that the combination of concept-based retrieval to the MT-based retrieval was able to improve the retrieval in some cases. The improvement was significant according to a paired t-test with confidence level .05 for French topics on the BL dataset and English and German topics on the ONB dataset. However in many cases the performance was similar to the baseline without statistical significance of the difference. We could therefore not reproduce the strong improvements e.g. presented in <ref type="bibr" coords="12,499.71,714.71,9.96,8.74" target="#b3">[4]</ref>.</p><p>In this paper we have presented a cross-language information retrieval approach based on multiple indexes for different languages and rank aggregation to combine the different partial results. The approach was developed in the light of the fact that the CLEF TEL dataset consists of records in different languages which also may contain fragments of more than one language. For this approach a language detection of all documents fragments of the dataset as well as translation of topics to all supported languages is necessary. Our results showed that for the CLEF08 and CLEF09 Ad-hoc task we were not able to improve retrieval result with this new model. The baseline consisting of a single index without language classification and a topic translated only to the index language achieved similar or even better results.</p><p>We also combined Machine Translation based retrieval with concept-based retrieval. The results showed that we were able to improve the baseline through the combination in some cases. However the improvement on the CLEF Ad-hoc task were not as strong as reported on other experiments in related work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,165.19,336.50,272.62,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Figure of all used indices in our retrieval framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="12,90.00,265.66,423.00,8.74;12,90.00,277.62,423.00,8.74;12,90.00,289.57,406.31,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Results for the CLEF2008 mono-lingual ad-hoc task on the BL dataset using different weightings of MT-based retrieval and concept-based retrieval combined by linear aggregation. The left most result corresponds to MT-based retrieval, the right most to concept-based retrieval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,126.83,110.82,349.33,166.74"><head>Table 1 :</head><label>1</label><figDesc>Average frequency of content fields of the TEL library catalog records.</figDesc><table coords="8,126.83,110.82,349.33,166.74"><row><cell>Field</cell><cell></cell><cell cols="2">Description</cell><cell></cell><cell></cell><cell cols="3">BL ONB BNF</cell></row><row><cell>title</cell><cell></cell><cell cols="3">The title of the document</cell><cell></cell><cell>1</cell><cell>.95</cell><cell>1.05</cell></row><row><cell cols="2">subject</cell><cell cols="5">Keyword list of contained subjects 2.22</cell><cell>3.06</cell><cell>0.71</cell></row><row><cell cols="4">alternative Alternative title</cell><cell></cell><cell></cell><cell>.11</cell><cell>.50</cell><cell>0</cell></row><row><cell cols="2">abstract</cell><cell cols="3">Abstract oft the document</cell><cell></cell><cell>.002</cell><cell>.004</cell><cell>0</cell></row><row><cell></cell><cell>BL</cell><cell></cell><cell></cell><cell>ONB</cell><cell></cell><cell></cell><cell>BNF</cell><cell></cell></row><row><cell>Lang</cell><cell>Tag</cell><cell>Det</cell><cell>Lang</cell><cell>Tag</cell><cell>Det</cell><cell>Lang</cell><cell>Tag</cell><cell>Det</cell></row><row><cell>English</cell><cell cols="2">61.8% 76.7%</cell><cell>German</cell><cell cols="2">69.6% 80.9%</cell><cell>French</cell><cell cols="2">56.4% 77.6%</cell></row><row><cell>French</cell><cell>5.3%</cell><cell>4.0%</cell><cell>English</cell><cell>11.9%</cell><cell>8.0%</cell><cell>English</cell><cell>12.9%</cell><cell>8.2%</cell></row><row><cell>German</cell><cell>4.1%</cell><cell>2.9%</cell><cell>French</cell><cell>2.8%</cell><cell>2.1%</cell><cell>German</cell><cell>4.1%</cell><cell>3.8%</cell></row><row><cell>Spanish</cell><cell>3.1%</cell><cell>2.0%</cell><cell>Italian</cell><cell>1.8%</cell><cell>1.5%</cell><cell>Italian</cell><cell>2.3%</cell><cell>1.4%</cell></row><row><cell>Russian</cell><cell>2.7%</cell><cell>1.7%</cell><cell>Esperanto</cell><cell>1.5%</cell><cell>1.5%</cell><cell>Spanish</cell><cell>2.0%</cell><cell>1.4%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,90.00,290.58,423.00,20.69"><head>Table 2 :</head><label>2</label><figDesc>Distribution of the 5 most frequent languages in each dataset, based on the language tags (Tag) and on the language detection model (Det).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,368.15,708.84,144.85,10.31"><head>Table 3 :</head><label>3</label><figDesc>4 , which contains texts collected Results of language detection using test data of different character sizes measured by classification accuracy.from the web and newspapers, and the JRC-Acquis Multilingual Parallel Corpus 5 , which contains documents published by the European Union translated in various languages.</figDesc><table coords="9,114.56,110.43,373.87,45.00"><row><cell>Test Size (characters)</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell></row><row><cell>Accuracy</cell><cell cols="6">22.59 % 34.82 % 58.55 % 81.17 % 92.45 % 97.33 %</cell></row><row><cell>Test Size (characters)</cell><cell>64</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>1024</cell><cell>2048</cell></row><row><cell>Accuracy</cell><cell cols="5">98.99 % 99.67 % 99.86 % 99.97 % 99.99 %</cell><cell>100 %</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="12,90.00,312.21,423.00,154.35"><head>Table 5 :</head><label>5</label><figDesc>Results on the CLEF 2009 Ad-Hoc Task. Statistical relevant improvements according to a paired t-test with confidence level .05 are marked with * .</figDesc><table coords="12,109.19,312.21,384.63,120.64"><row><cell>Topic</cell><cell></cell><cell></cell><cell>BL</cell><cell></cell><cell></cell><cell>ONB</cell><cell></cell><cell></cell><cell>BNF</cell><cell></cell></row><row><cell cols="2">Lang. Retrieval Method</cell><cell>MAP</cell><cell>P@10</cell><cell>R@100</cell><cell>MAP</cell><cell>P@10</cell><cell>R@100</cell><cell>MAP</cell><cell>P@10</cell><cell>R@100</cell></row><row><cell>en</cell><cell>Baseline (single index)</cell><cell>.35</cell><cell>.51</cell><cell>.55</cell><cell>.16</cell><cell>.26</cell><cell>.36</cell><cell>.25</cell><cell>.39</cell><cell>.45</cell></row><row><cell></cell><cell>Multiple Indexes</cell><cell>.33</cell><cell>.50</cell><cell>.52</cell><cell>.15</cell><cell>.24</cell><cell>.35</cell><cell>.22</cell><cell>.34</cell><cell>.45</cell></row><row><cell></cell><cell>Concept + Baseline</cell><cell>.35</cell><cell>.52</cell><cell>.54</cell><cell>.17  *</cell><cell>.27</cell><cell>.37</cell><cell>.25</cell><cell>.39</cell><cell>.45</cell></row><row><cell>de</cell><cell>Baseline (single index)</cell><cell>.33</cell><cell>.49</cell><cell>.53</cell><cell>.23</cell><cell>.35</cell><cell>.47</cell><cell>.24</cell><cell>.35</cell><cell>.45</cell></row><row><cell></cell><cell>Multiple Indexes</cell><cell>.31</cell><cell>.48</cell><cell>.51</cell><cell>.23</cell><cell>.34</cell><cell>.49</cell><cell>.22</cell><cell>.32</cell><cell>.43</cell></row><row><cell></cell><cell>Concept + Baseline</cell><cell>.33</cell><cell>.49</cell><cell>.53</cell><cell>.24  *</cell><cell>.35</cell><cell>.47</cell><cell>.24</cell><cell>.36</cell><cell>.45</cell></row><row><cell>fr</cell><cell>Baseline (single index)</cell><cell>.31</cell><cell>.48</cell><cell>.50</cell><cell>.15</cell><cell>.22</cell><cell>.31</cell><cell>.27</cell><cell>.38</cell><cell>.51</cell></row><row><cell></cell><cell>Multiple Indexes</cell><cell>.29</cell><cell>.45</cell><cell>.47</cell><cell>.14</cell><cell>.20</cell><cell>.32</cell><cell>.25</cell><cell>.35</cell><cell>.50</cell></row><row><cell></cell><cell>Concept + Baseline</cell><cell cols="2">.32 .51  *</cell><cell>.50</cell><cell>.15</cell><cell>.22</cell><cell>.31</cell><cell>.27</cell><cell>.37</cell><cell>.50</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,105.24,729.51,119.05,6.64"><p>http://alias-i.com/lingpipe/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="4,105.24,739.02,135.50,6.64"><p>http://ir.dcs.gla.ac.uk/terrier/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="5,105.24,737.54,407.76,6.99;5,90.00,747.00,75.75,6.99"><p>Cross-language links are those that link a certain article to a corresponding article in the Wikipedia database in another language.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="8,105.24,729.92,123.29,6.64"><p>http://corpora.uni-leipzig.de</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="9,105.24,670.16,114.32,6.64"><p>http://wt.jrc.it/lt/Acquis/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="9,105.24,679.66,119.05,6.64"><p>http://alias-i.com/lingpipe/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="9,105.24,689.17,182.07,6.64"><p>http://members.unine.ch/jacques.savoy/clef/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7" coords="9,105.24,698.67,122.79,6.64"><p>http://www.ranks.nl/resources</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8" coords="9,105.24,708.18,118.56,6.64"><p>http://snowball.tartarus.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9" coords="10,105.24,736.04,114.32,6.64"><p>http://translate.google.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10" coords="10,105.24,745.55,157.16,6.64"><p>http://code.google.com/p/research-esa</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11" coords="11,105.24,738.59,173.61,8.18"><p>http://www.csie.ntu.edu.tw/ ~cjlin/libsvm/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was funded by the <rs type="funder">Multipla</rs> project sponsored by the <rs type="funder">German Research Foundation (DFG)</rs> under grant number <rs type="grantNumber">38457858</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_9hcHu66">
					<idno type="grant-number">38457858</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="13,105.50,398.79,407.50,8.74;13,105.50,410.75,120.68,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="13,181.01,398.79,203.02,8.74">Combining approaches to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,405.01,398.79,107.98,8.74;13,105.50,410.75,37.33,8.74">Advances in Information Retrieval</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,105.50,430.67,407.50,8.74;13,105.50,442.63,407.51,8.74;13,105.50,454.58,195.45,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="13,268.22,430.67,244.77,8.74;13,105.50,442.63,110.07,8.74">Computing semantic relatedness using wikipedia-based explicit semantic analysis</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,238.15,442.63,274.85,8.74;13,105.50,454.58,86.55,8.74">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1606" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,105.50,474.51,407.50,8.74;13,105.50,486.46,331.28,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="13,273.88,474.51,239.12,8.74;13,105.50,486.46,85.28,8.74">CLEF 2008 Ad-Hoc Track: On-line Processing Experiments with Xtrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kürsten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,211.84,486.46,194.32,8.74">Working Notes of the Annual CLEF Meeting</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,105.50,506.39,407.50,8.74;13,105.50,518.34,283.76,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="13,228.39,506.39,284.62,8.74;13,105.50,518.34,37.76,8.74">Using Wikipedia and Wiktionary in Domain-Specific Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,164.32,518.34,194.32,8.74">Working Notes of the Annual CLEF Meeting</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,105.50,538.27,407.50,8.74;13,105.50,550.22,66.97,8.74" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="13,269.83,538.27,195.36,8.74">Introduction to Modern Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>McGraw-Hill, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,105.50,570.15,407.51,8.74;13,105.50,582.10,237.39,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="13,214.36,570.15,294.27,8.74">Cross-lingual Information Retrieval with Explicit Semantic Analysis</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sorg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,117.95,582.10,194.32,8.74">Working Notes of the Annual CLEF Meeting</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,105.50,602.03,407.50,8.74;13,105.50,613.98,407.51,8.74;13,105.50,625.94,393.73,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="13,261.98,602.03,251.03,8.74;13,105.50,613.98,192.62,8.74">An experimental comparison of explicit semantic analysis implementations for cross-language retrieval</title>
		<author>
			<persName coords=""><forename type="first">Philipp</forename><surname>Sorg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philipp</forename><surname>Cimiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,321.02,613.98,191.98,8.74;13,105.50,625.94,302.58,8.74">Proceedings of the International Conference on Applications of Natural Language to Information Systems (NLDB)</title>
		<meeting>the International Conference on Applications of Natural Language to Information Systems (NLDB)<address><addrLine>Saarbrücken</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
