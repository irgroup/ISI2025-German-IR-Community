<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,177.24,116.07,260.73,12.67;1,231.96,134.07,151.47,12.67">CLEF 2009 Ad Hoc Track Overview: TEL &amp; Persian Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,237.96,171.62,53.71,8.85"><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
							<email>ferro@dei.unipd.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,318.73,171.62,54.33,8.85"><forename type="first">Carol</forename><surname>Peters</surname></persName>
							<email>carol.peters@isti.cnr.it</email>
							<affiliation key="aff1">
								<orgName type="institution">ISTI-CNR</orgName>
								<address>
									<addrLine>Area di Ricerca</addrLine>
									<settlement>Pisa</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,177.24,116.07,260.73,12.67;1,231.96,134.07,151.47,12.67">CLEF 2009 Ad Hoc Track Overview: TEL &amp; Persian Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E9941CAE9DC3F14CFEBD7E3B4A72C0F5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 [Systems and Software]: Performance evaluation Experimentation, Performance, Measurement, Algorithms Multilingual Information Access, Cross-Language Information Retrieval, Word Sense Disambiguation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The 2009 Ad Hoc track was to a large extent a repetition of last year's track, with the same three tasks: Tel@CLEF, Persian@CLEF, and Robust-WSD. In this first of the two track overviews, we describe the objectives and results of the TEL and Persian tasks and provide some statistical analyses.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>From 2000 -2007, the ad hoc track at CLEF used exclusively collections of European newspaper and news agency documents <ref type="foot" coords="1,356.04,530.05,3.97,6.97" target="#foot_0">1</ref> . In 2008 it was decided to change the focus and to introduce document collections in a different genre (bibliographic records from The European Library -TEL<ref type="foot" coords="1,394.08,553.93,3.97,6.97" target="#foot_1">2</ref> ), a non-European language (Persian), and an IR task that would appeal to the NLP community (robust retrieval on word-sense disambiguated data). The 2009 Ad Hoc track has been to a large extent a repetition of last year's track, with the same three tasks: Tel@CLEF, Persian@CLEF, and Robust-WSD. An important objective has been to ensure that for each task a good reusable test collections is created.</p><p>In this first of the two track overviews we describe the activities of the TEL and Persian tasks <ref type="foot" coords="2,192.24,129.01,3.97,6.97" target="#foot_2">3</ref> .</p><p>TEL@CLEF: This task offered monolingual and cross-language search on library catalog. It was organized in collaboration with The European Library and used three collections derived from the catalogs of the British Library, the Bibliothéque Nationale de France and the Austrian National Library. The underlying aim was to identify the most effective retrieval technologies for searching this type of very sparse multilingual data. In fact, the collections contained records in many languages in addition to English, French or German. The task presumed a user with a working knowledge of these three languages who wants to find documents that can be useful for them in one of the three target catalogs.</p><p>Persian@CLEF: This activity was coordinated again this year in collaboration with the Database Research Group (DBRG) of Tehran University. We chose Persian as the first non-European language target collection for several reasons: its challenging script (a modified version of the Arabic alphabet with elision of short vowels) written from right to left; its complex morphology (extensive use of suffixes and compounding); its political and cultural importance. The task used the Hamshahri corpus of 1996-2002 newspapers as the target collection and was organised as a traditional ad hoc document retrieval task. Monolingual and cross-language (English to Persian) tasks were offered.</p><p>In the rest of this paper we present the task setup, the evaluation methodology and the participation in the two tasks (Section 2). We then describe the main features of each task and show the results (Sections 3 and 4). The final section provides a brief summing up. For information on the various approaches and resources used by the groups participating in the two tasks and the issues they focused on, we refer the reader to the papers in the relevant Ad Hoc sections of these Working Notes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Track Setup</head><p>As is customary in the CLEF ad hoc track, again this year we adopted a corpusbased, automatic scoring method for the assessment of the performance of the participating systems, based on ideas first introduced in the Cranfield experiments in the late 1960s <ref type="bibr" coords="2,239.79,528.86,10.00,8.85" target="#b4">[5]</ref>. The tasks offered are studied in order to effectively measure textual document retrieval under specific conditions. The test collections are made up of documents, topics and relevance assessments. The topics consist of a set of statements simulating information needs from which the systems derive the queries to search the document collections. Evaluation of system performance is then done by judging the documents retrieved in response to a topic with respect to their relevance, and computing the recall and precision measures. The pooling methodology is used in order to limit the number of manual relevance assessments that have to be made. As always, the distinguishing feature of CLEF is that it applies this evaluation paradigm in a multilingual setting. This means that the criteria normally adopted to create a test collection, consisting of suitable documents, sample queries and relevance assessments, have been adapted to satisfy the particular requirements of the multilingual context. All language dependent tasks such as topic creation and relevance judgment are performed in a distributed setting by native speakers. Rules are established and a tight central coordination is maintained in order to ensure consistency and coherency of topic and relevance judgment sets over the different collections, languages and tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Documents</head><p>As mentioned in the Introduction, the two tasks used different sets of documents.</p><p>The TEL task used three collections:</p><p>-British Library (BL); 1,000,100 documents, 1.2 GB; -Bibliothéque Nationale de France (BNF); 1,000,100 documents, 1.3 GB; -Austrian National Library (ONB); 869,353 documents, 1.3 GB.</p><p>We refer to the three collections (BL, BNF, ONB) as English, French and German because in each case this is the main and expected language of the collection. However, each of these collections is to some extent multilingual and contains documents (catalog records) in many additional languages.</p><p>The TEL data is very different from the newspaper articles and news agency dispatches previously used in the CLEF ad hoc track. The data tends to be very sparse. Many records contain only title, author and subject heading information; other records provide more detail. The title and (if existing) an abstract or description may be in a different language to that understood as the language of the collection. The subject heading information is normally in the main language of the collection. About 66% of the documents in the English and German collection have textual subject headings, in the French collection only 37%. Dewey Classification (DDC) is not available in the French collection; negligible (¡0.3%) in the German collection; but occurs in about half of the English documents (456,408 docs to be exact).</p><p>Whereas in the traditional ad hoc task, the user searches directly for a document containing information of interest, here the user tries to identify which publications are of potential interest according to the information provided by the catalog card. When we designed the task, the question the user was presumed to be asking was "Is the publication described by the bibliographic record relevant to my information need?"</p><p>The Persian task used the Hamshahri corpus of 1996-2002 newspapers as the target collection. This corpus was made available to CLEF by the Data Base Research Group (DBRG) of the University of Tehran. Hamshahri is one of the most popular daily newspapers in Iran. The Hamshahri corpus consists of 345 MB of news texts for the years 1996 to 2002 (corpus size with tags is 564 MB). This corpus contains more than 160,000 news articles about a variety of subjects and includes nearly 417000 different words. Hamshahri articles vary between 1KB and 140KB in size<ref type="foot" coords="4,276.00,129.01,3.97,6.97" target="#foot_3">4</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Topics</head><p>Topics in the CLEF ad hoc track are structured statements representing information needs. Each topic typically consists of three parts: a brief "title" statement; a one-sentence "description"; a more complex "narrative" specifying the relevance assessment criteria. Topics are prepared in xml format and uniquely identified by means of a Digital Object Identifier (DOI) <ref type="foot" coords="4,334.20,227.17,3.97,6.97" target="#foot_4">5</ref> .</p><p>For the TEL task, a common set of 50 topics was prepared in each of the 3 main collection languages (English, French and German) plus this year also in Chinese, Italian and Greek in response to specific requests. Only the Title and Description fields were released to the participants. The narrative was employed to provide information for the assessors on how the topics should be judged. The topic sets were prepared on the basis of the contents of the collections.</p><p>In ad hoc, when a task uses data collections in more than one language, we consider it important to be able to use versions of the same core topic set to query all collections. This makes it easier to compare results over different collections and also facilitates the preparation of extra topic sets in additional languages. However, it is never easy to find topics that are effective for several different collections and the topic preparation stage requires considerable discussion between the coordinators for each collection in order to identify suitable common candidates. The sparseness of the data makes this particularly difficult for the TEL task and leads to the formulation of topics that were quite broad in scope so that at least some relevant documents could be found in each collection. A result of this strategy is that there tends to be a considerable lack of evenness of distribution in relevant documents. For each topic, the results expected from the separate collections can vary considerably. An example of a TEL topic is given in Figure <ref type="figure" coords="4,203.90,468.50,3.90,8.85" target="#fig_0">1</ref>.</p><p>For the Persian task, 50 topics were created in Persian by the Data Base Research group of the University of Tehran, and then translated into English. The rule in CLEF when creating topics in additional languages is not to produce literal translations but to attempt to render them as naturally as possible. This was a particularly difficult task when going from Persian to English as cultural differences had to be catered for. An example of a CLEF 2009 Persian topic is given in Figure <ref type="figure" coords="4,203.90,552.14,3.90,8.85" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Relevance Assessment</head><p>The number of documents in large test collections such as CLEF makes it impractical to judge every document for relevance. Instead approximate recall values are calculated using pooling techniques. The results submitted by the groups &lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt; &lt;topic&gt; &lt;identifier&gt;10.2452/711-AH&lt;/identifier&gt; &lt;title lang="zh"&gt;深海生物&lt;/title&gt; &lt;title lang="en"&gt;Deep Sea Creatures&lt;/title&gt; &lt;title lang="fr"&gt;Créatures des fonds océaniques&lt;/title&gt; &lt;title lang="de"&gt;Kreaturen der Tiefsee&lt;/title&gt; &lt;title lang="el"&gt; &lt;/title&gt; &lt;title lang="it"&gt;Creature delle profondità oceaniche&lt;/title&gt; &lt;description lang="zh"&gt; 找有关世界上任何深海生物的出版物。 &lt;/description&gt; &lt;description lang="en"&gt; Find publications about any kind of life in the depths of any of the world's oceans. &lt;/description&gt; &lt;description lang="fr"&gt; Trouver des ouvrages sur toute forme de vie dans les profondeurs des mers et des océans. &lt;/description&gt; &lt;description lang="de"&gt; Finden Sie Veröffentlichungen über Leben und Lebensformen in den Tiefen der Ozeane der Welt. &lt;/description&gt; &lt;description lang="el"&gt; &lt;/description&gt; &lt;description lang="it"&gt; Trova pubblicazioni su qualsiasi forma di vita nelle profondità degli oceani del mondo. &lt;/description&gt; &lt;/topic&gt; participating in the ad hoc tasks are used to form a pool of documents for each topic and language by collecting the highly ranked documents from selected runs according to a set of predefined criteria. One important limitation when forming the pools is the number of documents to be assessed. Traditionally, the top 100 ranked documents from each of the runs selected are included in the pool; in such a case we say that the pool is of depth 100. This pool is then used for subsequent relevance judgments. After calculating the effectiveness measures, the results are analyzed and run statistics produced and distributed. The stability of pools constructed in this way and their reliability for post-campaign experiments is discussed in <ref type="bibr" coords="5,255.54,631.22,10.45,8.85" target="#b2">[3]</ref> with respect to the CLEF 2003 pools.</p><p>The main criteria used when constructing the pools in CLEF are: &lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt; &lt;topic&gt; &lt;identifier&gt;10.2452/641-AH&lt;/identifier&gt; &lt;title lang="en"&gt;Pollution in the Persian Gulf&lt;/title&gt; &lt;title lang="fa"&gt; &lt;/title&gt; &lt;description lang="en"&gt; Find information about pollution in the Persian Gulf and the causes. &lt;/description&gt; &lt;description lang="fa"&gt; &lt;/description&gt; &lt;narrative lang="en"&gt; Find information about conditions of the Persian Gulf with respect to pollution; also of interest is information on the causes of pollution and comparisons of the level of pollution in this sea against that of other seas. &lt;/narrative&gt; &lt;narrative lang="fa"&gt; &lt;/narrative&gt; &lt;/topic&gt; favour diversity among approaches adopted by participants, according to the descriptions of the experiments provided by the participants; -choose at least one experiment for each participant in each task, chosen among the experiments with highest priority as indicated by the participant; -add mandatory title+description experiments, even though they do not have high priority; -add manual experiments, when provided; -for bilingual tasks, ensure that each source topic language is represented.</p><p>From our experience in CLEF, using the tools provided by the DIRECT system <ref type="bibr" coords="6,167.60,484.70,10.00,8.85" target="#b0">[1]</ref>, we find that for newspaper documents, assessors can normally judge from 60 to 100 documents per hour, providing binary judgments: relevant / not relevant. Our estimate for the TEL catalog records is higher as these records are much shorter than the average newspaper article (100 to 120 documents per hour). In both cases, it can be seen what a time-consuming and resource expensive task human relevance assessment is. This limitation impacts strongly on the application of the criteria above -and implies that we are obliged to be flexible in the number of documents judged per selected run for individual pools.</p><p>This year, in order to create pools of more-or-less equivalent size, the depth of the TEL English, French, and German pools was 60 <ref type="foot" coords="6,372.12,590.17,3.97,6.97" target="#foot_5">6</ref> . For each collection, we included in the pool two monolingual and one bilingual experiments from each participant plus any documents assessed as relevant during topic creation.</p><p>As we only had a relatively small number of runs submitted for Persian, we were able to include documents from all experiments, and the pool was created with a depth of 80.</p><p>These pool depths were the same as those used last year. Given the resources available, it was not possible to manually assess more documents. For the CLEF 2008 ad hoc test collections, Stephen Tomlinson reported some sampling experiments aimed at estimating the judging coverage <ref type="bibr" coords="7,347.33,193.10,14.68,8.85" target="#b9">[10]</ref>. He found that this tended to be lower than the estimates he produced for the CLEF 2007 ad hoc collections. With respect to the TEL collections, he estimated that at best 50% to 70% of the relevant documents were included in the pools -and that most of the unjudged relevant documents were for the 10 or more queries that had the most known answers. Tomlinson has repeated these experiments for the 2009 TEL and Persian data <ref type="bibr" coords="7,239.77,264.74,10.00,8.85" target="#b8">[9]</ref>. Although for two of the four languages concerned (German and Persian), his findings were similar to last year's estimates, for the other two languages (English and French) this year's estimates are substantially lower. These findings need further investigation. They suggest that if we are to continue to use the pooling technique, we would perhaps be wise to do some more exhaustive manual searches in order to boost the pools with respect to relevant documents. We also need to consider more carefully other techniques for relevance assessment in the future such as, for example, the method suggested by Sanderson and Joho <ref type="bibr" coords="7,239.69,360.38,10.45,8.85" target="#b7">[8]</ref> or Mechanical Turk <ref type="bibr" coords="7,342.06,360.38,10.00,8.85" target="#b1">[2]</ref>.</p><p>Table <ref type="table" coords="7,178.68,374.54,4.98,8.85" target="#tab_0">1</ref> reports summary information on the 2009 ad hoc pools used to calculate the results for the main monolingual and bilingual experiments. In particular, for each pool, we show the number of topics, the number of runs submitted, the number of runs included in the pool, the number of documents in the pool (relevant and non-relevant), and the number of assessors.</p><p>The box plot of Figure <ref type="figure" coords="7,247.46,436.58,4.98,8.85" target="#fig_2">3</ref> compares the distributions of the relevant documents across the topics of each pool for the different ad hoc pools; the boxes are ordered by decreasing mean number of relevant documents per topic.</p><p>As can be noted, TEL French and German distributions appear similar and are slightly asymmetric towards topics with a greater number of relevant documents while the TEL English distribution is slightly asymmetric towards topics with a lower number of relevant documents. All the distributions show some upper outliers, i.e. topics with a greater number of relevant document with respect to the behaviour of the other topics in the distribution. These outliers are probably due to the fact that CLEF topics have to be able to retrieve relevant documents in all the collections; therefore, they may be considerably broader in one collection compared with others depending on the contents of the separate datasets.</p><p>For the TEL documents, we judged for relevance only those documents that are written totally or partially in English, French and German, e.g. a catalog record written entirely in Hungarian was counted as not relevant as it was of no use to our hypothetical user; however, a catalog record with perhaps the title and a brief description in Hungarian, but with subject descriptors in French, German or English was judged for relevance as it could be potentially useful. Our assessors  had no additional knowledge of the documents referred to by the catalog records (or surrogates) contained in the collection. They judged for relevance on the information contained in the records made available to the systems. This was a non trivial task due to the lack of information present in the documents. During the relevance assessment activity there was much consultation between the assessors for the three TEL collections in order to ensure that the same assessment criteria were adopted by everyone.</p><p>As shown in the box plot of Figure <ref type="figure" coords="10,313.94,202.82,3.90,8.85" target="#fig_2">3</ref>, the Persian distribution presents a greater number of relevant documents per topic with respect to the other distributions and is slightly asymmetric towards topics with a number of relevant documents. In addition, as can be seen from Table <ref type="table" coords="10,352.34,238.70,3.90,8.85" target="#tab_0">1</ref>, it has been possible to sample all the experiments submitted for the Persian tasks. This means that there were fewer unique documents per run and this fact, together with the greater number of relevant documents per topic suggests either that all the systems were using similar approaches and retrieval algorithms or that the systems found the Persian topics quite easy.</p><p>The relevance assessment for the Persian results was done by the DBRG group in Tehran. Again, assessment was performed on a binary basis and the standard CLEF assessment rules were applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Result Calculation</head><p>Evaluation campaigns such as TREC and CLEF are based on the belief that the effectiveness of Information Retrieval Systems (IRSs) can be objectively evaluated by an analysis of a representative set of sample search results. For this, effectiveness measures are calculated based on the results submitted by the participants and the relevance assessments. Popular measures usually adopted for exercises of this type are Recall and Precision. Details on how they are calculated for CLEF are given in <ref type="bibr" coords="10,281.46,455.66,9.91,8.85" target="#b3">[4]</ref>.</p><p>The individual results for all official Ad-hoc TEL and Persian experiments in CLEF 2009 are given in the Appendices of the CLEF 2009 Working Notes <ref type="bibr" coords="10,459.63,479.66,10.51,8.85" target="#b5">[6,</ref><ref type="bibr" coords="10,470.14,479.66,7.01,8.85" target="#b6">7]</ref>. You can also access them online at:</p><p>-Ad-hoc TEL:</p><p>• monolingual English: http://direct.dei.unipd.it/DOIResolver.do? type=task&amp;id=AH-TEL-MONO-EN-CLEF2009  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Participants and Experiments</head><p>As shown in Table <ref type="table" coords="11,220.82,493.94,3.90,8.85" target="#tab_2">2</ref>, a total of 13 groups from 10 countries submitted official results for the TEL task, while just four groups participated in the Persian task. A total of 231 runs were submitted with an average number of submitted runs per participant of 13.5 runs/participant.</p><p>Participants were required to submit at least one title+description ("TD") run per task in order to increase comparability between experiments. The large majority of runs (216 out of 231, 93.50%) used this combination of topic fields, 2 (0.80%) used all fields<ref type="foot" coords="11,230.64,575.65,3.97,6.97" target="#foot_6">7</ref> , 13 (5.6%) used the title field. All the experiments were conducted using automatic query construction. A breakdown into the separate tasks and topic languages is shown in Table <ref type="table" coords="11,329.02,601.58,3.90,8.85" target="#tab_3">3</ref>.</p><p>Seven different topic languages were used in the ad hoc experiments. As always, the most popular language for queries was English, with German second. However, it must be noted that English topics were provided for both the TEL and the Persian tasks. It is thus hardly surprising that English is the most used language in which to formulate queries. On the other hand, if we look only at the bilingual tasks, the most used source languages were German and French.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TEL@CLEF</head><p>The objective of this activity was to search and retrieve relevant items from collections of library catalog cards. The underlying aim was to identify the most effective retrieval technologies for searching this type of very sparse data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Tasks</head><p>Two subtasks were offered: Monolingual and Bilingual. In both tasks, the aim was to retrieve documents relevant to the query. By monolingual we mean that the query is in the same language as the expected language of the collection. By bilingual we mean that the query is in a different language to the expected language of the collection. For example, in an EN → FR run, relevant documents (bibliographic records) could be any document in the BNF collection (referred to as the French collection) in whatever language they are written. The same is true for a monolingual FR → FR run -relevant documents from the BNF collection could actually also be in English or German, not just French. Ten of the thirteen participating groups attempted a cross-language task; the most popular being with the British Library as the target collection. Six groups submitted experiments for all six possible official cross-language combinations. In addition, we had runs submitted to the English target with queries in Greek, Chinese and Italian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monolingual Results</head><p>Table <ref type="table" coords="12,162.84,644.30,4.98,8.85" target="#tab_4">4</ref> shows the top five groups for each target collection, ordered by mean average precision. The table reports: the short name of the participating group; the mean average precision achieved by the experiment; the DOI of the experiment; and the performance difference between the first and the last participant. Figures <ref type="figure" coords="13,169.59,402.62,7.81,8.85">4,</ref><ref type="figure" coords="13,180.15,402.62,3.90,8.85">6</ref>, and 8 compare the performances of the top participants of the TEL Monolingual tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bilingual Results</head><p>Table <ref type="table" coords="13,162.85,463.94,4.98,8.85" target="#tab_6">5</ref> shows the top five groups for each target collection, ordered by mean average precision. The table reports: the short name of the participating group; the mean average precision achieved by the experiment; the DOI of the experiment; and the performance difference between the first and the last participant. Figures <ref type="figure" coords="13,169.59,511.70,7.81,8.85">5,</ref><ref type="figure" coords="13,180.15,511.70,3.90,8.85">7</ref>, and 9 compare the performances of the top participants of the TEL Bilingual tasks. For bilingual retrieval evaluation, a common method is to compare results against monolingual baselines. We have the following results for CLEF 2009:</p><p>-X → EN: 99.07% of best monolingual English IR system; -X → FR: 94.00% of best monolingual French IR system; -X → DE: 90.06% of best monolingual German IR system. These figures are very encouraging, especially when compared with the results for last year for the same TEL tasks:</p><p>-X → EN: 90.99% of best monolingual English IR system; -X → FR: 56.63% of best monolingual French IR system;   In particular, it can be seen that there is a considerable improvement in performance for French and German This will be commented in the following section.</p><p>The monolingual performance figures for all three tasks are quite similar to those of last year but as these are not absolute values, no real conclusion can be drawn from this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Approaches</head><p>As stated in the introduction, the TEL task this year is a repetition of the task set last year. A main reason for this was to create a good reusable test collection with a sufficient number of topics; another reason was to see whether the experience gained and reported in the literature last year, and the opportunity to use last year's test collection as training data, would lead to differences in approaches and/or improvements in performance this year. Although we have exactly the same number of participants this year as last year, only five of the thirteen 2009 participants also participated in 2008. These are the groups tagged as Chemnitz, Cheshire, Karlsruhe, INESC-ID and Opentext. The last two of these groups only tackled monolingual tasks. These groups all tend to appear in the top five for the various tasks. In the following we attempt to examine briefly the approaches adopted this year, focusing mainly on the cross-language experiments.</p><p>In the TEL task in CLEF 2008, we noted that all the traditional approaches to monolingual and cross language retrieval were attempted by the different groups. Retrieval methods included language models, vector-space and probabilistic approaches, and translation resources ranged from bilingual dictionaries, parallel and comparable corpora to on-line MT systems and Wikipedia. Groups often used a combination of more than one resource. What is immediately noticeable in 2009 is that, although similarly to last year a number of different retrieval models were tested, there is a far more uniform approach to the translation problem.</p><p>Five of the ten groups that attempted cross-language tasks used the Google Translate functionality, while a sixth used the LEC Power Translator <ref type="bibr" coords="18,442.43,218.42,14.60,8.85" target="#b12">[13]</ref>. Another group also used an MT system combining it with concept-based techniques but did not disclose the name of the MT system used <ref type="bibr" coords="18,370.12,242.30,14.65,8.85" target="#b15">[16]</ref>. The remaining three groups used a bilingual term list <ref type="bibr" coords="18,273.38,254.30,14.61,8.85" target="#b16">[17]</ref>, a combination of resources including on-line and in house developed dictionaries <ref type="bibr" coords="18,291.02,266.30,14.61,8.85" target="#b18">[19]</ref>, and Wikipedia translation links <ref type="bibr" coords="18,452.00,266.30,14.61,8.85" target="#b17">[18]</ref>. It is important to note that four out of the five groups in the bilingual to English and bilingual to French tasks and three out of five for the bilingual to German task used Google Translate, either on its own or in combination with another technique. One group noted that topic translation using a statistical MT system resulted in about 70% of the mean average precision (MAP) achieved when using Google Translate <ref type="bibr" coords="18,238.04,337.94,14.60,8.85" target="#b19">[20]</ref>. Another group <ref type="bibr" coords="18,326.51,337.94,15.48,8.85" target="#b10">[11]</ref> found that the results obtained by simply translating the query into all the target languages via Google gave results that were comparable to a far more complex strategy known as Cross-Language Explicit Semantic Analysis, CL-ESA, where the library catalog records and the queries are represented in a multilingual concept space that is spanned by aligned Wikipedia articles. As this year's results were significantly better than last year's, can we take this as meaning that Google is going to solve the cross-language translation resource quandary? Taking a closer look at three groups that did consistently well in the crosslanguage tasks we find the following. The group that had the top result for each of the three tasks was Chemnitz <ref type="bibr" coords="18,311.29,461.30,14.59,8.85" target="#b14">[15]</ref>. They also had consistently good monolingual results. Not surprisingly, they appear to have a very strong IR engine, which uses various retrieval models and combines the results. They used Snowball stemmers for English and French and an n-gram stemmer for German. They were one of the few groups that tried to address the multilinguality of the target collections. They used the Google service to translate the topic from the source language to the four most common languages in the target collections, queried the four indexes and combined the results in a multilingual result set. They found that their approach combining multiple indexed collections worked quite well for French and German but was disappointing for English.</p><p>Another group with good performance, Karlsruhe <ref type="bibr" coords="18,378.01,584.54,14.64,8.85" target="#b15">[16]</ref>, also attempted to tackle the multilinguality of the collections. Their approach was again based on multiple indexes for different languages with rank aggregation to combine the different partial results. They ran language detectors on the collections to identify the different languages contained and translated the topics to the languages recognized. They used Snowball stemmers to stem terms in ten main languages, fields in other languages were not preprocessed. Disappointingly, a baseline con-sisting of a single index without language classification and a topic translated only to the index language achieved similar or even better results. For the translation step, they combined MT with a concept-based retrieval strategy based on Explicit Semantic Analysis and using the Wikipedia database in English, French and German as concept space.</p><p>A third group that had quite good cross-language results for all three collections was Trinity <ref type="bibr" coords="19,222.87,191.30,14.61,8.85" target="#b11">[12]</ref>. However, their monolingual results were not so strong. They used a language modelling retrieval paradigm together with a document re-ranking method which they tried experimentally in the cross-language context. Significantly, they also used Google Translate. Judging from the fact that they did not do so well in the monolingual tasks, this seems to be the probable secret of their success for cross-language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Persian@CLEF</head><p>This activity was again coordinated in collaboration with the Data Base Research Group (DBRG) of Tehran University.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tasks</head><p>The activity was organised as a typical ad hoc text retrieval task on newspaper collections. Two tasks were offered: monolingual retrieval; cross-language retrieval (English queries to Persian target) and 50 topics were prepared (see section 2.2). For each topic, participants had to find relevant documents in the collection and submit the results in a ranked list.</p><p>Table <ref type="table" coords="19,177.36,438.14,4.98,8.85" target="#tab_3">3</ref> provides a breakdown of the number of participants and submitted runs by task and topic language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Table <ref type="table" coords="19,161.64,504.62,4.98,8.85" target="#tab_8">6</ref> shows the top five groups for each target collection, ordered by mean average precision. The table reports: the short name of the participating group; the mean average precision achieved by the experiment; the DOI of the experiment; and the performance difference between the first and the last participant.</p><p>Figures <ref type="figure" coords="19,185.54,552.86,10.02,8.85" target="#fig_0">10</ref> and<ref type="figure" coords="19,219.03,552.86,10.01,8.85" target="#fig_4">11</ref> compare the performances of the top participants of the Persian tasks.</p><p>For bilingual retrieval evaluation, a common method is to compare results against monolingual baselines. We have the following results for CLEF 2009:</p><p>-X → FA: 5.50% of best monolingual Farsi IR system. This appears to be a very clear indication that something went wrong with the bilingual system that has been developed. These results should probably be discounted.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Approaches</head><p>We were very disappointed this year that despite the fact that 14 groups registered for the Persian task, only four actually submitted results. And only one of these groups was from Iran. We suspect that one of the reasons for this was that the date for submission of results was not very convenient for the Iranian groups. Furthermore, only one group <ref type="bibr" coords="21,298.22,380.06,15.49,8.85" target="#b17">[18]</ref> attempted the bilingual task with the very poor results cited above. The technique they used was the same as that adopted for their bilingual to English experiments, exploiting Wikipedia translation links, and the reason they give for the very poor performance here is that the coverage of Farsi in Wikipedia is still very scarce compared to that of many other languages.</p><p>In the monolingual Persian task, the top two groups had very similar performance figures. <ref type="bibr" coords="21,212.86,464.18,15.49,8.85" target="#b20">[21]</ref> found they had best results using a light suffix-stripping algorithm and by combining different indexing and searching strategies. Interestingly, their results this year do not confirm their findings for the same task last year when the use of stemming did not prove very effective. The other group <ref type="bibr" coords="21,163.66,512.06,15.48,8.85" target="#b13">[14]</ref> tested variants of character n-gram tokenization; 4-grams, 5-grams, and skipgrams all provided about a 10% relative gain over plain words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In CLEF 2009 we deliberately repeated the TEL and Persian tasks offered in 2008 in order to build up our test collections. Although we have not yet had sufficient time to assess them in depth, we are reasonably happy with the results for the TEL task: several groups worked on tackling the particular features of the TEL collections with varying success; evidence has been acquired on the effectiveness of a number of different IR strategies; there is a very strong indication of the validity of the Google Translate functionality.</p><p>On the other hand, the results for the Persian task were quite disappointing: very few groups participated; the results obtained are either in contradiction to those obtained previously and thus need further investigation <ref type="bibr" coords="22,405.05,143.06,15.49,8.85" target="#b20">[21]</ref> or tend to be a very straightforward repetition and confirmation of last year's results <ref type="bibr" coords="22,448.64,154.94,14.60,8.85" target="#b13">[14]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,147.60,480.69,319.37,8.16"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of TEL topic http://direct.dei.unipd.it/10.2452/711-AH.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,142.20,337.89,330.29,8.16"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example of Persian topic http://direct.dei.unipd.it/10.2452/641-AH.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,164.16,635.25,286.92,7.96"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Distribution of the relevant documents across the ad-hoc pools.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="16,256.44,725.01,102.39,7.96"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Bilingual German</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="20,255.12,727.77,104.92,7.96"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Bilingual Persian</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,144.48,127.29,311.76,521.92"><head>Table 1 .</head><label>1</label><figDesc>Summary information about CLEF 2009 pools. Box Plot of the Relevant Documents by Topic</figDesc><table coords="8,144.48,148.08,311.76,501.14"><row><cell cols="2">TEL English Pool (DOI 10.2454/AH-TEL-ENGLISH-CLEF2009)</cell></row><row><cell></cell><cell>26,190 pooled documents</cell></row><row><cell>Pool size</cell><cell>-23,663 not relevant documents -2,527 relevant documents</cell></row><row><cell></cell><cell>50 topics</cell></row><row><cell></cell><cell>31 out of 89 submitted experiments</cell></row><row><cell>Pooled Experiments</cell><cell>-monolingual: 22 out of 43 submitted experiments</cell></row><row><cell></cell><cell>-bilingual: 9 out of 46 submitted experiments</cell></row><row><cell>Assessors</cell><cell>4 assessors</cell></row><row><cell cols="2">TEL French Pool (DOI 10.2454/AH-TEL-FRENCH-CLEF2009)</cell></row><row><cell></cell><cell>21,971 pooled documents</cell></row><row><cell>Pool size</cell><cell>-20,118 not relevant documents -1,853 relevant documents</cell></row><row><cell></cell><cell>50 topics</cell></row><row><cell></cell><cell>21 out of 61 submitted experiments</cell></row><row><cell>Pooled Experiments</cell><cell></cell></row><row><cell></cell><cell>25,541 pooled documents</cell></row><row><cell>Pool size</cell><cell>-23,882 not relevant documents -1,559 relevant documents</cell></row><row><cell></cell><cell>50 topics</cell></row><row><cell></cell><cell>21 out of 61 submitted experiments</cell></row><row><cell>Pooled Experiments</cell><cell>-monolingual: 16 out of 35 submitted experiments</cell></row><row><cell></cell><cell>-bilingual: 5 out of 26 submitted experiments</cell></row><row><cell>Assessors</cell><cell>2 assessors</cell></row><row><cell cols="2">Persian Pool (DOI 10.2454/AH-PERSIAN-CLEF2009)</cell></row><row><cell></cell><cell>23,536 pooled documents</cell></row><row><cell>Pool size</cell><cell>-19,072 not relevant documents -4,464 relevant documents</cell></row><row><cell></cell><cell>50 topics</cell></row><row><cell></cell><cell>20 out of 20 submitted experiments</cell></row><row><cell>Pooled Experiments</cell><cell>-monolingual: 17 out of 17 submitted experiments</cell></row><row><cell></cell><cell>-bilingual: 3 out of 3 submitted experiments</cell></row><row><cell>Assessors</cell><cell>23 assessors</cell></row></table><note coords="8,247.68,360.93,208.56,7.96;8,247.68,371.85,189.58,7.96;8,144.60,387.12,43.56,7.89;8,240.96,387.69,38.87,7.96;8,192.24,401.76,228.30,7.89"><p>monolingual: 16 out of 35 submitted experiments bilingual: 5 out of 26 submitted experiments Assessors 1 assessor TEL German Pool (DOI 10.2454/AH-TEL-GERMAN-CLEF2009)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="10,158.64,546.53,336.31,117.48"><head></head><label></label><figDesc>• bilingual English: http://direct.dei.unipd.it/DOIResolver.do?type= task&amp;id=AH-TEL-BILI-X2EN-CLEF2009 • monolingual French: http://direct.dei.unipd.it/DOIResolver.do? type=task&amp;id=AH-TEL-MONO-FR-CLEF2009 • bilingual French: http://direct.dei.unipd.it/DOIResolver.do?type= task&amp;id=AH-TEL-BILI-X2FR-CLEF2009 • monolingual German: http://direct.dei.unipd.it/DOIResolver.do?</figDesc><table coords="10,158.64,631.52,336.31,32.50"><row><cell>type=task&amp;id=AH-TEL-MONO-DE-CLEF2009</cell></row><row><cell>• bilingual German: http://direct.dei.unipd.it/DOIResolver.do?type=</cell></row><row><cell>task&amp;id=AH-TEL-BILI-X2DE-CLEF2009</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,141.00,116.01,355.75,336.92"><head>Table 2 .</head><label>2</label><figDesc>CLEF 2009 Ad hoc participants.</figDesc><table coords="11,176.52,136.80,259.36,231.14"><row><cell></cell><cell>Ad hoc TEL participants</cell><cell></cell></row><row><cell>Participant</cell><cell>Institution</cell><cell>Country</cell></row><row><cell>aeb</cell><cell cols="2">Athens Univ. Economics &amp; Business Greece</cell></row><row><cell>celi</cell><cell>CELI Research srl</cell><cell>Italy</cell></row><row><cell>chemnitz</cell><cell cols="2">Chemnitz University of Technology Germany</cell></row><row><cell>cheshire</cell><cell>U.C.Berkeley</cell><cell>United States</cell></row><row><cell>cuza</cell><cell>Alexandru Ioan Cuza University</cell><cell>Romania</cell></row><row><cell>hit</cell><cell cols="2">HIT2Lab, Heilongjiang Inst. Tech. China</cell></row><row><cell>inesc</cell><cell>Tech. Univ. Lisbon</cell><cell>Portugal</cell></row><row><cell>karlsruhe</cell><cell>Univ. Karlsruhe</cell><cell>Germany</cell></row><row><cell>opentext</cell><cell>OpenText Corp.</cell><cell>Canada</cell></row><row><cell>qazviniau</cell><cell>Islamic Azaz Univ. Qazvin</cell><cell>Iran</cell></row><row><cell>trinity</cell><cell>Trinity Coll. Dublin</cell><cell>Ireland</cell></row><row><cell cols="2">trinity-dcu Trinity Coll. &amp; DCU</cell><cell>Ireland</cell></row><row><cell>weimar</cell><cell>Bauhaus Univ. Weimar</cell><cell>Germany</cell></row><row><cell></cell><cell>Ad hoc Persian participants</cell><cell></cell></row><row><cell>Participant</cell><cell>Institution</cell><cell>Country</cell></row><row><cell>jhu-apl</cell><cell>Johns Hopkins Univ.</cell><cell>USA</cell></row><row><cell>opentext</cell><cell>OpenText Corp.</cell><cell>Canada</cell></row><row><cell>qazviniau</cell><cell>Islamic Azaz Univ. Qazvin</cell><cell>Iran</cell></row><row><cell>unine</cell><cell>U.Neuchatel-Informatics</cell><cell>Switzerland</cell></row></table><note coords="11,141.00,395.66,80.66,8.85;11,158.64,407.09,338.11,9.96;11,168.60,420.31,185.83,8.62;11,158.64,430.97,336.31,9.96;11,168.60,444.31,196.27,8.62"><p><p>-Ad-hoc Persian:</p>• monolingual Farsi: http://direct.dei.unipd.it/DOIResolver.do?type= task&amp;id=AH-PERSIAN-MONO-FA-CLEF2009 • bilingual German: http://direct.dei.unipd.it/DOIResolver.do?type= task&amp;id=AH-PERSIAN-BILI-X2FA-CLEF2009</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="12,104.28,116.01,405.11,146.32"><head>Table 3 .</head><label>3</label><figDesc>Number of experiments by task and topic language and number of participants per task.</figDesc><table coords="12,104.28,147.84,405.11,114.50"><row><cell>Task</cell><cell cols="9">Chinese English Farsi French German Greek Italian Total Participants</cell></row><row><cell>TEL Mono English</cell><cell>-</cell><cell>46</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>46</cell><cell>12</cell></row><row><cell>TEL Mono French</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>35</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>35</cell><cell>9</cell></row><row><cell>TEL Mono German</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>35</cell><cell>-</cell><cell>-</cell><cell>35</cell><cell>9</cell></row><row><cell>TEL Bili English</cell><cell>3</cell><cell>0</cell><cell>0</cell><cell>15</cell><cell>19</cell><cell>5</cell><cell>1</cell><cell>43</cell><cell>10</cell></row><row><cell>TEL Bili French</cell><cell>0</cell><cell>12</cell><cell>0</cell><cell>0</cell><cell>12</cell><cell>0</cell><cell>2</cell><cell>26</cell><cell>6</cell></row><row><cell>TEL Bili German</cell><cell>1</cell><cell>12</cell><cell>0</cell><cell>12</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>26</cell><cell>6</cell></row><row><cell>Mono Persian</cell><cell>-</cell><cell>-</cell><cell>17</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>17</cell><cell>4</cell></row><row><cell>Bili Persian</cell><cell>-</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3</cell><cell>1</cell></row><row><cell>Total</cell><cell>4</cell><cell>73</cell><cell>17</cell><cell>62</cell><cell>66</cell><cell>5</cell><cell cols="2">4 231</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="13,135.96,116.01,329.15,233.20"><head>Table 4 .</head><label>4</label><figDesc>Best entries for the monolingual TEL tasks.</figDesc><table coords="13,135.96,136.80,329.15,212.42"><row><cell cols="2">Track Rank Participant</cell><cell>Experiment DOI</cell><cell>MAP</cell></row><row><cell></cell><cell>1st inesc</cell><cell>10.2415/AH-TEL-MONO-EN-CLEF2009.INESC.RUN11</cell><cell>40.84%</cell></row><row><cell></cell><cell>2nd chemnitz</cell><cell cols="2">10.2415/AH-TEL-MONO-EN-CLEF2009.CHEMNITZ.CUT 11 MONO MERGED EN 9 10 40.71%</cell></row><row><cell>English</cell><cell>3rd trinity 4th hit</cell><cell>10.2415/AH-TEL-MONO-EN-CLEF2009.TRINITY.TCDENRUN2 10.2415/AH-TEL-MONO-EN-CLEF2009.HIT.MTDD10T40</cell><cell>40.35% 39.36%</cell></row><row><cell></cell><cell cols="2">5th trinity-dcu 10.2415/AH-TEL-MONO-EN-CLEF2009.TRINITY-DCU.TCDDCUEN3</cell><cell>36.96%</cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell>10.50%</cell></row><row><cell></cell><cell>1st karlsruhe</cell><cell>10.2415/AH-TEL-MONO-FR-CLEF2009.KARLSRUHE.INDEXBL</cell><cell>27.20%</cell></row><row><cell></cell><cell>2nd chemnitz</cell><cell cols="2">10.2415/AH-TEL-MONO-FR-CLEF2009.CHEMNITZ.CUT 19 MONO MERGED FR 17 18 25.83%</cell></row><row><cell>French</cell><cell>3rd inesc 4th opentext</cell><cell>10.2415/AH-TEL-MONO-FR-CLEF2009.INESC.RUN12 10.2415/AH-TEL-MONO-FR-CLEF2009.OPENTEXT.OTFR09TDE</cell><cell>25.11% 24.12%</cell></row><row><cell></cell><cell>5th celi</cell><cell>10.2415/AH-TEL-MONO-FR-CLEF2009.CELI.CACAO FRBNF ML</cell><cell>23.61%</cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell>15.20%</cell></row><row><cell></cell><cell>1st opentext</cell><cell>10.2415/AH-TEL-MONO-DE-CLEF2009.OPENTEXT.OTDE09TDE</cell><cell>28.68%</cell></row><row><cell></cell><cell>2nd chemnitz</cell><cell>10.2415/AH-TEL-MONO-DE-CLEF2009.CHEMNITZ.CUT 3 MONO MERGED DE 1 2</cell><cell>27.89%</cell></row><row><cell>German</cell><cell cols="2">3rd inesc 4th trinity-dcu 10.2415/AH-TEL-MONO-DE-CLEF2009.TRINITY-DCU.TCDDCUDE3 10.2415/AH-TEL-MONO-DE-CLEF2009.INESC.RUN12</cell><cell>27.85% 26.86%</cell></row><row><cell></cell><cell>5th trinity</cell><cell>10.2415/AH-TEL-MONO-DE-CLEF2009.TRINITY.TCDDERUN1</cell><cell>25.77%</cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell>11.30%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="17,124.08,116.01,384.47,270.68"><head>Table 5 .</head><label>5</label><figDesc>Best entries for the bilingual TEL tasks.</figDesc><table coords="17,124.08,136.80,384.47,212.42"><row><cell cols="2">Track Rank Participant</cell><cell>Experiment DOI</cell><cell>MAP</cell></row><row><cell></cell><cell>1st chemnitz</cell><cell>10.2415/AH-TEL-BILI-X2EN-CLEF2009.CHEMNITZ.CUT 13 BILI MERGED DE2EN 9 10</cell><cell>40.46%</cell></row><row><cell></cell><cell>2nd hit</cell><cell>10.2415/AH-TEL-BILI-X2EN-CLEF2009.HIT.XTDD10T40</cell><cell>35.27%</cell></row><row><cell>English</cell><cell cols="2">3rd trinity 4th trinity-dcu 10.2415/AH-TEL-BILI-X2EN-CLEF2009.TRINITY-DCU.TCDDCUDEEN1 10.2415/AH-TEL-BILI-X2EN-CLEF2009.TRINITY.TCDDEENRUN3</cell><cell>35.05% 33.33%</cell></row><row><cell></cell><cell>5th karlsrhue</cell><cell>10.2415/AH-TEL-BILI-X2EN-CLEF2009.KARLSRUHE.DE INDEXBL</cell><cell>32.70%</cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell>23.73%</cell></row><row><cell></cell><cell>1st chemnitz</cell><cell cols="2">10.2415/AH-TEL-BILI-X2FR-CLEF2009.CHEMNITZ.CUT 24 BILI EN2FR MERGED LANG SPEC REF CUT 17 25.57%</cell></row><row><cell></cell><cell>2nd karlsrhue</cell><cell>10.2415/AH-TEL-BILI-X2FR-CLEF2009.KARLSRUHE.EN INDEXBL</cell><cell>24.62%</cell></row><row><cell>French</cell><cell>3rd chesire 4th trinity</cell><cell>10.2415/AH-TEL-BILI-X2FR-CLEF2009.CHESHIRE.BIENFRT2FB 10.2415/AH-TEL-BILI-X2FR-CLEF2009.TRINITY.TCDDEFRRUN2</cell><cell>16.77% 16.33%</cell></row><row><cell></cell><cell>5th weimar</cell><cell>10.2415/AH-TEL-BILI-X2FR-CLEF2009.WEIMAR.CLESA169283ENINFR</cell><cell>14.51%</cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell>69.67%</cell></row><row><cell></cell><cell>1st chemnitz</cell><cell>10.2415/AH-TEL-BILI-X2DE-CLEF2009.CHEMNITZ.CUT 5 BILI MERGED EN2DE 1 2</cell><cell>25.83%</cell></row><row><cell></cell><cell>2nd trinity</cell><cell>10.2415/AH-TEL-BILI-X2DE-CLEF2009.TRINITY.TCDENDERUN3</cell><cell>19.35%</cell></row><row><cell>German</cell><cell>3rd karlsrhue 4th weimar</cell><cell>10.2415/AH-TEL-BILI-X2DE-CLEF2009.KARLSRUHE.EN INDEXBL 10.2415/AH-TEL-BILI-X2DE-CLEF2009.WEIMAR.COMBINEDFRINDE</cell><cell>16.46% 15.75%</cell></row><row><cell></cell><cell>5th chesire</cell><cell>10.2415/AH-TEL-BILI-X2DE-CLEF2009.CHESHIRE.BIENDET2FBX</cell><cell>11.50%</cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell>124.60%</cell></row></table><note coords="17,141.00,376.73,263.79,9.96"><p>-X → DE: 53.15% of best monolingual German IR system.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="20,139.61,119.55,367.49,590.26"><head></head><label></label><figDesc>Hoc TEL Monolingual Persian Task Top 5 Participants -Standard Recall Levels vs Mean Interpolated Precision jhu-apl [Experiment JHUFASK41R400TD; MAP 49.38%; Pooled] unine [Experiment UNINEPE4; MAP 49.37%; Pooled] opentext [Experiment OTFA09TDE; MAP 39.53%; Pooled] qazviniau [Experiment IAUPERFA3; MAP 37.62%; Pooled] Hoc TEL Bilingual Persian Task Top 5 Participants -Standard Recall Levels vs Mean Interpolated Precision qazviniau [Experiment IAUPEREN3; MAP 2.72%; Pooled]</figDesc><table coords="20,139.61,119.55,352.12,590.26"><row><cell>Precision</cell><cell cols="7">0% 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Ad-Fig. 10. Monolingual Persian 10% 20% 30% 40% 50% 60% Recall</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell><cell>100%</cell></row><row><cell></cell><cell>Ad-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>100%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>90%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>80%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>60%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell>50%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>40%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0% 0%</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell><cell>100%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="21,135.48,116.01,330.17,163.24"><head>Table 6 .</head><label>6</label><figDesc>Best entries for the Persian tasks.</figDesc><table coords="21,135.48,135.12,330.17,144.14"><row><cell>Track</cell><cell>Rank Participant</cell><cell>Experiment DOI</cell><cell>MAP</cell></row><row><cell></cell><cell>1st jhu-apl</cell><cell cols="2">10.2415/AH-PERSIAN-MONO-FA-CLEF2009.JHU-APL.JHUFASK41R400TD 49.38%</cell></row><row><cell></cell><cell>2nd unine</cell><cell>10.2415/AH-PERSIAN-MONO-FA-CLEF2009.UNINE.UNINEPE4</cell><cell>49.37%</cell></row><row><cell>Monolingual</cell><cell>3rd opentext 4th qazviniau</cell><cell>10.2415/AH-PERSIAN-MONO-FA-CLEF2009.OPENTEXT.OTFA09TDE 10.2415/AH-PERSIAN-MONO-FA-CLEF2009.QAZVINIAU.IAUPERFA3</cell><cell>39.53% 37.62%</cell></row><row><cell></cell><cell>5th -</cell><cell>-</cell><cell>-%</cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell>31.25%</cell></row><row><cell></cell><cell>1st qazviniau</cell><cell>10.2415/AH-PERSIAN-BILI-X2FA-CLEF2009.QAZVINIAU.IAUPEREN3</cell><cell>2.72%</cell></row><row><cell></cell><cell>2nd -</cell><cell>-</cell><cell>-</cell></row><row><cell>Bilingual</cell><cell>3rd -4th -</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell>5th -</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Difference</cell><cell></cell><cell>-</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,144.72,635.01,335.74,7.96;1,144.72,645.93,219.50,7.96"><p>Over the years, this track has built up test collections for monolingual and cross language system evaluation in 14 European languages.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,144.72,656.85,175.72,8.16"><p>See http://www.theeuropeanlibrary.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="2,144.72,645.93,335.87,7.96;2,144.72,656.85,282.22,7.96"><p>As the task design was the same as last year, much of the task set-up section is a repetition of a similar section in our CLEF 2008 working notes paper.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,144.72,645.93,271.01,8.16"><p>For more information, see http://ece.ut.ac.ir/dbrg/hamshahri/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4" coords="4,144.72,657.45,88.96,7.57"><p>http://www.doi.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5" coords="6,144.72,635.01,335.78,7.96;6,144.72,645.93,335.71,7.96;6,144.72,656.85,192.25,7.96"><p>Tests made on NTCIR pools in previous years have suggested that a depth of 60 in normally adequate to create stable pools, presuming that a sufficient number of runs from different systems have been included.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6" coords="11,144.72,656.85,228.28,7.96"><p>The narrative field was only offered for the Persian task.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgements</head><p>The TEL task was studied in order to provide useful input to The European Library (TEL); we express our gratitude in particular to <rs type="person">Jill Cousins</rs>, Programme Director, and <rs type="person">Sjoerd Siebinga</rs>, Technical Developer of <rs type="affiliation">TEL</rs>. <rs type="person">Vivien Petras</rs>, <rs type="affiliation">Humboldt University, Germany</rs>, and <rs type="person">Nicolas Moreau</rs>, Evaluation and Language Resources <rs type="institution">Distribution Agency, France</rs>, were responsible for the creation of the topics and the supervision of the relevance assessment work for the ONB and BNF data respectively. We thank them for their valuable assistance.</p><p>We should also like to acknowledge the enormous contribution to the coordination of the Persian task made by the <rs type="institution">Data Base Research group of the University of Tehran</rs> and in particular to <rs type="person">Abolfazl AleAhmad</rs> and <rs type="person">Hadi Amiri</rs>. They were responsible for the preparation of the set of topics for the Hamshahri collection in Farsi and English and for the subsequent relevance assessments.</p><p>Least but not last, we would warmly thank <rs type="person">Giorgio Maria Di Nunzio</rs> for all the contributions he gave in carrying out the TEL and Persian tasks.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct coords="22,142.88,425.97,337.72,7.96;22,151.56,436.89,329.06,7.96;22,151.56,447.93,199.26,7.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="22,339.22,425.97,141.39,7.96;22,151.56,436.89,142.08,7.96">The Importance of Scientific Data Curation for Evaluation Campaigns</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Agosti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Di Nunzio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,449.41,436.89,31.22,7.95;22,151.56,447.93,130.51,7.95">DELOS Conference 2007 Working Notes</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Thanos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Borri</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="185" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.88,470.01,337.62,7.96;22,151.56,480.93,328.98,7.96;22,151.56,491.97,328.90,7.96;22,151.56,502.89,328.36,9.84;22,151.56,513.82,54.64,8.16" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="22,268.18,470.01,212.33,7.96;22,151.56,480.93,138.67,7.96">Can we get rid of TREC assessors? Using Mechanical Turk for relevance assessment</title>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mizzaro</surname></persName>
		</author>
		<ptr target="http://staff.science.uva.nl/~kamps/ireval/papers/paper_22.pdf" />
	</analytic>
	<monogr>
		<title level="m" coord="22,306.36,491.97,174.10,7.95;22,151.56,502.89,63.83,7.95">Proc. SIGIR 2009 Workshop on The Future of IR Evaluation</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Geva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Sakai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Trotman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</editor>
		<meeting>SIGIR 2009 Workshop on The Future of IR Evaluation</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.89,524.98,337.44,7.96;22,151.56,536.02,329.02,7.96;22,151.56,546.94,329.05,7.95;22,151.56,557.86,328.86,7.96;22,151.56,568.78,150.73,7.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="22,214.32,524.98,141.73,7.96">CLEF 2002 -Overview of Results</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,296.28,536.02,184.29,7.95;22,151.56,546.94,329.05,7.95;22,151.56,557.86,59.43,7.95">Advances in Cross-Language Information Retrieval: Third Workshop of the Cross-Language Evaluation Forum (CLEF 2002) Revised Papers</title>
		<title level="s" coord="22,271.46,557.86,178.50,7.96">Lecture Notes in Computer Science (LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2785</biblScope>
			<biblScope unit="page" from="9" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.88,579.94,337.54,7.96;22,151.56,590.98,329.11,7.96;22,151.56,601.90,328.98,7.95;22,151.56,612.82,329.02,7.96;22,151.56,623.86,285.20,7.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="22,268.89,579.94,151.53,7.96">CLEF 2003 Methodology and Metrics</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,346.93,590.98,133.74,7.95;22,151.56,601.90,328.98,7.95;22,151.56,612.82,206.91,7.95">Comparative Evaluation of Multilingual Information Access Systems: Fourth Workshop of the Cross-Language Evaluation Forum (CLEF 2003) Revised Selected Papers</title>
		<title level="s" coord="22,414.29,612.82,66.29,7.96;22,151.56,623.86,102.11,7.96">Lecture Notes in Computer Science (LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3237</biblScope>
			<biblScope unit="page" from="7" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="22,142.88,635.02,337.72,7.96;22,151.56,645.94,328.96,7.96;22,151.56,656.86,25.40,7.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="22,241.17,635.02,213.56,7.96">The Cranfield Tests on Index Languages Devices</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">W</forename><surname>Cleverdon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="22,315.73,645.94,135.78,7.95">Readings in Information Retrieval</title>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Spärck</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Willett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="47" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.88,119.73,337.74,7.96;23,151.56,130.77,49.08,7.96" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="23,280.08,119.73,183.72,7.96">Appendix A: Results of the TEL@CLEF Task</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Di Nunzio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="23,142.88,141.69,337.57,7.96;23,151.56,152.61,60.60,7.96" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="23,281.04,141.69,195.11,7.96">Appendix B: Results of the Persian@CLEF Task</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Di Nunzio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="23,142.88,163.65,337.75,7.96;23,151.56,174.58,329.12,7.96;23,151.56,185.50,329.10,7.95;23,151.56,196.54,296.90,7.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="23,264.98,163.65,199.04,7.96">Forming Test Collections with No System Pooling</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Joho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,391.69,174.58,88.99,7.95;23,151.56,185.50,329.10,7.95;23,151.56,196.54,93.08,7.95">Proc. 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2004)</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Bruza</surname></persName>
		</editor>
		<meeting>27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2004)<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="23,142.88,207.46,337.65,7.96;23,151.56,218.38,113.54,7.96" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="23,215.82,207.46,264.70,7.96;23,151.56,218.38,24.61,7.96">German, French, English and Persian Retrieval Experiments at CLEF</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tomlinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="23,142.55,229.42,337.91,7.96;23,151.56,240.34,328.75,7.96;23,151.56,251.26,328.84,7.96;23,151.56,262.30,269.12,7.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="23,211.06,229.42,176.11,7.96">Sampling Precision to Depth 10000 at CLEF</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tomlinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="23,413.22,229.42,67.24,7.96;23,151.56,240.34,328.75,7.96;23,151.56,251.26,229.92,7.96">Systems for Multilingual and Multimodal Information Access: 9th Workshop of the Cross-Language Evaluation Forum (CLEF 2008)</title>
		<title level="s" coord="23,388.81,251.26,91.59,7.96;23,151.56,262.30,83.26,7.96">Lecture Notes in Computer Science (LNCS</title>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008. 2009</date>
			<biblScope unit="volume">5706</biblScope>
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct coords="23,142.56,273.22,337.97,7.96;23,151.56,284.14,270.92,7.96" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="23,295.06,273.22,185.47,7.96;23,151.56,284.14,181.84,7.96">Evaluating Cross-Language Explicit Semantic Analysis and Cross Querying at TEL@CLEF</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Anderka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="23,142.55,295.18,338.12,7.96;23,151.56,306.10,193.21,7.96" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="23,236.18,295.18,244.49,7.96;23,151.56,306.10,122.75,7.96">Language Modeling and Document Re-Ranking: Trinity Experiments at TEL@CLEF-2009</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Wade</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="23,142.56,317.02,337.88,7.96;23,151.56,327.94,19.47,7.96" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="23,211.16,317.02,214.17,7.96">Multilingual Query Expansion for CLEF Adhoc-TEL</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">R</forename><surname>Larson</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="23,142.56,338.98,337.95,7.96;23,151.56,349.90,113.54,7.96" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="23,217.30,338.98,263.21,7.96;23,151.56,349.90,24.61,7.96">JHU Experiments in Monolingual Farsi Document Retrieval at CLEF</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mcnamee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="23,142.56,360.82,337.96,7.96;23,151.56,371.86,279.78,7.96" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="23,210.17,360.82,270.35,7.96;23,151.56,371.86,211.01,7.96">Chemnitz at CLEF 2009 Ad-Hoc TEL Task: Combining Different Retrieval Models and Addressing the Multilinguality</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kuersten</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="23,142.56,382.78,337.96,7.96;23,151.56,393.70,173.15,7.96" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="23,338.60,382.78,141.92,7.96;23,151.56,393.70,104.22,7.96">Cross-lingual Information Retrieval based on Multiple Indexes</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sorg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nicolay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="23,142.56,404.74,337.82,7.96;23,151.56,415.66,271.30,7.96" xml:id="b16">
	<monogr>
		<title level="m" type="main" coord="23,283.19,404.74,197.20,7.96;23,151.56,415.66,202.10,7.96">An Evaluation of Greek-English Cross Language Retrieval within the CLEF Ad-Hoc Bilingual Task</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Katsiouli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Kalamboukis</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="23,142.54,426.58,338.04,7.96;23,151.56,437.62,328.96,7.96;23,151.56,448.54,60.60,7.96" xml:id="b17">
	<monogr>
		<title level="m" type="main" coord="23,291.01,426.58,189.58,7.96;23,151.56,437.62,324.92,7.96">Query Wikification: Mining Structured Queries From Unstructured Information Needs using Wikipedia-based Semantic Analysis</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">H</forename><surname>Jadidinejad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Mahmoudi</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="23,142.56,459.46,338.00,7.96" xml:id="b18">
	<monogr>
		<title level="m" type="main" coord="23,229.92,459.46,184.33,7.96">CACAO Project at the TEL@CLEF 2009 Task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Bosca</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dini</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="23,142.55,470.50,337.75,7.96;23,151.56,481.42,328.99,7.96" xml:id="b19">
	<monogr>
		<title level="m" type="main" coord="23,345.94,470.50,134.36,7.96;23,151.56,481.42,261.11,7.96">TCD-DCU at TEL@CLEF 2009: Document Expansion, Query Translation and Language Modeling</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Wade</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="23,142.55,492.34,338.07,7.96;23,151.56,503.38,123.28,7.96" xml:id="b20">
	<monogr>
		<title level="m" type="main" coord="23,302.24,492.34,178.38,7.96;23,151.56,503.38,54.44,7.96">UniNE at CLEF 2009: Persian Ad Hoc Retrieval and IP</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Dolamic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Fautsch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Savoy</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
