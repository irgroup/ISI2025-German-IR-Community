<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,177.21,115.96,260.94,12.62;1,241.65,133.89,132.06,12.62">CLEF 2009 Ad Hoc Track Overview: Robust-WSD Task</title>
				<funder ref="#_DeyNAaD">
					<orgName type="full">European Commission</orgName>
				</funder>
				<funder ref="#_Dv2x798">
					<orgName type="full">Ministry of Education</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,167.47,171.56,57.47,8.74"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
							<email>e.agirre@ehu.es</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of the Basque Country</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,235.50,171.56,74.99,8.74"><forename type="first">Giorgio</forename><forename type="middle">Maria</forename><surname>Di</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padua</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,355.08,171.56,66.20,8.74"><forename type="first">Thomas</forename><surname>Mandl</surname></persName>
							<email>mandl@uni-hildesheim.de</email>
							<affiliation key="aff2">
								<orgName type="department">Information Science</orgName>
								<orgName type="institution">University of Hildesheim</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,274.02,183.51,62.85,8.74"><forename type="first">Arantxa</forename><surname>Otegi</surname></persName>
							<email>arantza.otegi@ehu.es</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of the Basque Country</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,177.21,115.96,260.94,12.62;1,241.65,133.89,132.06,12.62">CLEF 2009 Ad Hoc Track Overview: Robust-WSD Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">234B40516A24D6B19C76F0F597495D71</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Robust-WSD at CLEF 2009 aims at exploring the contribution of Word Sense Disambiguation to monolingual and multilingual Information Retrieval. The organizers of the task provide documents and topics which have been automatically tagged with Word Senses from WordNet using several state-of-the-art Word Sense Disambiguation systems. The Robust-WSD exercise follows the same design as in 2008. It uses two languages often used in previous CLEF campaigns (English, Spanish). Documents were in English, and topics in both English and Spanish. The document collections are based on the widely used LA94 and GH95 news collections. All instructions and datasets required to replicate the experiment are available from the organizers website (http://ixa2.si.ehu.es/clirwsd/). The results show that some top-scoring systems improve their IR and CLIR results with the use of WSD tags, but the best scoring runs do not use WSD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Robust-WSD task at CLEF 2009 aims at exploring the contribution of Word Sense Disambiguation to monolingual and multilingual Information Retrieval. The organizers of the task provide documents and topics which have been automatically tagged with Word Senses from WordNet using several stateof-the-art Word Sense Disambiguation systems. The task follows the same design as in 2008.</p><p>The robust task ran for the fourth time at CLEF 2009. It is an Ad-Hoc retrieval task based on data of previous CLEF campaigns. The robust task emphasizes the difficult topics by a non-linear integration of the results of individual topics into one result for a system, using the geometric mean of the average precision for all topics (GMAP) as an additional evaluation measure <ref type="bibr" coords="1,420.91,608.30,15.50,8.74" target="#b12">[13,</ref><ref type="bibr" coords="1,436.40,608.30,11.62,8.74" target="#b13">14]</ref>. Given the difficulty of the task, training data including topics and relevance assessments was provided for the participants to tune their systems to the collection.</p><p>For the second year, the robust task also incorporated word sense disambiguation information provided by the organizers to the participants. The task follows the 2007 joint SemEval-CLEF task <ref type="bibr" coords="2,292.88,118.99,10.52,8.74" target="#b1">[2]</ref> and the 2008 Robust-WSD exercise <ref type="bibr" coords="2,467.31,118.99,9.96,8.74" target="#b2">[3]</ref>, and has the aim of exploring the contribution of word sense disambiguation to monolingual and cross-language information retrieval. The goal of the task is to test whether WSD can be used beneficially for retrieval systems, and thus participants were required to submit at least one baseline run without WSD and one run using the WSD annotations. Participants could also submit four further baseline runs without WSD and four runs using WSD.</p><p>The experiment involved both monolingual (topics and documents in English) and bilingual experiments (topics in Spanish and documents in English). In addition to the original documents and topics, the organizers of the task provided both documents and topics which had been automatically tagged with word senses from WordNet version 1.6 using two state-of-the-art word sense disambiguation systems, UBC <ref type="bibr" coords="2,258.39,262.68,10.52,8.74" target="#b0">[1]</ref> and NUS <ref type="bibr" coords="2,316.94,262.68,9.96,8.74" target="#b6">[7]</ref>. These systems provided weighted word sense tags for each of the nouns, verbs, adjectives and adverbs that they could disambiguate.</p><p>In addition, the participants could use publicly available data from the English and Spanish wordnets in order to test different expansion strategies. Note that given the tight alignment of the Spanish and English wordnets, the wordnets could also be used to translate directly from one sense to another, and perform expansion to terms in another language.</p><p>The datasets used in this task can be used in the future to run further experiments. Check http://ixa2.si.ehu.es/clirwsd for information of how to access the datasets. Topics and relevance judgements are freely available. The document collection can be obtained from ELDA purchasing the CLEF Test Suite for the CLEF 2000-2003 Campaigns -Evaluation Package. As an alternative, the website offers the unordered set of words in each document, that is, the full set of documents where the positional information has been eliminated to avoid replications of the originals. Lucene indexes for the later are also available from the website.</p><p>In this paper, we first present the task setup, the evaluation methodology and the participation in the different tasks (Section 2). We then describe the main features of each task and show the results (Sections 3 -5). The final section provides a brief summing up. For information on the various approaches and resources used by the groups participating in this task and the issues they focused on, we refer the reader to the rest of the papers in the Robust-WSD part of the Ad Hoc section of these Proceedings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Setup</head><p>The Ad Hoc task in CLEF adopts a corpus-based, automatic scoring method for the assessment of system performance, based on ideas first introduced in the Cranfield experiments in the late 1960s <ref type="bibr" coords="2,330.99,620.25,9.96,8.74" target="#b7">[8]</ref>. The tasks offered are studied in order to effectively measure textual document retrieval under specific conditions. The test collections are made up of documents, topics and relevance assessments. The topics consist of a set of statements simulating information needs from which the systems derive the queries to search the document collections. Evaluation of system performance is then done by judging the documents retrieved in response to a topic with respect to their relevance, and computing the recall and precision measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Test Collections</head><p>The Documents. The robust task used existing CLEF news collections but with word sense disambiguation (WSD) information added. The word sense disambiguation data was automatically added by systems from two leading research laboratories, UBC <ref type="bibr" coords="3,216.54,238.97,10.52,8.74" target="#b0">[1]</ref> and NUS <ref type="bibr" coords="3,272.56,238.97,9.96,8.74" target="#b6">[7]</ref>. Both systems returned word senses from the English WordNet, version 1.6.</p><p>The document collections were offered both with and without WSD, and included the following The Topics. Topics are structured statements representing information needs. Each topic typically consists of three parts: a brief title statement; a one-sentence description; a more complex narrative the relevance assessment criteria. Topics are prepared in xml format and identified by means of a Digital Object Identifier (DOI)<ref type="foot" coords="3,161.47,416.77,3.97,6.12" target="#foot_1">2</ref> of the experiment <ref type="bibr" coords="3,249.54,418.34,15.50,8.74" target="#b11">[12]</ref> which allows us to reference and cite them.</p><p>The WSD robust task used existing CLEF topics in English and Spanish as follows:</p><p>- All topics were offered both with and without WSD. Topics in English were disambiguated by both UBC <ref type="bibr" coords="3,261.66,626.42,10.52,8.74" target="#b0">[1]</ref> and NUS <ref type="bibr" coords="3,317.39,626.42,10.52,8.74" target="#b6">[7]</ref> systems, yielding word senses from &lt;top&gt; &lt;num&gt;10.2452/141-WSD-AH&lt;/num&gt; &lt;EN-title&gt; &lt;TERM ID="10.2452/141-WSD-AH-1" LEMA="letter" POS="NNP"&gt; &lt;WF&gt;Letter&lt;/WF&gt; &lt;SYNSET SCORE="0" CODE="05115901-n"/&gt; &lt;SYNSET SCORE="0" CODE="05362432-n"/&gt; &lt;SYNSET SCORE="0" CODE="05029514-n"/&gt; &lt;SYNSET SCORE="1" CODE="04968965-n"/&gt; &lt;/TERM&gt; &lt;TERM ID="10.2452/141-WSD-AH-2" LEMA="bomb" POS="NNP"&gt; &lt;WF&gt;Bomb&lt;/WF&gt; &lt;SYNSET SCORE="0.888888888888889" CODE="02310834-n"/&gt; &lt;SYNSET SCORE="0" CODE="05484679-n"/&gt; &lt;SYNSET SCORE="0.111111111111111" CODE="02311368-n"/&gt; &lt;/TERM&gt; &lt;TERM ID="10.2452/141-WSD-AH-3" LEMA="for" POS="IN"&gt; &lt;WF&gt;for&lt;/WF&gt; &lt;/TERM&gt; ... WordNet version 1.6. A large-scale disambiguation system for Spanish was not available, so we used the first-sense heuristic, yielding senses from the Spanish wordnet, which is tightly aligned to the English WordNet version 1.6 (i.e., they share synset numbers or sense codes). An excerpt from a topic is shown in Figure <ref type="figure" coords="4,134.77,548.33,3.87,8.74" target="#fig_1">1</ref>, where each term in the topic is followed by its senses with their respective scores as assigned buy the automatic WSD system<ref type="foot" coords="4,355.60,558.71,3.97,6.12" target="#foot_2">3</ref> .</p><p>Relevance Assessment. The number of documents in large test collections such as CLEF makes it impractical to judge every document for relevance. Instead approximate recall values are calculated using pooling techniques. The robust WSD task used existing relevance assessments from previous years. The relevance assessments regarding the training topics were provided to participants before competition time.</p><p>The total number of assessments was 66,441 documents of which 4,327 were relevant. The distribution of the pool according to each year was the following: </p><formula xml:id="formula_0" coords="5,140.99,175.50,5.73,8.77">-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Result Calculation</head><p>Evaluation campaigns such as TREC and CLEF are based on the belief that the effectiveness of Information Retrieval Systems (IRSs) can be objectively evaluated by an analysis of a representative set of sample search results. For this, effectiveness measures are calculated based on the results submitted by the participants and the relevance assessments. Popular measures usually adopted for exercises of this type are Recall and Precision. Details on how they are calculated for CLEF are given in <ref type="bibr" coords="5,281.35,390.95,9.96,8.74" target="#b5">[6]</ref>.</p><p>The robust task emphasizes the difficult topics by a non-linear integration of the results of individual topics into one result for a system, using the geometric mean of the average precision for all topics (GMAP) as an additional evaluation measure <ref type="bibr" coords="5,173.59,438.92,15.50,8.74" target="#b12">[13,</ref><ref type="bibr" coords="5,189.08,438.92,11.62,8.74" target="#b13">14]</ref>.</p><p>The individual results for all official Ad Hoc experiments in CLEF 2009 are given in the one of the Appendices of the CLEF 2009 Working Notes <ref type="bibr" coords="5,439.46,462.98,9.96,8.74" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Participants and Experiments</head><p>As shown in Table <ref type="table" coords="5,218.73,514.27,3.87,8.74">1</ref>, 10 groups submitted 89 runs for the Robust tasks:</p><p>-8 groups submitted monolingual non-WSD runs (25 runs out of 89); -5 groups also submitted bilingual non-WSD runs (13 runs out of 89).</p><p>All groups submitted WSD runs (51 out of 89 runs):</p><p>-10 groups submitted monolingual WSD runs (33 out of 89 runs) -5 groups submitted bilingual WSD runs (18 out of 89 runs) Table <ref type="table" coords="5,177.35,620.25,4.98,8.74" target="#tab_2">2</ref> provides a breakdown of the number of participants and submitted runs by task. Note that jaen submitted a monolingual non-WSD run as if it was a WSD run, and that alicante missed to send their non-WSD run on time. The figures below are the official figures.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Table <ref type="table" coords="6,163.48,596.34,4.98,8.74" target="#tab_3">3</ref> shows the best results for the monolingual runs, and Table <ref type="table" coords="6,445.68,596.34,4.98,8.74" target="#tab_4">4</ref> shows the best results for the bilingual runs. In the following pages, Figures <ref type="figure" coords="6,446.84,608.30,4.98,8.74" target="#fig_3">2</ref> and<ref type="figure" coords="6,475.61,608.30,4.98,8.74" target="#fig_4">3</ref> compare the performances of the best systems in terms of average precision of the top participants of the Robust Monolingual and Monolingual WSD, and Figures <ref type="figure" coords="6,169.49,644.16,4.98,8.74" target="#fig_5">4</ref> and<ref type="figure" coords="6,195.89,644.16,4.98,8.74" target="#fig_6">5</ref> compare the performances of the best participants of the Robust Bilingual and Bilingual WSD. The comparison of the bilingual runs with respect to the monolingual results yield the following:</p><p>-ES → EN: 85.2% of best monolingual English IR system (MAP); -ES → EN WSD: 83.3% of best monolingual English IR system (MAP);</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Statistical Testing</head><p>When the goal is to validate how well results can be expected to hold beyond a particular set of queries, statistical testing can help to determine what differences between runs appear to be real as opposed to differences that are due to sampling issues. We aim to identify whether the results of the runs of a task are significantly different from the results of other tasks. In particular, we want to test whether there is any difference between applying WSD techniques or not. Significantly different in this context means that the difference between the performance scores for the runs in question appears greater than what might be expected by pure chance. As with all statistical testing, conclusions will be qualified by an error probability, which was chosen to be 0.05 in the following.  We have designed our analysis to follow closely the methodology used by similar analyses carried out for Text REtrieval Conference (TREC) <ref type="bibr" coords="8,399.03,449.34,14.61,8.74">[23]</ref>.</p><p>We used the MATLAB Statistics Toolbox, which provides the necessary functionality plus some additional functions and utilities.</p><p>Two tests for goodness of fit to a normal distribution were chosen using the MATLAB statistical toolbox: the Lilliefors test and the Jarque-Bera test. In the case of the CLEF tasks under analysis, both tests indicate that the assumption of normality is not violated for most of the data samples (in this case the runs for each participant).</p><p>The two tests were:</p><p>-Robust Monolingual vs Robust WSD Monolingual; -Robust Bilingual vs Robust WSD Bilingual.</p><p>In both cases, the t-test confirmed that the mean of the two distributions are different and, in particular, the mean of the monolingual distribution is greater than the mean of the robust monolingual WSD, and the same happens for the bilingual. This suggests some loss of performances due to the effect of the word sense disambiguation in both monolingual and bilingual tasks. However, there  are a few topics where the WSD techniques significantly improve the effectiveness of the retrieval; these are the cases worth studying from a WSD point of view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Analysis</head><p>In this section we focus on the comparison between WSD and non-WSD runs. Overall, the best MAP and GMAP results in the monolingual system were for two distinct runs which did not use WSD information. Several participants were able to obtain their best MAP and GMAP scores using WSD information. In the bilingual experiments, the best results in MAP was for non-WSD runs, but two participants were able to profit from the WSD annotations. As it is difficult to summarize the behavior of all participants below, we will only mention the performance of the best teams, as given in Tables <ref type="table" coords="9,350.61,584.11,4.98,8.74" target="#tab_3">3</ref> and<ref type="table" coords="9,377.42,584.11,3.87,8.74" target="#tab_4">4</ref>. The interested reader is directed to the working notes of each participant for additional details.</p><p>In the monolingual experiments, cf. Table <ref type="table" coords="9,333.83,608.30,3.87,8.74" target="#tab_3">3</ref>, the best results overall in MAP was for darmstadt. Their WSD runs scored very similar to the non-WSD runs, with a slight decrease of MAP (0.09 percentage points) and a slight increase of GMAP (0.07 percentage points) <ref type="bibr" coords="9,291.99,644.16,14.61,8.74" target="#b14">[15]</ref>. The second best MAP score and best GMAP was attained by reina <ref type="bibr" coords="9,263.94,656.12,15.50,8.74" target="#b15">[16]</ref> without WSD, with their WSD systems show-  ing a considerable performance drop. The third best MAP and second GMAP where obtained by uniba <ref type="bibr" coords="10,248.44,458.39,10.52,8.74" target="#b3">[4]</ref> using WSD. This team showed a 0.94 increase in MAP and 1.67 increase in GMAP with respect to their best non-WSD run. Another team showing high MAP and GMAP values was know-center <ref type="bibr" coords="10,433.56,482.30,14.61,8.74" target="#b10">[11]</ref>, which attained 0.52 improvements in MAP and 0.83 increase in GMAP with the use of WSD. Finally, geneva <ref type="bibr" coords="10,244.26,506.21,15.50,8.74" target="#b9">[10]</ref> also attained good results, but their WSD system also had a considerable drop in both MAP and GMAP. All in all, regarding the use of WSD in the monolingual task, two teams exhibited modest gains, two teams had quite large performance drops, and the teams reporting best results had very similar results.</p><p>In the bilingual experiments, cf. Table <ref type="table" coords="10,326.08,572.43,3.87,8.74" target="#tab_4">4</ref>, the best results overall in MAP were for reina with a system which did not use WSD annotations <ref type="bibr" coords="10,440.45,584.39,14.61,8.74" target="#b15">[16]</ref>. The best GMAP was for geneva using WSD <ref type="bibr" coords="10,307.36,596.34,14.61,8.74" target="#b9">[10]</ref>. Unfortunately, they did not submit any non-WSD run. Uniba <ref type="bibr" coords="10,253.09,608.30,10.52,8.74" target="#b3">[4]</ref> got the second best MAP, with better MAP for the non-WSD run and better GMAP for the WSD run. The differences were small in both cases (0.56 in MAP, 0.71 in GMAP). Those three teams had the highest results, well over 35% MAP, and the rest got more modest performances. know-center <ref type="bibr" coords="10,189.60,656.12,15.50,8.74" target="#b10">[11]</ref> reported better results using WSD information (0.66 MAP, 0.26  GMAP). Ufrgs <ref type="bibr" coords="11,203.30,434.51,10.52,8.74" target="#b4">[5]</ref> only submitted the WSD result. Finally ixa got low results, with small improvements using WSD information (0.33 MAP, 0.08 GMAP).</p><p>All in all, the exercise showed that some teams did improve results using WSD (close to 1 MAP point and more than 1 GMAP point in monolingual, and below 1 MAP/GMAP point in bilingual), but the best results for both monolingual and bilingual tasks were for systems which did not use WSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This new edition of the robust WSD exercise has measured to what extent IR systems could profit from automatic word sense disambiguation information. The conclusions on the monolingual subtask are similar to the conclusions of 2008. The evidence for using WSD in monolingual IR is mixed, with some top scoring groups reporting small improvements in MAP and GMAP, but with the best overall scores for systems not using WSD.</p><p>Regarding the cross-lingual task, the situation is very similar, but the improvements reported by using WSD are smaller.</p><p>Instructions and datasets to replicate the results (including Lucene indexes) are available from http://ixa.si.ehu.es/clirwsd.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,151.70,463.00,293.74,8.74;3,140.99,474.73,309.43,8.77;3,140.99,486.49,339.60,8.77;3,151.70,498.47,62.57,8.74;3,140.99,510.20,339.55,8.77;3,140.99,521.96,339.60,8.77;3,151.70,533.95,62.57,8.74;3,140.99,545.68,339.60,8.77;3,151.70,557.66,62.57,8.74;3,149.71,578.60,330.88,8.74;3,134.77,590.56,345.82,8.74;3,134.77,602.51,120.27,8.74"><head></head><label></label><figDesc>CLEF 2001; Topics 10.2452/41-AH -10.2452/90-AH; LA Times 94 -CLEF 2002; Topics 10.2452/91-AH -10.2452/140-AH; LA Times 94 -CLEF 2003; Topics 10.2452/141-AH -10.2452/200-AH; LA Times 94, Glasgow Herald 95 -CLEF 2004; Topics 10.2452/201-AH -10.2452/250-AH; Glasgow Herald 95 -CLEF 2005; Topics 10.2452/251-AH -10.2452/300-AH; LA Times 94, Glasgow Herald 95 -CLEF 2006; Topics 10.2452/301-AH -10.2452/350-AH; LA Times 94, Glasgow Herald 95 Topics from years 2001, 2002 and 2004 were used as training topics (relevance assessments were offered to participants), and topics from years 2003, 2005 and 2006 were used for the test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,171.54,464.22,272.28,8.14"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of Robust WSD topic: topic 10.2452/141-WSD-AH.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,205.98,115.91,203.41,7.89;6,136.16,136.71,67.89,7.86;6,394.40,136.71,65.61,7.86;6,136.16,148.07,30.97,7.86;6,187.36,148.07,203.49,7.86;6,455.41,148.07,4.61,7.86;6,136.16,159.03,229.99,7.86;6,455.41,159.03,4.61,7.86;6,136.16,169.99,254.69,7.86;6,455.41,169.99,4.61,7.86;6,136.16,180.94,26.89,7.86;6,187.36,180.94,178.79,7.86;6,455.41,180.94,4.61,7.86;6,136.16,191.90,26.89,7.86;6,187.36,191.90,204.26,7.86;6,455.41,191.90,4.61,7.86;6,136.16,202.86,26.89,7.86;6,187.36,202.86,203.49,7.86;6,455.41,202.86,4.61,7.86;6,136.16,213.82,12.03,7.86;6,187.36,213.82,179.56,7.86;6,455.41,213.82,4.61,7.86;6,136.16,224.78,12.03,7.86;6,187.36,224.78,178.79,7.86;6,455.41,224.78,4.61,7.86;6,136.16,235.74,12.03,7.86;6,187.36,235.74,204.26,7.86;6,455.41,235.74,4.61,7.86;6,136.16,246.70,12.03,7.86;6,187.36,246.70,203.49,7.86;6,455.41,246.70,4.61,7.86;6,136.16,257.66,16.64,7.86;6,187.36,257.66,203.49,7.86;6,455.41,257.66,4.61,7.86;6,136.16,268.62,230.75,7.86;6,455.41,268.62,4.61,7.86;6,136.16,279.57,229.99,7.86;6,455.41,279.57,4.61,7.86;6,136.16,290.53,255.46,7.86;6,455.41,290.53,4.61,7.86;6,136.16,301.49,254.69,7.86;6,455.41,301.49,4.61,7.86;6,136.16,312.45,19.99,7.86;6,187.36,312.45,179.56,7.86;6,455.41,312.45,4.61,7.86;6,136.16,323.41,19.99,7.86;6,187.36,323.41,178.79,7.86;6,455.41,323.41,4.61,7.86;6,136.16,334.37,19.99,7.86;6,187.36,334.37,204.26,7.86;6,455.41,334.37,4.61,7.86;6,136.16,345.33,19.99,7.86;6,187.36,345.33,203.49,7.86;6,455.41,345.33,4.61,7.86;6,136.16,356.29,19.78,7.86;6,187.36,356.29,179.56,7.86;6,455.41,356.29,4.61,7.86;6,136.16,367.25,19.78,7.86;6,187.36,367.25,178.79,7.86;6,455.41,367.25,4.61,7.86;6,136.16,378.20,19.78,7.86;6,187.36,378.20,203.49,7.86;6,455.41,378.20,4.61,7.86;6,136.16,389.16,22.53,7.86;6,187.36,389.16,179.56,7.86;6,455.41,389.16,4.61,7.86;6,136.16,400.12,22.53,7.86;6,187.36,400.12,178.79,7.86;6,455.41,400.12,4.61,7.86;6,136.16,411.08,22.53,7.86;6,187.36,411.08,204.26,7.86;6,455.41,411.08,4.61,7.86;6,136.16,422.04,22.53,7.86;6,187.36,422.04,203.49,7.86;6,455.41,422.04,4.61,7.86;6,136.16,433.00,32.00,7.86;6,187.36,433.00,178.79,7.86;6,455.41,433.00,4.61,7.86;6,136.16,443.96,32.00,7.86;6,187.36,443.96,203.49,7.86;6,455.41,443.96,4.61,7.86"><head>Table 1 .</head><label>1</label><figDesc>CLEF 2009 Ad Hoc Robust participants participant task No. experiments alicante AH-ROBUST-WSD-MONO-EN-TEST-CLEF2009 3 darmstadt AH-ROBUST-MONO-EN-TEST-CLEF2009 5 darmstadt AH-ROBUST-WSD-MONO-EN-TEST-CLEF2009 5 geneva AH-ROBUST-MONO-EN-TEST-CLEF2009 5 geneva AH-ROBUST-WSD-BILI-X2EN-TEST-CLEF2009 1 geneva AH-ROBUST-WSD-MONO-EN-TEST-CLEF2009 2 ixa AH-ROBUST-BILI-X2EN-TEST-CLEF2009 1 ixa AH-ROBUST-MONO-EN-TEST-CLEF2009 1 ixa AH-ROBUST-WSD-BILI-X2EN-TEST-CLEF2009 4 ixa AH-ROBUST-WSD-MONO-EN-TEST-CLEF2009 3 jaen AH-ROBUST-WSD-MONO-EN-TEST-CLEF2009 2 know-center AH-ROBUST-BILI-X2EN-TEST-CLEF2009 3 know-center AH-ROBUST-MONO-EN-TEST-CLEF2009 3 know-center AH-ROBUST-WSD-BILI-X2EN-TEST-CLEF2009 3 know-center AH-ROBUST-WSD-MONO-EN-TEST-CLEF2009 3 reina AH-ROBUST-BILI-X2EN-TEST-CLEF2009 5 reina AH-ROBUST-MONO-EN-TEST-CLEF2009 5 reina AH-ROBUST-WSD-BILI-X2EN-TEST-CLEF2009 5 reina AH-ROBUST-WSD-MONO-EN-TEST--WSD-MONO-EN-TEST-CLEF2009 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="8,134.77,390.60,345.83,7.89;8,134.77,401.59,54.50,7.86"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Mean average precision of the top 5 participants of the Robust Monolingual English Task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="9,134.77,390.60,345.82,7.89;9,134.77,401.59,74.46,7.86"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Mean average precision of the top 5 participants of the Robust WSD Monolingual English Task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="10,134.77,390.60,345.83,7.89;10,134.77,401.59,21.55,7.86"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Mean average precision of the top 5 participants of the Robust Bilingual English Task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="11,134.77,390.60,345.82,7.89;11,134.77,401.59,54.50,7.86"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Mean average precision of the top 5 participants of the Robust WSD Bilingual English Task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,134.77,175.53,345.83,101.13"><head></head><label></label><figDesc>Seven topics had no relevant documents at all: 10.2452/149-AH, 10.2452/161-AH, 10.2452/166-AH, 10.2452/186-AH, 10.2452/191-AH, 10.2452/195-AH, 10.2-452/321-AH. Each topic had an average of about 28 relevant documents and a standard deviation of 34, a minimum of 1 relevant document and a maximum of 229 relevant documents per topic.</figDesc><table coords="5,140.99,175.53,213.50,32.94"><row><cell>CLEF 2003: 23,674 documents, 1,006 relevant;</cell></row><row><cell>-CLEF 2005: 19,790 document, 2,063 relevant;</cell></row><row><cell>-CLEF 2006: 21,247 document, 1,258 relevant;</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,198.83,466.15,217.69,75.29"><head>Table 2 .</head><label>2</label><figDesc>Number of runs per track.</figDesc><table coords="6,198.83,486.92,217.69,54.51"><row><cell>Track</cell><cell cols="2"># Part. # Runs</cell></row><row><cell>Robust Mono English Test</cell><cell>8</cell><cell>25</cell></row><row><cell>Robust Mono English Test WSD</cell><cell>10</cell><cell>33</cell></row><row><cell>Robust Biling. English Test</cell><cell>5</cell><cell>13</cell></row><row><cell>Robust Biling. English Test WSD</cell><cell>5</cell><cell>18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="7,116.64,115.91,353.72,143.03"><head>Table 3 .</head><label>3</label><figDesc>Best entries for the robust monolingual task.</figDesc><table coords="7,116.64,136.68,353.72,122.26"><row><cell cols="2">Track Rank Participant</cell><cell>Experiment DOI</cell><cell>MAP GMAP</cell></row><row><cell></cell><cell cols="2">1st darmstadt 10.2415/AH-ROBUST-MONO-EN-TEST-CLEF2009.DARMSTADT.DA 4</cell><cell>45.09% 20.42%</cell></row><row><cell></cell><cell>2nd reina</cell><cell>10.2415/AH-ROBUST-MONO-EN-TEST-CLEF2009.REINA.ROB2</cell><cell>44.52% 21.18%</cell></row><row><cell>English</cell><cell>3rd uniba 4th geneva</cell><cell>10.2415/AH-ROBUST-MONO-EN-TEST-CLEF2009.UNIBA.UNIBAKRF</cell><cell>42.50% 17.93% 41.71% 17.88%</cell></row><row><cell></cell><cell cols="2">5th know-center 10.2415/AH-ROBUST-MONO-EN-TEST-CLEF2009.KNOW-CENTER.ASSO</cell><cell>41.70% 18.64%</cell></row><row><cell></cell><cell cols="3">1st darmstadt 10.2415/AH-ROBUST-WSD-MONO-EN-TEST-CLEF2009.DARMSTADT.DA WSD 4 45.00% 20.49%</cell></row><row><cell></cell><cell>2nd uniba</cell><cell cols="2">10.2415/AH-ROBUST-WSD-MONO-EN-TEST-CLEF2009.UNIBA.UNIBAKEYSYNRF 43.46% 19.60%</cell></row><row><cell cols="4">English 3rd know-center 10.2415/AH-ROBUST-WSD-MONO-EN-TEST-CLEF2009.KNOW-CENTER.ASSOWSD 42.22% 19.47%</cell></row><row><cell cols="2">WSD 4th reina</cell><cell>10.2415/AH-ROBUST-WSD-MONO-EN-TEST-CLEF2009.REINA.ROBWSD2</cell><cell>41.23% 18.38%</cell></row><row><cell></cell><cell>5th geneva</cell><cell cols="2">10.2415/AH-ROBUST-WSD-MONO-EN-TEST-CLEF2009.GENEVA.ISINUSLWTDN 38.11% 16.26%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,94.45,274.76,384.18,143.03"><head>Table 4 .</head><label>4</label><figDesc>Best entries for the robust bilingual task.</figDesc><table coords="7,94.45,295.54,384.18,122.26"><row><cell cols="2">Track Rank Participant</cell><cell>Experiment DOI</cell><cell>MAP GMAP</cell></row><row><cell></cell><cell>1st reina</cell><cell>10.2415/AH-ROBUST-BILI-X2EN-TEST-CLEF2009.REINA.BILI2</cell><cell>38.42% 15.11%</cell></row><row><cell></cell><cell>2nd uniba</cell><cell>10.2415/AH-ROBUST-BILI-X2EN-TEST-CLEF2009.UNIBA.UNIBACROSSKEYRF</cell><cell>38.09% 13.11%</cell></row><row><cell>Es-En</cell><cell cols="2">3rd know-center 10.2415/AH-ROBUST-BILI-X2EN-TEST-CLEF2009.KNOW-CENTER.BILIASSO 4th ufrgs 10.2415/AH-ROBUST-BILI-X2EN-TEST-CLEF2009.UFRGS.BILINGUAL</cell><cell>28.98% 06.79% 27.65% 07.37%</cell></row><row><cell></cell><cell>5th ixa</cell><cell>10.2415/AH-ROBUST-BILI-X2EN-TEST-CLEF2009.IXA.ESENNOWSD</cell><cell>18.05% 01.90%</cell></row><row><cell></cell><cell>1st uniba</cell><cell>10.2415/AH-ROBUST-WSD-BILI-X2EN-TEST-CLEF2009.UNIBA.UNIBACROSSKEYSYNRF</cell><cell>37.53% 13.82%</cell></row><row><cell></cell><cell>2nd geneva</cell><cell>10.2415/AH-ROBUST-WSD-BILI-X2EN-TEST-CLEF2009.GENEVA.ISINUSWSDTD</cell><cell>36.63% 16.02%</cell></row><row><cell cols="2">Es-En 3rd reina</cell><cell>10.2415/AH-ROBUST-WSD-BILI-X2EN-TEST-CLEF2009.REINA.BILIWSD2</cell><cell>30.32% 09.38%</cell></row><row><cell></cell><cell cols="2">4th know-center 10.2415/AH-ROBUST-WSD-BILI-X2EN-TEST-CLEF2009.KNOW-CENTER.BILIASSOWSD</cell><cell>29.64% 07.05%</cell></row><row><cell cols="2">WSD 5th ixa</cell><cell cols="2">10.2415/AH-ROBUST-WSD-BILI-X2EN-TEST-CLEF2009.IXA.ESEN1STTOPSBESTSENSE500DOCS 18.38% 01.98%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="11,136.43,118.29,361.98,255.46"><head></head><label></label><figDesc>Hoc Robust WSD Bilingual English Test Task Top 5 Participants -Standard Recall Levels vs Mean Interpolated Precision uniba [Experiment UNIBACROSSKEYSYNRF; MAP 37.53%; Not Pooled] geneva [Experiment ISINUSWSDTD; MAP 36.63%; Not Pooled] reina [Experiment BILIWSD2; MAP 30.32%; Not Pooled] know-center [Experiment BILIASSOWSD; MAP 29.64%; Not Pooled] ixa [Experiment ESEN1STTOPSBESTSENSE500DOCS; MAP 18.38%; Not Pooled]</figDesc><table coords="11,136.43,118.29,332.83,255.46"><row><cell>Ad-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>100%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>90%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>80%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>60%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell>50%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>40%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0% 0%</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell><cell>100%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.73,645.84,323.82,8.12"><p>A sample document and dtd are available at http://ixa2.si.ehu.es/clirwsd/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.73,657.44,89.44,7.47"><p>http://www.doi.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,144.73,656.80,289.90,8.12"><p>Full sample and dtd are available at http://ixa2.si.ehu.es/clirwsd/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Acknowledgements</head><p>The robust task was partially funded by the <rs type="funder">Ministry of Education</rs> (project <rs type="grantNumber">KNOW TIN2006-15049</rs>) and the <rs type="funder">European Commission</rs> (project <rs type="grantNumber">KYOTO ICT-2007-211423</rs>). We want to thank <rs type="person">Oier Lopez de Lacalle</rs>, who runs the <rs type="institution">UBC</rs> WSD system, and <rs type="person">Yee Seng Chan</rs>, <rs type="person">Hwee Tou Ng</rs> and <rs type="person">Zhi Zhong</rs>, who run the <rs type="institution">NUS</rs> WSD system. Their generous contribution was invaluable to run this exercise.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Dv2x798">
					<idno type="grant-number">KNOW TIN2006-15049</idno>
				</org>
				<org type="funding" xml:id="_DeyNAaD">
					<idno type="grant-number">KYOTO ICT-2007-211423</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="12,142.96,243.28,337.63,7.86;12,151.52,254.24,329.07,7.86;12,151.52,265.20,229.26,7.86" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,297.82,243.28,182.77,7.86;12,151.52,254.24,18.14,7.86">UBC-ALM: Combining k-NN with SVD for WSD</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Lopez De Lacalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,190.63,254.24,289.96,7.86;12,151.52,265.20,61.82,7.86">Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval 2007)</title>
		<meeting>the 4th International Workshop on Semantic Evaluations (SemEval 2007)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="341" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,275.93,337.64,7.86;12,151.52,286.89,329.07,7.86;12,151.52,297.85,278.09,7.86" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,151.52,286.89,325.19,7.86">SemEval-2007 Task01: Evaluating WSD on Cross-Language Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Lopez De Lacalle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Otegi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Vossen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,163.04,297.85,148.98,7.86">Proceedings of CLEF 2007 Workshop</title>
		<meeting>CLEF 2007 Workshop<address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,308.58,337.64,7.86;12,151.52,319.54,329.07,7.86;12,151.52,330.50,232.17,8.12" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,399.25,308.58,81.34,7.86;12,151.52,319.54,62.06,7.86">CLEF 2008: Ad Hoc Track Overview</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Di Nunzio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/" />
	</analytic>
	<monogr>
		<title level="m" coord="12,389.32,319.54,91.27,7.86;12,151.52,330.50,87.33,7.86">Working Notes for the CLEF 2009 Workshop</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Borri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,341.24,337.64,7.86;12,151.52,352.19,84.06,7.86" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="12,300.64,341.24,179.96,7.86;12,151.52,352.19,15.40,7.86">UNIBA-SENSE at CLEF 2009: Robust WSD task</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Semeraro</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="12,142.96,362.93,337.63,7.86;12,151.52,373.89,31.23,7.86" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,271.61,362.93,79.02,7.86">UFRGS@CLEF</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">B</forename><surname>Borges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><forename type="middle">P</forename><surname>Moreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,359.70,362.93,120.89,7.86">Retrieval by Numbers In this</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,384.62,337.64,7.86;12,151.52,395.58,329.07,7.86;12,151.52,406.54,329.07,7.86;12,151.52,417.50,329.07,7.86;12,151.52,428.46,234.00,7.86" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,261.63,384.62,153.74,7.86">CLEF 2003 Methodology and Metrics</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Braschler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,322.50,395.58,158.10,7.86;12,151.52,406.54,329.07,7.86;12,151.52,417.50,180.19,7.86">Comparative Evaluation of Multilingual Information Access Systems: Fourth Workshop of the Cross-Language Evaluation Forum (CLEF 2003) Revised Selected Papers</title>
		<title level="s" coord="12,338.80,417.50,141.80,7.86;12,151.52,428.46,26.34,7.86">Lecture Notes in Computer Science (LNCS</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Braschler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kluck</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3237</biblScope>
			<biblScope unit="page" from="7" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,439.19,337.63,7.86;12,151.52,450.15,329.07,7.86;12,151.52,461.11,329.07,7.86;12,151.52,472.07,99.45,7.86" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,298.29,439.19,182.30,7.86;12,151.52,450.15,217.01,7.86">NUS-PT: Exploiting Parallel Texts for Word Sense Disambiguation in the English All-Words Tasks</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,389.14,450.15,91.45,7.86;12,151.52,461.11,263.88,7.86">Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval 2007)</title>
		<meeting>the 4th International Workshop on Semantic Evaluations (SemEval 2007)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="253" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,482.80,337.63,7.86;12,151.52,493.76,329.07,7.86;12,151.52,504.72,228.12,7.86" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,213.68,482.80,191.95,7.86">The Cranfield Tests on Index Language Devices</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cleverdon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,236.88,493.76,136.90,7.86">Readings in Information Retrieval</title>
		<editor>
			<persName><forename type="first">Sparck</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Willett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename></persName>
		</editor>
		<meeting><address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publisher, Inc</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="47" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.96,515.45,337.63,7.86;12,151.52,526.41,329.07,8.12;12,151.52,537.37,142.14,8.12" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="12,264.95,515.45,159.75,7.86">Appendix C: Results of the Robust Task</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Di Nunzio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<ptr target="http://www.clef-campaign.org/(2008" />
	</analytic>
	<monogr>
		<title level="m" coord="12,264.64,526.41,183.99,7.86">Working Notes for the CLEF 2009 Workshop</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Borri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,548.11,337.98,7.86;12,151.52,559.07,60.72,7.86" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="12,311.64,548.11,164.64,7.86">UniGe at CLEF 2009 Robust WSD Task</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guyot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Falquet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Radhouani</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="12,142.62,569.80,337.98,7.86;12,151.52,580.76,164.15,7.86" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="12,315.00,569.80,165.59,7.86;12,151.52,580.76,95.46,7.86">Application of Axiomatic Approaches to Crosslanguage Retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Juffinger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Granitzer</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="12,142.62,591.49,337.98,7.86;12,151.52,602.45,194.36,8.12" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="12,214.89,591.49,124.67,7.86">The DOI Handbook -Edition 4</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Paskin</surname></persName>
		</author>
		<idno type="DOI">10.1000/186</idno>
		<ptr target="http://dx.doi.org/10.1000/186" />
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>International DOI Foundation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,613.19,337.97,7.86;12,151.52,624.15,329.07,7.86;12,151.52,635.10,329.07,7.86" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="12,213.74,613.19,159.23,7.86">On GMAP: and Other Transformations</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,257.09,624.15,223.50,7.86;12,151.52,635.10,153.32,7.86">Proc. 15th International Conference on Information and Knowledge Management (CIKM 2006)</title>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Tsotras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Liu</surname></persName>
		</editor>
		<meeting>15th International Conference on Information and Knowledge Management (CIKM 2006)<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,142.62,645.81,337.98,7.89;12,151.52,656.80,23.04,7.86" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,223.86,645.84,144.84,7.86">The TREC Robust Retrieval Track</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,380.06,645.84,56.14,7.86">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="11" to="20" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,142.62,119.67,337.98,7.86;13,151.52,130.63,329.07,7.86;13,151.52,141.59,134.84,7.86" xml:id="b14">
	<monogr>
		<title level="m" type="main" coord="13,310.98,119.67,169.61,7.86;13,151.52,130.63,329.07,7.86;13,151.52,141.59,66.16,7.86">Combining Probabilistic and Translation-Based Models for Information Retrieval based on Word Sense Annotations Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

<biblStruct coords="13,142.62,152.55,337.98,7.86;13,151.52,163.51,329.07,7.86;13,151.52,174.47,31.23,7.86" xml:id="b15">
	<monogr>
		<title level="m" type="main" coord="13,409.60,152.55,70.99,7.86;13,151.52,163.51,292.02,7.86">REINA at CLEF 2009 Robust-WSD Task: Partial Use of WSD Information for Retrieval</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zazo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">G</forename><surname>Figuerola</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">L</forename><surname>Alonso Berrocal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Gomez</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In this volume</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
