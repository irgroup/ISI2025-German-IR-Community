<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,90.00,148.86,422.99,15.15">UNIBA-SENSE @ CLEF 2009: Robust WSD task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,181.43,182.75,69.04,8.74"><forename type="first">Pierpaolo</forename><surname>Basile</surname></persName>
							<email>basilepp@di.uniba.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Bari Via</orgName>
								<address>
									<addrLine>E. Orabona</addrLine>
									<postCode>4 -70125</postCode>
									<settlement>Bari</settlement>
									<country key="IT">ITALY</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,257.90,182.75,72.78,8.74"><forename type="first">Annalina</forename><surname>Caputo</surname></persName>
							<email>acaputo@di.uniba.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Bari Via</orgName>
								<address>
									<addrLine>E. Orabona</addrLine>
									<postCode>4 -70125</postCode>
									<settlement>Bari</settlement>
									<country key="IT">ITALY</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,338.98,182.75,82.59,8.74"><forename type="first">Giovanni</forename><surname>Semeraro</surname></persName>
							<email>semeraro@di.uniba.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Bari Via</orgName>
								<address>
									<addrLine>E. Orabona</addrLine>
									<postCode>4 -70125</postCode>
									<settlement>Bari</settlement>
									<country key="IT">ITALY</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,90.00,148.86,422.99,15.15">UNIBA-SENSE @ CLEF 2009: Robust WSD task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8DC68316BABC2E2F3ED5D6BDBB6BC215</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Languages-Query Languages Measurement, Performance, Experimentation Information Retrieval, Word Sense Disambiguation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents the participation of the semantic N-levels search engine SENSE at the CLEF 2009 Ad Hoc Robust-WSD Task. During the participation at the same task of CLEF 2008, SENSE showed that WSD can be helpful to improve retrieval, even though the overall performance was not exciting mainly due to the adoption of a pure Vector Space Model with no heuristics. In this edition, our aim is to demonstrate that the combination of the N-levels model and WSD can improve the retrieval performance even when an effective retrieval model is adopted. To reach this aim, we worked on two different strategies. On one hand a new model, based on Okapi BM25, was adopted at each level. Moreover, we improved the word stemming algorithm and we normalized words removing some characters that made more evident the word mismatch problem. The use of these simple heuristics allowed us to increases of 106% the MAP value, compared to our best result obtained at CLEF 2008. On the other hand, we integrated a local relevance feedback technique, called Local Context Analysis, in both indexing levels of the system (keyword and word meaning). The hypothesis that Local Context Analysis can be effective even when it works on word meanings coming from a WSD algorithm is supported by experimental results. In Mono-lingual task MAP increased of about 2% exploiting disambiguation, while GMAP increased from 4% to 9% when we used WSD in both Mono-and Cross-lingual tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we present our participation at the CLEF 2009 Ad Hoc Robust-WSD Task. Our retrieval system is based on SENSE <ref type="bibr" coords="1,250.72,722.18,9.96,8.74" target="#b1">[2]</ref>, a semantic search engine which implements the N-levels model.</p><p>The main motivation behind our model is that the presence of multiple meanings for one word (polysemy), together with synonymy (occurring when different words have the same meaning), negatively affects the retrieval performance. Generally, the result is that, due to synonymy, relevant documents can be missed if they do not contain the exact query keywords, while wrong documents are deemed as relevant due to polysemy. These problems call for alternative methods that work not only at the lexical level of the documents, but also at the meaning level.</p><p>Therefore, in our interpretation semantic information could be captured from a text by looking at word meanings, as they are described in a reference dictionary (e.g. WordNet <ref type="bibr" coords="2,458.76,195.71,10.30,8.74" target="#b4">[5]</ref>). SENSE is an IR system which manages documents indexed at multiple separate levels: keywords and senses (word meanings). The system is able to combine keyword search with semantic information provided by the word meaning level.</p><p>The main idea underlying the definition of an open framework to model different semantic aspects (or levels) pertaining document content is that there are several ways to describe the semantics of a document. Each semantic facet needs specific techniques and ad-hoc similarity functions. To address this problem we propose a framework in which a different IR model is defined for each level in the document representation. Each level corresponds to a logical view that aims to describe one of the possible semantic spaces in which documents can be represented. The adoption of different levels is intended to guarantee acceptable system performance even when not all semantic representations are available for a document.</p><p>We suppose that the keyword level is always present and, when other levels are available too, they are exploited to enhance retrieval capabilities. Furthermore, our framework allows to associate each level with the appropriate representation and similarity measure. The following semantic levels are currently available in the framework: Keyword level -the entry level in which a document is represented by the words occurring in the text.</p><p>Word meaning level -at this level a document is represented through synsets obtained by WordNet, a semantic lexicon for the English language. A synset is a set of synonym words (with the same meaning). Word Sense Disambiguation (WSD) algorithms are adopted to assign synsets to words.</p><p>SENSE is able to manage different models for each level. In CLEF 2008 edition we adopted the standard Vector Space Model implemented in Lucene for both the keyword and the word meaning level. For CLEF 2009 our goal is to improve the overall retrieval performance adopting a more powerful model, called Okapi BM25, and the introduction of a pseudo-relevance feedback mechanism based on Local Context Analysis.</p><p>The rest of the paper is structured as follows: The indexing step adopted in SENSE is described in Section 2, while Section 3 presents the searching step. Moreover, Section 3 contains details about the Okapi BM25 model implemented in SENSE and the Local Context Analysis strategy. The details of the system setup for the CLEF competition are provided in Section 4. Finally, the experiments are described in Section 5. Conclusions and future work close the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Indexing</head><p>In CLEF Ad-Hoc WSD Robust track, documents and queries are provided in XML format. In order to index the documents and read the queries we developed an XML parser using the XMLBeans<ref type="foot" coords="2,508.53,655.37,3.97,6.12" target="#foot_0">1</ref> tool. Moreover, we produced an intermediate data format which contains all the data necessary to the N-levels model. For each token this format provides a set of features needful to build each level. In the case in point, for the keyword level the stemming of the word<ref type="foot" coords="2,420.73,691.24,3.97,6.12" target="#foot_1">2</ref> is provided, for the meaning one we provided the list of all possible meanings with the corresponding score.</p><p>An intermediate format is necessary because SENSE supports an indefinite number of levels, not restricted to keyword and meaning ones as in CLEF Ad-Hoc WSD Robust track. For that reason we developed a flexible indexing mechanism able to support further levels.</p><p>During the indexing we performed several text operations. One is stop words elimination. We built two different stop words lists, one for documents and one for queries. In this way we removed irrelevant words from queries, such as: find, report, information, provide, describe, include, discuss, specific, interest, concern. Moreover, before storing each token in a document, we replaced all occurrences of not alphanumeric characters with a single underscore character " ". This text normalization operation is also performed for queries during the search process. In that way the match between documents and query is not compromised.</p><p>As regards the meaning level, we index for each token only the WordNet synset with the highest score. For each document a bag of synsets is built. Hence, features at the word meaning level are synsets obtained from WordNet, a semantic lexicon for the English language. Consequently, the vocabulary at this level is the set of distinct synsets recognized in the collection by the WSD procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Searching</head><p>The local similarity functions for both the meaning and the keyword levels are computed using a modified version of the Lucene default document score, that implements the Okapi BM25 model described in Section 3.1. For the meaning level, both query and document vectors contain synsets instead of keywords.</p><p>In SENSE each level produces a list of documents ranked according to the similarity function defined for that level (local similarity function). Since the ultimate goal is to obtain a single list of documents ranked in decreasing order of relevance, a global ranking function is needed to merge all the result lists that come from each level. This function is independent of both the number of levels and the specific local scoring and similarity functions because it takes as input N ranked lists of documents and produces a unique merged list of the most relevant documents.</p><p>The aggregation of lists in a single one requires two steps: The first one produces the N normalized lists and the second one merges the N lists in a single one. The two steps are thoroughly described in <ref type="bibr" coords="3,145.51,477.06,9.96,8.74" target="#b1">[2]</ref>. In CLEF we adopt Z-Score normalization and CombSUM <ref type="bibr" coords="3,417.48,477.06,10.52,8.74" target="#b2">[3,</ref><ref type="bibr" coords="3,431.21,477.06,7.75,8.74" target="#b3">4]</ref> as score normalization and rank aggregation function, respectively. Each level can be combined using a different weighting factor in order to give different relevance to each level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Okapi BM25 model in SENSE</head><p>We employed Lucene API to build the SENSE search engine. An important change we made concerns the adoption of a new model, based on Okapi BM25 <ref type="bibr" coords="3,363.93,558.65,9.96,8.74" target="#b6">[7]</ref>, to implement a new weighting scheme and local similarity function at each level. In order to implement BM25 in SENSE we exploited the technique described in <ref type="bibr" coords="3,247.52,582.56,9.96,8.74" target="#b5">[6]</ref>. In particular, we adopted the BM25-based strategy which takes into account multi-field documents. Indeed, in our collection each document is represented by two fields: HEADLINE and TEXT. The multi-field representation reflects the XML structure of documents provided by the organizers.</p><p>First of all, in the multi-field representation the weight of each term is computed taking into account the aggregate amount of the term weights for all fields, as follows:</p><formula xml:id="formula_0" coords="3,216.14,658.87,296.86,29.42">weight(t, d) = c∈d occurs d t,c * boost c ((1 -b c ) + b c * lc avlc )<label>(1)</label></formula><p>where occurs d t,c is the occurrence of the term t in the field c, l c is the field length and avl c is the average length for the field c. b c is a constant related to the field length, similar to b constant in classical BM25 formula, while boost c is the boost factor applied to field c.</p><p>Then, the similarity between query and document is computed exploiting the accumulated weight for each term t that occurs both in the query q and in the document d.</p><formula xml:id="formula_1" coords="4,217.55,120.49,295.45,26.35">R(q, d) = t∈q idf (t) * weight(t, d) k 1 + weight(t, d)<label>(2)</label></formula><p>Inverse document frequency is computed according to the classical BM25 model:</p><formula xml:id="formula_2" coords="4,241.49,175.35,267.27,22.31">idf (t) = log N -df (t) + 0.5 df (t) + 0.5 (<label>3</label></formula><formula xml:id="formula_3" coords="4,508.76,182.09,4.24,8.74">)</formula><p>where N is the number of documents in the collection and df (t) is the number of documents where the term t appears. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Query Expansion and Term Reweighting</head><p>During 2008 edition of CLEF, SENSE showed promising results, although its overall performance was not exciting. This deterred us from using query expansion techniques. Indeed, a preliminary condition to avoid the query drift problem, an intrinsic problem for automatic query expansion methods, is to have a system with good precision in the first retrieved documents. The performance improvement expected as a consequence of the adoption of BM25 weighting scheme, made it possible the use of these techniques into our system. We extended the SENSE architecture by integrating a query expansion module, as well as a technique for term reweighting. We adopted the Local Context Analysis (LCA) <ref type="bibr" coords="4,250.72,461.84,9.96,8.74" target="#b7">[8]</ref>, a strategy that proved its effectiveness on several test collections. LCA is a local techniques as it analyzes only the first top-ranked documents that are assumed to be the relevant ones. LCA relies on the hypothesis that terms frequently occurring in the top-ranked documents frequently co-occur with all query terms in those documents too. We employed the LCA for both levels exploited in our experiments: keyword and word meaning. The underlying idea is that the LCA hypothesis could also be applied to the word meaning level, in which meanings are involved instead of terms. Therefore, we extended the original measure of co-occurrence degree in order to weigh a generic feature (keyword or word meaning) rather than just a term. According to the original formula, we define the following function:</p><formula xml:id="formula_4" coords="4,203.64,577.85,309.36,23.23">codegree(f, q i ) = log 10 (co(f, q i ) + 1) * idf (f ) log 10 (n) (4)</formula><p>codegree measures the degree of co-occurrence between the feature f and the query feature q i (co(f, q i )), but it takes also into account the frequency of f in the whole collection (idf (f )) and normalizes this value with respect to n, the number of documents in the top-ranked set.</p><formula xml:id="formula_5" coords="4,231.20,659.06,281.80,20.14">co(f, q i ) = d∈S tf (f, d) * tf (q i , d)<label>(5)</label></formula><formula xml:id="formula_6" coords="4,241.63,690.33,271.37,26.80">idf (f ) = min(1.0, log 10 N N f 5.0 )<label>(6)</label></formula><p>where tf (f, d) and tf (q i , d) are the frequency of f and q i in d respectively, S is the set of top-ranked documents, N is the number of documents in the collections and N f is the number of documents containing the feature f . For each level, we retrieve the n top-ranked documents for a query q by computing a function lca for each feature in the results set, as follows:</p><formula xml:id="formula_7" coords="5,212.39,143.80,300.61,21.69">lca(f, q) = qi∈q (δ + codegree(f, q i )) idf (qi)<label>(7)</label></formula><p>This formula is used to rank the list of features that occur in the top-ranked documents; δ is a smoothing factor and the exponent is used to give an higher impact to rare features. A new query q is created by adding the k top ranked features to the original query, each feature is weighed using the lca value. Hence, the new query is re-executed to obtain the final list of ranked documents for each level. Differently from the original work, we applied LCA to the top ranked documents rather than passages 3 . Moreover, no tuning is performed over the collection to set the parameters.</p><p>For the CLEF experiments, we decided to get the first ten top-ranked documents and to expand the query using the first ten ranked features. Finally, we set up the smoothing factor to 0.1 in order to boost those concepts that co-occur with the highest number of query features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">System setup</head><p>We exploited the SENSE framework to build our IR system for the CLEF evaluation. We used two different levels: keyword (using word stems) and word meaning (using WordNet synsets). All SENSE components involved in the experiments are implemented in Java using the version 2.3.2 of Lucene API. Experiments were run on an Intel Core 2 Quad processor at 2.6 GHz, operating in 64 bit mode, running Linux (UBUNTU 9.04), with 4 GB of main memory. Following CLEF guidelines, we performed two different tracks of experiments: Ad Hoc Robust-WSD Mono-language and Cross-language. Each track required two different evaluations: with and without synsets. We exploited several combinations between levels and the query relevance feedback method, especially for the meaning level. All query building methods are automatic and do not require manual operations. Moreover, we used different boosting factors for each topic field and gave more importance to the terms in the fields TITLE and DESCRIPTION. More details on the track are reported in the track overview paper <ref type="bibr" coords="5,326.53,457.77,9.96,8.74" target="#b0">[1]</ref>.</p><p>In particular for the Ad-Hoc Mono-language track we performed the following runs:</p><p>1. unibaKTD: the query is built using word stems in the fields TITLE and DESCRIPTION of the topics. All query terms are joined adopting the OR boolean operator. The terms in the TITLE field are boosted using a factor 8.</p><p>2. unibaKTDN: similar to the previous run, but in this case we add the NARRATIVE field and we adopt different term boosting values: 8 for TITLE, 1 for DESCRIPTION and 1 for NARRATIVE.</p><p>3. unibaKRF: we used the query produced in unibaKTDN adding a pseudo-relevance feedback mechanism which implements LCA.</p><p>4. unibaWsdTD: in this experiment we exploited only the word meaning level. The query is built using the synset with the highest score for each token. The synset score is also used to give a weight to the synset into the query. Synset boosting values are: 8 for TITLE and 2 for DESCRIPTION.</p><p>5. unibaWsdTDN: similar to the previous run, but in this case we add the NARRATIVE field. Synset boosting values are: 8 for TITLE, 2 for DESCRIPTION and 1 for NARRATIVE.</p><p>6. unibaWsdNL0802: in this experiment we exploit the N-level architecture of SENSE. For the keyword level we adopt the query method described in unibaKTDN and for the word meaning level that in unibaWsdTDN. The two levels are combined using a factor of 0.8 for keyword and 0.2 for meaning. Though a comparison with the CLEF 2008 results is not reported, we have to point out that the worst run without WSD (unibaKTD) registered a rise of 106% in MAP when compared to the best CLEF 2008 run. Analyzing the mono-lingual task, as expected the word meaning level used alone is not enough to reach good performance (unibaWsdTD, unibaWsdTDN ). However, an increase of 1,7% in MAP is obtained when word meanings are exploited in the N-levels model (unibaWsdNL0901 ) with respect to the keyword level alone (unibaKTDN ). Looking at the Nlevels results, we can notice the impact of word meanings on GMAP. In fact, as the weight of the word meaning level raises as the MAP decreases while the GMAP increases. In both runs, with or without WSD, the adoption of pseudo-relevance feedback techniques increases the MAP: 2.9% with WSD (unibaKeySynRF vs. unibaWsdNL0901 ) and 2.4% without WSD (unibaKRF vs. unibaKTDN ). Finally, LCA combined to WSD (unibaKeySynRF ) works better than LCA without WSD (unibaKRF ) with an increment in all measures (+2.3% MAP, +9.3% GMAP, +0.6% R-prec, +3.1% P@5, +0.9% P@10) and, in general, it shows the best results.</p><p>In bilingual task, queries are disambiguated using the first sense heuristics. This clearly has an impact on the use of synsets in the query processing and pseudo-relevance feedback steps. Performance of word meaning level are very bad. Moreover, runs without WSD generally outperform those with WSD, with an increment of 1.5% in MAP (unibaCrossKeyRF vs. uni-baCrossKeySynRF ). As LCA has shown to be helpful, with or without WSD, a higher increment is obtained without WSD: 2.09% in MAP (unibaCrossKeyRF vs. unibaCrossTDN ). Nevertheless, also in the bilingual task WSD has improved the GMAP with an increment of 5.42% (uni-baCrossKeySynRF vs. unibaCrossKeyRF ). The increment in GMAP emphasizes the improvement for poorly performing (low precision) topics. This suggests that WSD is especially useful for those topics with low scores in average precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We have described and tested SENSE, a semantic N -levels IR system which manages documents indexed at multiple separate levels: keywords and meanings. The system is able to combine keyword search with semantic information provided by other indexing levels.</p><p>Respect to the last participation of SENSE to CLEF, we introduce in this edition new features in order to improve the overall retrieval performance. In particular, we adopt the Okapi BM25 model for both keyword and word meaning levels. Moreover, we propose a pseudo-relevance feedback strategy based on Local Context Analysis. This strategy is applied to keyword and word meaning levels.</p><p>The results of the evaluation prove that the combination of keyword and word meaning can improve the retrieval performance. Only in cross-lingual task the combination of levels is outperformed by the only keyword level. Probably this is due to WSD technique adopted for Spanish topics. In particular, no WSD algorithms for Spanish are available and the organizers assign the first synset in Spanish-WordNet to each keyword in a topic. Moreover, the results prove that the pseudo-relevance feedback based on Local Context Analysis improves the IR performance.</p><p>As future research we plan to improve the pseudo-relevance feedback strategy. We can achieve this goal applying the Local Context Analysis to the merged list of documents provided by SENSE. Currently, the Local Context Analysis is applied separately to the top ranked documents present in each level: keyword and word meaning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,90.00,217.82,423.00,111.40"><head>Table 1 :</head><label>1</label><figDesc>Table 1 shows the BM25 parameters used in SENSE. Parameters are different for keyword level (HEADLIN E k , T EXT k ) and meaning level (HEADLIN E s , T EXT s ) BM25 parameters used in SENSE.</figDesc><table coords="4,184.62,252.94,233.26,54.51"><row><cell>Field</cell><cell>k1</cell><cell>N</cell><cell>avlc</cell><cell>bc</cell><cell>boostc</cell></row><row><cell cols="3">HEADLIN E k 3.25 166,726</cell><cell>7.96</cell><cell>0.70</cell><cell>2.00</cell></row><row><cell>T EXT k</cell><cell cols="4">3.25 166,726 295.05 0.70</cell><cell>1.00</cell></row><row><cell cols="3">HEADLIN Es 3.50 166,726</cell><cell>5.94</cell><cell>0.70</cell><cell>2.00</cell></row><row><cell>T EXTs</cell><cell cols="4">3.50 166,726 230.54 0.70</cell><cell>1.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,148.98,451.30,303.87,222.53"><head>Table 2 :</head><label>2</label><figDesc>Overview of experiments</figDesc><table coords="7,148.98,485.02,303.87,188.81"><row><cell>Run</cell><cell cols="3">MAP GMAP R-PREC</cell><cell>P@5</cell><cell>P@10</cell></row><row><cell>unibaKTD</cell><cell>.3962</cell><cell>.1684</cell><cell>.3940</cell><cell>.4563</cell><cell>.3888</cell></row><row><cell>unibaKTDN</cell><cell>.4150</cell><cell>.1744</cell><cell>.4082</cell><cell>.4713</cell><cell>.4019</cell></row><row><cell>unibaKRF</cell><cell>.4250</cell><cell>.1793</cell><cell>.4128</cell><cell>.4825</cell><cell>.4150</cell></row><row><cell>unibaWsdTD</cell><cell>.2930</cell><cell>.1010</cell><cell>.2854</cell><cell>.3838</cell><cell>.3256</cell></row><row><cell>unibaWsdTDN</cell><cell>.3238</cell><cell>.1234</cell><cell>.3077</cell><cell>.4038</cell><cell>.3544</cell></row><row><cell>unibaWsdNL0802</cell><cell>.4218</cell><cell>.1893</cell><cell>.4032</cell><cell>.4838</cell><cell>.4081</cell></row><row><cell>unibaWsdNL0901</cell><cell>.4222</cell><cell>.1864</cell><cell>.4019</cell><cell>.4750</cell><cell>.4088</cell></row><row><cell>unibaKeySynRF</cell><cell>.4346</cell><cell>.1960</cell><cell>.4153</cell><cell cols="2">.4975 .4188</cell></row><row><cell>unibaCrossTD</cell><cell>.3414</cell><cell>.1131</cell><cell>.3389</cell><cell>.4013</cell><cell>.3419</cell></row><row><cell>unibaCrossTDN</cell><cell>.3731</cell><cell>.1281</cell><cell>.3700</cell><cell>.4363</cell><cell>.3713</cell></row><row><cell>unibaCrossKeyRF</cell><cell>.3809</cell><cell>.1311</cell><cell>.3755</cell><cell>.4413</cell><cell>.3794</cell></row><row><cell>unibaCrossWsdTD</cell><cell>.0925</cell><cell>.0024</cell><cell>.1029</cell><cell>.1188</cell><cell>.1081</cell></row><row><cell>unibaCrossWsdTDN</cell><cell>.0960</cell><cell>.0050</cell><cell>.1029</cell><cell>.1425</cell><cell>.1188</cell></row><row><cell>unibaCrossWsdNL0802</cell><cell>.3675</cell><cell>.1349</cell><cell>.3655</cell><cell>.4455</cell><cell>.3750</cell></row><row><cell>unibaCrossWsdNL0901</cell><cell>.3731</cell><cell>.1339</cell><cell>.3635</cell><cell>.4475</cell><cell>.3769</cell></row><row><cell>unibaCrossKeySynRF</cell><cell>.3753</cell><cell>.1382</cell><cell>.3709</cell><cell cols="2">.4513 .3850</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,199.88,686.85,203.24,8.74"><head>Table 3 :</head><label>3</label><figDesc>Results of the performed experiments</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,723.70,107.32,6.99"><p>http://xmlbeans.apache.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,105.24,733.20,163.09,6.99"><p>Stemming is performed by Snowball library.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>7. unibaWsdNL0901: similar to the previous run, but using different combination factors: 0.9 for keyword and 0.1 for meaning.</p><p>8. unibaKeySynRF: in this experiment we exploit both the N-level architecture of SENSE and LCA. For the keyword level we adopt the query method described in unibaKRF and for the word meaning level the unibaWsdTDN applying pseudo-relevance feedback. The two levels are combined using a factor of 0.8 for keyword and 0.2 for meaning.</p><p>For the Ad-Hoc Cross-language track we performed the following runs:</p><p>1. unibaCrossTD: the query is built using word stems in the TITLE and DESCRIPTION fields of the topics. In the Cross-language track the topics are in Spanish, thus a translation of terms in English is required. We adopt Google Translation API to translate queries from Spanish to English. Term boosting values are: 8 for TITLE and 1 for DESCRIPTION.</p><p>2. unibaCrossTDN: similar to the previous run, adding the NARRATIVE field. Term boosting values are: 8 for TITLE, 1 for DESCRIPTION and 1 for NARRATIVE.</p><p>3. unibaCrossKeyRF: queries are built using the method described in unibaCrossTDN and pseudo-relevance feedback is applied using LCA.</p><p>4. unibaCrossWsdTD: the query is built using for each token the synset with the highest score. Synset boosting values are: 8 for TITLE and 2 for DESCRIPTION. It is important to note that in this case the synset with the highest score is always the first synset in Spanish WordNet because word sense disambiguation is not applied to Spanish topics.</p><p>5. unibaCrossWsdTDN: similar to the previous run, but in this case we add the NARRA-TIVE field.</p><p>6. unibaCrossWsdNL0802: in this experiment we exploit the N-level architecture of SENSE.</p><p>For the keyword level we adopt the query method described in unibaCrossTDN and for the word meaning level the unibaCrossWsdTDN. The two levels are combined using a factor 0.8 for keyword and a factor 0.2 for meaning.</p><p>7. unibaCrossWsdNL0901: similar to the previous run, but using different combination factors: 0.9 for keyword and 0.1 for meaning.</p><p>8. unibaCrossKeySynRF: in this experiment we exploit both the N-level architecture of SENSE and relevance feedback in the context of cross-language retrieval. For the keyword level we adopt the query method described in unibaCrossKeyRF and for the word meaning level the unibaCrossWsdTDN applying pseudo-relevance feedback. The two levels are combined using a factor 0.8 for keyword and 0.2 for meaning.</p><p>For all the runs we removed the stop words from both the index and the topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Session</head><p>The experiments were carried out on the CLEF Ad Hoc WSD-Robust dataset derived from the English CLEF data, which comprises corpora from "Los Angeles Times" and "Glasgow Herald", amounting to 166, 726 documents and 160 topics in English and Spanish. The relevance judgments were taken from CLEF.</p><p>Our evaluation has two main goals:</p><p>1. to prove that the combination of two levels outperforms a single level. Specifically, we want to investigate whether the combination of keyword and meaning levels turns out to be more effective than the keyword level alone, and how the performance varies.</p><p>2. to prove that Local Context Analysis improves the system performance. We exploit pseudorelevance feedback techniques in both levels, keyword and meaning. Our aim is to demonstrate the effectiveness of pseudo-relevance feedback when it is applied not only to a keyword but to a word meaning representation, too.</p><p>To measure retrieval performance, we adopted the Mean-Average-Precision (MAP) and the Geometric-Mean-Average-Precision (GMAP) calculated by CLEF organizers using the DIRECT system on the basis of the first 1,000 retrieved items per request. Table <ref type="table" coords="7,387.21,190.87,4.98,8.74">2</ref> summarizes the description of system setup for each run, while Table <ref type="table" coords="7,283.84,202.82,4.98,8.74">3</ref> shows the results of five metrics (Mean-Average-Precision, Geometric-Mean-Average-Precision, R-precision, P@5 and P@10 are the R-precision where R is set to 5 and 10 respectively) for each run.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,105.50,639.55,407.50,8.74;8,105.50,651.51,257.36,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,392.94,639.55,120.06,8.74;8,105.50,651.51,38.64,8.74">CLEF 2009: Ad Hoc Track Overview</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">M</forename><surname>Di Nunzio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,165.84,651.51,166.77,8.74">CLEF 2009 Workshop: Working notes</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,670.93,407.50,8.74;8,105.50,682.89,407.50,8.74;8,105.50,694.84,407.50,8.74;8,105.50,706.80,93.29,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,451.84,670.93,61.15,8.74;8,105.50,682.89,227.56,8.74">Enhancing semantic search using n-levels document representation</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">L</forename><surname>Gentile</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Degemmis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Lops</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Semeraro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,211.49,694.84,44.31,8.74">SemSearch</title>
		<title level="s" coord="8,325.71,694.84,125.83,8.74">CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Bloehdorn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Grobelnik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Mika</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</editor>
		<imprint>
			<publisher>CEUR-WS.org</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">334</biblScope>
			<biblScope unit="page" from="29" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,726.22,407.50,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,224.15,726.22,143.29,8.74">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,388.18,726.22,24.84,8.74">TREC</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,745.64,406.99,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,152.54,745.64,183.61,8.74">Analyses of multiple evidence combination</title>
		<author>
			<persName coords=""><forename type="first">J.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,357.70,745.64,25.85,8.74">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="267" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,112.02,397.75,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,165.91,112.02,169.31,8.74">Wordnet: a lexical database for english</title>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,343.74,112.02,65.57,8.74">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,131.95,407.51,8.74;9,105.50,143.90,407.50,8.74;9,105.50,155.86,315.04,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,293.87,131.95,215.57,8.74">Simple bm25 extension to multiple weighted fields</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,117.55,143.90,395.45,8.74;9,105.50,155.86,99.12,8.74">CIKM &apos;04: Proceedings of the thirteenth ACM international conference on Information and knowledge management</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,175.78,407.51,8.74;9,105.50,187.74,407.50,8.74;9,105.50,199.69,84.69,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,316.72,175.78,196.28,8.74;9,105.50,187.74,185.23,8.74">A probabilistic model of information retrieval: development and comparative experiments</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,300.71,187.74,157.88,8.74">Information Processing Management</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="809" to="840" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,219.62,407.50,8.74;9,105.50,231.57,229.77,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,210.64,219.62,302.36,8.74;9,105.50,231.57,32.82,8.74">Improving the effectiveness of information retrieval with local context analysis</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,146.83,231.57,97.04,8.74">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="112" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
