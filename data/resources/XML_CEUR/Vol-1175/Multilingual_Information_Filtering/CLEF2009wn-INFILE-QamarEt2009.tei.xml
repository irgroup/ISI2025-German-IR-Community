<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,126.84,148.73,349.38,15.51;1,215.64,170.69,171.97,15.51">Batch Document Filtering Using Nearest Neighbor Algorithm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,190.32,204.41,82.62,9.62"><forename type="first">Ali</forename><forename type="middle">Mustafa</forename><surname>Qamar</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire d&apos;Informatique de Grenoble (LIG)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,281.54,204.41,56.87,9.62"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
							<email>eric.gaussier@imag.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire d&apos;Informatique de Grenoble (LIG)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,346.11,204.41,66.57,9.62"><forename type="first">Nathalie</forename><surname>Denos</surname></persName>
							<email>nathalie.denos@imag.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire d&apos;Informatique de Grenoble (LIG)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,126.84,148.73,349.38,15.51;1,215.64,170.69,171.97,15.51">Batch Document Filtering Using Nearest Neighbor Algorithm</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5FC6C09897D50B1029EC7C9ED58F9340</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software Measurement, Performance, Experimentation, Algorithms Information Filtering, Batch algorithms, k-Nearest neighbor algorithm</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the participation of LIG lab, in the batch filtering task for the INFILE (INformation FILtering Evaluation) campaign of CLEF 2009. As opposed to the online task, where the server provides the documents one by one, all of the documents are provided beforehand in the batch task, which explains the fact that feedback is not possible in the batch task. We propose in this paper a batch algorithm to learn category specific thresholds in a multiclass environment where a document can belong to more than one class. The algorithm uses k-nearest neighbor algorithm for filtering the 100,000 documents into 50 topics. The experiments were run on the English corpus. Our experiments gave us a precision of 0.256 while the recall was 0.295. We had participated in the online task in INFILE 2008 where we had used an online algorithm using the feedbacks from the server. In comparison with INFILE 2008, the recall is significantly better in 2009, 0.295 vs 0.260. However the precision in 2008 were 0.306. Furthermore, the anticipation in 2009 was 0.43 as compared with 0.307 in 2008.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of the INFILE (INformation FILtering Evaluation) <ref type="bibr" coords="1,368.69,661.73,10.40,9.62" target="#b1">[2]</ref> campaign is to filter 100,000 documents into 50 topics (plus a category 'other'). Out of 50 topics, 30 are related to general news and events (e.g. national and international affairs, sports, politics etc.), whereas the rest concern scientific and technical subjects. A document can belong to zero, one or more topics, each topic being described by a set of sentences. In comparison with INFILE 2009, where there was only an online task, an additional batch filtering task was added in 2009. As opposed to the online task, where the server provides the documents one by one to the user, all of the documents are provided beforehand in the batch task. This explains the fact that feedback is not possible in the batch task. We had participated in the online task in 2008 <ref type="bibr" coords="2,365.04,111.77,10.00,9.62" target="#b2">[3]</ref>, and restricted ourselves to the batch one in 2009.</p><p>The k-nearest neighbor (kNN) algorithm is a largely investigated supervised learning algorithm due to its simplicity and performance. It aims at finding the k nearest neighbors of an example x (based either on similarity or distance) and then finding the most represented class in the nearest neighbors in order to classify x. Similarity has deemed to be more appropriate as compared to distance, specially while dealing with texts. In such case, the cosine measure is used instead of euclidean or mahanalobis distances.</p><p>In this paper, we develop a batch algorithm to learn category-specific thresholds in a multiclass environment. Our algorithm uses kNN algorithm along with cosine similarity, in order to filter the documents into various topics.</p><p>The rest of the paper is organized as follows: Section 2 describes the batch algorithm developed for the INFILE campaign, experiments and results are discussed in Section 3 while we conclude in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Batch Algorithm for the INFILE Campaign</head><p>In order to filter the documents into various topics, we use a similarity measure between new documents and topics, along with a set of thresholds on this similarity that evolve over time. The similarity between a new document d, to be filtered, and a topic t i can given as:</p><formula xml:id="formula_0" coords="2,175.92,367.84,337.12,28.05">sim(t i , d) = α * cos(t i , d) s1(ti,d) +(1 -α) max (d ′ =d,d ′ ∈ti) cos(d, d ′ ) s2(ti,d)<label>(1)</label></formula><p>where α ∈ [0,1]. The similarity given in equation 1 is based on two similarities: one based on a direct similarity between the new document and the topic (given by s 1 (t i , d)), and another one between the new document and the set of documents already assigned to the topic (s 2 (t i , d). One might think that only the first similarity would suffice. However, this is not the case since the topics and the documents do not share the same kind of structure and content. The second similarity helps us to find documents which are closer to documents which had already been assigned to a topic. α is used to control the importance of the two similarities. In the beginning, when no documents are assigned to any topic, only the similarity, s 1 (t i , d), is taken into account. This similarity is used to find a certain number of nearest neighbors for each of the document (we used 10 nearest neighbors) which eventually helps us to use the second similarity. A threshold was used for each of the 50 topics. We now describe the batch algorithm to filter the documents. As already mentioned, the feedback is not possible in this case since the complete set of documents is transferred to the user in one go.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Construction of initial set:</head><p>for each topic i (i ∈{101,102,...,150}</p><p>find 10 nearest neighbors based on s 1 = cos(t i , d) for each nearest neighbor found</p><formula xml:id="formula_1" coords="2,125.04,627.91,39.43,10.26">t i ⇐ N N</formula><p>Assignment of remaining documents to topics:</p><formula xml:id="formula_2" coords="2,90.00,667.01,118.00,85.88">α = 0.7 for each topic i θ i = min d∈ti sim(t i , d)) for each document d for each topic i if (sim(t i , d) ≥ θ i ) t i ⇐ d θ i = min(θ i , min d∈ti sim(t i , d))</formula><p>Yang et al. <ref type="bibr" coords="3,153.61,141.41,10.45,9.62" target="#b4">[5]</ref> have described a similar method, whereby they learn category-specific thresholds based on a validation set. An example is assigned to a particular category only if its similarity with the category surpasses a certain learned threshold. In contrary, we do not have a validation set to learn thresholds, however, we create a simulated one, by finding nearest neighbors for each of the 50 categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Comparison with Online Campaign 08</head><p>We present here, a detailed comparison between the batch algorithm of 2009 and the online algorithm developed for the online campaign, 08. For time constraints, we were not able to participate in the online task this year. The online algorithm is presented below: where l i represents the number of documents assigned to a topic i. For each topic, two thresholds are used: the first one (θ 1 ) allows filtering the documents in the early stages of the process (when only a few documents have been assigned to the topic). The value chosen for this threshold was 0.42. The second threshold (θ 2 ), however, works with the global similarity, after a certain number of documents have been assigned to the topic. In addition, the algorithm makes use of the feedbacks (50 in total), so that only relevant documents are placed in the different topics.</p><formula xml:id="formula_3" coords="3,90.00,295.71,207.03,11.32">Construction of initial set: α = 0.7, θ 1 = 0.</formula><p>The main difference between the two algorithms (batch and online) lies in the manner in which we construct the initial set of documents pertinent to the topics. In batch algorithm, we just rely on finding the 10 nearest neighbors for each topic, with the assumption that the nearest neighbors for a topic, would in general, belong to the topic under consideration. However, for the online algorithm, we use feedbacks if the similarity between a topic t i and a document d, is greater than a certain threshold (θ 1 ). The rest of the algorithms are almost the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We have run our algorithm on INFILE English corpus. For all of the documents, stemming was performed using Porter's algorithm. This was followed by the removal of stop-words, XML tags skipping and the building of a document vector (which associates each term with its frequency) using Rainbow <ref type="bibr" coords="3,155.53,706.85,10.00,9.62" target="#b3">[4]</ref>. A single run was submitted during the INFILE campaign where α was chosen to be 0.7. Initially, 10 nearest neighbors were found for each of the document based on the similarity s 1 (between a document and the topic). These documents were subsequently used to find s 2 . The experiment was divided into 4 sub-parts, each sub-part being run in parallel to increase the There are 1597 documents relevant to one or more topics in the INFILE data. The results for the different runs were evaluated based on different measures, namely, precision, recall, F-measure, linear utility, anticipation (added in 2009) and detection cost (see <ref type="bibr" coords="4,378.58,620.45,10.57,9.62" target="#b0">[1]</ref> and <ref type="bibr" coords="4,411.59,620.45,10.31,9.62" target="#b1">[2]</ref>). Utility is based on two parameters: importance given to a relevant document retrieved and the cost of a non-relevant document retrieved. Anticipation measure is designed to give more importance to systems that can find the first document in a given profile. Figure <ref type="figure" coords="4,317.81,656.21,4.98,9.62">1</ref> give us an insight on the number of relevant documents retrieved during Run 1 and 2. We do not see a significant change for Run 1, in terms of the number of documents retrieved during the entire process. However, Run 2 returns much more documents between 10,000-20,000 and 80,000-90,000 documents. The evolution of these measures, computed at different times in the process, after each 10,000 documents, are given in the Figure <ref type="figure" coords="4,505.22,704.09,3.90,9.62">2</ref>. The curve, at the bottom represents the detection cost. Similarly, for Run 1, the curve just above the one meant for detection cost, describes the anticipation. For Run 1, all of the measures randomly vary but increase significantly as compared to the initial values (for example, 0.04 in the beginning vs 0.125 at the end for anticipation, 0.12 to 0.19 for the F-measure etc.) during the course of the filtering process. For Run 2, all of the measures, except utility and precision (0.18 vs 0.30), randomly vary but remain the same at the end. Table <ref type="table" coords="5,133.20,328.49,4.98,9.62" target="#tab_1">1</ref> describes the different runs along with the number of documents retrieved and the number of relevant documents found. The average score (the values are first computed for each of the 50 profiles and then averaged) is given in Table <ref type="table" coords="5,314.55,352.37,3.90,9.62">2</ref>. We can note that Run 1 has the best recall (0.295) as compared with the second run. The F-measure for the two runs is roughly the same. However, Run 2 surpasses Run 1 in terms of average precision. The overall detection cost is very low in these two runs, with Run 1, being more economical. This is a strong point for these runs. The linear utility of Run 2 is greater than that of Run 1. On contrary, the anticipation for Run 1 (0.430) is significantly better than that of Run 2 (0.307).</p><p>We can easily conclude from these results, that for the online algorithm, the initial set of documents can be constructed in the same manner as that for the batch algorithm. In this case, we can find 10 nearest neighbors for each topic instead of relying on feedbacks from the server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented, in this paper, a simple extension of the kNN algorithm using thresholds to define a batch filtering algorithm. The results obtained can be deemed encouraging as the Fmeasure equals approximately 20%, for a collection of 100,000 documents and 50 topics, out of which only 1597 documents are relevant. In comparison with online results of 2008, we have a much better recall (almost 30% against 26% in 2008) along with a lower detection cost (0.002 vs 0.007) and a much better anticipation (0.430 vs 0.307). Considering the evolution of different measures, we had observed that the values for all of the measures increase, with the increase in the number of documents filtered. Furthermore, the comparison between batch and online algorithms indicate that, while building the initial set of documents, one can use the nearest neighbors for each topic, rather than relying on feedbacks from the server.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,208.80,308.81,185.39,9.62;4,48.01,329.85,251.23,195.96"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Score Evolution for Run 1 and 2</figDesc><graphic coords="4,48.01,329.85,251.23,195.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,106.92,118.73,389.19,139.70"><head>Table 1 :</head><label>1</label><figDesc>Detail about the different runs</figDesc><table coords="5,106.92,138.41,389.19,120.02"><row><cell>Name</cell><cell cols="3">Campaign Algorithm</cell><cell cols="2">Doc. ret Doc. ret -relevant</cell></row><row><cell cols="2">Run 1 IMAG 1 Batch 09</cell><cell cols="2">Batch (w/o feedback)</cell><cell>5513</cell><cell>413</cell></row><row><cell>Run 2 run5G</cell><cell cols="4">Online 08 Online (with feedback) 7638</cell><cell>601</cell></row><row><cell></cell><cell></cell><cell cols="2">Table 2: Run Scores</cell><cell></cell><cell></cell></row><row><cell cols="6">Precision Recall F-measure Linear Utility Detection Cost Anticipation</cell></row><row><cell>Run 1 0.256</cell><cell>0.295</cell><cell>0.206</cell><cell>0.205</cell><cell>0.002</cell><cell>0.430</cell></row><row><cell>Run 2 0.306</cell><cell>0.260</cell><cell>0.209</cell><cell>0.351</cell><cell>0.007</cell><cell>0.307</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="5,105.48,665.21,407.55,9.62;5,105.48,677.09,407.52,9.62;5,105.48,689.09,375.25,9.62" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,186.27,677.09,173.81,9.62">Overview of clef 2008 infile pilot track</title>
		<author>
			<persName coords=""><forename type="first">Romaric</forename><surname>Besancon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stéphane</forename><surname>Chaudiron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Djamel</forename><surname>Mostefa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olivier</forename><surname>Hamon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ismail</forename><surname>Timimi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Khalid</forename><surname>Choukri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,386.76,677.09,126.24,9.62;5,105.48,689.09,183.32,9.62">Working Notes of the Cross Language Evaluation Forum (CLEF 2008)</title>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09-19">17-19 September 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.48,709.01,407.68,9.62;5,105.48,720.89,407.68,9.62;5,105.48,732.89,200.18,9.62" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,105.48,720.89,317.52,9.62">The infile project: a crosslingual filtering systems evaluation campaign</title>
		<author>
			<persName coords=""><forename type="first">Romaric</forename><surname>Besancon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stéphane</forename><surname>Chaudiron</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Djamel</forename><surname>Mostefa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ismail</forename><surname>Timimi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Khalid</forename><surname>Choukri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,105.48,732.89,102.73,9.62">Proceedings of LREC&apos;08</title>
		<editor>
			<persName><surname>Elra</surname></persName>
		</editor>
		<meeting>LREC&apos;08<address><addrLine>Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.48,111.77,407.68,9.62;6,105.48,123.77,407.55,9.62;6,105.48,135.65,331.09,9.62" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,353.64,111.77,159.53,9.62;6,105.48,123.77,224.39,9.62">Working notes for the infile campaign : Online document filtering using 1 nearest neighbor</title>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Bodinier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Mustafa Qamar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,349.92,123.77,163.11,9.62;6,105.48,135.65,139.16,9.62">Working Notes of the Cross Language Evaluation Forum (CLEF 2008)</title>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09-19">17-19 September 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.48,155.69,407.67,9.62;6,105.48,167.57,90.62,9.62" xml:id="b3">
	<monogr>
		<title level="m" type="main" coord="6,183.14,155.69,330.02,9.62;6,105.48,167.57,60.37,9.62">Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.48,187.49,407.41,9.62;6,105.48,199.49,163.57,9.62" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,229.12,187.49,212.22,9.62">A re-examination of text categorization methods</title>
		<author>
			<persName coords=""><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,464.76,187.49,44.17,9.62">SIGIR &apos;99</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
