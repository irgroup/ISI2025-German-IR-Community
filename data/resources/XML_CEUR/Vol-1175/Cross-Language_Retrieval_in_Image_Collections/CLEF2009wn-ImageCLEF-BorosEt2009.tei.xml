<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,151.08,142.31,309.48,11.85;1,291.96,158.75,27.96,11.85">UAIC: Participation in ImageCLEF 2009 Robot Vision Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,218.16,194.80,60.51,8.48"><forename type="first">Emanuela</forename><surname>Boroş</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,285.12,194.80,51.25,8.48"><forename type="first">George</forename><surname>Roşca</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,342.96,194.80,50.23,8.48"><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">UAIC</orgName>
								<orgName type="department" key="dep2">Faculty of Computer Science</orgName>
								<orgName type="institution">&quot;Alexandru Ioan Cuza&quot; University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,151.08,142.31,309.48,11.85;1,291.96,158.75,27.96,11.85">UAIC: Participation in ImageCLEF 2009 Robot Vision Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DAF26A2202559C6D556F07AB0D687F6F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This year marked our first participation at the Robot Vision task a new task from ImageCLEF competition. The paper represents a brief description of our system as the solution to the problem of topological localization of a mobile robot using visual information. We were asked to determine the topological location of a robot based on images acquired with a perspective camera mounted on a robot platform. And so, we decided that we don't need an incremental learning system and we approached a statistical method that always will work the best results. We used to apply a feature-based method (SIFT 1 ) and two main systems in order to search and classify the given images written by us. At the same time, the systems preserve the recognition performance of the batch algorithm. The first system is reordering the images so we can get the most important/representative images for a room's category. This is done using SIFT. The second system is a brute one, just for testing the differences between this one and the first one, not selecting the representative images. We acquired a separation in directories for every room capturing the key points saved in files for every image (or representative) from every room. About the changes in the environment, the SIFT algorithm occupies itself. The entire recognition system consists in a server -client architecture. Server is processing one single search at a time, and after the search ends connection with the client closes. The resulting file is the asked-for file with the final results for the batched images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>ImageCLEF <ref type="foot" coords="1,189.72,524.00,2.81,5.11" target="#foot_1">2</ref> hosted in 2009 for the first time a Robot Vision task. The task addresses the problem of topological localization of a mobile robot using visual information. Specifically, we/participants were asked to determine the topological location of a robot based on images acquired with a perspective camera mounted on a robot platform.</p><p>We received training data consisting of an image sequence recorded in a five room subsection of an indoor environment under fixed illumination conditions and at a given time. We had to build a system able to answer the question "where are you?" (with possible answers "I'm in the kitchen, in the corridor", etc.) when presented with a test sequence containing images acquired in the previously observed part of the environment or in additional rooms that were not imaged in the training sequence. The test images were acquired 6-20 months later after the training sequence, possibly under different illumination settings. The system should assign each test image to one of the rooms that were present in the training sequence or indicate that the image comes from a room that was not included during training. Moreover, the system can refrain from making a decision (e.g. in the case of lack of confidence).</p><p>The algorithm must be able to provide information about the location of the robot separately for each test image (e.g. when only some of the images from the test sequences are available or the sequences are scrambled). This corresponds to the problem of global topological localization. However, results can also be reported for the case when the algorithm is allowed to exploit continuity of the sequences and rely on the test images acquired before the classified image.</p><p>We started with the release of annotated training and validation data. Training and validation were performed on a subset of the publicly available IDOL2 Database <ref type="bibr" coords="2,454.92,291.76,10.12,8.48" target="#b0">[1]</ref>. The database contains image sequences acquired in a five room subsection of an office environment, under three different illumination settings and over a time frame of 6 months. The test sequences were acquired in the same environment, 20 months after the training data, and contain additional rooms that were not imaged previously.</p><p>The rest of the paper is organized as follows: after a review of previous literature in the field (Section 2), we describe our system (UAIC System) (Section 3). Section 4 describes the experiments and then Section 5 presents the results. Section 6 draws conclusions regarding our participation in Robot Vision task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>The research on mainly object recognition has been increasing mostly in the mobile robotics community. In this project, in the first iteration is essentially the Sift algorithm <ref type="bibr" coords="2,183.48,461.20,11.03,8.48" target="#b2">[3]</ref> by David Lowe implemented. Therefore, the papers <ref type="bibr" coords="2,406.20,461.20,10.12,8.48" target="#b0">[1]</ref>, <ref type="bibr" coords="2,423.60,461.20,10.79,8.48" target="#b2">[3]</ref> (and by Lowe and referenced), and reference implementations were studied. The main objective of this paper represents, as mentioned before, a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features have to be invariant to a lot of changes that image must suffer.</p><p>In computer vision and image processing, these insights have lead to the construction of multi-scale representations of image data, obtained by embedding any given signal into a one-parameter family of derived signals <ref type="bibr" coords="2,385.92,547.72,41.29,8.48">(Burt 1981</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">UAIC System</head><p>Module details for our system of dynamical place recognition are presented in the sections below. The architecture of the system is similar to server-client architecture, and it is possible to accomplish more requests at a time. This change has brought performance improvement.</p><p>The vision of the project is a source of information all about local points of interest via the above channels to offer. The purpose of information and visualized objects is to organize and communicate valuable data to people, so they can derive increased knowledge that guides their thinking and behavior. Human learning processes mediate the building of knowledge from meaningful information and integration into one's knowledge base. Therefore, a proper understanding of human learning is important to consider while making any decision. Our need in imitating the human capability of learning has become the main purpose of science. And so, the problem proposed to solve is the problem of topological localization of a mobile robot using visual information acquired with a perspective camera mounted on a robot platform. We didn't choose a mechanism of incremental learning, we chose a statistical one as the people learn through observation, trial-and-error and experiment. As we now, learning happens during interaction. We managed the images ("interactions with objects" for human learning) so they become a mini-system of storing features of them.</p><p>This paper, as it has been said before, presents an algorithm able to recognize places on the basis of images' features, under possible variations between matching image pairs. Translations, rotations, scales and luminance changes can cause the difference of two pictures. It is virtually impossible to compare two images using traditional methods such as a direct comparison between gray values as it could be really simple with an existing API (Java Advanced Imaging API <ref type="foot" coords="4,385.80,151.04,2.81,5.11" target="#foot_2">3</ref> ).</p><p>In addition, this paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features have to be invariant to a lot of changes that image must suffer.</p><p>The goal of the iteration is a clear breakdown of the project objectives into individual fragments, which all run independently. We have decided the timing Features -boxed, and the key aspects in the beginning to be realized. Each iteration ends with a functional prototype.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Server Module</head><p>This module has two parts: one part necessary for training and one part necessary for classifying. The trainer supports both single images and directories. The images must be annotated in the given format. We did not use an existing tool, but our code (Java), based mostly on finding the keypoints (points of interest) of an image. The preparation for the trainer is scaling down the images, plus the well-known SIFT algorithm applied to them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Trainer component</head><p>The trainer supports both single images and directories and it is detecting and describing local features in images. The 'scale-invariant feature transform' features are local and based on the appearance of the object at particular interest points, and are invariant to image scale and rotation. In the process of training, all the chosen images will get through the key point localization processor, obtaining the keys' files. These files are included in testing the application.</p><p>In the process of training, all the chosen images will get through the key point localization processor, obtaining the keys' files. These files are included in testing the application. Additional to brute force in which all pictures are considered in training process, we have added a "get representative images" method. In this way, the trainer obtains for a single room that initially has 400 images, just 25 images. After that we have trained our application twice (that took us 2-4 days): one for all pictures and one for representative images. The brute trainer is analyzing all the images. More details are under SIFT algorithm subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">SIFT Algorithm</head><p>In the first iteration is essentially the SIFT algorithm [] implemented by Lowe David. Therefore, the papers (and by Lowe and referenced), and reference implementations were studied. Where no precise definition in the paper is available, values must be determined empirically. Finally, the stability and efficiency of the implementation will be tested. In addition, a review will take place whether the algorithm for the detection of three dimensional objects is appropriate. SIFT -Scale Invariant Feature Transform is a feature-based image matching approach, which lessens the damaging effects of image transformations to a certain extent. Features extracted by SIFT are invariant to image scaling and rotation, and partially invariant to photometric changes.</p><p>Four stages throughout the computation procedure as follows:</p><p>1. Scale-space extrema detection: first, use difference-of-Gaussian (DOG) to approximate Laplacian-of-Gaussian and build the image pyramid in scale space. Determine the keypoint candidates by local extrema detection. This is done by comparing each pixel in the DoG images to its eight neighbors at the same scale and nine corresponding neighboring pixels in each of the neighboring scales. If the pixel value is the maximum or minimum among all compared pixels, it is selected as a candidate keypoint. Scale-space extrema detection produces too many keypoints candidates, some of which are unstable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Strip unstable keypoints: use the Taylor expansion of the scale-space function</head><p>to reject those points that are not distinctive enough or are unsatisfactorily located near the edge. 3. Feature description: Local image gradients and orientations are computed around keypoints. A set of orientation, scale and location for each keypoint is used to represent it, which is significantly invariant to image transformations and luminance changes. 4. Feature matching: compute the feature descriptors in the target image in advance and store all the features in a shape-indexing feature database. To initiate the matching process for the new image, repeat steps 1-3 above and search for the most similar features in the database.</p><p>First of all, the images are mapped and then they are scaled up at double of their size.</p><p>After we assume that the image has a blur of at least 0.5 so an initial image smoothing is passed. This is the stage where the interest points, which are called keypoints in the SIFT framework, are detected. For this, the image is convolved with Gaussian filters at different scales, and then the differences of successive Gaussian-blurred images are taken. Keypoints are then taken as maxima/minima of the Difference of Gaussians (DoG) that occur at multiple scales. For scale-space extrema detection, the image is first convolved with Gaussian-blurs at different scales. The value of octave sigma detection parameter is 2.0, how was suggested by Lowe's research paper.</p><p>The convolved images are grouped by octave (the largest possible number of octaves), each holding levels per octave = 3 scales in scale-space. Each octave is downscaled by 0.5 and the scales in each octave represent a sigma change of to 2.0 * sigma octave. Then the Difference-of-Gaussian images are taken from adjacent Gaussian-blurred images per octave. Once DoG images have been obtained, keypoints are identified as local minima/maxima of the DoG images across scales. This is done by comparing each pixel in the DoG images to its eight neighbors at the same scale and nine corresponding neighboring pixels in each of the neighboring scales. If the pixel value is the maximum or minimum (in the images maps) among all compared pixels, it is selected as a candidate keypoint. Maxima and minima of the difference-of-Gaussian images are detected by comparing a pixel to its 26 neighbors in 3x3 regions at the current and adjacent scales (marked with circles). From Lowe's paper <ref type="bibr" coords="6,167.28,151.24,11.03,8.48" target="#b0">[1]</ref> it's not really clear whether we always need 3 neighborhoods spaces (3x3 regions) or should also search only one or two spaces.</p><p>This keypoint detection step is a variation of one of the blob detection methods developed by Lindeberg by detecting scale-space extrema of the scale normalized <ref type="bibr" coords="6,454.92,183.76,10.12,8.48" target="#b1">[2]</ref>. The difference of Gaussians operator can be seen as an approximation to the Laplacian, <ref type="bibr" coords="6,186.72,205.24,11.03,8.48" target="#b4">[5]</ref> here expressed in a pyramid setting. Scale-space extrema detection produces too many keypoint candidates, some of which are unstable. The next step in the algorithm is to perform a detailed fit to the nearby data for accurate location, scale, and ratio of principal curvatures <ref type="bibr" coords="6,302.88,237.76,10.12,8.48" target="#b3">[4]</ref>. This information allows points to be rejected that have low contrast (and are therefore sensitive to noise) or are poorly localized along an edge. After the edge filtering, each keypoint is assigned one or more orientations based on local image gradient directions. This is the key step in achieving invariance to rotation as the keypoint descriptor can be represented relative to this orientation and therefore achieves invariance to image rotation. All keypoints are written to files that represent the database for the server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Classifier component</head><p>In this iteration, the database for the management of point of interests will be created. The SIFT algorithm is designed is used to browse the database. It is also on the efficiency respected. The training data is from IDOL Database <ref type="bibr" coords="6,397.08,362.08,10.12,8.48" target="#b5">[6]</ref>. Access to the database is done by the server. The server loads once with all the keypoints files and waits for requests.</p><p>The brute finder/trainer is one type of classifier as it creates and loads all the meta files into memory.</p><p>The managed one creates the representative meta files for the representative images from the batch. First of all, when it gets through all the steps that we explained at SIFT algorithm subsection, it chooses only the images that have the almost 10-16 percent similarities with images treated before. In the end, we obtain only 10 from 50 -60 images appreciatively, also 10 meta files (with the keypoints for them), for the most representative images, in this case, every room that has been loaded as a training directory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Client Module</head><p>The client module is the tester and it has two phases: a naive one and a more precise one. This implies comparison at its bases. This module only sends one image to the server and receives a list of results that represents, in this case, the list with the rooms where the images for testing belong.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>In Robot Vision task participants were submitted 21 runs and we submitted 5 runs. Our runs details are presented in next table:</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,220.44,393.27,170.66,7.70;3,143.28,138.48,326.52,252.96"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: UAIC system used in Robot Vision task</figDesc><graphic coords="3,143.28,138.48,326.52,252.96" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,148.32,635.10,138.21,7.66"><p>SIFT: Scale Invariant Feature Transform</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="1,148.32,644.94,136.03,7.66"><p>ImageCLEF: http://www.imageclef.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="4,161.28,623.44,133.16,8.48;4,156.00,634.24,208.16,8.48"><p>Java Advanced Imaging API (JAI): http://java.sun.com/javase/technologies/desktop/media/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors would like to thank to the students Ştefan Orzu and <rs type="person">Bogdan Catrinescu</rs> for their help and support at different stages of system development.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The results were more explicit on the brute finder even though it took a lot of time to complete the training. The 'get representative' method didn't give the expected results, but it is faster like time duration in comparison with the brute method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This paper presents the UAIC system which took part in the Robot Vision task. We used to apply a feature-based method (SIFT) and two main systems in order to search and classify the given images.</p><p>The first system uses the most important/representative images for a room's category. The second system is a brute force one. The entire recognition system consists in a server -client architecture. Server is processing one single search at a time, and after the search ends connection with the client closes.</p><p>The results shows how the brute method is better like quality of results in comparison with get representative image method, but it is much slower like time duration. For the future we will try to do a combination between these two methods.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="8,146.43,166.50,321.83,7.66;8,153.96,176.22,180.77,7.66" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,202.32,166.50,185.01,7.66">Object recognition from local scale-invariant features</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,403.80,166.54,64.47,7.53;8,153.96,176.26,156.14,7.53">proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,146.43,185.94,321.99,7.66;8,153.96,195.78,264.07,7.66" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,204.72,185.94,164.50,7.66">Feature detection with automatic scale selection</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<ptr target="http://www.nada.kth.se/cvap/abstracts/cvap198.html" />
	</analytic>
	<monogr>
		<title level="j" coord="8,385.08,185.98,83.35,7.53;8,153.96,195.82,56.34,7.53">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,146.43,205.50,321.99,7.66;8,153.96,215.10,285.91,7.66" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,203.28,205.50,203.63,7.66">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<ptr target="http://citeseer.ist.psu.edu/lowe04distinctive.html" />
	</analytic>
	<monogr>
		<title level="j" coord="8,423.72,205.54,44.71,7.53;8,153.96,215.14,94.82,7.53">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,146.44,224.82,322.07,7.66;8,153.96,234.66,314.47,7.66;8,153.96,244.38,204.31,7.66" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,286.68,224.82,181.83,7.66;8,153.96,234.66,50.13,7.66">Real-time scale selection in hybrid multi-scale representations</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Bretzner</surname></persName>
		</author>
		<ptr target="http://www.nada.kth.se/cvap/abstracts/cvap279.html" />
	</analytic>
	<monogr>
		<title level="m" coord="8,222.96,234.70,73.16,7.53">Proc. Scale-Space&apos;03</title>
		<title level="s" coord="8,303.96,234.70,160.98,7.53">Springer Lecture Notes in Computer Science</title>
		<meeting>Scale-Space&apos;03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,146.43,254.10,252.62,7.66" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="8,204.84,254.10,127.76,7.66">Scale-space theory in computer vision</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Kluwer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,146.43,263.82,322.00,7.66;8,153.96,273.54,263.47,7.66" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="8,348.72,263.82,75.45,7.66">The IDOL2 database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pronobis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jensfelt</surname></persName>
		</author>
		<ptr target="http://cogvis.nada.kth.se/IDOL2/" />
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>CAS/CVAP, Tech. Rep</publisher>
		</imprint>
		<respStmt>
			<orgName>KTH</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
