<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,103.02,148.86,396.95,15.15;1,117.96,170.78,367.09,15.15;1,236.10,192.69,130.80,15.15">Text-and Content-based Approaches to Image Retrieval for the ImageCLEF 2009 Medical Retrieval Track</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,187.59,225.23,117.12,10.48"><roleName>Md</roleName><forename type="first">Matthew</forename><surname>Simpson</surname></persName>
							<email>simpsonmatt@mail.nih.gov</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National Center for Biomedical Communications National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">NIH</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,308.61,225.23,100.19,10.48"><forename type="first">Mahmudur</forename><surname>Rahman</surname></persName>
							<email>rahmanmm@mail.nih.gov</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National Center for Biomedical Communications National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">NIH</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,150.88,239.18,115.91,10.48"><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National Center for Biomedical Communications National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">NIH</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,276.94,239.18,73.86,10.48"><forename type="first">Sameer</forename><surname>Antani</surname></persName>
							<email>santani@mail.nih.gov</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National Center for Biomedical Communications National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">NIH</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,360.18,239.18,91.94,10.48"><forename type="first">George</forename><forename type="middle">R</forename><surname>Thoma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National Center for Biomedical Communications National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">NIH</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,148.78,256.11,51.92,10.48"><forename type="first">Lister</forename><surname>Hill</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National Center for Biomedical Communications National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">NIH</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,103.02,148.86,396.95,15.15;1,117.96,170.78,367.09,15.15;1,236.10,192.69,130.80,15.15">Text-and Content-based Approaches to Image Retrieval for the ImageCLEF 2009 Medical Retrieval Track</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6B93C72971DC2AEEEFD6BCB3A39681A6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.7 Digital Libraries</term>
					<term>I.4.8 [Image Processing and Computer Vision]: Scene Analysis-Object Recognition Measurement, Performance, Experimentation Image Retrieval, CBIR, Medical Imaging, Ontologies, UMLS</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes the participation of the Image and Text Integration (ITI) group from the United States National Library of Medicine (NLM) in the ImageCLEF 2009 medical retrieval track. Our methods encompass a variety of techniques relating to document summarization and text-and content-based image retrieval. Our text-based approach utilizes the Unified Medical Language System (UMLS) synonymy of concepts identified in information requests and image-related text to retrieve semantically relevant images. Our content-based approaches utilize similarity metrics based on computed "visual concepts" to identify visually similar images. In this article we present an overview of these approaches, discuss our experiences combining them into multimodal retrieval strategies, and describe our submitted runs and results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This article describes the participation of the Image and Text Integration (ITI) group from the United States National Library of Medicine (NLM) in the ImageCLEF 2009 medical retrieval track. This is our second year participating in ImageCLEFmed.</p><p>ImgeCLEFmed'09 <ref type="bibr" coords="1,187.04,702.26,15.50,8.74" target="#b16">[17]</ref> consists of two medical retrieval tasks. In the first task, a set of ad-hoc information requests are given, and the goal is to retrieve the most relevant images pertaining to each topic. In the second task, a set of case-based information requests are given, and the goal is to retrieve the most relevant articles describing case studies pertaining to the topic case.</p><p>In the following sections, we describe our text-based approach (Section 2), which is suitable for both retrieval tasks, and several content-based approaches (Section 3) to the ad-hoc retrieval task. Our text-based approach relies on mapping information requests and image-related text to concepts in the Unified Medical Language System (UMLS) <ref type="bibr" coords="2,351.06,147.89,15.50,8.74" target="#b12">[13]</ref> Metathesaurus, and our contentbased approaches analogously rely on mapping medical images to "visual concepts" using machine learning techniques.</p><p>In Section 4, we suggest strategies for combining our text-and content-based approaches, describe our submitted runs, and present their results. For the ad-hoc retrieval task, our best run, a multimodal feedback approach, achieved a Mean Average Precision (MAP) of 0.38, and our best automatic run, a text-based approach, achieved a MAP of 0.35. For the case-based retrieval task our automatic text-based approach achieved a MAP of 0.34 and was ranked 1st among all case-based run submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Text-based Image Retrieval</head><p>In this section we describe our text-based approach to image retrieval. Effective text-based medical image retrieval requires (1) a document representation that contains the most pertinent information describing the content of the image and potential information needs and (2) a retrieval strategy that is appropriate for the biomedical domain.</p><p>Our document representation consists of several automatically extracted search areas in addition to the image captions provided in the ImageCLEFmed'09 <ref type="bibr" coords="2,361.86,358.07,15.50,8.74" target="#b16">[17]</ref> collection. These fields include the title of the article in which the image appears, the article's abstract, a brief mention (one sentence) of the image from the article's full text, and the Medical Subject Headings (MeSH terms) assigned to the article. MeSH is a controlled vocabulary created by NLM to index biomedical articles. We provide a summary of each caption according to a structured representation of information needs that are relevant to the principles of evidence-based practise <ref type="bibr" coords="2,421.95,417.85,9.96,8.74" target="#b6">[7]</ref>. This search area includes automatically extracted fields relating to anatomy, diagnosis, population group, etc.</p><p>We use the Essie <ref type="bibr" coords="2,183.36,441.76,15.50,8.74" target="#b11">[12]</ref> search engine to index this collection of image documents and retrieve relevant images. Essie was originally developed by NLM to support the online registry of clinical research studies at ClinicalTrials.gov <ref type="bibr" coords="2,256.57,465.67,14.61,8.74" target="#b14">[15]</ref>, and now it serves several other information retrieval systems at NLM. Key features of Essie that make it particularly well-suited to the medical retrieval track include its automatic expansion of query terms along synonymy relationships in the UMLS Metathesaurus and its ability to weight term occurrences according the location of the document in which they occur. For example, term occurrences in an image caption can be given a higher weight than occurrences in the abstract of the article in which the image appears. Essie also expands query terms to include morphological variants derived from the UMLS SPECIALIST Lexicon instead of stemming.</p><p>To construct queries for each information request, we map topics to the UMLS using the MetaMap <ref type="bibr" coords="2,135.50,573.26,10.52,8.74" target="#b0">[1]</ref> tool and represent terms relating to image modality, clinical findings, and anatomy with their preferred UMLS names. Thus, each query consists of the conjunction of a set of UMLS concepts that are expanded by Essie during the retrieval process. For extracted modality terms that cannot be mapped to the UMLS, we perform an automatic term expansion based on a list of image modalities (originally created by the authors using RadLex<ref type="foot" coords="2,379.80,619.51,3.97,6.12" target="#foot_0">1</ref> as a starting point <ref type="bibr" coords="2,467.59,621.08,12.36,8.74" target="#b5">[6]</ref>) which we expanded using the UMLS synonymy and manually augmented with missing terms (mostly abbreviations) based on the authors' experience creating the ITI modalities hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Case-based Retrieval Task</head><p>Our retrieval strategy for the case-based retrieval task is identical to that of the ad-hoc task. However, since the retrieval unit of the case-based task is an entire article, to construct an appropriate document representation we perform a simple union of all the search areas for each image in the article. That is, a case-based document consists of a title, abstract, MeSH terms, and the caption, mention, and structured caption summary of each image contained in the article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Content-based Image Retrieval</head><p>In content-based image retrieval (CBIR), access to information is performed at a perceptual level based on automatically extracted low-level features (e.g., color, texture, shape, etc.) <ref type="bibr" coords="3,470.93,190.38,14.61,8.74" target="#b18">[19]</ref>. The performance of a CBIR system depends on the underlying image representation, usually in the form of a feature vector. Due to the limitations of the low-level features in CBIR and motivated by a learning paradigm, we explore classification at both the global collection level and the local individual image level in our submitted runs for ImageCLEFmed'09 <ref type="bibr" coords="3,403.32,238.20,14.61,8.74" target="#b16">[17]</ref>. In addition to the off-line supervised learning approach, we incorporate users' semantic perceptions interactively in the retrieval loop based on relevance feedback (RF) information. The following sections describe our feature representation schemes and the retrieval methods applied to the various visual and multimodal submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Feature Representation</head><p>To generate the feature vectors at different levels of abstraction, we extract both visual conceptbased feature based on a "bag of concepts" model comprising color and texture patches from local image regions <ref type="bibr" coords="3,152.63,355.89,15.50,8.74" target="#b20">[21]</ref> and various low-level global features including color, edge, and texture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Visual Concept-based Image Representation</head><p>In the ImageCLEFmed'09 collection <ref type="bibr" coords="3,251.03,399.86,14.61,8.74" target="#b17">[18]</ref>, it is possible to identify specific local patches in images that are perceptually and/or semantically distinguishable, such as homogeneous texture patterns in grey level radiological images and varying color and texture structures in microscopic pathology and dermoscopic images. The content of these local patches can be effectively modeled as "visual concepts" <ref type="bibr" coords="3,136.27,447.68,15.50,8.74" target="#b20">[21]</ref> by using supervised learning based classification techniques such as the Support Vector Machine (SVM).</p><p>For concept model generation, we utilize a voting-based multi-class SVM known as one-againstone or pairwise coupling (PWC) <ref type="bibr" coords="3,239.46,483.54,14.61,8.74" target="#b10">[11]</ref>. In developing training samples for this SVM, only local image patches that map to visual concept models are used. To accurately automatically segment and unambiguously and consistently label image segments, a fixed-partition based approach is used to divide the entire image space into an (r × r) grid of non-overlapping regions. Manual selection is applied to limit such patches in the training set to those that have a majority of their area (80%) covered by a single concept. In order to train the SVMs based on the local concept categories, a set of L labels are assigned as</p><formula xml:id="formula_0" coords="3,90.00,555.27,422.29,20.69">C = {c 1 , • • • , c i , • • • , c L }, where each c i ∈ C characterizes a local concept category.</formula><p>The training set of the local patches that comprise color and texture moment-based features, is annotated manually with the concept labels in a mutually exclusive way. Images in the data set are annotated with local concept labels by partitioning each image I j into an equivalent r × r grid of l region vectors as</p><formula xml:id="formula_1" coords="3,90.00,603.07,423.00,21.64">{x 1j , • • • , x kj , • • • , x lj }, where each x kj ∈ d is a d-dimensional combined</formula><p>color and texture feature vector. For each x kj , the local concept category probabilities are determined by the prediction of the multi-class SVMs:</p><formula xml:id="formula_2" coords="3,229.88,647.05,283.12,9.68">p ikj = P (y = i | x kj ), 1 ≤ i ≤ L.<label>(1)</label></formula><p>The category label of x kj is determined by the maximum probability score. Thus, the entire image is represented as a two-dimensional index linked to the concept labels assigned for each region.</p><p>Based on this encoding scheme, an image I j can be represented as a vector in a local concept space as</p><formula xml:id="formula_3" coords="3,230.01,712.75,282.99,15.05">f Concept j = [f 1j , • • • , f ij , • • • f Lj ] T<label>(2)</label></formula><p>where each f ij corresponds to the normalized frequency of a concept c i ,</p><formula xml:id="formula_4" coords="3,410.85,733.69,102.15,9.65">1 ≤ i ≤ L in image I j .</formula><p>The feature vector f Concept is viewed as a local concept distribution from a probabilistic viewpoint.</p><p>According to the notion of total probability <ref type="bibr" coords="4,283.77,112.02,9.96,8.74" target="#b8">[9]</ref>, an element f ij can be defined as</p><formula xml:id="formula_5" coords="4,230.69,133.17,282.31,30.55">f ij = l kj =1 P i|kj P k = 1 l l kj =1 P i|kj<label>(3)</label></formula><p>where P k is the probability of a region selected from image I j being the k j th region, which is 1/l, and P i|kj is the conditional probability that the selected k j th region in I j maps to the concept c i . In the context of the concept vector f concept j , the value of P i|kj is 1 if the region k j is mapped to the c i concept, or 0 otherwise. Due to the crisp membership value, this feature representation is sensitive to quantization errors. However, based on the probabilistic values of each region, an image I j is represented as</p><formula xml:id="formula_6" coords="4,196.97,235.25,316.03,55.80">f PVCV j = [ f1j • • • fij • • • fLj ] T , where fij = l k=1 p ikj P k = 1 l l k=1 p ikj ; for i = 1, 2, • • • , L<label>(4)</label></formula><p>where p ikj is determined based on <ref type="bibr" coords="4,237.88,301.60,11.62,8.74" target="#b0">(1)</ref>. In contrast to the simple concept vector f concept , this vector representation considers not only the similarity of different region vectors from different concepts but also the dissimilarity of those region vectors mapped to the same concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Low Level Global Feature Representation</head><p>In addition to the visual concepts of local image patches, we extract the following global features:</p><p>• Color Feature: To represent the spatial structure of images, we utilize the Color Layout Descriptor (CLD) of MPEG-7 <ref type="bibr" coords="4,253.70,402.32,9.96,8.74" target="#b2">[3]</ref>. The CLD represents the spatial layout of the images in a compact form. It is obtained by applying the discrete cosine transformation (DCT) on the 2D array of local representative colors in the YCbCr color space, where Y is the luma component and Cb and Cr are the blue and red chroma components. Each channel is represented by 8 bits and each of the 3 channels is averaged separately for the 8 × 8 image blocks. We extract a CLD with 10 Y , 3 Cb, and 3 Cr to form a 16-dimensional feature vector.</p><p>Images may also be represented as Color Coherence Vector (CCV) <ref type="bibr" coords="4,414.23,486.01,14.61,8.74" target="#b19">[20]</ref>, where a particular color's coherence is defined as the degree to which pixels of that color are members of large similarly-colored regions. A CCV stores the number of coherent versus incoherent pixels with each color. By separating coherent pixels from incoherent pixels, CCV's provide finer distinctions than color histograms. In addition, a histogram of edge direction is constructed, where the edge information contained in the images is processed and generated by using the Canny edge detection algorithm (with σ = 1, Gaussian masks of size = 9, low threshold = 1, and high threshold = 255). The corresponding edge directions are quantized into 72 bins of 5 • each. Scale invariance is achieved by normalizing the histograms with respect to the number of edge points in the image.</p><p>• Texture Feature: We extract texture features from the grey level co-occurrence matrix (GLCM) <ref type="bibr" coords="4,157.01,721.73,15.50,8.74" target="#b9">[10]</ref> of each image. In order to obtain efficient descriptors, the information contained in GLCM is traditionally condensed into a few statistical features. Four GLCM's for four different orientations (horizontal 0 • ,vertical 90 • , and two diagonals-45 • and 135 • ) are obtained and normalized to the entries [0,1] by dividing each entry by total number of pixels. Higher order features, such as energy, entropy, contrast, homogeneity and maximum probability are measured based on averaging features in GLCMs to form a 20-dimensional feature vector for an entire image.</p><p>• Average Grey Level Feature:</p><p>For different categories or within the same category, images in a collection may vary in size and undergo translations. Resizing them into a thumbnail of a fixed size can reduce the translational error and some of the noise due to the artifacts present in the images, especially for images in medical domain. Hence, a feature extraction is performed from the low-resolution scaled images where each image is converted to a gray-level image (one channel only) and scaled down to the size 64 × 64 regardless of the original aspect ratio. Next, the down-scaled image is partitioned further with a 16 × 16 grid to form small blocks of (4 × 4) pixels. The average gray value of each block is measured and concatenated to form a 256-dimensional feature vector.</p><p>• Other Features: We extract two additional features using the Lucene image retrieval (LIRE) library <ref type="bibr" coords="5,183.16,299.93,15.50,8.74" target="#b13">[14]</ref> including the Color Edge Direction Descriptor (CEDD) and the Fuzzy Color Texture Histogram (FCTH). CEDD incorporates color and texture information into one single histogram and requires low computational power compared to MPEG-7 descriptors. To extract texture information, CEDD uses a fuzzy version of the five digital filters proposed by the MPEG-7 EHD, which forms 6 texture areas <ref type="bibr" coords="5,377.31,347.75,9.96,8.74" target="#b3">[4]</ref>. This descriptor is appropriate for retrieving images even in cases with deformation, noise and smoothing. In contrast, FCTH uses the high frequency bands of the Haar Wavelet Transform in a fuzzy system to form 8 texture areas <ref type="bibr" coords="5,206.92,383.61,9.96,8.74" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fusion-based Image Similarity Matching</head><p>It is difficult to find a unique representation to compare images accurately for all types of queries. Feature descriptors at different levels of image representation are in diverse forms and are often complementary in nature. Data fusion, or multiple-evidence combination, describes a range of techniques where multiple pieces of information are combined to achieve improvements in retrieval effectiveness <ref type="bibr" coords="5,146.07,477.71,9.96,8.74" target="#b7">[8]</ref>. CBIR also adopts some of the ideas from data fusion, where the most commonly used approach is a linear combination of similarity matching scores of different features with predetermined weights. In this framework, the similarity between a query image I q and target image I j is described as Sim(I q , I j ) =</p><formula xml:id="formula_7" coords="5,292.69,525.58,220.32,22.37">F ω F Sim F (I q , I j )<label>(5)</label></formula><p>where F ∈ {Concept, EHD, CLD, CCV, CEDD, FCTH, etc.} and ω F are the weights within the different image representations. We now present several linear combination schemes including ones based on the online category prediction of a query image and on relevance feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Category-Specific Similarity Fusion</head><p>In this approach, the category of a query image at a global level is determined based on the SVM learning on a training set of 5000 images of 32 manually assigned and mutually exclusive categories from the ImageCLEFmed'05-07 collections <ref type="bibr" coords="5,279.11,648.25,14.61,8.74" target="#b15">[16]</ref>. Images are classified into three levels of detail as shown in Figure <ref type="figure" coords="5,164.17,660.20,3.87,8.74" target="#fig_1">1</ref>. For the SVM training, the radial basis function (RBF) is used and a 10-fold cross-validation is conducted to find the best tunable parameters C and γ of the RBF kernel. Only the best performing features are used in SVM classification. Our SVM implementation is based on the LIBSVM package <ref type="bibr" coords="5,202.07,696.07,9.96,8.74" target="#b1">[2]</ref>.</p><p>Based on the online categorization of a query image, precomputed category-specific feature weights (e.g., ω F ) are subsequently utilized in the linear similarity matching function. Based on this scheme, for example, a color feature will have more weight for microscopic pathology and dermatology images, whereas edge-and texture-related features will have more weight for radiographs.</p><p>In addition, to find the optimal weights we consider the 10-fold cross validation accuracy of each feature. The accuracies are based on SVM classification of the images in the training set of 5000 images. The weights are normalized based on the accuracies of the features subject to 0 ≤ ω F ≤ 1 and ω F = 1 for F ∈ {Concept, EHD, CLD, CCV, CEDD, FCTH, etc.}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Image Similarity Fusion Based on Relevance Feedback (RF)</head><p>We used a feedback-based similarity fusion technique, where feature weights are updated at each iteration by considering both the precision and the rank order of relevant images in individual result lists. As a result, the final rank-based retrieval is obtained through an adaptive and linear weighted combination of overall similarity, fusing individual level similarities. In this approach, to update the feature weights (e.g., ω F ), we first perform similarity matching based on equal weighting of each feature. After this initial retrieval, a relevance judgement is manually provided for the top K returned images. We then measure the effectiveness of the top K images as</p><formula xml:id="formula_8" coords="6,242.23,457.64,266.52,25.41">E = K i=1 Rank(i) K/2 × P (K) (<label>6</label></formula><formula xml:id="formula_9" coords="6,508.75,467.48,4.24,8.74">)</formula><p>where Rank(i) = 0 if the image in rank position i is not relevant and Rank(i) = (K -i)/(K -1) for relevant images. Hence, the function Rank(i) is monotonically decreasing from one (if the image at rank position one is relevant) down to zero (e.g., for a relevant image at rank position K). On the other hand, P (K) = R K /K is the precision at the top K, where R k is the number of relevant images in the top K retrieved results. Hence, equation ( <ref type="formula" coords="6,370.93,538.38,4.24,8.74" target="#formula_8">6</ref>) is basically the product of two factors: rank order and precision. The rank order factor takes into account the position in the retrieval set of the relevant images, whereas the precision is a measure of the retrieval accuracy, regardless of the position. Generally, the rank order factor is heavily biased for the position in the ranked list over the total number of relevant images, and the precision value ignores the rank order of the images. To balance both criteria, we use a performance measure that is the product of the rank order factor and precision. If there is more overlap between the relevant images of a particular retrieval set and the one provided through feedback, the performance score will be higher. Both terms on the right side of equation ( <ref type="formula" coords="6,309.34,634.03,4.24,8.74" target="#formula_8">6</ref>) will be one if all the top K returned images are considered relevant. The raw performance scores obtained by the above procedure are then normalized by the total score as Ê = ωF to generate the updated feature weights respectively. For the next iteration of retrieval with the same query, these modified weights are utilized for the similarity matching function by</p><formula xml:id="formula_10" coords="6,229.13,701.75,283.87,22.37">Sim(I q , I j ) = F ωF Sim F (I q , I j )<label>(7)</label></formula><p>This weight updating process might be continued as long as relevance judgements are available or until no changes are noticed due to the system convergence.</p><p>This section provides descriptions and retrieval results of our submitted textual and visual runs as well as our attempts at integrating the text-based and CBIR-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ad-hoc Retrieval Task</head><p>We submitted the following 9 runs for the 25 ad-hoc topics <ref type="bibr" coords="7,350.91,192.07,14.61,8.74" target="#b16">[17]</ref>:</p><p>1. ceb essie2 automatic: This is a textual run utilizing the approach described in Section 2.</p><p>Based on our previous experience with the ImageCLEFmed'08 <ref type="bibr" coords="7,393.41,223.95,15.50,8.74" target="#b17">[18]</ref> collection, we weighted the caption and title search areas more heavily than the other areas.</p><p>2. cbir fusion category: This is a visual run based on the category-specific similarity fusion approach described in Section 3.2.1. For this run, we selected only one query image for each topic and considered all features for similarity fusion as described in Section 3.1.</p><p>For each query, the category was determined based on SVM trained on 5000 images from ImageCLEFmed'05-07 collections <ref type="bibr" coords="7,264.15,303.66,14.61,8.74" target="#b15">[16]</ref>. The individual preassigned feature weights were selected based on the category-specific rules and utilized in the linear combination of similarity matching functions.</p><p>3. cbir fusion merge: This is a visual run similar to the above (cbir fusion category), but instead of only considering one image for each topic, we considered every query image for each topic and generated separate ranked lists for each retrieval result. For each topic, we took the top 500 retrieved images corresponding to each query image and merged them into a single ranked list for the topic.</p><p>4. cbir fusion cv merge: This is a visual run similar to the above (cbir fusion merge), but instead of utilizing category-specific rules for feature weights, we found the optimal weights by considering the normalized cross validation accuracies of each feature as described in Section 3.2.1. We merged the top 500 retrieved images for each query image into a single ranked list as before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head><p>. multimodal text qe cbir: This is a mixed run that combines the approaches described in ceb essie2 interactive and cbir fusion category. For each topic, we first performed the textual search. We then manually selected 3-5 of the highest ranked retrieved images as relevant. Finally, we computed the mean vector of these retrieved images and used it as the query for the visual search.</p><p>6. multimodal text rerank: This is a mixed run that combines the approaches described in ceb essie2 interactive and cbir fusion category. For each topic, we first performed the textual search and then re-ranked the retrieved images based on the scores of the visual search.</p><p>7. ceb interactive with pad: This is a mixed run that interactively combines all of the above approaches <ref type="bibr" coords="7,166.14,606.52,9.08,8.74" target="#b0">(1)</ref><ref type="bibr" coords="7,175.22,606.52,4.54,8.74" target="#b1">(2)</ref><ref type="bibr" coords="7,175.22,606.52,4.54,8.74" target="#b2">(3)</ref><ref type="bibr" coords="7,175.22,606.52,4.54,8.74" target="#b3">(4)</ref><ref type="bibr" coords="7,175.22,606.52,4.54,8.74" target="#b4">(5)</ref><ref type="bibr" coords="7,179.76,606.52,9.08,8.74" target="#b5">(6)</ref> in a text-based relevance feedback approach. For each topic, we manually selected relevant images from the top ten retrieved images of each of the above approaches.</p><p>We then selected additional query terms from the document representation of the relevant images (described in Section 2), and used this expanded query as the input to the textual search described in ceb essie2 automatic. We ranked these additional retrieved images below the ones manually selected as relevant.</p><p>8. text manual cbir rf: This is a mixed run similar to the approach described in multimodal text qe cbir. However, instead of manually choosing 3-5 images from the textual retrieval results, we automatically selected the top 5 images from ceb interactive with pad. We computed the mean vector of these images and used it as the input query to the approach described in in cbir fusion category. cbir rf: This is a visual feedback approach based on cbir fusion category. We manually selected 5 highly ranked images from the visual retrieval results as relevant. We then computed the mean vector of these retrieved images and used it as the query for another iteration of the visual search.</p><p>Table <ref type="table" coords="8,117.29,393.54,4.98,8.74" target="#tab_0">1</ref> presents the results of our submitted runs for the ad-hoc topics. ceb interactive with pad, a multimodal relevance feedback approach, achieved the highest precision (MAP = 0.38) of our submitted runs. This run was ranked 1st among all submitted multimodal approaches and 1st among all feedback approaches, although its MAP is lower than some automatic runs submitted by other participating groups. The noticeable increase in Precision at 5 retrieved images (P@5) of ceb interactive with pad is inherent in its retrieval strategy-the highest ranked images were manually selected from the top 10 retrieved images from 6 other approaches. ceb essie2 automatic (MAP = 0.35) ranked 14th among automatic textual runs (ITI the 5th ranked group). cbir fusion merge (MAP = 0.01) ranked 1st among submitted visual runs although this result is likely not statistically significant. Finally, among multimodal automatic approaches, multimodal text rerank (MAP = 0.27) ranked 8th (ITI the 4th ranked group).</p><p>For our three best runs, we evaluated the statistical significance of the increase in precision using the two-sided Wilcoxon signed rank test. At the 0.05 significance level, the differences in precision between ceb interactive with pad and ceb cases essie2 automatic and between ceb cases essie2 automatic and multimodal text rerank are not significant (p = 0.059 and p = 0.057, respectively), which is consistent with the null hypothesis of having the same mean. However, ceb interactive with pad significantly improves the precision of multimodal text rerank (p &lt; 0.001).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Case-based Retrieval Task</head><p>We submitted the following run for the 5 case-based topics <ref type="bibr" coords="8,350.13,631.10,14.61,8.74" target="#b16">[17]</ref>:</p><p>1. ceb cases essie2 automatic: This is a textual run based on the approach described in Section 2. We weighted the caption, title and anatomy search areas heavier than the other areas and favored articles indexed with MeSH terms indicative of case studies or clinical trials. Examples of such terms include "Case Reports," "Case-Control Studies" and "Cross-Sectional Studies" among several others. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,104.94,550.04,408.06,8.77;4,114.91,562.03,398.09,8.74;4,114.91,573.98,398.10,8.74;4,114.91,585.94,398.09,8.74;4,114.91,597.89,263.70,8.74;4,378.60,596.32,4.08,6.12;4,387.71,597.89,59.07,8.74;4,446.78,596.32,4.08,6.12;4,455.89,597.89,57.10,8.74;4,114.91,609.85,398.09,8.74;4,114.91,621.80,151.49,8.74"><head>•</head><label></label><figDesc>Edge Feature: To represent the global edge feature, the spatial distribution of edges are utilized by the Edge Histogram Descriptor (EHD)<ref type="bibr" coords="4,348.15,562.03,9.96,8.74" target="#b2">[3]</ref>. The EHD represents local edge distribution in an image by dividing the image into 4 × 4 sub-images and generating a histogram from the edges present in each of these sub-images. Edges in the image are categorized into five types-namely vertical, horizontal, 45 • diagonal, 135 • diagonal and non-directional edges. Finally, a histogram with 16 × 5 = 80 bins is obtained, corresponding to a 80-dimensional feature vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,187.25,229.44,228.50,8.74;6,110.70,109.18,381.60,105.15"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Classification structure of the training set.</figDesc><graphic coords="6,110.70,109.18,381.60,105.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,94.73,116.09,413.54,176.64"><head>Table 1 :</head><label>1</label><figDesc>Results of the 9 Submitted Runs for the Ad-hoc Task</figDesc><table coords="8,94.73,116.09,413.54,176.64"><row><cell>File Name</cell><cell>ID</cell><cell>Mode</cell><cell>Type</cell><cell cols="2">Recall MAP P@5</cell></row><row><cell cols="2">ITI 26 8 1244841659565.txt ceb interactive with pad</cell><cell>Mixed</cell><cell>Feedback</cell><cell>0.65</cell><cell>0.38 0.74</cell></row><row><cell cols="2">ITI 26 8 1243447590820.txt ceb essie2 automatic</cell><cell cols="2">Textual Automatic</cell><cell>0.66</cell><cell>0.35 0.65</cell></row><row><cell cols="2">ITI 26 8 1244811028909.txt multimodal text rerank</cell><cell cols="2">Mixed Automatic</cell><cell>0.66</cell><cell>0.27 0.49</cell></row><row><cell cols="2">ITI 26 8 1244842970604.txt text manual cbir rf</cell><cell>Mixed</cell><cell>Feedback</cell><cell>0.21</cell><cell>0.04 0.28</cell></row><row><cell cols="2">ITI 26 8 1244811851777.txt multimodal text qe cbir</cell><cell>Mixed</cell><cell>Manual</cell><cell>0.19</cell><cell>0.04 0.27</cell></row><row><cell cols="2">ITI 26 8 1244813032166.txt cbir fusion cv merge</cell><cell cols="2">Visual Automatic</cell><cell>0.12</cell><cell>0.01 0.09</cell></row><row><cell cols="2">ITI 26 8 1244813305029.txt cbir fusion merge</cell><cell cols="2">Visual Automatic</cell><cell>0.12</cell><cell>0.01 0.08</cell></row><row><cell cols="2">ITI 26 8 1244846828228.txt cbir rf</cell><cell>Visual</cell><cell>Feedback</cell><cell>0.13</cell><cell>0.01 0.06</cell></row><row><cell cols="2">ITI 26 8 1244812535094.txt cbir fusion category</cell><cell cols="2">Visual Automatic</cell><cell>0.13</cell><cell>0.01 0.06</cell></row><row><cell>File Name</cell><cell>ID</cell><cell>Mode</cell><cell>Type</cell><cell cols="2">Recall MAP P@5</cell></row><row><cell cols="4">ITI 26 8 1243520633864.txt ceb cases essie2 automatic Textual Automatic</cell><cell>0.78</cell><cell>0.34 0.32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,102.18,307.86,338.40,38.63"><head>Table 2 :</head><label>2</label><figDesc>Results of the Submitted Run for the Case-based Task 9.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="8,90.00,718.77,423.00,32.65"><head>Table 2</head><label>2</label><figDesc>presents the retrieval results of our submitted case-based run. ceb cases essie2 automatic achieved a MAP of 0.34 and was ranked 1st among all case-based submissions. sinai TA cbt (MAP = 0.26) was ranked 2nd.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,734.11,93.64,6.99"><p>http://radlex.org/viewer.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors wish to thank <rs type="person">Haiying Guan</rs>, <rs type="person">Rodney L. Long</rs> and <rs type="person">Zhiyun Xue</rs> for their valuable input during group discussions and time spent preparing relevance judgements for our interactive experiments.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes the retrieval strategies of the ITI group for the ImageCLEF 2009 medical retrieval track. For the ad-hoc task, we submitted 9 runs that include various combinations of our text-and content-based approaches in different retrieval scenarios. We submitted one automatic textual run for the case-based task. Many of our submitted runs were successful-most notably our case-based run, which was ranked 1st among all case-based run submissions.</p><p>Our results indicate that content-based approaches to image retrieval are not yet advanced enough to achieve the precision of text-based approaches, and in many cases can reduce the precision of text-based approaches when combined in a multimodal automatic scheme. However, precision can be improved by combining text-and content-based approaches in relevance feedback retrieval scenarios.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,110.48,374.88,402.52,8.74;9,110.48,386.84,402.52,8.74;9,110.48,398.79,176.17,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,187.54,374.88,325.46,8.74;9,110.48,386.84,81.40,8.74">Effective mapping of biomedical text to the UMLS metathesaurus: The MetaMap program</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">R</forename><surname>Aronson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,219.98,386.84,293.02,8.74;9,110.48,398.79,86.49,8.74">Proc. of the Annual Symp. of the American Medical Informatics Association (AMIA)</title>
		<meeting>of the Annual Symp. of the American Medical Informatics Association (AMIA)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,418.72,402.52,8.74;9,110.48,430.67,244.61,8.74" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="9,233.46,418.72,207.32,8.74">LIBSVM: A library for support vector machines</title>
		<author>
			<persName coords=""><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/∼cjlin/libsvm/" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,450.60,402.52,8.74;9,110.48,462.55,296.36,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,271.97,450.60,149.68,8.74">Overview of the MPEG-7 standard</title>
		<author>
			<persName coords=""><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Puri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,430.03,450.60,82.97,8.74;9,110.48,462.55,199.37,8.74">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="688" to="695" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,482.48,402.52,8.74;9,110.48,494.43,402.52,8.74;9,110.48,506.39,402.52,8.74;9,110.48,518.34,402.52,8.74;9,110.48,530.30,105.07,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,307.20,482.48,205.80,8.74;9,110.48,494.43,241.59,8.74">CEDD: Color and edge directivity descriptor: A compact descriptor for image indexing and retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,211.26,506.39,301.74,8.74;9,110.48,518.34,32.72,8.74">Proceedings of the 6th International Conference on Computer Vision Systems</title>
		<title level="s" coord="9,219.14,518.34,151.16,8.74">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Gasteratos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Vincze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</editor>
		<meeting>the 6th International Conference on Computer Vision Systems<address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5008</biblScope>
			<biblScope unit="page" from="312" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,550.22,402.52,8.74;9,110.48,562.18,402.52,8.74;9,110.48,574.13,333.65,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,295.01,550.22,217.99,8.74;9,110.48,562.18,176.55,8.74">FCTH: Fuzzy color and texture histogram: A low level feature for accurate image retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><forename type="middle">S</forename><surname>Boutalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,309.16,562.18,203.84,8.74;9,110.48,574.13,235.34,8.74">Proceedings of the 9th International Workshop on Image Analysis for Multimedia Interactive Services</title>
		<meeting>the 9th International Workshop on Image Analysis for Multimedia Interactive Services</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="191" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,594.06,402.52,8.74;9,110.48,606.01,402.52,8.74;9,110.48,617.97,402.52,8.74;9,110.48,629.93,22.69,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,392.34,594.06,120.66,8.74;9,110.48,606.01,329.03,8.74">Combining medical domain ontological knowledge and low-level image features for multimedia indexing</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Thoma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,461.66,606.01,51.34,8.74;9,110.48,617.97,340.83,8.74">Proc. of the Language Resources for Content-Based Image Retrieval Workshop (OntoImage)</title>
		<meeting>of the Language Resources for Content-Based Image Retrieval Workshop (OntoImage)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="18" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,649.85,402.53,8.74;9,110.48,661.81,302.79,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,265.81,649.85,247.19,8.74;9,110.48,661.81,89.32,8.74">Answering clinical questions with knowledge-based and statistical techniques</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,208.60,661.81,113.07,8.74">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="103" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,681.73,402.52,8.74;9,110.48,693.69,235.83,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,227.64,681.73,142.71,8.74">Combination of multiple searches</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Shaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,390.57,681.73,122.43,8.74;9,110.48,693.69,136.37,8.74">Overview of the Second Text Retrieval Conference (TREC-2)</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,110.48,713.61,402.52,8.74;9,110.48,725.57,22.69,8.74" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="9,175.17,713.61,200.41,8.74">Introduction to Statistical Pattern Recognition</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct coords="10,110.48,112.02,402.52,8.74;10,110.48,123.98,325.17,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,331.79,112.02,177.37,8.74">Textural features for image classification</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,110.48,123.98,233.31,8.74">IEEE Transactions on Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="610" to="621" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,143.90,402.52,8.74;10,110.48,155.86,89.11,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,243.10,143.90,150.16,8.74">Classification by pairwise coupling</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,403.61,143.90,105.55,8.74">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="451" to="471" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,175.78,402.52,8.74;10,110.48,187.74,402.52,8.74;10,110.48,199.69,84.13,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,345.38,175.78,167.61,8.74;10,110.48,187.74,132.47,8.74">Essie: A concept-based search engine for structured biomedical text</title>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">C</forename><surname>Ide</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">F</forename><surname>Loane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,254.37,187.74,254.24,8.74">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="253" to="263" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,219.62,402.52,8.74;10,110.48,231.57,213.76,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,310.67,219.62,158.01,8.74">The unified medical language system</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lindberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Mccray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,477.33,219.62,35.67,8.74;10,110.48,231.57,116.69,8.74">Methods of Information in Medicine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="281" to="291" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,251.50,402.52,8.74;10,110.48,263.45,402.52,8.74;10,110.48,275.41,73.61,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,269.98,251.50,243.02,8.74;10,110.48,263.45,27.16,8.74">LIRe: Lucene image retrival-an extensible java CBIR library</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Lux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">A</forename><surname>Chatzichristofis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,164.50,263.45,314.73,8.74">Proceedings of the 16th ACM International Conference on Multimedia</title>
		<meeting>the 16th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1085" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,295.33,402.52,8.74;10,110.48,307.29,340.59,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,239.80,295.33,269.31,8.74">Design and implementation of a national clinical trials registry</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">T</forename><surname>Mccray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><forename type="middle">C</forename><surname>Ide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,110.48,307.29,248.75,8.74">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="313" to="323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,327.21,402.52,8.74;10,110.48,339.17,402.52,8.74;10,110.48,351.12,142.64,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,472.29,327.21,40.71,8.74;10,110.48,339.17,296.70,8.74">Overview of the imageCLEFmed 2007 medical retrieval and annotation tasks</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Deserno</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,430.79,339.17,82.22,8.74;10,110.48,351.12,111.52,8.74">Working Notes for the CLEF 2007 Workshop</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,371.05,402.52,8.74;10,110.48,383.00,402.52,8.74;10,110.48,394.96,117.80,8.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,202.61,383.00,258.82,8.74">Overview of the CLEF 2009 medical image retrieval track</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Radhouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bakke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">E</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jr</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,486.36,383.00,26.64,8.74;10,110.48,394.96,87.17,8.74">CLEF Working Notes 2009</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,414.88,402.52,8.74;10,110.48,426.84,402.52,8.74;10,110.48,438.79,95.89,8.74" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,472.29,414.88,40.72,8.74;10,110.48,426.84,249.97,8.74">Overview of the imageCLEFmed 2008 medical image retrieval task</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">E</forename><surname>Kahn</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hatt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,383.44,426.84,129.56,8.74;10,110.48,438.79,64.77,8.74">Working Notes for the CLEF 2008 Workshop</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,458.72,402.52,8.74;10,110.48,470.68,402.52,8.74;10,110.48,482.63,215.12,8.74" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="10,368.11,458.72,144.89,8.74;10,110.48,470.68,338.73,8.74">A review of content-based image retrieval systems in medical applications-clinical benefits and future directions</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Michoux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bandon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Geissbuhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,456.08,470.68,56.93,8.74;10,110.48,482.63,133.23,8.74">International Journal of Medical Informatics</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,502.56,402.52,8.74;10,110.48,514.51,387.22,8.74" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="10,273.14,502.56,217.31,8.74">Comparing images using color coherence vectors</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Pass</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,110.48,514.51,298.37,8.74">Proceedings of the 4th ACM International Conference on Multimedia</title>
		<meeting>the 4th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="65" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,534.44,402.52,8.74;10,110.48,546.39,402.52,8.74;10,110.48,558.35,241.91,8.74" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="10,303.82,534.44,209.17,8.74;10,110.48,546.39,186.83,8.74">A medical image retrieval framework in correlation enhanced visual concept feature space</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Thoma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,320.36,546.39,192.65,8.74;10,110.48,558.35,211.23,8.74">Proceedings of the 22nd IEEE International Symposium on Computer-Based Medical Systems</title>
		<meeting>the 22nd IEEE International Symposium on Computer-Based Medical Systems</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
