<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,150.96,49.76,301.10,12.93">XRCE&apos;s Participation in ImageCLEF 2009</title>
				<funder ref="#_agGYKyV">
					<orgName type="full">French National</orgName>
				</funder>
				<funder ref="#_jcVKS3X">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,152.16,87.47,65.29,9.62"><forename type="first">Julien</forename><surname>Ah-Pine</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 chemin de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,228.00,87.47,85.38,9.62"><forename type="first">Stephane</forename><surname>Clinchant</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 chemin de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">LIG</orgName>
								<orgName type="institution">Univ. Grenoble I</orgName>
								<address>
									<addrLine>BP 53</addrLine>
									<postCode>-38041</postCode>
									<settlement>Grenoble cedex 9, Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,330.12,87.47,70.76,9.62"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 chemin de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,411.36,87.47,35.07,9.62"><forename type="first">Yan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xerox Research Centre Europe</orgName>
								<address>
									<addrLine>6 chemin de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,150.96,49.76,301.10,12.93">XRCE&apos;s Participation in ImageCLEF 2009</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5062C48013B6D82B64DCACDBC0A71DE6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes XRCE's participation in Large Scale Visual Concept Detection and Annotation Task [16] and Photo Retrieval Task [17]  of ImageCLEF 2009. Those tasks both use a new collection which is different and which is much larger than the ones used in past sessions. Moreover, new kinds of challenge to tackle were designed such as new categories to detect or new types of topic to deal with. Accordingly, our main motivations regarding our participation in this year's session are two fold. First, we wanted to apply ongoing work in our team in image and text processing. Second, we wanted to figure out if our cross-media approach and our diversity re-ranking techniques developped in past sessions, can perform well on a new challenging corpus. It turns out that the material that we describe in this paper both made of new and already well-established techniques allow to perform very well on those two tasks since the results we obtained with the systems we designed are top ranked.</p><p>3 Notice that this is systematically done through our cross-media technique. 4 The Cross-Entropy multiplied by -1 actually.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This year, we participated in two main tasks: the Large Scale Visual Concept Detection and Annotation Task (detailed in <ref type="bibr" coords="1,206.90,386.99,15.58,9.62" target="#b15">[16]</ref>) and the Photo Retrieval Task (detailed in <ref type="bibr" coords="1,415.10,386.99,14.81,9.62" target="#b16">[17]</ref>).</p><p>For the visual concept detection task, the main research questions were two-fold. First, the focus this year lied on the extension of the task concerning the amount and the diversity of concepts (53) used as types of annotation. There were also different kinds of concept such as abstract categories (landscape, family and friends, party life, ...), seasons, time of day, persons (no, single, big groups), quality (blurred, underexposed) and representation (portrait, macro image, canvas). Secondly, the aim was to assess whether a given ontology of concepts could help in classification or not.</p><p>Despite the fact that the concepts were presented in a hierarchy with a given ontology, we have not used neither the hierarchy nor the ontology in the training phase. Indeed we considered the task first as a multi-class multi-label image categorization problem and trained binary one versus all (OVA) classifiers for each concept. To train the classifiers, we used the Fisher Representation of images (as last year). However, we also used the adapted image GMMs which were combined with Fisher Representation by late fusion technique (score averaging). In the test phase and for each image, the classification scores were finally post-processed in order to ensure that the constraints of the ontology were not violated.</p><p>Similarly, the Photo Retrieval Task sets two new challenges: the scalability of the methods in order to cope with 500,000 images and the diversity of the top retrieved results. The basic methodologies used -Language Model for text retrieval, Fisher Vector image representation, transmedia fusion based cross-media similarity and clustering based re-ranking of top results -were ones already employed in previous years but with slight modifications and adaptations to better fit the actual task.</p><p>Furthermore, we had to design some new strategies to cope with on one hand, the large amount of data; on the other hand, the lack of information concerning the clusters in the second part of queries:</p><p>-Regarding the scalability problem, instead of pre-computing the whole cross-media similarity matrix off-line (as was done last year), we compute a cross-media similarity matrix "on-line" for each individual query (sub-)topic. This is actually done in two steps. First, we used a text-based retrieval technique. Then, we used the top list obtained from the latter in order to compute visual similarities and cross-media similarities as well. It is the computation of visual similarities that suffers the most from scalability issues. Accordingly, we abandoned the computation of similarity matrices over the whole collection of size 500, 000 Ã— 500, 000 since that was estimated to be very costly both in speed and storage. -Regarding the second part of queries, no cluster information was available. In that case, this is somehow similar to the approach we experimented for last year task as we did not use the cluster description. Therefore, we re-used some of our last year strategies to promote diversity among the top list namely clustering and KNN density based re-ranking. Furthermore it is worth noticing that if we use only the captions of the images queries 3 , we would get results illustrating only the corresponding sub-topics. Thus, to increase our chances to find new relevant sub-topics, we combined different sources of information. Basically, we combine the query title and the captions of images. This would ensure that the retrieved elements are relevant to the query topic but somehow this would also allow to have more diverse elements. Furthermore, we propose to enrich the query with terms that are the most related in the corpus using some standard term similarities.</p><p>The paper is structured as follows. Section 2 described the textual retrieval models. In section 3, we present the Fisher and adapted GMM representation of images. We recall our cross-media similarity measure in section 4 and our diversity seeking methods in 5. Before concluding, a detailed description of our runs in both challenges are presented in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Text Retrieval</head><p>We start from a traditional bag-of-word representation of pre-processed texts: pre-processing includes tokenization, lemmatization, and standard stopword removal. Two information retrieval models were considered: a standard language models (as our previous participation) and an information based model on a log-logistic distribution. We then present a query expansion mechanism that appeared to be relevant to the Photo Retrieval Task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Language Models</head><p>The idea of language models is to represent queries and documents by multinomial distributions <ref type="bibr" coords="2,90.00,478.79,15.61,9.62" target="#b21">[22,</ref><ref type="bibr" coords="2,107.16,478.79,12.85,9.62" target="#b19">20]</ref> .Those distributions are estimated by maximizing the likelihood. Then, documents distributions are smoothed by a Dirichlet Prior <ref type="bibr" coords="2,293.32,490.79,15.61,9.62" target="#b21">[22]</ref> and the Cross-Entropy 4 can be used to rank documents according to:</p><formula xml:id="formula_0" coords="2,174.84,526.08,338.20,20.87">sim txt (D 1 , D 2 ) = CE(D 1 |D 2 ) = w p(w|D 1 ) log(p(w|D 2 ))<label>(1)</label></formula><p>where D 1 and D 2 are two texts and w is a term from the bag-of-word representation. Notice that in the context of information retrieval, D 1 is the query that contains only a few keywords in comparison to a document. However, we also use language models to measure the similarity between pairs of documents in the collection so we rather introduce more general notations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Log-Logistic Model</head><p>We used in this paper a new family of IR models similar in spirit to DFR models <ref type="bibr" coords="2,459.84,633.35,10.00,9.62" target="#b3">[4]</ref>, which is based on probability distributions fitting well empirical data, and satisfying heuristic retrieval constraints <ref type="bibr" coords="2,140.76,657.23,10.57,9.62" target="#b5">[6,</ref><ref type="bibr" coords="2,153.01,657.23,7.05,9.62" target="#b6">7]</ref>.</p><p>The general idea of this family is the following one: due to different document length, discrete term frequencies are renormalized into continuous values as in <ref type="bibr" coords="3,365.64,64.67,10.00,9.62" target="#b3">[4]</ref>. Let m be the mean document length and y D the length of document D, x D w is the raw number of occurrences of term w in document D. Then, the normalized frequencies are:</p><formula xml:id="formula_1" coords="3,253.20,107.14,90.99,23.77">t D w = x D w log(1 + m y D )</formula><p>For each term w, we assume that those renormalized values follow a probability distribution P .</p><p>In our case, we suppose a log-logistic distribution. The log-logistic distribution is given by:</p><formula xml:id="formula_2" coords="3,233.52,174.90,279.52,24.55">P LL (X &lt; t; r, Î² = 1) = t Î² t Î² + r Î²<label>(2)</label></formula><p>and the parameter r w is estimated with: r w = Nw N , where N w is the number of document where w occurs in and N is the total number of document in the collection.</p><p>Finally, queries and documents are compared through a measure of surprise, or a mean of information of the following form:</p><formula xml:id="formula_3" coords="3,180.36,271.02,141.27,22.73">sim txt (D 1 , D 2 ) = wâˆˆD1âˆ©D2 -x D1</formula><p>w log(P LL (X â‰¥ t D2 w ; r w ))</p><formula xml:id="formula_4" coords="3,180.36,297.78,332.68,28.97">sim txt (D 1 , D 2 ) = wâˆˆD1âˆ©D2 x D1 w log( t D2 w + r w r w )<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Lexical Entailment / Term Similarity</head><p>Textual queries were very short with typical length of 1 or 2 words. In general, single keyword queries can be ambiguous. Query expansion techniques could help in finding several meanings or different contexts of the query word. As one of the goal was to promote diversity for the Photo Retrieval Task, query expansion methods could help in finding new clusters. In fact, if a term has several meanings or different contexts, the most similar words to this term should partially reflect the diversity of related topics associated to it. The Chi-Square statistics was used to measure the similarity between two words <ref type="bibr" coords="3,216.84,444.23,14.70,9.62" target="#b12">[13]</ref>, although any other term similarity measure or lexical entailment measure could be used. Recall that for two terms u, v, one can fill in a 2 by 2 contingency table with the number of documents that contains u and v, only u, only v or neither of them.</p><p>As the number of clusters were not known before-hand for the second part of queries, we decided to use this Chi-Square measure to only enrich the text queries of the second part. Hence, for each query word q w , we computed the Chi-Square statistics of the latter with all other words (including q w ). We kept only the top ten words and divided the scores by the maximum value (given by the inner statistic of q w with itself). Table <ref type="table" coords="3,263.29,528.59,4.98,9.62" target="#tab_0">1</ref> displays, for some query terms, the most similar terms with the renormalized Chi-Square statistics. To illustrate that co-occurrence measures can handle diversity of word senses, one can look at the most similar terms of the euro term. The most similar term bear the notion of lottery, currency or football event, which were somehow also indicated in the topic images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Image Representation</head><p>In this section, we first describe briefly our visual vocabulary modeled by Gaussian Mixture Model (section 3.2). Then, we present the Fisher Vector representation (section 3.2) which was used in both Visual Concept Detection Task (section 6.1) and Photo Retrieval Task (section 6.2). Finally, we present an alternative representation of images using adapted GMMs which was only used in the Visual Concept Detection Task (section 6.1). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Visual Vocabulary</head><p>We model the visual vocabulary <ref type="bibr" coords="4,231.62,228.11,15.61,9.62" target="#b20">[21,</ref><ref type="bibr" coords="4,248.78,228.11,7.81,9.62" target="#b7">8]</ref> with a Gaussian Mixture Model (GMM) where each Gaussian corresponds to a visual word <ref type="bibr" coords="4,236.54,240.11,10.57,9.62" target="#b8">[9,</ref><ref type="bibr" coords="4,248.78,240.11,11.71,9.62" target="#b18">19]</ref>. Let Î» = {w i , Âµ i , Î£ i , i = 1, ..., N } be the set of parameters of P where w i , Âµ i and Î£ i denote respectively the weight, mean vector and covariance matrix of Gaussian i and where N denotes the number of Gaussians. Let P i be the distribution of Gaussian i so that we have</p><formula xml:id="formula_5" coords="4,217.08,286.26,291.70,30.89">P (x) = N i=1 w i P i (x) = N i=1 w i N (Âµ i , Î£ i ). (<label>4</label></formula><formula xml:id="formula_6" coords="4,508.78,296.51,4.25,9.62">)</formula><p>The visual vocabulary is supposed to describe the content of any image and, therefore, it is trained off-line on a varied set of images. Let X = {x t , t = 1, ..., T } be the set of training observations extracted from these images. The estimation of Î» is performed by maximizing the log-likelihood function log P (X|Î») under an independence assumption:</p><formula xml:id="formula_7" coords="4,239.04,383.10,274.00,30.77">log P (X|Î») = T t=1 log P (x t |Î»)<label>(5)</label></formula><p>as described in <ref type="bibr" coords="4,157.92,425.27,14.70,9.62" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fisher Representation of Images</head><p>As image representation, we use the Fisher Vector proposed in <ref type="bibr" coords="4,372.12,479.51,14.70,9.62" target="#b17">[18]</ref>. This is an extension of the bag-of-visual-words representation. The main idea is to characterize the image I (represented by a set of samples x t ) with the gradient of the log-likelihood âˆ‡ Î» log P (I|Î»). In the case of a GMM with isotropic covariance matrix (see section 3.1 and <ref type="bibr" coords="4,322.45,515.39,14.80,9.62" target="#b17">[18]</ref>), we have the following formulas:</p><formula xml:id="formula_8" coords="4,209.64,537.66,299.15,30.77">âˆ‚L(I|Î») âˆ‚w i = T t=1 Î³ t (i) w i - Î³ t (1) w 1 for i â‰¥ 2 , (<label>6</label></formula><formula xml:id="formula_9" coords="4,508.79,547.91,4.25,9.62">)</formula><formula xml:id="formula_10" coords="4,209.64,572.46,299.15,30.77">âˆ‚L(I|Î») âˆ‚Âµ d i = T t=1 Î³ t (i) x d t -Âµ d i (Ïƒ d i ) 2 , (<label>7</label></formula><formula xml:id="formula_11" coords="4,508.79,582.71,4.25,9.62">)</formula><formula xml:id="formula_12" coords="4,209.64,607.38,299.15,30.65">âˆ‚L(I|Î») âˆ‚Ïƒ d i = T t=1 Î³ t (i) (x d t -Âµ d i ) 2 (Ïƒ d i ) 3 - 1 Ïƒ d i . (<label>8</label></formula><formula xml:id="formula_13" coords="4,508.79,617.51,4.25,9.62">)</formula><p>where Î³ t (i) = P (i|x t , Î»), the superscript d denotes the d-th dimension of a vector and Ïƒ d i are the diagonal elements of Î£ i . In practice, we only use the partial derivatives with respect to the means and standard deviations since adding the partial derivative with respect to the mixture weights does not improve accuracy.</p><p>The main advantage of this representation, which we will call Fisher vector, is that it transforms a variable length sample (number of local patches in the image) into a class independent fixed length representation whose size is only dependent on the number of parameters in the model (Î»).</p><p>Before feeding these vectors to a classifier or computing similarities between images, each vector is first normalized using the Fisher Information matrix F Î» (see <ref type="bibr" coords="5,362.75,100.79,15.61,9.62" target="#b17">[18]</ref> for the computational details):</p><formula xml:id="formula_14" coords="5,248.04,121.89,265.00,15.13">f I = F -1/2 Î» âˆ‡ Î» log P (I|Î»)<label>(9)</label></formula><p>with</p><formula xml:id="formula_15" coords="5,203.04,168.18,187.93,12.29">F Î» = E XP âˆ‡ Î» log P (I|Î»)âˆ‡ Î» log P (I|Î») T .</formula><p>and then re-normalized to have an L1-norm equal to 1.</p><p>In the case of image retrieval, to obtain the similarity between two images I 1 and I 2 , we compute the L1-norm of the two Fisher vectors:</p><formula xml:id="formula_16" coords="5,149.76,238.74,363.28,22.49">sim img (I 1 , I 2 ) = norm max -||f I1 -f I2 || 1 = norm max - i |f i I1 -f i I2 |<label>(10)</label></formula><p>where f i are the elements of the normalized vector f and norm max = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Images as Adapted Mixtures of Gaussian</head><p>We adapt here the method proposed in <ref type="bibr" coords="5,263.43,325.55,14.70,9.62" target="#b11">[12]</ref>, where each image is represented by a GMM adapted from a common "universal" GMM (Î») using the maximum a posteriori (MAP) criterion. Let I = {x t , t = 1, ..., T } denote the set of "adaptation samples" extracted from one image. The goal of MAP estimation is to maximize the posterior probability P (Î» a |I) or equivalently log P (I|Î» a ) + log P (Î» a ), where Î» a is an "adaptation" of Î» (see <ref type="bibr" coords="5,304.19,373.43,15.61,9.62" target="#b11">[12]</ref> for details). The advantages of this representation are two fold. MAP provides a more accurate estimate of the GMM parameters compared to standard maximum likelihood estimation (MLE) in the challenging case where the cardinality of the vector set is small. Moreover, there is a correspondence between the Gaussians of two GMMs adapted from a common distribution and one can take advantage of this fact to compute efficiently the probabilistic similarity.</p><p>To measure the similarity between two images, we use the Kullback-Leibler Divergence (KLD) between two continuous distributions P and P â€² :</p><formula xml:id="formula_17" coords="5,227.64,476.63,285.40,24.60">KL(P ||P â€² ) = x P (x) log P (x) P â€² (x) dx<label>(11)</label></formula><p>However, there is no closed-form expression for the KLD between two GMMs P (x) = N i=1 Î± i P i (x) and P â€² (x) = N j=1 Î² j P â€² j (x). Therefore, we use the KLK approximation proposed by Liu et Perronnin <ref type="bibr" coords="5,121.58,539.87,14.70,9.62" target="#b11">[12]</ref>:</p><formula xml:id="formula_18" coords="5,200.28,552.18,308.31,30.89">KL(P ||P â€² ) â‰ˆ N i=1 Î± i KL(P i ||P â€² i ) + log Î± i Î² i . (<label>12</label></formula><formula xml:id="formula_19" coords="5,508.59,562.43,4.45,9.62">)</formula><p>4 Cross-media similarity</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Cross-media similarities</head><p>This section deals with the intermediate fusion we used in our experiments. It aims at combining visual and textual similarities in an efficient manner.</p><p>The method is similar to the one we introduced in the 2007's session <ref type="bibr" coords="5,408.24,674.39,10.57,9.62" target="#b4">[5]</ref> and used in last year session <ref type="bibr" coords="5,123.47,686.39,10.00,9.62" target="#b2">[3]</ref>. This method has proved to significantly leverage multimedia retrieval performances.</p><p>Moreover, cross-media similarities that we obtain using this approach will be the basis for diversitybased extensions that will be discussed in the next section. As a result, we recall the basic concepts of this approach in the following.</p><p>In general terms, we assume that S t and S i are two similarity matrices over the same set of multimedia objects denoted O i ; i = 1, . . . , K. The former matrix is related to textual based similarities whereas the latter matrix is rather based on visual similarities. Typically, S t is related to Eq. ( <ref type="formula" coords="6,126.18,125.75,4.25,9.62" target="#formula_0">1</ref>) or Eq. ( <ref type="formula" coords="6,175.04,125.75,4.25,9.62" target="#formula_4">3</ref>) and S i to Eq. <ref type="bibr" coords="6,253.33,125.75,16.43,9.62" target="#b9">(10)</ref>. We actually use a linear transformation of the latter matrices in order to normalize the proximity measures distribution of each row so as to obtain a similarity value distribution between 0 and 1.</p><p>Then, let Îº(S, k) be the thresholding function that, for all rows of S, puts to zero all values that are lower than the k th highest value and keeps all other components to their initial value.</p><p>Accordingly, we define the cross-media similarity matrices that combine two mono-media similarity matrices as follows:</p><formula xml:id="formula_20" coords="6,243.00,224.63,265.59,10.32">Sim img-txt = Îº(S i , k i ).S t (<label>13</label></formula><formula xml:id="formula_21" coords="6,508.59,224.63,4.45,9.62">)</formula><formula xml:id="formula_22" coords="6,242.76,239.63,270.28,10.32">Sim txt-img = Îº(S t , k t ).S i<label>(14)</label></formula><p>where the . symbol designates the standard matrix product.</p><p>In that context, this intermediate fusion method can be seen as a graph similarity mixture through a two-step diffusion process, the first step being performed in one mode and the second step being performed in the other one (see <ref type="bibr" coords="6,278.65,301.31,10.57,9.62" target="#b4">[5,</ref><ref type="bibr" coords="6,290.90,301.31,7.81,9.62" target="#b2">3,</ref><ref type="bibr" coords="6,300.26,301.31,7.81,9.62" target="#b1">2]</ref> for further details).</p><p>Let us precise that in the more specific case of information retrieval, we are given a multimedia query q (q t denoting the text part and q i the image part of q). In that context, as far as the notations are concerned, we rather have the following cross-media score definition:</p><formula xml:id="formula_23" coords="6,240.48,362.99,272.56,10.32">Score img-txt = Îº(s i , k i ).S t<label>(15)</label></formula><formula xml:id="formula_24" coords="6,240.36,377.99,272.68,10.32">Score txt-img = Îº(s t , k t ).S i<label>(16)</label></formula><p>where s t is the similarity row vector of a given textual query q t with a set of multimedia objects (their text part) and s i is similarly, the similarity row vector of a given image query q i with the the same set of multimedia objects (but their image part).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Fusing all similarities</head><p>Cross-media similarities that we have recalled in the previous subsection, attempt to better fill in the semantic gap between images and texts. They allow to reinforce the mono-media similarities. Therefore, the final similarity we used is a late fusion of mono-media and cross-media similarities. This late combination have proved to provide better results according to our experiments in previous sessions of ImageCLEF Photo Retrieval Task. The final pairwise similarity matrix that evaluates the proximity relationships between multimedia items of a set of elements is given by:</p><formula xml:id="formula_25" coords="6,185.04,586.91,328.00,10.32">Sim = Î± t S t + Î± i S i + Î± it Sim img-txt + Î± ti Sim txt-img<label>(17)</label></formula><p>where Î± t , Î± i , Î± it , Î± ti are four weights that sum to 1. In our experiments we used the following distribution:</p><formula xml:id="formula_26" coords="6,147.12,623.39,184.09,10.32">Î± t = 5/12, Î± i = 1/4, Î± it = 1/4, Î± ti = 1/12.</formula><p>Similarly, when we are given a multimedia query, the final relevance score is computed as follows:</p><formula xml:id="formula_27" coords="6,176.88,664.55,336.16,10.32">Score = Î± t s t + Î± i s i + Î± it Score img-txt + Î± ti Score txt-img<label>(18)</label></formula><p>where the weight distribution is set in the same manner as previously.</p><p>One of the aims of ImageCLEFPhoto 2009 is to promote diversity in the top search results so that the first retrieved elements are not redundant. Therefore, we investigated different strategies to promote such diversity:</p><p>1. Using different sources of information for the same query: for example the caption of images, the query title and an enriched query. We can combine these different top lists using the Round Robin principle. 2. Using a density based re-ranking of the top elements. 3. Using a clustering based re-ranking of the top elements.</p><p>In this section we introduce these latter techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Round Robin Approach</head><p>The main idea of Round Robin principle is that each individual (in our case the different top lists) takes its turn to make a single list. We take for each top list the elements with respect to their rank and we update a single list by taking the following element top list after top list. A same item can appear in different top lists. Thus, we first verify whether a coming element is already in the merged list or not. If not it is appended to the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Density Based Re-ranking</head><p>This approach consists in identifying among a top list, peaks with respect to some estimated density functions. As density measure d, we used a simple one which is the sum of similarities (or distances) of the k nearest neighbors. Thus, given an object O i , we define:</p><formula xml:id="formula_28" coords="7,254.16,406.62,258.88,30.89">d(O i ) = k j=1 S(O i , O i j )<label>(19)</label></formula><p>where O i j ; j = 1, . . . , k are the k nearest neighbors of O i and the proximity measure between two multimedia objects can be based on visual similarity<ref type="foot" coords="7,344.16,462.93,3.97,6.97" target="#foot_0">5</ref> given by Eq. <ref type="bibr" coords="7,412.70,464.63,16.43,9.62" target="#b9">(10)</ref>, textual similarity (between their captions<ref type="foot" coords="7,191.16,474.81,3.97,6.97" target="#foot_1">6</ref> ) based on Eq. ( <ref type="formula" coords="7,264.55,476.51,4.25,9.62" target="#formula_4">3</ref>) or cross-media similarities (as described in section 4.1).</p><p>Finally, we re-rank the objects according to this measure by ranking first the objects that are the most "dense" and by discarding the nearest neighbors<ref type="foot" coords="7,344.16,499.41,3.97,6.97" target="#foot_2">7</ref> of these latter elements added to the list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Clustering Based Re-ranking</head><p>Our clustering based re-ranking approach we tested is similar to last year's session so hear we will only briefly recall (see <ref type="bibr" coords="7,189.25,581.99,10.57,9.62" target="#b2">[3]</ref> for more details).</p><p>We assume here that we are given an ordered top list of objects Q and a similarity matrix S between these objects (both Q and S could be visual, textual or cross-modal based). S is normalized such that for each row, the maximal element takes the value 1 and the minimal element the value 0. We apply the Relational Analysis (RA) approach for the clustering step in order to find homogeneous themes among the set of objects <ref type="bibr" coords="7,314.87,642.47,15.61,9.62" target="#b14">[15,</ref><ref type="bibr" coords="7,332.04,642.47,12.85,9.62" target="#b13">14,</ref><ref type="bibr" coords="7,346.44,642.47,7.81,9.62" target="#b2">3,</ref><ref type="bibr" coords="7,355.92,642.47,7.04,9.62" target="#b0">1]</ref>.</p><p>The clustering function that we want to optimize with respect to X is the following one:</p><formula xml:id="formula_29" coords="8,147.36,68.83,365.68,71.82">C(S, X) = |Q| i,i â€² =1 ï£« ï£¬ ï£¬ ï£¬ ï£¬ ï£¬ ï£­ S(O i , O i â€² ) - 1 |S + | (Oi,O i â€² )âˆˆS + S(O i , O i â€² ) constant threshold ï£¶ ï£· ï£· ï£· ï£· ï£· ï£¸ X(O i , O i â€² ) (20)</formula><p>where</p><formula xml:id="formula_30" coords="8,118.80,149.87,86.70,10.32">X(O i , O i â€² ) = 1 if O i</formula><p>and O i â€² are in the same cluster and X(O i , O i â€² ) = 0 otherwise; and S + is the set of pairs of objects which similarity measure is strictly positive:</p><formula xml:id="formula_31" coords="8,90.00,160.05,423.01,24.01">S + = {(O i , O i â€² ) âˆˆ Q Ã— Q : S(O i , O i â€² ) &gt; 0}.</formula><p>From Eq. ( <ref type="formula" coords="8,153.16,185.75,8.21,9.62">20</ref>), we can see that the more the similarity between two objects exceeds the mean average of strictly positive similarities, the greater the chances for them to be in the same cluster. This clustering function is based upon the central tendency deviation principle <ref type="bibr" coords="8,431.19,209.63,10.00,9.62" target="#b0">[1]</ref>. In order to find a partition represented by X that maximizes the objective function we used the same clustering algorithm described in <ref type="bibr" coords="8,188.30,233.51,10.57,9.62" target="#b2">[3,</ref><ref type="bibr" coords="8,200.54,233.51,7.05,9.62" target="#b0">1]</ref>. Notice that this approach doesn't require to fix the number of clusters. This property turns out to be an advantage for finding diverse relevant themes among the objects.</p><p>After the clustering step, we have to define a re-ranking strategy which takes into account the diversity provided by the clustering results. The main idea of our approach is to represent, among the first re-ranked results, elements which belong to different clusters until a stopping criterion is fulfilled. The strategy employed is described in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Re-ranking strategy for a (sub-)topic</head><p>Require: A (sub-)topic q, an ordered list Q according to some relevance score between q and Qi; i = 1, . . . , |Q| and R the clustering results of objects in Q. Let L1, L2, L3 and CL be empty lists and i = 2. Add Q1 as first element of the re-ranked list L1 and R(Q1) (the cluster id of Q1) to the cluster list CL while i â‰¤ |Q| and Stopping criterion is not fulfilled do if R(Qi) âˆˆ CL then Append Qi to L2 else Append Qi to L1 and add R(Qi) in CL end if i = i + 1 end while Put if not empty the complementary list of objects from Qi to Q |Q| in L3. Extend L1 with L2 then with L3 and return L1.</p><p>The stopping criterion in Algorithm 1 we used is related to a parameter denoted nbdiv âˆˆ 1, . . . , Îº<ref type="foot" coords="8,122.88,530.61,3.97,6.97" target="#foot_3">8</ref> . It is the maximal number of different clusters that must be represented among the first results. Let us assume that nbdiv = 10. Then, this implies that the first 10 elements of the reranked list have to belong to 10 different clusters (assuming that Îº â‰¥ 10). Once 10 different clusters are appended, the complementary list (from the 11 th rank to the |Q| th rank), is constituted of the remaining multimedia objects sorted in the original list without taking into account the cluster membership information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Runs description</head><p>In this year, we have participated in two tasks of ImageCLEFPhoto 2009: Large Scale Visual Concept Detection and Annotation and Photo Retrieval Tasks. In this section, we describe in more details our runs and show the obtained results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Large Scale Visual Concept Detection and Annotation Task</head><p>The Visual Concept Detection and Annotation Task (described in details in <ref type="bibr" coords="9,414.74,72.35,14.81,9.62" target="#b15">[16]</ref>), had the objective to identify 53 visual concepts in users' photos organized in a hierarchy. The main goal was to indicate the presence or the absence of these concepts ensuring some given ontological constraint between categories (eg. a picture cannot be labeled with summer and spring in the same time, etc).</p><p>In spite of the fact that the concepts were presented in a hierarchy with a given ontology, we have not used neither the hierarchy nor the ontology in the training phase but considered the task as a multi-class multi-label image categorization problem and trained binary one versus all classifiers for each concept. Then for each image the classification scores were post-processed in order to ensure that the constraints of the ontology were not violated.</p><p>To do this, two types of low-level local features were extracted: SIFT-like features (referred here as texture) and local RGB statistics (referred as color). Both type of features were extracted on a multi-level image grid and the dimensionality of the feature vectors was reduced to 50 through Principal Component Analysis. We built a visual vocabulary in both feature spaces and used these "'universal" vocabularies to define higher level representations (Fisher Vectors and Adapted Mixtures of Gaussian) as described in section 3. Hence, using the training data we built four one-against-all discriminative classifiers for each concept using alternatively color and texture and Fisher Vectors and image-GMMs.</p><p>To train the classifiers, we used our own implementation of Sparse Logistic Regression (SLR) <ref type="bibr" coords="9,90.00,299.51,14.70,9.62" target="#b10">[11]</ref>, (i.e. logistic regression with a Laplacian prior), L1-norm for Fisher Vectors (see section 3.2) and the KLK approximation proposed by Liu et Perronnin <ref type="bibr" coords="9,350.24,311.39,15.61,9.62" target="#b11">[12]</ref> (see section 3.3).</p><p>We transformed the classifier scores s in "probabilities" with the sigmoid mapping (1 + exp(s)) -1 and for each concept simply averaged the four scores.</p><p>Finally, in order to ensure the constraints of the ontology were not violated, we post-processed the scores as follows:</p><p>1. For concepts that were modelled as disjoint to each other, we normalized their "probabilities" to sum to 1. 2. For a parent concept, we ensured that its score is higher or equal than the maximum score of his children. 3. For a concept that implies another concept, we corrected only if their scores were conflicting to each other.</p><p>Two measures were used to determine the quality of the annotations. One for the evaluation per concept and the other one for the evaluation per photo. The first one was the Equal Error Rate (EER) and the Area under Curve (AUC). The second measure is the proposed hierarchical measure (HM) that considers the relations between concepts and the agreement of annotators on concepts. Figure <ref type="figure" coords="9,165.60,505.67,4.98,9.62" target="#fig_0">1</ref> plots all the results submitted to the challenge (blue) and in red our results. We can see that our system was among the top performing systems using the hierarchical measure (with or without annotator agreements) and averaging the four scores are at third position (with a score of 0.789).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Photo Retrieval Task</head><p>The Photo Retrieval Task of ImageCLEF 2009 (described in details by <ref type="bibr" coords="9,415.33,590.75,14.81,9.62" target="#b16">[17]</ref>), was intended to provide a further study of the importance of diversity in image search results. This aspect was already one of the goal of last year's session. However, this years task elevates the research scope by using a new data set containing half a million images.</p><p>Participants had to produce a ranking holding both relevant and diverse objects.</p><p>The definition of what constitutes diversity varied across topics. In the first part of the challenge, the "cluster title" fields, clearly indicated what the clustering criteria were. An additional "Cluster description" tag gave even more precision however, we haven't used it. In the second part of the challenge, only three relevant example images were given with the query title, without any Therefore, in our experiments we designed two different strategies to handle the two distinct parts. The steps of the first part are given by Algorithm 2. In the experiments, all runs used the same output for this part 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Image retrieval part 1</head><p>Require: query title and query images with their captions.</p><p>for Each topic do for Each sub-topic m do Rank the images according to the text similarity between the "the caption of the query image" (we assume that it is relevant to the query sub-topic) and the captions of all other images. Retain the top M images and compute the M Ã— M cross-media similarity matrix using Eq. ( <ref type="formula" coords="10,495.12,523.90,7.55,8.66" target="#formula_25">17</ref>). Re-rank the images according to their cross-media similarity with the query image (using the image and its caption). As the query image belongs to the top M images, this is equivalent to the re-ranking of the row corresponding to the query image in the M Ã— M matrix. end for Merge the m ranked lists corresponding to the different sub-topics using the Round Robin technique (eliminating from the list the query images and the exact duplicates coming from different lists). end for</p><p>The second part was more challenging especially from the diversity viewpoint since we had no particular indication either about the number of clusters or about the clustering criteria. Therefore, we experimented different strategies for this part. We considered different sources of information (Table <ref type="table" coords="10,121.44,674.39,4.46,9.62" target="#tab_1">2</ref>) for the query and different ranking criteria (Table <ref type="table" coords="10,355.80,674.39,3.88,9.62" target="#tab_2">3</ref>). The steps of the second part are given by Algorithm 3.</p><p>In order to re-rank the retrieved objects, we can use either the density based measure or the clustering algorithm. For the density based, we set k=10 neighbors and the distance used was the visual similarity. However, the clustering methods used the cross-media similarity, such as defined in Eq. ( <ref type="formula" coords="11,121.80,88.55,8.12,9.62" target="#formula_25">17</ref>)). Nevertheless, it is worth noticing that any kinds of similarity measures (visual, textual, or cross-modal) could be used in order to assess the diversity in the aforementioned re-ranking techniques. As it can be seen from Algorithm 3, we can generate a variety of different final lists by using different strategies. In the challenge we submitted 4 of them, summarized in Table <ref type="table" coords="11,453.12,371.15,3.90,9.62" target="#tab_3">4</ref>. The results were ranked in terms of two measures, the precision at 10 (P10) retrieved items and the cluster recall at 10 (CR10). The two measures were combined using the F1-measure. According to this measure XRCEXKNND strategy was the best with F 1 = 0.8087 followed by XRCECLUST and KNND.</p><p>XRCE1 got a much lower F1 score (0.7439) which shows that the re-ranking was important to enhance the retrieved list. In fact, XRCE1 is a run without any re-ranking to remove nearduplicate elements. It just used different representations of the query and applied a standard ranking algorithm. Regarding the second part of queries, our re-ranking strategies allow to leverage this baseline run with CR10 equal to 60% to runs with around 80% CR10.</p><p>We have not explored yet all the possibilities given by Algorithm 3. Can we reach the same performance with pure text runs ? Is the density measure better than the clustering methods, or are they complementary ?</p><p>In order to have final conclusions about the best strategy to be applied, we will compare the different individual and combined lists when the cluster judgements will be available.</p><p>We have participated in two tasks, Large Scale Visual Concept Detection and Annotation and Photo Retrieval. In both cases, we kept our leading position in spite of the fact that both tasks presented new challenges.</p><p>The basic methodologies used in our systems were mainly those used in previous years with slight modifiations and adaptations to better fit the actual tasks. No particular parameter tuning was necessary to achieve the good performances we obtained, on a new corpus. This shows the robustness of our methods.</p><p>However, we would like to better analyze our results and particularly for the Photo Retrieval task. In our approach, text-based retrieval is the cornerstone. To which extent, did the visual information leverage these performances ? Was the query expansion able to find new clusters when seeking to promote diversity ? and so on ... Such investigations could be beneficial in order to better understand the properties of our systems and, in a more general perspective, to better tackle information access problems in large image/text collections.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="10,90.48,335.62,422.01,8.66"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. ImageClef Large Scale Visual Concept Detection Task results, the red star represents our system</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,192.48,49.66,217.84,128.54"><head>Table 1 .</head><label>1</label><figDesc>Query Terms and their most similar terms</figDesc><table coords="4,192.48,70.42,217.84,107.78"><row><cell>1</cell><cell>obama 1</cell><cell>strike 1</cell><cell>euro 1</cell></row><row><cell>2</cell><cell>barack 0.98</cell><cell>hunger 0.04</cell><cell>million 0.05</cell></row><row><cell cols="3">3 springfield 0.16 protest 0.02</cell><cell>billion 0.05</cell></row><row><cell>4</cell><cell>illinois 0.16</cell><cell>worker 0.01</cell><cell>currency 0.03</cell></row><row><cell>5</cell><cell>senator 0.09</cell><cell>caracas 0.01</cell><cell>2004 0.03</cell></row><row><cell>6</cell><cell>freezing 0.08</cell><cell>led 0.01</cell><cell>coin 0.02</cell></row><row><cell>7</cell><cell cols="2">formally 0.08 venezuela 0.01</cell><cell>devil 0.02</cell></row><row><cell>8</cell><cell>ames 0.07</cell><cell>chavez 0.001</cell><cell>qualify 0.02</cell></row><row><cell cols="4">9 democrat 0.06 nationwide 0.001 qualification 0.02</cell></row><row><cell cols="3">10 paperwork 0.04 retaliatory 0.001</cell><cell>profit 0.01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="11,197.52,152.38,204.96,60.98"><head>Table 2 .</head><label>2</label><figDesc>Source of Information</figDesc><table coords="11,197.52,171.46,204.96,41.90"><row><cell>Type</cell><cell>Acronym</cell></row><row><cell>Captions of Image Query</cell><cell>ICPT</cell></row><row><cell>Query Title</cell><cell>QRW</cell></row><row><cell cols="2">Enriched Query with most similar Terms ENT</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="11,198.12,272.50,203.48,51.38"><head>Table 3 .</head><label>3</label><figDesc>Ranking Criterion</figDesc><table coords="11,198.12,293.26,203.48,30.62"><row><cell>Simply Apply the Cross Media Similarity Measure</cell></row><row><cell>Density based Re-ranking</cell></row><row><cell>Clustering based Re-ranking</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="11,121.68,411.10,359.57,71.90"><head>Table 4 .</head><label>4</label><figDesc>List of the combinations used for official runs and their F1-measure</figDesc><table coords="11,121.68,430.18,359.57,52.82"><row><cell>Run Name</cell><cell></cell><cell>List Used for Round Robin</cell><cell></cell><cell>F1</cell></row><row><cell>XRCE1</cell><cell>ICPT xsim rank</cell><cell>ENT txt rank</cell><cell>QRW txt rank</cell><cell>74.4</cell></row><row><cell>KNND</cell><cell>ICPT xsim rank</cell><cell>ENT knndens rerank</cell><cell></cell><cell>76.2</cell></row><row><cell cols="3">XRCECLUST ICPT cluster rerank ENT cluster rerank</cell><cell></cell><cell>79.4</cell></row><row><cell>XRCEXKNND</cell><cell>ICPT xsim rank</cell><cell cols="3">QRW knndens rerank ENT knndens rerank. 80.9</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0" coords="7,99.96,665.14,271.24,8.66"><p>In that case, we use the image part Ii of the multimedia object Oi.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_1" coords="7,99.96,676.06,256.36,8.66"><p>In that case, it is the text part Di of the multimedia object Oi.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_2" coords="7,99.96,687.10,80.43,8.66"><p>2 nearest neighbors.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_3" coords="8,99.96,687.10,255.39,8.66"><p>Îº is the number of clusters found during the clustering process.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was partially supported by the <rs type="funder">French National</rs> Project <rs type="grantNumber">Omnia</rs> <rs type="grantNumber">ANR-06-CIS6-01</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_agGYKyV">
					<idno type="grant-number">Omnia</idno>
				</org>
				<org type="funding" xml:id="_jcVKS3X">
					<idno type="grant-number">ANR-06-CIS6-01</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Image retrieval part 2</head><p>Require: query title and query images with their captions for Each topic do Create three top lists using different text queries: if ICPT (image captions) then for Each query image i = 1 to 3 do Rank the images according to the text similarity between the "the caption of the query image" (we assume that it is relevant to the query sub-topic) and the captions of all other images (see section 2 for details).</p><p>Retain the top M images and compute the M Ã— M cross-media similarity matrix using Eq. <ref type="bibr" coords="14,493.23,153.46,15.71,8.65" target="#b16">(17)</ref>. Re-rank the images according to one of the following strategies: if ICPT xsim rank then Rank according to their cross-media similarity with the query image using Eq. ( <ref type="formula" coords="14,459.81,186.22,7.85,8.65">18</ref>). else {ICPT cluster rerank} Rank according to their cross-media similarity with the query image such as for ICPT xsim rank. Re-rank the images of the top list of ICPT xsim rank using the clustering based re-ranking method described in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">(using cross-modal similarities). else {ICPT knndens rerank}</head><p>Rank according to their cross-media similarity with the query image such as for ICPT xsim rank. Re-rank the top list of ICPT txt rank using density based re-ranking described in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">(using cross-modal similarities). end if end for Merge the 3 ranked lists corresponding to the 3 images using the Round Robin technique (eliminating from the list the query images and the exact duplicates coming from different lists). else {QRW (query title)}</head><p>Rank the images according to one of the following strategies: if QRW txt rank then Rank the images according to the text similarity between the query title and the captions of all other images and consider the top M . else {QRW cluster rerank} Rank the images according to the text similarity between the query title and the captions of all other images and consider the top M such as for QRW txt rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Re-rank the top list of QRW txt rank using the clustering based re-ranking method (using crossmodal similarities). else {QRW knndens rerank}</head><p>Rank the images according to the text similarity between the query title and the captions of all other images and consider the top M such as for QRW txt rank. Re-rank the top list of QRW txt rank using the density based re-ranking method (using crossmodal similarities). end if else {ENT (enriched query)} Rank the images according to one of the following strategies: if ENT txt rank then Rank the images according to the text similarity between the query enriched with lexical entailment (as described in Eq. ( <ref type="formula" coords="14,215.74,569.86,4.06,8.65">3</ref>) and the captions of all other images and consider the top M . else {ENT cluster rerank} Rank the images according to the text similarity between the query enriched with lexical entailment and the captions of all other images and consider the top M such as for ENT txt rank Re-rank the top list of ENT txt rank using the clustering based re-ranking method (with visual or textual or cross-modal similarities). else {ENT knndens rerank} Rank the images according to the text similarity between the query enriched with lexical entailment and the captions of all other images and consider the top M such as for ENT txt rank. Re-rank the top list of ENT txt rank using the density based re-ranking method (with visual or textual or cross-modal similarities). end if end if Optionally, merge several of the above mentioned ranked lists using the Round Robin approach described in 5.1. end for</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,98.12,339.34,414.83,8.66;12,106.80,350.38,55.23,8.66" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="12,160.55,339.34,271.93,8.66">Cluster analysis based on the central tendency deviation principle</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ah-Pine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,455.52,339.34,57.43,8.65;12,106.80,350.38,25.18,8.65">Proceedings of ADMA</title>
		<meeting>ADMA</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,98.12,361.18,414.88,8.66;12,106.80,372.10,386.92,8.66" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="12,446.86,361.18,66.14,8.66;12,106.80,372.10,206.30,8.66">Crossing textual and visual content in different application scenarios</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ah-Pine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bressan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Hoppenot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Renders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,321.24,372.10,92.51,8.65">Multimedia Tools Appl</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="56" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,98.12,382.90,414.66,8.66;12,106.80,393.82,365.05,8.66" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="12,382.66,382.90,130.12,8.66">Xrce&apos;s participation to imageclef</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ah-Pine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cifarelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Renders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,143.40,393.82,177.14,8.65">Working Notes of the 2008 CLEF Workshop</title>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09">2008. September 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,98.12,404.62,414.80,8.66;12,106.80,415.54,372.04,8.66" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="12,310.04,404.62,202.88,8.66;12,106.80,415.54,184.73,8.66">Probabilistic models of information retrieval based on measuring the divergence from randomness</title>
		<author>
			<persName coords=""><forename type="first">Gianni</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cornelis Joost</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,300.24,415.54,89.65,8.65">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="357" to="389" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,98.12,426.34,414.80,8.66;12,106.80,437.26,271.34,8.66" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="12,284.34,426.34,130.07,8.66">Xrce&apos;s participation to imageclef</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Renders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,453.12,426.34,59.80,8.65;12,106.80,437.26,113.66,8.65">Working Notes of the 2007 CLEF Workshop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09">2007. September 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,98.12,445.78,414.78,10.94;12,106.80,458.98,208.84,8.66" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="12,270.21,448.06,242.69,8.66;12,106.80,458.98,136.91,8.66">Bridging language modeling &amp; divergence from randomness models: A log-logistic model for ir</title>
		<author>
			<persName coords=""><forename type="first">StÃ©phane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ã‰ric</forename><surname>Gaussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,262.32,458.98,24.73,8.65">ICTIR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,98.12,467.50,388.75,10.94;12,106.80,480.70,171.52,8.66" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="12,267.33,469.78,219.53,8.66;12,106.80,480.70,101.05,8.66">Retrieval constraints and word frequency distributions a log-logistic model for ir</title>
		<author>
			<persName coords=""><forename type="first">StÃ©phane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ã‰ric</forename><surname>Gaussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,226.44,480.70,22.49,8.65">CIKM</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,98.12,491.50,414.81,8.66;12,106.80,502.42,333.40,8.66" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="12,369.74,491.50,143.18,8.66;12,106.80,502.42,36.74,8.66">Visual categorization with bags of keypoints</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,163.08,502.42,249.14,8.65">ECCV Workshop on Statistical Learning for Computer Vision</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,98.12,513.22,414.83,8.66;12,106.80,524.14,253.12,8.66" xml:id="b8">
	<monogr>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<title level="m" coord="12,343.51,513.22,169.43,8.66;12,106.80,524.14,37.99,8.66">Improving &quot;bag-of-keypoints&quot; image categorisation</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>University of Southampton</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="12,97.78,534.94,277.83,8.66" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="12,168.84,534.94,86.32,8.65">Clustering Algorithms</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>John Wiley and Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,97.78,545.74,414.95,8.66;12,106.80,556.66,228.76,8.66" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="12,360.11,545.74,152.63,8.66;12,106.80,556.66,169.63,8.66">Sparse multinomial logistic regression: Fast algorithms and generalization bounds</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Krishnapuram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Hartemink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,285.00,556.66,21.44,8.65">PAMI</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,97.78,567.46,415.09,8.66;12,106.80,578.38,127.24,8.66" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="12,209.74,567.46,303.14,8.66;12,106.80,578.38,54.69,8.66">Similarity measure between unordered vector sets with application to image categorization</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,181.32,578.38,23.16,8.65">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,97.78,589.18,415.22,8.66;12,106.80,600.10,281.68,8.66" xml:id="b12">
	<monogr>
		<title level="m" type="main" coord="12,345.35,589.18,167.65,8.66;12,106.80,600.10,253.95,8.66">Review: Foundations of statistical natural language processing, christopher d. manning and hinrich schtze</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hinrich</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lillian</forename><surname>Schiitze</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,97.78,610.90,415.02,8.66;12,106.80,621.82,159.40,8.66" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="12,252.11,610.90,220.97,8.66">Heuristic approach of the similarity aggregation problem</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Marcotorchino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="12,480.00,610.90,32.81,8.65;12,106.80,621.82,81.77,8.65">Methods of operation research</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="395" to="404" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,97.78,632.62,415.12,8.66;12,106.80,643.66,240.75,8.66" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="12,265.19,632.62,204.54,8.66">Optimisation en analyse de donnÃ©es relationnelles</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Marcotorchino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="12,493.56,632.62,19.35,8.65;12,106.80,643.66,98.87,8.65">Data Analysis and informatics</title>
		<meeting><address><addrLine>North Holland Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,97.78,654.34,415.16,8.66;12,106.80,665.38,398.91,8.66" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="12,247.66,654.34,265.29,8.66;12,106.80,665.38,61.95,8.66">Overview of the clef 2009 large scale -visual concept detection and annotation task</title>
		<author>
			<persName coords=""><forename type="first">Stefanie</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Dunker</surname></persName>
		</author>
		<ptr target="http://www.imageclef.org/2009/PhotoAnnotation" />
	</analytic>
	<monogr>
		<title level="m" coord="12,188.16,665.38,81.83,8.65">CLEF working notes</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,97.78,676.06,414.89,8.66;12,106.80,687.10,352.94,8.66" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="12,291.44,676.06,221.24,8.66;12,106.80,687.10,62.48,8.66">Diversity in photo retrieval: overview of the imageclefphoto task 2009</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Paramita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<ptr target="http://www.imageclef.org/2009/photo" />
	</analytic>
	<monogr>
		<title level="m" coord="12,189.24,687.10,81.83,8.65">CLEF working notes</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,97.79,53.38,415.25,8.66;13,106.80,64.42,20.80,8.66" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="13,221.26,53.38,243.81,8.66">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,484.08,53.38,23.16,8.65">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,97.79,75.34,415.03,8.66;13,106.80,86.26,101.55,8.66" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="13,322.50,75.34,190.31,8.66;13,106.80,86.26,29.47,8.66">Adapted vocabularies for generic visual categorization</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Bressan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,155.64,86.26,23.07,8.65">ECCV</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,97.79,97.30,415.16,8.66;13,106.80,108.22,406.23,8.65;13,106.80,119.14,152.55,8.66" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="13,213.46,97.30,223.52,8.66">A language modelling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,456.24,97.30,56.71,8.65;13,106.80,108.22,406.23,8.65;13,106.80,119.14,34.38,8.65">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,97.79,130.18,415.12,8.66;13,106.80,141.10,103.46,8.66" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="13,230.12,130.18,278.88,8.66">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,118.32,141.10,20.96,8.65">ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,97.78,152.02,415.26,8.66;13,106.80,163.06,316.35,8.66" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="13,207.34,152.02,305.70,8.66;13,106.80,163.06,81.90,8.66">A study of smoothing methods for language models applied to ad hoc to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="13,207.96,163.06,96.41,8.65">Proceedings of SIGIR&apos;01</title>
		<meeting>SIGIR&apos;01</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
