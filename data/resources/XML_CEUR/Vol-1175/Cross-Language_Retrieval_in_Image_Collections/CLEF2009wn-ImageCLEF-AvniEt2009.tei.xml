<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,188.16,56.61,235.90,11.87;1,164.28,71.73,283.82,11.87">Addressing the ImageClef 2009 Challenge Using a Patch-based Visual Words Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,221.04,85.71,30.32,8.50"><forename type="first">U</forename><surname>Avni</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel-Aviv University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,259.32,85.71,51.00,8.50"><forename type="first">J</forename><surname>Goldberger</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,331.80,85.71,51.72,8.50"><forename type="first">H</forename><surname>Greenspan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel-Aviv University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">IBM Almaden Research Center</orgName>
								<address>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,188.16,56.61,235.90,11.87;1,164.28,71.73,283.82,11.87">Addressing the ImageClef 2009 Challenge Using a Patch-based Visual Words Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">878922434D324259BC0B8B6EAE0B445C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation at the ImageClef 2009 medical annotation task. In this task we have used the bag-of-words approach for image representation. We submitted one run, using support-vector-machines trained on the visual word histograms in multiple scales. In this task our result ranked first, with error score of 852.8.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In the last several years, "patch-based" representations and "bag-of-features" classification techniques have been proposed for general object recognition tasks <ref type="bibr" coords="1,258.13,274.23,24.96,8.50">[1 -6]</ref>. In these approaches, a shift is made from the pixel entity to a "patch" -a small window centered on the pixel. In its most simplified form, raw pixel values (intensities) within the window are used as the components of the feature vector. It is possible to take the patch information as a collection of pixel values, or to shift the representation to a different set of features based on the pixels, such as SIFT features <ref type="bibr" coords="1,519.48,306.63,10.13,8.50" target="#b6">[7]</ref>, and reduce the dimensionality of the representation via dimensionality reduction techniques, such as principlecomponent analysis (PCA) <ref type="bibr" coords="1,183.12,328.35,10.04,8.50" target="#b7">[8]</ref>.</p><p>A very large set of patches are extracted from an image. Each small patch shows a localized "glimpse" at the image content; the collection of thousands and more such patches, randomly selected, have the capability to identify the entire image content (similar to a puzzle being formed from its pieces). A dictionary of words is learned over a large collection of patches, extracted from a large set of images. Once a global dictionary is learned, each image is represented as a collection of words (also known as a "bag of words", or "bag of features"), using an indexed histogram over the defined words. The matching between images, or between an image and an image class, can then be defined as a distance measure between the representative histograms. In categorizing an image as belonging to a certain image class, well-known classifiers, such as the k-nearest neighbor and support-vector machines (SVM) <ref type="bibr" coords="1,450.96,425.79,10.13,8.50" target="#b8">[9]</ref>, are used.</p><p>Patch-based methods have evolved from texton methods in texture analysis <ref type="bibr" coords="1,382.44,447.51,10.26,8.50" target="#b0">[1,</ref><ref type="bibr" coords="1,395.40,447.51,7.80,8.50" target="#b1">2]</ref> and were motivated from the text processing world <ref type="bibr" coords="1,151.68,458.31,10.13,8.50" target="#b2">[3]</ref>. In the classical bag-of-features approach, spatial information and geometrical relationship between patches is lost. Recent works have shown that including the spatial information as additional features per patch may provide additional mage characterization strength. The patch-based, bag-of-features approach is simple, computationally efficient, and shows robustness to occlusions and spatial variations. Using this approach, a substantial increase in performance capabilities in general computer-vision object and scene classification tasks has been demonstrated [e.g., <ref type="bibr" coords="1,158.16,512.43,6.90,8.50" target="#b3">4,</ref><ref type="bibr" coords="1,169.80,512.43,6.84,8.50" target="#b4">5]</ref>. Motivated by these works, and the by success of works based on similar approach in ImageClef2007 challenges <ref type="bibr" coords="1,182.52,523.35,15.06,8.50" target="#b9">[10,</ref><ref type="bibr" coords="1,200.04,523.35,12.48,8.50" target="#b10">11]</ref> we have developed a retrieval and classification system for large medical databases, and put it to the test in ImageClefMed 2008 tasks. This work is an enhancement of the classification system we have submitted to the medical annotation challenge in ImageClef 2008 <ref type="bibr" coords="1,327.96,544.83,14.54,8.50" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Medical Image Annotation Task</head><p>In this task we are presented with 12,72 based on labeling standards from the last four years. Label sets from 2 respectively. Label sets from 2007 and 2008 contain 116 and 196 IRMA codes. The goal is to classify about 2000 unseen images according the four label sets. Error evaluation scheme is described in the task website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>We model an image as a collection of local patches, where a patch is a small rectangular sub region of the image. Each patch is represented as a codeword index out of a finite vocabulary of visual codewords. Images are compared and classified based on this discrete and compact representation.</p><p>We built a dictionary from a random extracts patches of a fixed size of 9x9 pixels 1 variance. We then compute a covariance matrix of eigenvectors. The 6 vectors with the highest energy are shown in Figure</p><p>These eigenvectors are later used as a base for the rest of the patches in the database. added to the feature set, in order to include information about the visual words layout. Running k this set produces 1000 dictionary visual words. A sample dictionary is displayed in Figure <ref type="figure" coords="2,421.20,378.99,3.51,8.50" target="#fig_0">2</ref>.</p><p>The dictionary building process is repeated in 3 image scales: full resolution, 1/2 scale and 1/8 scale. The resulting dictionary is a concatenation of the 3 dictionaries fr extracted from each image using a dense grid the multi-scale dictionary. Image classification is performed on the word histograms by an SVM classifier with classification is implemented using one-vs label, without using the hierarchical nature of the code. The classi for replacing trailing '0's with '*'. This does not damage the error score if the last digit in the true code is '0', and can reduce the error score if the last digit is non Task 729 classified x-ray images, labeled according to four label sets, labels are based on labeling standards from the last four years. Label sets from 2005 and 2006 contain 57 and 116 labels, respectively. Label sets from 2007 and 2008 contain 116 and 196 IRMA codes. The goal is to classify about 2000 unseen images according the four label sets. Error evaluation scheme is described in the task website. model an image as a collection of local patches, where a patch is a small rectangular sub region of the image. Each patch is represented as a codeword index out of a finite vocabulary of visual codewords. Images are compared and iscrete and compact representation.</p><p>random subset of 400 images from the database. The dictionary building process atches of a fixed size of 9x9 pixels with a grid of 6 pixels spacing, patches are normalized to have then compute a covariance matrix of a set of roughly 2,000,000 patches, and appl eigenvectors. The 6 vectors with the highest energy are shown in Figure <ref type="figure" coords="2,353.28,235.35,3.45,8.50">1</ref>.</p><p>Fig. <ref type="figure" coords="2,283.20,335.55,3.64,8.50">1</ref>: PCA components used as a base for the rest of the patches in the database. Patch center coordinates are added to the feature set, in order to include information about the visual words layout. Running kctionary visual words. A sample dictionary is displayed in Figure <ref type="figure" coords="2,421.20,378.99,3.51,8.50" target="#fig_0">2</ref>.</p><p>dictionary building process is repeated in 3 image scales: full resolution, 1/2 scale and 1/8 scale. The resulting dictionary is a concatenation of the 3 dictionaries from the 3 scales. In the image representation step extracted from each image using a dense grid-around every pixel. An image is represented as a word histogram over vs-one heuristic. In the training step each IRMA code is treated as a separate using the hierarchical nature of the code. The classifier output is returned without using wildcards, except for replacing trailing '0's with '*'. This does not damage the error score if the last digit in the true code is '0', and can e if the last digit is non-zero.</p><p>labeled according to four label sets, labels are 005 and 2006 contain 57 and 116 labels, respectively. Label sets from 2007 and 2008 contain 116 and 196 IRMA codes. The goal is to classify about 2000 unseen images according the four label sets. Error evaluation scheme is described in the task website. model an image as a collection of local patches, where a patch is a small rectangular sub region of the image. Each patch is represented as a codeword index out of a finite vocabulary of visual codewords. Images are compared and of 400 images from the database. The dictionary building process are normalized to have 0 mean and et of roughly 2,000,000 patches, and apply PCA to find its Patch center coordinates are -means algorithm on dictionary building process is repeated in 3 image scales: full resolution, 1/2 scale and 1/8 scale. The resulting om the 3 scales. In the image representation step, patches are is represented as a word histogram over kernel. Multi-class one heuristic. In the training step each IRMA code is treated as a separate er output is returned without using wildcards, except for replacing trailing '0's with '*'. This does not damage the error score if the last digit in the true code is '0', and can</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,258.96,594.95,94.52,8.43;2,79.56,616.95,369.16,8.50;2,449.16,615.56,3.79,5.64;2,233.64,627.75,299.36,8.50;2,133.80,638.55,399.08,8.50;2,79.56,649.47,453.24,8.50;2,156.96,660.15,108.06,8.50;2,238.20,452.76,137.40,140.28"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Dictionary layoutImage classification is performed on the word histograms by an SVM classifier with ߯ ଶ vs-one heuristic. In the training step each IRMA code is treated as a separate using the hierarchical nature of the code. The classifier output is returned without using wildcards, except for replacing trailing '0's with '*'. This does not damage the error score if the last digit in the true code is '0', and can e if the last digit is non-zero.</figDesc><graphic coords="2,238.20,452.76,137.40,140.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,188.28,169.08,235.68,214.56"><head></head><label></label><figDesc></figDesc><graphic coords="3,188.28,169.08,235.68,214.56" type="bitmap" /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and Results</head><p>Kernel parameter ߛ and SVM tradeoff parameters C were exhaustively searched to minimize cross-validation average error over 5 experiments, where in each experiment 2000 random images served as test data. Parameter tweaking was done using the 2007 code labels. Figure <ref type="figure" coords="3,239.40,130.59,4.68,8.50">3</ref> displays the error landscape of the scanned parameters space. The optimal parameters set was used in all four classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3: SVM classifier parameters</head><p>The classification on the actual test data of our run is shown in Table <ref type="table" coords="3,341.40,418.11,3.51,8.50">1</ref>. The total running time for the whole system, training and classification, was approximately 90 minutes on a dual quad-core Intel Xeon 2.33 GHz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>We presented a classification system for large medical databases, based on compact bag-of-features image representation. The system achieves comparatively good results in the ImageClef 2009 medical annotation challenge, while maintaining efficient computation times.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="4,91.35,81.51,420.09,8.50;4,104.76,92.31,155.22,8.50" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="4,221.52,81.51,289.92,8.50;4,104.76,92.31,74.12,8.50">Representing and recognizing the visual appearance of materials using threedimensional textons</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,185.04,92.35,17.33,8.35">IJCV</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="44" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,91.35,114.03,439.29,8.50;4,104.76,124.71,16.38,8.50" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="4,240.72,114.03,185.72,8.50">Texture classification: are filter banks necessary?</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,439.20,114.07,30.19,8.35">CVPR</title>
		<imprint>
			<biblScope unit="volume">03</biblScope>
			<biblScope unit="page" from="691" to="698" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,91.35,146.43,426.53,8.50;4,104.76,157.35,212.82,8.50" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="4,236.62,146.43,272.99,8.50">Video Google: A Text Retrieval Approach to Object Matching in Videos</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,104.76,157.39,149.32,8.35">Proc. Ninth Int&apos;l Conf. Computer Vision</title>
		<meeting>Ninth Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1470" to="1478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,91.35,178.95,440.57,8.50;4,104.76,189.75,220.38,8.50" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="4,229.80,178.95,265.59,8.50">A Bayesian Hierarchical Model for Learning Natural Scene Categories</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,501.60,178.99,30.32,8.35;4,104.76,189.79,180.28,8.35">Proc. of IEEE Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="524" to="531" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,91.35,211.47,415.23,8.50" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="4,192.96,211.47,223.28,8.50">Sampling strategies for bag-of-features image classification</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,432.36,211.51,24.08,8.35">ECCV</title>
		<imprint>
			<biblScope unit="volume">06</biblScope>
			<biblScope unit="page" from="406" to="503" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,91.35,233.07,437.33,8.50;4,104.76,243.99,137.88,8.50" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="4,258.60,233.07,270.08,8.50;4,104.76,243.99,53.56,8.50">Towards optimal bag-of-features for object categorization and semantic video retrieval</title>
		<author>
			<persName coords=""><forename type="first">Y-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C-W &amp;</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="4,164.16,244.03,20.84,8.35">CIVR</title>
		<imprint>
			<biblScope unit="volume">2007</biblScope>
			<biblScope unit="page" from="494" to="501" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,91.35,265.59,412.59,8.50" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="4,169.92,265.59,200.25,8.50">Object recognition from local scale-invariant features</title>
		<author>
			<persName coords=""><forename type="middle">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,386.28,265.63,42.10,8.35">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,91.35,287.19,355.47,8.50" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="4,189.96,287.23,154.24,8.35">Neural Networks for Pattern Recognition</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,91.35,308.79,357.03,8.50" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="4,178.56,308.79,156.19,8.50">The Nature of Statistical Learning Theory</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Springer Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,95.56,330.51,402.76,8.50;4,104.76,341.31,348.42,8.50" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="4,284.88,330.51,213.44,8.50;4,104.76,341.31,78.72,8.50">CLEF2007 Image Annotation Task: an SVM-based Cue Integration Approach</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,200.52,341.31,170.91,8.50">Working Notes of the 2007 CLEF Work-shop</title>
		<meeting><address><addrLine>Budapest, Hungary</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,95.56,363.03,414.56,8.50;4,104.76,373.83,288.42,8.50" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="4,207.00,363.03,259.20,8.50">Sparse patch-histograms for object classification in cluttered images</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,483.00,363.07,27.12,8.35;4,104.76,373.83,16.94,8.50">DAGM 2006</title>
		<title level="s" coord="4,130.68,373.87,131.78,8.35">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4174</biblScope>
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="4,95.56,395.55,429.44,8.50;4,104.76,406.35,139.44,8.50" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="4,282.12,395.55,127.71,8.50">TAU MIPLAB at ImageClef 2008</title>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Avni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="4,426.12,395.55,98.88,8.50;4,104.76,406.35,65.44,8.50">working notes of the 2008 CLEF Work-shop</title>
		<meeting><address><addrLine>Aarhus, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
