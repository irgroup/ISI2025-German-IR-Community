<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,177.00,115.72,261.24,12.93;1,162.72,133.72,289.93,12.93">Multimodal Medical Image Retrieval Improving Precision at ImageCLEF 2009</title>
				<funder ref="#_mnbEzh5">
					<orgName type="full">NLM</orgName>
				</funder>
				<funder ref="#_VWeC4yZ">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_7s4VKyp">
					<orgName type="full">Swiss National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName coords="1,141.84,170.85,67.27,9.96"><forename type="first">Sa√Ød</forename><surname>Radhouani</surname></persName>
							<email>radhouan@ohsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Informatics &amp; Clinical Epidemiology Oregon Health and Science</orgName>
								<orgName type="institution">University (OHSU)</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,217.35,170.85,119.96,9.96"><forename type="first">Jayashree</forename><surname>Kalpathy-Cramer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Informatics &amp; Clinical Epidemiology Oregon Health and Science</orgName>
								<orgName type="institution">University (OHSU)</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,345.16,170.85,63.63,9.96"><forename type="first">Steven</forename><surname>Bedrick</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Informatics &amp; Clinical Epidemiology Oregon Health and Science</orgName>
								<orgName type="institution">University (OHSU)</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,416.40,170.85,52.31,9.96"><forename type="first">Brian</forename><surname>Bakke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Informatics &amp; Clinical Epidemiology Oregon Health and Science</orgName>
								<orgName type="institution">University (OHSU)</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,276.12,182.85,63.26,9.96"><forename type="first">William</forename><surname>Hersh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Medical Informatics &amp; Clinical Epidemiology Oregon Health and Science</orgName>
								<orgName type="institution">University (OHSU)</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,177.00,115.72,261.24,12.93;1,162.72,133.72,289.93,12.93">Multimodal Medical Image Retrieval Improving Precision at ImageCLEF 2009</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5B89DC72BEE82BD806D4818648FBAC5A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Languages-Query Languages Measurement, Performance, Experimentation Medical Image Retrieval, Performance Evaluation, Image Classification, Image Modality Extraction, Domain Dimensions</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present results from Oregon Health &amp; Science University's participation in the medical retrieval task of ImageCLEF 2009. This year, we focused on improving retrieval performance, especially early precision, in the task of solving medical multimodal queries. These queries contain visual data, given as a set of image-examples, and textual data, provided as a set of words belonging to three dimensions: Anatomy, Pathology, and Modality. To solve these queries, we use both textual and visual data in order to better interpret the semantic content of the queries. Indeed, using the textual data associated with the image, it is relatively easy to extract anatomy and pathology, but it is challenging to extract the modality, since this is not always explicitly described in the text. To overcome this problem, we utilized the visual data. We combined both text-based and visual-based search techniques to provide a unique ranked list of relevant documents for each query. The obtained results showed that our approach outperforms our baseline by 43% in MAP and 71% in precision at top 5 documents. This is due to the use of domain dimensions and the combination of both visual-based and text-based search techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Advances in digital imaging technologies and the increasing prevalence of Picture Archival and Communication Systems (PACS) have led to a substantial growth in the number of digital images stored in hospitals and medical systems in recent years. Medical images can form an essential component of a patient's health records, and the ability to retrieve them can be useful in several tasks, including diagnosis, education, research, and so on.</p><p>Image retrieval systems (IRS) do not currently perform as well as their text counterparts <ref type="bibr" coords="2,191.77,226.29,10.00,9.96" target="#b0">[1]</ref>. Medical and other IRS have historically relied on indexing annotations or captions associated with the images. The last few decades, however, have seen advancements in the area of content-based image retrieval (CBIR) <ref type="bibr" coords="2,134.76,262.17,10.57,9.96" target="#b1">[2,</ref><ref type="bibr" coords="2,147.00,262.17,7.05,9.96" target="#b2">3]</ref>. Although CBIR systems have demonstrated success in fairly constrained medical domains including pathology, dermatology, chest radiology, and mammography, they have demonstrated poor performance when applied to databases with a wide spectrum of imaging modalities, anatomies, and pathologies <ref type="bibr" coords="2,447.96,298.05,10.57,9.96" target="#b0">[1,</ref><ref type="bibr" coords="2,460.09,298.05,4.13,9.96">[4]</ref><ref type="bibr" coords="2,464.22,298.05,4.13,9.96" target="#b4">[5]</ref><ref type="bibr" coords="2,468.35,298.05,8.26,9.96" target="#b5">[6]</ref>.</p><p>In this paper, we address the problem of solving medical multimodal queries, by focusing especially on improving early precision (i.e., precision at top 5 documents). The queries we deal with in ImageCLEF 2009 contain visual data, given as a set of image-examples, and textual data, provided by a set of words belonging to the categories Anatomy, Pathology, and Modality. Using only a visual-based search technique, it is relatively feasible to identify the modality of a medical image, but it is very challenging to extract from an image the anatomy or the pathology (e.g., slight fracture of a bone). On the other hand, using a text-based search technique, it is relatively easy to extract the anatomy and the pathology from a text, but it is not obvious to identify the modality. Indeed, this latter is not always explicitly described in the text, since the writer might use general words, such as "this image ...," to write their medical report. To overcome these problems, retrieval performance, especially early precision, can be improved demonstrably by merging the results of textual and visual search techniques <ref type="bibr" coords="2,183.48,477.45,7.80,9.96" target="#b6">[7]</ref><ref type="bibr" coords="2,191.29,477.45,3.90,9.96" target="#b7">[8]</ref><ref type="bibr" coords="2,195.19,477.45,7.80,9.96" target="#b8">[9]</ref>.</p><p>In the rest of this paper, we first present a brief description of our system (Section 2). Sections 3 and 4 are dedicated respectively to our visual-based search technique and text-based search technique. We describe the official runs in Section 5, and the corresponding results in Section 6. Finally, we conclude this paper and provide some perspectives (Section 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description of Our Adaptive Medical Image Retrieval System</head><p>Starting in 2007, we created and have continued to evolve a multimodal image retrieval system based on an open-source framework that allows the incorporation of user search preferences. We designed a flexible database schema that allows us to easily incorporate new collections while facilitating retrieval using both text and visual techniques. The 2009 ImageCLEF collection consists of 74,902 medical images and annotations associated with them [10]. This collection contains images and captions from Radiology and Radiographics, two Radiological Society of North America (RSNA) journals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Database and Web Application</head><p>We used the Ruby programming language <ref type="foot" coords="3,328.56,186.94,3.97,4.84" target="#foot_0">1</ref> , with the open source Ruby On Rails<ref type="foot" coords="3,156.60,198.94,3.97,4.84" target="#foot_1">2</ref> web application framework. The PostgreSQL<ref type="foot" coords="3,367.44,198.94,3.97,4.84" target="#foot_2">3</ref> relational database was used to store mapping between the images and the various fields associated with the image. The title, full caption and precise caption, as provided in the data distribution, were indexed. The captions and titles in the collection are currently indexed and we continue to add indexable fields for incorporating visual information. The data distribution included an xml file with the image id, the captions of the images, the titles of the journal articles in which the image had appeared and the PubMed ID of the journal article. In addition, a compressed file containing the approximately 74,900 images was provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Query Parser and Search Engine</head><p>Our system presents a variety of search options to the user including Boolean OR, AND, and "exact match." There are also options to perform fuzzy searches, as well as a custom query parser. A critical aspect of our system is the query parser, written in Ruby. Ferret, a Ruby port of the popular Lucene system, was used in our system as the underlying search engine. The custom query parser performs stop-word removal using a specially-constructed list of stopwords. The custom query parser is highly customizable, and the user has several configuration options from which to choose. The first such option is modality limitation. If the user selects this option, the query is parsed to extract the desired modality, if available. Using the modality fields described in the previous section, only those images that are of the desired modality are returned. This is expected to improve the precision, as only images of the desired modality would be included within the result set. However, there could be a loss in recall if the process of modality extraction and classification is inaccurate.</p><p>The system is linked to the UMLS Metathesaurus; the user may choose to perform manual or automatic query expansion using synonyms from the Metathesarus. In the manual mode, a list of synonyms is presented to the user, which the user can choose to add to the query. In the automatic mode, all synonyms of the UMLS preferred term are added to the query. Another configuration option is the "stem and star" option, in which all the terms in the query are first stemmed. A wildcard (*) is then appended to the word to allow the search of words containing the desired root. The last option allows the user to only send unique terms to the search engine. This can be useful when using the UMLS option, as many of the synonyms have a lot of overlap in the preferred terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Modality Classification and Annotation</head><p>Medical images often begin life with rich metadata in the form of DICOM headers describing their imaging modality or anatomy. However, since most teaching or on-line image collections are made up of compressed standalone JPEG files, it is very common for medical images to exist sans metadata. In previous work <ref type="bibr" coords="4,134.76,191.97,10.00,9.96" target="#b7">[8]</ref>, we described a modality classifier that could identify the imaging modality for medical images using supervised machine learning. We extended that work to the new dataset used for ImageCLEF 2009. We created additional tables in the database to store image information that was created using a variety of image processing techniques in MATLAB <ref type="foot" coords="4,326.28,238.90,3.97,4.84" target="#foot_3">4</ref> . These include color and intensity histograms as well as measures of texture using gray-level co-occurrence matrices and discrete cosine transforms. These features can be used to find images that are visually similar to the query image.</p><p>One of the biggest challenges in creating such a modality classifier is creating a labeled training dataset of sufficient size and quality. Our system, as previously described <ref type="bibr" coords="4,178.68,311.97,10.00,9.96" target="#b7">[8]</ref>, relied on a external training set of modality-labeled images for its supervised learning. In 2009, as in 2008, we did not use any external databases for training the modality classifier. Instead, a Ruby text parser was written to extract the modality from the image captions for all images in the collection using regular expressions as well as a simple Bayesian classifier.</p><p>Note that the quality and accuracy of these labels are not as good as in case of the external training set used in previous experiments. Images where a unique modality could be identified based on the caption were used for training the modality classifier. Grey scale images were classified into a set of modalities including x-rays, CT, MRI, ultrasound and nuclear medicine. Color image classes include gross pathology, microscopy, and endoscopy. The rest of the dataset (i.e., images for which zero or more than one modalities were parsed) was classified using the above classifier. We created two fields in the database for the modality that were indexed by our search engine. The first field contained the modality as extracted by the text parser, and the second contained the modality resulting from the classification process using visual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Text Processing and Analysis</head><p>Our text processing module is very simple; after applying a stop-word list, we index documents and queries using the Vector Space Model (VSM). During the retrieval process, a list of relevant documents can be ranked with regard to their relevance to the corresponding query.</p><p>While no specific treatment was applied to documents, we theorized that it would be useful to use external knowledge to interpret the queries' semantic content. Indeed, each query contains a precise description of a user need materialized by a set of words belonging to three semantic categories: modality of the image (e.g., MRI, x-ray, etc.), anatomy (e.g., leg, head, etc.), and pathology (e.g., cancer, fracture, etc.). We call these categories "domain dimensions," and define them as follow: "A dimension of a domain is a concept used to express the themes in this domain" <ref type="bibr" coords="5,262.81,154.17,14.70,9.96" target="#b10">[11]</ref>. The idea behind our approach is that, in a given domain, a theme can be developed with reference to a set of dimensions of this domain. For instance, a Physician wishing to write a report about a medical image, first they focus on a domain (Medicine), next they refer to specific dimensions of this domain (e.g., Anatomy), then they choose words from this dimension (e.g., femur), and finally they write their report.</p><p>In order to resolve CLEF queries, we proposed using the domain dimensions to interpret their semantic content. To do so, we first need to define the dimensions. For this purpose, we use external resources, such as ontologies or thesaurus, to define each dimension by a hierarchy of concepts. Every concept is denoted by a set of words. Thereafter, to identify dimensions from a query, we extract query's words depending on the dimension hierarchy they belong to. Once dimensions are extracted from each query, we use them to search for relevant documents. In particular, we use Boolean operators on query's dimensions in order to reformulate the initial text of the query and better represent its semantic content. For instance, if we assume that a relevant document must contain all the dimensions belonging to the query, we should use the operator AND between the query's words that represent these dimensions in order to query the document collection.</p><p>Mainly, our querying process contains two steps. The first step consists in using the initial query's text to search for documents based on the VSM. The result of this step is a list of ranked documents called D. The second step consists of selecting, from D, those documents that satisfy the Boolean expression formulated based on the domain dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Runs Submitted</head><p>We submitted a total of 9 offical runs. The search options for the different runs are provided in Table <ref type="table" coords="5,235.58,497.49,3.90,9.96" target="#tab_0">1</ref>. All our runs are automatic; 2 of them are based on textual data and 8 are based on both textual and visual data (mixed).</p><p>The "ohsu j no mod" run is based on the VSM, where each document/query is represented by a vector of words. The result of this run, considered as the baseline, will be compared to those obtained by the other runs, which are based domain dimensions and/or both textual and visual data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Modality Extraction-Based Runs</head><p>We submitted two mixed runs that used the automatically extracted modality to filter results. The custom query parser first extracted the desired modality from the query, if it existed. The "ohsu j mod1" run used the custom parser to remove stop-words from the query and limit the results to the desired modality. This run was expected to have high precision but potentially lower recall as it did not use any term expansion. Also, if the modality classier was not accurate or the modality extraction from the textual query was too strict, the results could be limited. In order to try to increase the recall, we also submitted a run labeled "ohsu j umls" where term expansion based on the UMLS metathesaurus was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dimensions-Based Runs</head><p>With a view to defining the domain dimensions, we utilized the UMLS metathesaurus. The domain dimensions were defined using the UMLS semantic types as follows:</p><p>Anatomy: "Body Part," "Organ, or Organ Component," "Body Space or Junction," "Body Location or Region," and "Cell" Pathology: "Sign or Symptom," "Finding," "Pathologic Function," "Injury or Poisoning," "Disease or Syndrome," "Neoplastic Process," "Neoplasms," "Anatomical Abnormality," "Congenital Abnormality," adn "Acquired Abnormality" Modality: "Manufactured Object," and "Diagnostic Procedure" After automatically extracting the dimensions from each query, we used them to perform the following runs.</p><p>OHSU SR1: We use dimensions to rank the retrieved documents for a given query. Documents that contain the three dimensions belonging to the query are considered as most relevant and are therefore highly ranked. They are followed by those documents that are only missing the modality. Indeed, the latter dimension is not always explicitly described in the text, since the writer might use general words, such as "this image ...," to write their medical report. Finally, in the third rank, we find documents that contain at least one of the query dimensions.</p><p>Since the modality is not always explicitly described in the text, we use the visual data to extract it from images (Section 3). Thereafter, we use it to re-rank the document list obtained using the textual data. In particular, documents of which the modality has been extracted from image are ranked on the top. In all the following runs, we apply this technique using the result of the "ohsu j mod1" run. For simplicity of writing, we call this process "checking modality technique." OHSU SR2: Re-rank the document list obtained during run OHSU SR1 by applying the "checking modality technique." OHSU SR3: Re-rank the document list obtained during run ohsu j no mod by applying the "checking modality technique." OHSU SR4: We select, for each query, those documents that contain the Anatomy and the Pathology belonging to this query. The obtained result is re-ranked by applying the "checking modality technique." OHSU SR5: From the result of the "OHSU SR2" run, we randomly select one image from each article. Indeed, in CLEF documents, a textual article might contain more than one image. In this run, if an article is retrieved by the textual approach, we randomly select one of its images (instead of keeping all images as we have been doing for the other runs).</p><p>OHSU SR6: We select from the result of the "OHSU SR1" run, only those documents of which the modality has been extracted during the "ohsu j mod1" run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Discussion</head><p>For each query, the results are measured by mean average precision of top 1000 documents (MAP), precision at 10 documents (p@10), and precision at 5 documents (p@5). The results given by the baseline run are 0.1223 (MAP), 0.416 (p@10), and 0.38 (p@5). Obtained results are presented in Table <ref type="table" coords="7,423.12,432.21,4.98,9.96" target="#tab_1">2</ref> where rows correspond to the runs, and values correspond to their corresponding results. We also include data from the best runs (based on MAP and p@5) in ImageCLEF 2009 campaign. As described above, our system has been designed to improve precision, perhaps at the expense of recall. Since we do not use any advanced natural language processing and we filter images based on purported modality, we were expecting a relatively low recall. Consequently, as the MAP is highly dependent and limited by recall, we believe that it makes more sense to compare our results to those obtained by the other ImageCLEF 2009's participants at an early precision level (p@5 or p@10). We have a high early precision (p@5 = 0.712) obtained by performing the "ohsu j umls" run. This is the second best result obtained in the ImageCLEF 2009 campaign; the first being equal to 0.744 obtained by an interactive run. In fact, it had the highest p@5 for automatic runs in Image-CLEF2009.</p><p>Independent of the other ImageCLEF 2009 participants, most of our runs outperform our baseline, reaching an improvement of 43% in MAP (OHSU SR1) and 71% in p@5 (ohsu j umls). From the result of the "OHSU SR1" run, we notice that the use of domain dimensions is of great interest in solving medical multimodal queries. Indeed, by using domain dimensions, we highlight the "relevant words" that describe the queries' semantic content. Using these words, the system can retrieve only documents that contain the anatomy, the modality, and the pathology described in the query text.</p><p>We notice that at p@5 and p@10, all our mixed runs outperform our baseline. This is not surprising, because the first ranked documents are those that have been retrieved both by the text-based search technique and the visual-based search technique. This supports our previous conclusions, which confirm that the retrieval performance can be improved demonstrably by merging the results of textual and visual search techniques <ref type="bibr" coords="8,307.10,405.69,7.80,9.96" target="#b6">[7]</ref><ref type="bibr" coords="8,314.91,405.69,3.90,9.96" target="#b7">[8]</ref><ref type="bibr" coords="8,318.81,405.69,7.80,9.96" target="#b8">[9]</ref>.</p><p>Compared to the baseline, our results decreased in two runs in terms of MAP. First, in the "OHSU SR4" run, where the modality dimension was ignored during the querying process. This decrease might be explained by the fact that the modality is described in some documents, and its use is thought to beneficial. This is notably obvious in the "OHSU SR1" run when we used the three dimensions and obtained the highest performance. Secondly, in the "OHSU SR5" run, only one image was randomly selected from each article. An increase in the performance would have been surprising, since this technique is not accurate at all, and there is a high risk that the selected image is irrelevant. Indeed, it is better to keep all images of each article, thus, if one of them is relevant to the corresponding query, it will be retrieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>In order to improve early precision in the task of solving medical multimodal images, we combined a text-based search technique with a visual-based one. The first technique consists in using domain dimensions to highlight relevant words that describe the queries' semantic content. While anatomy and pathology are relatively easy to identify from textual documents, it is quite challenging to identify the modality dimension. To overcome this problem, we used a visual-based search technique that allows automatically extracting the modality from images. The obtained results in terms of precision at p@5 and p@10 are very encouraging and outperform our baseline.</p><p>Among the ImageCLEF 2009's particiants, we obtained the second best overall and best automatic result in terms of precision at p@5. However, in terms of MAP, even though our results outperform our baseline, they are significantly below the best performance obtained in ImageCLEF 2009. This was expected, since we were only focused on improving early precision and did not use any advanced natural language processing to improve the recall. For our future work, we will focus on this issue. We believe that our current text-based search technique has room for improvement. We plan to use further textual processing, such as term expansion and pseudo-relevance feedback, in order to improve our recall, and hope to be able to compare our results to the best ImageCLEF 2009' performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,181.68,115.32,229.39,139.76"><head>Table 1 .</head><label>1</label><figDesc>Description of OHSU runs.</figDesc><table coords="6,181.68,136.08,229.39,119.00"><row><cell>Run Name</cell><cell>Retrieval</cell><cell>Run Type Data Used</cell></row><row><cell></cell><cell>Method</cell><cell></cell></row><row><cell>ohsu j umls</cell><cell>Mixed</cell><cell>Automatic Full caption</cell></row><row><cell>ohsu j mod1</cell><cell>Mixed</cell><cell>Automatic Full caption</cell></row><row><cell>OHSU SR1</cell><cell>Textual</cell><cell>Automatic Full caption</cell></row><row><cell>OHSU SR2</cell><cell>Mixed</cell><cell>Automatic Full caption</cell></row><row><cell>OHSU SR3</cell><cell>Mixed</cell><cell>Automatic Full caption</cell></row><row><cell>OHSU SR4</cell><cell>Mixed</cell><cell>Automatic Full caption</cell></row><row><cell>OHSU SR5</cell><cell>Mixed</cell><cell>Automatic Full caption</cell></row><row><cell>OHSU SR6</cell><cell>Mixed</cell><cell>Automatic Full caption</cell></row><row><cell>ohsu j no mod</cell><cell>Textual</cell><cell>Automatic Full caption</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="7,202.32,502.20,210.78,149.36"><head>Table 2 .</head><label>2</label><figDesc>Results of OHSU runs.</figDesc><table coords="7,202.32,521.16,210.78,130.40"><row><cell>Run Name</cell><cell>MAP P@5 P@10</cell></row><row><cell>ohsu j umls</cell><cell>0.1753 0.712 0.664</cell></row><row><cell>ohsu j mod1</cell><cell>0.1698 0.592 0.552</cell></row><row><cell>OHSU SR1</cell><cell>0.1756 0.592 0.536</cell></row><row><cell>OHSU SR2</cell><cell>0.1582 0.624 0.54</cell></row><row><cell>OHSU SR3</cell><cell>0.1511 0.608 0.524</cell></row><row><cell>OHSU SR4</cell><cell>0.1147 0.6 0.484</cell></row><row><cell>OHSU SR5</cell><cell>0.1133 0.584 0.516</cell></row><row><cell>OHSU SR6</cell><cell>0.1646 0.68 0.612</cell></row><row><cell>ohsu j no mod</cell><cell>0.1223 0.416 0.38</cell></row><row><cell>Best in p@5</cell><cell>0.3775 0.744 0.716</cell></row><row><cell>Best in MAP</cell><cell>0.4293 0.696 0.664</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="3,144.72,634.82,117.03,8.27"><p>http://www.ruby-lang.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,144.72,645.74,126.39,8.27"><p>http://www.rubyonrails.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="3,144.72,656.66,121.71,8.27"><p>http://www.postgresql.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="4,144.72,656.66,117.03,8.27"><p>http://www.mathworks.com/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We acknowledge the support of <rs type="funder">NLM</rs> Training Grant <rs type="grantNumber">2T15LM007088</rs>, <rs type="funder">NSF</rs> Grant <rs type="grantNumber">ITR-0325160</rs>, and the <rs type="funder">Swiss National Science Foundation</rs> grant <rs type="grantNumber">PBGE22-121204</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_mnbEzh5">
					<idno type="grant-number">2T15LM007088</idno>
				</org>
				<org type="funding" xml:id="_VWeC4yZ">
					<idno type="grant-number">ITR-0325160</idno>
				</org>
				<org type="funding" xml:id="_7s4VKyp">
					<idno type="grant-number">PBGE22-121204</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="9,142.88,413.76,337.89,8.96;9,151.56,424.68,328.97,8.96;9,151.56,435.71,172.25,8.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,447.33,413.76,33.44,8.96;9,151.56,424.68,314.25,8.96">Advancing biomedical image retrieval: Development and analysis of a test collection</title>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">N</forename><surname>Gorman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Ruch</surname></persName>
		</author>
		<idno>M</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,475.79,424.68,4.73,8.96;9,151.56,435.71,91.69,8.96">J Am Med Inform Assoc</title>
		<imprint>
			<biblScope unit="page">2082</biblScope>
			<date type="published" when="2006-06">June 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.88,446.88,337.60,8.96;9,151.56,457.80,328.94,8.96;9,151.56,468.71,191.96,8.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="9,396.47,446.88,84.01,8.96;9,151.56,457.80,148.21,8.96">Content-based image retrieval at the end of the early years</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,307.53,457.80,169.27,8.96;9,151.56,468.71,88.99,8.96">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1349" to="1380" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on</note>
</biblStruct>

<biblStruct coords="9,142.88,480.00,337.80,8.96;9,151.56,490.91,283.51,8.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="9,309.07,480.00,171.61,8.96;9,151.56,490.91,70.92,8.96">Medical image databases: A content-based retrieval approach</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">D</forename><surname>Tagare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">C</forename><surname>Jaffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,230.87,490.91,99.49,8.96">J Am Med Inform Assoc</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="184" to="198" />
			<date type="published" when="1997-05">May 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.88,502.08,337.60,8.96;9,151.56,513.00,329.00,8.96;9,151.56,524.03,328.97,8.96;9,151.56,534.96,229.14,8.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="9,369.39,513.00,111.16,8.96;9,151.56,524.03,328.97,8.96;9,151.56,534.96,65.44,8.96">Automated storage and retrieval of thin-section ct images to assist diagnosis: System description and preliminary assessment</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">M</forename><surname>Aisen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">S</forename><surname>Broderick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Winer-Muram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">E</forename><surname>Brodley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Pavlopoulou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Dy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">R</forename><surname>Shyu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Marchiori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,225.45,534.96,40.13,8.96">Radiology</title>
		<imprint>
			<biblScope unit="volume">228</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="265" to="270" />
			<date type="published" when="2003-07">July 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.88,546.11,337.67,8.96;9,151.56,557.16,328.99,8.96;9,151.56,568.08,328.91,8.96;9,151.56,578.99,124.49,8.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="9,358.49,546.11,122.06,8.96;9,151.56,557.16,170.65,8.96">Towards a computer-aided diagnosis system for pigmented skin lesions</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Schmid-Saugeona</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Guillodb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Thirana</surname></persName>
		</author>
		<idno type="PMID">12573891</idno>
	</analytic>
	<monogr>
		<title level="j" coord="9,333.06,557.16,147.49,8.96;9,151.56,568.08,304.14,8.96">Computerized Medical Imaging and Graphics: The Official Journal of the Computerized Medical Imaging Society</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="78" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.88,590.28,337.80,8.96;9,151.56,601.19,328.99,8.96;9,151.56,612.11,320.35,8.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="9,375.40,590.28,105.27,8.96;9,151.56,601.19,328.99,8.96;9,151.56,612.11,18.33,8.96">A review of content-based image retrieval systems in medical applications-clinical benefits and future directions</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Michoux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Bandon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Geissbuhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="9,177.71,612.11,179.30,8.96">International Journal of Medical Informatics</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2004-02">February 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,142.88,623.28,337.67,8.96;9,151.56,634.31,328.80,8.96;9,151.56,645.23,328.92,8.96;9,151.56,656.16,313.87,8.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="9,340.74,623.28,139.81,8.96;9,151.56,634.31,171.18,8.96">Medical image retrieval and automated annotation: Ohsu at imageclef 2006</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,453.33,645.23,27.15,8.96">CLEF</title>
		<title level="s" coord="9,216.84,656.16,140.25,8.96">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Gey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Karlgren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Stempfhuber</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4730</biblScope>
			<biblScope unit="page" from="660" to="669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.88,119.04,337.42,8.96;10,151.56,130.08,328.85,8.96;10,151.56,140.99,239.46,8.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,291.20,119.04,189.10,8.96;10,151.56,130.08,203.09,8.96">Automatic image modality based classification and annotation to improve medical image retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
		<idno type="PMID">17911931</idno>
	</analytic>
	<monogr>
		<title level="j" coord="10,362.24,130.08,118.17,8.96;10,151.56,140.99,63.39,8.96">Studies in Health Technology and Informatics</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">Pt 2</biblScope>
			<biblScope unit="page" from="1334" to="1338" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.88,151.91,337.73,8.96;10,151.56,162.96,329.06,8.96;10,151.56,173.87,218.83,8.96;10,134.76,184.79,7.79,8.96" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,405.23,151.91,75.38,8.96;10,151.56,162.96,230.15,8.96">Combining textual and visual ontologies to solve medical multimodal queries</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Radhouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Hweelim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Pierre Chevallet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Falquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,389.64,162.96,90.98,8.96;10,151.56,173.87,124.68,8.96">Multimedia and Expo, IEEE International Conference</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page" from="1853" to="1856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,172.92,184.80,307.69,8.96;10,151.56,195.84,328.82,8.96;10,151.56,206.76,266.39,8.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,253.51,195.84,222.72,8.96">Overview of the medical retrieval task at imageclef 2009</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Eggel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Radhouani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Bakke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">K</forename><surname>Jr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,151.56,206.76,174.57,8.96">Working Notes of the CLEF 2009 workshop</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,142.55,217.68,338.04,8.96;10,151.56,228.72,328.99,8.96;10,151.56,239.64,152.13,8.96" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Radhouani</surname></persName>
		</author>
		<title level="m" coord="10,215.86,217.68,264.73,8.96;10,151.56,228.72,106.58,8.96">Un mod√®le de recherche d&apos;information orient√© pr√©cision fond√© sur les dimensions de domaine</title>
		<meeting><address><addrLine>France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>University of Geneva, Switzerland, and University of Grenoble</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
