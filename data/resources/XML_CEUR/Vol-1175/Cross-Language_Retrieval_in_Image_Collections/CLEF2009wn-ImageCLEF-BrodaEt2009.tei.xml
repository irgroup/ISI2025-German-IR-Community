<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,109.04,148.86,384.92,15.15;1,90.00,170.78,423.00,15.15;1,222.09,192.69,158.82,15.15">Multimodal Photo Retrieval through Finding Similar Documents Enhanced with Visual Cluesa Baseline Method</title>
				<funder ref="#_v5TYxQj">
					<orgName type="full">Ministry of Science and Higher Education Republic of Poland</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,183.12,226.59,61.06,8.74"><forename type="first">Bartosz</forename><surname>Broda</surname></persName>
							<email>bartosz.broda@pwr.wroc.pl</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Informatics</orgName>
								<orgName type="institution">Wroclaw University of Technology</orgName>
								<address>
									<addrLine>27 Wybrzee Wyspiaskiego 50</addrLine>
									<postCode>-370</postCode>
									<settlement>Wrocaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.42,226.59,85.74,8.74"><forename type="first">Mariusz</forename><surname>Paradowski</surname></persName>
							<email>mariusz.paradowski@pwr.wroc.pl</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Informatics</orgName>
								<orgName type="institution">Wroclaw University of Technology</orgName>
								<address>
									<addrLine>27 Wybrzee Wyspiaskiego 50</addrLine>
									<postCode>-370</postCode>
									<settlement>Wrocaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,346.27,226.59,73.61,8.74"><forename type="first">Halina</forename><surname>Kwanicka</surname></persName>
							<email>halina.kwasnicka@pwr.wroc.pl</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Informatics</orgName>
								<orgName type="institution">Wroclaw University of Technology</orgName>
								<address>
									<addrLine>27 Wybrzee Wyspiaskiego 50</addrLine>
									<postCode>-370</postCode>
									<settlement>Wrocaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,109.04,148.86,384.92,15.15;1,90.00,170.78,423.00,15.15;1,222.09,192.69,158.82,15.15">Multimodal Photo Retrieval through Finding Similar Documents Enhanced with Visual Cluesa Baseline Method</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5E760E0420A0C9A0962C60413C6EBB5A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Languages-Query Languages Measurement, Performance, Experimentation Information retrieval, Image Retrieval, Integrated Region Matching, Mallows Distance, Document similarity, Vector Space Model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image retrieval till today is one of the most challenging problems in computer science. Even though there are lots of researches performed around the World, an efficient, user friendly image retrieval system still seems to be an unachievable goal. Image-CLEF Photo Retrieval Track allows to compare various approaches to this challenging problem. In this paper we present a staring point of our research, connected to a joint Polish-Singaporean research project, titled: Framework for Visual Information Retrieval and Building Content-based Visual Search Engines. Various techniques, published in the literature are gathered and orchestrated together. A reference image retrieval system is build, supporting both image queries, text queries and joint textimage queries. In our work we have tried to capture state-of-the-art in text and image retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we present work performed during our participation in ImageCLEF Photo Retrieval Task. The presented approach is actually a baseline, the starting point of our research. The work is done as a part of joint Polish-Singaporean research project, titled Framework for Visual Information Retrieval and Building Content-based Visual Search Engines. The aim of our work is to build image retrieval methods, incorporating both image matching techniques, 'soft' image distance measures and knowledge based methods. As this is a starting point, all presented techniques are not a novelty. They are already presented in the research literature for at least several years. However, the key difficulty is such combination of these known approaches to have a viable and efficient image retrieval system.</p><p>We participate in ImageCLEF contest for the first time. Our goal is actually to check: Where we are?, comparing to other, often much larger and more experienced research teams. We have achieved the goal by preparing a simple image retrieval system, using state-of-the-art techniques described in the literature. On the other hand, since the contest took place new ideas have appeared and are currently under heavy research. We hope that these ideas will be verified by the next contest -ImageCLEF 2010.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>As the name suggest ImageCLEF 2009 Photo Retrieval Task<ref type="foot" coords="2,362.63,248.90,3.97,6.12" target="#foot_0">1</ref> (henceforth, ICPRT2009) is one of the tasks of information retrieval. The goal of information retrieval, and more specific image retrieval, is to satisfy users information needs -finding images that are relevant to user queries. In ICPRT2009 those needs are specified by detailed description of user queries in form of topics. Topics were divided into two groups containing 25 queries. In first group every topic is described by title and number of clusters. A cluster has title, description and image example. The second group of topics had only title and several image examples. Figure <ref type="figure" coords="2,397.15,322.20,4.98,8.74" target="#fig_1">1</ref> shows one of the topics developed for ICPRT2009 competition.  One of the main goals of the ICPRT2009 competition is to encourage participants to focus on diversity in retrieved collection of images. User queries often do not contain the true intentions, delivered in an machine understandable (or even human understandable) form. They are ambiguous by definition. Taking this into account, diversity is one of the approaches to diminish the problem of query ambiguity. In classical information retrieval, when users searches for word "bank", it is not possible to determine whether user wants to find information about river banks or financial institutions<ref type="foot" coords="3,190.46,110.45,3.97,6.12" target="#foot_1">2</ref> . In image retrieval this corresponds to searching for a Formula 1 bolid by presenting image of a bolid during the race (probably accompanied by short description). Does the user want to find any images of bolids or should the pictures be taken during the race? Perhaps user is interested in pictures of only one team or only from the same event? One of the approaches to this problem is to focus on finding all potentially relevant images that encompasses as broad area of topics as possible.</p><p>One of the most interesting things about the ICPRT2009 is that the work is performed on the real image database from Belga News Agency and the topics created on the basis of analysis of Belga query logs.</p><p>The image database consists of 498,920 images. Every image is accompanied by caption, or annotation, i.e., a few English sentences describing image content. The format of captions is not formally standardized, so basically a caption can contain anything. Fortunately, one can observe a pattern that is usually followed: at the beginning the uppercase letters contains image identifier, date and place followed by description of image content (normal case). Caption is usually ended by attribution of authorship, again written in upper case. Figure <ref type="figure" coords="3,374.75,279.39,4.98,8.74" target="#fig_3">2</ref> shows example of images with caption.</p><p>The size of the database introduces interesting efficiency problems to solve, especially with regard to image processing. At first, image feature vectors need to be computed from the image database. Afterward, image distance calculations need to be made. As it is presented later on in the paper, a single distance calculation requires solving an optimization problem. Such process, repeated for hundreds of thousands of images, is very time consuming. On the other hand, processing almost 500,000 documents (captions) might seems difficult. On the contrary, due to short length of captions, it is rather easy and straightforward process. It is worth noting that file with captions in raw text format has only 162MB, which is little comparing to other information retrieval tasks that has to deal with hundreds of gigabytes of data, e.g., <ref type="bibr" coords="3,410.51,398.94,10.52,8.74" target="#b1">[2,</ref><ref type="bibr" coords="3,422.55,398.94,11.62,8.74" target="#b15">16]</ref>. Creating all the necessary data structures in our case takes from 7 to 15 minutes on commodity PC depending on the experiment setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Development of the Baseline Method</head><p>It is very difficult to define visual similarity between images in a formal way. This is because one would have to know semantics of a given image. Note that there can be a huge difference between images that are visually and semantically similar. Two images of silver cars can be visually very similar, even if one of the images shows an Audi and the other one shows only a toy car, a model of Mercedes. User searching for pictures of Audi would not be satisfied with images of toy cars. On the other hand if the user would be interesting in any silver car picture taken from the side of the car he might be perfectly satisfied with that result. We think that extraction of semantic clues from text is significantly easier then from images so for the ICPRT2009 we assumed model that uses textual features as a primary knowledge source. After initial processing of textual data, visual data is used to refine the results presented to user.</p><p>There is a plethora of methods for textual information retrieval <ref type="bibr" coords="3,395.67,596.99,14.61,8.74" target="#b9">[10]</ref>. Combining this with techniques using visual data in image retrieval <ref type="bibr" coords="3,302.49,608.95,10.52,8.74" target="#b2">[3]</ref> gives very large number of possible ways to approach ICPRT2009.</p><p>As mentioned earlier in the paper, the presented research is a part of joint Polish-Singaporean research project, titled: Framework for Visual Information Retrieval and Building Content-based Visual Search Engines. One of aims of our project is to develop methods for finding visually similar images to a given one using only visual information. We divided the work into two stages: creating ranked list of similar images for every image in a topic and combining those lists considering all the images in the topic. We separated textual processing from visual, because we wanted to focus on robust and reliable techniques from both visual and textual point of views separately. After that we have developed a method for combining both knowledge sources. Our main aim in this research is to create prototype method for image retrieval for which precision of retrieval is  the most important factor. As mentioned earlier, we are treating this work as a test ground for developing a baseline method upon which we are going to build more complete solutions later on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Using Visual Clues</head><p>Similarity search in image domain always have and still is a great challenge <ref type="bibr" coords="4,420.87,635.46,9.96,8.74" target="#b2">[3]</ref>. Despite there are hundreds of different approaches proposed in the literature, there is no general one working for a large domain of images. To make an image retrieval system, one has to decide about three key components: image distance function, image segmentation method and feature extraction method.</p><p>Proper selection of all these components, so they fit together, is a difficult problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Distance function</head><p>In our work we have examined several methods widely discussed in the literature. Local image distances are our major interest topic. Local image distance is a distance calculated between individual segments of images and later on transformed into an image distance. Such distance operates on sets of feature vectors (they are not ordered), instead on single feature vectors. This means that a single image I has to be defined in terms of its segments (and feature vectors) i k : k = {1, ..n} as follows:</p><formula xml:id="formula_0" coords="5,262.79,171.80,250.21,9.65">I = {i 1 , i 2 , ..., i n }.<label>(1)</label></formula><p>As a result of our research the decision is made to use a variant of Mallows distance, called Integrated Region Matching <ref type="bibr" coords="5,214.57,201.68,10.52,8.74" target="#b7">[8,</ref><ref type="bibr" coords="5,226.81,201.68,11.62,8.74" target="#b16">17]</ref>. The method has been proposed by Wang in 2001 as a part of SIMPLIcity image retrieval system. Our experiments have shown that till today it is one of the most effective means of image retrieval. The distance function is defined as an optimization problem:</p><formula xml:id="formula_1" coords="5,228.82,261.46,284.18,20.06">D(I, J) = min S=[sij ] i∈I j∈J s ij d(i, j),<label>(2)</label></formula><p>constrained by:</p><formula xml:id="formula_2" coords="5,147.75,310.83,365.25,31.18">s ij ≥ 0, 1 ≤ i ≤ |I|, 1 ≤ j ≤ |J|, |I| i=1 |J| j=1 s ij = |I| i=1 p i = |J| j=1 q j = 1,<label>(3)</label></formula><formula xml:id="formula_3" coords="5,182.37,352.44,330.63,31.18">|J| j=1 s ij = p i , 1 ≤ i ≤ |I|, |I| i=1 s ij = q j , 1 ≤ j ≤ |J|,<label>(4)</label></formula><p>where: I, J -feature vector sets, i, j -single vectors belonging to I and J, respectively, S = [s ij ] -significance matrix, the search space of the optimization methods, p i -probability of a region i ∈ I (usually equal to segment relative size), q j -probability of a region j ∈ J (usually equal to segment relative size), d(i, j) -vector distance measure, usually Euclidean distance.</p><p>Of course the optimization problem itself is very challenging and finding a global solution is simply not feasible. Integrated Region Matching approach is in fact an iterative greedy algorithm of image segment pairing. Distance calculation between two images does not take too much time, however calculating it for the whole database requires much computational power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Image segmentation</head><p>The second mentioned key component is image segmentation. Processed image database is a general type database, containing various kinds of images, encompassing many visual domains. For such databases supervised image segmentation approach would have to consider all these domains and is not a good choice. This means, that an unsupervised image segmentation, with all its advantages and disadvantages has to be selected.</p><p>Unsupervised methods may be even further divided into block-based approaches (fixed image cuts) and region-based approaches (segmentation in a classical sense). Block-based methods have been adopted to automatic image annotation as an effective means of image segmentation <ref type="bibr" coords="5,478.30,639.24,9.96,8.74" target="#b3">[4]</ref>. Our earlier research in automatic image annotation <ref type="bibr" coords="5,297.15,651.19,10.52,8.74" target="#b5">[6,</ref><ref type="bibr" coords="5,309.03,651.19,12.73,8.74" target="#b10">11]</ref> also confirmed this observation. Generated feature vectors are much less prone to changes due to slight changes of image content.</p><p>The chosen image segmentation approach is a regular grid method. The grid itself has 5 × 5 dimensions for every image in the database. This means that each image is split into 25 identical in size, rectangular blocks.</p><p>As VSM is commonly known technique in natural language processing community we will outline only main points for readers with no experience in this area. The most important concept in VSM is that a document D i is represented as a vector in an n-dimensional feature space, where n is a number of different terms found during indexing:</p><formula xml:id="formula_4" coords="7,236.22,171.80,272.53,9.65">D i =&lt; tf i,1 , tf i,2 , . . . , tf i,n &gt; , (<label>5</label></formula><formula xml:id="formula_5" coords="7,508.75,171.80,4.24,8.74">)</formula><p>where t i,j is number of occurrences of term j in document i. As some frequencies of occurrences can be accidental it is best to use some weighting scheme. For this work we also focused on using classic and robust method, i.e., tf.idf weighting scheme <ref type="bibr" coords="7,330.64,213.13,14.61,8.74" target="#b12">[13]</ref>. Instead of using raw term frequencies we use weights, that are calculated for term t in document d in the following way:</p><formula xml:id="formula_6" coords="7,249.41,244.84,259.36,23.22">tf.idf t,d = tf t,d • log N df t , (<label>6</label></formula><formula xml:id="formula_7" coords="7,508.76,251.58,4.24,8.74">)</formula><p>where N is number of documents (captions), and df t is number of documents containing the term t. Representation of captions as a vectors enables usage of many similarity (or distance) measures known form literature. We used cosine as a similarity measure for our baseline method, as it is shown many times that it copes well with high dimensional data spaces, including documents represented in VSM <ref type="bibr" coords="7,180.05,322.10,10.52,8.74" target="#b0">[1,</ref><ref type="bibr" coords="7,191.86,322.10,7.75,8.74" target="#b8">9,</ref><ref type="bibr" coords="7,200.91,322.10,11.62,8.74" target="#b9">10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Combining Text with Images</head><p>As precision is our top priority rather than diversity, we did not come up with very elaborate techniques for problem of merging visual and textual features into the final model. Both, the visual and textual part of our system produces ranked lists of images for every image that appeared in topics accompanied by a score. For generation of textual list we used a cosine measure, which is a similarity measure giving values from 0 to 1 (in case of positive vector values). On the other hand, Integrated Region Matching (IRM) is an unbounded distance function. To convert distance to similarity we subtract from one normalized value of IRM function. After the conversion we simply multiply values of both functions if cosine is lower then threshold t = 0.8. We introduced the threshold in order to preserve almost perfect matches from textual phase.</p><p>The multiplication of both similarity functions results in a single ranked list of images for every image in topics. As every topic contain a few images, only one question remains: how to combine different similarity lists into one list for topic. In this step we also use very simple methods. First method we considered is naive joining of sorted similarity lists into one big list sorted by similarities. Duplicated images are removed from this list and the top 20 images are presented to user (or for evaluation). Second method is more balanced : we calculate k as a simple division of 20 by a number of clusters in the topic. Then from every list we select k best images and combine them all into resulting list. In case of duplicates we draw more images from randomly selected list among the lists containing duplicates. Resulting list is also sorted by similarity values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>For evaluation we submitted five runs: four using both textual and visual clues and one for textual data only. These five runs differ only in the method of combining similarity lists for individual images into final list for topics. The other difference is usage of stemmer. Table <ref type="table" coords="7,453.51,650.00,4.98,8.74" target="#tab_0">1</ref> summarizes result achieved by our system. The measures used for evaluation are precision, cluster recall and F1-measure at different cutoff levels. For brevity, we show only two cutoff levels: at 10 and 20 documents. The former is used by organizers to rank all participating system in ICPRT2009 and the latter is the number of results we submitted for each topic.</p><p>Results for runs presented in the Table <ref type="table" coords="7,278.08,709.78,4.98,8.74" target="#tab_0">1</ref> differ very little. Not surprisingly with higher cutoff value the precision is lowered and the cluster recall is higher. Usage of stemmer has little impact on this task. Balanced scheme of similarity list joining seems to be better, especially in terms of cluster recall. Nevertheless, the difference is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stemmer Type</head><p>Modality CR@10 CR@20 P@10 P@20 F@10 F@20 no balanced TXTIMG 0. We approached the development of our baseline system with the proper method for evaluation the results in mind. That is why we decided to create independently from organizers a system for evaluation of precision. We did not consider evaluation of cluster recall, because as mentioned earlier we focus on the quality of retrieved list of similarities, especially with regard to visual similarity. Figure <ref type="figure" coords="8,171.25,284.63,4.98,8.74" target="#fig_4">3</ref> shows example screen from application developed for supporting of manual annotation of results. All the results are stored in database and automatically retrieved when needed, so the pair of images has to be annotated only once even if it repeatedly occurs in different experiments. To shorten time needed for evaluation we added functionality for selection of statistically significant random sample from whole result set <ref type="bibr" coords="8,367.15,332.45,9.96,8.74" target="#b4">[5]</ref>. Surprisingly the result of evaluation that were obtained by our team were significantly lower then those prepared by ICPRT2009 organizers (see Tab. 1). Our best method did not achieved better precision than 0.56 (at cutoff of 20 documents). We think that main cause for this is that we evaluated n times more images for every topic then in submitted runs, where n is number of clusters in the topic. Another reason for lower precision is the problem that some images might have very vague caption or caption that did not corresponds exactly to the cluster description. For example, topics number 21 with the title "princess maxima" contains cluster with Princess Maxima appearing in different years. The cluster that describes year 2002 has an image of Princess Maxima during skiing. Naturally, textual part of the system is mislead into finding images containing other people skiing with similar captions. Also visually other people skiing are more similar than pictures from different time of the same year with Princess Maxima.</p><p>Figure <ref type="figure" coords="8,135.48,745.64,4.98,8.74" target="#fig_5">4</ref> shows example of five best images retrieved for second cluster of topic 19 titled "justine   henin kim clijsters". The cluster description says that relevant cluster should contain photographs of both Justine Henin and Kim Clijsters in the foreground to be relevant. This example shows an interesting case, because retrieved images are both semantically and visually similar to user query. On the other hand, we want to show the problems of usage of visual similarity based on high level statistical features derived from images segmented with grid for semantically oriented user queries. Figure <ref type="figure" coords="10,183.31,112.02,4.98,8.74" target="#fig_7">5</ref> shows query image containing Fortis Bank logo, and most similar image from whole Belga repository using only visual features. Without consideration for semantics of the image those two images are highly similar in our opinion. Problem shown on Fig. <ref type="figure" coords="10,466.41,135.93,4.98,8.74" target="#fig_7">5</ref> is not an isolated case. This is the reason that we did not consider using only visual clues for submitting runs for ICPRT2009.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Further Work</head><p>We presented a system implementing baseline method for image retrieval that took part in Image-CLEF 2009 Photo Retrieval task. The method consists of three fairly independent components: visual, textual and merged. Splitting our system onto those parts has many benefits. We can improve each individual component independently, without affecting others, we are able to measure how much textual and visual modules contribute to the final results, etc.</p><p>In visual image retrieval part we have orchestrated three important components: image distance function, an image segmentation method and feature extraction approach. All these components play an important role is the system and are responsible for the quality of achieved visual processing.</p><p>Textual image retrieval by caption is based on the well-known Vector Space Model using tf.idf weighting scheme and cosine as a measure of similarity between images captions <ref type="bibr" coords="10,451.11,334.16,14.61,8.74" target="#b12">[13]</ref>. We used also very naive technique for merging results of visual and textual components. Surprisingly we obtained rather high results with F1-measure lower only by 0.13 from the best system participating in this edition of Photo Retrieval Task. More surprisingly, when we consider only precision, which we were aiming at, the difference of our best approach from the most precise system in the competition is even lower: 0.05.</p><p>Obviously as this is our baseline system, there are many areas of improvement. From textual point of view one can try plethora of methods that were developed recently. Usage of deeper syntactic and semantic analysis can improve performance. Also, merging step in our method needs dramatical improvement. We consider using clustering algorithms for better partitioning both the data and the subset of the data containing only retrieved images. Developing a measure of certainty of retrieval for both visual and textual parts will lead to more intelligent ways of joining results using different modality.</p><p>As mentioned through the paper, we are intensely developing various approaches in image retrieval and automatic image annotation. It is worth mentioning the improved version of MAGMA <ref type="bibr" coords="10,90.00,513.49,15.50,8.74" target="#b14">[15]</ref> image annotation system, which also took part in ImageCLEF 2009 contest. We have also proposed a theoretical model of optimal automatic image annotation and its practical realization, called Greedy Resulted Words Count Optimizer <ref type="bibr" coords="10,294.62,537.40,11.44,8.74" target="#b6">[7,</ref><ref type="bibr" coords="10,306.05,537.40,11.44,8.74" target="#b10">11]</ref>. Another concept being currently researched (but not yet published) is an extension of Integrated Region Matching matching. All presented image retrieval methods are based purely on spatial, color and texture features. We are also working on integration of those features with local features, such like SIFT or Pattern-based approximations of Patches using Hough Transform <ref type="bibr" coords="10,321.02,585.22,14.61,8.74" target="#b13">[14]</ref>. We hope to use at least some of the mentioned approaches in coming ImageCLEF 2010.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,95.43,531.45,115.93,6.99;2,95.43,540.91,115.93,6.99;2,95.43,550.38,115.93,6.99;2,95.43,559.84,115.93,6.99;2,95.43,569.30,115.93,6.99;2,95.43,578.77,115.93,6.99;2,95.43,588.23,115.93,6.99;2,95.43,597.70,108.48,6.99;2,214.68,531.45,138.22,6.99;2,214.68,540.91,138.22,6.99;2,214.68,550.38,138.22,6.99;2,214.68,559.84,70.57,6.99;2,301.57,559.84,51.34,6.99;2,214.68,569.30,138.22,6.99;2,214.68,578.77,138.22,6.99;2,214.68,588.23,19.53,6.99;2,356.22,531.45,151.34,6.99;2,356.22,540.91,151.35,6.99;2,356.22,550.38,151.34,6.99;2,356.22,559.84,151.34,6.99;2,356.22,569.30,31.52,6.99"><head></head><label></label><figDesc>(a) Cluster title: kim gevaert. Description: Relevant images will show photographs of Kim Gevaert. Images showing her and other people are relevant if she is shown in the foreground. Other images where Kim is in the foreground are irrelevant. (b) Cluster title: agfa gevaert. Description: Images are relevant if they contain photographs of the Agfa-Gevaert company. Relevant images include those showing the logos, buildings or any aspects of the company. (c) Cluster title: hellebaut gevaert. Description: Relevant images show photographs of Hellebaut and Kim Gevaert. Images showing only one of them are not relevant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,90.00,620.75,423.00,8.74;2,90.00,632.70,121.98,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of user query in form of a topic. The topic has number 7, is titled gevaert and contains only three clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,91.58,248.60,199.34,6.99;4,91.58,258.07,199.33,6.99;4,91.58,267.53,199.34,6.99;4,91.58,277.00,199.34,6.99;4,91.58,286.46,84.16,6.99;4,294.24,248.60,217.18,6.99;4,294.24,258.07,217.18,6.99;4,294.24,267.53,217.18,6.99;4,294.24,277.00,217.18,6.99;4,294.24,286.46,217.18,6.99;4,294.24,295.92,100.36,6.99;4,96.02,447.09,206.09,6.99;4,96.02,456.55,206.10,6.99;4,96.02,466.01,206.09,6.99;4,96.02,475.48,206.09,6.99;4,96.02,484.94,206.10,6.99;4,96.02,494.41,206.09,6.99;4,96.02,503.87,206.10,6.99;4,96.02,513.34,206.09,6.99;4,96.02,522.80,79.56,6.99;4,305.44,447.09,201.54,6.99;4,305.44,456.55,201.54,6.99;4,305.44,466.01,201.54,6.99;4,305.44,475.48,201.54,6.99;4,305.44,484.94,201.54,6.99;4,305.44,494.41,201.54,6.99;4,305.44,503.87,147.77,6.99"><head></head><label></label><figDesc>(a) BRU199 -20031012 -METTET, BELGIUM : Illustration picture shows a pilot making smoke during a burn-put wheelie at the Superbiker Grand Prix of Mettet, Sunday 12 October 2003, in Mettet. BELGA PHOTO JOHN THYS (b) The early morning rising sun over the Bavarian town of Marktoberdorf gives a golden glow to the cloud cover Tuesday 2nd December 2003.. This area just short of the Alpine region continues to enjoy extraordinary mild temperatures with 16degrees c. recorded yesterday. EPA/Karl-Josef Hildenbrand COLOR (c) Japan's maglev train setting a world speed record on an experimental track in Yamanashi Province, Tuesday 02 Decemeber 2003. The three-car magnetically levitated train reached a maximum speed of 581 kilometers per hour with technicians on board, according to Central Japan Railway Co. (JR Tokai) and the governmentaffiliated Railway Technical Research Institute, who operates the experimental train. EPA/EVERETT KENNEDY BROWN (d) LHT22 -20010223 -LAHTI, FINLAND : From L to R Germany's Martin Schmitt, silver medal, Poland's Adam Malysz, gold medal and Austria's Martin Hoellwarth, bronze medal, jubilate on the podium after the K 90 ski jump final at the Nordic World Ski Championships in Lahti on Friday, 23 February 2001. EPA PHOTO EPA-ANJA NIEDRINGHAUS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,102.54,544.85,397.92,8.74"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of image with caption (annotation) from Belga News Agency collection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,90.00,555.63,423.00,8.74;8,90.00,567.58,423.00,8.74;8,90.00,579.54,423.00,8.74;8,90.00,591.49,128.51,8.74;8,90.00,352.99,423.00,187.53"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example screen from evaluation application. On the left is the window showing 20 most similar ids of images to the image in cluster shown on the right returned by one of our methods. When user selects id of image for evaluation the image is shown in the frame. All the resulting images are already evaluated.</figDesc><graphic coords="8,90.00,352.99,423.00,187.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,120.51,499.50,361.97,8.74;9,115.48,524.12,292.09,93.56"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example of our method: query image 4(a) with five most similar images.</figDesc><graphic coords="9,115.48,524.12,292.09,93.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="9,140.98,664.62,321.04,8.74;9,424.31,524.13,63.22,93.55"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Example of visual similarity for one of the images in the topics.</figDesc><graphic coords="9,424.31,524.13,63.22,93.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="8,96.13,125.17,410.75,89.98"><head>Table 1 :</head><label>1</label><figDesc>Results of experiments sorted by F1-measure at with cut-off at 10 position.</figDesc><table coords="8,279.67,125.17,227.21,8.74"><row><cell>5991</cell><cell>0.7300</cell><cell>0.8</cell><cell>0.71 0.6837 0,7356</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,735.89,131.26,6.64"><p>http://imageclef.org/2009/photo</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="3,105.24,747.00,390.06,6.99"><p>At least without asking the user for clarification or introduction of additional techniques like user profile.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,105.24,716.22,291.31,6.99"><p>Method taken from Bio-medical Imaging Java library, see: http://bij.isi.uu.nl/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="6,105.24,725.72,146.25,6.99"><p>Creating heuristics that would work for</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="50" xml:id="foot_4" coords="6,265.53,725.72,247.47,6.99;6,90.00,735.19,211.92,6.99"><p>topics provided by ICPRT2009 organizer would be feasible, but we assumed that description are not constrained in any way.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment This work is financed from the <rs type="funder">Ministry of Science and Higher Education Republic of Poland</rs> resources in 2008-2010 years as a <rs type="grantName">Poland-Singapore joint</rs> research project <rs type="grantNumber">65/N-SINGAPORE/2007/0</rs>. It is supported by the <rs type="institution">DCS-Lab</rs>, which is operated by the <rs type="institution">Department of Distributed Computer Systems (DDCS) at the Institute of Informatics, Wroclaw University of Technology, Wroclaw, Poland</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_v5TYxQj">
					<idno type="grant-number">65/N-SINGAPORE/2007/0</idno>
					<orgName type="grant-name">Poland-Singapore joint</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Feature extraction</head><p>The last key component are image features. In this research we have focused on three types of image segment features: location, color and texture. Such approach is popular among image retrieval and automatic image annotation research, e.g. <ref type="bibr" coords="6,336.78,154.32,9.96,8.74" target="#b3">[4]</ref>. Let us now describe features we are using. Location related features are:</p><p>• normalized region size (which is constant due to grid segmentation),</p><p>• region average x and y coordinates.</p><p>Color features are rather straightforward. Only two basic color models are used:</p><p>• region intensity means red, green and blue (RGB color model),</p><p>• region intensity standard deviations red, green and blue,</p><p>• region intensity means hue, brightness and saturation (HSV color model),</p><p>• region intensity standard deviations hue, brightness and saturation.</p><p>Texture features are much more complex. They include result of image processing by Sobel edge detector, Hessian-based edge detector 3 and concurrence matrices:</p><p>• region intensity of Sobel edge detector means for red, green and blue channels,</p><p>• region intensity of Sobel edge detector standard deviations for red, green and blue channels,</p><p>• region intensity of Hessian-based edge detector means for red, green and blue channels,</p><p>• region intensity of Hessian-based edge detector standard deviations for red, green and blue channels,</p><p>• region concurrence matrix values: correlation, entropy, homogeneity, contrast, dissimilarity, energy and sum of all values for red, green and blue channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Using Textual Data</head><p>The idea for textual retrieval is similar to described in previous section for visual features. First we extract textual features for each image in the collection and than we use similarity measure to compare pairs of images. As this is baseline method for our group, we assumed only very shallow processing without usage of elaborate language processing tools. Also we used classic representation of textual data, namely Vector Space Model (VSM) <ref type="bibr" coords="6,382.84,565.23,15.50,8.74" target="#b9">[10,</ref><ref type="bibr" coords="6,399.63,565.23,11.62,8.74" target="#b12">13]</ref>.</p><p>After initial inspection of both the topics and images captions we decided to use only images caption. We assumed that captions describe the image in a better way than cluster descriptions for VSM. For humans, cluster description is more informative, but it would require deeper level of processing and usage of more elaborate language processing tools. E.g., we would need to resolve negations for searching for terms that are irrelevant 4 , prepare special stop-list, etc.</p><p>We treat each caption as a document. After preprocessing the documents are represented in VSM. We use cosine as a measure of similarity. Preprocessing involves two steps. We use a stoplist, i.e., a list of a few hundred words that do not contribute much to semantics of a document, like prepositions and articles. In some of our experiments we also use classic Porter stemming algorithm <ref type="bibr" coords="6,135.04,684.78,14.61,8.74" target="#b11">[12]</ref>. This allows us to both reduce index size and treat different morphological variants of a word (e.g., plural forms of a word) as one object called stem.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="11,110.48,133.84,402.52,8.74;11,110.48,145.80,402.52,8.74;11,110.48,157.75,402.52,8.74;11,110.48,169.71,402.52,8.74;11,110.48,181.66,341.74,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="11,282.64,133.84,230.36,8.74;11,110.48,145.80,246.06,8.74">Experiments in documents clustering for the automatic acquisition of lexical semantic networks for Polish</title>
		<author>
			<persName coords=""><forename type="first">Bartosz</forename><surname>Broda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maciej</forename><surname>Piasecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,450.19,157.75,62.81,8.74;11,110.48,169.71,402.52,8.74;11,110.48,181.66,46.33,8.74">Proceedings of the Sixteenth International Conference on Intelligent Information Systems, Advances in Soft Computing</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Mieczysaw</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Adam</forename><surname>Kopotek</surname></persName>
		</editor>
		<editor>
			<persName><surname>Przepirkowski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Sawomir</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Krzysztof</forename><surname>Wierzcho</surname></persName>
		</editor>
		<editor>
			<persName><surname>Trojanowski</surname></persName>
		</editor>
		<meeting>the Sixteenth International Conference on Intelligent Information Systems, Advances in Soft Computing<address><addrLine>Warsaw</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Publishing House EXIT</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,199.95,402.52,8.74;11,110.48,211.90,336.77,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="11,297.52,199.95,194.44,8.74">Overview of the TREC 2004 terabyte track</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,110.48,211.90,217.44,8.74">Proceedings of the 13th Text REtrieval Conference</title>
		<meeting>the 13th Text REtrieval Conference<address><addrLine>Gaithersburg, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,230.19,402.52,8.74;11,110.48,242.15,328.06,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="11,366.72,230.19,146.28,8.74;11,110.48,242.15,111.75,8.74">Image retrieval: Ideas, influences, and trends of the new age</title>
		<author>
			<persName coords=""><forename type="first">Ritendra</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dhiraj</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,230.93,242.15,147.33,8.74">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,260.44,402.52,8.74;11,110.48,272.39,402.52,8.74;11,110.48,284.35,361.04,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="11,312.01,260.44,200.99,8.74;11,110.48,272.39,93.08,8.74">Multiple bernoulli relevance models for image and video annotation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><forename type="middle">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,229.21,272.39,283.78,8.74;11,110.48,284.35,182.25,8.74">Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1002" to="1009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,302.63,384.14,8.74" xml:id="b4">
	<monogr>
		<title level="m" type="main" coord="11,184.37,302.63,105.37,8.74">Determining sample size</title>
		<author>
			<persName coords=""><forename type="first">Glenn</forename><forename type="middle">D</forename><surname>Israel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
		<respStmt>
			<orgName>University of Florida</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="11,110.48,320.92,402.53,8.74;11,110.48,332.88,402.52,8.74;11,110.48,344.83,43.72,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="11,315.20,320.92,197.80,8.74;11,110.48,332.88,110.72,8.74">Fast image auto-annotation with discretized feature distance measures</title>
		<author>
			<persName coords=""><forename type="first">Halina</forename><surname>Kwasnicka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mariusz</forename><surname>Paradowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,231.65,332.88,227.60,8.74">Machine Graphics and Vision International Journal</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,363.12,402.52,8.74;11,110.48,375.08,402.52,8.74;11,110.48,387.03,22.69,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="11,315.51,363.12,197.49,8.74;11,110.48,375.08,215.98,8.74">Resulted word counts optimization -a new approach for better automatic image annotation</title>
		<author>
			<persName coords=""><forename type="first">Halina</forename><surname>Kwasnicka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mariusz</forename><surname>Paradowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,339.61,375.08,85.89,8.74">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3562" to="3571" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,405.32,402.52,8.74;11,110.48,417.27,133.76,8.74" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="11,321.49,405.32,191.51,8.74;11,110.48,417.27,34.67,8.74">Irm: Integrated region matching for image retrieval</title>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gio</forename><surname>Wiederhold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="147" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,435.56,402.52,8.74;11,110.48,447.52,75.00,8.74" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="11,250.81,435.56,236.76,8.74">Foundations of Statistical Natural Language Processing</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,465.81,402.52,8.74;11,110.48,477.76,193.83,8.74" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="11,302.36,465.81,156.16,8.74">Introduction to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schtze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,496.05,402.52,8.74;11,110.48,508.00,355.44,8.74" xml:id="b10">
	<monogr>
		<title level="m" type="main" coord="11,207.78,496.05,305.22,8.74;11,110.48,508.00,30.30,8.74">Automatic Image Annotation as an Effective Method for Image Captioning</title>
		<author>
			<persName coords=""><forename type="first">Mariusz</forename><surname>Paradowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Poland</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Wroclaw University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note>in Polish</note>
</biblStruct>

<biblStruct coords="11,110.48,526.29,302.76,8.74" xml:id="b11">
	<monogr>
		<title level="m" type="main" coord="11,173.07,526.29,140.77,8.74">An algorithm for suffix stripping</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="313" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,544.58,402.52,8.74;11,110.48,556.54,123.40,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="11,271.61,544.58,191.00,8.74">A vector space model for automatic indexing</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,470.48,544.58,42.52,8.74;11,110.48,556.54,19.49,8.74">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="613" to="620" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,574.82,402.52,8.74;11,110.48,586.78,402.52,8.74;11,110.48,598.73,22.69,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="11,184.13,574.82,328.87,8.74;11,110.48,586.78,174.23,8.74">Building local features from pattern-based approximations of patches: Discussion onmoments and hough transform</title>
		<author>
			<persName coords=""><forename type="first">Andrzej</forename><surname>Sluzek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,292.41,586.78,216.24,8.74">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,617.02,402.52,8.74;11,110.48,628.98,402.52,8.74;11,110.48,640.93,237.10,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="11,438.79,617.02,74.21,8.74;11,110.48,628.98,402.52,8.74;11,110.48,640.93,28.28,8.74">Magma -efficient method for image annotation in low dimensional feature space based on multivariate gaussian models</title>
		<author>
			<persName coords=""><forename type="first">Michal</forename><surname>Stanek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bartosz</forename><surname>Broda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Halina</forename><surname>Kwasnicka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mariusz</forename><surname>Paradowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="11,173.35,640.93,98.35,8.74">Proc. of IMCSIT 2009</title>
		<meeting>of IMCSIT 2009</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>accepted</note>
</biblStruct>

<biblStruct coords="11,110.48,659.22,402.53,8.74;11,110.48,671.18,402.52,8.74;11,110.48,683.13,147.41,8.74" xml:id="b15">
	<analytic>
	</analytic>
	<monogr>
		<title level="m" coord="11,332.15,659.22,180.86,8.74;11,110.48,671.18,136.65,8.74">Proceedings of The Seventeenth Text REtrieval Conference, TREC 2008</title>
		<editor>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lori</forename><forename type="middle">P</forename><surname>Buckland</surname></persName>
		</editor>
		<meeting>The Seventeenth Text REtrieval Conference, TREC 2008<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">November 18-21, 2008</date>
			<biblScope unit="page" from="500" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,110.48,713.37,402.53,8.74;11,110.48,725.33,402.52,8.74;11,110.48,737.28,118.62,8.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="11,326.65,713.37,186.35,8.74;11,110.48,725.33,126.68,8.74">Simplicity: Semantics-sensitive integrated matching for picture libraries</title>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gio</forename><surname>Wiederhold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="11,245.57,725.33,267.43,8.74;11,110.48,737.28,21.83,8.74">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="947" to="963" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
