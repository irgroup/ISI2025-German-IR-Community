<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,114.66,74.45,365.84,12.58;1,179.82,92.93,235.61,12.58">ImageCLEF 2009 Medical Image Annotation Task: PCTs for Hierarchical Multi-Label Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,156.48,120.51,70.89,8.74"><forename type="first">Ivica</forename><surname>Dimitrovski</surname></persName>
						</author>
						<author>
							<persName coords="1,235.55,120.51,51.39,8.74"><forename type="first">Dragi</forename><surname>Kocev</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Knowledge Technologies</orgName>
								<orgName type="institution">Jozef Stefan Institute</orgName>
								<address>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,295.16,120.51,74.74,8.74"><forename type="first">Suzana</forename><surname>Loskovska</surname></persName>
						</author>
						<author>
							<persName coords="1,378.18,120.51,57.45,8.74"><forename type="first">Saso</forename><surname>Dzeroski</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Knowledge Technologies</orgName>
								<orgName type="institution">Jozef Stefan Institute</orgName>
								<address>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Electrical Engineering and Information Technologies</orgName>
								<address>
									<settlement>Skopje</settlement>
									<country key="MK">Macedonia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,114.66,74.45,365.84,12.58;1,179.82,92.93,235.61,12.58">ImageCLEF 2009 Medical Image Annotation Task: PCTs for Hierarchical Multi-Label Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FA82AFCC1F3ED8C5820EFE63D53296CE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing; H.3.3 Information Search and Retrieval Automatic Image Annotation</term>
					<term>Scale Invariant Feature Transform</term>
					<term>Edge Histogram Descriptor</term>
					<term>Hierarchical Multi-Label Classification</term>
					<term>Predictive Clustering Trees</term>
					<term>Random Forests</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we describe an approach for the automatic medical image annotation task of the 2009 CLEF cross-language image retrieval campaign (ImageCLEF). This work is focused on the process of feature extraction from radiological images and hierarchical multi-label classification. To extract features from the images we used an edge histogram descriptor as global feature and SIFT histogram as local feature. These feature vectors were combined through simple concatenation in one feature vector with 2080 variables. With the combination of global and local features we want to tackle the problem of intra-class variability vs. inter-class similarity and the problem of data unbalance between train and test datasets. For classification we selected an extension of the predictive clustering trees (PCTs) able to handle data types organized in hierarchy. Furthermore, we constructed ensembles (Bagging and Random Forests) that use PCTs as base classifiers to improve the performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.22" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The amount of medical images produced nowadays is constantly growing. Manual description and annotation of each image is time consuming, expensive and impractical. This calls for development of automatic image annotation algorithms that can perform the task reliably. With the automatic annotation an image is classified into set of classes. If these classes are organized in a hierarchy then it is a case of hierarchical multilabel classification.</p><p>This paper describes our approach for the medical image annotation task of ImageCLEF 2009. The objective of this task is to provide the IRMA (Image Retrieval in Medical Applications) code <ref type="bibr" coords="1,450.08,610.41,11.70,8.74" target="#b0">[1]</ref> for each image of a given set of previously unseen medical (radiological) images. The results of the classification step can be used for multilingual image annotations as well as for DICOM standard header corrections.</p><p>A database of 12677 fully classified radiographs, taken randomly from medical routine, is provided to be used in any way to train a classifier. Images are labeled according to four classification label sets considering:</p><p>- In the test phase we have to classify and assigned the correct labels to 1733 radiographs according to the four different schemes. The IRMA coding system consists of four axes with three to four positions, each in {0,…, 9, a,…, z}, where "0" denotes "unspecified" to determine the end of a path along an axis:</p><p>-T (Technical): image modality -D (Directional): body orientation -A (Anatomical): body region examined -B (Biological): biological system examined The code is strictly hierarchical because each sub-code element is connected to only one code element. The element to the right is a sub element of the element to the left. These characteristics of the IRMA code lead as to exploit the code hierarchy and construct an automatic annotation system based on predictive clustering trees framework for hierarchical multi-label classification <ref type="bibr" coords="2,314.83,180.75,10.65,8.74" target="#b1">[2]</ref>. This approach was directly applicable for the datasets of ImageCLEF 2007 and ImageCLEF 2008 because the images were labeled according to the IRMA code scheme. To apply the same algorithm for the ImageCLEF 2005 and ImageCLEF 2006 datasets we mapped the class numbers with the corresponding IRMA codes. The images from ImageCLEF 2005 dataset belong to more than one IRMA code. In the classification process we used the most general IRMA code to describe these images.</p><p>Automatic image classification relies on numerical features that are computed from the pixel values. In our approach we used an edge histogram descriptor as global feature and SIFT histogram as local feature. These feature vectors were combined through simple concatenation in one feature vector with 2080 variables. With the combination of global and local features we want to tackle the problem of intra-class variability vs. inter-class similarity and the problem of data unbalance between train and test datasets.</p><p>The rest of the paper is organized as follows: section 2 describes the feature extracted from images. Section 3 gives details on the predictive clustering trees framework and its use for hierarchical multi-label classification. In section 4 we explain the experimental setup. Section 5 reports the obtained results. Conclusions and summary are given in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Feature Extraction from Images</head><p>This section describes the features we used to describe the X-ray images from ImageCLEF 2009. We shortly describe the edge histogram descriptor and scale invariant feature transform. In the classification phase we used the feature vector obtained with simple concatenation of these features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Edge Histogram Descriptor</head><p>Edge detection is a fundamental problem of computer vision and has been widely investigated <ref type="bibr" coords="2,472.73,455.67,10.61,8.74" target="#b2">[3]</ref>. The goal of edge detection is to mark the points in a digital image at which the luminous intensity changes sharply. Edge representation of an image drastically reduces the amount of data to be processed, yet it retains important information about the shapes of objects in the scene. Edges in images constitute an important feature to represent their content. One way of representing such an important edge feature is to use a histogram. An edge histogram in the image space represents the frequency and the directionality of the brightness changes in the image. To represent this unique feature, in MPEG-7, there is a edge histogram descriptor (EHD) in the image. The EHD basically represents the distribution of five types of edges in each local area called a sub-image. As shown in Figure <ref type="figure" coords="2,99.75,547.65,3.76,8.74" target="#fig_0">1</ref>, the sub-image is defined by dividing the image space into 4×4 nonoverlapping blocks. Thus, the image partition always yields 16 equal-sized sub-images regardless of the size of the original image. To characterize the sub-image, we then generate a histogram of edge distribution for each sub-image. Edges in the sub-images are categorized into five types: vertical, horizontal, 45-degree diagonal, 135-degree diagonal and non-directional edges. Thus, the histogram for each sub-image represents the relative frequency of occurrence of five types of edges in the corresponding sub-image.</p><p>As a result, each local histogram contains five bins. Each bin corresponds to one of five edge types. Since there are 16 sub-images in the image, a total of 5×16=80 histogram bins is required. Note that each of the 80histogram bins has its own semantics in terms of location and edge type. For example, the bin for the horizontal type edge in the sub-image located at (0,0) in Figure <ref type="figure" coords="3,290.23,153.99,5.01,8.74" target="#fig_0">1</ref> carries the information of the relative population of the horizontal edges in the top-left local region of the image. The edge detection was performed using Canny edge detection algorithm <ref type="bibr" coords="3,151.48,177.03,10.63,8.74" target="#b3">[4]</ref>.</p><p>Because of the low contrast of the X-ray images we applied a contrast enhancement technique for the images used in our experiments. The contrast enhancement was done through histogram equalization for the central part of the images, because the image corners have only black pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SIFT histogram</head><p>Many different techniques for detecting and describing local image regions have been developed <ref type="bibr" coords="3,491.56,259.65,10.61,8.74" target="#b4">[5]</ref>. The Scale Invariant Feature Transform (SIFT) was proposed as a method of extracting and describing keypoints which are reasonably invariant to changes in illumination, image noise, rotation, scaling, and small changes in viewpoint <ref type="bibr" coords="3,113.46,294.15,10.61,8.74" target="#b4">[5]</ref>.</p><p>For content based image retrieval good response times are required and this is hard to achieve using the huge amount of data obtained by local features. The dimension of this feature is extremely high because the size of the keypoint descriptor is 128 dimensional vector. To reduce the dimensionality we use histograms of local features <ref type="bibr" coords="3,105.38,340.11,10.64,8.74" target="#b5">[6]</ref>. With this approach the amount of data is reduced by estimating the distribution of local features for every image.</p><p>The creation of these histograms is a three step procedure. First, the key-points are extracted from all database images, where a key-point is described with a 128 vector of numerical values. The key-points are then clustered in 2000 clusters. Afterwards, for each key-point we discard all information except the identifier of the most similar cluster center. A histogram of the occurring patch-cluster identifiers is created for each image. This results in a 2000 dimensional histogram per image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Ensembles for PCTs</head><p>In this section we discuss the approach we used to classify the data at hand. We shortly describe the learning of the ensembles and the predictive clustering trees (PCT) framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PCTs for Hierarchical-Multi Label Classification</head><p>In the PCT framework <ref type="bibr" coords="3,190.49,506.07,10.66,8.74" target="#b6">[7]</ref>, a tree is viewed as a hierarchy of clusters: the top-node corresponds to one cluster containing all data, which is recursively partitioned into smaller clusters while moving down the tree. PCTs can be constructed with a standard "top-down induction of decision trees" (TDIDT) algorithm. The heuristic for selecting the tests is the reduction in variance caused by partitioning the instances. Maximizing the variance reduction maximizes cluster homogeneity and improves predictive performance. With instantiation of the variance and prototype function the PCTs can handle different types of data, e.g. multiple targets <ref type="bibr" coords="3,481.08,563.55,11.67,8.74" target="#b7">[8]</ref> or time series <ref type="bibr" coords="3,96.13,575.07,10.62,8.74" target="#b8">[9]</ref>. A detailed description of the PCT framework can be found in <ref type="bibr" coords="3,360.74,575.07,10.57,8.74" target="#b6">[7]</ref>.</p><p>To apply PCTs to the task of HMLC first the example labels are represented as vectors with Boolean components. The i-th component of the vector is 1 if the example belongs to class c i and 0 otherwise (See Figure <ref type="figure" coords="3,70.92,609.57,3.62,8.74">2</ref>). Then the variance of a set of examples (S) is defined as the average squared distance between each example's label v i and the mean label v of the set, i.e.,</p><formula xml:id="formula_0" coords="3,257.34,632.93,86.40,40.57">S v v d S Var i i ∑ = 2 ) , ( ) (</formula><p>The higher levels of the hierarchy are more important: an error in the upper levels costs more than an error on the lower levels. Considering that, weighted Euclidean distance is used as a distance measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∑ -=</head><formula xml:id="formula_1" coords="3,231.72,731.47,129.71,22.38">i i i i v v c w v v d 2 , 2 , 1<label>2 1</label></formula><p>) )( ( ) , ( where v k,i is the i'th component of the class vector v k of an instance x k , and the class weights w(c) decrease with the depth of the class in the hierarchy. Second, in the case of HMLC, the notion of majority class does not apply in a straightforward manner. Each leaf in the tree stores the mean v of the vectors of the examples that are sorted in that leaf. Each component of v is the proportion of examples i v in the leaf that belong to class c i . An example arriving in the leaf can be predicted to belong to class c i if i v is above some threshold t i . The threshold can be chosen by a domain expert. A detailed description of the PCTs for HMLC can be found in <ref type="bibr" coords="4,434.25,142.29,15.33,8.74" target="#b9">[10]</ref>.</p><p>Figure <ref type="figure" coords="4,99.53,315.51,3.90,8.74">2</ref>: A toy hierarchy. Class label names reflect the position in the hierarchy, e.g., '2.1' is a subclass of '2'. The set of classes {1, 2, 2.2}, indicated in bold in the hierarchy, and represented as a vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ensemble Methods</head><p>An ensemble is a set of classifiers constructed with a given algorithm. Each new example is classified by combining the predictions of every classifier from the ensemble. These predictions can be combined by taking the average (for regression tasks) or the majority vote (for classification tasks) <ref type="bibr" coords="4,402.99,392.13,15.29,8.74" target="#b10">[11]</ref>, <ref type="bibr" coords="4,425.95,392.13,15.33,8.74" target="#b11">[12]</ref>, or by taking more complex combinations. We consider two ensemble learning techniques that have primarily been used in the context of decision trees: bagging and random forests.</p><p>Bagging <ref type="bibr" coords="4,128.79,426.63,16.64,8.74" target="#b10">[11]</ref> constructs the different classifiers by making bootstrap replicates of the training set and using each of these replicates to construct one classifier. Each bootstrap sample is obtained by randomly sampling training instances, with replacement, from the original training set, until an equal number of instances is obtained.</p><p>A random forest <ref type="bibr" coords="4,163.31,472.65,16.68,8.74" target="#b11">[12]</ref> is an ensemble of trees, where diversity among the predictors is obtained by using bagging, and by changing the feature set during learning. More precisely, at each node in the decision trees, a random subset of the input attributes is taken, and the best feature is selected from this subset. The number of attributes that are retained is given by a function f of the total number of input attributes x (e.g.,</p><formula xml:id="formula_2" coords="4,79.74,518.43,150.69,15.31">⎣ ⎦ 1 log ) ( , ) (<label>, 1 ) ( 2</label></formula><formula xml:id="formula_3" coords="4,74.58,519.02,256.91,12.46">+ = = = x x f x x f x f ,…). By setting x x f = ) (</formula><p>, we obtain the bagging procedure. The PCTs for HMLC are used as base classifiers. Average is applied to combine the different predictions because the leaf's prototype is the proportion of examples that belong to it. This means that a threshold should be specified to make a prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Design</head><p>We decided to split the training images into training and development images because the organizers didn't supply development set. To tune the system for different distribution of the images in the classes of the training set and the test set we generated several splits with different distribution of the images in the classes of the training and development data.</p><p>We decided to learn a classifier for each axis separately (see Section 1). From each of the datasets we learn a PCT for HMLC and Ensembles of PCTs (Bagging and Random Forests). The ensembles consisted of 100 unpruned trees. The feature subset size for Random Forests was set to 11 (using the formula</p><formula xml:id="formula_4" coords="4,106.32,686.51,95.16,15.07">⎣ ⎦ ) 2080 ( log ) 2080 ( 2 = f</formula><p>). To compare the performance of a single tree and an ensemble we use Precision-Recall (PR) curves. These curves are obtained with varying the value for the threshold: a given threshold corresponds to a single point from the PR-curve. For more information, see <ref type="bibr" coords="4,424.97,716.13,15.33,8.74" target="#b9">[10]</ref>. According to these experiments and previous research the ensembles of PCTs showed increased performance as compared to a single PCT when applied for hierarchical annotation of medical images <ref type="bibr" coords="4,375.60,739.17,15.33,8.74" target="#b12">[13]</ref>. Furthermore, the Bagging and Random Forest methods give similar results and because the Random Forest method is much faster than the Bagging method we submitted only the results for the Random Forest method.</p><p>To select an optimal value of the threshold (t), we performed validation on the different development sets. The threshold values that give the best results were used for the prediction of the unlabelled radiographs according to the four different classification schemes (see Section 1).</p><p>To reduce the intra-class variability for axis D and improve the prediction performance we decided to modify the hierarchy for this axis and include the first code of axis A from the corresponding IRMA code. Figure <ref type="figure" coords="5,100.39,153.99,5.01,8.74" target="#fig_1">3</ref> presents example images that have same code for axis D but are visually different. After inclusion of the first code from the axis A these images belong to different classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>For ImageCLEF 2009 medical annotation task we submitted one run. In this task our result was third among the participating groups, with total error score of 1352.56. The results for the particular datasets are shown in Table <ref type="table" coords="5,135.12,483.99,3.77,8.74" target="#tab_1">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper presented a hierarchical multi-label classification approach for medical image annotation. For efficient image representation we used edge histogram descriptor and SIFT histogram. The predictive modeling problem that we consider is to learn PCTs and ensembles of PCTs that predict a hierarchical annotation of an Xray image.</p><p>The proposed approach can be easily extended with new feature extraction methods, and can thus be applied to other domains. The proposed approach for hierarchical annotation can be easily applied to arbitrary domains because it can handle hierarchies with arbitrary sizes (bigger hierarchies, hierarchies that are organized as trees or directed acyclic graphs).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,172.98,723.93,206.91,8.74;2,167.76,574.02,217.56,141.54"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Definition of sub-image and image-block.</figDesc><graphic coords="2,167.76,574.02,217.56,141.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,188.82,411.69,217.79,8.74;5,125.88,185.88,343.68,217.20"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example images with same value for axis D.</figDesc><graphic coords="5,125.88,185.88,343.68,217.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,163.26,501.51,249.73,104.74"><head>Table 1 :</head><label>1</label><figDesc>Error score for the medical image annotation run</figDesc><table coords="5,163.26,525.51,210.22,80.74"><row><cell>classification label sets</cell><cell>Error score</cell></row><row><cell>2005</cell><cell>549</cell></row><row><cell>2006</cell><cell>433</cell></row><row><cell>2007</cell><cell>128.1</cell></row><row><cell>2008</cell><cell>242.26</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="6,92.22,99.33,432.20,8.74;6,92.22,112.53,432.17,8.74;6,92.22,125.73,359.75,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="6,412.14,99.33,112.28,8.74;6,92.22,112.53,135.37,8.74">The IRMA code for unique classification of medical images</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">M</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kohnen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><forename type="middle">B</forename><surname>Wein</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Berthold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,237.41,112.53,71.44,8.74;6,342.40,112.53,181.99,8.74;6,92.22,125.73,128.01,8.74">PACS and Integrated Medical Information Systems: Design and Evaluation</title>
		<imprint>
			<biblScope unit="volume">5033</biblScope>
			<biblScope unit="page" from="440" to="451" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct coords="6,92.22,138.99,432.24,8.74;6,92.22,152.19,268.36,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="6,354.39,138.99,170.07,8.74;6,92.22,152.19,51.69,8.74">Decision trees for hierarchical multi-label classification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Vens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Struyf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Schietgat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dzeroski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Blockeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,150.07,152.19,105.62,8.74">Machine Learning Journal</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="185" to="214" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.22,165.45,432.30,8.74;6,92.22,178.65,233.36,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="6,191.64,165.45,182.38,8.74">Edge Detection Techniques An Overview</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ziou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tabbone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,385.84,165.45,138.68,8.74;6,92.22,178.65,129.10,8.74">International Journal of Pattern Recognition and Image Analysis</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="537" to="559" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.22,191.85,432.23,8.74;6,92.22,205.11,154.79,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="6,145.35,191.85,188.07,8.74">A computational approach to edge detection</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">F</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,342.32,191.85,182.12,8.74;6,92.22,205.11,45.92,8.74">IEEE Trans Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986-11">Nov 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.22,218.31,432.29,8.74;6,92.22,231.57,166.40,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="6,144.79,218.31,235.83,8.74">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,387.60,218.31,136.92,8.74;6,92.22,231.57,25.03,8.74">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.22,244.77,432.34,8.74;6,92.22,257.97,220.84,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,235.80,244.77,270.25,8.74">Discriminative training for object recognition using image patches</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,92.22,257.97,36.95,8.74">CVPR 05</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="157" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.23,275.25,397.84,8.74;6,490.08,269.05,5.04,5.65;6,496.92,275.25,27.46,8.74;6,92.22,288.45,58.36,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,257.50,275.25,155.84,8.74">Top-down induction of clustering trees</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Blockeel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>De Raedt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ramon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,430.59,275.25,59.48,8.74;6,490.08,269.05,5.04,5.65;6,496.92,275.25,21.97,8.74">Proc. of the 15 th ICML</title>
		<meeting>of the 15 th ICML</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.22,301.65,432.17,8.74;6,92.22,314.91,195.28,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,268.96,301.65,186.93,8.74">Ensembles of Multi-Objective Decision Trees</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kocev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Vens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Struyf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dzeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,475.67,301.65,48.71,8.74;6,92.22,314.91,48.28,8.74">Proc. of the ECML 2007</title>
		<meeting>of the ECML 2007</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4701</biblScope>
			<biblScope unit="page" from="624" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.22,328.11,432.14,8.74;6,92.22,341.31,209.64,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="6,295.61,328.11,228.75,8.74;6,92.22,341.31,20.56,8.74">Analysis of Time Series Data with Predictive Clustering Trees</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dzeroski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Gjorgjioski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Slavkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Struyf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,130.20,341.31,65.57,8.74">KDID</title>
		<imprint>
			<biblScope unit="volume">06</biblScope>
			<biblScope unit="page" from="63" to="80" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct coords="6,92.22,354.57,432.29,8.74;6,92.22,367.77,330.94,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="6,348.50,354.57,176.02,8.74;6,92.22,367.77,51.50,8.74">Decision trees for hierarchical multi-label classification</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Vens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Struyf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Schietgat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dzeroski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Blockeel</surname></persName>
		</author>
		<idno type="DOI">-10.1007/s10994-008-5077-3</idno>
	</analytic>
	<monogr>
		<title level="j" coord="6,149.88,367.77,104.00,8.74">Machine Learning Journal</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.22,381.03,374.15,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="6,142.76,381.03,75.02,8.74">Bagging predictors</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,224.15,381.03,104.07,8.74">Machine Learning Journal</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.23,394.23,317.52,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="6,142.84,394.23,63.79,8.74">Random Forests</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="6,213.04,394.23,104.15,8.74">Machine Learning Journal</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,92.23,407.43,432.21,8.74;6,92.23,418.95,432.24,8.74;6,92.23,430.47,251.31,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="6,337.27,407.43,182.76,8.74">Hierarchical annotation of medical images</title>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Dimitrovski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kocev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Loskovska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Dzeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,92.23,418.95,432.24,8.74;6,92.23,430.47,68.05,8.74">Proceedings of the 11th International Multiconference, Information Society -IS 2008, Data Mining and Data Warehouses</title>
		<meeting>the 11th International Multiconference, Information Society -IS 2008, Data Mining and Data Warehouses<address><addrLine>Ljubljana, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10">Oct. 2008</date>
			<biblScope unit="page" from="170" to="174" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
