<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,109.80,97.77,383.78,17.22;1,121.92,119.73,359.20,17.22;1,253.44,141.57,96.37,17.22">UPMC/LIP6 at ImageCLEFannotation 2009: Large Scale Visual Concept Detection and Annotation</title>
				<funder ref="#_8HKtAJD">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,148.08,175.93,75.93,9.96"><forename type="first">Ali</forename><surname>Fakeri-Tabrizi</surname></persName>
						</author>
						<author>
							<persName coords="1,231.25,175.93,64.14,9.96"><forename type="first">Sabrina</forename><surname>Tollari</surname></persName>
						</author>
						<author>
							<persName coords="1,302.44,175.93,71.52,9.96"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
						</author>
						<author>
							<persName coords="1,382.02,175.93,73.05,9.96"><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Université Pierre et Marie Curie -Paris</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Laboratoire d&apos;Informatique de Paris</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">UMR CNRS 7606</orgName>
								<address>
									<addrLine>104 avenue du président Kennedy</addrLine>
									<postCode>75016</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,109.80,97.77,383.78,17.22;1,121.92,119.73,359.20,17.22;1,253.44,141.57,96.37,17.22">UPMC/LIP6 at ImageCLEFannotation 2009: Large Scale Visual Concept Detection and Annotation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A28B1878FACBBBC6352A90CB934AACBF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries Measurement, Performance, Experimentation SVM, Graph models, Cooccurrences Analysis, Multi-Class Multi-Label Image Classification, Visual Concepts</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present the LIP6 annotation models for the ImageCLEF2009annotation task. We focus on two challenges: 1-the large scale annotation and 2-the application of an ontology. We propose two models of classifiers for the first challenge. In the case of second challenge, we try to detect the relations between the concepts via a cooccurrence matrix. By this way, we avoid the exclusive concepts tagged to one image.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In ImageCLEFannotation 2009 <ref type="bibr" coords="1,228.26,549.97,9.91,9.96" target="#b1">[2]</ref>, there are two main challenges: the large scale annotation and the application of an ontology (hierarchies and relations) for improving the image annotation.In the first challenge, we encounter the imbalanced data set. We propose for the second challenge to reduce the score of the concept, with the lowest score, when two concepts are exclusive.</p><p>The application of our methods is compared by two classifications techniques, SVM and graph classification. SVM is used as a common classifier to give us a baseline for comparison with the Graph classification technic. As an other classifier, with the given image data set, we create a graph. The labels of different classes are propagated through this graph and at the end, we have the classification of different parts of graph.</p><p>In the section 2, we propose our models for image annotation adapted for imbalanced data set problem. In the section 3, we show a method for considering the effects of the existing hierarchy between the concepts, on the results of classifiers. The experiences are illustrated in section 4. The conclusion and perspectives will be presented in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Annotation models 2.1 Imbalanced class problem</head><p>Machine learning algorithms usually assume that the training set is well-balanced. Therefore they suppose that all misclassification errors have an equal cost. But data in real world is often imbalanced. It means that some classes have much more examples than the others. We can call the classes which have more examples, the majority classes and the ones which have fewer examples, the minority classes. Using overall accuracy in this case is not an appropriate evaluation measurement because of the dominating effect of the majority classes.</p><p>In the given data set, we have 53 classes. For each class, we have obviously different numbers of positive examples. For some classes, we have such few numbers of positives (negatives) that they will be dominated by the majority classes. It means we have not a balance between the majority and minority classes to learn a model. Therefore we have an imbalanced class problem which should be solved to have the good and reasonable classifications.</p><p>To solve this problem two strategies are performed: We define the cost of ignorance in the minority class higher than in the majority class. Then the classifier will be instigated to find a better model. So we have a loss function in which the misclassification's cost depends on the number of class members.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SVM</head><p>Given an implicit embedding φ and training data (x i , y i ) from 2 classes such that y i ∈ {-1, 1}, a Support Vector Machine finds a hyperplane w T φ(x) + b = 0 that separates the two classes as well as possible. The learnt hyperplane is optimal in the sense that it maximises the margin while minimizing some measures of loss on the training data. More formally, the primal formulation of SVM is</p><formula xml:id="formula_0" coords="2,252.24,469.33,98.08,23.64">M in w,ξ 1 2 w t w + C1 t ξ</formula><p>subject to y i (wφ(x i ) + b) ≥ 1ξ i and ξ ≥ 0 where φ(x i )φ(x j ) = K(x i , x j ) with C being a user specified misclassification penalty. <ref type="foot" coords="2,237.72,509.37,3.97,6.97" target="#foot_0">1</ref>We have run a SVM classifier<ref type="foot" coords="2,230.64,521.37,3.97,6.97" target="#foot_1">2</ref> , which uses a linear kernel, on the training base to learn a model for the given corpus. Therefore, a basic classifier will be obtained by this model. As explained above, we have the imbalanced class problem. Here we use a ROC area as the loss function as proposed in <ref type="bibr" coords="2,144.50,558.61,9.91,9.96" target="#b0">[1]</ref>. So we consider not only the misclassification in each learning iteration, but also the number of positive and negative examples in order to avoid the fault ignorance. ROCarea can be computed from the number of swapped pairs SwappedP aires = { (i, j) : (y i &gt; y j )and(w T x i &lt; w T x j )} i.e. the number of pairs of examples that are in the wrong order.</p><formula xml:id="formula_1" coords="2,228.60,640.54,144.72,23.07">ROCarea = 1 - SwappedP aires #pos.#neg</formula><p>Here 1-ROCarea is used as the value of misclassification in loss function for each iteration. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graph Classification</head><p>A graph is created in which the nodes represent the images. Each node is whether annotated by many concepts (images in training base) or is not annotated yet (images in test base). The idea is to classify the nodes which represent the images. The similarity value between each pair of nodes is calculated. This value will be the weight of the edge between that pair of nodes. In this study the similarity value is based on the Euclidean distance:</p><formula xml:id="formula_2" coords="3,251.28,371.25,99.95,11.80">Sim(X, Y ) = e -D(X,Y )</formula><p>in which D represents the euclidean distance. We propagate the labels through the graph. <ref type="bibr" coords="3,297.36,402.97,12.77,9.96" target="#b3">[4]</ref> The propagation process is done through the edges weighted by similarity values. This is the reason why we can have the same labels for the similar images; i.e the propagation through an edge with a high value will be stronger than one with a low value.</p><p>We formulate this process as below:</p><formula xml:id="formula_3" coords="3,105.00,461.37,367.96,45.17">G = xi∈X ∆(f θ (x i ), y i ) + α E g w (i, j)(f (x i ) -f (x j )) 2 + β E ∆ ′ (g w (i, j), y(i, j)) in which • xi∈X ∆(f θ (x i ), y i )</formula><p>is the error of wrong labeling; 2 is the factor for penalizing edges between the nodes with different labels. g w is the predicted value for the weight of the edge.</p><formula xml:id="formula_4" coords="3,105.00,515.77,136.60,11.90">• α E g w (i, j)(f (x i ) -f (x j ))</formula><p>• β E ∆ ′ (g w (i, j), y(i, j)) penalize the classification errors on the edges weighted by g w in compare with the existing edges in reality.</p><p>The goal is to minimize the G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hierarchy relations between concepts</head><p>In this study, a tag collection consists of 53 classes. For each image, a subset of the collection will be assigned. For each image, we perform a hierarchical filter on its labels' prediction. The ontological relations between the concepts are considered by this filter. The idea of using this filter is to minimize the probable conflicts. With the hypothesis of the independency and negliging the logical relationships between the classes, we can annotate each image by the results obtained from the classifiers. We suppose that a classifier predicts two classes X and Y as the tags for an image. If the logical relationship between X and Y is negligible, we will have the same possibility for both classes to be tagged to the image. As we do not suppose any relation between X and Y, it seems to be reasonable. But if we take into account the relations between the classes, we may improve the results. As an example, if we consider X='DAY' and Y='NIGHT', then the hypothesis of appearing the both classes in same time will be contradictory. In the contradictory cases such as the example we mentioned above, the ontological or logical relations between classes will help us to improve the results by correction the misclassification. This improvement will be done by adding more information which did not take into account during the learning process. We used the same method which proposed in <ref type="bibr" coords="4,427.69,387.73,10.00,9.96" target="#b2">[3]</ref>. In figure 2, we illustrate an example in which we suppose to have 5 classes that are predicted for an image. The prediction is done by the score values given by the classifier: for each class, we have a real value Sc i between -1 and 1 as the output. Normally, a sign function will be performed on Sc i to detect the class of the image. The figure <ref type="figure" coords="4,246.12,435.49,4.98,9.96" target="#fig_1">2</ref> illustrates the class 'Animals' with a negative value which means that the image will not be assigned by this tag. The 4 other tags have the positive values. These positives values are not sufficient to annotate their corresponding tags to the image. As we see 'Day' and 'Night' are not compatible in the same time. To find these contradictions, we create a co-occurrence matrix which based on the concepts. This matrix will consider the degree of relation's strength between each concept:</p><formula xml:id="formula_5" coords="4,242.64,517.21,110.41,10.33">COOC(X, Y ) = I X ∩ I Y</formula><p>in which I Z represents the set of images which are tagged by Z.</p><p>By considering the less values (not so much related) in this matrix, we can interpret the classes which are contradictories, i.e. they can not be presented at the same time as an image annotation. In figure <ref type="figure" coords="4,130.93,575.05,3.90,9.96" target="#fig_1">2</ref>, we find out that the weak value for COOC means a problem. It means that we can automatically find the contradictions. Now the question will be posed is: Which one? 'Day' or 'Night'. It is simple. We annotate the image by the class with stronger value and the other class will be eliminated. In this case, we save 'Day' with score value 0.34 and we remove 'night' with 0.11.</p><p>In addition, we define an optimism rate (OR), which signifies an acceptation degree of negative values. By this method, we expect to trap the misclassified examples. If we suppose OR=0, it will be the normal case. This means, the classifier is surely good enough and we don't expect to retrieve the hidden misclassifications. If OR=-0.1, all of the scores with a value more than -0.1 will be associated to positive class. By this method we decrease the risks of misclassifications and of having the contradictory classes. The algorithm 1 indicates this approach: We used this logic to estimate the misclassifications. Note that in this approach, we try to modify the output of our learning model and not the proper learning model. Table <ref type="table" coords="5,461.54,464.77,4.98,9.96" target="#tab_1">1</ref> illustrates how this method works. Here we have 6 different candidate topics to be tagged to a given image.</p><p>We have the cooccurrence non-zero value between all pairs of the concepts except for the pair (Day,Night). When an exclusivity is detected, the less score will be subtracted by 2 times of value of OR. We continue until having the score greater than OR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We have a training base including 5000 images of Flickr and a test base of 13000 images of Flickr. Each image is tagged by one or more of the 53 given concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visual features</head><p>As we need the features to learn, we use the color-based visual descriptors in this paper. Therefore we segment the images into 3 horizontal regions with the same sizes. For each region, we compute a color histogram in the HSV space. We believe that these visual descriptors are particularly interesting for general concepts (i.e. not objects), as for instance: sky, sunny, vegetation, sea... (see figure <ref type="figure" coords="5,137.64,673.45,3.88,9.96" target="#fig_2">3</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Submitted Runs</head><p>We have performed the SVM classification (run1) and applied 3 different OR values. OR=0 (run2), OR=-0.1 (run3) and OR=-0.2 (run5). We consider also a classification without hierarchy (run1) to study the effects of hierarchy and its probable improvements. As a new approach, we had run a graph classifier and applied the hierarchy on it (run4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run1-LIP6RUN1</head><p>We used exclusively visual information here. Each image was cut in 3 horizontal regions with the same size and for each part, the visual features were extracted by HSV histogram. As a result, for each image, we had a vector in 51 dimensions (3*17).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run2-LIP6RUN2</head><p>We used the same visual information as the run1 but the prediction obtained in this level is modified by the hierarchy method. We consider OR=1.0 which means we expect to find the misclassification till the scores -0.1. Thereby we try to apply the hierarchy to remove the contradictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run3-LIP6RUN3</head><p>The same thing as Run2 but with value -0.15 for OR.</p><p>Run4-LIP6RUN4 Here we have used the PCA of the images as the visual information. We run the graph classification method on these data and the obtained prediction is modified by the hierarchy method with the value -0.1 for OR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run5-LIP6RUN5</head><p>The same thing as Run2 but with value -0.2 for OR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and discussion</head><p>Table <ref type="table" coords="6,117.24,567.85,4.98,9.96" target="#tab_2">2</ref> shows the official results. We can notice that the best results are obtained with the SVM classifieur without using the hierarchical relations between concepts. Our run based on graph classification obtains lower scores than the random run, this means that we should have make a mistake building this run. The more OR is low, the more the EER and the AUC are low, but we can notice that the average annotation scores do not change varying OR values. The average annotation scores for the last 4 runs are very lower than the scores of the random run, this should mean that we may inverse the applications of the OR filter. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this working note, we performed two classifiers to annotate the unlabeled images: SVM and Graph. We had an imbalanced data set and we tried to solve this problem. Graph classification did not give us the good results. We will study this method more in our future works. We tried to use the ontological relations between the concepts to improve the results of the classifiers. The experiences show that the application of ontology did not give better results. We might modify the hierarchy method in order to have more improvement.</p><p>As perspectives, we will change the first method of balancing the data. We will try to add the examples of minority class in the data-set still having the same number of minorities and majorities. Therefore, we expect to decrease the information lost. We will more study the effects of OR and we will try the method of hierarchy without OR to find out if it is efficient in reality or has better effects. In hierarchy method, we may be able to detect the other types of relations; for example, we expect to find the generalization relations between the concepts and by that we can modify the results of classifiers.In graph classification, we will perform a multi-classes model to find out if it will be better than 53 binary models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,90.00,214.81,422.97,9.96;3,90.00,226.69,423.19,9.96;3,90.00,238.69,423.11,9.96;3,90.00,250.69,344.20,9.96;3,109.56,58.86,384.00,131.50"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Semi supervised graph labelling task. The goal is to label the unlabelled nodes of a partially labeled graph. (a): Images graph: Each node represents an image, each edge represents a weighted similarity between images (b): Labeled images (colored nodes) and unlabeled images (non-colored nodes) in the same graph (c): The output is a fully labeled graph</figDesc><graphic coords="3,109.56,58.86,384.00,131.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,90.00,238.33,423.20,9.96;4,90.00,250.21,423.00,9.96;4,90.00,262.21,39.60,9.96;4,133.20,58.88,336.50,155.00"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example of hierarchy effects on results obtained by the classifier; COOC represents the value of cooccurrence between two classes. Sc is the real value of the class predicted by the classifier.</figDesc><graphic coords="4,133.20,58.88,336.50,155.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,90.00,221.77,423.17,9.96;6,90.09,60.23,448.50,137.00"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Dividing image on three horizontal segment to extract the histogram (HSV) of each part</figDesc><graphic coords="6,90.09,60.23,448.50,137.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,102.12,252.97,411.01,76.68"><head></head><label></label><figDesc>1. Removing the examples from the majority set: in this method, we select the same number of examples from both the minority classes and the majority classes. The examples which are selected from the majority classes are chosen randomly. In this way, we will have all the examples of the minority class and in fact we remove many examples of the majority class. Therefore the number of positive and negative examples will be balanced for each class.</figDesc><table coords="2,102.12,319.69,234.99,9.96"><row><cell>2. Choose a convenient loss function for the classifier:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,90.00,61.09,423.21,371.76"><head>Table 1 :</head><label>1</label><figDesc>Algorithm 1 Hierarchy 1: For each image I in test set, Get labels score set S which is predicted by classifiers: S = {Sc i } i=1..53 in which Sc i ∈ [-1, 1] is the score predicted by the classifier for label Sc i . 2: Sort S descending 3: k ← 1 4: WHILE Sc k ≥ OR 5: ∀t &gt; k , IF COOC(c k , c t ) = 0 THEN An example of hierarchy effects on results obtained by the classifier; COOC represents the value of co-occurrence between two classes. Sc is the real value of the class predicted by the classifier.</figDesc><table coords="5,106.92,146.05,356.49,241.08"><row><cell cols="2">Sc t ← Sc t + 2  *  OR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">resort Sc descending</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>k ← k + 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ENDIF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ENDWHILE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Step1</cell><cell>Spring</cell><cell cols="2">Night Animals</cell><cell>Trees</cell><cell>Mountains</cell><cell>Day</cell></row><row><cell></cell><cell>-0.08</cell><cell>0.01</cell><cell>-0.21</cell><cell>0.19</cell><cell>0.41</cell><cell>0.34</cell></row><row><cell>Step2</cell><cell>Mountains</cell><cell>Day</cell><cell>Trees</cell><cell>Night</cell><cell>Spring</cell><cell>Animals</cell></row><row><cell></cell><cell>0.41</cell><cell>0.34</cell><cell>0.19</cell><cell>0.01</cell><cell>-0.08</cell><cell>-0.21</cell></row><row><cell cols="2">Step3 Mountains</cell><cell>Day</cell><cell>Trees</cell><cell>Night</cell><cell>Spring</cell><cell>Animals</cell></row><row><cell></cell><cell>0.41</cell><cell>0.34</cell><cell>0.19</cell><cell>0.01</cell><cell>-0.08</cell><cell>-0.21</cell></row><row><cell cols="3">Step4 Mountains Day</cell><cell>Trees</cell><cell>Spring</cell><cell>Night</cell><cell>Animals</cell></row><row><cell></cell><cell>0.41</cell><cell>0.34</cell><cell>0.19</cell><cell>-0.08</cell><cell>-0.19</cell><cell>-0.21</cell></row><row><cell cols="3">Step5 Mountains Day</cell><cell>Trees</cell><cell>Spring</cell><cell>Night</cell><cell>Animals</cell></row><row><cell></cell><cell>0.41</cell><cell>0.34</cell><cell>0.19</cell><cell>-0.08</cell><cell>-0.19</cell><cell>-0.21</cell></row><row><cell cols="3">Step6 Mountains Day</cell><cell>Trees</cell><cell>Spring</cell><cell>Night</cell><cell>Animals</cell></row><row><cell></cell><cell>0.41</cell><cell>0.34</cell><cell>0.19</cell><cell>-0.08</cell><cell>-0.19</cell><cell>-0.21</cell></row><row><cell cols="3">Step7 Mountains Day</cell><cell>Trees</cell><cell>Spring</cell><cell>Night</cell><cell>Animals</cell></row><row><cell></cell><cell>0.41</cell><cell>0.34</cell><cell>0.19</cell><cell>-0.08</cell><cell>-0.19</cell><cell>-0.21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,124.20,60.13,354.72,128.28"><head>Table 2 :</head><label>2</label><figDesc>Official results. All runs are fully automatic.</figDesc><table coords="7,295.80,60.13,183.12,21.96"><row><cell></cell><cell cols="2">Average Annotation Score</cell></row><row><cell>EER AUC</cell><cell>with</cell><cell>without</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,687.39,334.78,7.35"><p>http://research.microsoft.com/en-us/um/people/manik/projects/trade-off/svm.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,105.24,696.87,177.07,7.35"><p>http://svmlight.joachims.org/svm_perf.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work was partially supported by the <rs type="funder">French National Agency of Research</rs> (<rs type="grantNumber">ANR-06-MDCA-002 AVEIR</rs> project).</p><p>We thank <rs type="person">Nicolas Usunier</rs> for his help with loss fucntion definition in SVM classification.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8HKtAJD">
					<idno type="grant-number">ANR-06-MDCA-002 AVEIR</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,105.48,520.93,407.53,9.96;7,105.48,532.93,353.17,9.96" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="7,200.88,520.93,289.77,9.96">A support vector method for multivariate performance measures</title>
		<author>
			<persName coords=""><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,105.48,532.93,321.49,9.96">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.48,552.85,407.42,9.96;7,105.48,564.85,112.93,9.96" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="7,264.39,552.85,199.23,9.96">Visual concept detection and annotation task</title>
		<author>
			<persName coords=""><forename type="first">Stefanie</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Dunker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,486.36,552.85,26.54,9.96;7,105.48,564.85,59.99,9.96">CLEF working notes</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.48,584.77,407.71,9.96;7,105.48,596.65,407.65,9.96;7,105.48,608.65,407.56,9.96;7,105.48,620.65,254.41,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,236.33,596.65,276.80,9.96;7,105.48,608.65,34.79,9.96">Using visual concepts and fast visual diversity to improve image retrieval</title>
		<author>
			<persName coords=""><forename type="first">Sabrina</forename><surname>Tollari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marcin</forename><surname>Detyniecki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Fakeri-Tabrizi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christophe</forename><surname>Marsala</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Massih-Reza</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,164.88,608.65,348.16,9.96;7,105.48,620.65,223.03,9.96">Evaluating Systems for Multilingual and Multimodal Information Access -9th Workshop of the Cross-Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.48,640.57,407.56,9.96;7,105.48,652.45,384.25,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,265.61,640.57,247.43,9.96;7,105.48,652.45,23.22,9.96">Learning from labeled and unlabeled data on a directed graph</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Schlkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,150.12,652.45,308.81,9.96">Proceedings of the 22nd International Conference on Machine Learning</title>
		<meeting>the 22nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
