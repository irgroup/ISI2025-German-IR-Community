<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,143.82,74.39,307.67,12.58">I2R AT IMAGECLEF PHOTO RETRIEVAL 2009</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,232.68,103.08,49.16,9.02"><forename type="first">Sheng</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName coords="1,301.31,103.08,61.36,9.02"><forename type="first">Joo-Hwee</forename><surname>Lim</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Institute for Infocomm Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">STAR</orgName>
								<address>
									<addrLine>Singapore ; 1 Fusionopolis Way</addrLine>
									<postCode>138632</postCode>
									<settlement>Singapore</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,143.82,74.39,307.67,12.58">I2R AT IMAGECLEF PHOTO RETRIEVAL 2009</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">29BE0E5127BA8138FCF65A4AD6EEDA03</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Languages-Query Languages Measurement, Performance, Experimentation Language model, information retrieval, re-ranking, name entity extraction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the paper, we introduce our systems and methods to promote the diversity of the ad-hoc photog retrieval in ImageCLEF 2009. The image database in this year is quite different from the previous years, not only increasing the corpus size from 20,000 images to half millionm but also changing the domain from the travel to news. Most of queries are related to person names and the text information in image documents is rich. Thus we put a lot of effort on text while using the visual information to elimiate or down-rank similar images in order to make top images visually dissimilar. To reduce the ambiguarity of query and infer the implicit dimension of diversity space, name entity extraction is applied on the top documents in order to extract informative phrase patterns to faciliate query expansion. Name entity based query expansion makes our system placed in the top performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the past years, the task of the photographic image retrieval works on the images in the travel domain and the database only has 20,000 images. The image document includes two modalities i.e. visual image and text description. Thus the information needs of the user query are formulated as keywords or image exemplars. In this year, the domain of the benchmark image database is changed to the news. The image content (visual and text description) in the news domain is quite different from the previous. So do the information needs, i.e. query types. In news domain, most of the queries and text descriptions of images are related with the personal names and events. Here are two query examples, one having the description about the meaning of diversity in the field of &lt;clusterTitle&gt; besides the key information need in the field of &lt;title&gt; while another only has key information frequently having one keyword.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 2 &lt;top&gt; &lt;num&gt; 26 &lt;/num&gt;</head><p>The above two queries are all about the personal names. The key information need in the query only has one keyword, i.e. leterme and obama. Obviously they have great ambiguity because there are many personal names containing the leterme or obama. The diversity search prefers the system to return as much as both relevant and diverse images in the top document to reflect the ambiguity of the query. This is the promotion of photo retrieval task this year and last year.</p><p>In the two-modality based image retrieval, the diversity is characterized by the image visual content or the text descriptions attached to the image. Thus we can improve the diversity performance by grouping the visually-similar images and only selecting one or a few representative images in a group to represent the whole cluster. The visual similarity can be measured by matching one against another to see whether they are significantly different <ref type="bibr" coords="2,490.78,223.20,10.61,9.02" target="#b1">[2]</ref>. We can also find cues of similarity among image documents from analyzing text descriptions of image documents. The text cues may come from the bag-of-words, name entities, etc.</p><p>From our past experiences of participating photo retrieval <ref type="bibr" coords="2,300.96,263.70,10.85,9.02" target="#b2">[3,</ref><ref type="bibr" coords="2,315.64,263.70,7.23,9.02" target="#b3">4]</ref>, the best system efficiently combine the text-based ranking system with the visual-based system. Thus our system architecture in this year also uses the two modalities. Simple and computation efficient visual features such as global colour moments and histogram, which work well in the previous data, do not operate well in this new dataset, because the content of images in this year are objects such as person. It requires us to extract other effective visual features, e.g. histogram of orientation which achieves higher performance in person detection <ref type="bibr" coords="2,190.60,321.18,10.61,9.02" target="#b6">[7]</ref>. However, computation cost of visual feature extraction and selection is very high for the large-scale database in comparison with the text feature. Thus, we pay much attention on text feature in this year and reuse the tools developed in the past for visual feature extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">System description</head><p>As introduced in the above, the text-based ranking system and the content-based image ranking system are individually built based on the text descriptions and visual descriptors respectively. To down-rank the visually similar images, we also apply the visual descriptors to re-rank the output from the text-based system. Before we discuss the details of submitted runs, we first discuss the index structure for large-scale retrieval system. This year the database has about half million image documents while it only has ~20,000 images in the previous corpus. It is crucial to build an index structure for efficiently and effectively processing the query rather than linearly scanning the database. For the contentbased image retrieval system, the visual content of each image is characterized by a high dimensional feature. Although there are a lot of methods to index the documents, we apply the multi-probe locality sensitive hashing (MPLSH)<ref type="foot" coords="2,521.10,465.85,3.24,5.83" target="#foot_0">1</ref>  <ref type="bibr" coords="2,524.34,467.94,10.69,9.02" target="#b0">[1]</ref>, which is proved to be superior to the original LSH <ref type="bibr" coords="2,265.15,479.46,10.64,9.02" target="#b7">[8]</ref>. For the text-based retrieval system, we have shown the success of language model based information retrieval in the past years <ref type="bibr" coords="2,317.16,490.98,10.87,9.02" target="#b2">[3,</ref><ref type="bibr" coords="2,331.01,490.98,7.23,9.02" target="#b3">4]</ref>. Thus we still use the approach to index the text documents and process the query. For efficiency issue, we use the language model based search engine toolkit, LEMUR, developed by UMASS and CMU in this year <ref type="foot" coords="2,237.00,511.87,3.24,5.83" target="#foot_1">2</ref> . Using these two toolkits, the query response of the systems is fast, which significantly reduce the time of tuning systems.</p><p>In the next, we will introduce five runs submitted for the official evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 1: LRI2R_TCT_TXT</head><p>It is the basic run submitted in order to validate the efficiency of name entity extraction for the query expansion. In the run, only the text modality is used. The keywords of the query are the words occurred in the fields of title and cluster title without using the cluster description. Each field represents a different information need of the user. For example, the title is a general information need while the cluster title gives a detailed information need. Thus, we formulate multiple sub-queries using these filed and then linearly combine multiple rank lists to reach a final rank list for the query. For example, for the first query discussed in Section 1, we have 4 sub-queries and thus the final rank is from fusing four rank lists. Without any prior knowledge of which field should be weighed more, the equal weight is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 2: LRI2R_TI_TXT</head><p>The run is to evaluate query expansion using name entity extraction. The name entity extraction operates on the text descriptions corresponding to the top-N (In the experiment, N=20) image documents in the Run 1. The name entity extraction tags the words in the text description using four categories: person, organization, location and others. We will only consider the first two categories. Then we search the phrase patterns occurred closely with the query keywords occurred in the title and cluster title used in Run 1, in a window size of 10 words. We assume that each phrase pattern may represent one dimension in the diversity space and we treat each phrase pattern as an individual sub-query similar to Run 1. Thus, we have mined multiple cluster titles automatically for each query. Then the final ranking is gotten using the same operation in Run 1.</p><p>Here we show the found cluster titles for the two queries in the above: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 3: LRI2R_FUSE_TCTI_TXT</head><p>The run is just to combine the above two runs linearly and equally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run 4: LRI2R_DIVERSITY_TCTI_TXTIMG</head><p>This run is based on the Run 3. We use the visual feature to re-rank the rank list output from the Run 3. The visual feature we used is derived from the SIFT descriptor using the bag-of-visterm. In our implementation, two visual dictionaries with the size of 512 and 256 are individually generated using k-means on 20,000 randomly selected images from the database. We use the heuristic way similar to <ref type="bibr" coords="3,285.23,327.65,11.65,9.02" target="#b8">[9]</ref> to re-rank initial list. The procedure is shown in Figure <ref type="figure" coords="3,530.98,327.65,3.77,9.02">1</ref>. Thus we not only keep the order of the initial rank but also move the visually similar images into the bottom of the rank. That makes the top images visually dissimilar as much as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1 Diversity rank using visual feature Run 5: LRI2R_TCTI_TXTIMG</head><p>The fun is a linear combination run between Run 3 and a run of content based image retrieval system. The CBIR run is similar to our past system in <ref type="bibr" coords="3,183.79,566.15,10.83,9.02" target="#b2">[3,</ref><ref type="bibr" coords="3,199.02,566.15,7.22,9.02" target="#b3">4]</ref>, which uses three types of visual features, i.e. pyramid histogram of oriented gradients, Gabor texture and HSV-space histogram. From our initial analysis on the rank list of CBIR, its performance is bad and there are few images relevant in the top-10 besides.</p><p>In the 5 runs, only the last run uses the query image exemplars while the others mainly depend on the text analysis except for the run 4 uses the visual feature for re-ranking. There is only the last run is two-modality based retrieval while the others are the text-based retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>We list the official evaluation results in Tables <ref type="table" coords="3,247.15,672.96,3.97,9.02">1</ref><ref type="table" coords="3,251.12,672.96,3.97,9.02">2</ref><ref type="table" coords="3,255.10,672.96,3.97,9.02">3</ref>. The overall performance on 50 queries is shown in Table <ref type="table" coords="3,498.15,672.96,3.75,9.02">1</ref>. We can see the best run is the Run 2, which uses name entity extraction for query expansion. Comparing with its baseline, i.e. Run 1, its precision at the top-10, P@10, has a significant improvement from 0.79 to 0.848, while the cluster recall, CR@10 is increased to 0.671 from 0.657. This makes the best run achieves F-measure 0.7492. The achievement name entity extraction can mine the useful pattern for disambiguate the query and to discover the other dimensions of diversity space which are not in the query. The discovered diversity dimensions play an even important role for the queries in part 2, where the query only has a few words, most only having one word and without any diversity indicator, i.e. cluster title. This observation can be enhanced when analysing the performance on the queries in part 2 in Table <ref type="table" coords="3,531.12,753.42,3.77,9.02">3</ref>. For this type of query, the best run is Run 2 while the Run 1 becomes the worst. Their gap in terms of diversity metric Input: Initial rank Output: Diversity rank 1. Empty the diversity stack and non-novelty stack 2. Push the top-1 in initial rank into diversity stack and remove it from the latter stack.</p><p>3. Do a. Choose next document in initial rank and calculate its visual similarity with the documents in diversity stack b. If maximal similarity is lower than a threshold, then the document is novel, push it into diversity stack. Otherwise, push it into non-novelty stack c. Remove the document from the initial rank 4. While (initial rank is not empty) 5. Put all document in non-novelty stack in the diversity stack.</p><p>(CR@10, P@10, F-measure) is even bigger. The F-measure in Run 2 is 0.7528 compared with 0.6556 in Run 1. However, when we analyze their diversity performance in the queries of part 1 in Table <ref type="table" coords="4,419.82,84.66,3.77,9.02">2</ref>, it is seen that the Run 1 is better than the Run 2. It indicates that the manually selected cluster titles are superior to our automatically selected. This is reasonable because the automatically found phrase patterns may have a few noises. But the performance gap in this case only has ~3% in F-measure, not such bigger as in Table <ref type="table" coords="4,308.54,119.16,3.76,9.02">3</ref>, where the F-measure difference is ~9%. The analysis strongly demonstrates name entity extraction is a very useful technology for diversity search, especially when the query is ambiguous and diversity dimensions are not indicated. Now we analyze how the visual based re-ranking effects on the diversity by comparing the diversity performance between Run 4 and its corresponding baseline, Run 3. From Table <ref type="table" coords="4,327.18,171.18,3.76,9.02">1</ref>, the F-measure for Run 4 is 0.6987 compared with 0.6842 for the Run 3 on 50 queries. Thus, visual re-ranking has a little benefit on the ranking performance. Its effect on the two types of queries is similar when analyzing their individual metrics in Table <ref type="table" coords="4,391.11,194.16,5.01,9.02">2</ref> and<ref type="table" coords="4,415.54,194.16,7.53,9.02">3.</ref> The linear fusion is widely used when combining multiple rank lists. What does it work on diversity search? The Run 3 is a combination of Run 1 and Run 2. From Table <ref type="table" coords="4,276.76,223.20,3.77,9.02">1</ref>, we find that the Run 3 is the worst of all 5 runs, although individually the Run 1 and Run 2 are in the second and first rank position. The Run 5 is a combination of Run 3 and a CBIR system, which has a little improvement over Run 3. When combining the CBIR, the improvement is not such significant as our past years' systems <ref type="bibr" coords="4,211.46,257.70,10.88,9.02" target="#b2">[3,</ref><ref type="bibr" coords="4,225.41,257.70,7.22,9.02" target="#b3">4]</ref>. The reason should be caused by the poor performance of CBIR this year. Our experiences on CBIR in the travel domain cannot work well in the news domain. Thus, more powerful visual features for object images must be developed and person identification technology should work in the domain.</p></div>			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,62.46,764.52,113.34,9.02"><p>http://lshkit.sourceforge.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,62.46,776.04,117.71,9.02"><p>http://www.lemurproject.org/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rank</head><p>System Name Query Type Modality CR@10 P@10 map F-measure Title </p><note type="other">Cluster</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In the paper we describe the details of our submitted runs and analyze their diversity performance. Our analysis indicates two useful technology for promoting diversity search: 1) name entity extraction based query expansion and 2) visual based re-ranking. Name entity extraction can mine the phrase patterns to characterize the diversity space of information need of the user while the visual based re-ranking can further update ranking to make the top document visually dissimilar. Our successfull systems do not exploit the query image exemplars. The system of combining textbased retrieval and content-based image retrieval does not give an obvious gain. In future, we will dig into the two successful technologies to further improve the diversity performance.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="5,78.47,198.78,435.78,9.02;5,92.70,210.30,41.98,9.02" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,322.10,198.78,154.03,9.02">Modeling LSH for performance tuning</title>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Josephson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,482.67,198.78,31.58,9.02;5,92.70,210.30,36.73,9.02">Proc. of CIKM&apos;08</title>
		<meeting>of CIKM&apos;08</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,78.47,221.76,455.27,9.02" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,269.56,221.76,179.83,9.02">Visual diversification of image search results</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">H</forename><surname>Van Leuken</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Olivares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,455.50,221.76,71.93,9.02">Proc. of WWW&apos;09</title>
		<meeting>of WWW&apos;09</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,78.47,233.28,451.98,9.02;5,92.70,244.80,426.28,9.02" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,239.28,233.28,291.17,9.02;5,92.70,244.80,135.05,9.02">IPAL at CLEF 2008: mixed-modality based image search, novelty based re-ranking and extended matching</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-P</forename><surname>Chevallet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">&amp; J.-H</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,234.63,244.80,279.52,9.02">Working Notes for the CLEF 2008 Cross Language Evaluation Forum</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,78.47,256.26,459.92,9.02;5,92.70,267.78,350.27,9.02" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,340.37,256.26,198.02,9.02;5,92.70,267.78,183.10,9.02">IPAL at ImageClef 2007 mixing features, models and knowledge, Working Notes for the CLEF</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J.-P</forename><surname>Chevallet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">H D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">&amp; J.-H</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,300.82,267.78,137.29,9.02">Cross Language Evaluation Forum</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,78.47,279.30,448.01,9.02" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,224.41,279.30,219.83,9.02">A language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Jay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,450.35,279.30,71.48,9.02">Proc. of SIGIR&apos;98</title>
		<meeting>of SIGIR&apos;98</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,78.47,290.76,433.41,9.02;5,92.70,302.28,184.74,9.02" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="5,257.13,290.76,254.75,9.02;5,92.70,302.28,107.91,9.02">Incorporating non-local information into information extraction systems by Gibbs sampling</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,207.45,302.28,64.88,9.02">Proc. of ACL&apos;05</title>
		<meeting>of ACL&apos;05</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,78.47,313.80,390.87,9.02" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="5,173.84,313.80,212.95,9.02">Histograms of oriented gradients for human detection</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,393.25,313.80,70.85,9.02">Proc. of CVPR&apos;05</title>
		<meeting>of CVPR&apos;05</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,78.47,325.26,452.54,9.02;5,92.70,336.78,75.04,9.02" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="5,185.98,325.26,340.62,9.02">Near-optimal hashing algorithms for approximate nearest neighbor in high dimension</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Andoni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,92.70,336.78,69.94,9.02">Proc. of FOCS&apos;06</title>
		<meeting>of FOCS&apos;06</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="5,78.47,348.24,458.60,9.02;5,92.70,359.76,44.21,9.02" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="5,319.61,348.24,179.73,9.02">Visual diversification of image search results</title>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">H</forename><surname>Van Leuken</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Olivares</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">V</forename><surname>Zwol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,505.45,348.24,31.62,9.02;5,92.70,359.76,37.89,9.02">Proc. of WWW&apos;09</title>
		<meeting>of WWW&apos;09</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
