<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,119.31,148.79,364.38,15.48">INRIA-LEARs participation to ImageCLEF 2009</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,121.27,190.32,59.45,8.64"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LEAR Team</orgName>
								<orgName type="institution">INRIA Rhône-Alpes</orgName>
								<address>
									<addrLine>655 Avenue de l&apos;Europe</addrLine>
									<postCode>38330</postCode>
									<settlement>Montbonnot</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,187.96,190.32,81.99,8.64"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LEAR Team</orgName>
								<orgName type="institution">INRIA Rhône-Alpes</orgName>
								<address>
									<addrLine>655 Avenue de l&apos;Europe</addrLine>
									<postCode>38330</postCode>
									<settlement>Montbonnot</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,276.79,190.32,67.28,8.64"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LEAR Team</orgName>
								<orgName type="institution">INRIA Rhône-Alpes</orgName>
								<address>
									<addrLine>655 Avenue de l&apos;Europe</addrLine>
									<postCode>38330</postCode>
									<settlement>Montbonnot</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,351.23,190.32,65.03,8.64"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LEAR Team</orgName>
								<orgName type="institution">INRIA Rhône-Alpes</orgName>
								<address>
									<addrLine>655 Avenue de l&apos;Europe</addrLine>
									<postCode>38330</postCode>
									<settlement>Montbonnot</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,423.46,190.32,58.27,8.64"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LEAR Team</orgName>
								<orgName type="institution">INRIA Rhône-Alpes</orgName>
								<address>
									<addrLine>655 Avenue de l&apos;Europe</addrLine>
									<postCode>38330</postCode>
									<settlement>Montbonnot</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,119.31,148.79,364.38,15.48">INRIA-LEARs participation to ImageCLEF 2009</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6CE0445DFC876DDB024E12609CD76E0B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>I.5 [Pattern Recognition]: I.5.1 Models</term>
					<term>I.5.2 Design Methodology</term>
					<term>I.5.4 Applications Measurement, Performance, Experimentation Image Categorization, Nearest Neighbors, Similarity Measures, Feature Selection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We participated in the Photo Annotation and Photo Retrieval tasks of ImageCLEF 2009. For the Photo Annotation task we compared TagProp, SVMs, and logistic discriminant (LD) models. TagProp is a nearest-neighbor based system that learns a distance measure between images to define the neighbors. In the second system a separate SVM is trained for each annotation word. The third system treats mutually exclusive terms more naturally by assigning a probabilities to the mutually exclusive terms that sum up to one. The experiments show that (i) both TagProp and SVMs benefit from a distance combination learned with TagProp, (ii) the TagProp system, which has very few trainable parameters, performs somewhat worse than SVM in terms of EEC and AUC but better than the SVM runs in terms of the hierarchical image annotation score (HS), and (iii) LD is best in terms of HS and close to the SVM run in terms of EEC and AUC.</p><p>In our experiments for the Photo Retrieval task we compare a system using only visual search, with systems that include a simple form of text matching, and/or duplicate removal to increase the diversity in the search results. For the visual search we use our image matching system that is efficient and yields state-of-the-art image retrieval results. From the evaluation of the results we find that the adding some form of text matching is crucial for retrieval, and that (unexpectedly) the duplicate removal step did not improve results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In our participation to ImageCLEF we submitted runs in the Photo Annotation and Photo Retrieval tasks. Considering the best submitted run of each team, our systems achieved a second-best result for both tasks out of the 19 participants for each task. As the systems used for both tasks differ significantly we discuss them separately below.</p><p>In this section we present the system we used for the Photo Annotation task, and the results that were obtained. In Section 2.1, we first present our tag prediction model, and in Section 2.2 we describe the set of image features we used. We present our experimental results in Section 2.3, and our conclusions in Section 2.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">A Discriminatively Trained Nearest Neighbor Model</head><p>Our TagProp method, short for Tag Propagation, is a new nearest neighbor type model that predicts tags by taking a weighted combination of the tag absence/presence among neighbors. For a more detailed presentation and additional experimental results see <ref type="bibr" coords="2,300.65,241.58,10.58,8.64" target="#b1">[2]</ref>. The main features of TagProp are the following. First, the weights for neighbors are based on their distance, and set automatically by maximizing the likelihood of annotations in a set of training images. Second, our model allows the integration of metric learning. This enables us to optimize a linear combination of several distance measures to define the neighbor weights for the tag prediction task. Third, TagProp includes word-specific logistic discriminant models. These models use the weighted nearest neighbor tag predictions as inputs and are able, using just two parameters per word, to boost or suppress the tag presence probabilities for words that are very frequent or very rare. This results in a significant increase in the number of words that are recalled, i.e. assigned to at least one test image. Our tag prediction model is conceptually simple, yet has been shown to outperform current state-of-the-art image annotation methods on standard data sets <ref type="bibr" coords="2,422.57,349.18,10.58,8.64" target="#b1">[2]</ref>.</p><p>Weighted Nearest Neighbor Tag Prediction. Our goal is to predict the relevance of annotation tags for images. We assume that some visual similarity or distance measures between images are given, abstracting away from their precise definition. To model image annotations, we use Bernoulli models for each keyword. The dependencies between keywords in the training data are not explicitly modeled, but are implicitly exploited in our model.</p><p>We use y iw ∈ {-1, +1} to denote the absence/presence of keyword w for image i, hence encoding the image annotations. The tag presence prediction p(y iw = +1) for image i is a weighted sum over the training images, indexed by j:</p><formula xml:id="formula_0" coords="2,222.96,482.64,290.04,19.91">p(y iw = +1) = j π ij p(y iw = +1|j),<label>(1)</label></formula><formula xml:id="formula_1" coords="2,215.52,511.38,297.48,23.30">p(y iw = +1|j) = 1 - for y jw = +1, otherwise,<label>(2)</label></formula><p>where π ij denotes the weight of image j for predicting the tags of image i. We require that π ij ≥ 0, and j π ij = 1. We use to avoid zero prediction probabilities, and in practice we set = 10 -5 . To estimate the parameters that control the weights π ij we maximize the log-likelihood of the predictions of training annotations. Taking care to set the weight of training images to themselves to zero, i.e. π ii = 0, our objective is to maximize</p><formula xml:id="formula_2" coords="2,251.97,619.78,261.03,19.91">L = i,w c iw ln p(y iw ),<label>(3)</label></formula><p>where c iw is a cost that takes into account the imbalance between keyword presence and absence. Indeed, in practice, we often have many more tag absences than presences. Depending on the source of the annotations of the training images, absences can be much noisier than presences. This is the case when most tags in annotations are relevant, but the annotation do not include all relevant tags. This happens in user annotations taken from photo sharing sites such as Flickr, as users will not annotate each image with all relevant tags, but rather just put a hand-full of mostly relevant tags. To balance tag absences and presences, we set c iw = 1/n + if y iw = +1, where n + is the total number of positive labels, and likewise c iw = 1/n - when y iw = -1.</p><p>The weights π ij of training images j used when predicting tags for image i can be defined on their rank, or distance. Using the rank, we always assign a weight γ k to the k-th nearest neighbour. Thus π ij = γ k if j is the k-th nearest neighbor of i. Using the distances, we assign a weight relative to the similarity between the images. Which has the advantage that weights depend smoothly on the similarity, which is crucial if the distance is to be adjusted during training. Using distances, the weight of a training image j for an image i is defined as</p><formula xml:id="formula_3" coords="3,238.53,190.12,274.47,24.72">π ij = exp (-d w (i, j)) j exp (-d w (i, j )) ,<label>(4)</label></formula><p>where d w (i, j) = w d ij with d ij a vector of base distances between image i and j, and w contains the positive coefficients of the linear distance combination. Note that the number of parameters equals the number of base distances that are combined. When we use a single distance, referred to as the SD variant, w is a scalar that controls the decay of the weights with distance, and it is the only parameter of the model. When multiple distances are used, the variant is referred to as ML, for "metric learning".</p><p>Word-specific Logistic Discriminant Models. Weighted nearest neighbor approaches tend to have relatively low recall scores, which is easily understood as follows. In order to receive a high probability for the presence of a tag, it needs to be present among most neighbors with a significant weight. This, however, is unlikely to be the case for rare tags. Even if we are lucky enough to have a few neighbors annotated with the rare tag, we tend to predict the presence with a low probability.</p><p>To overcome this, we introduce word-specific logistic discriminant models that can boost the probability for rare tags and decrease it for very frequent ones. The logistic model uses weighted neighbor predictions by defining</p><formula xml:id="formula_4" coords="3,232.04,406.92,280.96,9.65">p(y iw = +1) = σ(α w x iw + β w ),<label>(5)</label></formula><formula xml:id="formula_5" coords="3,270.01,423.86,242.99,19.91">x iw = j π ij y jw ,<label>(6)</label></formula><p>where σ(z) = (1 + exp(-z)) -1 and x iw is the weighted average of annotations for tag w among the neighbors of i, which is equivalent to Eq. (1) up to an affine transformation. The word-specific models add two parameters to estimate for each word.</p><p>Training the Model. To reduce the computational cost of training the model, we do not compute all pairwise π ij . Rather, for each i we compute them only over a large set, and assume the remaining π ij to be zero. For each i, we select K neighbors such that we maximise k * = min{k d }, where k d is the largest neighbor rank for which neighbors 1 to k of base distance d are included among the selected neighbors. In this way we are likely to include all images with large π ij regardless of the distance combination w that is learnt. Therefore, after determining these neighborhoods, our algorithm scales linearly with the number of training images. The log-likelihood of the word-specific logistic discriminant with fixed π ij , is concave in {α w , β w }, and can be trained per keyword. To optimize the log-likelihood with respect to w we can again use the projected gradient method. In practice we estimate the parameters w and {α w , β w } in an alternating fashion. We observe rapid convergence, typically after alternating the maximization three times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image Features</head><p>We extract different types of features commonly used for image search and categorisation. We use two types of global image descriptors: Gist features <ref type="bibr" coords="3,290.94,686.19,15.27,8.64" target="#b9">[10]</ref>, and color histograms with 16 bins in each color channel for RGB, LAB, HSV representations. Local features include SIFT <ref type="bibr" coords="3,402.87,698.14,11.61,8.64" target="#b6">[7]</ref> as well as a robust hue descriptor <ref type="bibr" coords="3,132.79,710.10,15.27,8.64" target="#b13">[14]</ref>, both extracted densely on a multi-scale grid or on Harris-Laplacian interest points. Each local feature descriptor is quantized using k-means on samples from the training set. Images are then represented as a 'bag-of-words' histogram. All descriptors but Gist are L1-normalised and also computed in a spatial arrangement <ref type="bibr" coords="3,189.84,745.96,10.58,8.64" target="#b5">[6]</ref>. We compute the histograms over three horizontal regions of the image, and concatenate them to form a new global descriptor, albeit one that encodes some information of the spatial layout of the image. To limit color histogram sizes, here, we reduced the quantization to 12 bins in each channel.</p><p>This results in 15 distinct descriptors, namely one Gist descriptor, 6 color histograms and 8 bag-offeatures (2 detectors x 2 descriptors x 2 layouts). To compute the distances from the descriptors we follow previous work and use L2 as the base metric for Gist, L1 for global color histograms, and χ 2 for the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Experimental Results</head><p>For the details of the Photo Annotation task we refer to <ref type="bibr" coords="4,321.21,219.04,10.58,8.64" target="#b8">[9]</ref>. Below, before presenting the experimental results, we first briefly describe the systems that were used to generate the five runs that we submitted. We also give an overview of systems we used and the runtime of different components.</p><p>Submitted Runs. We submitted five runs: two TagProp runs, two SVM runs, and a run using a logistic discriminant model.</p><p>• TagProp-SD. For this run we used the TagProp model with a single distance, and with the wordspecific logistic model. As distance we used a simple, equally weighted, sum of all 15 base distances (which we first all normalized to have a maximum value of 1). This model has a single parameter that controls the decay of the weights, and 2 parameters for each keyword (2 × 53), in total 107 parameters.</p><p>• TagProp-ML. For this run we used the TagProp model with metric learning, i.e. a (positive) linear combination of the 15 base distances is learned. In this model 15 parameters are learned to combine the distances, and 2 parameters for each keyword; 121 parameters in total.</p><p>• SVM-SD. For this run we have trained an SVM for each separate keyword. The kernel is a standard RBF kernel of the form k(x, y) = exp (-d(x, y)/λ), where d is the 'flat' distance combination also used in TagProp-SD, and λ is set to the average of all pairwise distances between training images. To obtain probabilities for the presence of each keyword, we learn a separate sigmoidal transformation for each keyword on the SVM output values. This model involves for each keyword 3.000 parameters for the SVM weight vector, plus one parameter for the bias term, and two for the sigmoid transformation. Thus in total 53 × 3003 = 159.159 parameters.</p><p>• SVM-ML. For this run we have first learned a linear distance combination using TagProp-ML, and then use this combined distance for the SVM. Again we use an RBF kernel, where distances are normalized by the average pairwise distance among all paris of training images. The number of parameters is the same as that of the SVM-SD system, plus the 15 parameters for the learned distance.</p><p>• LD-ML. This run uses the same kernel as SVM-ML. However, rather than learning an SVM per annotation term, we learned a multi-class logistic discriminant model. Mutually exclusive terms were grouped so that the classifier gives probabilities over these terms that sum to one. For other terms this run is very similar to SVM-ML, except that here the logistic loss is minimized rather than the hinge loss. The number of parameters is the same as for the SVM-ML run.</p><p>We note that due to lack of time we did not use cross-validation to optimize the regularization parameter of the SVM and LD models. Instead, we did not use regularization, and simply minimized the empirical loss. We expect a modest but significant increase in performance when optimizing over the regularization parameter. For details on SVM and LD we refer to machine learning textbooks, such as <ref type="bibr" coords="4,440.86,662.28,10.58,8.64" target="#b0">[1]</ref>.</p><p>While all of our runs produce probabilities for the tag presence, only the last run produces multinomial probabilities for mutually exclusive image labels (e.g. for 'Spring', 'Summer', 'Fall', 'Winter', and 'No Season').</p><p>We did not apply any post-processing to enforce the requirements imposed by the hierarchical scoring method, which requires that for mutually exclusive image labels, exactly one of them has a score &gt; 0.5. We expect that using some simple post-processing to enforce these requirements the image annotation scores, but not the EEC and AUC measures, could be improved.  <ref type="table" coords="5,114.99,230.91,3.88,8.64">1</ref>: Performance of our five runs in terms of EEC (lower is better), AUC (higher is better), and the Hierarchical Scoring method with and without user agreement (higher is better). The best result among our five runs for each evaluation measure are printed in bold. The best and median result among all participants are also included.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation of Results.</head><p>In Table <ref type="table" coords="5,230.42,306.74,4.98,8.64">1</ref> we present the evaluation scores that we obtained with our different runs. We also included the best and median result among all submitted runs. First of all, we note that when we consider the best run of each of the 19 participating teams, our result is the second best in terms of EEC or AUC (and fifth in terms of the hierarchical annotation measures HS+A and HS-A). All of our submitted runs are clearly above the level of the median result.</p><p>Considering the results of our two TagProp runs, we see that there is a clear advantage of including metric learning to learn an appropriate combination of the 15 distance measures. This is in accordance with earlier experimental results <ref type="bibr" coords="5,203.22,390.43,10.58,8.64" target="#b1">[2]</ref>. Similarly, the SVM-ML run that uses the distance combination learned with TagProp also performs better than the SVM-SD run that uses the default distance combination. From cross-validation experiments we observed that the same holds for the LD classifier.</p><p>Interestingly, the SVM runs are better than the TagProp runs in terms of EEC and AUC, but worse in terms of the hierarchical scoring methods (HS+A and HS-A). The reason for this different evaluation result is not clear. It is possible that it is an effect of the class balancing in TagProp.</p><p>The LD-ML run results in probabilities that sum to one for mutually exclusive terms. Even though we did not force one of the probabilities to be &gt; 0.5 we see that this runs scores significantly better than the others in terms of the HS+A and HS-A. However, in terms of EEC and AUC this run performs slightly worse than the SVM-ML run.</p><p>System Overview and Computational Cost. All systems were implemented using mixed C, C++ and Matlab code, and were run on a Standalone PC with four Q6800 processors at 2.93GHz and 8Gb of RAM.</p><p>The image annotation systems we implemented can be summarized as follows. In the "training" stage we process the 3.000 annotated training images to find the parameters of the model, and in the "testing" stage we use the model to predict the relevance of annotation terms for the 18.000 test images. Both stages proceed in a similar manner:    The feature extraction and quantization stage is common to all our submitted runs. Using our implementation the run times were as follows:</p><p>• Feature extraction: 5h16m for all 18.000 test images, for one test image: 1.05s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• K-means local feature quantization (training images only): 5h08m</head><p>• Applying k-means quantization: 1h30m for all 18.000 test images, for one test image: 0.30s.</p><p>• Computing distances and neighborhoods: 1h42m + 5m, for one test image: 0.39s + 0.02s.</p><p>The time needed to train the different models, and to apply them to the 18.000 test images are given in Table <ref type="table" coords="6,114.55,376.13,3.74,8.64" target="#tab_1">2</ref>. Including the feature extraction stage the total to annotate one test image is approximately 1.77s. which is almost entirely spent on feature extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Conclusion</head><p>The performance of the TagProp runs are somewhat behind that of the SVM runs in terms of EEC and AUC, while the situation is reversed in terms of HS+A and HS-A. Among our runs the LD-ML run is best in terms of HS+A and HS-A, and not far behind the SVM-ML run in terms of EEC and AUC. All models benefit from using the distance combination learned with TagProp rather than the default distance combination. From these results we think it is an interesting option to integrate the metric learning within the LD learning framework, this is related to the multiple kernel learning framework <ref type="bibr" coords="6,428.42,495.06,15.27,8.64" target="#b14">[15]</ref>.</p><p>In future work we plan to perform similar comparisons also on data sets where the annotations are very noisy, as they are often used in the image annotation literature (e.g. user-provided tags of Flickr images, as opposed to carefully assigned image labels as in this evaluation or the PASCAL VOC evaluation). Such an evaluation is interesting as it will show how tolerant the models are to noise in the image labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Photo Retrieval Task</head><p>For the details of the photo retrieval task we refer to <ref type="bibr" coords="6,299.35,598.77,15.27,8.64" target="#b10">[11]</ref>.</p><p>To develop the mixed text/image retrieval tools, no validation set with a ground truth was available. We optimized our techniques by manually looking through the query results of the relatively small test set. We submitted distinct runs for promising techniques among which we could not tell the best one.</p><p>Our algorithm is structured as a series of filters that process a stream of potential results. The stream is output by an initial large-scale indexing system (a source) that performs queries in the 500 000-entries data set. The components for this chain may process images or text. They are described in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Text processing components</head><p>Due to our limited expertise on text processing, we use simple word-based parsing techniques. We do not use any semantic analysis of the text. This means, in particular, that we do not exploit the cluster descriptions. This word-based analysis is relatively effective for the Belgavox data set, because captions are made up to be used with word queries.</p><p>For queries, we used only the topic titles, not the cluster titles. Indeed, most of the cluster titles are combinations of the topic title, plus a sink that captures all other instances of the topic title (eg. the topic title "koekelberg" is associated with topic titles "fernand koekelberg", "dewael koekelberg" and the sink "koekelberg -fernand -dewael"). This means that we use the same information in Part 1 as in Part 2 of the queries.</p><p>For captions, we remove the most common form of meta-data, that usually includes the date and location of the picture and some terms of use. We detect it as a capitalized string followed with " : ". We then remove all punctuation and capital letters, split the string into words, and remove stop-words.</p><p>The text components are:</p><p>Text Source: the words occurring in the image captions are indexed in an inverted file. Results are ordered by the number of occurrences of the query topic (or the minimum of occurrences if the topic title contains several words). Ex-aequo captions are ordered randomly.</p><p>Text Filter: an input image is removed if it has no caption or a query word is missing from the caption.</p><p>Text Duplicate Filter: input captions are compared with the captions of images that have already been returned as final results. If the distance is below a threshold t txt , the captions are considered too similar and the image is removed. The distance we use is a letter edit distance (implemented as a discrete time warping), normalized by the length of the longest of the two captions. Thus, this is an approximate duplicate filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Image processing components 3.2.1 Pre-processing</head><p>All the images are pre-processed to remove white margins and calibration patterns. Without this, images could be considered similar only because of their patterns. These patterns are easy to recognize because they are always at a border of the image and there are only a few kinds of patterns: a CMYK version and a RGB version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Image matching</head><p>Our baseline system builds upon the BOF image querying method <ref type="bibr" coords="7,355.63,510.35,16.60,8.64" target="#b12">[13]</ref> and recent extensions <ref type="bibr" coords="7,462.69,510.35,10.79,8.64" target="#b3">[4,</ref><ref type="bibr" coords="7,475.93,510.35,7.47,8.64" target="#b4">5,</ref><ref type="bibr" coords="7,485.85,510.35,11.83,8.64" target="#b11">12]</ref>. In the following we briefly describe the steps used here.</p><p>Local descriptors and assignment. We extract image regions with the Hessian-affine detector <ref type="bibr" coords="7,483.94,548.83,11.62,8.64" target="#b7">[8]</ref> and compute SIFT descriptors <ref type="bibr" coords="7,197.65,560.78,11.62,8.64" target="#b6">[7]</ref> for these regions. To obtain a bag-of-features representation for an image, we assign each descriptor to the closest visual word (Euclidean distance) from a visual vocabulary. The visual vocabulary, of size k, is obtained by k-means clustering performed on an independent data set of Flickr images. Such a nearest-neighbor quantizer, which assigns an index q(x) to a descriptor x, implicitly divides the feature space into cells, i.e., the regions of a Voronoi diagram corresponding to the space partitioning.</p><p>Hamming Embedding (HE). HE provides a more precise representation of the descriptors than only the quantized index <ref type="bibr" coords="7,156.01,659.04,10.58,8.64" target="#b3">[4]</ref>, i.e., it adds a compact binary representation. This representation subdivides each cell associated with a given visual word into regions. Associating a binary signature s(x) with a descriptor x refines the descriptor matching, as two descriptors x and y match if they are assigned to the same visual word, i.e., if q(x) = q(y), and if the Hamming distance h(s(x), s(y)) between their binary signatures is lower or equal than a threshold h t . We set the signature length to 64 bit.</p><p>Weightings. The histogram of visual word occurrences is weighted using the TF-IDF weighting scheme of <ref type="bibr" coords="8,100.88,124.29,16.60,8.64" target="#b12">[13]</ref> and subsequently normalized with the L2 norm. The TF-IDF weighting factor of a matching score is</p><formula xml:id="formula_6" coords="8,247.19,143.58,261.93,26.73">w tfidf (x) = log N N q(x) 2 (<label>7</label></formula><formula xml:id="formula_7" coords="8,509.13,153.84,3.87,8.64">)</formula><p>where N is the total number of descriptors in the database and N w is the number of descriptors in the data set assigned to visual word w. This weighting reduces the influence of visual words that occur often in the whole database.</p><p>In <ref type="bibr" coords="8,115.84,212.45,10.58,8.64" target="#b3">[4]</ref>, the Hamming distance results in a binary decision, i.e., two descriptors match or not. However, the distance reflects the closeness of descriptors and should be taken into account. Since we have higher confidence in smaller distances, we weight them with a higher score.</p><p>The weight associated with a Hamming distance between binary signatures s(x) and s(y) is obtained with a Gaussian function <ref type="bibr" coords="8,191.83,260.27,10.79,8.64" target="#b4">[5]</ref>:</p><formula xml:id="formula_8" coords="8,222.95,276.64,286.18,23.88">w hd (x, y) = exp - h(s(x), s(y)) 2 σ 2 . (<label>8</label></formula><formula xml:id="formula_9" coords="8,509.13,285.27,3.87,8.64">)</formula><p>where σ = 16.</p><p>We also apply two kinds of weightings that take into account the observed frequency of the descriptors. This reduces the influence of repeating patterns in images (intra-image burstiness) and re-occurring patterns in the whole database (inter-image bustiness) <ref type="bibr" coords="8,269.74,345.99,10.58,8.64" target="#b4">[5]</ref>. In contrast with TF-IDF weighting, these weightings take into account descriptor distances and can be applied when all the query descriptors have been matched.</p><p>The point matching scores, weighted with w tfidf (x), w hd and the burstiness corrections, are summed up to produce an image matching score s.</p><p>Differences with a "same-scene" recognition method. We found that the matching performed by the default method is too strict: only images of the exact same scene are retrieved. In this case, we are more interested in image category recognition. Therefore we relaxed the image matching by:</p><p>• using a coarse visual word quantization (k = 1000 visual words instead of 20 000 or 200 000);</p><p>• using a permissive Hamming Threshold (h t = 32 instead of 24). This lets through 1/2 of the point matches, instead of 6 %;</p><p>• not performing the Weak Geometry Check. This allows large re-combinations of the scene geometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Indexing</head><p>In order to compute the image matching score efficiently, the set of descriptors of the image data set is stored in a structure similar to the inverted file used in text retrieval, and used in the image search system of <ref type="bibr" coords="8,100.79,568.52,15.27,8.64" target="#b12">[13]</ref>. This structure is composed of k lists of descriptor entries, each corresponding to a visual word. For a given visual word, the list contains an entry per descriptor that was assigned to this visual word. The entry contains the index of the image where the point was detected and the binary signature associated with the descriptor.</p><p>Compared to an exhaustive scan of the data set descriptors, this greatly reduces the complexity, because only the descriptors assigned to the same visual word as the query descriptor are processed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Components</head><p>We developed the following image components: image source: the data set images are indexed with an inverted file on their visual words. The query images are the example images of a topic. image duplicate filter: each time a result image is output, it is added to an inverted file. The filter only lets through images that are different enough from the ones in the inverted file (image matching score s im &lt; t im ). This is an approximate duplicate filter. We also developed a facial similarity component based on <ref type="bibr" coords="9,333.39,318.64,10.58,8.64" target="#b2">[3]</ref>. However, for most images, this technique is unreliable compared to the precise information given by the names in the captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiments</head><p>Here we describe our retrieval system and comment on its results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Structure</head><p>Figure <ref type="figure" coords="9,119.34,422.99,4.98,8.64" target="#fig_0">1</ref> shows the structure of our retrieval system for run 2. In addition to the components described above, we use an interleave component that interleaves the images from several input streams, and a duplicate filter that filters out images that have been returned already.</p><p>Interleaving the results from the various sources improves the diversity in the first results of the system. The duplicate filters are ordered from fastest to slowest: first the exact duplicate filter , then the two approximate ones, and . The approximate duplicate filters are intended to increase the diversity of the results by filtering out very similar output images.</p><p>The runtime for the whole process is in the order of a few seconds per query. The most expensive components are the image sources, runtimes are analysed in <ref type="bibr" coords="9,313.17,518.63,10.58,8.64" target="#b3">[4]</ref>. Indexing the 500.000 images with 9 machines took about 6 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">The runs</head><p>Run 2 is the most complete version of our retrieval system. The other runs are described as restrictions of this basic run: Run 2: The thresholds for the approximate duplicate filters are tight (t txt = 0.2, t im = 30), which means that images that are only slightly similar are let through.</p><p>Run 3: here the thresholds are adjusted so that more images are recognized as duplicates (t txt = 1, t im = 24).</p><p>Run 1: the text source component is removed. The text filter remains in effect.</p><p>Run 5: this one is similar to run 1, with the approximate duplicate filters and omitted as well.</p><p>Run 4: this run removes all text components , , .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Results</head><p>Text+image runs. Relative to other participants, our results are worse on part 1 (rank 10) of the queries than on part 2 (rank 4). This shows that other groups were able to use the cluster titles, that we ignored, in a meaningful way. Run 5 (F measure = 0.762) obtains better results than Run 1 (F measure = 0.758). Thus, it seems that our attempts to increase the diversity were not fruitful.</p><p>Our runs with text sources, Run 2 and Run 3 (maximum F measure = 0.737), have clearly lower results than the image-only sources. This is probably due to our simplistic text analysis: we use no more information than a simple "grep -c" could give. In particular, we cannot order captions that have the same number of occurrences of a query word.</p><p>Overall, for text+image, our runs are well behind the best ones from Xerox-SAS (F measure = 0.81), but perform well compared to other participants.</p><p>Image only run. Run 4 got the best result for image-only queries, albeit with a small number of participants: we obtain F measure = 0.22 vs. F measure = 0.17 for the runner-up. This is due to our very effective (and efficient) large-scale image indexing system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,102.45,604.25,240.16,8.64"><head>1 .</head><label>1</label><figDesc>Compute global and local image features for each image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,102.45,623.79,321.66,9.03"><head>2 .</head><label>2</label><figDesc>Training only: Compute a k-means clustering for each type of local features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,102.45,644.10,410.55,8.64;5,102.45,664.03,226.47,8.64;5,102.45,683.57,273.98,9.03;5,102.45,703.49,330.10,9.03;5,102.45,723.42,224.57,9.03"><head>3 . 5 . 7 .</head><label>357</label><figDesc>Map the local features to the k-means centers to produce a 'bag of words' histogram for each image. 4. For each image compute distances to training images. TagProp only: Determine nearest training images of each image. 6. SVM+LD only: For each image evaluate kernel function to all training images. Training only: Find parameters of prediction model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="9,115.32,286.59,369.27,8.64"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Structure of the retrieval system for Run 2. Thick arrows represent image streams.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,102.45,743.34,325.70,9.03"><head>Table 2 :</head><label>2</label><figDesc>Time needed to train the annotation models from 3.000 annotated images, and time needed to apply them to the 18.000 test iamges. The feature extraction stage is not included.</figDesc><table coords="6,95.98,112.42,140.83,82.76"><row><cell>Method</cell><cell cols="2">Training Testing</cell></row><row><cell>TagProp-SD</cell><cell>7.9 s.</cell><cell>1.2 s.</cell></row><row><cell>TagProp-ML</cell><cell>20.4 s.</cell><cell>7.5 s.</cell></row><row><cell>SVM-SD</cell><cell>81.6 s.</cell><cell>63.4 s.</cell></row><row><cell>SVM-ML</cell><cell cols="2">138.6 s. 109.6s.</cell></row><row><cell>LD-ML</cell><cell>2h5m</cell><cell>74.5 s.</cell></row></table><note coords="5,102.45,743.34,325.70,9.03"><p>8. Testing only: Use prediction model to compute relevance of annotation terms.</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="10,111.58,355.78,303.73,8.64" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,157.52,355.78,162.79,8.64">Pattern recognition and machine learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Spinger-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.58,374.47,401.42,8.64;10,111.58,386.42,271.43,8.64" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,340.70,374.47,172.30,8.64;10,111.58,386.42,201.79,8.64">Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,331.81,386.42,21.03,8.64">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.58,405.11,401.42,8.64;10,111.58,417.06,92.41,8.64" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,287.29,405.11,225.71,8.64;10,111.58,417.06,23.01,8.64">Is that you? Metric learning approaches for face identification</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,152.79,417.06,21.03,8.64">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.58,435.75,401.42,8.64;10,111.58,447.71,210.55,8.64" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,260.08,435.75,252.92,8.64;10,111.58,447.71,73.19,8.64">Hamming embedding and weak geometric consistency for large scale image search</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,203.13,447.71,23.25,8.64">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="304" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.58,466.39,364.50,8.64" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,261.82,466.39,142.25,8.64">On the burstiness of visual elements</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,422.66,466.39,22.81,8.64">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.58,485.08,401.42,8.64;10,111.58,497.03,192.69,8.64" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,267.44,485.08,245.56,8.64;10,111.58,497.03,121.02,8.64">Beyond bags of features: spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,250.86,497.03,22.81,8.64">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.58,515.72,381.93,8.64" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,152.27,515.72,225.92,8.64">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,385.87,515.72,18.82,8.64">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.58,534.41,401.42,8.64;10,111.58,546.36,22.42,8.64" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,239.01,534.41,190.10,8.64">Scale and affine invariant interest point detectors</title>
		<author>
			<persName coords=""><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,435.65,534.41,18.82,8.64">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="86" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.58,565.05,401.42,8.64;10,111.58,577.00,272.17,8.64" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,224.86,565.05,288.14,8.64;10,111.58,577.00,59.38,8.64">Overview of the CLEF 2009 large scale visual concept detection and annotation task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dunker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,189.05,577.00,103.90,8.64">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.58,595.69,401.42,8.64;10,111.58,607.64,154.12,8.64" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,221.33,595.69,291.67,8.64;10,111.58,607.64,33.70,8.64">Modeling the shape of the scene: a holistic representation of the spatial envelope</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,153.07,607.64,18.82,8.64">IJCV</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.58,626.33,401.42,8.64;10,111.58,638.29,275.77,8.64" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,285.04,626.33,227.96,8.64;10,111.58,638.29,62.22,8.64">Diversity in photo retrieval: overview of the ImageCLEF-Photo task 2009</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Paramita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,192.65,638.29,103.90,8.64">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.58,656.97,401.42,8.64;10,111.58,668.93,169.52,8.64" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,353.91,656.97,159.09,8.64;10,111.58,668.93,97.33,8.64">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,227.68,668.93,22.81,8.64">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.58,687.61,401.42,8.64;10,111.58,699.57,51.20,8.64" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,220.92,687.61,275.63,8.64">Video Google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,111.58,699.57,21.03,8.64">ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.58,718.25,331.65,8.64" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,242.79,718.25,128.27,8.64">Coloring local feature extraction</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,389.27,718.25,23.25,8.64">ECCV</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,111.58,736.94,378.85,8.64" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,204.44,736.94,216.70,8.64">Learning the discriminative power-invariance trade-off</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,439.23,736.94,21.03,8.64">ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
