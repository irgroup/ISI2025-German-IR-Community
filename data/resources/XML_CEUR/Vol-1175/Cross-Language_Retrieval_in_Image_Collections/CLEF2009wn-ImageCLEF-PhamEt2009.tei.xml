<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,141.24,148.86,320.53,15.15;1,250.49,170.78,102.02,15.15;1,175.38,192.75,252.23,15.11">Visual Language Modeling for Mobile Localization LIG participation at RobotVision&apos;09</title>
				<funder>
					<orgName type="full">Rgion Rhones Alpes (projet LIMA)</orgName>
				</funder>
				<funder ref="#_vWPe2XV">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,175.63,226.59,74.88,8.74"><forename type="first">Trong-Ton</forename><surname>Pham</surname></persName>
							<email>ttpham@imag.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire Informatique de Grenoble (LIG)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,261.07,226.59,75.85,8.74"><forename type="first">Loïc</forename><surname>Maisonnasse</surname></persName>
							<email>loic.maisonnasse@insa-lyon.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Laboratoire d&apos;InfoRmatique en Image et Systemes d&apos;information (LIRIS)</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,347.49,226.59,75.41,8.74"><forename type="first">Philippe</forename><surname>Mulhem</surname></persName>
							<email>mulhem@imag.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire Informatique de Grenoble (LIG)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,141.24,148.86,320.53,15.15;1,250.49,170.78,102.02,15.15;1,175.38,192.75,252.23,15.11">Visual Language Modeling for Mobile Localization LIG participation at RobotVision&apos;09</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A3D7598B20C422CEE94A3C03C75EF27A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>Algorithms, Theory Information retrieval, visual language model, late fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This working note presents our novel approach for scene recognition (i.e. localization of mobile robot using visual information) in the RobotVision task <ref type="bibr" coords="1,413.67,333.32,10.52,8.74" target="#b0">[1]</ref> based on language model <ref type="bibr" coords="1,172.85,345.27,9.97,8.74" target="#b1">[2]</ref>. Language model has been successfully used for information retrieval (specifically for textual retrieval). In recent study <ref type="bibr" coords="1,341.05,357.23,9.97,8.74" target="#b2">[3]</ref>, this model has also showed a good performance on modeling the visual information. For this reason, it can be used to address several problems in image understanding such as: scene recognition, image retrieval, etc. We have developed a visual language framework to participate in RobotVision'09 task this year. This framework consists of 3 principal components: a training step, a matching step and a post-processing step. Finally, we present the results of our approach on both validation set and test set released by the ImageCLEF's organizer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This year is the first year of RobotVision track <ref type="bibr" coords="1,304.15,646.47,10.52,8.74" target="#b0">[1]</ref> and of LIG participation in this track. The main task is to exploit the location information within a known environment of a mobile robot based on the visual information. The difficulty of this task is that the robot has to recognize the room in different illumination conditions and adapt as the environment changes (such as moving people or objects, new furniture added over the time, etc.). This might pose a problem for a visual recognition system as the trained data usually obtained at a fixed time. In the meanwhile, the system has to provide the location of the robot in real-time and in different time spans (from 6 months to 20 months).</p><p>Over the years, several classical approaches in computer vision have been proposed for this problem. In <ref type="bibr" coords="2,147.75,123.98,9.96,8.74" target="#b3">[4]</ref>, the authors suggested an appearance-based method using Support Vector Machine (SVM) to cope with illumination and pose changes. This method achieved a satisfactory performance when considering a short time interval between training and testing phrases. Another possible approach is to detect the interest point (such as SIFT, Harris-Laplace, etc.) and do a topological matching of these points <ref type="bibr" coords="2,247.80,171.80,9.97,8.74" target="#b4">[5]</ref>. This is simple approach but quite effective for recognizing some type of non rigid object (for example: building, car, motorbike, etc.). However, this method is heavily based on the quality of the interest points detected.</p><p>To participate in this competition, we reuse our visual language approach presented in <ref type="bibr" coords="2,502.49,207.66,10.51,8.74" target="#b2">[3]</ref> with the enhancement to cope with specific conditions of this task. Our model has showed a good robustness and adaptability with different kind of image representations as well as different type of visual features. With the graph representation, we have presented another layer of image understanding that is closer to the semantic layer. Moreover, graph model is integrated well with the foundation of standard language model <ref type="bibr" coords="2,276.25,267.44,10.51,8.74" target="#b1">[2]</ref> as showed in <ref type="bibr" coords="2,344.93,267.44,9.96,8.74" target="#b5">[6]</ref>. The prominent class for each image is computed based on their likelihood value. We also employed the Kullback-Leibler divergence as proposed in the classical language model approaches. In order to enhance the classification quality, we performed some post processing of the ranked result based on their relevance values. The validating process has shown a good performance of our system on different weather conditions over the time span of 6 months.</p><p>In the next section, this paper is organized as follows: Section 2 presents our visual language approach for modeling a scene. Section 3 describes the validating process based on the training and validation set. Section 4 reports our submitted run to the ImageCLEF for evaluation. The paper then concludes with the discussion and future direction of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our approach</head><p>We have applied our visual language modeling framework for the competition this year. Our model has been well demonstrated to work with scene recognition as it takes the advantage of a robust platform from the standard language model in IR fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image modeling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Image representation</head><p>We used 2 types of image representation in order to capture different visual information in image content</p><p>• Regular patch: images are divided into regular size patches. In order to make image representation robust with changing of the camera zoom, we have applied a multi-partition of images into 5x5 patches and 10x10 patches.</p><p>• Interest point: invariant keypoints are detected using Lowe's interest point detector. These keypoints are invariant with affine transformation and illumination. Local features are then extracted for each keypoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Feature extraction</head><p>From the validation set, we have learned that the color information performs quite badly in the case of changing illumination. In the same lighting condition, the color histogram could give some good results. However, in the case with a brutal changing of light condition (such as training in night condition and testing with the sunny condition) the system fails to make a satisfied judgment. So we decided to use only some features that are less sensitive with the illumination to represent the visual feature. We have extracted the following features in our experiment:</p><p>• HSV color histogram: we extract the color information from HSV color space. Each patch is represented by a vector of 512 dimensions.</p><p>• Multi-scale canny edge histogram: we used canny operator to detect the contour of objects as presented in <ref type="bibr" coords="3,218.59,123.98,9.96,8.74" target="#b6">[7]</ref>. An 80-dimensional vector was used to capture magnitudes and gradient of the contours for each patch. We have captured this information in 2 different scales of image (10x10 patches and 5x5 patches).</p><p>• Color SIFT: SIFT features are extracted using D. Lowe's detector <ref type="bibr" coords="3,414.03,167.55,9.96,8.74" target="#b7">[8]</ref>. Region around the keypoint is described by a 128-dimensional vector for each R, G, B channel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Visual vocabulary construction</head><p>Based on the analogy of image and text (i.e. visual word -word), for each feature, we construct a visual vocabulary of 500 visual words using k-means clustering algorithm. Each visual word will be designated to a concept c. Each image will then be represented using theses concepts and we used them to build our language model in the next step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual language modeling</head><p>In <ref type="bibr" coords="3,101.83,305.71,9.96,8.74" target="#b2">[3]</ref>, we have presented the image as a probabilistic graph which allows capturing the visual complexity of an image. Images are represented by a set of weighted concepts, connected through a set of directed associations. The concepts aim at characterizing the content of the image whereas the associations express the spatial relations between concepts. Our assumption is that the concepts are represented by non-overlapping regions extracted from images.</p><p>In this competition we used a reduced version of this model. We do not take into account the relationship between concepts. We thus assume that each document image d (equivalent each query image q) is represented by a set of weighted concepts W C . The concepts correspond to a visual word used to represent the image. The weight of concepts captures the number of occurrences of this concept in image. Denoting C the set of concepts over all the whole collection, W C can be define as a set of pairs (c, w(c, d)), where c is an element of C and w(c, d) is the number of times c occur in the document image i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Language model</head><p>We rely on a language model defined over concepts, as proposed in <ref type="bibr" coords="3,401.95,481.39,9.97,8.74" target="#b5">[6]</ref>, which we refer to as Conceptual Unigram Model. We assume that a query q or a document d is composed of a set W C of weighted concepts, each concept being conditionally independent to the others.</p><p>Contrary to <ref type="bibr" coords="3,161.17,517.25,10.52,8.74" target="#b5">[6]</ref> that compute a query likelihood, we compute the relevance status value rsv of a document image d for query q by using Kullback-Leiber divergence between the document model M d computed over the document image d and the query model M q computed over the query image q. By relying on the concept independence hypothesis, this leads to:</p><formula xml:id="formula_0" coords="3,124.54,586.34,388.46,9.65">RSV kld (q, d) = -D (M q M d )<label>(1)</label></formula><formula xml:id="formula_1" coords="3,188.42,600.88,324.58,55.15">= ci∈C P (c i |M q ) log P (c i |M q ) P (c i |M d ) (2) = ci∈C log(P (c i |M q ) * P (c i |M d )) - ci∈C log(P (c i |M q ) * P (c i |M q ))<label>(3)</label></formula><p>where P (c i |M d ) and P (c i |M q ) are the probability of the concept c i in the model estimated over the document d and query q respectively. Since the last element of the decomposition correspond to query entropy and does not affect documents ranking, we only compute the following decomposition:</p><formula xml:id="formula_2" coords="3,195.61,736.73,313.15,20.06">RSV kld (q, d) ∝ ci∈C log(P (c i |M q ) * P (c i |M d )) (<label>4</label></formula><formula xml:id="formula_3" coords="3,508.76,736.73,4.24,8.74">)</formula><p>where the quantity P (c i |M d ) is estimated through maximum likelihood (as is standard in the language modeling approach to IR), using Jelinek-Mercer smoothing:</p><formula xml:id="formula_4" coords="4,206.69,153.21,306.31,23.23">P (c i |M d ) = (1 -λ u ) F d (c i ) F d + λ u F c (c i ) F c<label>(5)</label></formula><p>where F d (c), representing the sum of the weight of c in all graphs from document image d and F d the sum of all the document concept weights in d. The functions F c are similar, but defined over the whole collection (i.e. over the union of all the images from all the documents of the collection). The parameter λ u corresponds to the Jelinek-Mercer smoothing. It plays the role of an IDF parameter, and helps taking into account reliable information when the information from a given document is scarce. For this part, the quantity P (c i |M q ) is estimated through maximum likelihood without smoothing on the query:</p><formula xml:id="formula_5" coords="4,250.61,286.52,262.39,23.23">P (c i |M q ) = F q (c i ) F q<label>(6)</label></formula><p>where F q (c), representing the sum of the weight of c in all graphs from query image q and F q the sum of all the query concept weights in q. The final result of each query image is a ranked list of documents associated with their rsv value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Querying</head><p>Using this model, we query the training set with each test image using one type of concepts (i.e. concepts obtains with one feature). Thus for each test image we obtain a list, standard in IR, that contains all the training set images ranked according to the rsv defined in the previous part. This list can be represented as:</p><formula xml:id="formula_6" coords="4,246.88,456.81,266.12,9.65">IL q = [(d, rsv(q, d))]<label>(7)</label></formula><p>Where IL q is a ranked list of image for query q, d is one image of the training set and rsv(q, d) is the rsv computed for this query and document images.</p><p>Assuming a function that, for each training image given its room id, we can obtain the room id of any image from the ranked list. Then, in our basic approach, we associate the query image with the room id of the best ranked image. As we can represent one image with different features and as we have more than one images of each room in the training, we will present in the following a post-processing steps to take advantage of these considerations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Post-processing of the results</head><p>We perform some fine-tuning steps of this results in order to enhance the accuracy of our system as presented in Figure <ref type="figure" coords="4,189.93,607.35,3.88,8.74" target="#fig_0">1</ref>.</p><p>• Linear fusion: we take the advantage of the different features extracted from the images.</p><p>We represent an image by a set of concept sets C i , each C i corresponding to a visual feature.</p><p>Assuming that all the concepts sets are independent one to another, we fuse the Kullback-Leiber divergence of individual sets of concepts using a sum:</p><formula xml:id="formula_7" coords="4,233.77,700.46,279.24,19.91">RSV (Q, D) = i RSV kld (q i , d i )<label>(8)</label></formula><p>where Q = q i and D = d i are the set of concept sets corresponding to the query image and to the document image respectively. </p><formula xml:id="formula_8" coords="5,255.76,471.97,257.24,9.65">RL q = [(r, RSV r (q, r)]<label>(9)</label></formula><p>with</p><formula xml:id="formula_9" coords="5,229.49,532.50,283.51,21.20">RSV r (q, r) = f n-best (q,r) RSV (q, d)<label>(10)</label></formula><p>where r correspond to a room and f n-best is a function that select the n images with the best RSV belonging to the room r.</p><p>• Filtering the unknown room: we measured a difference from the score of the 4th room to the 1st room in the room list RL. If the difference is big enough (&gt; threshold β) we keep this image. Otherwise we remove it from the list (or consider as an unknown room). In our experiment, we fixed the value β = 0.003.</p><p>• Smoothing window: we exploited the continuity in a sequence of images by smoothing the result in the temporal direction. To do that, we use a smoothing window sliding on the classified image sequences. Here, we choose the width of window w = 40 (i.e. 20 images before and after the classified image). As the result, the score of the smoothed image is the mean value of their neighborhood images.</p><formula xml:id="formula_10" coords="5,191.22,735.17,317.36,24.19">RSV window (Q i , R) = j∈[j-w/2;j+w/2] RSV (Q j , R) w (<label>11</label></formula><formula xml:id="formula_11" coords="5,508.57,743.79,4.43,8.74">)</formula><p>where w is the width of the smoothing window. In the real case, we could only use a semi-window which considers only the images before the current classified image. This leads to:</p><formula xml:id="formula_12" coords="6,193.48,170.30,315.10,24.19">RSV semi-window (Q i , R) = j∈[j-w;j] RSV (Q j , R) w (<label>12</label></formula><formula xml:id="formula_13" coords="6,508.57,178.92,4.43,8.74">)</formula><p>where w is the width of the semi-window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Validating process</head><p>The validation aims at evaluating robustness of the algorithms to visual variations that occur over time due to the changing conditions and human activity. We trained our system with the night3 condition set and tested against all the other conditions from validation set. Our objective was to understand the behavior of our system with the changing conditions and with different types of features. Moreover, the validation process can help us to fine-tune the model parameters that the latter will be used for the official test. We built 3 different language models corresponding with 3 types of visual features. The training set used is night3 set. Model Mc and Me correspond with the color histogram and the edge histogram extracted from image with the division of 5x5 patches. Model Ms corresponds with the SIFT color feature extracted from interest points. We measure the precision of system using the accuracy rate. Summary of the results is reported in Table <ref type="table" coords="6,393.42,381.93,3.88,8.74" target="#tab_0">1</ref>. We noticed that, in the same condition (e.g. night-night), the HSV color histogram Mc outperformed all the other models. However, in different conditions, the result of this model dropped significantly (from 84% to 29%). It showed that the color information is very sensitive with the changing of illumination condition. On the other hand, the edge model (Me) and the SIFT color model (Ms) are practically robust with the changing of condition. In the worst condition (nightsunny), we still obtained a quite good recognition rate of 52% for Me and 55% for Ms. As the result, edge histogram and SIFT feature are shosen as the appropriate features for our recognition system.</p><p>Follow is the results of the post-processing step based on the ranked list of Me and Ms (Table <ref type="table" coords="6,90.00,597.33,3.88,8.74" target="#tab_1">2</ref>). The fusion of these 2 models gives overall 8% of improvement. The regrouping step (as expected) helped to pop-up some prominent rooms from the score list by averaging room's n-best scores. The filtering takes part in eliminating some of the uncertain decisions base on the difference of their score after the regrouping step. Finally, the smoothing step (which is an optional step) helps to increase the performance of a sequence of images significantly by 20% more.</p><p>For the official test, we have constructed 3 models based on the validating process. We eliminated the HSV histogram model because of its poor performance on different lighting conditions and there was a little chance to have the same condition. We used the same visual vocabulary of 500 visual concepts generated for night3 set. Each model provided a ranked result corresponding with the test sequence released. The post-processing steps were performed similar to the validating process employing the same parameters. Follows are the visual language models built for the competition: Based on the 3 visual models constructed, we have submitted 6 runs to the ImageCLEF evaluation.</p><p>• 01-LIG-Me1Me2Ms: linear fusion of the results coming from 3 models (Score = 328)</p><p>• 02-LIG-Me1Me2Ms-Rk15: re-ranking the result of 01-LIG-Me1Me2Ms with the regrouping of top 15 scores for each room (Score = 415)</p><p>• 03-LIG-Me1Me2Ms-Rk15-Fil003: if the result of the 1st and the 4th in the ranked list is too small (i.e. β = 0.003), we remove image that from the list. We refrain the decision from some cases other than to mark them as unknown room (Score = 456.5)</p><p>• 04-LIG-Me1Me2Ms-Rk2-Diff20: re-ranking the result of 01-LIG-Me1Me2Ms with the regrouping of top 2 scores for each room and using smoothing window (±20 images/frame) to update the room-id from image sequences (Score = 706)</p><p>• 05-LIG-Me1Ms-Rk15: same as 02-LIG-Me1Me2Ms-Rk15 but with the fusion of 2 types of image representation. (Score = 25)</p><p>• 06-LIG-Me1Ms-Rk2-Diff20: same as 04-LIG-Me1Me2Ms-Rk2-Diff20 but with the fusion of 2 model Me1 and Ms (Score = 697) Note: run 04-LIG-Me1Me2Ms-Rk2-Diff20 and run 06-LIG-Me1Ms-Rk2-Diff20 are invalid as we used the images after the classified image for the smoothing window.</p><p>Our best run 03-LIG-Me1Me2Ms-Rk15-Fil003 for the obligatory track is ranked at 12 th place among 21 runs submitted in overall. Although, run 04-LIG-Me1Me2Ms-Rk2-Diff20 had not met the criteria of the optional task which only used the sequence before the classified image. Nervertheless, it has increased by roughly 250 points from the best obligatory run. It means that we still have room to improve the performance of our systems with the valid smoothing window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have presented a novel approach for localization of a mobile robot using visual language modeling. Theorically, this model fits within the standard language modeling approach which is well developed for IR. On the other hand, this model helps to capture in the same time the generality of the visual concepts associated with the regions from a single image or sequence of images.</p><p>The validation process has proved a good recognition rate of our system against different illumination conditions. We believe that a good extension of this model is possible in the real scenario of scene recognition (more precisely for robot self-localization). With the addition of more visual features, enhancement of system robustness and choosing the right parameter, this could be the solution to the future recognition system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,90.00,357.15,423.00,8.74;5,90.00,369.11,127.42,8.74;5,132.30,108.86,338.40,233.18"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Post-processing step of results. (1) is the scheme for the obligatory track and (2) is the scheme for the optional track</figDesc><graphic coords="5,132.30,108.86,338.40,233.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,104.94,227.46,408.06,8.77;7,104.94,247.38,404.95,8.77;7,104.94,267.31,281.97,8.77;7,104.94,289.26,408.05,8.74;7,90.00,301.21,423.01,8.74;7,90.00,313.17,41.55,8.74"><head>•</head><label></label><figDesc>Me1: visual language model based on edge histogram extracted from 10x10 patches division • Me2: visual language model based on edge histogram extracted from 5x5 patches division • Ms: visual language model based on color SIFT local features Our test has been performed on a quad core 2.00GHz computer with 8Gb of memory. The training took about 3 hours on a whole night3 set. Classification of the test sequence executed in real time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,95.91,413.26,411.18,59.25"><head>Table 1 :</head><label>1</label><figDesc>Results obtained on different conditions with 3 visual language models (Mc, Me, Ms)</figDesc><table coords="6,170.73,425.51,261.55,46.99"><row><cell>Train</cell><cell>Test</cell><cell cols="3">HSV(Mc) Edge(Me) SIFT color(Ms)</cell></row><row><cell>night3</cell><cell>night2</cell><cell>84.24%</cell><cell>59.45%</cell><cell>79.20%</cell></row><row><cell cols="2">night3 cloudy2</cell><cell>39.33%</cell><cell>58.62%</cell><cell>60.60%</cell></row><row><cell cols="2">night3 sunny2</cell><cell>29.04%</cell><cell>52.37%</cell><cell>54.78%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,139.99,628.65,323.01,34.78"><head>Table 2 :</head><label>2</label><figDesc>Result of the post-processing step based on 2 models Me and Ms</figDesc><table coords="6,144.70,640.35,313.60,23.08"><row><cell>Train</cell><cell>Test</cell><cell cols="2">Fusion Regrouping</cell><cell>Filtering</cell><cell>Smoothing</cell></row><row><cell cols="2">night3 sunny2</cell><cell>62%</cell><cell cols="3">67% (n=15) 72% (β=0.003) 92%(k=20)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work was partly supported by the <rs type="funder">French National Agency of Research</rs> (<rs type="grantNumber">ANR-06-MDCA-002</rs>) and by the <rs type="funder">Rgion Rhones Alpes (projet LIMA)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_vWPe2XV">
					<idno type="grant-number">ANR-06-MDCA-002</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="8,105.50,281.33,407.51,8.74;8,105.50,293.28,209.14,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,295.73,281.33,196.71,8.74">Overview of the clef 2009 robot vision track</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pronobis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jensfelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,105.50,293.28,112.47,8.74">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,313.21,407.51,8.74;8,105.50,325.17,255.43,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="8,247.98,313.21,244.74,8.74">A language modeling approach to information retrieval</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">M</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,105.50,325.17,225.27,8.74">Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,345.09,407.50,8.74;8,105.50,357.05,275.25,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,371.60,345.09,141.39,8.74;8,105.50,357.05,46.70,8.74">Visual language model for scene recognition</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Maisonnasse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mulhem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,186.32,357.05,115.60,8.74">Proceedings of SinFra&apos;2009</title>
		<meeting>SinFra&apos;2009<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,376.97,407.51,8.74;8,105.50,388.93,407.51,8.74;8,105.50,400.88,186.47,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,337.78,376.97,175.23,8.74;8,105.50,388.93,125.97,8.74">Svm-based discriminative accumulation scheme for place recognition</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pronobis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><forename type="middle">Martnez</forename><surname>Mozos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,272.88,388.93,240.12,8.74;8,105.50,400.88,155.26,8.74">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA08)</title>
		<meeting>the IEEE International Conference on Robotics and Automation (ICRA08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,420.81,407.50,8.74;8,105.50,432.76,151.52,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,178.77,420.81,230.69,8.74">Object recognition from local scale-invariant features</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,430.99,420.81,82.00,8.74;8,105.50,432.76,121.07,8.74">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,452.69,407.50,8.74;8,105.50,464.64,394.38,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="8,312.45,452.69,195.84,8.74">Model fusion in conceptual language modeling</title>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Maisonnasse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Chevalet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,130.95,464.64,269.65,8.74">31st European Conference on Information Retrieval (ECIR09)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="240" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,484.57,407.50,8.74;8,105.50,496.52,236.45,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,344.08,484.57,168.92,8.74;8,105.50,496.52,41.86,8.74">Efficient use of mpeg-7 edge histogram descriptor</title>
		<author>
			<persName coords=""><forename type="first">Dong</forename><forename type="middle">Kwon</forename><surname>Chee Sun Won</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Soo-Jun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,168.42,496.52,59.95,8.74">ETRI Journal</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,516.45,407.50,8.74;8,105.50,528.40,215.07,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="8,182.12,516.45,249.44,8.74">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,456.08,516.45,56.93,8.74;8,105.50,528.40,121.46,8.74">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
