<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,164.76,98.73,273.63,15.51;1,153.36,120.69,296.33,15.51;1,111.00,142.53,380.93,15.51">LSIS Scaled Photo Annotations: Discriminant Features SVM versus Visual Dictionary based on Image Frequency</title>
				<funder ref="#_FYBTzkx">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_NtSJ7Vx">
					<orgName type="full">Research Fund for the Doctoral Program of Higher Education of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,173.16,176.03,78.30,9.96"><forename type="first">Zhong-Qiu</forename><surname>Zhao</surname></persName>
							<email>zhongqiuzhao@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab. Sciences de l&apos;Information et des Systemes LSIS</orgName>
								<orgName type="institution" key="instit1">University of Sud Toulon-Var</orgName>
								<orgName type="institution" key="instit2">USTV</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">UMR CNRS 6168</orgName>
								<address>
									<settlement>La Garde</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer &amp; Information</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.44,176.03,68.56,9.96"><forename type="first">Herve</forename><surname>Glotin</surname></persName>
							<email>glotin@univ-tln.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab. Sciences de l&apos;Information et des Systemes LSIS</orgName>
								<orgName type="institution" key="instit1">University of Sud Toulon-Var</orgName>
								<orgName type="institution" key="instit2">USTV</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">UMR CNRS 6168</orgName>
								<address>
									<settlement>La Garde</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,347.52,176.03,77.70,9.96"><forename type="first">Emilie</forename><surname>Dumont</surname></persName>
							<email>emilie.r.dumont@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab. Sciences de l&apos;Information et des Systemes LSIS</orgName>
								<orgName type="institution" key="instit1">University of Sud Toulon-Var</orgName>
								<orgName type="institution" key="instit2">USTV</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">UMR CNRS 6168</orgName>
								<address>
									<settlement>La Garde</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,164.76,98.73,273.63,15.51;1,153.36,120.69,296.33,15.51;1,111.00,142.53,380.93,15.51">LSIS Scaled Photo Annotations: Discriminant Features SVM versus Visual Dictionary based on Image Frequency</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E6C16E2394D8E7307AC09D7564CCAD09</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Managment]: Cross-Language Retrieval in Image Collections (ImageCLEF)-ImageCLEFphoto LDA, Visual Dictionary, Generalized Descriptor of Fourier, Profile Entropy Feature, SVM</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we used only visual information to implement ImageCLEF2009 Photo Annotation Task. Firstly, we extract various visual features: HSV and EDGE histograms, Gabor, and recent Descriptor of Fourier and Profile Entropy Features. Then for each concept and features, we compute Linear Discriminant Analysis (LDA) to decrease the high dimension impact. Finally, we train support vector machines (SVMs), for which the outputs are considered as the confidences with which the samples belong to the concept.</p><p>Also we propose a second model, an improved version of a Visual Dictionary (VD), which is built by visual words extracted for frequency templates in the training set.</p><p>We describe the results of these 2 models, topics by topics, and we give perspectives for our VD method, that is more faster than SVM, and better than SVM for some topics. We also show that among the 19 teams, our SVM(LDA) run attains the AUC score of 0.721, and then occupies the 8th AUC rank among the 19 teams involved in this campaign, while our VD models would occupy the 10th rank.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>ImageCLEF (http://www.imageclef.org/ImageCLEF2009) is the cross-language image retrieval track run as part of the Cross Language Evaluation Forum (CLEF) campaign. This track evaluates retrieval of images described by text captions based on queries in a different language; both text and image matching techniques are potentially exploitable.</p><p>Therefore, the task provides a training database of 5000 images which are labeled by 53 concepts which has a hierarchy. Only these data are used to train retrieval models. The test database consists of 13000 images, for each of which participating groups are required to determine the presence/absence of the concepts. More details on the dataset can be found in <ref type="bibr" coords="1,435.43,695.03,14.58,9.96" target="#b12">[13]</ref>.</p><p>We use various features described in nexte section: PEF <ref type="bibr" coords="2,351.45,61.43,15.29,9.96" target="#b1">[2,</ref><ref type="bibr" coords="2,370.70,61.43,7.03,9.96" target="#b2">3]</ref>, HSV and EDGE histograms, new Descriptor of Fourier <ref type="bibr" coords="2,203.77,73.43,9.89,9.96" target="#b6">[7]</ref>, and Gabor. The we use an LDA to reduce their dimension. Finally, we use the Least Square support vector machine (LS-SVM) to produce concept similarity. Another original method called Visual Dictionary is proposed and implemented in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Visual Features</head><p>We use a new feature, the pixel 'profile' entropy (PEF) <ref type="bibr" coords="2,336.52,152.03,9.89,9.96" target="#b1">[2]</ref>, giving the entropy of a pixel profiles in horizontal and vertical directions. The advantage of PEF is to combine raw shape and texture representations, with a low CPU cost feature, and already gave good performances (second best rank in the official ImagEval 2006 campaign (see www.imageval.org)).</p><p>Here we use extended PEF <ref type="bibr" coords="2,228.26,199.91,10.43,9.96" target="#b2">[3]</ref> using the harmonic mean of the pixel of each row or column. The idea is that the object or pixel region distribution, which is lost in arithmetic mean projection, could be partly represented by the harmonic mean. These two projections are then expected to give complementary and/or concept dependant information. PEF are computed into three equal horizontal and vertical image slices, yieding to a total of 150 dimensions.</p><p>We also use classical features : HSV and EDGE histograms, and Gabor, and recent Descriptor of Fourier robust to rotation <ref type="bibr" coords="2,216.00,271.67,9.98,9.96" target="#b6">[7]</ref>. We train our two models on these features that represent a total of 400 dimensions. We use LDA to reduce the feature dimensions as depicted in next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Linear Discriminant Analysis</head><p>In general, the LDA <ref type="bibr" coords="2,179.69,338.39,15.46,9.96" target="#b10">[11]</ref> is used to find an optimal subspace for classification in which the ratio of the between-class scatter and the within-class scatter is maximized. Let the between-class scatter matrix be defined as</p><formula xml:id="formula_0" coords="2,231.24,369.88,138.84,30.45">S B = c i=1 n i (X i -X)(X i -X) T</formula><p>and the within-class scatter matrix be defined as</p><formula xml:id="formula_1" coords="2,218.16,423.88,165.12,30.69">S W = c i=1 X k ∈Ci (X k -X i )(X k -X i ) T</formula><p>where X = ( n j=1 X j )/n is the mean image of the ensemble, and X i = ( n i j=1 X i j )/n i is the mean image of the ith class, n i is the number of samples in the ith class, c the number of classes, and C i the ith class. As a result, the optimal subspace, E optimal by the LDA can be determined as follows:</p><formula xml:id="formula_2" coords="2,198.24,514.24,206.52,24.93">E optimal = arg max E |E T S B E| |E T S W E| = [c 1 , c 2 , ..., c c-1 ]</formula><p>where [c 1 , c 2 , ..., c c-1 ] is the set of generalized eigenvectors of S B and S W corresponding to the largest generalized eigenvalues</p><formula xml:id="formula_3" coords="2,225.10,557.75,149.49,32.29">λ i ; i = 1, 2, ..., c -1, i.e., S B E i = λ i S W E i , i = 1, 2, ..., c -1</formula><p>Thus, the feature vector, P , for any query face images, X, in the most discriminant sense can be calculated as follows: P = E T optimal U T X In our image retrieval task, LDA output only 1 dimension since the classification problem for each concept is 2-class.</p><p>In order to design fast image retrieval systems, we use the Least Squares Support Vector Machine (LS-SVM). The SVM <ref type="bibr" coords="3,185.82,95.27,10.43,9.96" target="#b0">[1]</ref> first maps the data into a higher dimensional input space by some kernel functions, and then learns a separating hyperspace to maximize the margin. Currently, because of its good generalization capability, this technique has been widely applied in many areas such as face detection, image retrieval, and so on <ref type="bibr" coords="3,281.82,131.15,10.43,9.96" target="#b3">[4,</ref><ref type="bibr" coords="3,295.25,131.15,7.03,9.96" target="#b4">5]</ref>. The SVM is typically based on an ε-insensitive cost function, meaning that approximation errors smaller than ε will not increase the cost function value. This results in a quadratic convex optimization problem. So instead of using an ε-insensitive cost function, a quadratic cost function can be used. The least squares support vector machines (LS-SVM) <ref type="bibr" coords="3,137.97,178.91,10.43,9.96" target="#b5">[6]</ref> are reformulations to the standard SVMs which lead to solving linear KKT systems instead, which is quite computationally attractive. Thus, in all our experiments, we will use the LS-SVMlab1.5 (http://www.esat.kuleuven.ac.be/sista/lssvmlab/).</p><p>In our experiments, the RBF kernel</p><formula xml:id="formula_4" coords="3,225.72,235.39,151.56,12.01">K(x 1 -x 2 ) = exp(-|x 1 -x 2 | 2 /σ 2 )</formula><p>is selected as the kernel function of our LS-SVM. So there is a corresponding parameter, σ , to be tuned. A large value of σ 2 indicates a stronger smoothing. Moreover, there is another parameter, γ, needing tuning to find the tradeoff between to stress minimizing of the complexity of the model and to stress good fitting of the training data points.</p><p>We set these two parameters as respectively. So a total of hundred SVMs were constructed for each SVM model, and then we selected the best SVM using the validation set.</p><formula xml:id="formula_5" coords="3,217.92,327.07,27.65,11.20">σ 2 = [</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Visual Dictionary Method</head><p>The visual dictionary is an original method to annotated images which is an improvement of the method proposed in <ref type="bibr" coords="3,182.82,458.87,9.89,9.96" target="#b7">[8]</ref>. We construct a Concept Visual Dictionary composed by visual words intended to represent semantic concept which consists of five steps :</p><p>• Visual elements. Images are decomposed into visual elements where a visual element is an image area, i.e. images are split into a regular grid.</p><p>• Representation of visual elements. We use the most classical and intuitive approach consisting in representing a visual word by usual features HSV, GABOR, EDGE, and also PEF <ref type="bibr" coords="3,499.37,534.59,9.98,9.96" target="#b2">[3]</ref>, and DF <ref type="bibr" coords="3,151.66,546.47,9.98,9.96" target="#b6">[7]</ref>.</p><p>• Global Visual Dictionary. For each feature, we cluster visual elements using the K-Means algorithm with a predefined number of clusters and using the Euclidean distance in order to group visual elements and to smooth some visual artifacts. And then, for each cluster, we select the medoid to be a visual word and to compose the visual dictionary of a feature.</p><p>• Image transcription. Based on the Global Visual Dictionary, we replace visual elements by the nearest visual word in the visual dictionary. And then, the image representation is based on the frequency of the visual words within the image for each feature.</p><p>• Concept Visual Dictionary. We select the most discriminative visual words for a concept given to compose a Concept Visual Dictionary. To filter the words, we use a entropy-based reduction, which is developed from work carried out in <ref type="bibr" coords="3,357.05,689.99,14.58,9.96" target="#b9">[10]</ref>. In a second step, we propose an adaptation of the common text-based paradigm to annotated images. We used the tf-idf weighting scheme <ref type="bibr" coords="4,291.94,281.51,10.43,9.96" target="#b8">[9]</ref> in the vector space model together with cosine similarity to determine the similarity between a visual document and a concept. To use this scheme, we represent an image by the frequency of the visual words within the image for different features : HSV, GABOR, PEF, EDGE and DF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head><p>The models based on SVM to implement the image retrieval in the task is shown in Figure <ref type="figure" coords="4,488.47,371.87,5.03,9.96" target="#fig_1">1</ref> and contains the following steps:</p><p>Step 1) Split the VCDT labeled image dataset into 2 sets, namely training image dataset and validation set.</p><p>Step 2) Extract the visual features from the training image data using our extraction method; Learn and perform LDA reduction on these features; train and generate lots of SVM with different parameters.</p><p>Step 3) Use the validation set to select the best model</p><p>Step 4) Extract the visual features from the VCDT test image database using our extraction method; perform LDA reduction on these features; and then use the best model to find the best discriminant feature.</p><p>Step 5) Sort the test images by the distances from the positive training images and produce the final rank result.</p><p>The same train and development sets have been used for the VD and SVM training. We submitted five runs to the official evaluation, from which the two best are depicted here :</p><formula xml:id="formula_6" coords="4,105.00,571.67,82.38,9.96">• Run SVM(LDA)</formula><p>It consists in performing SVM on the LDA of [ PEF150 + HSV + EDGE + DF ] features to reduce the impact of the highdimension malediction. The test of 10K images and 50 topics costed 2 minutes on usual pentium IV PC.</p><p>• Run VD Is is a vector search system, using small icons from the images. The visual features are the HSV, edge, and Gabor. This model needs only 2 hours of training on a pentium IV 3Ghz, 4 GRam, and test is faster than SVM.</p><p>The Area Under the Curve (Receiving Operator Caracteristisc integral) for each topic and method are depicted in figure <ref type="figure" coords="4,222.03,695.03,3.90,9.96" target="#fig_2">2</ref>. )) is better than VD with AUC = 0.72. It is our best run, it occupies the 8th rank among the 19 participating teams. The same SVM(LDA) strategy has been applied on an another set of features (AVEIR group features described in <ref type="bibr" coords="5,145.65,479.15,14.70,9.96" target="#b11">[12]</ref>), but results to AUC = 0.50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The SVM model has an higher average AUC than VD (0.722 against 0.682), but VD is lighter than SVM, and is, for some topics, better than it. The table <ref type="table" coords="5,366.29,545.87,5.03,9.96" target="#tab_2">3</ref> gives the list of worst and best topics for VD compared to SVM. The worst are for example "Snow, Winter, Sky, Desert, Beach ...", that are maybe topics with one clear visual representation, for example we can imagine a dominant color and texture for snow or sky ... On the contrary, VD is better than SVM for "Flower, Vehicle, Food, Autumn,..." that are maybe concepts with higher visual variations in color, texture... Thus it suggests that statistics of simpler visual concepts are maybe better modelized by SVM, while more complex visual concepts may be better represented by our Visual Dictionary model. The respective performances of these two models shall also be tied with the number of training samples. We currently investigate research on this promising improved VD, and we propose an optimal fusion with SVM in order to benefit of the properties of the both.   <ref type="table" coords="6,214.22,659.27,3.90,9.96" target="#tab_2">3</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,245.57,328.31,139.54,9.96;3,90.00,350.27,16.07,9.96;3,235.32,362.27,132.43,9.96"><head></head><label></label><figDesc>4 25 100 400 600 800 1000 2000] and γ = [4 8 16 32 64 128 256 512]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,174.00,239.03,251.40,9.96;4,90.00,58.97,423.00,165.57"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The framework for image retrieval of each topic</figDesc><graphic coords="4,90.00,58.97,423.00,165.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,90.00,611.39,422.67,9.96;6,90.00,623.39,422.72,9.96;6,90.00,635.39,422.65,9.96;6,90.00,647.27,422.67,9.96;6,90.00,659.27,132.01,9.96"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Area Under the Curve (AUC) evaluations for each topic and method. The number of each topic is the one given by the organizers. Here they are sorted according to the relative performance of VD against SVM. The absolute AUC for SVM and for VD (o-) are given in the right figure, while their relative difference in percent is given in the left. The 10 worst and best topics are depicted in Table3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,90.00,68.39,454.01,384.84"><head>Table 1 :</head><label>1</label><figDesc>Our two best submitted runs of ImageCLEF2009 Photo Annotation Task</figDesc><table coords="5,90.00,80.03,454.01,373.20"><row><cell>RUN</cell><cell>LAB</cell><cell>Method</cell><cell></cell><cell cols="2">Average Equal Error Rate AUC</cell></row><row><cell cols="4">SVM(LDA) LSIS SVM(LDA([PEF150+HSV+EDGE+DF]))</cell><cell>0.3308</cell><cell>0.7209</cell></row><row><cell>VD</cell><cell>LSIS</cell><cell cols="2">Visual Dictionary</cell><cell>0.3607</cell><cell>0.6855</cell></row><row><cell cols="5">Table 2: The team best runs of ImageCLEF2009 Photo Annotation Task (from the Official eval-</cell></row><row><cell>uations)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>RANK</cell><cell>LAB</cell><cell>Average EER</cell><cell>AUC</cell></row><row><cell></cell><cell>1</cell><cell>ISIS</cell><cell>0.234476</cell><cell>0.838699</cell></row><row><cell></cell><cell>2</cell><cell>LEAR</cell><cell>0.249469</cell><cell>0.823105</cell></row><row><cell></cell><cell>3</cell><cell>FIR2</cell><cell>0.253566</cell><cell>0.817159</cell></row><row><cell></cell><cell>4</cell><cell>CVIUI2R</cell><cell>0.253296</cell><cell>0.813893</cell></row><row><cell></cell><cell>5</cell><cell>XRCE</cell><cell>0.267301</cell><cell>0.802704</cell></row><row><cell></cell><cell>6</cell><cell>bpacad</cell><cell>0.291718</cell><cell>0.773133</cell></row><row><cell></cell><cell>7</cell><cell>MMIS</cell><cell>0.312366</cell><cell>0.744231</cell></row><row><cell></cell><cell>* 8</cell><cell>LSIS (SVM(LDA))</cell><cell>0.330819</cell><cell>0.720931</cell></row><row><cell></cell><cell>9</cell><cell>IAM</cell><cell>0.330401</cell><cell>0.714825</cell></row><row><cell></cell><cell>10</cell><cell>LIP6</cell><cell>0.372169</cell><cell>0.673089</cell></row><row><cell></cell><cell>11</cell><cell>MRIM</cell><cell>0.383630</cell><cell>0.643450</cell></row><row><cell></cell><cell>12</cell><cell>AVEIR</cell><cell>0.440589</cell><cell>0.550866</cell></row><row><cell></cell><cell>-</cell><cell>Random</cell><cell>0.500280</cell><cell>0.499307</cell></row><row><cell></cell><cell>13</cell><cell>CEA</cell><cell>0.500495</cell><cell>0.469035</cell></row><row><cell></cell><cell cols="2">14 TELECOM ParisTech</cell><cell>0.526302</cell><cell>0.459922</cell></row><row><cell></cell><cell>15</cell><cell>Wroclaw</cell><cell>0.446024</cell><cell>0.220957</cell></row><row><cell></cell><cell>16</cell><cell>KameyamaLab</cell><cell>0.452374</cell><cell>0.164048</cell></row><row><cell></cell><cell>17</cell><cell>UAIC</cell><cell>0.479700</cell><cell>0.105589</cell></row><row><cell></cell><cell>18</cell><cell>INAOE</cell><cell>0.484685</cell><cell>0.099306</cell></row><row><cell></cell><cell>19</cell><cell>apexlab</cell><cell>0.482693</cell><cell>0.070400</cell></row><row><cell cols="5">We show in Table 2 that the SVM(LDA([PEF150+HSV+EDGE+DF]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,108.60,68.39,385.49,141.60"><head>Table 3 :</head><label>3</label><figDesc>Lists of the 10 worst and ten best topics for the VD method compared to SVM Ten worst topics for VD Ten best topics for VD</figDesc><table coords="7,191.52,92.39,183.75,117.60"><row><cell>Snow</cell><cell>Flowers</cell></row><row><cell>Desert</cell><cell>Partly-Blurred</cell></row><row><cell>Day</cell><cell>Partly-Blurred</cell></row><row><cell>Sky</cell><cell>Motion-Blur</cell></row><row><cell>Single-Person</cell><cell>Vehicle</cell></row><row><cell>Overall-Quality</cell><cell>Macro</cell></row><row><cell>No-Visual-Time</cell><cell>No-Blur</cell></row><row><cell>Beach-Holidays</cell><cell>Food</cell></row><row><cell>Out-of-focus</cell><cell>Autumn</cell></row><row><cell>Winter</cell><cell>Citylife</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work was supported by <rs type="funder">French National Agency of Research</rs> (<rs type="grantNumber">ANR-06-MDCA-002</rs>) and <rs type="funder">Research Fund for the Doctoral Program of Higher Education of China</rs> (<rs type="grantNumber">200803591024</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_FYBTzkx">
					<idno type="grant-number">ANR-06-MDCA-002</idno>
				</org>
				<org type="funding" xml:id="_NtSJ7Vx">
					<idno type="grant-number">200803591024</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="7,105.47,322.43,302.11,9.96" xml:id="b0">
	<monogr>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m" coord="7,159.67,322.43,110.93,9.96">Statistical learning theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.47,342.35,407.30,9.96;7,100.56,354.35,347.09,9.96" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="7,156.20,342.35,325.87,9.96">Information retrieval and robust perception for a scaled multi-structuration</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>Toulon</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University Sud Toulon-Var</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Thesis for habilitation of research direction</note>
</biblStruct>

<biblStruct coords="7,105.47,374.27,407.21,9.96;7,100.56,386.15,412.20,9.96;7,100.56,398.15,99.12,9.96" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="7,264.24,374.27,248.44,9.96;7,100.56,386.15,98.44,9.96">Efficient Image Concept Indexing by Harmonic &amp; Arithmetic Profiles Entropy</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,208.72,386.15,233.54,9.96">IEEE International Conference on Image Processing</title>
		<meeting><address><addrLine>Cairo, Egypt</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">November 7-11, (2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.47,418.07,407.42,9.96;7,100.56,430.07,288.19,9.96" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="7,205.13,418.07,217.61,9.96">Face detection using spectral histograms and SVMs</title>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">A</forename><surname>Waring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,431.02,418.07,81.88,9.96;7,100.56,430.07,185.25,9.96">IEEE Transactions on Systems, Man, and Cybernetics, Part B</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="467" to="476" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.47,449.99,407.19,9.96;7,100.56,461.87,412.46,9.96;7,100.56,473.87,27.80,9.96" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="7,231.15,449.99,263.86,9.96">Support Vector Machine active learning for image retrieval</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chang</forename><surname>Edward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,100.56,461.87,310.68,9.96">Proceedings of the ninth ACM international conference on Multimedia</title>
		<meeting>the ninth ACM international conference on Multimedia<address><addrLine>Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="107" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.47,493.79,407.29,9.96;7,100.56,505.79,162.88,9.96" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="7,250.55,493.79,214.23,9.96">Least Squares Support Vector Machine Classifiers</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,483.63,493.79,29.13,9.96;7,100.56,505.79,78.41,9.96">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="293" to="300" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.47,525.71,407.32,9.96;7,100.56,537.59,412.33,9.96;7,100.56,549.59,27.80,9.96" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="7,376.13,525.71,136.66,9.96;7,100.56,537.59,284.93,9.96">Generalized Fourier Descriptors with Applications to Objects Recognition in SVM Context, In 30</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Smach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lemaitre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Miteran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Atri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,393.13,537.59,91.13,9.96">J. Math Imaging Vis</title>
		<imprint>
			<biblScope unit="page" from="43" to="71" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.47,569.51,407.31,9.96;7,100.56,581.51,412.19,9.96;7,100.56,593.39,30.56,9.96" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="7,219.76,569.51,163.78,9.96">Video search using a visual dictionary</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dumont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Merialdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,403.56,569.51,109.22,9.96;7,100.56,581.51,246.83,9.96">CBMI 2007, 5th International Workshop on Content-Based Multimedia Indexing</title>
		<meeting><address><addrLine>Bordeaux, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">June 25-27, 2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,105.47,613.31,407.33,9.96;7,100.56,625.31,144.22,9.96" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="7,243.75,613.31,201.79,9.96">Introduction to Modern Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>McGraw-Hill, Inc</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.50,645.23,402.18,9.96;7,100.56,657.23,135.35,9.96" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="7,223.60,645.23,270.77,9.96">Fuzzy-Rough Data Reduction with Ant Colony Optimization</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="7,100.56,657.23,104.19,9.96">Fuzzy Sets and Systems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="7,110.50,677.15,402.27,9.96;7,100.56,689.03,213.74,9.96" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="7,330.18,677.15,122.30,9.96">Eigenfaces versus fisher faces</title>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">P</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="7,458.82,677.15,53.95,9.96;7,100.56,689.03,128.42,9.96">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.50,61.43,402.15,9.96;8,100.56,73.43,218.18,9.96" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="8,197.09,61.43,315.56,9.96;8,100.56,73.43,59.41,9.96">Comparison of Various AVEIR Visual Concept Detectors with an Index of Carefulness</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,180.06,73.43,53.82,9.96">ImageClef</title>
		<imprint>
			<biblScope unit="volume">09</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,110.50,93.35,402.10,9.96;8,100.56,105.35,315.16,9.96" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="8,213.14,93.35,299.47,9.96;8,100.56,105.35,90.88,9.96">Overview of the CLEF 2009 Large Scale -Visual Concept Detection and Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dunker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,199.23,105.35,112.06,9.96">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
