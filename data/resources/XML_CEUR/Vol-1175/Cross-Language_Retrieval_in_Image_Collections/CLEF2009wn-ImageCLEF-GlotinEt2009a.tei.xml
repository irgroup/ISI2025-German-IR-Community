<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,102.60,148.73,398.27,16.37;1,135.48,170.69,332.30,16.37">Comparison of Various AVEIR Visual Concept Detectors with an Index of Carefulness</title>
				<funder ref="#_HBaX49E">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,131.28,204.41,41.31,9.62"><forename type="first">H</forename><surname>Glotin</surname></persName>
							<email>glotin@univ-tln.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Systems &amp; Information S ences</orgName>
								<orgName type="laboratory">LSIS UMR CNRS 6168</orgName>
								<orgName type="institution">Univ. Sud Toulon-Var</orgName>
								<address>
									<settlement>Toulon</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,183.12,204.41,74.66,9.62"><forename type="first">A</forename><surname>Fakeri-Tabrizi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Université Pierre et Marie Curie -Paris</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,268.08,204.41,47.68,9.62"><forename type="first">P</forename><surname>Mulhem</surname></persName>
							<affiliation key="aff4">
								<orgName type="laboratory">Lab. d&apos;Informatique de Grenoble</orgName>
								<orgName type="institution" key="instit1">Univ. Joseph Fourier</orgName>
								<orgName type="institution" key="instit2">LIG UMR CNRS</orgName>
								<address>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,326.40,204.41,52.46,9.62"><forename type="first">M</forename><surname>Ferecatu</surname></persName>
							<email>marin.ferecatu@telecom-paristech.fr</email>
							<affiliation key="aff5">
								<orgName type="department">Institut TELECOM ParisTech</orgName>
								<orgName type="laboratory">LTCI UMR CNRS 5141</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,389.40,204.41,33.79,9.62"><forename type="first">Z</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Systems &amp; Information S ences</orgName>
								<orgName type="laboratory">LSIS UMR CNRS 6168</orgName>
								<orgName type="institution">Univ. Sud Toulon-Var</orgName>
								<address>
									<settlement>Toulon</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer &amp; Information</orgName>
								<orgName type="institution">Hefei Univ. of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,440.04,204.41,40.34,9.62"><forename type="first">S</forename><surname>Tollari</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Université Pierre et Marie Curie -Paris</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,196.80,218.45,46.13,9.62"><forename type="first">G</forename><surname>Quenot</surname></persName>
							<affiliation key="aff4">
								<orgName type="laboratory">Lab. d&apos;Informatique de Grenoble</orgName>
								<orgName type="institution" key="instit1">Univ. Joseph Fourier</orgName>
								<orgName type="institution" key="instit2">LIG UMR CNRS</orgName>
								<address>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,256.68,218.45,37.70,9.62"><forename type="first">H</forename><surname>Sahbi</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Institut TELECOM ParisTech</orgName>
								<orgName type="laboratory">LTCI UMR CNRS 5141</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,308.16,218.45,48.41,9.62"><forename type="first">E</forename><surname>Dumont</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Systems &amp; Information S ences</orgName>
								<orgName type="laboratory">LSIS UMR CNRS 6168</orgName>
								<orgName type="institution">Univ. Sud Toulon-Var</orgName>
								<address>
									<settlement>Toulon</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,367.20,218.45,50.43,9.62"><forename type="first">P</forename><surname>Gallinari</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Université Pierre et Marie Curie -Paris</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">UMR CNRS 7606 LIP6</orgName>
								<address>
									<postCode>F-75016</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,102.60,148.73,398.27,16.37;1,135.48,170.69,332.30,16.37">Comparison of Various AVEIR Visual Concept Detectors with an Index of Carefulness</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E5089DA6E0C8EA40E0E6E667E9FCC1B5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2.3 [Database Management]: Cross-Language Retrieval in Image Collections (ImageCLEF) Carefulness Index, SVM, Fusion, SIFT, Gabor, HSV, Profile Entropy Feature, Ontology</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual annotation is still an open issue. The Content Based community admits that a plurality of features and systems shall be considered. We present in this paper four very different strategies using not only visual information but also text, to implement ImageCLEF2009 Photo Annotation Task. The visual features are various, such as HSV, Gabor, EDGE, SIFT, and some more recent. Then we study each model performances, and propose a new measure, the Carefulness Index (Q) computed on the histogram of the model's outputs. Q seems to be correlated with the model performances.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This year, the annotation task focuses on scaling the algorithms to thousands of images and possibly more, which is a very difficult task. Indeed, image annotation is still an unsolved problem and recent state of the art algorithms perform less than satisfactorily on most image databases. The image annotation task uses 53 concepts, many of them being holistic, that is they are not associated with some part of an image, but with the visual impressions extracted from the whole image. Furthermore, even the concepts corresponding to objects are associated with the entire image and not to some part of it. Local methods, for example those based on the extraction of keypoints or image regions, are less likely to function correctly in this case.</p><p>In order to analyse which strategy shall be optimal for this kind of task, we depict four very different models that have been built independently to each other. We give their performances, and propose a new measure, the Carefulness Index (Q) computed on the histogram of the model's outputs. Q seems to be correlated with the model performances. We also analyse simple fusion models. In average the best model is the simplest, the arithmetic average, compared to the selection of the a priori best model, or an early fusion model.</p><p>The next section presents the four models, then the results are analysed and the Carefulness Index measure is proposed in section 4. Other comments on the models performances are given before to conclude.</p><p>2 The four different models</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model 1: SVM based on HSV with ROC loss function</head><p>We used the color-based visual descriptors in this model. As in <ref type="bibr" coords="2,368.53,416.57,10.00,9.62" target="#b6">[7]</ref>, we segment the images into 3 horizontal regions with the same sizes. We believe that these visual segmentation is particularly interesting for general concepts (i.e. not objects), as for instance: sky, sunny, vegetation, sea... (see figure <ref type="figure" coords="2,137.64,452.45,3.88,9.62" target="#fig_0">1</ref>). For each region, we compute a color histogram in the HSV space.</p><p>We train a SVM classifier<ref type="foot" coords="2,216.12,463.28,3.97,4.84" target="#foot_0">1</ref> which has a linear kernel.Because of the imbalanced class problem, we use a ROC area as the loss function as proposed in <ref type="bibr" coords="2,330.36,476.33,9.91,9.62" target="#b0">[1]</ref>. So we consider not only the misclassification in each learning iteration, but also the number of positive and negative examples in order to avoid the fault ignorance. ROCarea can be computed from the number of swapped pairs SwappedP aires = { (i, j) : (y i &gt; y j )and(w Each feature is a 128-dimension vector. A visual vocabulary containing 4000 dimensions was then generated using the SIFT features of the learning set, yielding to a 4000 dimensions vector for each image. The third feature set, called HSVGAB, is an early fusion of colour and texture features. We used a 64 dimensions HSV colour histogram concatenated with a 40 dimensions vector describing gabor filters energy (7 dimensions, 5 scales). For the RGB, SIFT, and the HSVGAB features we used then a simple one against all SVM (RBF kernel) that learns the probability for one sample of belonging to each concept. For the SIFT features, we used additionally a multiple SVM leaning process. Consider one concept C having pc positive samples, and nc negative samples (ni = 5000 -pc ¿¿ pc). We define Nc SVM with all the positive samples and 2*pc negative samples, so that union the negative samples as all SVMs cover all the pc negatives samples of C. Each of these SVM learns the probability of belonging to each class concept/non concept. For one concept, we sum-up then the results for all the NC SVMs. We applied then a scaling in a way to fit the learning set a priori probabilities. Then we select the best feature/learning combination for each concept. We took into account the hierarchy of concept in the following way: a) when conflicts occur (for instance the tag Day and the tag Night are associated to one image of the test set), we keep unchanged the larger value tag, and we decrease (linearly) the value all the other conflicting tags, b) we propagated the concepts values in a bottom-up way if the values of the generic concept is increased, otherwise we do not update the values. More details can be found in <ref type="bibr" coords="3,449.55,291.17,10.00,9.62" target="#b3">[4]</ref>.</p><formula xml:id="formula_0" coords="2,90.00,522.55,334.98,29.16">T x i &lt; w T x j )} i.e.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model 3: average of Gabor-HSV SVMs and of Visual Dictionary</head><p>This model is an average of only visual information models, based on SVM and Visual Dictionary approaches on some new features depicted in <ref type="bibr" coords="3,289.92,349.25,10.00,9.62" target="#b4">[5]</ref>. As some of these models were proposed for the first time, we decided to build for this paper an average model that is the arithmetic average of three sub-models.</p><p>In sum, the Model 3 is built from various visual features: HSV, EDGE, Gabor, and the recent DF and Profile Entropy Features (PEF) <ref type="bibr" coords="3,300.29,397.01,9.91,9.62" target="#b6">[7]</ref>. Firstly for each concept, we compute Linear Discriminant Analysis (LDA) and we train support vector machines (SVMs) <ref type="bibr" coords="3,423.49,409.01,9.91,9.62" target="#b4">[5]</ref>. We also consider the SVM trained on the PEF. Third, we merge a Visual Dictionary (VD) model, which constructs a concept visual dictionary composed by visual words <ref type="bibr" coords="3,324.75,432.89,9.91,9.62" target="#b4">[5]</ref>. We notice after the evaluation that this average is suboptimal, it is below the 8th AUC rank that is taken by one of its component (LSIS best run). However, it produces complementary estimates to the other models proposed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Model 4: fast (unprecised) Canonical Correlation model</head><p>This model 4 is focused on global image descriptors and favors fast algorithms that can scale to thousands of images and annotation concepts. First, we represent each image using a text descriptor and a global visual descriptor. As visual descriptors we use global color, texture and shape features, similar to those presented in MPEG7. We use Canonical Correlation Analysis (CCA) to infer a latent space where the two representation are most correlated. Given the visual features of an unseen image, we fist project it to the CCA space and then we infer the linear combination of input concepts that is most correlated with it. We then back-project the result into the input space and we normalize it to [0, 1]. A value close to 1 means that the corresponding concept is likely to be found in the image, while a value close to zero suggests the contrary.</p><p>The tradeoff in our method is a slight loss of precision, but we make up for this in speed (we use less than 1 sec. for both training and prediction on an average 2.5 GHz PC). Moreover, adding new concepts to our method is straightforward and do not require training separate models for each concept. More details can be found in <ref type="bibr" coords="3,281.66,658.37,9.91,9.62" target="#b5">[6]</ref>.</p><p>3 Results and discussions on a carefulness index</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Global performances</head><p>We give the average Area Under the Curve (AUC) of the four models in figure <ref type="figure" coords="3,438.37,733.49,4.98,9.62">2</ref> for each topics, and their average in table 1 including comparison to the best runs of each team participating to the campaign. We see that AUC(Model 1) &gt; AUC(Model 2) &gt; AUC(Model 3) &gt; AUC(Model 4).</p><p>For comparison, three basic fusion models are computed. The first one, called early fusion, is a SVM trained on the merged features of the four models. The second is the simple arithmetic average of the outputs of the four models (late fusion). The last one, called 'best1' is the selection of the best model according to the training performances. The best fusion of the four models is the late fusion which gives an average AUC of 0.55, and occupies the 12th rank among the 19 teams in the official VCDT evaluation. Anyway, it is worst than the best model. We analyse in detail each model performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performances are correlated with a Carefulness Index</head><p>In order to analyse each model results, we depict in figure <ref type="figure" coords="4,343.34,581.57,4.98,9.62" target="#fig_1">3</ref> the histograms of the outputs of each model M1, ..., M4 on the test set. The shape of each histogram largely differs from one model to another. We then investigate a simple statistics that may indicates from this shape the quality of the model.</p><p>A detailed analyse of central and extreme values of these histograms reveal that for the best model, the center (bins 5 and 6) is bigger than the extremities (bins 1 and 10). We then compute a simple ratio:</p><formula xml:id="formula_1" coords="4,105.00,689.21,135.73,9.62">Q = h(center)/h(extremities),</formula><p>where h is the histogram here of 10 bins, so h(center) = h(5) + h(6) and h(extremities) = h(1) + h10). In figure <ref type="figure" coords="5,144.97,333.17,4.98,9.62">4</ref> we give the log(Q) values and the AUC results for each of the four models. We see that when Q decreases, AUC is also decreasing, moreover the ranks given by Q are similar to the AUC ranks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Depicting the results of very different models we enlighted a simple statistics on the raw model outputs that seems to be tied to its performances. The very different models we tested have different carefulness index. The experiments show that more careful is a model, more it AUC increases. This result shall be confirmed on other raw distributions of other model outputs. This kind of global shape statistics are interesting for scaled systems, where fast and unsupervised estimates of visual detector quality shall be possible. Further work will be conducted in this field in the AVEIR group. Figure <ref type="figure" coords="7,122.17,661.73,3.90,9.62">2</ref>: Area Under the Curve (AUC) evaluations for each Model and topic (number are the original ones given by the organizers). The topics are here sorted according to the STD between the four models (Right). The early, late and best1 fusions are depicted in the Left figure. We see that all these naive fusions are always worst than the best model. The best fusion is best1 for high STD between the four models, and the early fusion is nearly the worst for all topics.  Figure <ref type="figure" coords="8,120.85,718.37,3.90,9.62">4</ref>: The relation between Q index and the AUC for the four models. Log(Q) are the positive (blue) values, while the negative (red) are the log(AUC). We see that when the carefulness index Q decreases, AUC is also decreasing, moreover the ranks given by Q are similar to the AUC ranks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,90.00,237.29,423.17,9.62;2,132.31,109.30,338.02,103.25"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Dividing image on three horizontal segment to extract the histogram (HSV) of each part</figDesc><graphic coords="2,132.31,109.30,338.02,103.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="8,90.00,400.61,423.06,9.62;8,90.00,412.49,423.03,9.62;8,90.00,424.49,423.03,9.62;8,90.00,436.49,423.02,9.62;8,90.00,448.37,249.97,9.62;8,90.00,110.13,423.00,265.69"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Histograms of the similarities of the concepts, generated by each of the four individual models (M1 to M4), and for each topics. The 53 topics are represented by incremental colors from blue to red. These histograms give the raw behavior of each models. The experiment shows that their central and extreme values have a simple relation with the AUC of the model: AUC(Model 1) &gt; AUC(Model 2) &gt; AUC(Model 3) &gt; AUC(Model 4).</figDesc><graphic coords="8,90.00,110.13,423.00,265.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,210.17,689.41,3.35,5.58;8,277.51,689.41,3.35,5.58;8,344.90,689.41,3.35,5.58;8,412.24,689.41,3.35,5.58;8,169.56,684.33,6.87,5.58;8,164.54,653.98,11.90,5.58;8,173.08,623.63,3.35,5.58;8,168.05,593.27,8.38,5.58;8,173.08,562.92,3.35,5.58;8,168.05,532.57,8.38,5.58;8,173.08,502.21,3.35,5.58;8,168.05,471.91,8.38,5.58;8,303.19,696.60,19.44,5.58;8,156.09,617.99,5.58,4.69;8,156.09,604.91,5.58,11.40;8,156.09,586.48,5.58,16.75;8,156.09,583.13,5.58,1.68;8,156.09,576.76,5.58,4.69;8,156.09,563.35,5.58,11.74;8,156.09,536.88,5.58,24.79"><head></head><label></label><figDesc>log(Q) ; in neg. log(AUC)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,90.00,542.09,423.13,165.02"><head>the number of pairs of examples that are in the wrong order</head><label></label><figDesc></figDesc><table coords="2,90.00,562.12,423.13,144.99"><row><cell>ROCarea = 1 -</cell><cell>SwappedP aires #pos.#neg</cell></row><row><cell cols="2">. Here 1-ROCarea is used as the value of misclassification in loss function for each iteration. More</cell></row><row><cell>details on this model can be found in [2].</cell><cell></cell></row><row><cell cols="2">2.2 Model 2: RBG, SIFT, Gabor, and ontology SVMs</cell></row><row><cell cols="2">This model uses three sets of features: The first one is based on 512 bins RGB histogram of</cell></row><row><cell cols="2">the three horizontal stripes (same height of 1/3 of the image height, whole width of the image as</cell></row><row><cell cols="2">presented in previous section). Histograms are normalized and they result is a 1536 histogram. The</cell></row><row><cell cols="2">second set of features are SIFT, using software provided by K. van de Sande [SAND08]. The SIFT</cell></row><row><cell cols="2">features are extracted from regions selected according to Harris-Laplace feature points detection.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="4,90.00,190.37,423.02,304.70"><head>Table 1 :</head><label>1</label><figDesc>The official table results including the four models and the fusion AVEIR model, and the best runs of each submitting team of the ImageCLEF2009 Photo Annotation Task. The Rank is given only for the best run of each team.</figDesc><table coords="4,187.32,226.01,228.66,269.06"><row><cell>RANK</cell><cell>LAB</cell><cell>Average EER</cell><cell>AUC</cell></row><row><cell>1</cell><cell>ISIS</cell><cell>0.234476</cell><cell>0.838699</cell></row><row><cell>2</cell><cell>LEAR</cell><cell>0.249469</cell><cell>0.823105</cell></row><row><cell>3</cell><cell>FIR2</cell><cell>0.253566</cell><cell>0.817159</cell></row><row><cell>4</cell><cell>CVIUI2R</cell><cell>0.253296</cell><cell>0.813893</cell></row><row><cell>5</cell><cell>XRCE</cell><cell>0.267301</cell><cell>0.802704</cell></row><row><cell>6</cell><cell>bpacad</cell><cell>0.291718</cell><cell>0.773133</cell></row><row><cell>7</cell><cell>MMIS</cell><cell>0.312366</cell><cell>0.744231</cell></row><row><cell>8</cell><cell>LSIS</cell><cell>0.330819</cell><cell>0.720931</cell></row><row><cell>9</cell><cell>IAM</cell><cell>0.330401</cell><cell>0.714825</cell></row><row><cell>+ 10</cell><cell>Model 1</cell><cell>0.372169</cell><cell>0.673089</cell></row><row><cell>+ 11</cell><cell>Model 2</cell><cell>0.382840</cell><cell>0.644589</cell></row><row><cell>+</cell><cell>Model 3</cell><cell>0.430236</cell><cell>0.600746</cell></row><row><cell>* 12</cell><cell>AVEIR</cell><cell>0.440589</cell><cell>0.550866</cell></row><row><cell>-</cell><cell>Random</cell><cell>0.500280</cell><cell>0.499307</cell></row><row><cell>13</cell><cell>CEA</cell><cell>0.500495</cell><cell>0.469035</cell></row><row><cell>+ 14</cell><cell>Model 4</cell><cell>0.526302</cell><cell>0.459922</cell></row><row><cell>15</cell><cell>Wroclaw</cell><cell>0.446024</cell><cell>0.220957</cell></row><row><cell cols="2">16 KameyamaLab</cell><cell>0.452374</cell><cell>0.164048</cell></row><row><cell>17</cell><cell>UAIC</cell><cell>0.479700</cell><cell>0.105589</cell></row><row><cell>18</cell><cell>INAOE</cell><cell>0.484685</cell><cell>0.099306</cell></row><row><cell>19</cell><cell>apexlab</cell><cell>0.482693</cell><cell>0.070400</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="5,90.00,119.21,423.04,199.70"><head>Table 2 :</head><label>2</label><figDesc>Lists of the 10 topics having the lowest STD between the four model (LEFT), and the biggest STD (RIGHT) if the border estimates of a model are rare, that is if the model if 'careful' (most of the decision are close to the decision boundary). Thus we call this index the 'Carefulness Index'.</figDesc><table coords="5,105.00,143.45,337.86,163.46"><row><cell cols="2">Ten topics with lowest STD Ten best topics with highest STD</cell></row><row><cell>Fancy</cell><cell>Underexposed</cell></row><row><cell>Aesthetic-Impression</cell><cell>Beach-Holidays</cell></row><row><cell>Motion-Blur</cell><cell>Sunset-Sunrise</cell></row><row><cell>Partly-Blur</cell><cell>Night</cell></row><row><cell>Overall-Quality</cell><cell>Sea</cell></row><row><cell>No-Blur</cell><cell>Neutral-Illumination</cell></row><row><cell>Canvas</cell><cell>Clouds</cell></row><row><cell>Sunny</cell><cell>Landscape-Nature</cell></row><row><cell>Plants</cell><cell>Sky</cell></row><row><cell>Still-Life</cell><cell>Water</cell></row><row><cell>Q is high</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="2,105.24,740.49,177.07,7.35"><p>http://svmlight.joachims.org/svm_perf.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgment</head><p>This work was supported by <rs type="funder">French National Agency of Research</rs> (<rs type="grantNumber">ANR-06-MDCA-002</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HBaX49E">
					<idno type="grant-number">ANR-06-MDCA-002</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="5,105.48,593.09,407.71,9.62;5,105.48,605.09,320.56,9.62" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="5,168.48,593.09,286.08,9.62">A Support Vector Method for Multivariate Performance Measures</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,474.96,593.09,38.24,9.62;5,105.48,605.09,289.36,9.62">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.48,625.01,407.74,9.62;5,105.48,636.89,407.51,9.62;5,105.48,648.89,53.89,9.62" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="5,362.41,625.01,150.81,9.62;5,105.48,636.89,294.66,9.62">UPMC/LIP6 at ImageCLEFannotation 2009, Large Scale Visual Concept Detection and Annotation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fakeri-Tabrizi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tollari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,421.69,636.89,91.30,9.62;5,105.48,648.89,20.10,9.62">CLEF working notes 2009</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.48,668.81,407.52,9.62;5,105.48,680.81,310.36,9.62" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="5,279.64,668.81,233.37,9.62;5,105.48,680.81,49.95,9.62">Evaluation of Color Descriptors for Object and Scene Recognition</title>
		<author>
			<persName coords=""><forename type="first">Gevers</forename><forename type="middle">T</forename><surname>Vandesande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,175.69,680.81,91.86,9.62">Proceedings of CVPR</title>
		<meeting>CVPR<address><addrLine>Anchorage, Alaska, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.48,700.73,407.51,9.62;5,105.48,712.61,51.16,9.62" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="5,178.09,700.73,219.74,9.62">MRIM-LIG at ImageCLEF2009 photo annotation</title>
		<author>
			<persName coords=""><surname>Mulhem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,420.49,700.73,92.50,9.62;5,105.48,712.61,20.10,9.62">CLEF working notes 2009</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="5,105.48,732.53,407.61,9.62;5,105.48,744.53,396.40,9.62" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="5,271.98,732.53,241.12,9.62;5,105.48,744.53,231.19,9.62">LSIS Scaled Photo Annotations -Discriminant Features SVM vs Visual Dictionary based on Image Frequency</title>
		<author>
			<persName coords=""><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dumont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="5,357.03,744.53,113.80,9.62">CLEF working notes 2009</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.48,111.77,407.53,9.62;6,105.48,123.77,340.71,9.62" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="6,237.64,111.77,275.37,9.62;6,105.48,123.77,175.59,9.62">TELECOM ParisTech at ImageClef 2009: Large Scale Visual Concept Detection and Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ferecatu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sahbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,301.34,123.77,113.91,9.62">CLEF working notes 2009</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.48,143.69,407.51,9.62;6,105.48,155.69,407.66,9.62;6,105.48,167.57,96.51,9.62" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="6,264.41,143.69,248.59,9.62;6,105.48,155.69,97.70,9.62">Efficient Image Concept Indexing by Harmonic &amp; Arithmetic Profiles Entropy</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,212.30,155.69,231.45,9.62">IEEE International Conference on Image Processing</title>
		<meeting><address><addrLine>Cairo, Egypt</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-11">November 7-11 (2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="6,105.48,187.49,407.53,9.62;6,105.48,199.49,315.49,9.62" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="6,207.75,187.49,305.26,9.62;6,105.48,199.49,90.99,9.62">Overview of the CLEF 2009 Large Scale -Visual Concept Detection and Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dunker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="6,204.38,199.49,112.07,9.62">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
