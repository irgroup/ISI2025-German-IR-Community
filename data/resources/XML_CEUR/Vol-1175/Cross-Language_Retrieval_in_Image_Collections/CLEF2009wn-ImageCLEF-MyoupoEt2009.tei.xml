<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,104.63,148.86,393.73,15.15;1,224.57,170.78,153.87,15.15">Visual Reranking for Image Retrieval over the Wikipedia Corpus</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,144.40,203.31,82.75,10.48;1,227.15,201.70,1.41,6.99"><forename type="first">Débora</forename><surname>Myoupo</surname></persName>
							<email>debora.myoupo@gmail.comherve.le-borgne</email>
						</author>
						<author>
							<persName coords="1,259.04,203.31,81.19,10.48;1,340.23,201.70,1.88,6.99"><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
							<email>adrian.popescu@telecom.eu</email>
							<affiliation key="aff2">
								<orgName type="department">Telecom Bretagne</orgName>
								<orgName type="institution">Technople Brest-Iroise</orgName>
								<address>
									<postCode>29238</postCode>
									<settlement>Brest</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,372.12,203.31,86.49,10.48;1,458.60,201.70,1.41,6.99"><forename type="first">Hervé</forename><surname>Le Borgne</surname></persName>
						</author>
						<author>
							<persName coords="1,250.05,217.26,102.91,10.48;1,352.95,215.64,1.41,6.99"><forename type="first">Pierre-Alain</forename><surname>Moëllic</surname></persName>
							<email>pierre-alain.moellic@cea.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Multilingual Multimedia Knowledge Engineering Laboratory</orgName>
								<orgName type="institution">CEA LIST</orgName>
								<address>
									<addrLine>18, route du panorama</addrLine>
									<region>BP6</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">FONTENAY AUX ROSES</orgName>
								<address>
									<postCode>F-92265</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,104.63,148.86,393.73,15.15;1,224.57,170.78,153.87,15.15">Visual Reranking for Image Retrieval over the Wikipedia Corpus</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5D8369320A6CA274B4208BE8952991A3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image retrieval</term>
					<term>query expansion</term>
					<term>Wikipedia</term>
					<term>CBIR</term>
					<term>image reranking</term>
					<term>k-NN</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the approach we developed for the WikipediaMM task on 2009 <ref type="bibr" coords="1,114.91,353.24,9.96,8.74" target="#b3">[4]</ref>, which builds on our last year contribution. The main novelties are the refinement of textual query expansion procedure and the introduction of a k-NN based visual reranking procedure. Our main purpose was to test whether combining textual and content based retrieval improves over purely textual search and the results we report here confirm that combining modalities results in a significant improvement of results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The wikipediaMM task consists of retrieving images from a large-scale collection of heterogenous images <ref type="bibr" coords="1,123.80,514.06,10.52,8.74" target="#b4">[5,</ref><ref type="bibr" coords="1,138.72,514.06,7.01,8.74" target="#b3">4]</ref>. We propose a fully automatic approach which first retrieves images for a given query using textual query expansion and then reranks the results of the textual run using visual representations of the queries. Query expansion exploits the categorical structure of Wikipedia in order to find and rank relevant concepts from the collaborative encyclopedia. Terms in the query are compared to Wikipedia categories and articles which matched a maximum number of such terms are ranked best. We build visual models for each topic by retrieving Web images which match that topic from Google Image and Yahoo! Image and reranking them using a k-NN approach and an negative set of images which contains diversified images. Each Web image is compared to all other images retrieved with the same query and to the negative set and is well ranked if it is visually distant from the external class. Since Web images are usually noisy, we retain only the best ranked images for creating the visual model. Then, each image retrieved from the Wikipedia corpus is compared to the visual model and to the negative set in order to favor those which are close to the visual model. We index images using both local (bags of visual words) and global (texture-color) descriptors and report results for both types of indexers as well as for their fusion. There are two main common points to these steps. The first is that we use an intermediate conceptual level as a reference to describe the images. The second is that in both cases, we use external sources (from the web) to build these conceptual level.</p><p>Last year, we introduced a Wikipedia based query expansion procedure which takes advantage of the categorical structure of the encyclopedia <ref type="bibr" coords="2,284.00,145.80,13.17,8.74" target="#b1">[2]</ref> and we obtained encouraging results. This year we refine our approach by modifying the way query related concepts are ranked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data sources</head><p>The main resource we exploited was Wikipedia. Dumps of the encyclopaedia are regularly provided for a free use. We downloaded the April 2009 English dump, which contains over 2.6 million articles and is provided as a single file, in XML format. Next, we split the dump into individual articles in order to process the information faster. The information in Wikipedia spans over a large number of conceptual domains, with a high number of articles describing known people, places, entertainment, organisations animals and plants. Each article is placed in at least one category, a property that facilitates the extraction of IsA relations from the encyclopedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Conceptual neighbourhood building</head><p>Wikipedia images are accompanied by brief textual descriptions and query expansion is an appealing way to improve recall and, if performed in a judicious way, to also improve results precision. Topics are preprocessed in order to eliminate stop words and eliminate visual terms from a closed list (including image(s), photograph(s) etc.). Then, we lemmatize remaining terms and arrange them using their term frequency in Wikipedia in order to favor rare terms (which are more likely to be discriminant than frequent words). Then, we compare terms in the query to Wikipedia categories and retain all articles that match at least on term in the query. A limit of 5000 articles is imposed to speed up the processing. For instance, a query with orthodox icons with Jesus will have related concepts such as Christ the Redeemer and Theotokos but also Our Lady of Kazan or Manhattan.</p><p>It it obvious that these concepts are not equally relevant to the query and it is necessary to rank them so as to put the most pertinent first. Whereas Christ the Redeemer and Theotokos match all terms in orthodox icons with Jesus, Our Lady of Kazan matches only orthodox icons and Manhattan is found because orthodox appears in one of its categories (United States Places with Orthodox Jewish communities). We rank concepts by counting the number of terms from the initial query which appear in each article's categories and by favoring related concepts which match rare terms in the query. At this point, there are usually several concepts with the same score and in order to differentiate them we refine the ranking by answering the following questions:</p><p>• is the concept ambiguous?</p><p>• do all terms in the initial query appear in the first paragraph of the Wikipedia article?</p><p>• do all terms in the initial query appear in the article's text?</p><p>The refined list of related concepts will favor unambiguous elements and concepts which contain all terms in the query either in the first paragraph of the associated article (which is often a definition) or in the remaining text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Textual retrieval</head><p>Relevant images are found by launching queries with the initial terms and the expanded queries in the following order:</p><p>• all terms in the initial queries and related concepts</p><p>• the initial query • a part of the initial queries and related concepts</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• related concepts or a part of the initial query</head><p>The intuition behind this type of querying is that an image described by many terms (from the initial query and the related concepts) is more likely to be relevant than another image described by fewer terms. Weights are applied to the terms in the initial query and to related concepts and individual image scores are calculated by adding these scores.</p><p>Let R t be the list of images returned by the textual matching procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Visual reranking</head><p>The basic idea of our content based reranking procedure is that an image which is visual close to the visual model of a query is more likely to be a good answer than another image which is less similar to the visual model. To evaluate the similarities between the R t images and a topic, we need to obtain a low-level description of that particular topic. We create a visual model (a positive set of images R pos which depicts the concept) using Web images downloaded from Google Image and Yahoo! Image. A negative set R neg containing diversified images is manually constructed and used as an outlier for all topics in order to discard images which are not visually close to the topic's visual model. Then we use the visual coherence to evaluate the relevance of both sets (R all = R pos ∪ R neg ) and rerank them. Eventually, we merge this new ordered list R t using window merging or blocks merging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visual coherence</head><p>The visual coherence of an image is a metric mesuring the similarities beetween an image and a concept. This metric is computed using two sets of images : a positive set R pos containing N pos various relevant images of the concept and a negative set R neg of N neg non relevant images. These two sets compose the visual prototype of the concept. We compute the visual coherence score as a couple of scores :</p><p>False neighbours: For an image, we search for its N neigh closest neighbours in R pos as well as its N neigh closest neighbors in R neg . The visual similarity is calculated using a low-level descriptor (global or local) and the euclidean distance. The two lists of N neigh neighbours are then merged and the first part of the VC score is defined as the number of neighbours which belong to the negative set among the first N neigh images of this 2 × N neigh size list. In an ideal case, an image which perfectly represents the concepts will not have any pictures of R neg among its N neigh closest neighbours. Inversely, the more negative elements among the first N neigh neighbours are found, the less the image is a good representative of the concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distances:</head><p>The second score is the sum of the distances of their N sum closest neighbours in R pos .</p><p>A small value of this sum implies that the image is visually similar to the concept in the descriptor's space. The distances computation is based on the same descriptor as in the previous step.</p><p>This algorithm is strongly dependant to the descriptor and the metric.</p><p>To rank a set of images according to their likeness to a concept, we sort them using only the first score (which is a number between 0 and N neigh ). For two images having the same number of neighbors, we use the second score of the visual coherence to refine the ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Visual prototype conception</head><p>We hypothesize that a visual prototype of a topic can be extracted by querying a web-scale image search engine with that particular topic and by reranking the answers in order to reduce noise. N q images. We also download a set of N neg various images that we index to build the negative set R neg . The negative set contains diversified images which depict a large number of concepts such as mountain, dog, car, football or protest. Several content-based descriptors are then computed from the raw set of images. For the visual prototype to be effective, the retained images need to be as accurate as possible and it is necessary to filter out noisy results returned by the Web search engine. For this, we compute the VC on the raw set: for each image, the N q -1 other images of the set are temporarily considered as R pos and used with R neg to compute its VC score. We keep the top N pos results only, that are considered good enough to represent the concept and be part of its visual prototype. Since the visual coherence computation depends on the features, a visual prototype is thus also defined for each descriptor. It is finally composed of an ordered list of N pos positive images and a set of N neg negative images relatively to the concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Textual results reranking</head><p>Let R t be the list of images returned by the textual matching procedure. It can now be reordered using the visual prototypes using the same k-NN method used to build the prototype itself. In practice, we compute the signature of each image of R t and its distance to all the images of the visual prototype (both to the R pos and the R neg set) and compute their visual coherence. Since the visual prototype covers different aspects of the topic, we expect that a relevant result from the textual run to be related to some of the images composing the visual prototype. Since the visual prototype is dependant on the descriptor, we run experiments using:</p><p>Descriptor Reranking One descriptor is chosen. We use the visual prototype created with this descriptor to compute the visual coherence scores of the R t list. Then, the list is simply ranked according to the VC score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Late-fusion Reranking</head><p>The picture is reordered according to the sum of its ranks into the lists coming from several Descriptor Rerankings (i.e. using several descriptors) (cf. figure <ref type="figure" coords="4,486.65,642.24,3.87,8.74" target="#fig_0">1</ref>).</p><p>Early-fusion Reranking We fuse the visual prototypes which depict the same concept but use two different descriptors by crediting each image with the sum of its rank. Then, the new visual prototype is used to build each Descriptor Reranking. The Descriptors Rerankings are merged using the Late-fusion Reranking approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Textual and visual information merging</head><p>Textual results have weights which express their closeness to the topic. We describe two methods for merging textual and visual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Window merging</head><p>In this process, we consider two rankings : R t and a content-based ranking (Descriptor Reranking, Late-fusion Reranking, Early-fusion Reranking). A window of S window images slides on the textual order:</p><p>1. All the elements in the window are reranked according to the content based order.</p><p>2. The first one is selected as a new member of the window merging order.</p><p>3. Next, it is removed from the window.</p><p>4. Then, we add the next element of R t in the window.</p><p>We repeat the process until all the elements of R t are ranked (cf. figure2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Blocks merging</head><p>The R t ranking is divided into blocks and each block is reranked accordind to the same contentbased order. A block is composed of images which are similarly related to the topic. For instance described by all terms in the initial query and a related concept or described only by all the terms is the initial query. Given a query with orthodox icons with Jesus, two images which are annotated with all the terms in the query and with Christ the Redeemer, respectively Theotokos are in the same block, a third image annotated with orthodox icons with Jesus is in a second block and another image tagged with Manhattan (concept which is loosely related to the query via orthodox ) is in a third block. This is a kind of fusion since the order of the blocks keeps the R t ranking but, within each block, we use the content-based ranking (cf. figure <ref type="figure" coords="5,439.77,669.27,3.87,8.74" target="#fig_2">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental validation</head><p>Our method has been evaluated in the WikipediaMM task of the ImageCLEF 2009 campaign <ref type="bibr" coords="5,499.72,724.04,9.96,8.74" target="#b3">[4]</ref>.</p><p>In these experiments, we consider each query as a concept. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiments</head><p>For the visual prototypes creation, we query each query's heading using both Google and Yahoo! search engines to benefit from the difference between the index and improve our prototype variety. In our case, N q = 100: only the top 50 pictures of each search engine results are selected to illustrate the concept. After processing the raw set, the new positive set contains N pos = 50 images.</p><p>For practical reasons, all the concepts share the same R neg negative set even if it should be ideally redefined for each query. Moreover, it is not a trivial task to build a coherent noisy class without relation with the concept, thus, in practice, this set contains concepts we consider as generic. We fixed N neg to 300.</p><p>To compute the visual coherence score, N neigh and N sum are both fixed to 10 : the diversity of a concept can make two pictures semantically relevant but very far from each other according to a descriptor. The descriptors used are the color and texture based Local Edge Pattern (LEP, derived from <ref type="bibr" coords="6,148.87,567.91,10.79,8.74" target="#b0">[1]</ref>) and classical bags of features <ref type="bibr" coords="6,297.03,567.91,9.96,8.74" target="#b5">[6]</ref>. Hence, for each query, we obtain three visual prototypes : LEP visual prototype, Bag of feature visual prototype and the Early-fusion visual prototype which results from the mix of the two previous. Finally we get four content-based rerankings : the texture LEP reranking, the Bag of features reranking, the Late-fusion reranking and the Early-fusion reranking.</p><p>For the Window merging method, we use a window of S window = 10 elements. For the Blocks merging method, we divide R t according to textual score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>The results of our methods are reported in table 1. Note that we highlight some mistakes in the generation of the runs using the window merging approach bringing about differences with the results providing with the official runs (see footnotes). We report here the results of the correct runs, these runs do not outperform our best submissions but are still better than text only baseline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion</head><p>Globally, these results lead to highlight the following points:</p><p>• The use of the content-based reranking (i.e the second step) always improves the results compared to the text-based baseline. However, we noticed during our preliminary experiments that the results can decrease when the reranking is global (i.e without window or block merging approaches). We expected such results because we retain up to 1000 results for each topic and the test collection contains around 150000 images. Consequently, a large part of the 1000 answers are not relevant and it is useful to exploit the textual ranking.</p><p>• The block merging is more efficient than the sliding window approach. The difference between block and window is significant (around 3 points in MAP). This proves that splitting the textual results according to the relatedness of each image to the query makes sense and</p><p>• Bag-of-feature (local descriptors) give slightly better results than texture LEP descriptor (global features). When looking at the P@10 scores, we notice that the bag of features runs return the best results.</p><p>• Fusion reranking significantly improves MAP results (around 1 point) in comparison to simple descriptor reranking. This finding confirms that when dealing with diversified topics, it is interesting to merge local and global descriptors in order to obtain better results.</p><p>• The late-fusion reranking gives better results than the early-fusion one but the difference is not significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and future work</head><p>We presented methods for expanding textual queries and merging textual and visual information that are adapted for retrieving images in large datasets. Emphasis was put on designing techniques which can easily be scaled-up to larger image repositories. Textual results were somehow 1 official result with the wrongly generated run was map=0.1182 P@10=0.2267 P@20=0.1889 2 official result with the wrongly generated run was map=0.1178 P@10=0.2267 P@20=0.1900 3 official result with the wrongly generated run was map=0.0944 P@10=0.1889 P@20=0.1544 4 official result with the wrongly generated run was map=0.0921 P@10=0.1800 P@20=0.1633</p><p>disappointing and we are currently investigating why this happened and how they can be improved. One interesting work direction is to replace the refinement part of the concept ranking with techniques such as explicit semantic analysis <ref type="bibr" coords="8,314.55,135.93,10.52,8.74" target="#b7">[8]</ref> and it to the current performances of the system. We will also test the effects of the query expansion on larger datasets (such as the Web corpus) in order to compare it to standard search engine results. We are confident that results are likely to improve in terms of precision and diversity because the related concepts cover various aspects of a topic.</p><p>A second line of work concerns the visual processing in our system. We designed a k-NN based method for image reranking which is fast enough to be introduced in a search engine's pipeline given that the images are preindexed. We will explore the effects of introducing other content descriptors. Visual coherence was used at an image level but it is easy to compute a similar metric for topics and try to use it in order to determine automatically whether the visual reranking will be efficient (intuitively, it will work well for visually coherent queries). Visual coherence at a topic level can also be exploited in order to determine which descriptor to use for the reranking (it is probable for the best results to be obtained for the descriptor which provides the best separation between the positive and the negative sets).</p><p>Finally, it is interesting to explore how to adapt the size of the window (window merging) or the limits of the blocks (Block merging) to each list returned for each topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Aknowledgement</head><p>We thank the Direction Générale des Entreprises for funding us through the regional business cluster Systematic (project POPS) and Cap Digital (project Mediatic).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,181.89,297.14,239.21,8.74;4,150.00,108.86,303.00,163.20"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Scheme of the Late-fusion Reranking process</figDesc><graphic coords="4,150.00,108.86,303.00,163.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,193.13,289.39,216.75,8.74;5,90.09,108.86,422.81,165.41"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Scheme of the Window merging process</figDesc><graphic coords="5,90.09,108.86,422.81,165.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,198.87,374.24,205.26,8.74;6,203.96,108.86,195.08,240.30"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Scheme of the Block merging process</figDesc><graphic coords="6,203.96,108.86,195.08,240.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,58.28,110.43,485.94,233.84"><head>Table 1 :</head><label>1</label><figDesc>Results of the CEA LIST at ImageCLEF 2009 WikipediaMM Task. Notations: bofbags of (visual) features; tlep -texture -color descriptor; block -block fusion; window-window fusion.</figDesc><table coords="7,110.28,110.43,121.35,8.74"><row><cell>Run Reranking procedure</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,105.50,436.75,407.50,8.74;8,105.50,448.70,204.29,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="8,280.92,436.75,227.92,8.74">Image classification using color, texture and regions</title>
		<author>
			<persName coords=""><forename type="first">Ya-Chun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shu-Yuan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,105.50,448.70,106.71,8.74">Image Vision Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="759" to="776" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,468.63,407.50,8.74;8,105.50,480.58,407.50,8.74;8,105.50,492.54,71.98,8.74" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="8,353.54,468.63,159.45,8.74;8,105.50,480.58,216.39,8.74">Conceptual Image Retrieval over the Wikipedia Corpus Working notes for the CLEF</title>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Le Borgne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre-Alain</forename><surname>Moëllic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-09">2008. 17-19 September 2008</date>
			<pubPlace>Aarhus, Denmark</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Workshop</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,512.46,407.50,8.74;8,105.50,524.42,407.50,8.74;8,105.50,536.37,407.50,8.74;8,105.50,548.33,231.77,8.74" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="8,361.96,512.46,151.03,8.74;8,105.50,524.42,93.49,8.74">Conceptuel Image retrieval over a Large Scale Database</title>
		<author>
			<persName coords=""><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hervé</forename><surname>Le Borgne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre-Alain</forename><surname>Moëllic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,221.43,524.42,291.56,8.74;8,105.50,536.37,365.50,8.74">Proceedings of the 9th Workshop of the Cross-Language Evaluation Forum</title>
		<title level="s" coord="8,480.18,536.37,32.82,8.74;8,105.50,548.33,116.73,8.74">Lecture Notes in Computer Science</title>
		<meeting>the 9th Workshop of the Cross-Language Evaluation Forum</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5706</biblScope>
		</imprint>
	</monogr>
	<note>Evaluating Systems for Multilingual and Multimodal Information Access</note>
</biblStruct>

<biblStruct coords="8,105.50,568.25,407.51,8.74;8,105.50,580.21,205.96,8.74" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="8,267.47,568.25,241.00,8.74">Overview of the wikipediaMM task at ImageCLEF 2009</title>
		<author>
			<persName coords=""><forename type="first">Theodora</forename><surname>Tsikrika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jana</forename><surname>Kludas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,105.50,580.21,109.14,8.74">CLEF workng notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,105.50,600.13,407.51,8.74;8,105.50,612.09,407.50,8.74;8,105.50,624.04,407.50,8.74;8,105.50,636.00,163.89,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="8,267.47,600.13,241.00,8.74">Overview of the wikipediaMM task at ImageCLEF 2008</title>
		<author>
			<persName coords=""><forename type="first">Theodora</forename><surname>Tsikrika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jana</forename><surname>Kludas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="8,117.08,612.09,395.92,8.74;8,105.50,624.04,241.67,8.74">Proceedings of the 9th Workshop of the Cross-Language Evaluation Forum</title>
		<title level="s" coord="8,355.72,624.04,153.00,8.74">Lecture Notes in Computer Science</title>
		<meeting>the 9th Workshop of the Cross-Language Evaluation Forum</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5706</biblScope>
			<biblScope unit="page" from="539" to="550" />
		</imprint>
	</monogr>
	<note>Evaluating Systems for Multilingual and Multimodal Information Access</note>
</biblStruct>

<biblStruct coords="8,105.50,655.92,407.51,8.74;8,105.50,667.88,407.50,8.74;8,105.50,679.84,117.12,8.74" xml:id="b5">
	<monogr>
		<title level="m" type="main" coord="8,379.65,655.92,133.36,8.74;8,105.50,667.88,280.68,8.74">Local features and kernels for classifcation of texture and object categories: An in-depth study</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno>RR-5737</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>INRIA Rhône-Alpes</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="8,105.50,699.76,407.50,8.74;8,105.50,711.72,407.50,8.74;8,105.50,723.67,127.03,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="8,394.91,699.76,118.08,8.74;8,105.50,711.72,170.14,8.74">Jain Content-Based Image Retrieval at the End of the Early Years</title>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="8,283.33,711.72,207.39,8.74">IEEE Trans. on Patt. Anal. and Machine Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="467" to="477" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,105.50,112.02,407.50,8.74;9,105.50,123.98,305.50,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="9,264.93,112.02,248.08,8.74;9,105.50,123.98,109.25,8.74">Computing semantic relatedness using Wikipedia-based explicit semantic analysis</title>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,222.17,123.98,91.44,8.74">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1606" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
