<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,95.39,146.21,412.21,18.08">TIA-INAOE&apos;s Participation at ImageCLEF 2009</title>
				<funder ref="#_J3fefrV">
					<orgName type="full">CONACyT</orgName>
				</funder>
				<funder ref="#_ANc9rpM">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,131.52,181.27,86.17,10.46"><forename type="first">Hugo</forename><forename type="middle">Jair</forename><surname>Escalante</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computational Sciences National Institute of Astrophysics</orgName>
								<orgName type="department" key="dep2">Optics and Electronics Luis Enrique</orgName>
								<orgName type="laboratory">Research Group on Machine Learning for Image Processing and Information Retrieval</orgName>
								<address>
									<addrLine>Erro No. 1</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,225.41,181.27,77.29,10.46"><forename type="first">Jesús</forename><forename type="middle">A</forename><surname>González</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computational Sciences National Institute of Astrophysics</orgName>
								<orgName type="department" key="dep2">Optics and Electronics Luis Enrique</orgName>
								<orgName type="laboratory">Research Group on Machine Learning for Image Processing and Information Retrieval</orgName>
								<address>
									<addrLine>Erro No. 1</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,310.69,181.27,88.76,10.46"><forename type="first">Carlos</forename><forename type="middle">A</forename><surname>Hernández</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computational Sciences National Institute of Astrophysics</orgName>
								<orgName type="department" key="dep2">Optics and Electronics Luis Enrique</orgName>
								<orgName type="laboratory">Research Group on Machine Learning for Image Processing and Information Retrieval</orgName>
								<address>
									<addrLine>Erro No. 1</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,407.66,181.27,59.04,10.46"><forename type="first">Aurelio</forename><surname>López</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computational Sciences National Institute of Astrophysics</orgName>
								<orgName type="department" key="dep2">Optics and Electronics Luis Enrique</orgName>
								<orgName type="laboratory">Research Group on Machine Learning for Image Processing and Information Retrieval</orgName>
								<address>
									<addrLine>Erro No. 1</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,133.99,195.22,64.89,10.46"><forename type="first">Manuel</forename><surname>Montes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computational Sciences National Institute of Astrophysics</orgName>
								<orgName type="department" key="dep2">Optics and Electronics Luis Enrique</orgName>
								<orgName type="laboratory">Research Group on Machine Learning for Image Processing and Information Retrieval</orgName>
								<address>
									<addrLine>Erro No. 1</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,207.10,195.22,72.85,10.46"><forename type="first">Eduardo</forename><surname>Morales</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computational Sciences National Institute of Astrophysics</orgName>
								<orgName type="department" key="dep2">Optics and Electronics Luis Enrique</orgName>
								<orgName type="laboratory">Research Group on Machine Learning for Image Processing and Information Retrieval</orgName>
								<address>
									<addrLine>Erro No. 1</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,287.88,195.22,42.82,10.46"><forename type="first">Elias</forename><surname>Ruiz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computational Sciences National Institute of Astrophysics</orgName>
								<orgName type="department" key="dep2">Optics and Electronics Luis Enrique</orgName>
								<orgName type="laboratory">Research Group on Machine Learning for Image Processing and Information Retrieval</orgName>
								<address>
									<addrLine>Erro No. 1</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,338.58,195.22,57.28,10.46"><forename type="first">Luis</forename><forename type="middle">E</forename><surname>Sucar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computational Sciences National Institute of Astrophysics</orgName>
								<orgName type="department" key="dep2">Optics and Electronics Luis Enrique</orgName>
								<orgName type="laboratory">Research Group on Machine Learning for Image Processing and Information Retrieval</orgName>
								<address>
									<addrLine>Erro No. 1</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,403.70,195.22,65.32,10.46"><forename type="first">Luis</forename><surname>Villaseñor</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computational Sciences National Institute of Astrophysics</orgName>
								<orgName type="department" key="dep2">Optics and Electronics Luis Enrique</orgName>
								<orgName type="laboratory">Research Group on Machine Learning for Image Processing and Information Retrieval</orgName>
								<address>
									<addrLine>Erro No. 1</addrLine>
									<postCode>72840</postCode>
									<settlement>Puebla</settlement>
									<country key="MX">México</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,95.39,146.21,412.21,18.08">TIA-INAOE&apos;s Participation at ImageCLEF 2009</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">25E71D408A084E6FAE52B1EBFD0B607B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 [Information Systems and Applications]: Information Search and Retrieval-Retrieval models</term>
					<term>Selection process</term>
					<term>Information Filtering Performance, Experimentation Multimedia image retrieval</term>
					<term>Image annotation</term>
					<term>Result diversification</term>
					<term>Semantic cohesion modeling</term>
					<term>Document re-ranking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This working note describes the participation of TIA-INAOE in the Photographic Retrieval and the Large Scale Image Annotation tracks at ImageCLEF2009. We developed specific methods for each track with the goal of exploiting the information available while maximizing annotation and retrieval performance. On the one hand, for the retrieval track, we proposed a post processing technique for re-ranking documents according to different diversity categories. With this formulation we considered both visual and textual features and we incorporated information of the different categories by which topics are clustered. Results obtained with this technique suggest it is a promising method for result diversification. However, we still need to deal with issues that affect the retrieval performance. On the other hand, for the annotation task, we adopted a simple annotation technique based on KNN classification. Only global features were considered under this formulation. The output of the KNN method was then refined by means of an energy-based model that attempts to maximize the semantic cohesion among labels assigned to each image. We considered information of the annotation hierarchy to constraint certain labeling configurations. Results obtained with this technique give evidence that the KNN approach is an effective annotation technique despite being rather simple. The refinement strategy resulted useful for improving the labeling performance of an ineffective annotation method; although we could not improve the labeling performance of a strong baseline. Summarizing, the results obtained at ImageCLEF2009 are encouraging and motivate further research in several directions that we are currently exploring.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This working note describes the participation of TIA-INAOE in the Photographic Retrieval and the Large Scale Image Annotation tracks at ImageCLEF2009. A total of 10 runs were submitted that comprise different settings of our proposed methods for image annotation and retrieval. The proposed methods aim exploiting the information available while maximizing annotation and retrieval performance.</p><p>On the one hand, for the retrieval task, we adopted a two stages retrieval process. In the first stage, an initial image search is performed by using only textual information to obtain potentially relevant documents. In a second stage the candidate documents are re-ranked by considering the different diversity categories (clusters) as provided by the organizers. For re-ranking we considered both visual and textual information. Results with this approach are mixed: whereas the reranking technique resulted helpful for diversifying retrieval results, the best retrieval performance (MAP) was obtained with the baseline retrieval method. This result suggests that textual retrieval methods are better suited for this collection.</p><p>On the other hand, for the annotation task, we adopted a three stages methodology. In a first stage, candidate labels for test images are selected with a KNN approach, which consists of obtaining the labels assigned to the K-nearest neighbors (in the training set) of the test image. In the second stage, an energy-based model is used to select, among the candidate labels, the disjoint labels for the image, see <ref type="bibr" coords="2,193.72,335.60,9.96,10.46" target="#b3">[4]</ref>; this model uses the output of the KNN method and label co-occurrence statistics. In a third stage, we consider the labels selected in the second stage to obtain the optional labels for the test image. Applying this method took about 0.25 seconds per image, which makes it attractive for large scale annotation. Annotation results are contradictory: we obtained acceptable performance when considering the evaluation measure based on the annotation hierarchy <ref type="bibr" coords="2,488.07,383.42,10.52,10.46" target="#b4">[5,</ref><ref type="bibr" coords="2,502.49,383.42,7.01,10.46" target="#b3">4]</ref>; however, the performance of our methods is rather limited in terms of EER and area under the ROC curve, we analyze these results below.</p><p>The rest of this document is organized as follows. In the next section we describe the approach we adopted for the photographic retrieval task and the results obtained with this technique. Next, in Section 3, we present the annotation method we proposed as well as official results of this method. Finally, in Section 4, we describe the conclusions derived from this work and we outline future work directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Photographic retrieval</head><p>We proposed a two stage approach for the photographic retrieval task at ImageCLEF2009; our methodology is depicted in Figure <ref type="figure" coords="2,242.55,533.82,3.87,10.46" target="#fig_0">1</ref>. In the first stage a set of m potentially relevant documents is retrieved by using a text-based image retrieval technique. In the second stage the m documents are re-ranked by taking into account the initial score assigned to documents and the similarity of candidate documents to the diversity clusters provided by the organizers. We proposed this formulation for two main reasons: 1) we wanted to take advantage of the topic clusters for diversifying retrieval results and 2) we wanted to make more efficient the search process. The latter was accomplished because the initial search can be performed efficiently (over the 500, 000 documents) and once the set of potentially relevant documents is reduced to a subset of documents we can compare images in acceptable time and we can apply more complex strategies over this reduced set of documents. The rest of this section describes in detail our method. Further details on the task and on the collection are described by Lestari et al. <ref type="bibr" coords="2,339.46,653.38,9.96,10.46" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Feature extraction</head><p>The proposed method considers both textual and visual information for representing documents. As textual features we consider a tf-idf weighting scheme over words for representing documents, see the next section. As visual features we use color histograms on both RGB (256 bins) and HSI (128 bins per channel), for a total of 640 visual attributes. We also performed preliminary experiments with other visual features, including: edge histograms, color histograms in CIE-Lab, texture features and local descriptors; however, the RGB and HSI color histograms resulted more effective (according to an empirical evaluation we conducted) for retrieving images under the Euclidean distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Initial retrieval</head><p>For the initial retrieval we considered the vector space model (VSM) under tf-idf weighting for representing documents <ref type="bibr" coords="3,195.48,446.85,9.96,10.46" target="#b0">[1]</ref>; we used the cosine similarity for comparing documents. In particular, we used the TMG Matlab R toolbox for indexing and retrieval <ref type="bibr" coords="3,375.00,458.80,9.96,10.46" target="#b8">[9]</ref>. Because of the size of the collection we indexed the collection by batches of 20, 000 documents.</p><p>We considered textual information only, because computing the Euclidean distance, between query images and the 500, 000 images that compose the collection, would be very computationally expensive. Thus, the re-ranking approach was also used for efficiency reasons (query images and a reduced set of m-images can be compared in acceptable time).</p><p>For querying we used all of the textual information available in topics (i.e. title, cluster titles and cluster descriptions); this is motivated because at this stage we wanted to retrieve documents that were related to the query as a whole, so that in the next stage the search can be refined. We ranked the documents by their similarity to the query and we keep the m = 1, 000 top ranked documents for the second stage; our baseline run consists of returning these 1, 000 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Re-ranking based on multimedia features</head><p>In the second stage we re-ranked the m-documents obtained by the initial retrieval method. For each topic category (cluster), j, we assigned a score to each of the m-documents, d i∈{1,...,m} , as follows:</p><formula xml:id="formula_0" coords="3,192.57,658.92,320.42,13.90">sc X f inal (d X i , q X j ) = λ × sc initial (d i ) + ×S X (d X i , q X j )<label>(1)</label></formula><p>where sc initial is the similarity score obtained from the initial retrieval stage and S X (d X i , q X ) is an estimate of the similarity between the i th -document and the j th -sub-query under modality X. λ is a scalar weighting the contribution of the first term. A sub-query q X j is the part of the topic corresponding to the j th diversity cluster, where j ∈ {1, . . . , C} and C the number of categories associated with the topic. The superscript X indicates which information modality is used: X = T means that textual information was considered (e.g. cluster title), X = V indicates that visual information was used (i.e. cluster image) and X = M means that both textual and visual information wee considered. When X = T , we used the cosine similarity as S T ; when X = V we used the (normalized) inverse of the Euclidean distance as S V ; when X = M we used</p><formula xml:id="formula_1" coords="4,90.00,145.33,131.02,13.03">S M = w m 1 × S T + w m 2 × S V</formula><p>, where the scalars w m 1 and w m 2 weight the contribution of each modality.</p><p>For each category j, the score assigned to the m documents was sorted in descending order; thus for each category we had a different ranking for the m-documents. The C-rankings were combined (by means of round robin) to generate a final ranking for the m-documents. The top 1, 000 documents according to the final ranking were submitted for evaluation. For the topics that do not have textual information (i.e. topics 25 to 50) we used the provided query images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Submitted runs and results</head><p>Table <ref type="table" coords="4,117.37,264.41,4.98,10.46">1</ref> summarizes the five runs we submitted for the photographic retrieval task; while Table <ref type="table" coords="4,508.02,264.41,4.98,10.46" target="#tab_0">2</ref> shows the results obtained by these runs. We report the (average across topics of) mean average precision (MAP), cluster recall at 20 documents (C20), R-precision (RP), precision at 20 documents and the ratio of relevant retrieved / relevant documents (RR/R). The parameters involved in our method were set empirically, analyzing the results of each configuration, Table <ref type="table" coords="4,462.63,312.23,4.98,10.46" target="#tab_0">2</ref> shows the parameter settings used for each run.</p><formula xml:id="formula_2" coords="4,105.24,346.85,274.74,50.61">ID Par. Description R-1 - Textual retrieval method, see Section 2.2. R-2 w m 1 = 4; w m 2 = 1</formula><p>The m-documents obtained by the initial retrieval method are reranked according to the score in Equation ( <ref type="formula" coords="4,385.44,385.50,3.87,10.46" target="#formula_0">1</ref>); although the topic was not separated into categories under this formulation. R-3 λ = 0.25 Re-ranking technique with X = V , see Section 2.3. R-4 λ = 0.25 Re-ranking technique with X = T , see Section 2.3. R-5 λ = 0.25;</p><formula xml:id="formula_3" coords="4,142.10,446.47,44.83,23.92">w m1 = 4; w m 2 = 1</formula><p>Re-ranking technique with X = M , see Section 2.3.</p><p>Table <ref type="table" coords="4,167.64,480.29,3.87,10.46">1</ref>: Runs submitted by our team to the photographic retrieval track. The obtained results are mixed. The best retrieval performance, in terms of MAP and RP, was obtained with the baseline (i.e. a textual retrieval technique); P20 was higher with the R-5 configuration. Nevertheless, the difference in retrieval performance between the baseline and the other runs was of less than 0.03.</p><formula xml:id="formula_4" coords="4,157.04,524.87,255.67,24.80">ID MAP C-20 RP P20 RR/R R-</formula><p>In terms of results diversification (i.e. C20), it is observed an improvement over the baseline for all of the runs (rows 3-6 in Table <ref type="table" coords="4,252.72,702.20,3.87,10.46" target="#tab_0">2</ref>). The largest improvement in C20 was obtained with the methods that considered visual information (i.e. R-3 and R-5). The run R-3, which used only visual information for re-ranking documents, resulted particularly helpful for diversifying retrieval results (this run was ranked 67 out of 84). These results suggest that the re-ranking technique can be helpful for diversifying results and that using different modalities for the initial search and the re-ranking technique results in better performance. Note that the performance of the reranking technique depends on the initial retrieval, thus we expect better diversification of results when better retrieval methods are considered for the initial search. We are studying this research direction. Retrieval performance is slightly affected by applying the re-ranking method. However, the performance of the initial search method was rather limited: this method was ranked 47 out of the 85 submitted runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Discussion</head><p>The results obtained by the TIA-INAOE team in the photographic retrieval task at Image-CLEF2009 may seem discouraging in a first instance. However, interesting findings can be drawn from our participation: the proposed re-ranking technique resulted helpful for result diversification, although it slightly affects the retrieval performance; better diversification performance was obtained when the re-ranking was based on visual information only; better diversification performance is expected if a better search engine is used for the initial retrieval; as whole, the proposed formulation can be helpful for efficient multimedia retrieval in large scale image collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Large scale image annotation</head><p>We proposed a three steps methodology for facing the annotation task at ImageCLEF2009. For each test image, we identified a subset of candidate labels by comparing the test image to the training ones. Then, we selected the disjoint labels for the image by means of an energy-based model. Next, optional labels were assigned by taking into account co-occurrence statistics.</p><p>The method described in this section is based in the assumption that similar images have associated similar labels. Thus, for each test image we considered the labels assigned to the K-most similar training images; then, we applied different strategies for selecting disjoint and optional labels for a test image. The benefits of adopting this methodology are annotation efficiency, implementation simplicity and the competitive performance that can be obtained with the proposed formulation. The rest of this section describes our methodology and the obtained results. Further details on the task and on the collection are described by Nowak et al. <ref type="bibr" coords="5,401.28,474.60,9.96,10.46" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature extraction</head><p>We used global features to represent images. In particular, we considered an RGB color histogram (256 bins), texture features extracted from the co-occurrence matrix (88 values), an edge histogram (360 bins) and an HSI color histogram with 128 bins per channel for a total of 1, 088 attributes. Each image was represented by its vector of features; thus, hereafter, we will refer as images to both the images themselves and the vectors of features representing the images. For comparing images we used a weighted Euclidean distance, where a different weight is used for each subset of features (i.e. RGB, texture, edge, HSI). The weights were set empirically by trial and error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">KNN for image annotation</head><p>The first step in our methodology (depicted in Figure <ref type="figure" coords="5,324.64,638.88,4.43,10.46">2</ref>) is to obtain the k-most similar trainingimages to each test image I T , we denote this set of images by I T N N . We are interested in the labels associated with the images in I T N N , we denote the corresponding set of labels by L T N N and we call it the set of candidate labels for I T . We call the positions of label l T i to the set of positions, in the sorted set I T N N , occupied by images that have l T i as annotation. Then, we assigned a score to each label l T i ∈ L T N N as follows:</p><formula xml:id="formula_5" coords="5,153.59,719.01,359.41,13.89">Ψ(l T i ) = α r × w r (l T i , L T N N ) + α a × w a (l T i , L T N N ) + α r × w s (l T i , L T N N )<label>(2)</label></formula><p>where w r (l i , L T N N ) is the normalized frequency of occurrence of label l T i in L T N N ; w a (l i , L T N N ) is the average of positions of label l T i ; w s (l i , L T N N ) is the standard deviation of the positions of label l T i ; α r , α a and α s are scalars that weight the contribution of each term into the final score. The KNN approach to image annotation. The k-most similar images (middle) to the test image (left) were obtained; then, a weight was assigned to each label according to its repetition in the set of images and to the position of the images in which the label appeared. The top-t can labels be used to annotate the image.</p><p>The scores assigned to the candidate labels were used by the model to be described below. Alternatively, a rather simple labeling approach consists in sorting the candidate labels, in descending order of Ψ(l i ) (Figure <ref type="figure" coords="6,226.87,424.02,3.87,10.46">2</ref>, right), and using the top t-labels for annotating the test image I T . We call this setting our baseline<ref type="foot" coords="6,244.67,434.91,3.97,7.32" target="#foot_0">1</ref> run. This annotation approach is based in the work of Makadia et al., where the labeling problem is faced as one of retrieval <ref type="bibr" coords="6,379.99,447.93,9.96,10.46" target="#b2">[3]</ref>. Nevertheless, in this work we are using a different scheme for weighting labels, and we introduce a novel labeling refinement method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Alternative re-ranking</head><p>We also considered an alternative re-ranking approach that aims refining the ranking of the candidate images (as obtained with global attributes) by considering local features. Under this technique, we obtained the k 0 -most similar images (k 0 &gt; k) to each test image, using the features described above and the Euclidean distance. Next, we re-ranked this k 0 -images by using a naïve Bayesian classifier (NBC). The NBC evaluates the pixel similitude between a patch of the image in the test set, with several patches of the k 0 images, using the Euclidean distance as well. Patches were obtained from little regions in the image that were passed through Gabor and max filters. This is according to a simplified Bayesian approximation of a bio inspired model of the visual cortex <ref type="bibr" coords="6,107.08,611.78,9.96,10.46" target="#b6">[7]</ref>. As above, the top-k images (in the new ranking) were considered the nearest neighbors I T N N of the test image I T . Despite a different approach was used for ranking images, we used the score from Equation (2) to rank the labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Labeling refinement</head><p>Once we identified a set of candidate labels for a test image, as described in Section 3.2, we applied a labeling refinement method for selecting the disjoint labels for the image. Intuitively, we wanted to select, from the set of candidate labels, the best combination of disjoint labels, using co-occurrence statistics calculated from the training set.</p><p>We defined an energy-based model for the disjoint categories (i.e. Seasons, Place, TimeOfDay, Illumination, Blurring and Persons) using a variable a j per category, with j ∈ {1, . . . , 6}. Each random variable can take values from its corresponding set of possible labels (e.g. the variable corresponding to the category TimeOfDay, can take the values: Day, Night or No-Visual-Time); we denote the assignment of label l x to variable a j with a x j , thus a x j can be considered a label itself. Additionally, we restricted the values that each variable can take, by considering as possible values only to those labels that appear in the set of candidate labels (i.e. L T N N ). Figure <ref type="figure" coords="7,474.33,182.26,4.98,10.46" target="#fig_1">3</ref> depicts the modeling process for a particular test image.  <ref type="formula" coords="7,259.66,452.37,4.24,10.46" target="#formula_5">2</ref>) is normalized and used as input for the model. We considered a fully connected model.</p><p>The goal of the model is to select the configuration of labels A (i.e. a label assignment per category) that maximizes the cohesion among the labels assigned to the image. Accordingly, we assigned an energy value to each configuration of labels as follows:</p><formula xml:id="formula_6" coords="7,187.68,533.25,325.32,25.77">E(A) = - a j ∈A δ × Ψ(a x j ) + × a j ∈A a h ∈η a j ρ(a x j , a y h )<label>(3)</label></formula><p>Where Ψ(a x j ) as in Equation ( <ref type="formula" coords="7,235.84,570.27,4.24,10.46" target="#formula_5">2</ref>) and ρ(a x j , a y h ) is a factor that weights the association between labels l x and l y , assigned to categories a j and a h , respectively; η aj is the set of neighbors<ref type="foot" coords="7,477.55,582.26,3.97,7.32" target="#foot_1">2</ref> , under the model, of category a j , as we are using a fully complete graph the set of neighbors for a j is η a j = a p ∈ A : p = j. δ weights the contribution of the initial ranking to the energy of the configuration. We used co-occurrence statistics to estimate the association between labels. Specifically, we estimate ρ(a x j , a y h ) as follows:</p><formula xml:id="formula_7" coords="7,250.72,653.54,262.28,24.93">ρ(a x j , a y h ) = #(l x , l y ) #(l x )#(l y )<label>(4)</label></formula><p>where #(l x ) is the number of images in the training set in which label l x occurs and #(l x , l y ) is the number of images in which both l x and l y co-occur. For specific labels l x and l y , the higher ρ(a x j , a y h ) the more both labels are associated. Note that ρ(a x j , a y h ) can be calculated for any pair of labels (disjoint and optional); thus we use this association information in the next section for selecting optional labels as well. Equation (3) assigns low energy values to correct configurations and large values to incorrect ones. Therefore, the problem of selecting disjoint labels for a test image is reduced to that of finding the configuration of labels that minimizes Equation (3); for this work we used iterated conditioned modes (ICM) for this task <ref type="bibr" coords="8,379.83,146.40,9.96,10.46" target="#b7">[8]</ref>.</p><p>The energy-based model returns labels for each disjoint category. Intuitively, the model selects the combination of labels that maximizes their semantic cohesion. This method is based on the method proposed in <ref type="bibr" coords="8,183.68,182.26,10.52,10.46" target="#b1">[2]</ref> for region-labeling; in this paper we extend such a model to work for image-level annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Annotating images</head><p>Once we selected the disjoint labels we assigned optional labels to each test image as follows. We assigned a score to each candidate label l T i (that does not belong to any disjoint category), identified in Section 3.2, as follows:</p><formula xml:id="formula_8" coords="8,238.38,284.72,274.62,31.54">ζ(l T i ) = Ψ(l T j ) × 6 j=1 ρ(l T i , l dj )<label>(5)</label></formula><p>where Ψ(l T j ) and ρ(l T i , l dj ) are defined as above and l dj is the label assigned to the j th disjoint category. We ranked labels according to ζ(l T i ) and used the top-n labels for labeling the test image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Postprocessing</head><p>For generating the final annotation for a test image we applied the following postprocessing. First, regarding the number of labels, we assigned the top 4 optional labels to each test image, as 4 is the average number of optional labels that were used for annotating the training images. Second, when a leaf-label was chosen as optional label for an image, we also included its parent label, as appear in the annotation hierarchy defined by the organizers. Thus, for example, if the label Lake was considered as optional label for the image, we also included the label Water. Of course, this is only applicable to optional labels that appear in the hierarchy as leafs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Submitted runs and results</head><p>Table <ref type="table" coords="8,116.68,517.01,4.98,10.46" target="#tab_2">3</ref> summarizes the five runs we submitted for the large scale annotation task; whereas Table <ref type="table" coords="8,508.02,517.01,4.98,10.46" target="#tab_3">4</ref> shows the results obtained by such runs. We show the following performance measures: the hierarchical measure described in <ref type="bibr" coords="8,220.68,540.93,9.96,10.46" target="#b4">[5]</ref>, two variants are available: H-A is the hierarchical performance with annotator agreement, whereas H-WA is the performance without annotator agreement, the higher the values of H-A and H-WA the better the annotation performance, see <ref type="bibr" coords="8,443.87,564.83,10.52,10.46" target="#b3">[4,</ref><ref type="bibr" coords="8,457.28,564.83,7.75,10.46" target="#b4">5]</ref> for further details. Also, the average of equal error rate (EER) and the area under the ROC curve (AUC) were considered for evaluation. The parameters involved in our method were set by using cross validation using the training set of images.</p><p>From Table <ref type="table" coords="8,158.78,612.66,4.98,10.46" target="#tab_3">4</ref> we can see that the baseline method is a rather strong baseline, confirming the results reported by Makadia et al. <ref type="bibr" coords="8,248.32,624.61,9.96,10.46" target="#b2">[3]</ref>. The performance of the baseline was not improved by applying the energy-based model. This can be due to the fact that we did not use a good set of parameters for the model. In cross validation experiments we obtained better performance in both EER and AUC, thus it seems we overfitted the data.</p><p>The worst performance, in terms of H-A and H-WA, was obtained when local information was used for ranking labels according to the KNN approach. Hence these local features were not helpful for re-ranking. An interesting result, however, is that when the energy-based model was applied with the labels ranked according to local features, the energy-based model was able to improve the performance of the former significantly (compare the performances of runs A-2 and A-4). This result suggest the energy-based model can be helpful when the initial labeling is not good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ID Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A-1</head><p>KNN; the score in Equation ( <ref type="formula" coords="9,275.42,123.69,4.24,10.46" target="#formula_5">2</ref>) is used for assigning labels. A-2 KNN-RR; we use the alternative re-ranking with local features (see Section 3.2.1) and the score in Equation ( <ref type="formula" coords="9,266.01,148.00,4.24,10.46" target="#formula_5">2</ref>) for assigning labels. A-3 KNN + EBM; candidate labels selected as in A-1, the energy-based model is used to select the disjoint labels and optional labels are selected as described in Sections 3.3, 3.4 and 3.5 A-4 KNN-RR + EBM; candidate labels selected as in A-2, the energy-based model is used to select the disjoint labels and optional labels are selected as described in Sections 3.3, 3.4 and 3.5 A-5 KNN-RRW + EBM; same as A-4, but we use a larger δ value. It is interesting to note that whereas the performance of our runs in H-A and H-WA was, to some extent, satisfactory, the performance in terms of EER and AUC was rather limited. Our best run (A-1) was ranked 26 out of 74 submitted runs in terms of H-A and H-WA. However, the same run was ranked 59 and 66 out 74 in terms of EER and AUC, respectively. This result suggest that the method can label images as whole satisfactorily, although its per-label performance is limited; which is not surprising as we have not developed visual concept detectors per class. Note that the main goal of assigning labels to images is to support annotation based image retrieval methods, which use labels assigned to the images as a whole. Thus, it seems that our method could support effectively this form of image retrieval, we will study this aspect as future work, currently we are conducting an more in-deep analysis of the results.</p><p>Finally, the processing time 3 of our methods is quite acceptable, this time could be further reduced if we use software that is less computationally expensive (we used Matlab R for all of our experiments) and if we optimize our code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Discussion</head><p>The results obtained by the TIA-INAOE team are encouraging. The KNN approach to image labeling proved to be a very effective method for image annotation, regardless of its simplicity and generality. Despite the proposed energy-based model did not improve the performance of the KNN method, it was able to improve significantly the performance of the KNN-RR method. Thus suggesting the energy-based model can be helpful when the initial method is not very effective; this is a desired behavior of the model. Note that the energy-based model is still under development and that we have fixed the number of labels that are assigned to an image, also the parameter selection process can be improved. In general terms, our annotation methodology offers a good tradeoff in terms of annotation performance (in H-A and H-WA) and processing time. We would like to emphasize that the proposed approach for labeling refinement is not restricted to our KNN annotation method. It can be applied as a postprocessing step with any annotation method, provided that the labels can be ranked, hence, showing the generality of the method and the potential impact it can have. Summarizing, the energy-based model is intuitively sound and is a promising method in which we are still working on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>We have described the participation of TIA-INAOE at ImageCLEF2009. Our team submitted runs for the Photographic Retrieval and for the Large Scale Annotation tracks. We proposed specific methods for each track. On the one hand, we described a re-ranking approach that aims at maximizing the diversity of retrieved documents at the first positions. Our results show that whereas the proposed technique can improve the diversity of results, the base retrieval system still needs to be improved. On the other hand, we adopted a simple method for image annotation, and introduced a labeling refinement technique with the goal of improving the annotations as obtained with the former method. Our results suggest the KNN approach is effective for annotation and very efficient. However, there are still several issues with our refinement method that we are currently working on.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,162.61,320.92,277.80,10.46;3,103.74,111.78,393.44,195.22"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Diagram of the proposed approach to image retrieval.</figDesc><graphic coords="3,103.74,111.78,393.44,195.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,90.01,428.46,423.00,10.46;7,90.00,440.42,423.01,10.46;7,90.01,452.37,423.01,10.46;7,90.00,464.32,155.34,10.46;7,103.34,219.24,393.44,195.66"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of the proposed energy-based model. Each variable (node) represents a disjoint category. A variable can take values labels from the corresponding category only (shaded box). The estimate from Equation (2) is normalized and used as input for the model. We considered a fully connected model.</figDesc><graphic coords="7,103.34,219.24,393.44,195.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="4,154.38,539.21,296.20,81.74"><head>Table 2 :</head><label>2</label><figDesc>Official retrieval results obtained with the submitted runs.</figDesc><table coords="4,157.04,539.21,293.54,59.87"><row><cell>1</cell><cell>0.2901</cell><cell>0.4299</cell><cell>0.3411</cell><cell>0.5550</cell><cell>263.20/697.74</cell></row><row><cell>R-2</cell><cell>0.2723</cell><cell>0.4737</cell><cell>0.3300</cell><cell>0.5540</cell><cell>262.92/697.74</cell></row><row><cell>R-3</cell><cell>0.2710</cell><cell>0.5787</cell><cell>0.3298</cell><cell>0.5580</cell><cell>262.92/697.74</cell></row><row><cell>R-4</cell><cell>0.2706</cell><cell>0.4855</cell><cell>0.3274</cell><cell>0.5340</cell><cell>262.92/697.74</cell></row><row><cell>R-5</cell><cell>0.2645</cell><cell>0.5534</cell><cell>0.3242</cell><cell>0.5690</cell><cell>262.92/697.74</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,140.66,254.75,321.70,106.84"><head>Table 3 :</head><label>3</label><figDesc>Runs submitted by our team to the large scale annotation track.</figDesc><table coords="9,167.01,287.36,267.35,74.22"><row><cell>ID</cell><cell>H-A</cell><cell cols="2">H-WA EER</cell><cell>AUC</cell><cell>Time (s)</cell></row><row><cell>A-1</cell><cell>0.7592</cell><cell>0.7317</cell><cell>0.4862</cell><cell>0.1008</cell><cell>0.15</cell></row><row><cell>A-2</cell><cell>0.5329</cell><cell>0.5125</cell><cell>0.4847</cell><cell>0.0993</cell><cell>0.24</cell></row><row><cell>A-3</cell><cell>0.7281</cell><cell>0.6966</cell><cell>0.4929</cell><cell>0.0442</cell><cell>0.25</cell></row><row><cell>A-4</cell><cell>0.7323</cell><cell>0.7018</cell><cell>0.4924</cell><cell>0.0622</cell><cell>0.26</cell></row><row><cell>A-5</cell><cell>0.7418</cell><cell>0.7127</cell><cell>0.4872</cell><cell>0.0947</cell><cell>0.23</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,148.74,372.99,305.52,10.46"><head>Table 4 :</head><label>4</label><figDesc>Official annotation results obtained with the submitted runs.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="6,105.24,737.08,407.88,8.37"><p>Note that we have applied the same postprocessing described below for selecting labels under this formulation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="7,105.24,744.06,297.16,8.37"><p>Note that these neighbors are different from the neighbors considered in Section</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2" coords="7,405.23,744.06,13.18,8.37"><p>3.2.   </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. The authors thank the organizers of ImageCLEF2009 because of their support. This work was partially supported by <rs type="funder">CONACyT</rs> under project grant <rs type="grantNumber">61335</rs> and scholarship <rs type="grantNumber">205834</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_J3fefrV">
					<idno type="grant-number">61335</idno>
				</org>
				<org type="funding" xml:id="_ANc9rpM">
					<idno type="grant-number">205834</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="10,104.71,370.86,362.31,9.41" xml:id="b0">
	<monogr>
		<title level="m" type="main" coord="10,257.85,370.86,119.69,9.41">Modern Information Retrieval</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Pearson E. L</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,104.71,385.80,408.12,9.41;10,104.71,396.76,249.89,9.41" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,289.94,385.80,219.11,9.41">Maximizing the semantic cohesion for region labeling</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Sucar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="10,104.71,396.76,221.73,9.41">Submitted to International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,104.71,411.71,408.26,9.41;10,104.71,422.66,408.25,9.41;10,104.71,433.63,92.82,9.41" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="10,262.18,411.71,142.10,9.41">A new baseline for image annotation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Pavlovi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,421.97,411.71,91.00,9.41;10,104.71,422.66,210.55,9.41">ECCV&apos;08: Proceedings of the 10th European Conference on Computer Vision</title>
		<meeting><address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5304</biblScope>
			<biblScope unit="page" from="316" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,104.71,448.57,408.12,9.41;10,104.71,459.53,408.24,9.41" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="10,205.94,448.57,306.89,9.41;10,104.71,459.53,15.39,9.41">Overview of the clef 2009 large scale -visual concept detection and annotation task</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Dunker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,308.36,459.53,81.17,9.41">CLEF working notes</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Borri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-10">October 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,104.71,474.48,408.29,9.41;10,104.71,485.43,408.12,9.41;10,104.71,496.39,305.17,9.41" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,229.37,474.48,247.23,9.41">Multilabel classification evaluation using ontology information</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lukashevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,495.35,474.48,17.65,9.41;10,104.71,485.43,408.12,9.41;10,104.71,496.39,15.17,9.41;10,187.14,496.39,117.26,9.41">Proceedings of the First ESWC Workshop on Inductive Reasoning and Machine Learning on the Semantic Web</title>
		<meeting>the First ESWC Workshop on Inductive Reasoning and Machine Learning on the Semantic Web<address><addrLine>Heraklion, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">474</biblScope>
		</imprint>
	</monogr>
	<note>CEUR Workshop Proceedings</note>
</biblStruct>

<biblStruct coords="10,104.71,511.34,408.12,9.41;10,104.71,522.29,408.26,9.41;10,104.71,533.25,56.57,9.41" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,288.32,511.34,224.51,9.41;10,104.71,522.29,63.24,9.41">Diversity in photo retrieval: Overview of the imageclefphoto task 2009</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Paramita</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,364.54,522.29,82.49,9.41">CLEF working notes</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Borri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Nardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</editor>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-10">October 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,104.71,548.20,408.12,9.41;10,104.71,559.16,408.12,9.41;10,104.71,570.11,408.12,9.41;10,104.71,581.08,39.27,9.41" xml:id="b6">
	<monogr>
		<title level="m" type="main" coord="10,391.75,548.20,121.08,9.41;10,104.71,559.16,404.20,9.41">A theory of object recognition: computations and circuits in the feedforward path of the ventral stream in primate visual cortex</title>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kouh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Cadieu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">U</forename><surname>Knoblich</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<idno>#2005-036</idno>
		<imprint>
			<date type="published" when="2005-12">Dec 2005</date>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">AI Memo</note>
</biblStruct>

<biblStruct coords="10,104.71,596.02,408.26,9.41;10,104.71,606.97,182.07,9.41" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,156.08,596.02,293.37,9.41">Image Analysis, Random Fields and Markov Chain Monte Carlo Methods</title>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" coord="10,104.71,606.97,114.03,9.41">Applications of Mathematics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,104.71,621.92,408.12,9.41;10,104.71,632.88,408.24,9.41;10,104.71,643.83,277.64,9.41" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,249.69,621.92,263.14,9.41;10,104.71,632.88,81.91,9.41">Tmg: A matlab toolbox for generating term-document matrices from text collections</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Zeimpekis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Gallopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,402.92,632.88,110.02,9.41;10,104.71,643.83,147.18,9.41">Grouping Multidimensional Data: Recent Advances in Clustering</title>
		<editor>
			<persName><forename type="first">C</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicholas</forename><forename type="middle">J</forename><surname>Kogan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="187" to="210" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
