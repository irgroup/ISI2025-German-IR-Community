<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,115.50,91.10,335.99,12.19">Combined Content based and Semantic Image Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,202.92,105.88,60.43,8.74"><forename type="first">Ioannis</forename><surname>Boutsis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics Athens</orgName>
								<orgName type="institution">University of Economics and Business Athens</orgName>
								<address>
									<postCode>104</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,269.95,105.88,94.11,8.74"><forename type="first">Theodore</forename><surname>Kalamboukis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics Athens</orgName>
								<orgName type="institution">University of Economics and Business Athens</orgName>
								<address>
									<postCode>104</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,115.50,91.10,335.99,12.19">Combined Content based and Semantic Image Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D35D37AF56298C95DDD86F8F1A2D655B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>i-score (Image Semantic and COntent based REtrieval system) [1]  developed at the Information Processing Laboratory combines two open source software libraries, Lire [2] and Lucene [3], with the aim to investigate the impact of images text-description in the quality and the effectiveness of image retrieval. In our runs for the ImagrCLEF2009 track the default Lucene's text analysis (stopword removal and stemming) was performed and the default Lucene's score function was used to evaluate the queries. Also all the duplicate descriptions of the images were removed from the database and a link was added to each record instead referring to a unique text. 39310 unique texts were remained in the database. In both tasks Ad-Hoc and Case-based the semantic retrieval outperformed by far the visual and consequently the mixed retrieval. This is sensible for at least in our case we have used a naïve visual retrieval procedure. However give us promising evidence that techniques from textual retrieval can improve image retrieval in both the performance and efficiency</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="567.0" lry="708.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="567.0" lry="708.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="567.0" lry="708.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Construction of the Indexes</head><p>Two indexes were created automatically, one for the database of the images for visual retrieval and one for their descriptions for semantic retrieval. For the images' data-base the index was created using Lire's DefaultDocumentBuilder and as an Analyzer Lire's SimpleAnalyzer. As a result the low level characteristics that we keep for each image are ScalableColor, ColorLayout and EdgeHistogram as they are defined at mpeg7. For the texts' data-base firstly the HTML tags were removed. Then all the duplicate texts were removed and a link was added to each record instead referring to a unique text. 39310 unique texts were remained in the database. The index was based on the Lucene library and for each field of the images' records, that we want to be able to search, the following analysis was performed: The LowerCaseTokenizer was used and we have tokenized wherever the character is not a letter. Lucene's standard stop-words list was used and Porter's stemming algorithm applied on the remaining terms. Finally the filter (LengthFilter) was used to remove the terms that are either very small or very big to enter in a java stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Searching</head><p>For the visual retrieval the WeightedSearcher of Lire was used. The similarity measure was a weighted sum of the partial similarities due to each low level characteristic. In all our runs these values were set to: colorHistogramWeight=0.5 (ScalableColor), colorDistributionWeight=1.0 (ColorLayout) and textureWeight=0.7 (EdgeHistogram) wherever the input image was coloured and colorHistogramWeight=0.3, colorDistributionWeight=0.3 and textureWeight=1.0 for the black and white ones.</p><p>For the semantic retrieval, queries were subject to the same analysis as described in the indexing procedure (stopword removal, stemming) and the resulting sequence of terms was passed to Lucene. The default Lucene's score function was used to evaluate the queries. Finally, at the combined retrieval the scoring function was defined as a linear combination of both, the image search and the corresponding text search, i.e. Mixed_score = 0.8* textScore + 0.2 * imageScore</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs for CLEF-2009</head><p>The results for Image CLEF have been created off-line in order to be saved in trec_eval format. Thus, we have created seven runs as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Topics</head><p>Run 1 -Visual: A visual search was performed for each of the images given as input and the results were combined, taking the average score. If the visual query contains p images then the score of a retrieved image, i, is given by</p><formula xml:id="formula_0" coords="2,180.13,264.64,206.01,30.54">∑ = = p k k i i sim visual p i SCORE visual 1 ) , ( _ 1 ) ( _</formula><p>Run 2 -Semantic: For each topic the semantic index was searched. Run 3 -Mixed: The semantic query was performed first. The visual retrieval was restricted to the images whose corresponding text was retrieved by the semantic query. If S is the set of images returned by the semantic query then</p><formula xml:id="formula_1" coords="2,106.26,342.13,354.16,23.19">S i i SCORE visual i SCORE semantic i SCORE mixed ∈ + = ) ( _ * 2 . 0 ) ( _ * 8 , 0 ) ( _</formula><p>Run 4 : For each topic the retrieval was based on the type -semantic, visual, mixed -described in the topics2009.xml file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Case Based Topics</head><p>Run 5 -Visual: A visual retrieval is applied to each topic and the score of an article is defined as the average of all visual similarities between the query-image and all the images in that article. Run 6 -Semantic: The score of an article is defined as the average of all the semantic similarities between the semantic query and all the captions in that article retrieved by the query. Run 7 -Mixed: A visual and a semantic retrieval are applied as in the runs 5 and 6 and the results are combined using percentages(80-20) for the text retrieval and image retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Concluding Remarks</head><p>The results of our runs in ImageCLEFmed are summarized the table 1. In both tasks Ad-Hoc and Casebased the semantic retrieval outperformed by far the visual and consequently the mixed retrieval. This is sensible for at least in our case we have used a naïve visual retrieval procedure. However, this is a general remark in the results of all the participants, which give us promising evidence that techniques from textual retrieval may improve image retrieval in both the performance and efficiency. Indeed there is a lot of space for improvements techniques using relevance feedback techniques and domain ontologies or categorizing the images are currently under investigation. Definitely the availability of very large image collections accompanied with descriptions, like the one in CLEFmed track, will contribute positively in this direction. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,70.92,108.70,423.69,139.24"><head>Table 1 .</head><label>1</label><figDesc>Performance results of IPL laboratory for the ImageCLEFmed track.</figDesc><table coords="3,71.22,108.70,423.40,115.72"><row><cell>Task</cell><cell></cell><cell>num rel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>retr</cell><cell>MAP</cell><cell>P5</cell><cell>P10</cell><cell>P20</cell><cell>P30</cell><cell>P100</cell><cell>P1000</cell></row><row><cell>AdHoc</cell><cell>Textual</cell><cell cols="3">1803/2362 0,3362 0,672</cell><cell>0,604</cell><cell>0,554</cell><cell>0,504</cell><cell cols="2">0,3152 0,0721</cell></row><row><cell>Ret</cell><cell>Mixed</cell><cell cols="3">1381/2362 0,1466 0,376</cell><cell>0,328</cell><cell>0,28</cell><cell cols="3">0,2587 0,1696 0,0552</cell></row><row><cell></cell><cell>Not</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">applicable 824/2362</cell><cell cols="2">0,1255 0,328</cell><cell>0,288</cell><cell>0,246</cell><cell>0,224</cell><cell cols="2">0,1328 0,033</cell></row><row><cell></cell><cell>Visual</cell><cell>242/2362</cell><cell cols="2">0,0048 0,024</cell><cell>0,028</cell><cell>0,026</cell><cell>0,024</cell><cell>0,024</cell><cell>0,0097</cell></row><row><cell>Case</cell><cell>Textual</cell><cell>93/95</cell><cell cols="2">0,1912 0,32</cell><cell>0,24</cell><cell>0,21</cell><cell cols="2">0,1867 0,116</cell><cell>0,0186</cell></row><row><cell>based</cell><cell>Mixed</cell><cell>57/95</cell><cell>0,0159 0</cell><cell></cell><cell>0</cell><cell>0</cell><cell>0,02</cell><cell>0,016</cell><cell>0,0114</cell></row><row><cell>Ret</cell><cell>Visual</cell><cell>39/95</cell><cell>0,0085 0</cell><cell></cell><cell>0</cell><cell>0</cell><cell cols="2">0,0133 0,01</cell><cell>0,0078</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
