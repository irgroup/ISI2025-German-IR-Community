<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,105.12,148.73,392.96,15.51;1,110.28,170.69,382.53,15.51">A particle-filter-based self-localization method using invariant features as visual information</title>
				<funder ref="#_TH6Te5Q #_M2HDVmU">
					<orgName type="full">FEDER</orgName>
				</funder>
				<funder ref="#_qEeBYRh">
					<orgName type="full">Spanish &quot;Junta de Comunidades de Castilla-La Mancha (Consejería de Educación y Ciencia)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,147.48,204.41,96.61,9.62"><forename type="first">Jesús</forename><surname>Martínez-Gómez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Castilla-La Mancha</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,252.23,204.41,107.19,9.62"><forename type="first">Alejando</forename><surname>Jiménez-Picazo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Castilla-La Mancha</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,367.07,204.41,88.87,9.62"><forename type="first">Ismael</forename><surname>García-Varea</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Castilla-La Mancha</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,105.12,148.73,392.96,15.51;1,110.28,170.69,382.53,15.51">A particle-filter-based self-localization method using invariant features as visual information</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EBA13831FE27DCEF17BEB9309D706B24</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.4 [Cross-Language Retrieval in Image Collections]: H.4.4 Robot Vision Evaluation</term>
					<term>Image Classification</term>
					<term>Robot Localization Mobile robots</term>
					<term>image processing</term>
					<term>localization</term>
					<term>particle filters</term>
					<term>Scale-Invariant Features Transform</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article presents a new approach to robot localization in indoor environments. The system presents a Monte Carlo method using SIFT for the visual step. The scope of the system is indoor environments where no artificial landmarks are necessary. The complete pose &lt; x, y, θ &gt; can be obtained. Results obtained in the RobotVision@ImageCLEF competition proved the goodness of the algorithm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.0" lry="842.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Self-localization is one of the harder problems in mobile robot research. Intelligent robots need to perform different tasks depending on their own pose. If a robot's pose is not well estimated, the behaviour obtained will not be correct.</p><p>Different approaches have been successfully implemented to deal with real-world problems (noise, uncertainty, low-quality images) <ref type="bibr" coords="1,267.62,586.49,10.57,9.62" target="#b2">[3,</ref><ref type="bibr" coords="1,282.15,586.49,7.81,9.62" target="#b3">4,</ref><ref type="bibr" coords="1,293.91,586.49,12.73,9.62" target="#b10">11,</ref><ref type="bibr" coords="1,310.71,586.49,6.97,9.62" target="#b8">9]</ref>. Most of these approaches combine the information obtained from two different sources: perception and odometry. Perception uses all the robot's sensors to retrieve information from the environment. Main sensors are distance sensors and vision cameras. Odometry can be used to estimate the pose of the robot using the last pose and the set of movements the robot has performed so far. Both sources (perception and odometry) are noisy and with a high uncertainty, so the combination of these sources of information must be performed appropriately.</p><p>The RoboCup 1 competition is a good scenario where different solutions have been proposed over the last few years. These proposals use natural or artificial landmarks in the environment to estimate the robot's pose (goals, beacons and field lines). Within these controlled environments, the absolute position of these landmarks does not vary and the robot's pose can be obtained by estimating distances and orientation in relation to these elements. Other environments, such as that proposed for the RobotVision@ImageCLEF, do not have natural landmarks to be used for robot pose estimation. These dynamic environments force us to use other techniques without the use of landmarks. These techniques are commonly based on the information obtained from distance sensors, such as sonar or laser.</p><p>The RobotVision@ImageCLEF task addresses the challenge of localization of a mobile robot using only visual information. Neither distance sensors nor odometry information is provided with the final test sequence. This is why this article is focused on image processing and how to estimate a robot's pose using this processing.</p><p>The approach presented here carries out localization by using Scale-invariant feature transform <ref type="bibr" coords="2,505.09,207.41,13.24,9.62" target="#b5">[6]</ref> for the visual step. The principles of the particle-filter-based Monte Carlo <ref type="bibr" coords="2,414.96,219.41,10.45,9.62" target="#b1">[2]</ref> method are applied with some variations. The combination of this localization method with a robust image processing algorithm allows the robot to reduce the uncertainty about its pose when captured images have a good resolution. The algorithm keeps information about the robot's pose even when the quality of the images decreases.</p><p>Different experiments using training image sequences and the final validation test sequence have been carried out to evaluate our proposal. These experiments were performed for the RobotVision@ImageCLEF task (see <ref type="bibr" coords="2,212.63,303.05,10.45,9.62" target="#b0">[1]</ref> for more information).</p><p>The article is organized as follows: SIFT techniques are outlined in Section 2; we describe the Monte Carlo algorithm in Section 3 and in Section 4 we explain the approach we adopted for the task. Section 5 describes the experiments performed and the results obtained. Finally, the conclusions and areas for future work are given in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Scale-Invariant Feature Transform</head><p>Scale-Invariant Feature Transform <ref type="bibr" coords="2,236.13,405.65,13.86,9.62" target="#b6">[7]</ref> (SIFT) is a computer vision algorithm, developed to detect key features in images. This algorithm was developed and published by David Lowe in 1999, and now there are different versions and variations available.</p><p>The main idea of the algorithm is to apply different transformations and study the points of the image which are invariant under these transformations. These extracted points can be used to perform object recognition by carrying out a matching between images representing the same object or scenario.</p><p>Features extracted are invariant to image scale and rotation, and they are also robust to noise and changes in viewpoint. An important characteristic of systems developed to perform object recognition using SIFT is that they are robust to partial object occlusion.</p><p>The SIFT implementation used was developed by R. Hess, from the Oregon State University<ref type="foot" coords="2,508.56,524.00,3.97,5.52" target="#foot_1">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Algorithm</head><p>Different steps are necessary to extract invariant points. First, the image is convolved with Gaussian filters at different scales. Key locations are selected at maxima and minima of difference of these filters. After this step, too many candidates are obtained. A second step discards low contrast key points by interpolating the data near each candidate point. The interpolation is performed using the Taylor expansion <ref type="bibr" coords="2,421.00,619.37,17.99,9.62" target="#b9">[10]</ref> of the difference of the Gaussian scale-space function. Finally, the algorithm eliminates candidates not located on edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Problems and restrictions</head><p>The main problems of using this algorithm are the long execution time and the high demand on memory. The execution time necessary to extract features from a 309 x 240 image (size used for the task) is about 0.5 seconds using a 2.13GHz dual core processor 3 . The matching between two extracted feature sets takes 0.1 seconds using a nearest-neighbour search in high-dimensional spaces.</p><p>If we denote T ext as the time necessary to extract the features and T mat as the time to perform the matching, the processing time T proc can be defined using Eq.1, where n is the number of images to compare with.</p><formula xml:id="formula_0" coords="3,251.04,183.19,262.00,10.33">T proc = T ext + T mat * n<label>(1)</label></formula><p>The main problem of the execution time is that it depends on the number of images to compare with. For the RobotVision@ImageCLEF task, where the final training set contains 1690 frames, the time needed to classify one image using our computer will be 0.5 + 1690 * 0.1 ≈ 2.82 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Monte Carlo localization method</head><p>Different formal methods for the localization problem have been proposed over the last few years. Nowadays, two methods are widely used and most localization systems are defined using their principles. These methods are particle filters and (Extended) Kalman filters <ref type="bibr" coords="3,425.41,304.01,10.00,9.62" target="#b8">[9]</ref>.</p><p>Kalman filters are a robust method that works properly under optimal conditions. Its execution time and demand on space are small, but an important drawback is that it is necessary to know the robot's initial pose. Moreover, this method presents problems when the uncertainty about the pose increases, or when the robot can suddenly be kidnapped (such as in the RoboCup environment).</p><p>Particle filters spread particles over the environment. Each one of these particles represents a robot's pose &lt; x, y, θ &gt;. An iterative process applies an odometry step to each one of these particles and performs the weighting step. The evaluation of each particle is obtained by processing the information sensed from the environment, using the camera or distance sensors. After all the particles have been pondered, a sampling step is applied. A new population of particles is obtained from the last iteration, and the particles are sampled with replacement using their goodness value as selection probability. Best candidates should be duplicated, and worst particles should disappear from the environment. After some iterations, the algorithm will converge and all the particles will be around the real robot's pose, as can be observed in Fig. <ref type="figure" coords="3,349.83,459.41,3.90,9.62" target="#fig_0">1</ref>. The main advantage of the algorithm is that uncertainty about pose estimation can be kept throughout the process. If the robot detects that it is close to an internal door, particles will be distributed at different parts in the environment. This situation can be observed in Fig. <ref type="figure" coords="4,477.14,135.65,3.90,9.62" target="#fig_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Process</head><p>The process of a particle-filter-based localization method consists of a prediction and an update phase. These phases are repeated iteratively until a specific stop condition is reached. For instance, the stop condition can be the end of a test frame sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Prediction Phase</head><p>In this step we start with the set of particles from the last iteration. We apply the movement model to each one of these particles. In order to apply this phase correctly, it will be necessary to have an accurate odometry model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Update Phase</head><p>In this phase, the information obtained from sensors is taken into account. Each particle is weighted using the likelihood of obtaining the measurement from the pose it represents. After all the particles are weighted, the resampling step is applied. This resampling step generates a new set of particles. The weight of each particle is used as the selection probability for the set of particles to be part of the next iteration of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SIMD approach</head><p>Our proposal for the RobotVision@ImageCLEF task was to develop a localization method using particle-filter principles and SIFT for the update phase. We used the KHL-IDOL2 <ref type="bibr" coords="4,447.00,682.61,10.57,9.62" target="#b7">[8]</ref> available set of frames to train the system. The information retrieved after applying SIFT to training frames was the complete pose &lt; x, y, θ &gt;, the correctly-classified room and the set of invariant points extracted from the frame.</p><p>The definition of these points was stored in data files. Each training frame has an associated file with all the obtained SIFT points. This allows us not to have to recompute SIFT points when an image matching process has to be performed, with the associated saving in computing time. These files also stored the pose and the room corresponding to the image. By reading the complete file sequence we can obtain all the training information.</p><p>We presented two approaches for two types of problems. The first one (obligatory track) corresponds to global topological localization, where the algorithm must provide information separately for each test image. For this problem, only visual processing (without any localization method) is applied. The second problem (optional track) corresponds to typical localization tasks, where image sequences are processed and the order is important. Visual processing is completed with a particle-filter-based method to develop the localization algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Obligatory Track -Image Processing</head><p>As mentioned above, only image processing is applied for this track. We uses a multi-classifier to label each image as an environment room. The first approach was to use only SIFT to classify each image, but the results obtained were not satisfactory due to the problems of SIFT with changing lighting conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">SIFT processing</head><p>SIFT processing is performed by extracting invariant features from the frame to be classified. These features are compared with the features extracted from the training sequence. The similarity between two frames is obtained by performing a matching between SIFT points extracted from them. This similarity is pondered as the percentage of common points.</p><p>After matching all the training frames with the current frame, we have to classify it. The current frame will be classified using the most similar training frames. This can be performed because all the training frames are correctly labelled with the room. We process only the best n training frames, using similarity to select the best candidates.</p><p>To classify a frame using the best n training frames, we compute the sum of all its weights. This summing is performed separately for the different rooms and finally, each room R i will have an associated sum of weights sum i . Each frame is classified as the room that obtained the highest sum of weights.</p><p>To show how this works, Tab. 1 presents the best 11 pondered training frames. With these pondered frames, we compute the sum of the weights separately for each room. After this sum is applied, we obtain the result that can be observed in Tab. 2. The current frame would be classified as Printer Area (PA). In order to avoid classifying frames when the uncertainty about the robot's pose is high, we only classify frames when the maximum sum of weights for a room (2.56 in Tab. 2 ) is higher than 60% of the sum of all the weights (3.22 in Tab. 2 ).</p><p>A significant drawback to this reasoning is the execution time. It depends on the number of comparisons we perform. For the final RobotVision@ImageCLEF experiment, the training data sequence has 1690 frames. With a comparison time close to 0.18 secs and an SIFT extraction time of 0.6 secs, it is necessary to spend more than 3 minutes to classify a single image. For the complete test sequence, the execution time was around 3.5 days of computing time.</p><p>Lighting changes are one of the main problems of using SIFT to localize the robot, because features extracted depend on the lighting conditions. During the development of the system, the results obtained were only acceptably good with training and test data acquired under the same lighting conditions. Due to this, we decided to add additional processing to improve the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Additional processing</head><p>SIFT was complemented with basic processing, based on line and square recognition. We apply a Hough <ref type="bibr" coords="6,123.15,227.81,14.63,9.62" target="#b4">[5]</ref> transform to study the distribution of the lines and squares obtained. Using images from the training sequences, we discovered some characteristics that could be detected using lines and squares.</p><p>These characteristics are unchanging sections from the environment that can be used as natural landmarks. Examples of these landmarks are the corridor ceiling, or the external door located in the printer area. We decided to use restrictive detection algorithms to avoid false positives. This processing was added to SIFT detection, and some examples of these detections can be observed in Fig. <ref type="figure" coords="6,121.93,311.45,4.98,9.62" target="#fig_2">3</ref> and<ref type="figure" coords="6,149.66,311.45,3.90,9.62" target="#fig_3">4</ref>.  The main advantage of this processing is that the execution time on a typical current computer is very low (about 0.003 milliseconds).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Complete image processing</head><p>Taking into account the preliminary results we assume that the additional processing is more reliable than SIFT processing. Because of this, and the reduced execution time, we classify an image using the additional processing whenever possible. Otherwise, we extract the SIFT features and perform a comparison with the complete training sequence. This image processing techniques is used for the visual step of the algorithm developed for the optional track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Optional Track</head><p>For the optional track, we propose an algorithm using particle-filter principles, and then we need to define the particle representation, the prediction step and the update step. It is worthy of note that despite the goal of the tracks being to classify each frame of the sequence into a specific room, our approach using a particle filter is also capable of providing the most reliable pose of the robot within the environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Particle representation</head><p>The particles used to localize the robot represent poses. A robot pose is defined using three parameters: x and y for position and θ for orientation. All these parameters have continuous numeric values. The limits for x and y values are the boundary of the environment. In this case, values for the x component are between 0 and 1500 cm. Limits for the y component are 0 and 2500 cm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Prediction Phase</head><p>The information necessary to perform the prediction phase, related to the movement model, can be extracted from the training images. We assume that robot movement will be similar to that performed during the training (training frames were acquired while the robot was moving). Due to this, the robot's average velocity can be obtained from the difference between the poses of the training sequence. We obtain the average linear and angular velocity, which are defined in centimeters (and degrees) per frame.</p><p>In this step we add some uncertainty, commonly termed white noise, to the process. We modify the pose of each one of the particles using the movement model and a random variation. This variation depends on the particle's weight w t . The idea is to apply a higher variation to the worst particles, in order to keep the best candidates with minor changes. We assume that the w t value is between 0 and 1.</p><p>The robot's pose at instant t is denoted by &lt; x t , y t , θ t &gt;. Linear and angular velocities are denoted by v l and v a . A random value rand between 0 and 1 was used to model the white noise. The maximum variations for x, y and θ are denoted by maxvar x , maxvar y and maxvar θ .</p><p>To illustrate this process, we describe below the necessary processing for the prediction phase. The robot's estimated pose for the instant t + 1, taking into account the pose estimated for the instant t, will be:</p><formula xml:id="formula_1" coords="7,105.00,523.60,111.16,66.99">• x ′ t+1 = x t + v l * sin(θ t ) • y ′ t+1 = y t + v l * cos(θ t ) • θ ′ t+1 = θ t + v a Maximum variations</formula><p>for x, y and θ components are computed using the particle's weight w t .</p><formula xml:id="formula_2" coords="7,105.00,599.92,147.52,49.48">• maxvar ′ x = maxvar x * (1 -w t ) • maxvar ′ y = maxvar y * (1 -w t ) • maxvar ′ θ = maxvar θ * (1 -w t )</formula><p>After applying the white noise, the estimated pose values will be:</p><formula xml:id="formula_3" coords="7,105.00,676.12,210.38,49.48">• x t+1 = x ′ t+1 + maxvar ′ x -2 * maxvar ′ x * rand • y t+1 = y ′ t+1 + maxvar ′ y -2 * maxvar ′ y * rand • θ t+1 = θ ′ t+1 + maxvar ′ θ -2 * maxvar ′</formula><p>θ * rand Movement applied to the particles will represent the distance and orientation variation we hope to obtain for the robot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Update Phase</head><p>This phase obtains the weight for each particle (w p ). We use the information obtained from the robot's sensors and in this case, visual information is the only type that can be used. Thus, each particle is evaluated using the frame captured at the instant t, namely f t .</p><p>The idea we propose is to use the image processing shown in section 4.1. In this case, we have to evaluate all the particles using the information extracted from the current frame, specially its SIFT points. Each particle is evaluated matching these points with those extracted from the frame taken from the pose the particle represents. The problem is that we only have SIFT points extracted from a small number of poses in the environment. These poses are the x, y and θ values of the images from the training sequence.</p><p>It will be necessary to search for SIFT points representing the nearest pose to the particle's pose. With bigger training sequences, the search space will be larger and the particle's evaluation will be more realistic. If a particle can only be evaluated with a distant SIFT representation, the evaluation will not be reliable.</p><p>The environment information is extracted during system training. While the training is being performed, we create a frame population. Each training frame (tf ) stores the pose, the code of the room where the picture was taken and the SIFT points extracted from this image.</p><p>Algorithm 1 shows the general processing scheme for particle evaluation. w pi ← % of common points; end for 9: end for The execution time necessary to perform this step will be fully dependent on the number of particles, the size of the training sequence and the similarity between particles at instant t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Particle Evaluation</head><p>Additionally to SIFT processing, line and square detection can be used to detect the robot's position. If this process works, we will obtain the room where the robot is located and that information will be used to correctly classify the current frame.</p><p>Once all particles are weighted, we have to classify the frame. For particle-filter localization methods, this step corresponds to position estimation. Usually, the robot's position is estimated using particle information: x, y and weight w. Mean position is obtained as a pondered mean, taking into account particle weight:</p><formula xml:id="formula_4" coords="8,231.36,599.47,140.34,20.66">x = i=0 x i × w i , ȳ = i=0 y i × w i</formula><p>It is possible to use the same reasoning and classify the frame as the room located at &lt; x, ȳ &gt; position. Problems arise when mean position does not corresponds to any room. To avoid this situation, we apply the same approach shown in section 4.1, adding the weights of all particles separately for each room.</p><p>If line and square detection classifies the frame with a room label, we have two possible scenarios: this room is the same as that obtained from all particles, or these rooms are different. If we obtain the same label, we assume that the algorithm is going well. Otherwise, the room will be labelled according to the result of line and square detection. In this situation, the particle population will be transformed to represent positions from the correct room, as will be explained in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Additional process</head><p>The first implementation of our system was a Monte Carlo localization method with the prediction and update phases as described above. This reasoning presented several problems, and it was necessary to modify the system, adding more processing.</p><p>The main problem was that some test frames were not represented by any training frame. This happens when there are no training frames from the pose where the test frame was taken or when SIFT matching fails. The consequence of both cases is that the weight w pi of the particles (or the % of points obtained in the matching process stated in Algorithm1) decreases continuously and they are spread over the environment. If this happens and we find false positives, the system will fail and the robot's location will not be correctly obtained. False positives occur due to noisy frames, luminance changes or when few SIFT points can be extracted from the test frame. With this situation, the system can converge at wrong environment positions. Reinitializing the particle population over the environment to recover from these situations did however obtain good results.</p><p>The first experiments added an extra step to the original Monte Carlo process. This step performs a population initialization when the best particle weight is below a certain threshold. This modification improved system behaviour when the localization failed and the particles converged at wrong environment positions. However, the system became unstable and the algorithm had problems to converge. This situation can be observed in Fig. <ref type="figure" coords="9,270.99,333.41,3.90,9.62" target="#fig_5">5</ref>, where frame 11 fails and makes the system's accuracy decrease. All the particles are spread over the environment and the algorithm needs 8 new frames to recover from this situation. Fig. <ref type="figure" coords="9,245.30,357.29,4.98,9.62" target="#fig_5">5</ref> shows the error for the position estimation. The estimation error increases quickly and affects the algorithm's performance. This happened when there was only one problematic frame with a controlled convergence situation. This new modification should not affect the stability of the algorithm, increasing its vulnerability to problematic frames. These frames will be noisy pictures or those that have problems with SIFT matching.</p><p>To avoid such situations, the next step was to define a state for the algorithm based on its stability. The stability of the algorithm can be estimated by studying the particle poses. We store a number n of previous estimated robot poses. The process will be stable if the variation obtained for the x and y components of the last n pose estimations is sufficiently small. This happens when all the particles are close together without high variations between frames.</p><p>When the algorithm is stable, no population initializations are performed. Otherwise, the initialization will depend on the instability level. If the algorithm has been stable for the last few frames and suddenly it becomes unstable, initialization will be performed over a restricted area. This area will be a circumference centered at the most reliable robot position, obtained from previous iterations (with a stable process). The radius of this circumference depends on the instability level. This level is obtained with the best particle's weight for the last n iterations. Using this proposal, the algorithm becomes robust in those situations described above, with noisy and problematic pictures.</p><p>If the algorithm has been unstable for the last few frames and a new initialization has to be applied, all the particles will be spread over the environment.</p><p>All this processing is shown in Fig. <ref type="figure" coords="9,260.90,745.37,3.90,9.62" target="#fig_6">6</ref>. In order to show how the system works, Fig. <ref type="figure" coords="10,301.09,340.97,4.98,9.62" target="#fig_7">7</ref> presents three consecutive frames. For the first one, the algorithm was stable, and the robot's most probable pose was obtained (red circle). With the next frame, the system fails and the process becomes unstable. The algorithm performs a particle initialization for the third frame, but new particles are only created in the area denoted by the green circle. This circle is centered at the position &lt; x, y &gt; obtained from the most reliable pose with the first frame. In addition to this reasoning, and taking into account the CLEF task characteristics, we can add some processing for classification. The idea is to classify the images into rooms only when we are sure. This happens when the algorithm is stable and all the particles are close together. Otherwise, the current frame will not be classified in order to avoid failing. The stability of the process will depend on the threshold selected as minimum goodness value for the particles.</p><p>A special situation that should be detected is unknown rooms. This happens when a frame corresponds to a new room not available during the training process. We propose studying the particle distribution after applying the odometry step. If most of these particles obtain a new position beyond the limits of the original scenario, the robot will have entered an unknown room.</p><p>We assume the robot's movements for the test sequences will be similar to those obtained with the training sequences. The most common movement obtained from training sequences is going straight ahead. The algorithm will detect unknown rooms if the following situation arises: In this situation, the algorithm will classify the room as unknown. We selected 25% as the minimum percentage of particles outside a room to label the current frame as unknown.</p><formula xml:id="formula_5" coords="11,105.00,169.27,81.71,9.96">• Instant t (before</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head><p>The experiments were carried out using the KHL-IDOL2 <ref type="bibr" coords="11,350.53,365.81,10.57,9.62" target="#b7">[8]</ref> database as training and test sequences. Both tracks (obligatory and optional) can be evaluated using a script (provided by the competition organization) that obtains the score for a single test sequence. This script can obtain the score for six test sequences, available during the training period (2 sequences for each set of illumination conditions). For the final experiment, algorithms must be trained using a specific data sequence. Testing was performed on a test sequence with 1690 frames. This final test sequence couldn't be evaluated using the script, and the final results were obtained by submitting the classified test sequence to a dedicated web page.</p><p>For both tracks, we will show the results of applying our approach with different combinations of training and test frame sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Obligatory track</head><p>For this track, it was only necessary to define the number of training frames to be used in order to classify a test frame. Using a huge values for this parameter, we accumulate noise from all the frames. We obtained unsatisfactory results due to the disparity between the number of training frames for the different rooms. For most training sequences, the number of corridor frames was always much bigger than the number for the other rooms. With a balanced training sequences this problem would disappear because the noise will be homogeneous for all the rooms. Using only the best training frame (instead of a number n of frames) we obtained bad results for the test frames belonging to the frontier of two different rooms.</p><p>Based on the experimental results, the best value for the number of training frames used to classify a frame was 20. All the experiments were performed using that value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Preliminary Experiments</head><p>The experiments were carried out using the KHL-IDOL2 training and test sequences. We trained three systems using the different illumination conditions. Each one of these systems was tested using three frame sequences (obtained under different lighting conditions). Test and training sequences were different. We repeated the experiment for each combination of test and training data sequences. Tab. 3 shows the final score, the number of correctly (C) and misclassified (M) images, and the number of not classify (NC) images. The data shown in Tab. 3 demonstrates that the system developed highly depends on lighting changes. There is an important negative impact of using different illumination conditions for training and test sequences. Best results, for all the test sequences, were always obtained using the training sequence acquired under the same illumination conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Final experiment</head><p>The final results obtained using the final image test sequence was published in the official track website. The final score obtained was 511.0. Complete results can be observed in Tab. The winner of this track was the Glasgow University (Multimedia Information Retrieval Group), which obtained 890.5 as final score. Our proposal obtained the 10th position (21 different runs were submitted). This result was not successful, but allows us to justify our proposal for the optional track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Optional track</head><p>For the optional track, different parameters can be defined to select the best system configuration. The most important parameter is the number of particles (n). Another important parameter to define is the threshold to be used to define the algorithm stability, which should be tuned taking into account the expected variations for the lighting conditions. As mentioned in section 4.2.4 when the best particle weight is below this threshold the algorithm becomes unstable, and therefore a population initialization is performed.</p><p>For example, a value for this threshold of 7% of common points between two frames is a small value when given the same lighting conditions. Otherwise that percentage can be suitable if training lighting conditions are different from test lighting conditions.</p><p>For the final experiment, where training frames were taken under night illumination conditions, which are completely different from the test lighting conditions, the minimum threshold was set to 5%. This value was maintained for all the experiments, but using the same illumination conditions for training and test, this threshold value should be greater.</p><p>The final number of particles (n) will be 24 for all the experiments. Using this small number, a complete test sequence (≈ 960 frames) can be classified in 56 minutes. The execution time for the algorithm is not fixed, and it will be higher for a big number of re-initializations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Preliminary experiments</head><p>Some preliminary experiments were performed before the final test sequence was available. These experiments were carried out using the same training and test sequences than those used for the preliminary experiments performed in Section 5.1.1. Tab. 5 shows the final score, the number of correctly (C) and misclassified (M) images, and the number of not classify (NC) images. We can observe how the method obtains good results using training and test sequences acquired under different lighting conditions. For all the test sequences, (at least) 60% of frames were correctly classified. It is worthy of note that scores obtained for the different test sequences are not so strongly dependent on training illumination conditions as those obtained in Tab. 3. What is more, the best score for cloudy test frames was obtained with sunny training sequence. Making a comparison between these results and those obtained for the obligatory track, we can state that the localization method obtains significantly better results for the optional task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Final experiment</head><p>Different runs of the algorithm were submitted to the website and the best score obtained was 916.5. This score was the highest for all submitted runs of all participants. In view of the obtained results, SIMD 4 group of University of Castilla-La Mancha was the winner for the optional track. Complete results can be observed in Tab. 6. Only three groups presented a proposal for this track, and the three best scores were 916.5, 884.5 and 853.0. Making a comparison of our proposals for the two tracks, it can be noticed that the final score was significantly improved (from 511.0 to 916.5). The number of misclassified frames was similar for both proposals, but the localization system used for the optional track increased the number of correctly classified frames. Because of this, the number of non classified images decreased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Final</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>According to the results obtained for the optional track, our proposal becomes a robust alternative to traditional localization methods for indoor environments. It uses the principles of particle filter and Scale-Invariant Feature Transform to estimate the pose of the robot. The short execution time of our proposal allows the system to be used in real time. The system works properly with variable lighting conditions and changing indoor environments.</p><p>The results obtained for the obligatory track shows the important problem of using SIFT with lighting changes. SIFT must be complemented with other techniques if lighting changes appear. The long execution time for feature extraction and matching makes it necessary to reduce the number of comparisons to be performed.</p><p>For future work, we aim to develop a general localization system capable of being trained automatically using the robot and its vision system. We shall also study the possible integration of the information extracted from distance sensors and odometry.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,90.00,739.73,423.17,9.62;3,100.56,480.50,401.80,244.40"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of particle convergence after three iterations of the MC localization algorithm</figDesc><graphic coords="3,100.56,480.50,401.80,244.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,147.00,373.49,309.20,9.62;4,216.96,156.94,169.49,201.72"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Particle distribution when the robot detects an internal door</figDesc><graphic coords="4,216.96,156.94,169.49,201.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,98.16,432.05,406.77,9.62;6,185.16,332.51,232.84,84.83"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Corridor detection. Purple lines are candidates and green lines are the correct ones</figDesc><graphic coords="6,185.16,332.51,232.84,84.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,90.00,568.61,423.14,9.62;6,90.00,580.49,18.93,9.62;6,121.68,465.00,359.60,88.78"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Printer area detection. Purple squares are candidates and greens squares are the correct ones</figDesc><graphic coords="6,121.68,465.00,359.60,88.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="9,101.04,507.65,401.09,9.62;9,185.16,400.86,232.54,92.08"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Problems encountered with particle re-initialization. Error for position estimation</figDesc><graphic coords="9,185.16,400.86,232.54,92.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="10,220.32,310.37,162.35,9.62;10,174.60,108.99,253.87,186.55"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: General Processing Scheme</figDesc><graphic coords="10,174.60,108.99,253.87,186.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="10,123.36,638.45,356.36,9.62;10,121.68,421.16,359.28,202.46"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Particle initialization over a restricted area for three consecutive frames</figDesc><graphic coords="10,121.68,421.16,359.28,202.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="11,189.96,169.61,66.52,9.62;11,126.12,189.53,184.62,9.62;11,126.12,205.49,290.66,9.62;11,126.12,221.33,165.76,9.62;11,105.00,240.91,145.12,9.96;11,126.12,261.29,182.46,9.62;11,126.12,277.13,325.19,9.62"><head>-</head><label></label><figDesc>odometry step) Most particles are in the same room R1 -Positions &lt; x, y &gt; of these particles are close to a room's bound -Particles are oriented to this bound • Instant t (after odometry step) -Most particles are outside the room R1 -Positions &lt; x, y &gt; of these particles do not correspond to trained rooms</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="5,106.56,536.45,389.94,103.10"><head>Table 1 :</head><label>1</label><figDesc>Best 11  training frames with their associated weight and room</figDesc><table coords="5,106.56,536.45,389.94,103.10"><row><cell>Frame number</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell></row><row><cell>Weight</cell><cell cols="11">0.07 0.42 0.34 0.14 0.48 0.87 0.24 0.12 0.04 0.45 0.05</cell></row><row><cell>Room</cell><cell>CR</cell><cell>PA</cell><cell>PA</cell><cell>CR</cell><cell>PA</cell><cell>PA</cell><cell>CR</cell><cell>CR</cell><cell>BO</cell><cell>PA</cell><cell>BO</cell></row><row><cell></cell><cell></cell><cell>Room</cell><cell></cell><cell>PA</cell><cell>CR</cell><cell>BO</cell><cell cols="2">EO KT</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="8">Sum of weights 2.56 0.57 0.09 0.00 0.00</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="5,178.20,651.77,246.70,9.62"><head>Table 2 :</head><label>2</label><figDesc>Separate sum of weights for the different rooms</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="12,90.00,111.65,442.26,103.58"><head>Table 3 :</head><label>3</label><figDesc>Score, number of correct, incorrect and non classified frames for different combinations of training and tests frame sequences</figDesc><table coords="12,92.11,111.65,440.15,69.74"><row><cell></cell><cell>Test Sequences</cell></row><row><cell>Training Seq.</cell><cell>Night (952 frames) Score C M NC Score Cloudy (928 frames) C M NC Score Sunny (909 frames) C M NC 531 643 224 85 285 433 265 230 265.5 421 311 177 Cloudy (915 fr.) 270.5 426 311 215 404.5 538 267 123 420.5 534 227 148 Night (1034 fr.) Sunny (950 fr.) 285.5 435 299 218 358.5 457 197 247 509 615 212 82</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="12,90.00,339.53,423.01,90.74"><head>Table 4 :</head><label>4</label><figDesc><ref type="bibr" coords="12,470.65,339.53,3.90,9.62" target="#b3">4</ref>. Score, number of correct, incorrect and non classified frames for the best run in the obligatory track</figDesc><table coords="12,171.96,362.09,259.21,34.34"><row><cell></cell><cell></cell><cell>Final Test Sequence</cell><cell></cell></row><row><cell cols="4">Score Correctly Classified Misclassified Not Classified</cell></row><row><cell>511.0</cell><cell>676</cell><cell>330</cell><cell>684</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="13,90.00,189.29,447.42,103.58"><head>Table 5 :</head><label>5</label><figDesc>Score, number of correct, incorrect and non classified frames for different combinations of training and tests frame sequences</figDesc><table coords="13,92.11,189.29,445.31,69.74"><row><cell></cell><cell>Test Sequences</cell></row><row><cell>Training Seq</cell><cell>Night (952 frames) Score C M NC Score Cloudy (928 frames) C M NC Score Sunny (909 frames) C M NC Night (1034 fr.) 837.5 861 47 44 534.0 635 202 91 476.5 560 167 182 Cloudy (915 fr.) 600.5 695 189 68 680.5 748 135 45 733.5 774 81 54 Sunny (950 fr.) 725.0 791 132 29 701.0 769 136 23 798.5 823 49 37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="13,90.00,481.13,423.01,68.18"><head>Table 6 :</head><label>6</label><figDesc>Score, number of correct, incorrect and non classified frames for the best run in the optional track</figDesc><table coords="13,171.96,481.13,259.21,34.34"><row><cell></cell><cell></cell><cell>Test Sequence</cell><cell></cell></row><row><cell cols="4">Score Correctly Classified Misclassified Not Classified</cell></row><row><cell>916.5</cell><cell>1072</cell><cell>311</cell><cell>217</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="1,105.24,736.97,95.51,7.68"><p>http://www.robocup.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="2,105.24,733.01,149.19,7.68"><p>http://web.engr.oregonstate.edu/∼hess/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was partially supported by the <rs type="funder">Spanish "Junta de Comunidades de Castilla-La Mancha (Consejería de Educación y Ciencia)</rs>" under <rs type="grantNumber">PCI08-0048-8577</rs> and <rs type="grantNumber">PBI-0210-7127</rs> Projects, <rs type="funder">FEDER</rs> funds, and the <rs type="programName">Spanish research programme Consolider Ingenio 2010</rs> <rs type="projectName">MIPRCV</rs> (<rs type="grantNumber">CSD2007-00018</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_qEeBYRh">
					<idno type="grant-number">PCI08-0048-8577</idno>
				</org>
				<org type="funded-project" xml:id="_TH6Te5Q">
					<idno type="grant-number">PBI-0210-7127</idno>
					<orgName type="project" subtype="full">MIPRCV</orgName>
					<orgName type="program" subtype="full">Spanish research programme Consolider Ingenio 2010</orgName>
				</org>
				<org type="funding" xml:id="_M2HDVmU">
					<idno type="grant-number">CSD2007-00018</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct coords="14,110.52,328.97,402.52,9.62;14,110.52,340.85,209.29,9.62" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="14,292.49,328.97,202.68,9.62">Overview of the CLEF 2009 robot vision track</title>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pronobis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jensfelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,110.52,340.85,112.06,9.62">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,110.52,360.77,402.62,9.62;14,110.52,372.77,285.97,9.62" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="14,326.11,360.77,182.70,9.62">Monte carlo localization for mobile robots</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,122.88,372.77,243.02,9.62">Proc. of the IEEE International Conference on Robotics</title>
		<meeting>of the IEEE International Conference on Robotics</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,110.52,392.69,402.47,9.62;14,110.52,404.69,215.53,9.62" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="14,273.07,392.69,194.34,9.62">Active markov localization for mobile robots</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,477.00,392.69,35.99,9.62;14,110.52,404.69,109.93,9.62">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="195" to="207" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,110.52,424.61,402.58,9.62;14,110.52,436.49,327.61,9.62" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="14,271.75,424.61,241.35,9.62;14,110.52,436.49,38.30,9.62">Markov localization for mobile robots in dynamic environments</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,157.92,436.49,179.69,9.62">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">391-427</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,110.52,456.41,402.66,9.62;14,110.52,468.41,43.57,9.62" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="14,211.72,456.41,223.66,9.62">Method and means for recognizing complex patterns</title>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Hough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,468.49,456.41,44.69,9.62">US Patent</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">654</biblScope>
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,110.52,488.33,402.39,9.62;14,110.52,500.33,307.70,9.62" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="14,161.77,488.33,225.22,9.62">Object recognition from local scale-invariant features</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,405.36,488.33,107.55,9.62;14,110.52,500.33,87.27,9.62">International Conference on Computer Vision</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,110.52,520.25,402.51,9.62;14,110.52,532.13,176.90,9.62" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="14,164.53,520.25,245.92,9.62">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,419.28,520.25,93.75,9.62;14,110.52,532.13,84.63,9.62">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,110.52,552.05,402.61,9.62;14,110.52,564.05,342.25,9.62" xml:id="b7">
	<monogr>
		<title level="m" type="main" coord="14,340.62,552.05,118.74,9.62">The KTH-IDOL2 database</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Pronobis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Jensfelt</surname></persName>
		</author>
		<idno>CVAP304</idno>
		<imprint>
			<date type="published" when="2006-10">October 2006</date>
		</imprint>
		<respStmt>
			<orgName>Kungliga Tekniska Hoegskolan, CVAP/CAS</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="14,110.52,583.97,402.53,9.62;14,110.52,595.97,365.54,9.62" xml:id="b8">
	<monogr>
		<title level="m" type="main" coord="14,213.51,583.97,164.55,9.62">Kalman filters and robot localization</title>
		<author>
			<persName coords=""><forename type="first">Rudy</forename><forename type="middle">R</forename><surname>Negenbornj</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>Utrecht, Netherlands</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Institute of Information and Computer Science, Utrecht University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct coords="14,110.52,615.89,402.54,9.62;14,110.52,627.77,79.34,9.62" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="14,283.97,615.89,95.51,9.62">The taylor expansions</title>
		<author>
			<persName coords=""><forename type="first">Akira</forename><surname>Nishino</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yasunari</forename><surname>Shidama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" coord="14,389.88,615.89,104.15,9.62">Formalized Mathematics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="195" to="200" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,110.52,647.69,402.50,9.62;14,110.52,659.69,261.74,9.62" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="14,228.87,647.69,222.41,9.62">Robust color segmentation for the robocup domain</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Wasik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Saffiotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="14,461.52,647.69,51.50,9.62;14,110.52,659.69,181.60,9.62">Proc. of the Int. Conf. on Pattern Recognition (ICPR)</title>
		<meeting>of the Int. Conf. on Pattern Recognition (ICPR)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="651" to="654" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
