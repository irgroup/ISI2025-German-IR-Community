<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" coord="1,101.38,148.86,400.25,15.15;1,129.40,170.78,344.20,15.15">Overview of the CLEF 2009 Large-Scale Visual Concept Detection and Annotation Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,235.61,204.67,64.67,8.74"><forename type="first">Stefanie</forename><surname>Nowak</surname></persName>
							<email>stefanie.nowak@idmt.fraunhofer.de</email>
							<affiliation key="aff0">
								<orgName type="department">Semantic Audio-Visual Systems</orgName>
								<orgName type="institution">Fraunhofer IDMT</orgName>
								<address>
									<settlement>Ilmenau</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName coords="1,308.95,204.67,58.45,8.74"><forename type="first">Peter</forename><surname>Dunker</surname></persName>
							<email>peter.dunker@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">Semantic Audio-Visual Systems</orgName>
								<orgName type="institution">Fraunhofer IDMT</orgName>
								<address>
									<settlement>Ilmenau</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" coord="1,101.38,148.86,400.25,15.15;1,129.40,170.78,344.20,15.15">Overview of the CLEF 2009 Large-Scale Visual Concept Detection and Annotation Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F68190320A9FC2CFC81ABB587FD0CC1E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-06-26T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Storage and Retrieval]: H.3.1 Content Analysis and Indexing</term>
					<term>H.3.3 Information Search and Retrieval</term>
					<term>H.3.4 Systems and Software</term>
					<term>H.3.7 Digital Libraries</term>
					<term>H.2 [Database Managment]: H.2.4 Systems-Multimedia Databases Measurement, Performance, Experimentation, Benchmark Image Classification and Annotation, Knowledge Structures, Evaluation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The large-scale visual concept detection and annotation task (LS-VCDT) in Image-CLEF 2009 aims at the detection of 53 concepts in consumer photos. These concepts are structured in an ontology which implies a hierarchical ordering and which can be utilized during training and classification of the photos. The dataset consists of 18.000 Flickr photos which were manually annotated with 53 concepts. 5.000 photos were used for training and 13.000 for testing. Altogether 19 research groups participated and submitted 73 runs. Two evaluation paradigms have been applied, the evaluation per concept and the evaluation per photo. The evaluation per concept was performed by calculating the Equal Error Rate (EER) and the Area Under Curve (AUC). For the evaluation per photo a recently proposed hierarchical measure was utilized that takes the hierarchy and the relations of the ontology into account and calculates a score per photo. For the concepts, an average AUC of 84% could be achieved, including concepts with an AUC of 95%. The classification performance for each photo ranged between 69% and 100% with an average score of 90%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic methods for archiving, indexing and retrieving multimedia content become more and more important through the steadily increasing amount of digital data on the web and at home. These methods are often difficult to compare as they are evaluated on different kinds of datasets and concerning different concepts to be annotated. CLEF is an evaluation initative that aims at comparing approaches and results in cross-language retrieval for 10 years now. One track of CLEF is ImageCLEF which deals with the evaluation of image-based approaches in the medical and consumer photo domain. This year ImageCLEF posed six task. In the LS-VCDT the participants were asked to annotate a number of photos with a defined set of concepts in a multilabel scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description, Database and Ontology</head><p>The focus of LS-VCDT lies on the automatic detection and annotation of concepts in a large consumer photo collection. It mainly poses two challenges:</p><p>1. Can image classifiers scale to the large amount of concepts and data? 2. Can an ontology (hierarchy and relations) help in large scale annotations?</p><p>In this task, the MIR Flickr 25.000 image dataset <ref type="bibr" coords="2,305.39,205.57,15.50,8.74" target="#b9">[10]</ref> is utilized. This collection consists of 25.000 photos from Flickr with creative commons license. Most of them contain EXIF data, stored in a separate text file. We used altogether 18.000 of theses photos, annotated them manually with the defined visual concepts and provided them to the participants.</p><p>The training set consists of 5.000 and the testset of 13.000 images of the photoset. All images have multiple annotations. Most annotations refer to holistic visual concepts and are annotated at an image-based level. Altogether we provided the annotations for 53 concepts in rdf format and as plain text files. The visual concepts are organized in a small ontology. Participants may use the hierarchical order of the concepts and the relations between concepts for solving the annotation task. It was not allowed to use additional data for the training of the systems to ensure comparability among the groups.</p><p>The LS-VCDT is an extension of the former VCDT 2008 concerning the amount of data available and the amount of concepts to be annotated. In 2008, the database was quite small with about 1.800 images for training, 1.000 images for testing and 17 concepts to be detected.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Annotation Process</head><p>The annotation process was realized in three steps. First the annotation of all photos was performed by several annotators, second a validation step of these annotations was conducted and third an agreement between different annotators for the same concepts and photos was calculated.</p><p>The annotation of 18.000 photos was performed by 43 persons from the Fraunhofer IDMT. The number of photos that were annotated by one person varied between 30 and 2.500 images. All annotators were provided with a definition of the concepts and example images with the goal to allow a consistent annotation amongst the large number of persons. It was important that the concepts are represented over the whole image. Some of the concepts exclude each other, others can be depicted simultaneously. One example photo per concept is illustrated in Fig. <ref type="figure" coords="3,478.78,226.05,4.98,8.74" target="#fig_0">1</ref> and<ref type="figure" coords="3,508.02,226.05,4.98,8.74">a</ref> complete list of all concepts can be found in Table <ref type="table" coords="3,306.61,238.01,3.88,8.74" target="#tab_4">1</ref>. The frequency of each concept in the training and test sets is also depicted.</p><p>After this first annotation step, a validation of the annotations was performed. Due to the number of people, the number of photos and the ambiguity of some image contents, the annotations were not consistent throughout the database. Three persons performed a validation by screening only those photos that a) were annotated with concept X and b) that were not annotated with concept X. In the first case they had to delete all annotations for concepts that were not depicted at the photo and so were wrongly assigned. In the second case the goal was to find the photos where an annotation for concept X was missing but where the concept was visible.</p><p>Additionally, a subset of 100 photos was annotated by 11 different persons. These annotations are used to calculate an agreement between annotators for different concepts and photos. The agreement on concepts is illustrated in Table <ref type="table" coords="3,287.97,369.51,3.88,8.74" target="#tab_4">1</ref>. For each photo and each concept, the annotation of the majority of annotators was regarded as correct and the percentage of annotators that annotated correct is utilized as agreement factor. This agreement is used in the Hierarchical Score (HS) as scaling factor (see Sec. 2.3). In case of a low agreement the algorithm assumes that the concept is ambiguous and therefore reduces the costs if the system wrongly assigns this concept. Regrettably, there was no possibility to annotate each photo by two or three persons to get a validation and an agreement on concepts over the whole set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ontology</head><p>In addition to the photos and their annotation an ontology was provided. All concepts are structured in this ontology. Fig. <ref type="figure" coords="3,214.35,499.48,4.98,8.74" target="#fig_1">2</ref> shows a simple hierarchical organization of a part of the concepts. The hierarchy allows to make assumptions about the assignment of concepts to documents. E.g., if a photo is classified to contain trees, it also contains plants. Then, next to the is-a relationship of the hierarchical organization of concepts, additionally other relationships between concepts determine possible label assignments. The ontology restricts e.g., that for a certain sub-node only one concept can be assigned at a time (disjoint items) or that a special concept (like portrait) postulates other concepts like persons or animals.</p><p>The ontology allows the participants to incorporate knowledge in their classification algorithms, and to make assumptions about which concepts are probable in combination with certain labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluation Measures</head><p>The evaluation of submissions to LS-VCDT considers two evaluation paradigms. We are interested in the evaluation per concept and in the evaluation per photo. For the evaluation per concept, the EER and the AUC of the ROC curves summarize the performance of the individual runs. The EER is defined as the point where the false acceptance rate of a system is equal to the false rejection rate. These scores were also used in the VCDT task 2008 and allow to compare the results of the different groups to some overlapping concepts. The evaluation per photo is assessed with a recently proposed hierarchical measure <ref type="bibr" coords="3,263.27,713.13,14.62,8.74" target="#b14">[15]</ref>. It considers partial matches between system output and ground truth and calculates misclassification costs for each missing or wrongly annotated concept per image. The score is based on structure information (distance between concepts in the  hierarchy), relationships from the ontology and the agreement between annotators for a concept. The calculation of misclassification costs favours systems that annotate a photo with concepts close to the correct ones more than systems that annotate concepts that are far away in the hierarchy from the correct concepts. (E.g. for the single-label classification case depicted in Fig. <ref type="figure" coords="5,472.09,395.57,3.88,8.74" target="#fig_1">2</ref>, system 1 gets lower misclassification costs than system 2.)</p><p>3 Results</p><p>19 participants submitted results to the LS-VCDT task in altogether 73 runs. The number of runs was restricted to a maximum of 5 runs per group. In Table <ref type="table" coords="5,145.38,486.20,4.98,8.74" target="#tab_5">2</ref> the results for the evaluation per concept are illustrated. The team with the best results (ISIS University of Amsterdam) achieves an EER of 23% and an AUC of 84% in average for their best run. One run with pseudo-random numbers was added by the organizers. In this case for each concept a random number between 0 and 1 was generated that denotes the confidence of the annotation for the EER/AUC computation and that was rounded to 0 or 1 for the hierarchical measure per photo. The random numbers achieve an EER and AUC of 50%.</p><p>In Table <ref type="table" coords="5,146.95,557.93,4.98,8.74" target="#tab_7">3</ref> the results for each concept are summarized. In average the concepts could be detected with an EER of 23% and an AUC of 84%. A great amount of these concepts was classified best by the ISIS group. It is obvious that the aesthetic concepts (Aesthetic_Impression, Overall_Quality and Fancy) are classified worst (EER greater than 38% and AUC smaller than 66%.). This is not suprising due to the subjective nature of these concepts which also made the groundtruthing very difficult. The best classified concepts are Clouds (AUC: 96%), Sunset-Sunrise(AUC: 95%), Sky(AUC: 95%) and Landscape-Nature (AUC: 94%).</p><p>In Table <ref type="table" coords="5,144.93,641.62,4.98,8.74" target="#tab_8">4</ref> the results for the evaluation per photo are summarized. The classification performance per photo ranges between 69% and 100% with an average of 90%. The best results in terms of HS were achieved by the XRCE group with 83% annotation score over all photos. It can be seen from the table that the ranking of the groups is different than for the EER/AUC. It seems that some of the groups took the ontology information into account (at least in a post-processing step) and others ignored it. The include of the annotator agreements does not change the results substantially. The scores are a bit worse as the measure is stricter, but the ranking of the groups remains. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Submitted Technologies</head><p>This subsection gives a brief overview about the submitted technologies of the participants. Further information about each approach can be found in the corresponding papers. The IAM group <ref type="bibr" coords="6,186.62,435.35,10.52,8.74" target="#b8">[9]</ref> focuses on visual-terms, which are created by low-level features mainly based on SIFT and a following codebook quantization. For machine learning, the Cross Language Latent Indexing method was applied which maps the concept names and the visual-terms into a semantic space. The decision classification is handled by estimating the smallest cosine distance between concepts and visual-terms. With the different runs, a successive expansion of the concept hierarchy was tried, which results in no improvement.</p><p>The algorithm of the TELECOM ParisTech group <ref type="bibr" coords="6,341.45,507.09,10.52,8.74" target="#b6">[7]</ref> was designed especially for the largescale scenario, which means a low complexity for the processing and an easy extension to a variety of concepts, by accepting a decrease of precision. The algorithm utilizes global visual features and text features generated out of the 53 visual concepts via PCA. A Canonical Correlation Analysis is used to capture linear relationships between these different features spaces.</p><p>The UPMC/LIP6 group <ref type="bibr" coords="6,229.43,566.86,10.52,8.74" target="#b5">[6]</ref> utilizes a simple HSV histogram feature calculated in 3 horizontal segments and a linear kernel SVM for learning.</p><p>The MRIM-LIG group <ref type="bibr" coords="6,220.90,590.77,15.50,8.74" target="#b12">[13]</ref> combines RGB histograms, SIFT and Gabor features. For the learning phase, different SVM combinations are trained and as a priori, the best feature/SVM setup for each concept is used.</p><p>The LSIS group <ref type="bibr" coords="6,187.70,626.64,15.50,8.74" target="#b17">[18]</ref> combines different features, e.g. HSV, edge, gabor or profile entropy and applies a Visual Dictionary with a visual-word approach.</p><p>The AVEIR submissions <ref type="bibr" coords="6,228.18,650.55,10.52,8.74" target="#b7">[8]</ref> are from a joint group of the individual participants: Telecom ParisTech, LSIS, MRIM-LIG and UPMC/LIP6. The AVEIR submissions seem to be equal to the individual submissions. An efficient and reliable combination or fusion method based on a carefulness index is discussed only theoretically in the working notes.</p><p>The KameyamaLab group <ref type="bibr" coords="6,232.84,698.37,15.50,8.74" target="#b15">[16]</ref> proposed a system with joint global color and texture features as well as local features based on saliency regions. Additionally, a gist of scene feature is used. For the assignment of concept labels, a KNN classifier is applied.</p><p>The ISIS group <ref type="bibr" coords="6,180.48,734.23,15.50,8.74" target="#b16">[17]</ref> applies a system that is based on four main steps. First, a sampling strat-  egy is applied that combines a spatial pyramid approach and saliency points detection. Second, SIFT features are extracted in different color spaces. To reduce the amount of visual features, a codebook transformation is utilized in the third step and the frequency information of predefined codewords is used as final feature. The final learning step is based on SVM with χ 2 kernel. The runs differ mainly in the number of SIFT features used and the codebook generation. The FIRST group <ref type="bibr" coords="8,194.34,464.79,10.52,8.74" target="#b1">[2]</ref> used SIFT features on different color channels and pyramid histograms over color intensities. The SIFT features are combined by the bag of words approach. For classification, a SVM was applied with average kernel, sparse L1 MKL and non-sparse L p MKL kernel.</p><p>The XRCE group <ref type="bibr" coords="8,194.06,500.65,10.52,8.74" target="#b0">[1]</ref> uses a set of different features, a GMM image representation, a Fisher vector and local RGB statistics and SIFT features. The local features are extracted on a multi-level image-grid. For the classification, a Sparse Logistic Regression approach was applied. In the postprocessing, the hierarchical structure, disjoint-concepts and relating concepts were considered.</p><p>The SZTAKI 1 group <ref type="bibr" coords="8,212.64,548.47,10.52,8.74" target="#b2">[3]</ref> used SIFT features and a graph-based segmentation algorithm. Based on the segments, color histograms, shape and DFT features are estimated. The SIFT features are post-processed with a GMM, and a Fisher kernel is applied on the features derived from the segmentation. For classification, a binary logistic regression approach is utilized. As one of a few groups the SZTAKI group applied the connections between concepts in the provided ontology among others to estimate correlations of appearing concepts.</p><p>The INRIA-LEAR group <ref type="bibr" coords="8,232.36,620.20,10.52,8.74" target="#b3">[4]</ref> utilizes a bag-of-features setup with global features namely a gist of scene descriptor and different color histograms applied in three horizontal regions of the image and the local SIFT feature quantized with k-means. In two runs a weighted nearest neighbor tag prediction method is applied and in two runs a SVM for each concept is used. The fifth run uses a SVM classifier, trained for multi-class separation. The SVM runs performed better than the tag prediction, whilst the tag prediction was ten times faster. No post-processing on the ontology rules was performed, therefore the results of the HS are worse than the EER.</p><p>The TIA-INAOE group <ref type="bibr" coords="8,224.86,703.89,10.52,8.74" target="#b4">[5]</ref> provided an algorithm based on global features, e.g. color and edge histograms. As baseline run, a KNN classifier is used and the most often appearing concepts in the top nearest neighbor training images were assigned. A further label refinement process concentrates on co-occurrence statistics of the disjoint concepts of training set.</p><p>The CVIU I2R group <ref type="bibr" coords="9,214.28,123.98,15.50,8.74" target="#b13">[14]</ref> provides a system that utilizes various global and local features, e.g. color and edge histograms, color coherence vector, census transform and different SIFT features. Furthermore, a local region search algorithm is used to previously select a relevant bounding box for each concept. In combination with a SVM with χ 2 kernel a feature selection process is applied to choose the most relevant features for each concept. For the disjoint concepts, the probabilities were manipulated in order to have a single concept above 0.5.</p><p>The UAIC group <ref type="bibr" coords="9,194.73,195.71,15.50,8.74" target="#b10">[11]</ref> utilizes four modules. First, a face detection software was used to estimate the number of faces in images. Unfortunately, this modules breaks the rules of the LS-VCDT, because the face detector is based on a Viola and Jones detector which was trained with data that was not provided in this task. The second module concentrates on clustering of training images, whereas most concepts were set to a score of 0.5 if no decision could be made. The same process was applied in an EXIF data processing module. The last module sets default values to disjoint concepts depending on their occurrence in the training data.</p><p>The MMIS submissions <ref type="bibr" coords="9,229.03,279.39,15.50,8.74" target="#b11">[12]</ref> utilize global color histogram, Tamura texture and Gabor features. Selected features are estimated in nine subregions and concatenated to the overall feature vector. A non-parametric density estimation is applied and a baseline approach by global feature weighting was submitted. The other four submissions use different parameter combinations for word correlations to semantic similarity. The runs differ by the source of the semantic similarity space, which was estimated by the training data, by Google Web search, WordNet and Wikipedia measure. The submission based on the training data achieved the best results.</p><p>Summarizing the approaches, some facts can be driven. The groups that used local features like SIFT achieved better results than the groups relying solely on global features. Most groups that investigated the concept hierarchy and analyzed, e.g. the correlations between the concepts, could achieve better ranks evaluated with the hierarchical measure than with the EER. The information about the computational performance are difficult to compare, because the information range from 72 hours for the complete process, to 1 second for training and testing. In the 2010 task, a more detailed specification for this information is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper summarises the ImageCLEF 2009 LS-VCDT. Its aim was to automatically annotate photos with 53 concepts in a multilabel scenario. An additionally provided ontology could be used to enrich the classification system. The results show that in average the task could be solved reasonably well with the best system achieving an AUC of 84% for all photos. Four other groups got an AUC score over or equal to 80%. Evaluated on the concept basis, the concepts could be annotated in average with an AUC of 84%. In terms of HS, the best system annotated all photos with an average annotation rate of 83%. Three other systems were very close to these results with 83%, 82% and 81%. Part of the groups used the ontology for post-processing or to learn correlations of concepts. No participant integrated the ontology in a reasoning system and tried to apply this system for the classification task. The large number of concepts and photos posed no problem to the classification systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgment</head><p>This work has been partly supported by grant No. 01MQ07017 of the German research program THESEUS funded by the Ministry of Economics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,90.00,695.83,423.00,8.74;2,90.00,707.78,115.26,8.74"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example photos for each concept. The numbers below the photos denote the concept number (see alsoTable1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="5,90.00,315.92,423.00,8.74;5,90.00,327.88,360.43,8.74;5,116.59,111.23,376.16,181.01"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of an ontology fragment for image annotation. The concepts are hierarchically structured and different types of relationships are exemplarily highlighted.</figDesc><graphic coords="5,116.59,111.23,376.16,181.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="2,166.25,707.78,24.07,8.74"><head>Table</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="4,90.00,140.49,423.00,583.26"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table coords="4,152.87,140.49,297.27,539.84"><row><cell>No.</cell><cell>Concept</cell><cell>Train (%)</cell><cell>Test (%)</cell><cell>Annotator</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Agreement</cell></row><row><cell></cell><cell>Partylife</cell><cell>3,26</cell><cell>4,18</cell><cell>0.97</cell></row><row><cell></cell><cell>Familiy Friends</cell><cell>13,18</cell><cell>15,26</cell><cell>0.91</cell></row><row><cell></cell><cell>Beach Holidays</cell><cell>1,56</cell><cell>2,82</cell><cell>0.96</cell></row><row><cell></cell><cell>Building Sights</cell><cell>10,74</cell><cell>11,17</cell><cell>0.93</cell></row><row><cell></cell><cell>Snow</cell><cell>1,66</cell><cell>1,35</cell><cell>1.0</cell></row><row><cell></cell><cell>Citylife</cell><cell>13,6</cell><cell>15,88</cell><cell>0.90</cell></row><row><cell></cell><cell>Landscape Nature</cell><cell>15,92</cell><cell>17,13</cell><cell>0.94</cell></row><row><cell></cell><cell>Sports</cell><cell>1,56</cell><cell>2,41</cell><cell>0.99</cell></row><row><cell></cell><cell>Desert</cell><cell>0,36</cell><cell>0,35</cell><cell>0.99</cell></row><row><cell></cell><cell>Spring</cell><cell>2,02</cell><cell>0,62</cell><cell>0.98</cell></row><row><cell></cell><cell>Summer</cell><cell>12,82</cell><cell>7,80</cell><cell>0.87</cell></row><row><cell></cell><cell>Autumn</cell><cell>1,7</cell><cell>1,56</cell><cell>0.98</cell></row><row><cell></cell><cell>Winter</cell><cell>3,0</cell><cell>1,92</cell><cell>0.99</cell></row><row><cell></cell><cell>No Visual Season</cell><cell>80,46</cell><cell>88,10</cell><cell>0.84</cell></row><row><cell></cell><cell>Indoor</cell><cell>28,58</cell><cell>24,50</cell><cell>0.90</cell></row><row><cell></cell><cell>Outdoor</cell><cell>53,42</cell><cell>51,48</cell><cell>0.96</cell></row><row><cell></cell><cell>No Visual Place</cell><cell>18</cell><cell>24,02</cell><cell>0.88</cell></row><row><cell></cell><cell>Plants</cell><cell>11,4</cell><cell>26,84</cell><cell>0.91</cell></row><row><cell></cell><cell>Flowers</cell><cell>5,08</cell><cell>4,38</cell><cell>0.95</cell></row><row><cell></cell><cell>Trees</cell><cell>9,86</cell><cell>12,71</cell><cell>0.94</cell></row><row><cell></cell><cell>Sky</cell><cell>20,92</cell><cell>27,47</cell><cell>0.92</cell></row><row><cell></cell><cell>Clouds</cell><cell>12,78</cell><cell>14,57</cell><cell>0.95</cell></row><row><cell></cell><cell>Water</cell><cell>6,7</cell><cell>11,35</cell><cell>0.97</cell></row><row><cell></cell><cell>Lake</cell><cell>1,04</cell><cell>1,35</cell><cell>0.98</cell></row><row><cell></cell><cell>River</cell><cell>1,42</cell><cell>1,83</cell><cell>0.99</cell></row><row><cell></cell><cell>Sea</cell><cell>3,02</cell><cell>3,30</cell><cell>0.97</cell></row><row><cell></cell><cell>Mountains</cell><cell>2,12</cell><cell>3,93</cell><cell>0.98</cell></row><row><cell></cell><cell>Day</cell><cell>54,02</cell><cell>52,28</cell><cell>0.88</cell></row><row><cell></cell><cell>Night</cell><cell>7,24</cell><cell>6,82</cell><cell>0.96</cell></row><row><cell></cell><cell>No Visual Time</cell><cell>38,76</cell><cell>40,90</cell><cell>0.88</cell></row><row><cell></cell><cell>Sunny</cell><cell>13,66</cell><cell>15,60</cell><cell>0.88</cell></row><row><cell></cell><cell>Sunset Sunrise</cell><cell>5,12</cell><cell>4,45</cell><cell>0.99</cell></row><row><cell></cell><cell>Canvas</cell><cell>2,28</cell><cell>2,78</cell><cell>0.98</cell></row><row><cell></cell><cell>Still Life</cell><cell>8,4</cell><cell>8,40</cell><cell>0.91</cell></row><row><cell></cell><cell>Macro</cell><cell>5,74</cell><cell>14,72</cell><cell>0.94</cell></row><row><cell></cell><cell>Portrait</cell><cell>10,3</cell><cell>15,79</cell><cell>0.92</cell></row><row><cell></cell><cell>Overexposed</cell><cell>0,96</cell><cell>1,61</cell><cell>0.99</cell></row><row><cell></cell><cell>Underexposed</cell><cell>5,92</cell><cell>4,45</cell><cell>0.92</cell></row><row><cell></cell><cell>Neutral Illumination</cell><cell>93,12</cell><cell>93,95</cell><cell>0.89</cell></row><row><cell></cell><cell>Motion Blur</cell><cell>2,84</cell><cell>3,30</cell><cell>0.97</cell></row><row><cell></cell><cell>Out of focus</cell><cell>1,98</cell><cell>1,55</cell><cell>0.98</cell></row><row><cell></cell><cell>Partly Blurred</cell><cell>30,56</cell><cell>26,51</cell><cell>0.86</cell></row><row><cell></cell><cell>No Blur</cell><cell>64,62</cell><cell>68,65</cell><cell>0.83</cell></row><row><cell></cell><cell>Single Person</cell><cell>21,74</cell><cell>20,82</cell><cell>0.96</cell></row><row><cell></cell><cell>Small Group</cell><cell>8,44</cell><cell>9,18</cell><cell>0.96</cell></row><row><cell></cell><cell>Big Group</cell><cell>2,8</cell><cell>3,25</cell><cell>0.98</cell></row><row><cell></cell><cell>No Persons</cell><cell>67,02</cell><cell>66,78</cell><cell>0.89</cell></row><row><cell></cell><cell>Animals</cell><cell>9,22</cell><cell>8,46</cell><cell>0.99</cell></row><row><cell></cell><cell>Food</cell><cell>2,98</cell><cell>3,77</cell><cell>0.98</cell></row><row><cell></cell><cell>Vehicle</cell><cell>4,26</cell><cell>7,59</cell><cell>0.97</cell></row><row><cell></cell><cell>Aesthetic Impression</cell><cell>16,98</cell><cell>18,04</cell><cell>0.75</cell></row><row><cell></cell><cell>Overall Quality</cell><cell>25,82</cell><cell>14,29</cell><cell>0.81</cell></row><row><cell></cell><cell>Fancy</cell><cell>14,88</cell><cell>13,33</cell><cell>0.84</cell></row></table><note coords="4,130.38,703.05,382.62,8.74;4,90.00,715.01,287.48,8.74"><p>Summary of the frequencies of each concept in the training and test sets. On the right the agreements between annotators are depicted for each concept.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="6,90.00,111.79,423.01,258.18"><head>Table 2 :</head><label>2</label><figDesc>Summary of the results for the evaluation per concept. The table shows the EER and AUC for the best run per group and the averaged EER and AUC for all runs of one group.</figDesc><table coords="6,293.08,111.79,158.79,6.09"><row><cell>Best Run</cell><cell>Average Runs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="7,90.00,698.32,423.00,20.69"><head>Table 3 :</head><label>3</label><figDesc>Overview of concepts and results per concept in terms of the best EER and best AUC per concept and the name of the group which achieved these results.</figDesc><table coords="8,284.57,111.79,172.96,6.09"><row><cell>Best Run</cell><cell>Average Runs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="8,90.00,349.27,423.00,32.65"><head>Table 4 :</head><label>4</label><figDesc>Summary of the results for the evaluation per photo. The table illustrates the average hierarchical score (HS) over all photos for the best run per group and the average HS per group. HS* denotes the scores if the annotator agreements are ignored during computation.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="9,110.48,730.62,402.51,8.74;9,110.48,742.57,209.14,8.74" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="9,325.06,730.62,183.40,8.74">XRCE&apos;s Participation in ImageCLEF 2009</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ah-Pine</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="9,110.48,742.57,112.47,8.74">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,112.02,402.51,8.74;10,110.48,123.98,402.53,8.74;10,110.48,135.93,56.98,8.74" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="10,250.34,112.02,262.65,8.74;10,110.48,123.98,242.69,8.74">Fraunhofer FIRST&apos;s Submission to ImageCLEF2009 Photo Annotation Task: Non-sparse Multiple Kernel Learning</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,363.13,123.98,113.37,8.74">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,155.43,402.52,8.74;10,110.48,167.38,343.76,8.74" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">B</forename><surname>Daroczy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Petras</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Benczur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Fekete</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Nemeskey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Siklosi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Weiner</surname></persName>
		</author>
		<title level="m" coord="10,110.48,167.38,247.09,8.74">SZTAKI @ ImageCLEF 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>CLEF working notes 2009</note>
</biblStruct>

<biblStruct coords="10,110.48,186.88,402.51,8.74;10,110.48,198.84,335.74,8.74" xml:id="b3">
	<monogr>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<title level="m" coord="10,413.24,186.88,99.76,8.74;10,110.48,198.84,239.07,8.74">INRIA-LEARs participation to ImageCLEF 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>CLEF working notes 2009</note>
</biblStruct>

<biblStruct coords="10,110.48,218.33,402.52,8.74;10,110.48,230.29,402.52,8.74;10,110.48,242.24,178.94,8.74" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="10,255.86,230.29,217.16,8.74">TIA-INAOE&apos;s Participation at ImageCLEF 2009</title>
		<author>
			<persName coords=""><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">A</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">A</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Montex</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">E</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Villasenor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,486.36,230.29,26.64,8.74;10,110.48,242.24,82.26,8.74">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,261.74,402.51,8.74;10,110.48,273.70,402.52,8.74;10,110.48,285.65,115.42,8.74" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="10,373.44,261.74,139.56,8.74;10,110.48,273.70,302.95,8.74">UPMC/LIP6 at ImageCLEFannotation 2009: Large Scale Visual Concept Detection and Annotation</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fakeri-Tabrizi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tollari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,422.75,273.70,90.25,8.74;10,110.48,285.65,18.74,8.74">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,305.15,402.52,8.74;10,110.48,317.10,393.63,8.74" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="10,237.53,305.15,275.47,8.74;10,110.48,317.10,175.42,8.74">TELECOM ParisTech at ImageClef 2009: Large Scale Visual Concept Detection and Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ferecatu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sahbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,294.97,317.10,112.47,8.74">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,336.60,402.52,8.74;10,110.48,348.56,402.52,8.74;10,110.48,360.51,386.33,8.74" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="10,300.62,348.56,212.38,8.74;10,110.48,360.51,168.40,8.74">Comparison of Various AVEIR Visual Concept Detectors with an Index of Carefulness</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Fakeri-Tabrizi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mulhem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ferecatu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Tollari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Quenot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Sahbi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dumont</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,287.67,360.51,112.47,8.74">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.48,380.01,402.52,8.74;10,110.48,391.96,350.71,8.74" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="10,228.92,380.01,284.08,8.74;10,110.48,391.96,132.81,8.74">IAM@ImageCLEFPhotoAnnotation 2009: Naive application of a linear-algebraic semantic space</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><forename type="middle">H</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,252.05,391.96,112.47,8.74">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,411.46,402.53,8.74;10,110.48,423.42,402.52,8.74;10,110.48,435.37,149.20,8.74" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="10,284.67,411.46,163.28,8.74">The MIR Flickr Retrieval Evaluation</title>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">J</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,472.39,411.46,40.61,8.74;10,110.48,423.42,398.38,8.74">MIR &apos;08: Proceedings of the 2008 ACM International Conference on Multimedia Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,454.87,402.52,8.74;10,110.48,466.82,209.14,8.74" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="10,286.39,454.87,221.95,8.74">UAIC at ImageCLEF 2009 Photo Annotation Task</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Vamanu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Croitoru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,110.48,466.82,112.47,8.74">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,486.32,402.53,8.74;10,110.48,498.28,315.95,8.74" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="10,278.01,486.32,234.99,8.74;10,110.48,498.28,97.74,8.74">MMIS at ImageCLEF 2009: Non-parametric Density Estimation Algorithms</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Llorente</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Rüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,217.29,498.28,112.47,8.74">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,517.77,402.52,8.74;10,110.48,529.73,402.52,8.74" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="10,367.22,517.77,145.78,8.74;10,110.48,529.73,190.25,8.74">MRIM-LIG at ImageCLEF 2009: Photo Retrieval and Photo Annotation tasks</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Mulhem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J-P</forename><surname>Chevallet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Quenon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><forename type="middle">Al</forename><surname>Batal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,307.94,529.73,110.23,8.74">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,549.23,402.53,8.74;10,110.48,561.18,178.94,8.74" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="10,221.25,549.23,252.15,8.74">I2R ImageCLEF Photo Annotation 2009 Working Notes</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,486.36,549.23,26.64,8.74;10,110.48,561.18,82.26,8.74">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,580.68,402.53,8.74;10,110.48,592.64,402.52,8.74;10,110.48,604.59,402.52,8.74;10,110.48,616.55,149.46,8.74" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="10,247.29,580.68,265.71,8.74;10,110.48,592.64,15.94,8.74">Multilabel Classification Evaluation using Ontology Information</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Lukashevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,148.72,592.64,364.28,8.74;10,110.48,604.59,402.52,8.74;10,110.48,616.55,34.84,8.74">The 1st Workshop on Inductive Reasoning and Machine Learning on the Semantic Web -IRMLeS 2009, co-located with the 6th Annual European Semantic Web Conference (ESWC)</title>
		<meeting><address><addrLine>Heraklion, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,636.04,402.52,8.74;10,110.48,648.00,265.86,8.74" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="10,247.62,636.04,265.37,8.74;10,110.48,648.00,47.55,8.74">Joint Contribution of Global and Local Features for Image Annotation</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Sarin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">W</forename><surname>Kameyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,167.20,648.00,112.47,8.74">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,667.50,402.52,8.74;10,110.48,679.45,402.52,8.74;10,110.48,691.41,22.70,8.74" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="10,373.82,667.50,139.18,8.74;10,110.48,679.45,207.60,8.74">The University of Amsterdam&apos;s Concept Detection System at ImageCLEF 2009</title>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,328.09,679.45,113.44,8.74">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,110.47,710.90,402.52,8.74;10,110.48,722.86,402.53,8.74;10,110.48,734.81,56.98,8.74" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="10,279.60,710.90,233.40,8.74;10,110.48,722.86,246.72,8.74">LSIS Scale Photo Annotations: Discriminant Features SVM versus Visual Dictionary based on Image Frequency</title>
		<author>
			<persName coords=""><forename type="first">Z-Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Dumont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" coord="10,365.72,722.86,111.42,8.74">CLEF working notes 2009</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
